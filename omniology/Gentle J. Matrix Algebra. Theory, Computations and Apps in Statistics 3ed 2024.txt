Springer Texts in Statistics
James E. Gentle
Matrix 
Algebra
Theory, Computations and Applications 
in Statistics
Third Edition

Springer Texts in Statistics 
Series Editors 
G. Allen, Department of Statistics, Rice University, Houston, TX, USA 
R. De Veaux, Department of Mathematics and Statistics, Williams College, 
Williamstown, MA, USA 
R. Nugent, Department of Statistics, Carnegie Mellon University, Pittsburgh, 
PA, USA

Springer Texts in Statistics (STS) includes advanced textbooks from 3rd- to 
4th-year undergraduate levels to 1st- to 2nd-year graduate levels. Exercise sets 
should be included. The series editors are currently Genevera I. Allen, Richard 
D. De Veaux, and Rebecca Nugent. Stephen Fienberg, George Casella, and 
Ingram Olkin were editors of the series for many years.

James E. Gentle 
Matrix Algebra 
Theory, Computations and 
Applications in Statistics 
Third Edition

James E. Gentle 
Fairfax, VA, USA 
ISSN 1431-875X
ISSN 2197-4136 (electronic) 
Springer Texts in Statistics 
ISBN 978-3-031-42143-3
ISBN 978-3-031-42144-0 (eBook) 
https://doi.org/10.1007/978-3-031-42144-0 
1st edition: © Springer Science+Business Media, LLC 2007 
2nd edition: © Springer International Publishing AG 2017 
3rd edition: © The Editor(s) (if applicable) and The Author(s), under exclusive license to 
Springer Nature Switzerland AG 2024 
This work is subject to copyright. All rights are solely and exclusively licensed by the 
Publisher, whether the whole or part of the material is concerned, speciﬁcally the rights 
of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on 
microﬁlms or in any other physical way, and transmission or information storage and re-
trieval, electronic adaptation, computer software, or by similar or dissimilar methodology 
now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in 
this publication does not imply, even in the absence of a speciﬁc statement, that such names 
are exempt from the relevant protective laws and regulations and therefore free for general 
use. 
The publisher, the authors, and the editors are safe to assume that the advice and informa-
tion in this book are believed to be true and accurate at the date of publication. Neither the 
publisher nor the authors or the editors give a warranty, expressed or implied, with respect 
to the material contained herein or for any errors or omissions that may have been made. 
The publisher remains neutral with regard to jurisdictional claims in published maps and 
institutional aﬃliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland 
AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland 
Paper in this product is recyclable.

To María

Preface to Third Edition 
This book is diﬀerent from the several other books on the general topic of 
“matrix algebra and statistics” or “linear algebra and statistics” in its more 
extensive coverage of the applications to statistical linear models (mostly in 
Part II, especially Chap. 9) and the discussions of numerical computations 
(mostly in Part III). This book also includes numerous examples of R in 
matrix computations. 
The lengths of the chapters vary; I emphasized unity of topics rather than 
consistency of numbers of pages. Some topics receive attention at multiple 
places in the book. The repetition is intentional because of diﬀerent motiva-
tions or points of view. The book has extensive cross-references and a large 
index to facilitate ﬁnding discussions in other parts of the book. 
The book can serve diﬀerent audiences. It can be used as a general reference 
book for an applied mathematician, a self-learner, or someone who just needs 
a refresher. The extensive index should be useful for such persons. 
The book, especially Part I, the ﬁrst seven chapters, can serve as a text for 
a mathematically oriented course in linear algebra for advanced undergradu-
ates or graduates. 
It could also serve as a textbook for a course in linear models. The primary 
emphasis would be on Part II, with extensive cross-references to the relevant 
theory in other chapters. 
A course in statistical computing or numerical analysis could be based on 
the nitty-gritty of computer arithmetic, the computational issues, and general 
design of algorithms in Part III. The emphasis would be on numerical linear 
algebra, and there would likely be extensive cross-references to the underlying 
theory in other chapters. 
There are separate sections in several chapters that discuss the R pro-
gramming system, starting from the basics and going through some of the 
more advanced features of R. There are also many examples and numerous 
exercises using R. This could serve as a quick course or a refresher course in 
R. R is also used for illustration in various places in the text.

VIII
Preface to Third Edition
As in the revisions for the second edition, in this third edition I have 
corrected all known remaining typos and other errors; I have (it is hoped) 
clariﬁed certain passages; I have added some additional material; and I have 
enhanced the Index. I have also added exercises in some of the chapters. 
The overall organization of chapters has been preserved, but some sections 
have been changed. The two chapters that have been changed most are the 
original Chap. 4, which is now Chap. 7 and has more coverage of multivariate 
probability distributions, and Chap. 9, with more material on linear models. 
In this edition, I discuss the R software system much more frequently. It 
is likely that most readers know R, but I give a brief introduction to R in 
Chap. 1. The most commonly used objects in this book are of class matrix, 
but I use data.frame for the linear models of Chap. 9. I do not use any of the 
objects, functions, or operators in the set of Tidy packages, which are very 
popular nowadays. 
I require use of R in several exercises, and assume the reader/user has, or 
will develop, at least a moderate level of competence in use of R. R, as any 
software system, is learned through usage. Some of the exercises, especially in 
Part III, also require competence in Fortran or C. 
The notation and terms that I use are “standard”; that is, they are (I 
think) among the most commonly used ones in discussions of matrices and 
linear algebra, especially by statisticians. Before delving into the book, the 
reader may want to take a quick look at Appendix A, and  then  refer to it  
whenever it is necessary to refresh the recognition of a symbol or term. 
In previous editions, I had included answers to selected exercises. In this 
edition, I have moved the solutions online. 
I thank the readers of the ﬁrst and second editions who informed me 
of errors, or who made suggestions for improvement. I particularly thank 
Professor M. Yatov for extensive comments on notation and deﬁnitions, as 
well as for noting several errata and gaps in logic. I also thank Shijia Jin 
for many general comments and suggestions. Any remaining typos, omissions, 
and so on are entirely my own responsibility. 
I thank John Chambers, Robert Gentleman, and Ross Ihaka for their foun-
dational work on R. I thank the R Core Team and the many package devel-
opers and those who maintain the packages that make R more useful. 
Again, I thank my wife, María, to whom this book is dedicated, for everything.

Preface to Third Edition
IX
I would appreciate receiving suggestions for improvement and notiﬁcation of 
errors. Notes on this book, hints and solutions to exercises, and errata are 
available at 
https://mason.gmu.edu/~jgentle/books/matbk/ 
Fairfax County, VA, USA
James E. Gentle 
June 30, 2023

Preface to Second Edition 
In this second edition, I have corrected all known typos and other errors; I 
have (it is hoped) clariﬁed certain passages; I have added some additional 
material; and I have enhanced the Index. 
I have added a few more comments about vectors and matrices with com-
plex elements, although, as before, unless stated otherwise, all vectors and 
matrices in this book are assumed to have real elements. I have begun to 
use “normal d normal e normal t left parenthesis upper A right parenthesisdet(A)” rather than “StartAbsoluteValue upper A EndAbsoluteValue|A|” to represent the determinant of A, except in 
a few cases. I have also expressed some derivatives as the transposes of the 
expressions I used formerly. 
I have put more conscious emphasis on “user-friendliness" in this edition. 
In a book, user-friendliness is primarily a function of references, both internal 
and external, and of the index. As an old software designer, I’ve always thought 
that user-friendliness is very important. To the extent that internal references 
were present in the ﬁrst edition, the positive feedback I received from users of 
that edition about the friendliness of those internal references (“I liked the fact 
that you said ‘equation (x.xx) on page yy’, instead of just ‘equation (x.xx)’. 
”) encouraged me to try to make the internal references even more useful. 
It’s only when you’re “eating your own dogfood”, that you become aware of 
where details matter, and in using the ﬁrst edition, I realized that the choice of 
entries in the Index was suboptimal. I have spent signiﬁcant time in organizing 
it, and I hope that the user will ﬁnd the Index to this edition to be very useful. 
I think that it has been vastly improved over the index in the ﬁrst edition. 
The overall organization of chapters has been preserved, but some sec-
tions have been changed. The two chapters that have been changed most are 
Chaps. 3 and 12. Chapter 3, on the basics of matrices, got about 30 pages 
longer. It is by far the longest chapter in the book, but I just didn’t see any 
reasonable way to break it up. In Chap. 12 of the ﬁrst edition, “Software for 
Numerical Linear Algebra”, I discussed four software systems or languages: 
C/C++, Fortran, MATLAB, and R, and did not express any preference for 
one over another. In this edition, although I occasionally mention various 
languages and systems, I now limit most of my discussion to Fortran and R.

XII
Preface to Second Edition
There are many reasons for my preference for these two systems. R is ori-
ented toward statistical applications. It is open source and freely distributed. 
As for Fortran versus C/C++, Python, or other programming languages, I 
agree with the statement by Hanson and Hopkins (2013, page ix), “. . . Fortran 
is currently the best computer language for numerical software.” Many people, 
however, still think of Fortran as the language their elders (or they themselves) 
used in the 1970s. (On a personal note, Richard Hanson, who passed away 
recently, was a member of my team that designed the IMSL C Libraries in 
the mid1980s. Not only was C much cooler than Fortran at the time, but the 
ANSI committee working on updating the Fortran language was so fractured 
by competing interests, that approval of the revision was repeatedly delayed. 
Many numerical analysts who were not concerned with coolness turned to C 
because it provided dynamic storage allocation and it allowed ﬂexible argu-
ment lists, and the Fortran constructs could not be agreed upon.) 
Language preferences are personal, of course, and there is a strong “cool-
ness factor” in choice of a language. Python is currently one of the coolest 
languages, but I personally don’t like the language for most of the stuﬀ I do. 
Although this book has separate parts on applications in statistics and 
computational issues as before, statistical applications have informed the 
choices I made throughout the book, and computational considerations have 
given direction to most discussions. 
I thank the readers of the ﬁrst edition who informed me of errors. Two 
people in particular made several meaningful comments and suggestions. Clark 
Fitzgerald not only identiﬁed several typos, he made several broad suggestions 
about organization and coverage that resulted in an improved text (I think). 
Andreas Eckner found, in addition to typos, some gaps in my logic, and also 
suggested better lines of reasoning at some places. (Although I don’t follow 
an itemized “theorem-proof” format, I try to give reasons for any non-obvious 
statements I make.) I thank Clark and Andreas especially for their comments. 
Any remaining typos, omissions, gaps in logic, and so on are entirely my 
responsibility. 
Again, I thank my wife, María, to whom this book is dedicated, for everything. 
I used TEX via  LATEX2ε to write the book. I did all of the typing, program-
ming, etc., myself, so all misteaks (mistakes!) are mine. I would appreciate 
receiving suggestions for improvement and notiﬁcation of errors. Notes on 
this book, including errata, are available at 
https://mason.gmu.edu/~jgentle/books/matbk/ 
Fairfax County, VA, USA
James E. Gentle 
July 14, 2017

Preface to First Edition 
I began this book as an update of Numerical Linear Algebra for Applications 
in Statistics, published by Springer in 1998. There was a modest amount of 
new material to add, but I also wanted to supply more of the reasoning behind 
the facts about vectors and matrices. I had used material from that text in 
some courses, and I had spent a considerable amount of class time proving 
assertions made but not proved in that book. As I embarked on this project, 
the character of the book began to change markedly. In the previous book, 
I apologized for spending 30 pages on the theory and basic facts of linear 
algebra before getting on to the main interest: numerical linear algebra. In 
the present book, discussion of those basic facts takes up over half of the book. 
The orientation and perspective of this book remains numerical linear al-
gebra for applications in statistics. Computational considerations inform the 
narrative. There is an emphasis on the areas of matrix analysis that are im-
portant for statisticians, and the kinds of matrices encountered in statistical 
applications receive special attention. 
This book is divided into three parts plus a set of appendices. The three 
parts correspond generally to the three areas of the book’s subtitle — theory, 
computations, and applications — although the parts are in a diﬀerent order, 
and there is no ﬁrm separation of the topics. 
Part I, consisting of Chaps. 1 through 6, covers most of the material in 
linear algebra needed by statisticians. (The word “matrix” in the title of the 
present book may suggest a somewhat more limited domain than “linear alge-
bra”; but I use the former term only because it seems to be more commonly 
used by statisticians and is used more or less synonymously with the latter 
term.) 
The ﬁrst four chapters cover the basics of vectors and matrices, concen-
trating on topics that are particularly relevant for statistical applications. In 
Chap. 7, it is assumed that the reader is generally familiar with the basics of 
partial diﬀerentiation of scalar functions. Chapters 4 through 6 begin to take 
on more of an applications ﬂavor, as well as beginning to give more consid-
eration to computational methods. Although the details of the computations

XIV
Preface to First Edition
are not covered in those chapters, the topics addressed are oriented more to-
ward computational algorithms. Chapter 4 covers methods for decomposing 
matrices into useful factors. 
Chapter 5 addresses applications of matrices in setting up and solving 
linear systems, including overdetermined systems. We should not confuse sta-
tistical inference with ﬁtting equations to data, although the latter task is a 
component of the former activity. In Chap. 5, we address the more mechanical 
aspects of the problem of ﬁtting equations to data. Applications in statistical 
data analysis are discussed in Chap. 9. In those applications, we need to make 
statements (that is, assumptions) about relevant probability distributions. 
Chapter 6 discusses methods for extracting eigenvalues and eigenvectors. 
There are many important details of algorithms for eigenanalysis, but they are 
beyond the scope of this book. As with other chapters in Part I, Chap. 6 makes 
some reference to statistical applications, but it focuses on the mathematical 
and mechanical aspects of the problem. 
Although the ﬁrst part is on “theory”, the presentation is informal; neither 
deﬁnitions nor facts are highlighted by such words as “Deﬁnition”, “Theorem”, 
“Lemma”, and so forth. It is assumed that the reader follows the natural 
development. Most of the facts have simple proofs, and most proofs are given 
naturally in the text. No “Proof” and “Q.E.D.” or “ 
” appear to indicate 
beginning and end; again, it is assumed that the reader is engaged in the 
development. For example, on page 378: 
If A is nonsingular and symmetric, then upper A Superscript negative 1A−1 is also symmetric because 
left parenthesis upper A Superscript negative 1 Baseline right parenthesis Superscript normal upper T Baseline equals left parenthesis upper A Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1(A−1)T = (AT)−1 = A−1. 
The ﬁrst part of that sentence could have been stated as a theorem and 
given a number, and the last part of the sentence could have been introduced 
as the proof, with reference to some previous theorem that the inverse and 
transposition operations can be interchanged. (This had already been shown 
before page 378 — in an unnumbered theorem of course!) 
None of the proofs are original (at least, I don’t think they are), but in most 
cases I do not know the original source, or even the source where I ﬁrst saw 
them. I would guess that many go back to C. F. Gauss. Most, whether they 
are as old as Gauss or not, have appeared somewhere in the work of C. R. Rao. 
Some lengthier proofs are only given in outline, but references are given for 
the details. Very useful sources of details of the proofs are Harville (1997), 
especially for facts relating to applications in linear models, and Horn and 
Johnson (1991) for more general topics, especially those relating to stochastic 
matrices. The older books by Gantmacher (1959) provide extensive coverage 
and often rather novel proofs. These two volumes have been brought back into 
print by the American Mathematical Society. 
I also sometimes make simple assumptions without stating them explicitly. 
For example, I may write “for all i” when  i is used as an index to a vector. 
I hope it is clear that “for all i” means only “for i that correspond to indices 
of the vector”. Also, my use of an expression generally implies existence. For

Preface to First Edition
XV
example, if “AB” is used to represent a matrix product, it implies that “A 
and B are conformable for the multiplication AB”. Occasionally I remind the 
reader that I am taking such shortcuts. 
The material in Part I, as in the entire book, was built up recursively. In the 
ﬁrst pass, I began with some deﬁnitions and followed those with some facts 
that are useful in applications. In the second pass, I went back and added 
deﬁnitions and additional facts that lead to the results stated in the ﬁrst 
pass. The supporting material was added as close to the point where it was 
needed as practical and as necessary to form a logical ﬂow. Facts motivated by 
additional applications were also included in the second pass. In subsequent 
passes, I continued to add supporting material as necessary and to address 
the linear algebra for additional areas of application. I sought a bare-bones 
presentation that gets across what I considered to be the theory necessary for 
most applications in the data sciences. The material chosen for inclusion is 
motivated by applications. 
Throughout the book, some attention is given to numerical methods for 
computing the various quantities discussed. This is in keeping with my be-
lief that statistical computing should be dispersed throughout the statistics 
curriculum and statistical literature generally. Thus, unlike in other books on 
matrix “theory”, I describe the “modiﬁed” Gram-Schmidt method, rather than 
just the “classical” GS. (I put “modiﬁed” and “classical” in quotes because, to 
me, GS is MGS. History is interesting, but in computational matters, I do 
not care to dwell on the methods of the past.) Also, condition numbers of 
matrices are introduced in the “theory” part of the book, rather than just 
in the “computational” part. Condition numbers also relate to fundamental 
properties of the model and the data. 
The diﬀerence between an expression and a computing method is em-
phasized. For example, often we may write the solution to the linear system 
upper A x equals bAx = b as upper A Superscript negative 1 Baseline bA−1b. Although this is the solution (so long as A is square and of 
full rank), solving the linear system does not involve computing upper A Superscript negative 1A−1. We may  
write upper A Superscript negative 1 Baseline bA−1b, but we know we can compute the solution without inverting the 
matrix. 
“This is an instance of a principle that we will encounter repeatedly: 
the form of a mathematical expression and the way the expression 
should be evaluated in actual practice may be quite diﬀerent.” 
(The statement in quotes appears word for word in several places in the book.) 
Standard textbooks on “matrices for statistical applications” emphasize 
their uses in the analysis of traditional linear models. This is a large and im-
portant ﬁeld in which real matrices are of interest, and the important kinds of 
real matrices include symmetric, positive deﬁnite, projection, and generalized 
inverse matrices. This area of application also motivates much of the discussion 
in this book. In other areas of statistics, however, there are diﬀerent matrices 
of interest, including similarity and dissimilarity matrices, stochastic matri-
ces, rotation matrices, and matrices arising from graph-theoretic approaches

XVI
Preface to First Edition
to data analysis. These matrices have applications in clustering, data mining, 
stochastic processes, and graphics; therefore, I describe these matrices and 
their special properties. I also discuss the geometry of matrix algebra. This 
provides a better intuition of the operations. Homogeneous coordinates and 
special operations in normal upper I normal upper R cubedIR3 are covered because of their geometrical applications 
in statistical graphics. 
Part II addresses selected applications in data analysis. Applications are 
referred to frequently in Part I, and of course, the choice of topics for coverage 
was motivated by applications. The diﬀerence in Part II is in its orientation. 
Only “selected” applications in data analysis are addressed; there are appli-
cations of matrix algebra in almost all areas of statistics, including the theory 
of estimation, which is touched upon in Chap. 7 of Part I. Certain types of 
matrices are more common in statistics, and Chap. 8 discusses in more detail 
some of the important types of matrices that arise in data analysis and sta-
tistical modeling. Chapter 9 addresses selected applications in data analysis. 
The material of Chap. 9 has no obvious deﬁnition that could be covered in a 
single chapter (or a single part, or even a single book), so I have chosen to 
discuss brieﬂy a wide range of areas. Most of the sections and even subsections 
of Chap. 9 are on topics to which entire books are devoted; however, I do not 
believe that any single book addresses all of them. 
Part III covers some of the important details of numerical computations, 
with an emphasis on those for linear algebra. I believe these topics constitute 
the most important material for an introductory course in numerical analysis 
for statisticians and should be covered in every such course. 
Except for speciﬁc computational techniques for optimization, random 
number generation, and perhaps symbolic computation, Part III provides the 
basic material for a course in statistical computing. All statisticians should 
have a passing familiarity with the principles. 
Chapter 10 provides some basic information on how data are stored and 
manipulated in a computer. Some of this material is rather tedious, but it 
is important to have a general understanding of computer arithmetic before 
considering computations for linear algebra. Some readers may skip or just 
skim Chap. 10, but the reader should be aware that the way the computer 
stores numbers and performs computations has far-reaching consequences. 
Computer arithmetic diﬀers from ordinary arithmetic in many ways; for ex-
ample, computer arithmetic lacks associativity of addition and multiplication, 
and series often converge even when they are not supposed to. (On the com-
puter, a straightforward evaluation of sigma summation Underscript x equals 1 Overscript normal infinity Endscripts xE∞
x=1 x converges!) 
I emphasize the diﬀerences between the abstract number system normal upper I normal upper RIR, called 
the reals, and the computer number system normal upper I normal upper FIF, the ﬂoating-point numbers 
unfortunately also often called “real”. Table 10.3 on page 557 summarizes some 
of these diﬀerences. All statisticians should be aware of the eﬀects of these 
diﬀerences. I also discuss the diﬀerences between normal upper Z normal upper ZZZ, the abstract number 
system called the integers, and the computer number system normal upper I normal upper III, the ﬁxed-

Preface to First Edition
XVII
point numbers. (Appendix A provides deﬁnitions for this and other notation 
that I use.) 
Chapter 10 also covers some of the fundamentals of algorithms, such as 
iterations, recursion, and convergence. It also discusses software development. 
Software issues are revisited in Chap. 12. 
While Chap. 10 deals with general issues in numerical analysis, Chap. 11 
addresses speciﬁc issues in numerical methods for computations in linear al-
gebra. 
Chapter 12 provides a brief introduction to software available for com-
putations with linear systems. Some speciﬁc systems mentioned include the 
IMSL™ libraries for Fortran and C, Octave or MATLAB® (or MATLAB®), 
and R or S-PLUS® (or S-Plus®). All of these systems are easy to use, and 
the best way to learn them is to begin using them for simple problems. I do 
not use any particular software system in the book, but in some exercises, and 
particularly in Part III, I do assume the ability to program in either Fortran or 
C and the availability of either R or S-Plus, Octave or MATLAB, and Maple® 
or Mathematica®. My own preferences for software systems are Fortran and 
R, and occasionally these preferences manifest themselves in the text. 
Appendix A collects the notation used in this book. It is generally “stan-
dard” notation, but one thing the reader must become accustomed to is the 
lack of notational distinction between a vector and a scalar. All vectors are 
“column” vectors, although I usually write them as horizontal lists of their 
elements. (Whether vectors are “row” vectors or “column” vectors is generally 
only relevant for how we write expressions involving vector/matrix multipli-
cation or partitions of matrices.) 
I write algorithms in various ways, sometimes in a form that looks similar 
to Fortran or C and sometimes as a list of numbered steps. I believe all of the 
descriptions used are straightforward and unambiguous. 
This book could serve as a basic reference either for courses in statistical 
computing or for courses in linear models or multivariate analysis. When the 
book is used as a reference, rather than looking for “Deﬁnition” or “Theo-
rem”, the user should look for items set oﬀ with bullets or look for numbered 
equations, or else should use the Indexor Appendix A, beginning on page 653. 
The prerequisites for this text are minimal. Obviously some background in 
mathematics is necessary. Some background in statistics or data analysis and 
some level of scientiﬁc computer literacy are also required. References to rather 
advanced mathematical topics are made in a number of places in the text. To 
some extent this is because many sections evolved from class notes that I 
developed for various courses that I have taught. All of these courses were at 
the graduate level in the computational and statistical sciences, but they have 
had wide ranges in mathematical level. I have carefully reread the sections 
that refer to groups, ﬁelds, measure theory, and so on, and am convinced that 
if the reader does not know much about these topics, the material is still 
understandable, but if the reader is familiar with these topics, the references 
add to that reader’s appreciation of the material. In many places, I refer to

XVIII
Preface to First Edition
computer programming, and some of the exercises require some programming. 
A careful coverage of Part III requires background in numerical programming. 
In regard to the use of the book as a text, most of the book evolved in one 
way or another for my own use in the classroom. I must quickly admit, how-
ever, that I have never used this whole book as a text for any single course. I 
have used Part III in the form of printed notes as the primary text for a course 
in the “foundations of computational science” taken by graduate students in 
the natural sciences (including a few statistics students, but dominated by 
physics students). I have provided several sections from Parts I and II in online 
PDF ﬁles as supplementary material for a two-semester course in mathemati-
cal statistics at the “baby measure theory” level (using Shao, 2003). Likewise, 
for my courses in computational statistics and statistical visualization, I have 
provided many sections, either as supplementary material or as the primary 
text, in online PDF ﬁles or printed notes. I have not taught a regular “applied 
statistics” course in almost 30 years, but if I did, I am sure that I would draw 
heavily from Parts I and II for courses in regression or multivariate analy-
sis. If I ever taught a course in “matrices for statistics” (I don’t even know if 
such courses exist), this book would be my primary text because I think it 
covers most of the things statisticians need to know about matrix theory and 
computations. 
Some exercises are Monte Carlo studies. I do not discuss Monte Carlo 
methods in this text, so the reader lacking background in that area may need 
to consult another reference in order to work those exercises. The exercises 
should be considered an integral part of the book. For some exercises, the re-
quired software can be obtained from netlib. Exercises in any of the chapters, 
not just in Part III, may require computations or computer programming. 
Penultimately, I must make some statement about the relationship of 
this book to some other books on similar topics. Much important statisti-
cal theory and many methods make use of matrix theory, and many statis-
ticians have contributed to the advancement of matrix theory from its very 
early days. Widely used books with derivatives of the words “statistics” and 
“matrices/linear-algebra” in their titles include Basilevsky (1983), Graybill 
(1983), Harville (1997), Schott (2004), and Searle (1982). All of these are use-
ful books. The computational orientation of this book is probably the main 
diﬀerence between it and these other books. Also, some of these other books 
only address topics of use in linear models, whereas this book also discusses 
matrices useful in graph theory, stochastic processes, and other areas of appli-
cation. (If the applications are only in linear models, most matrices of interest 
are symmetric, and all eigenvalues can be considered to be real.) Other dif-
ferences among all of these books, of course, involve the authors’ choices of 
secondary topics and the ordering of the presentation.

Preface to First Edition
XIX
Acknowledgments 
I thank John Kimmel of Springer for his encouragement and advice on this 
book and other books on which he has worked with me. I especially thank 
Ken Berk for his extensive and insightful comments on a draft of this book. 
I thank my student Li Li for reading through various drafts of some of the 
chapters and pointing out typos or making helpful suggestions. I thank the 
anonymous reviewers of this edition for their comments and suggestions. I also 
thank the many readers of my previous book on numerical linear algebra who 
informed me of errors and who otherwise provided comments or suggestions 
for improving the exposition. Whatever strengths this book may have can be 
attributed in large part to these people, named or otherwise. The weaknesses 
can only be attributed to my own ignorance or hardheadedness. 
I thank my wife, María, to whom this book is dedicated, for everything. 
I used TEX via  LATEX2ε to write the book. I did all of the typing, program-
ming, etc., myself, so all misteaks are mine. I would appreciate receiving sug-
gestions for improvement and notiﬁcation of errors. 
Fairfax County, VA, USA
James E. Gentle 
June 12, 2007

Contents 
Preface to Third Edition.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .VII 
Preface to Second Edition.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  XI 
Preface to First Edition.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .XIII 
1 
Introduction....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1 
1.1 Vectors....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  2 
1.2 Arrays....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3 
1.3 Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  4 
1.4 Representation of Data.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  6 
1.5 What You Compute and What You Don’t....  . . . . . . . . . . . . . . .  7 
1.6 R.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  8 
1.6.1 R Data Types.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  9 
1.6.2 Program Control Statements.... . . . . . . . . . . . . . . . . . . . . .  9 
1.6.3 Packages.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  10 
1.6.4 User Functions and Operators....  . . . . . . . . . . . . . . . . . . .  11 
1.6.5 Generating Artiﬁcial Data.... . . . . . . . . . . . . . . . . . . . . . . .  13 
1.6.6 Graphics Functions....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  13 
1.6.7 Special Data in R.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  14 
1.6.8 Determining Properties of a Computer System in R.... . 14 
1.6.9 Documentation: Finding R Functions and Packages... . . 15 
1.6.10 Documentation: R Functions, Packages, and Other 
Objects....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  15 
1.6.11 The Design of the R Programming Language... . . . . . . .  16 
1.6.12 Why I Use R in This Book....  . . . . . . . . . . . . . . . . . . . . . .  16

XXII
Contents
Part I Linear Algebra 
2 
Vectors and Vector Spaces.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19 
2.1 Operations on Vectors.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  19 
2.1.1 Linear Combinations and Linear Independence.... . . . . .  20 
2.1.2 Vector Spaces and Spaces of Vectors....  . . . . . . . . . . . . . .  22 
2.1.3 Basis Sets for Vector Spaces.... . . . . . . . . . . . . . . . . . . . . .  31 
2.1.4 Inner Products.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  32 
2.1.5 Norms.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  35 
2.1.6 Normalized Vectors.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  40 
2.1.7 Metrics and Distances.... . . . . . . . . . . . . . . . . . . . . . . . . . .  41 
2.1.8 Orthogonal Vectors and Orthogonal Vector Spaces.... . .  43 
2.1.9 The “One Vector”....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  44 
2.2 Cartesian Coordinates and Geometrical Properties of Vectors... 45 
2.2.1 Cartesian Geometry....  . . . . . . . . . . . . . . . . . . . . . . . . . . . .  45 
2.2.2 Projections.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  46 
2.2.3 Angles Between Vectors.... . . . . . . . . . . . . . . . . . . . . . . . . .  47 
2.2.4 Orthogonalization Transformations: Gram–Schmidt... . . 48 
2.2.5 Orthonormal Basis Sets.... . . . . . . . . . . . . . . . . . . . . . . . . .  50 
2.2.6 Approximation of Vectors....  . . . . . . . . . . . . . . . . . . . . . . .  51 
2.2.7 Flats, Aﬃne Spaces, and Hyperplanes.... . . . . . . . . . . . . .  53 
2.2.8 Cones....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  53 
2.2.9 Vector Cross Products in IR3.... . . . . . . . . . . . . . . . . . . . .  56 
2.3 Centered Vectors, and Variances and Covariances of Vectors... 58 
2.3.1 The Mean and Centered Vectors.... . . . . . . . . . . . . . . . . .  59 
2.3.2 The Standard Deviation, the Variance, and Scaled 
Vectors.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  59 
2.3.3 Covariances and Correlations Between Vectors... .  . . . . .  61 
Appendix: Vectors in R....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  62 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  66 
3 
Basic Properties of Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . .  73 
3.1 Basic Deﬁnitions and Notation.... . . . . . . . . . . . . . . . . . . . . . . . . .  73 
3.1.1 Multiplication of a Matrix by a Scalar.... . . . . . . . . . . . .  74 
3.1.2 Symmetric and Hermitian Matrices.... . . . . . . . . . . . . . . .  74 
3.1.3 Diagonal Elements.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  74 
3.1.4 Diagonally Dominant Matrices....  . . . . . . . . . . . . . . . . . . .  75 
3.1.5 Diagonal and Hollow Matrices....  . . . . . . . . . . . . . . . . . . .  75 
3.1.6 Matrices with Other Special Patterns of Zeroes.... . . . . .  75 
3.1.7 Matrix Shaping Operators....  . . . . . . . . . . . . . . . . . . . . . . .  77 
3.1.8 Partitioned Matrices and Submatrices....  . . . . . . . . . . . . .  80 
3.1.9 Matrix Addition.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  82 
3.1.10 The Trace of a Square Matrix.... . . . . . . . . . . . . . . . . . . . .  84

Contents
XXIII
3.2 The Determinant....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  85 
3.2.1 Deﬁnition and Simple Properties....  . . . . . . . . . . . . . . . . .  85 
3.2.2 Determinants of Various Types of Square Matrices.... . .  87 
3.2.3 Minors, Cofactors, and Adjugate Matrices.... . . . . . . . . .  89 
3.2.4 A Geometrical Perspective of the Determinant... . . . . . .  94 
3.3 Multiplication of Matrices and Multiplication of 
Vectors and Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  95 
3.3.1 Matrix Multiplication (Cayley).... . . . . . . . . . . . . . . . . . . .  96 
3.3.2 Cayley Multiplication of Matrices with Special 
Patterns....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  99 
3.3.3 Elementary Operations on Matrices.... . . . . . . . . . . . . . . . 102 
3.3.4 The Trace of a Cayley Product that Is Square....  . . . . . . 110 
3.3.5 The Determinant of a Cayley Product of Square 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110 
3.3.6 Outer Products of Vectors....  . . . . . . . . . . . . . . . . . . . . . . . 112 
3.3.7 Bilinear and Quadratic Forms: Deﬁniteness....  . . . . . . . . 112 
3.3.8 Anisometric Spaces.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114 
3.3.9 The Hadamard Product.... . . . . . . . . . . . . . . . . . . . . . . . . . 115 
3.3.10 The Kronecker Product....  . . . . . . . . . . . . . . . . . . . . . . . . . 116 
3.3.11 The Inner Product of Matrices....  . . . . . . . . . . . . . . . . . . . 118 
3.4 Matrix Rank and the Inverse of a Matrix.... . . . . . . . . . . . . . . . . 121 
3.4.1 Row Rank and Column Rank....  . . . . . . . . . . . . . . . . . . . . 121 
3.4.2 Full Rank Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122 
3.4.3 Rank of Elementary Operator Matrices and Matrix 
Products Involving Them.... . . . . . . . . . . . . . . . . . . . . . . . . 123 
3.4.4 The Rank of Partitioned Matrices, Products of 
Matrices, and Sums of Matrices.... . . . . . . . . . . . . . . . . . . 123 
3.4.5 Full Rank Partitioning.... . . . . . . . . . . . . . . . . . . . . . . . . . . 125 
3.4.6 Full Rank Matrices and Matrix Inverses.... . . . . . . . . . . . 126 
3.4.7 Matrix Inverses.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128 
3.4.8 Full Rank Factorization....  . . . . . . . . . . . . . . . . . . . . . . . . . 131 
3.4.9 Multiplication by Full Rank Matrices....  . . . . . . . . . . . . . 132 
3.4.10 Nonfull Rank and Equivalent Matrices.... . . . . . . . . . . . . 134 
3.4.11 Gramian Matrices: Products of the Form AT A....  . . . . . 137 
3.4.12 A Lower Bound on the Rank of a Matrix Product... . . . 138 
3.4.13 Determinants of Inverses.... . . . . . . . . . . . . . . . . . . . . . . . . 139 
3.4.14 Inverses of Products and Sums of Nonsingular 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 
3.4.15 Inverses of Matrices with Special Forms....  . . . . . . . . . . . 142 
3.4.16 Determining the Rank of a Matrix....  . . . . . . . . . . . . . . . . 143 
3.5 The Schur Complement in Partitioned Square Matrices....  . . . . 143 
3.5.1 Inverses of Partitioned Matrices.... . . . . . . . . . . . . . . . . . . 144 
3.5.2 Determinants of Partitioned Matrices.... . . . . . . . . . . . . . 144 
3.6 Linear Systems of Equations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 
3.6.1 Solutions of Linear Systems.... . . . . . . . . . . . . . . . . . . . . . . 145

XXIV
Contents
3.6.2 Null Space: The Orthogonal Complement.... . . . . . . . . . . 147 
3.6.3 Orthonormal Completion....  . . . . . . . . . . . . . . . . . . . . . . . . 148 
3.7 Generalized Inverses.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148 
3.7.1 Immediate Properties of Generalized Inverses... . . . . . . . 148 
3.7.2 The Moore–Penrose Inverse.... . . . . . . . . . . . . . . . . . . . . . . 151 
3.7.3 Generalized Inverses of Products and Sums of 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153 
3.7.4 Generalized Inverses of Partitioned Matrices.... . . . . . . . 153 
3.8 Orthogonality.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .154 
3.8.1 Orthogonal Matrices: Deﬁnition and Simple 
Properties.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 154 
3.8.2 Unitary Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155 
3.8.3 Orthogonal and Orthonormal Columns.... . . . . . . . . . . . . 155 
3.8.4 The Orthogonal Group.... . . . . . . . . . . . . . . . . . . . . . . . . . . 155 
3.8.5 Conjugacy.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 156 
3.9 Eigenanalysis: Canonical Factorizations.... . . . . . . . . . . . . . . . . . 156 
3.9.1 Eigenvalues and Eigenvectors Are Remarkable... . . . . . .  157 
3.9.2 Basic Properties of Eigenvalues and Eigenvectors... .  . . . 158 
3.9.3 The Characteristic Polynomial....  . . . . . . . . . . . . . . . . . . . 160 
3.9.4 The Spectrum....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163 
3.9.5 Similarity Transformations....  . . . . . . . . . . . . . . . . . . . . . . 168 
3.9.6 Schur Factorization.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 
3.9.7 Similar Canonical Factorization: Diagonalizable 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171 
3.9.8 Properties of Diagonalizable Matrices.... . . . . . . . . . . . . . 174 
3.9.9 Eigenanalysis of Symmetric Matrices.... . . . . . . . . . . . . . . 176 
3.9.10 Generalized Eigenvalues and Eigenvectors.... . . . . . . . . . 181 
3.9.11 Singular Values and the Singular Value 
Decomposition (SVD)....  . . . . . . . . . . . . . . . . . . . . . . . . . . . 182 
3.10 Positive Deﬁnite and Nonnegative Deﬁnite Matrices.... . . . . . . . 185 
3.10.1 Eigenvalues of Positive and Nonnegative Deﬁnite 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186 
3.10.2 Inverse of Positive Deﬁnite Matrices.... . . . . . . . . . . . . . . 186 
3.10.3 Diagonalization of Positive Deﬁnite Matrices.... . . . . . . . 186 
3.10.4 Square Roots of Positive and Nonnegative Deﬁnite 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 187 
3.11 Matrix Norms....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .187 
3.11.1 Matrix Norms Induced from Vector Norms... .  . . . . . . . . 188 
3.11.2 The Frobenius Norm—The “Usual” Norm... . . . . . . . . . . 191 
3.11.3 Other Matrix Norms.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193 
3.11.4 Matrix Norm Inequalities....  . . . . . . . . . . . . . . . . . . . . . . . . 193 
3.11.5 The Spectral Radius.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194 
3.11.6 Convergence of a Matrix Power Series.... . . . . . . . . . . . . . 195 
3.12 Approximation of Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198 
3.12.1 Measures of the Diﬀerence between Two Matrices...  . . .  199

Contents
XXV
3.12.2 Best Approximation with a Matrix of Given Rank... . . . 199 
Appendix: Matrices in R....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 
4 
Matrix Transformations and Factorizations.... . . . . . . . . . . . . . 217 
4.1 Linear Geometric Transformations.... . . . . . . . . . . . . . . . . . . . . . . 219 
4.1.1 Invariance Properties of Linear Transformations.... . . . . 219 
4.1.2 Transformations by Orthogonal Matrices.... . . . . . . . . . . 221 
4.1.3 Rotations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221 
4.1.4 Reﬂections.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 223 
4.1.5 Translations; Homogeneous Coordinates.... . . . . . . . . . . . 224 
4.2 Householder Transformations (Reﬂections).... . . . . . . . . . . . . . . . 225 
4.2.1 Zeroing All Elements but One in a Vector.... . . . . . . . . . 226 
4.2.2 Computational Considerations.... . . . . . . . . . . . . . . . . . . . 227 
4.3 Givens Transformations (Rotations)....  . . . . . . . . . . . . . . . . . . . . 228 
4.3.1 Zeroing One Element in a Vector.... . . . . . . . . . . . . . . . . . 229 
4.3.2 Givens Rotations That Preserve Symmetry.... . . . . . . . . 230 
4.3.3 Givens Rotations to Transform to Other Values... .  . . . . 230 
4.3.4 Fast Givens Rotations.... . . . . . . . . . . . . . . . . . . . . . . . . . . 231 
4.4 Factorization of Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 
4.5 LU and LDU Factorizations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 232 
4.5.1 Properties: Existence.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 233 
4.5.2 Pivoting.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .236 
4.5.3 Use of Inner Products....  . . . . . . . . . . . . . . . . . . . . . . . . . . . 236 
4.5.4 Properties: Uniqueness.... . . . . . . . . . . . . . . . . . . . . . . . . . . 237 
4.5.5 Properties of the LDU Factorization of a Square 
Matrix....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237 
4.6 QR Factorization.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238 
4.6.1 Related Matrix Factorizations.... . . . . . . . . . . . . . . . . . . . . 239 
4.6.2 Matrices of Full Column Rank....  . . . . . . . . . . . . . . . . . . . 239 
4.6.3 Nonfull Rank Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . 240 
4.6.4 Determining the Rank of a Matrix....  . . . . . . . . . . . . . . . . 241 
4.6.5 Formation of the QR Factorization....  . . . . . . . . . . . . . . . 242 
4.6.6 Householder Reﬂections to Form the QR Factorization...242 
4.6.7 Givens Rotations to Form the QR Factorization.... . . . . 243 
4.6.8 Gram-Schmidt Transformations to Form the 
QR Factorization.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243 
4.7 Factorizations of Nonnegative Deﬁnite Matrices....  . . . . . . . . . . 244 
4.7.1 Square Roots.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 
4.7.2 Cholesky Factorization....  . . . . . . . . . . . . . . . . . . . . . . . . . . 245 
4.7.3 Factorizations of a Gramian Matrix....  . . . . . . . . . . . . . . . 248 
4.8 Approximate Matrix Factorization.... . . . . . . . . . . . . . . . . . . . . . . 249 
4.8.1 Nonnegative Matrix Factorization....  . . . . . . . . . . . . . . . . 249

XXVI
Contents
4.8.2 Incomplete Factorizations....  . . . . . . . . . . . . . . . . . . . . . . . 250 
Appendix: R Functions for Matrix Computations and for Graphics...251 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254 
5 
Solution of Linear Systems....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259 
5.1 Condition of Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260 
5.1.1 Condition Number....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261 
5.1.2 Improving the Condition Number....  . . . . . . . . . . . . . . . . 266 
5.1.3 Numerical Accuracy.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267 
5.2 Direct Methods for Consistent Systems....  . . . . . . . . . . . . . . . . . . 268 
5.2.1 Gaussian Elimination and Matrix Factorizations....  . . . . 268 
5.2.2 Choice of Direct Method.... . . . . . . . . . . . . . . . . . . . . . . . . 272 
5.3 Iterative Methods for Consistent Systems.... . . . . . . . . . . . . . . . . 273 
5.3.1 The Gauss-Seidel Method with Successive 
Overrelaxation....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273 
5.3.2 Conjugate Gradient Methods for Symmetric 
Positive Deﬁnite Systems.... . . . . . . . . . . . . . . . . . . . . . . . . 275 
5.3.3 Multigrid Methods.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 
5.4 Iterative Reﬁnement....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 279 
5.5 Updating a Solution to a Consistent System.... . . . . . . . . . . . . . 280 
5.6 Overdetermined Systems: Least Squares....  . . . . . . . . . . . . . . . . . 282 
5.6.1 Least Squares Solution of an Overdetermined System... 284 
5.6.2 Least Squares with a Full Rank Coeﬃcient Matrix.... . . 286 
5.6.3 Least Squares with a Coeﬃcient Matrix Not of Full 
Rank.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 287 
5.6.4 Weighted Least Squares.... . . . . . . . . . . . . . . . . . . . . . . . . . 289 
5.6.5 Updating a Least Squares Solution of an 
Overdetermined System....  . . . . . . . . . . . . . . . . . . . . . . . . . 289 
5.7 Other Solutions of Overdetermined Systems.... . . . . . . . . . . . . . . 290 
5.7.1 Solutions That Minimize Other Norms of the 
Residuals.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291 
5.7.2 Regularized Solutions.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 294 
5.7.3 Other Restrictions on the Solutions.... . . . . . . . . . . . . . . . 295 
5.7.4 Minimizing Orthogonal Distances....  . . . . . . . . . . . . . . . . . 296 
Appendix: R Functions for Solving Linear Systems.... . . . . . . . . . . . . 299 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300 
6 
Evaluation of Eigenvalues and Eigenvectors.... . . . . . . . . . . . . . 303 
6.1 General Computational Methods.... . . . . . . . . . . . . . . . . . . . . . . . 304 
6.1.1 Numerical Condition of an Eigenvalue Problem....  . . . . . 304 
6.1.2 Eigenvalues from Eigenvectors and Vice Versa.... . . . . . . 306 
6.1.3 Deﬂation.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306 
6.1.4 Preconditioning.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 
6.1.5 Shifting.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 
6.2 Power Method....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309

Contents XXVII
6.3 Jacobi Method.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311 
6.4 QR Method.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .314 
6.5 Krylov Methods....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 
6.6 Generalized Eigenvalues.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 
6.7 Singular Value Decomposition....  . . . . . . . . . . . . . . . . . . . . . . . . . . 318 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 320 
7 
Real Analysis and Probability Distributions of Vectors 
and Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 323 
7.1 Basics of Diﬀerentiation.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 325 
7.1.1 Continuity.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .326 
7.1.2 Notation and Properties.... . . . . . . . . . . . . . . . . . . . . . . . . . 326 
7.1.3 Diﬀerentials.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 328 
7.1.4 Use of Diﬀerentiation in Optimization.... . . . . . . . . . . . . . 328 
7.2 Types of Diﬀerentiation.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 329 
7.2.1 Diﬀerentiation with Respect to a Scalar....  . . . . . . . . . . . 329 
7.2.2 Diﬀerentiation with Respect to a Vector.... . . . . . . . . . . . 330 
7.2.3 Diﬀerentiation with Respect to a Matrix.... . . . . . . . . . . 335 
7.3 Integration....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 337 
7.3.1 Multidimensional Integrals and Integrals Involving 
Vectors and Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 338 
7.3.2 Change of Variables; The Jacobian.... . . . . . . . . . . . . . . . 338 
7.3.3 Integration Combined with Other Operations....  . . . . . . 339 
7.4 Multivariate Probability Theory....  . . . . . . . . . . . . . . . . . . . . . . . . 340 
7.4.1 Random Variables and Probability Distributions... . . . . 340 
7.4.2 Distributions of Transformations of Random Variables...344 
7.4.3 The Multivariate Normal Distribution.... . . . . . . . . . . . . . 346 
7.4.4 Distributions Derived from the Multivariate Normal... . 350 
7.4.5 Chi-Squared Distributions.... . . . . . . . . . . . . . . . . . . . . . . . 350 
7.4.6 Wishart Distributions....  . . . . . . . . . . . . . . . . . . . . . . . . . . . 353 
7.5 Multivariate Random Number Generation.... . . . . . . . . . . . . . . . 355 
7.5.1 The Multivariate Normal Distribution.... . . . . . . . . . . . . . 355 
7.5.2 Random Correlation Matrices....  . . . . . . . . . . . . . . . . . . . . 356 
Appendix: R for Working with Probability Distributions and for 
Simulating Random Data....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 357 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360

XXVIII Contents
Part II Applications in Statistics and Data Science 
8 
Special Matrices and Operations Useful in Modeling and 
Data Science.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367 
8.1 Data Matrices and Association Matrices....  . . . . . . . . . . . . . . . . . 368 
8.1.1 Flat Files....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 368 
8.1.2 Graphs and Other Data Structures....  . . . . . . . . . . . . . . . 369 
8.1.3 Term-by-Document Matrices.... . . . . . . . . . . . . . . . . . . . . . 376 
8.1.4 Sparse Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 377 
8.1.5 Probability Distribution Models....  . . . . . . . . . . . . . . . . . . 377 
8.1.6 Derived Association Matrices....  . . . . . . . . . . . . . . . . . . . . 377 
8.2 Symmetric Matrices and Other Unitarily Diagonalizable 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378 
8.2.1 Some Important Properties of Symmetric Matrices... . . 378 
8.2.2 Approximation of Symmetric Matrices and an 
Important Inequality....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 379 
8.2.3 Normal Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383 
8.3 Nonnegative Deﬁnite Matrices: Cholesky Factorization... . . . . . 384 
8.3.1 Eigenvalues of Nonnegative Deﬁnite Matrices... . . . . . . . 385 
8.3.2 The Square Root and the Cholesky Factorization.... . . . 386 
8.3.3 The Convex Cone of Nonnegative Deﬁnite Matrices... . . 386 
8.4 Positive Deﬁnite Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387 
8.4.1 Leading Principal Submatrices of Positive Deﬁnite 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 389 
8.4.2 The Convex Cone of Positive Deﬁnite Matrices.... . . . . . 389 
8.4.3 Inequalities Involving Positive Deﬁnite Matrices.... . . . . 389 
8.5 Idempotent and Projection Matrices.... . . . . . . . . . . . . . . . . . . . . 391 
8.5.1 Idempotent Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 391 
8.5.2 Projection Matrices: Symmetric Idempotent Matrices... 396 
8.6 Special Matrices Occurring in Data Analysis.... . . . . . . . . . . . . . 397 
8.6.1 Gramian Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 398 
8.6.2 Projection and Smoothing Matrices....  . . . . . . . . . . . . . . . 399 
8.6.3 Centered Matrices and Variance-Covariance Matrices... 403 
8.6.4 The Generalized Variance.... . . . . . . . . . . . . . . . . . . . . . . . 406 
8.6.5 Similarity Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 
8.6.6 Dissimilarity Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . 408 
8.7 Nonnegative and Positive Matrices....  . . . . . . . . . . . . . . . . . . . . . . 409 
8.7.1 Properties of Square Positive Matrices....  . . . . . . . . . . . . 411 
8.7.2 Irreducible Square Nonnegative Matrices....  . . . . . . . . . . 412 
8.7.3 Stochastic Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 
8.7.4 Leslie Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417 
8.8 Other Matrices with Special Structures.... . . . . . . . . . . . . . . . . . . 418 
8.8.1 Helmert Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 418

Contents
XXIX
8.8.2 Vandermonde Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . 419 
8.8.3 Hadamard Matrices and Orthogonal Arrays....  . . . . . . . . 420 
8.8.4 Toeplitz Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421 
8.8.5 Circulant Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .423 
8.8.6 Fourier Matrices and the Discrete Fourier Transform... . 424 
8.8.7 Hankel Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 427 
8.8.8 Cauchy Matrices.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 428 
8.8.9 Matrices Useful in Graph Theory.... . . . . . . . . . . . . . . . . . 429 
8.8.10 Z-Matrices and M-Matrices.... . . . . . . . . . . . . . . . . . . . . . . 432 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 433 
9 
Selected Applications in Statistics .... . . . . . . . . . . . . . . . . . . . . . 437 
9.1 Linear Models....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .438 
9.1.1 Fitting the Model.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 441 
9.1.2 Least Squares Fit of Full-Rank Models.... . . . . . . . . . . . . 443 
9.1.3 Least Squares Fits of Nonfull-Rank Models....  . . . . . . . . 444 
9.1.4 Computing the Solution....  . . . . . . . . . . . . . . . . . . . . . . . . . 447 
9.1.5 Properties of a Least Squares Fit.... . . . . . . . . . . . . . . . . . 449 
9.1.6 Linear Least Squares Subject to Linear Equality 
Constraints.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451 
9.1.7 Weighted Least Squares.... . . . . . . . . . . . . . . . . . . . . . . . . . 452 
9.1.8 Updating Linear Regression Statistics.... . . . . . . . . . . . . . 453 
9.1.9 Linear Smoothing.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 455 
9.1.10 Multivariate Linear Models.... . . . . . . . . . . . . . . . . . . . . . . 456 
9.2 Statistical Inference in Linear Models....  . . . . . . . . . . . . . . . . . . . 458 
9.2.1 The Probability Distribution of ε.... . . . . . . . . . . . . . . . . . 459 
9.2.2 Estimability....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 462 
9.2.3 The Gauss-Markov Theorem....  . . . . . . . . . . . . . . . . . . . . . 465 
9.2.4 Testing Linear Hypotheses....  . . . . . . . . . . . . . . . . . . . . . . . 466 
9.2.5 Statistical Inference in Linear Models with 
Heteroscedastic or Correlated Errors.... . . . . . . . . . . . . . . 467 
9.2.6 Statistical Inference for Multivariate Linear Models... . . 468 
9.3 Principal Components.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470 
9.3.1 Principal Components of a Random Vector....  . . . . . . . . 470 
9.3.2 Principal Components of Data....  . . . . . . . . . . . . . . . . . . . 471 
9.4 Condition of Models and Data....  . . . . . . . . . . . . . . . . . . . . . . . . . 474 
9.4.1 Ill-Conditioning in Statistical Applications....  . . . . . . . . . 474 
9.4.2 Variable Selection.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 475 
9.4.3 Principal Components Regression.... . . . . . . . . . . . . . . . . 476 
9.4.4 Shrinkage Estimation....  . . . . . . . . . . . . . . . . . . . . . . . . . . . 477 
9.4.5 Statistical Inference About the Rank of a Matrix... . . . . 479 
9.4.6 Incomplete Data.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 483 
9.5 Stochastic Processes....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 486 
9.5.1 Markov Chains.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .487 
9.5.2 Markovian Population Models.... . . . . . . . . . . . . . . . . . . . . 489 
9.5.3 Autoregressive Processes.... . . . . . . . . . . . . . . . . . . . . . . . . 491

XXX
Contents
9.6 Optimization of Scalar-Valued Functions.... . . . . . . . . . . . . . . . . 494 
9.6.1 Stationary Points of Functions....  . . . . . . . . . . . . . . . . . . . 495 
9.6.2 Newton’s Method....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 495 
9.6.3 Least Squares.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497 
9.6.4 Maximum Likelihood.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 502 
9.6.5 Optimization of Functions with Constraints.... . . . . . . . . 503 
9.6.6 Optimization Without Diﬀerentiation.... . . . . . . . . . . . . . 509 
Appendix: R for Applications in Statistics.... . . . . . . . . . . . . . . . . . . . . 510 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 517 
Part III Numerical Methods and Software 
10 Numerical Methods .... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 527 
10.1 Software Development .... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528 
10.1.1 Standards.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 528 
10.1.2 Coding Systems.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 529 
10.1.3 Types of Data.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 
10.1.4 Missing Data.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530 
10.1.5 Data Structures.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 531 
10.1.6 Computer Architectures and File Systems.... . . . . . . . . . 531 
10.2 Digital Representation of Numeric Data....  . . . . . . . . . . . . . . . . . 532 
10.2.1 The Fixed-Point Number System.... . . . . . . . . . . . . . . . . . 533 
10.2.2 The Floating-Point Model for Real Numbers.... . . . . . . . 535 
10.2.3 Language Constructs for Representing Numeric Data... 542 
10.2.4 Other Variations in the Representation of Data: 
Portability of Data....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 547 
10.3 Computer Operations on Numeric Data.... . . . . . . . . . . . . . . . . . 549 
10.3.1 Fixed-Point Operations....  . . . . . . . . . . . . . . . . . . . . . . . . . 550 
10.3.2 Floating-Point Operations....  . . . . . . . . . . . . . . . . . . . . . . . 551 
10.3.3 Language Constructs for Operations on Numeric Data...556 
10.3.4 Software Methods for Extending the Precision... . . . . . .  558 
10.3.5 Exact Computations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 561 
10.4 Numerical Algorithms and Analysis.... . . . . . . . . . . . . . . . . . . . . . 561 
10.4.1 Error in Numerical Computations....  . . . . . . . . . . . . . . . . 562 
10.4.2 Eﬃciency.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 570 
10.4.3 Iterations and Convergence....  . . . . . . . . . . . . . . . . . . . . . . 576 
10.4.4 Computations Without Storing Data....  . . . . . . . . . . . . . . 578 
10.4.5 Other Computational Techniques.... . . . . . . . . . . . . . . . . . 579 
Appendix: Numerical Computations in R.... . . . . . . . . . . . . . . . . . . . . 582 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583

Contents
XXXI
11 Numerical Linear Algebra .... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591 
11.1 Computer Storage of Vectors and Matrices....  . . . . . . . . . . . . . . . 591 
11.1.1 Storage Modes.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 
11.1.2 Strides....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 
11.1.3 Sparsity.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 592 
11.2 General Computational Considerations for Vectors and 
Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 593 
11.2.1 Relative Magnitudes of Operands.... . . . . . . . . . . . . . . . . . 593 
11.2.2 Iterative Methods.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 595 
11.2.3 Assessing Computational Errors.... . . . . . . . . . . . . . . . . . . 596 
11.3 Multiplication of Vectors and Matrices.... . . . . . . . . . . . . . . . . . . 597 
11.3.1 Strassen’s Algorithm.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599 
11.3.2 Matrix Multiplication Using MapReduce....  . . . . . . . . . . 601 
11.4 Other Matrix Computations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 601 
11.4.1 Rank Determination.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 602 
11.4.2 Computing the Determinant.... . . . . . . . . . . . . . . . . . . . . . 603 
11.4.3 Computing the Condition Number.... . . . . . . . . . . . . . . . . 603 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604 
12 Software for Numerical Linear Algebra....  . . . . . . . . . . . . . . . . . 607 
12.1 General Considerations.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 608 
12.1.1 Software Development and Open-Source Software...  . . .  608 
12.1.2 Integrated Development, Collaborative Research, 
and Version Control....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 
12.1.3 Finding Software....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609 
12.1.4 Software Design.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 610 
12.1.5 Software Development, Maintenance, and Testing.... . . . 619 
12.1.6 Reproducible Research.... . . . . . . . . . . . . . . . . . . . . . . . . . . 622 
12.2 Software Libraries.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 624 
12.2.1 BLAS.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .624 
12.2.2 Level 2 and Level 3 BLAS, LAPACK, and Related 
Libraries.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 627 
12.2.3 Libraries for High-Performance Computing.... . . . . . . . . 628 
12.2.4 The IMSL Libraries....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 632 
12.3 General-Purpose Languages and Programming Systems... .  . . .  634 
12.3.1 Programming Considerations.... . . . . . . . . . . . . . . . . . . . . 636 
12.3.2 Modern Fortran.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 638 
12.3.3 C and C++.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 640 
12.3.4 Python.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 641 
12.3.5 MATLAB and Octave.... . . . . . . . . . . . . . . . . . . . . . . . . . . . 642 
Appendix: R Software for Numerical Linear Algebra.... . . . . . . . . . . . 644 
Exercises....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 647

XXXII Contents
Appendices 
A Notation and Deﬁnitions.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 
A.1 General Notation....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653 
A.2 Computer Number Systems.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . 655 
A.3 General Mathematical Functions and Operators.... . . . . . . . . . . 656 
A.4 Linear Spaces and Matrices....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . 659 
A.4.1 Norms and Inner Products.... . . . . . . . . . . . . . . . . . . . . . . 661 
A.4.2 Matrix Shaping Notation....  . . . . . . . . . . . . . . . . . . . . . . . . 662 
A.4.3 Notation for Rows or Columns of Matrices....  . . . . . . . . . 664 
A.4.4 Notation Relating to Matrix Determinants....  . . . . . . . . . 664 
A.4.5 Matrix-Vector Diﬀerentiation....  . . . . . . . . . . . . . . . . . . . . 664 
A.4.6 Special Vectors and Matrices.... . . . . . . . . . . . . . . . . . . . . . 665 
A.4.7 Elementary Operator Matrices....  . . . . . . . . . . . . . . . . . . . 666 
A.5 Models and Data....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666 
Bibliography.... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .667 
Index....  . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .677

1 
Introduction 
Vectors and matrices are useful in representing multivariate numeric data, 
and they occur naturally in working with linear equations or when expressing 
linear relationships among objects. Numerical algorithms for a variety of tasks 
involve matrix and vector arithmetic. An optimization algorithm to ﬁnd the 
minimum of a function, for example, may use a vector of ﬁrst derivatives and 
a matrix of second derivatives; and a method to solve a diﬀerential equation 
may use a matrix with diagonals of numerical diﬀerences. 
There are various precise ways of deﬁning vectors and matrices, but we 
will generally think of them merely as linear or rectangular arrays of num-
bers, or scalars, on which an algebra is deﬁned. Unless otherwise stated, we 
will assume the scalars are real numbers. We denote both the set of real num-
bers and the ﬁeld of real numbers as normal upper I normal upper RIR. (The  ﬁeld is the set together with 
the two operators.) Occasionally, we will take a geometrical perspective for 
vectors and will consider matrices to deﬁne geometrical transformations. In 
all contexts, however, the elements of vectors or matrices are real numbers 
(or, more generally, members of a ﬁeld). When the elements are not members 
of a ﬁeld (names or characters, e.g.), we will use more general phrases, such 
as “ordered lists” or “arrays.” 
Scalars, vectors, and matrices are fundamentally diﬀerent kinds of objects, 
and in these ﬁrst few chapters, we maintain their separateness. In applications 
in later chapters, however, we often blur the distinctions; a matrix with one 
column may morph into a vector, and a 1 times 11 × 1 may be treated as a scalar. 
Many of the operations covered in the ﬁrst few chapters, especially the 
transformations and factorizations in Chap. 4, are important because of their 
use in solving systems of linear equations, which will be discussed in Chap. 5; 
in computing eigenvectors, eigenvalues, and singular values, which will be 
discussed in Chap. 6; and in the applications in Chap. 9. 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 1 
1

2
1 Introduction
Throughout the ﬁrst few chapters, we emphasize the facts that are impor-
tant in statistical applications. We also discuss some aspects of the R software 
system and illustrate its use in manipulating vectors and matrices. 
It is very important to understand that the form of a mathematical expres-
sion and the way the expression should be evaluated in actual practice may 
be quite diﬀerent. We remind the reader of this fact from time to time. That 
there is a diﬀerence in mathematical expressions and computational methods 
is one of the main messages of Chaps. 10 and 11. (An example of this, in nota-
tion that we will introduce later, is the expression upper A Superscript negative 1 Baseline bA−1b. If our goal is to solve 
a linear system upper A x equals bAx = b, we probably should never compute the matrix inverse 
upper A Superscript negative 1A−1 and then multiply it times b. Nevertheless, it may be entirely appropriate 
to write the expression upper A Superscript negative 1 Baseline bA−1b.) 
1.1 Vectors 
For a positive integer n, a vector (or n-vector) is an n-tuple, ordered (multi)set, 
or array of n numbers, called elements or scalars. The number of elements is 
called the order, or sometimes the “length,” of the vector. An n-vector can be 
thought of as representing a point in n-dimensional space. In this setting, the 
“length” of the vector may also mean the Euclidean distance from the origin to 
the point represented by the vector, that is, the square root of the sum of the 
squares of the elements of the vector. This Euclidean distance will generally 
be what we mean when we refer to the length of a vector (see page 37). In 
general, “length” is measured by a norm; see Sect. 2.1.5, beginning on page 35. 
We usually use a lowercase letter to represent a vector, and we use the 
same letter with a single subscript to represent an element of the vector. 
The ﬁrst element of an n-vector is the ﬁrst (1 normal s normal t1st) element and the last is the 
n normal t normal hnth element. (This statement is not a tautology; in some computer systems, 
the ﬁrst element of an object used to represent a vector is the 0 normal t normal h0th element 
of the object. This sometimes makes it diﬃcult to preserve the relationship 
between the computer entity and the object that is of interest.) Although we 
are very concerned about computational issues, we will use paradigms and 
notation that maintain the priority of the object of interest rather than the 
computer entity representing it. 
We may write the n-vector x as 
x equals Start 3 By 1 Matrix 1st Row x 1 2nd Row vertical ellipsis 3rd Row x Subscript n EndMatrixx =
⎛
⎜
⎝
x1
...
xn
⎞
⎟
⎠
(1.1) 
or more commonly as 
x equals left parenthesis x 1 comma ellipsis comma x Subscript n Baseline right parenthesis periodx = (x1, . . . , xn).
(1.2)

1.2 Arrays
3
We make no distinction between these two notations, although in some con-
texts we think of a vector as a “column,” so the ﬁrst notation may be more 
natural. The simplicity of the second notation recommends it for common use. 
(And this notation does not require the additional symbol for transposition 
that some people use when they write the elements of a vector horizontally.) 
Two vectors are equal if and only if they are of the same order and each 
element of one vector is equal to the corresponding element of the other. 
Our view of vectors essentially associates the elements of a vector with 
the coordinates of a Cartesian geometry. There are other, more abstract, ways 
of developing a theory of vectors that are called “coordinate-free,” but we 
will not pursue those approaches here. For most applications in statistics, the 
approach based on coordinates is more useful. 
Thinking of the coordinates simply as real numbers, we use the notation 
normal upper I normal upper R Superscript nIRn
(1.3) 
to denote the either set of n-vectors with real elements or those vectors to-
gether with operations on them. 
This notation reinforces the notion that the coordinates of a vector corre-
spond to the direct product of single coordinates. The direct product of two 
sets is denoted as “times×” and deﬁned for sets A and B as the set 
StartSet left parenthesis a comma b right parenthesis comma normal s period normal t period a element of upper A comma b element of upper B EndSet period{(a, b), s.t. a ∈A, b ∈B}.
The operation is interpreted to be associative; that is, the elements of left parenthesis upper A times upper B right parenthesis times upper C(A ×
B) × C are of the form left parenthesis a comma b comma c right parenthesis(a, b, c), rather than the more complicated objects of 
the form left parenthesis left parenthesis a comma b right parenthesis comma c right parenthesis((a, b), c). Hence, 
normal upper I normal upper R Superscript n Baseline equals normal upper I normal upper R times normal upper I normal upper R Superscript n minus 1 Baseline equals normal upper I normal upper R Superscript n minus 1 Baseline times normal upper I normal upper R equals normal upper I normal upper R times midline horizontal ellipsis times normal upper I normal upper R left parenthesis n normal t normal i normal m normal e normal s right parenthesis periodIRn = IR × IRn−1 = IRn−1 × IR = IR × · · · × IR
(n times).
1.2 Arrays 
Arrays are structured collections of elements corresponding in shape to lines, 
rectangles, or rectangular solids. The number of dimensions of an array is often 
called the rank of the array. Thus, a vector is an array of rank 1, and a matrix 
is an array of rank 2. A scalar, which can be thought of as a degenerate array, 
has rank 0. When referring to computer software objects, “rank” is generally 
used in this sense. (This term comes from its use in describing a tensor. A  
rank 0 tensor is a scalar, a rank 1 tensor is a vector, a rank 2 tensor is a 
square matrix, and so on. In our usage referring to arrays, we do not require 
that the dimensions be equal, however.) When we refer to “rank of an array,” 
we mean the number of dimensions. When we refer to “rank of a matrix,” 
we mean something diﬀerent, as we discuss in Sect. 3.4. In linear algebra, this 
latter usage is far more common than the former.

4
1 Introduction
1.3 Matrices 
A matrix is a rectangular or two-dimensional array. We speak of the rows and 
columns of a matrix. The rows or columns can be considered to be vectors, 
and we often use this equivalence. An n times mn × m matrix is one with n rows and 
m columns. The number of rows and the number of columns determine the 
shape of the matrix. Note that the shape is the doubleton left parenthesis n comma m right parenthesis(n, m), not  just  
a single number such as the ratio. If the number of rows is the same as the 
number of columns, the matrix is said to be square. 
All matrices are two-dimensional in the sense of “dimension” used above. 
The word “dimension,” however, when applied to matrices, often means some-
thing diﬀerent, namely, the number of columns. (This usage of “dimension” 
is common both in geometry and in traditional statistical applications.) The 
number of columns is also sometimes called the “order” of the matrix. The 
shape (a doubleton) is sometimes called the “size.” For a square matrix, the 
“size” is often a single number. These terms are intuitive, but they may be 
somewhat ambiguous. I will use them occasionally, but, it is hoped, in an 
unambiguous sense. 
We usually use an uppercase letter to represent a matrix. To represent an 
element of the matrix, we usually use the corresponding lowercase letter with 
a subscript to denote the row and a second subscript to represent the column. 
If a nontrivial expression is used to denote the row or the column, we separate 
the row and column subscripts with a comma. 
Although vectors and matrices are fundamentally quite diﬀerent types of 
objects, we can bring some unity to our discussion and notation by occasion-
ally considering a vector to be a “column vector” and in some ways to be the 
same as an n times 1n × 1 matrix. (This has nothing to do with the way we may write 
the elements of a vector. The notation in Eq. (1.2) is more convenient than  
that in Eq. (1.1) and so will generally be used in this book, but its use does not 
change the nature of the vector in any way. Likewise, this has nothing to do 
with the way the elements of a vector or a matrix are stored in the computer.) 
When we use vectors and matrices in the same expression, however, we use 
the symbol “normal upper TT” (for “transpose”) as a superscript to represent a vector that 
is being treated as a 1 times n1 × n matrix. 
The ﬁrst row is the 1 normal s normal t1st (ﬁrst) row, and the ﬁrst column is the 1 normal s normal t1st (ﬁrst) 
column. (Again, we remark that computer entities used in some systems to 
represent matrices and to store elements of matrices as computer data some-
times index the elements beginning with 0. Furthermore, some systems use 
the ﬁrst index to represent the column and the second index to indicate the 
row. We are not speaking here of the storage order – “row major” versus “col-
umn major” – we address that later, in Chap. 11. Rather, we are speaking 
of the mechanism of referring to the abstract entities. In image processing, 
for example, it is common practice to use the ﬁrst index to represent the col-
umn and the second index to represent the row. In the software packages IDL 
and PV-Wave, for example, there are two diﬀerent kinds of two-dimensional

1.3 Matrices
5
objects: “arrays,” in which the indexing is done as in image processing, and 
“matrices,” in which the indexing is done as we have described.) 
The n times mn × m matrix A can be written as 
upper A equals Start 3 By 3 Matrix 1st Row 1st Column a 11 2nd Column ellipsis 3rd Column a Subscript 1 m Baseline 2nd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 3rd Row 1st Column a Subscript n Baseline 1 Baseline 2nd Column ellipsis 3rd Column a Subscript n m Baseline EndMatrix periodA =
⎡
⎢⎣
a11 . . . a1m
...
...
...
an1 . . . anm
⎤
⎥⎦.
(1.4) 
We also write the matrix A above as 
upper A equals left parenthesis a Subscript i j Baseline right parenthesis commaA = (aij),
(1.5) 
with the indices i and j ranging over StartSet 1 comma ellipsis comma n EndSet{1, . . . , n} and StartSet 1 comma ellipsis comma m EndSet{1, . . . , m}, respectively. 
We use the notation upper A Subscript n times mAn×m to refer to the matrix A and simultaneously to 
indicate that it is n times mn × m, and we use the notation 
normal upper I normal upper R Superscript n times mIRn×m
(1.6) 
to  refer to the  set of all  n times mn × m matrices with real elements. 
We use the notation left parenthesis upper A right parenthesis Subscript i j(A)ij to refer to the element in the i normal t normal hith row and the 
j normal t normal hjth column of the matrix A; that is,  in  Eq. (1.4), left parenthesis upper A right parenthesis Subscript i j Baseline equals a Subscript i j(A)ij = aij. 
Two matrices are equal if and only if they are of the same shape and each 
element of one matrix is equal to the corresponding element of the other. 
Although vectors are column vectors and the notation in Equations (1.1) 
and (1.2) represents the same entity, that would not be the same for matrices. 
If x 1 comma ellipsis comma x Subscript n Baselinex1, . . . , xn are scalars 
upper X equals Start 3 By 1 Matrix 1st Row x 1 2nd Row vertical ellipsis 3rd Row x Subscript n EndMatrixX =
⎡
⎢⎣
x1
...
xn
⎤
⎥⎦
(1.7) 
and 
upper Y equals left bracket x 1 comma ellipsis comma x Subscript n Baseline right bracket commaY = [x1, . . . , xn],
(1.8) 
then X is an n times 1n×1 matrix and Y is a 1 times n1×n matrix and upper X not equals upper YX ̸= Y unless n equals 1n = 1. (Y 
is the transpose of X.) Although an n times 1n × 1 matrix is a diﬀerent type of object 
from a vector, we may treat X in Eq. (1.7) or  upper Y Superscript normal upper TY T in Eq. (1.8) as a vector when 
it is convenient to do so. Furthermore, although a 1 times 11 × 1 matrix, a 1-vector, 
and a scalar are all fundamentally diﬀerent types of objects, we will treat a 
one by one matrix or a vector with only one element as a scalar whenever it 
is convenient. 
We sometimes use the notation a Subscript asterisk ja∗j to correspond to the j normal t normal hjth column of 
the matrix A and use a Subscript i asteriskai∗to represent the (column) vector that corresponds 
to the i normal t normal hith row. Using that notation, the n times mn × m matrix A in Eq. (1.4) can  be  
written as 
upper A equals Start 3 By 1 Matrix 1st Row a Subscript 1 asterisk Superscript normal upper T Baseline 2nd Row vertical ellipsis 3rd Row a Subscript n asterisk Superscript normal upper T EndMatrixA =
⎡
⎢⎣
aT
1∗
...
aT
n∗
⎤
⎥⎦
(1.9)

6
1 Introduction
or as 
upper A equals left bracket a Subscript asterisk 1 Baseline comma ellipsis comma a Subscript asterisk m Baseline right bracket periodA = [a∗1, . . . , a∗m] .
(1.10) 
One of the most important uses of matrices is as a transformation of a vec-
tor by vector/matrix multiplication. Such transformations are linear (a term 
that we deﬁne later). Although one can occasionally proﬁtably distinguish 
matrices from linear transformations on vectors, for our present purposes, 
there is no advantage in doing so. We will often treat matrices and linear 
transformations as equivalent. 
Many of the properties of vectors and matrices we discuss hold for an 
inﬁnite number of elements, but we will assume throughout this book that 
the number is ﬁnite. 
Subvectors and Submatrices 
We sometimes ﬁnd it useful to work with only some of the elements of a vector 
or matrix. We refer to the respective arrays as “subvectors” or “submatrices.” 
We also allow the rearrangement of the elements by row or column permuta-
tions and still consider the resulting object as a subvector or submatrix. In 
Chap. 3, we will consider special forms of submatrices formed by “partitions” 
of given matrices. 
The two expressions (1.9) and  (1.10) represent special partitions of the 
matrix A. 
1.4 Representation of Data 
Before we can do any serious analysis of data, the data must be represented 
in some structure that is amenable to the operations of the analysis. In simple 
cases, the data are represented by a list of scalar values. The ordering in the 
list may be unimportant, and the analysis may just consist of computation of 
simple summary statistics. In other cases, the list represents a time series of 
observations, and the relationships of observations to each other as a function 
of their order and distance apart in the list are of interest. Often, the data 
can be represented meaningfully in two lists that are related to each other by 
the positions in the lists. The generalization of this representation is a two-
dimensional array in which each column corresponds to a particular type of 
data. 
A major consideration, of course, is the nature of the individual items of 
data. The observational data may be in various forms: quantitative measures, 
colors, text strings, and so on. Prior to most analyses of data, they must be 
represented as real numbers. In some cases, they can be represented easily 
as real numbers, although there may be restrictions on the mapping into the 
reals. (For example, do the data naturally assume only integral values, or 
could any real number be mapped back to a possible observation?)

1.5 What You Compute and What You Don’t
7
The most common way of representing data is by using a two-dimensional 
array in which the rows correspond to observational units (“instances”) and 
the columns correspond to particular types of observations (“variables” or 
“features”). If the data correspond to real numbers, this representation is the 
familiar X data matrix. Much of this book is devoted to the matrix theory 
and computational methods for the analysis of data in this form. This type of 
matrix, perhaps with an adjoined vector, is the basic structure used in many 
familiar statistical methods, such as regression analysis, principal component 
analysis, analysis of variance, multidimensional scaling, and so on. 
There are other types of structures based on graphs that are useful in rep-
resenting data. A graph is a structure consisting of two components: a set of 
points, called vertices or nodes, and a set of pairs of the points, called edges. 
(Note that this usage of the word “graph” is distinctly diﬀerent from the 
more common one that refers to lines, curves, bars, and so on to represent 
data pictorially. The phrase “graph theory” is often used, or overused, to em-
phasize the present meaning of the word.) A graph script upper G equals left parenthesis upper V comma upper E right parenthesisG = (V, E) with vertices 
upper V equals StartSet v 1 comma ellipsis comma v Subscript n Baseline EndSetV = {v1, . . . , vn} is distinguished primarily by the nature of the edge elements 
left parenthesis v Subscript i Baseline comma v Subscript j Baseline right parenthesis(vi, vj) in E. Graphs are identiﬁed as complete graphs, directed graphs, trees, 
and so on, depending on E and its relationship with V . A tree may be used 
for data that are naturally aggregated in a hierarchy, such as political unit, 
subunit, household, and individual. Trees are also useful for representing clus-
tering of data at diﬀerent levels of association. In this type of representation, 
the individual data elements are the terminal nodes, or “leaves,” of the tree. 
In another type of graphical representation that is often useful in “data 
mining” or “learning,” where we seek to uncover relationships among objects, 
the vertices are the objects, either observational units or features, and the 
edges indicate some commonality between vertices. For example, the vertices 
may be text documents, and an edge between two documents may indicate 
that a certain number of speciﬁc words or phrases occur in both documents. 
Despite the diﬀerences in the basic ways of representing data, in graphical 
modeling of data, many of the standard matrix operations used in more tra-
ditional data analysis are applied to matrices that arise naturally from the 
graph. 
However the data are represented, whether in an array or a network, the 
analysis of the data is often facilitated by using “association” matrices. The 
most familiar type of association matrix is perhaps a correlation matrix. We 
will encounter and use other types of association matrices in Chap. 8. 
1.5 What You Compute and What You Don’t 
The applied mathematician or statistician routinely performs many computa-
tions involving vectors and matrices. Many of those computations follow the 
methods discussed in this text.

8
1 Introduction
For a given matrix X, I will often refer to its inverse upper X Superscript negative 1X−1, its determinant 
normal d normal e normal t left parenthesis upper X right parenthesisdet(X), its  Gram  upper X Superscript normal upper T Baseline upper XXTX, a matrix formed by permuting its columns upper E Subscript left parenthesis pi right parenthesis Baseline upper XE(π)X, a  
matrix formed by permuting its rows upper X upper E Subscript left parenthesis pi right parenthesisXE(π), and other transformations of the 
given matrix X. These derived objects are very important and useful. Their 
usefulness, however, is primarily conceptual. 
When working with a real matrix X whose elements have actual known 
values, it is not very often that we need or want the actual values of elements 
of these derived objects. Because of this, some authors try to avoid discussing 
or referring directly to these objects. 
When I write something like left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline y(XTX)−1XTy, I do not mean that you should 
compute upper X Superscript normal upper T Baseline upper XXTX and upper X Superscript normal upper T Baseline yXTy, then compute left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1(XTX)−1, and then ﬁnally multiply 
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1(XTX)−1 and upper X Superscript normal upper T Baseline yXTy. I assume you know better than to do that. If you don’t 
know it yet, I hope after reading this book, you will know why not to. 
1.6 R 
The software I will mention and use occasionally in this book is R. R is an 
open-source, freely distributed system begun by Robert Gentleman and Ross 
Ihaka in the mid-1990s and based on the design of S by John Chambers. It has 
become one of the most widely used statistical software packages. Its devel-
opment and maintenance has been subsumed by a group of statisticians and 
computer scientists called the R Project Team. Separate executable versions 
for Linux, Microsoft Windows, and Apple macOS are available for download 
at 
https://www.r-project.org/ 
RStudio is an integrated R development system that includes a program edi-
tor. It is available from Posit at 
https://posit.co/ 
R is an object-oriented system with an extensive set of pre-deﬁned objects 
and methods. R provides the standard set of operators and functions for the 
common mathematical operations for numeric objects. Numeric data can be 
operated on by use of the ordinary arithmetic operators, +, -, *, /, and  ^. 
Other more specialized binary operators are enclosed between two “%” sym-
bols; for example, %/% means “integer division,” that is, division followed by 
rounding downward to the closest integer. The user can also deﬁne additional 
binary operators (see below). 
R has many useful functions, both for general computations and for sta-
tistical data analysis. The arguments are positional, but they can be assigned 
by name. Some may be initialized to a default value. 
The user can deﬁne additional functions (see below). Many other functions 
are provided through packages that, although not part of the core R, can be 
easily installed.

1.6 R
9
The standard prompt to enter a statement is “>”. R statements are line-
oriented but continue from one line to another until a syntactically complete 
statement is terminated by an end-of-line. A new statement can be started on 
the same line following a “;” delimiter. White space in an R statement has 
no meaning. Assignment is made by “<-” or by “=”. A comment statement 
in R begins with a pound sign, “#”. A block of R statements can be grouped 
together, to deﬁne a function, for example, by “{” and  “}”. 
1.6.1 R Data Types 
R provides for several types of data, numeric, character, logical, and so on. 
The basic datum in R is a one-dimensional sequence of quantities of the same 
type. Logical variables are essentially the same a numeric variables, with a 
value of 1 indicating true and a value of 0 indicating false. A logical variable 
can be initialized by an assignment using either TRUE, T, FALSE, or  F. Logical 
relational operators result in a logical value. For example, the logical relation 
3<5 has a value of TRUE, and the logical relation 3==5 has a value of FALSE. 
Character variables can be initialized by an assignment using either ’ and 
’ or " and ". Character strings can be concatenated and can be compared 
lexically. 
If an R statement consists of just the name of an object, the object is 
printed to the standard output device. The display includes the numbers of 
the elements. Even if there is only one element, “[1]” is part of the display. 
> y <- 2  
> y  
[1] 2 
> y < 2  
[1] FALSE 
> x <- 6  
> (y < 2 )*x  
[1] 0 
> # The next statement adds 2x and y-squared, 
> # computes the square root, and displays the result. 
> sqrt(2*x+y^2) 
[1] 4 
1.6.2 Program Control Statements 
Statements can be executed conditionally based on logical variables in if or 
elseif clauses.

10
1 Introduction
> x <- 6  
> y <- 2  
> if (y<x){ 
>
y<-y+4 
>
x<-3 
> }  
> x  
[1] 3 
> y  
[1] 6 
A sequence of R commands, possibly with modiﬁed data, can be performed 
repeatedly by use of the for directive to produce a loop of actions. The R 
statements that are to be repeated are enclosed in { and }. If there  is  only  
one statement to be repeated, as in the example below, the { and } may be 
omitted (but the statement must be on the same line as the for statement). 
The for loop uses an index that takes on successively the values in a vector. 
The syntax is simple as seen in the illustration: 
> x <- c(1,3) 
> y <- 0  
> for (i in x){ 
>
y <- y + 2*i + 4 
> }  
> y  
[1] 16 
1.6.3 Packages 
A group of functions and other objects can be packaged together in a “library” 
or “package,” and then, after the package is installed with the local R system, 
all of the functions together with associated data and documentation can be 
made available in an R session just by loading the package. 
Many packages can be obtained from the Comprehensive R Archive Net-
work (CRAN) at the same website as above or from GitHub. The GUI that 
comes with the R system on most platforms includes menu items that list 
available packages and then allows chosen ones to be installed. Alternatively, 
the install.packages function can be used. The library function can be 
used to load packages.

1.6 R
11
The R system generally consists of a set of packages, a “base” package, a 
“stats” package, and so on. When the R program is invoked, a standard set 
of packages are loaded. Beyond those, the user can load any number of other 
packages that have been installed on the local system. 
Within an R session, the available functions include all of those in the 
loaded packages. Sometimes, there may be name conﬂicts (two functions with 
the same name). The conﬂicts are resolved in the order of the loading of the 
packages; last-in gets preference. Reference to a function name often includes 
the package that includes the function; for example, “tr {psych}” refers to 
the tr function in the psych package. (This notation is not used in the R 
statement itself; the R statement invoking tr brings in the currently active 
version of tr.) 
Documentation for a new package can be developed (using the R package 
roxygen or by other methods), and when the new package is loaded, the 
documentation becomes available to the user. This package, which is available 
on CRAN, can be described as a documentation system. It is also available at 
https://roxygen.org/ 
There are very many R packages available on CRAN. Some packages ad-
dress a fairly narrow area of application, but some provide a wide range of 
useful functions. Two notable ones of the latter type are MASS, developed pri-
marily by Bill Venables and Brian Ripley, and Hmisc, developed primarily by 
Frank Harrell. 
A problem with many of the packages at CRAN is that some of them 
were not written, developed, or tested by people with knowledge and skill in 
software development. Some of these packages are not of high quality. Another 
problem with packages on CRAN is that they may change from time to time, 
so that computed results may not be easily replicated. 
1.6.4 User Functions and Operators 
One of the most useful features of R is the ability for a user to write an R 
function. The syntax is very simple and can be illustrated by writing a user 
function we will call hypot. It computes the length of the hypotenuse of a 
right triangle, given the lengths of the two sides, a and b. In this example, one 
argument is initialized to a default value, so if that argument is not present 
in the user’s call, it will take the default value.

12
1 Introduction
> hypot <- function(a, b=a){ 
>
return(sqrt(a^2+b^2)) 
> }  
> x <- 3; y <- 4  
> hypot(x,y) 
[1] 5 
> hypot(x) 
[1] 4.242641 
If there is only one statement in the function, as in the example below, the 
{ and } may be omitted, and the statement just follows the function( ) on 
the same line. The return function is not necessary, and a simple function 
such as hypot could be written in one line: 
> hypot <- function(a, b=a) sqrt(a^2+b^2) 
A function with exactly two arguments that results in a singleton may 
easily be implemented as a binary operator. The function hypot above is 
an example; it is an operation on a and b. It could be implemented as an 
operator, with some symbol to designate the operator. Conversely, a binary 
operation such as “plus+” could be implemented as a function, say add. Some  
binary operations naturally suggest a function, and others suggest an operator. 
R allows the user to deﬁne an operator using the same format as for deﬁn-
ing a function. The form of the name of user-deﬁned operator is "%character-
string%", where  “character-string” is usually a single character. (Recall the 
operator “%/%” above.)  
"%hyp%" <- function(a,b){ 
return(sqrt(a^2+b^2)) 
} 
x <- 3; y <- 4  
x %hyp% y 
[1] 5 
In a simple case, the computations of the function can be expressed in 
ordinary R statements. In other cases, the computations can be expressed in 
a compiler language such as Fortran or C, compiled, and then linked into R. 
In either case, the user’s function is invoked by its name, just as if it were 
part of the original R system. The R package Rcpp (see page 645) facilitates 
the incorporation of Fortran or C functions into R.

1.6 R
13
Figure 1.1. Histogram of Simulated Normal Data 
1.6.5 Generating Artiﬁcial Data 
Some R functions that we will ﬁnd useful for illustrating R software are func-
tions for generating random data. These function names begin with “r,” and 
additional letters indicate the type of probability distribution; for example, 
rnorm generates random samples that are similar to one from a normal distri-
bution, and runif simulates data from a uniform distribution. The set.seed 
function causes the same “random” sample to generated each time. 
1.6.6 Graphics Functions 
R has extensive graphics capabilities. We will rarely use the graphics functions 
in this book, but I will just mention two graphics functions: plot which pro-
duces a two-dimensional scattergram and hist which produces a histogram. 
The following R code generates a sample of 100 random numbers from a 
normal distribution and then makes a histogram of the sample. 
> set.seed(12345) 
> x <- rnorm(100) 
> hist(x) 
The output is shown in Fig. 1.1. 
We discuss an R function for three-dimensional graphics, called persp, in  
Exercise 4.10 on page 255.

14
1 Introduction
1.6.7 Special Data in R 
In statistical datasets, we often encounter “missing data.” This can occur when 
the dataset consists of multiple measurements on each observational unit and 
some of these measurements are not available for a given observational unit. 
If all measurements on the unit are missing, the observation itself is merely 
omitted, but if some measurements are available, then we just need a method 
of indicating that a cell is missing. R provides a special value, NA, meaning 
“not applicable,” to indicate that the actual value is missing. If any operand 
in an operation has a value of NA, the result of the operation has a value of NA. 
Since when an object with a value of NA is compared with any other object, 
the result is NA, R provides a logical function, is.na, to determine if an object 
has a value of NA. We will discuss missing values further on page 510. 
There are three special numeric values, Inf, -Inf, and  NaN: 
> 5/0 
[1] Inf 
> -5/0 
[1] -Inf 
> 5/0 - 5/0 
[1] NaN 
> 0/0 
[1] NaN 
The values participate in arithmetic operations as would be expected in 
the extended reals; operations with normal infinity∞or negative normal infinity−∞generally yield inﬁnite values 
of the appropriate sign. Undeﬁned operations, including any operation with 
NaN, yield a NaN (“not a number”). 
R also supports imaginary numbers, which are created by the function 
complex with the keyword argument imaginary or with the special symbol 
i preceded by a literal; thus, complex(imaginary=3) and 3i both represent 
3 normal i3i. Both  complex(real=2,imaginary=-3) and 2-3i represent the complex 
number 2 minus 3 normal i2 −3i. The vectors and matrix discussed in this book are real, but 
the eigenvalues of a real matrix may comprise imaginary components. 
1.6.8 Determining Properties of a Computer System in R 
R provides access to special computer values, such as the “machine epsilon.” 
These values are stored in a structure named .Machine. The extraction opera-
tor “$” can be used to access an individual value, such as .Machine$double. 
eps, which is used to determine whether or not two ﬂoating-point values 
are essentially equal. These machine values are often used in numerical com-
putations. Some R functions have an argument called “tol” used to detect 
“near zeros”; .Machine$double.eps is often an appropriate setting for this

1.6 R
15
tolerance. We will discuss these special numbers that determine computer 
properties further in Sect. 10.2.2. 
1.6.9 Documentation: Finding R Functions and Packages 
The quickest way to ﬁnd R software is usually to use a web search program 
including the term “R.” The R Seek organization provides a restricted web 
search just for R software: 
https://rseek.org/ 
An index of R software for speciﬁc tasks is the views site at cran: 
https://cran.r-project.org/web/views/ 
The menu in the GUI that comes with the R system on most platforms 
may also be useful in ﬁnding what functionality is available. 
1.6.10 Documentation: R Functions, Packages, and Other Objects 
There is extensive documentation incorporated in the software system itself. 
There are many books on R and there is an active user community. 
If a package has been loaded in an R session, then in that session, the 
operator “?” followed by the name of an R function (with or without quotes) 
in an attached package will connect with an online man page for the function. 
The “??” operator followed by the name of an R function will list documen-
tation sources for functions that contain that character string in all (most) of 
the packages in CRAN. 
All of the functions in a package that is already loaded can be listed using 
ls along with the name of the package. The name of the package is entered 
as a character string and hence must be enclosed in quotes. For example, if 
the L1pack package is loaded, we have 
> ls("package:L1pack") 
[1] "dlaplace"
"l1fit"
"lad"
"lad.fit" "lad.fit.BR" 
[6] "lad.fit.EM" "plaplace" "qlaplace" "rlaplace" "rmLaplace" 
[11] "simulate.lad" 
Documentation for some packages has been stored in a form that is ac-
cessible by the R function vignette. The documentation and examples from 
vignette are generally more extensive than from other online help. They are 
often in the form of PDF ﬁles or web pages. 
The ﬁrst question is whether or not a given package has any vignettes and 
if so what objects in the package have vignettes. The function with no argu-
ments, vignette(), provides a list of all installed (not just loaded) packages 
that have documentation available via vignette and what topics are available 
or which objects in each package have vignettes.

16
1 Introduction
Since R is an ever-evolving package, vignettes for packages may be added 
from time to time. For example, at the time of this writing, there are no 
vignettes for the L1pack package, but there are for the Matrix package. 
Using vignette(), we see that the name of one is Intro2Matrix. Using  
vignette("Intro2Matrix"), we get a nice 9-page PDF ﬁle explaining the 
features of the package. 
1.6.11 The Design of the R Programming Language 
The R system can be viewed from various perspectives. At one level, it is just 
a calculator that performs linear algebraic computations with a simple and 
intuitive interface. 
More complicated computational problems involve objects such as lists 
(page 251). Some computational problems beneﬁt from a fully object-oriented 
paradigm, and a set of objects with “S4 classes” allow this (an example of an 
S4  class is in Exercise  12.16 on page 650). Most of the R objects we will deal 
with in this book are of an “S3 class.” 
We will not go into the details of the R system at this level, but the 
interested reader is referred to Chambers (2008) or Wickham (2019). 
1.6.12 Why I Use R in This Book 
R is one of the most useful tools a statistician, data scientist, or numerical 
analyst can have. The time taken to learn to use that tool eﬀectively is time 
well-spent. 
A second reason I encourage use of R while reading this book is to help in 
learning the concepts and properties of vector and matrix operations. Seeing 
the operations on real numbers ﬁxes the ideas. 
** * ** 
The way to learn R, as with any software system, is to learn how to do 
one or two simple things and then to use it for more things. The help system 
with the “?” operator may be used extensively. 
Exercises in later chapters will require use of R. The R appendices in 
those chapters introduce the R system as needed, but sometimes the exercises 
require features of the language that I have not discussed. 
It is not the purpose of this book to provide a tutorial or to “teach” R.

Part I 
Linear Algebra

2 
Vectors and Vector Spaces 
Vector spaces are mathematical structures consisting of collections of vectors 
together with operations on those vectors. The elements of the vectors must 
themselves be members of ﬁelds, which are mathematical structures consisting 
of sets and two operations (usually called addition and multiplication) deﬁned 
on those sets. The two most common and familiar ﬁelds are the ﬁeld of real 
numbers and the ﬁeld of complex numbers. The ﬁeld of real numbers is denoted 
by IR and is just called the reals. 
In this chapter, we discuss a wide range of basic topics related to vectors 
of real numbers. Some of the properties carry over to vectors over other ﬁelds, 
such as complex numbers, but the reader should not assume this. Occasionally, 
for emphasis, we will refer to “real” vectors or “real” vector spaces, but unless 
it is stated otherwise, we are assuming that the vectors and vector spaces 
are real. The topics and the properties of vectors and vector spaces that we 
emphasize are motivated by applications in the data sciences. 
While reading about vectors and operations with them, it may be instruc-
tive to work with numerical examples. The R software system provides an 
excellent way of doing this. The introductory material beginning on page 8 
and the appendix to this chapter about vectors in R, beginning on page 62, 
provide enough information to get started using R to manipulate vectors. 
2.1 Operations on Vectors 
The elements of the vectors we will use in the following are real numbers, that 
is, elements of IR. We call elements of IR scalars. Vector operations are deﬁned 
in terms of operations on real numbers. 
Two vectors can be added if they have the same number of elements. 
The sum of two vectors is the vector whose elements are the sums of the 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 2 
19

20
2 Vectors and Vector Spaces
corresponding elements of the vectors being added. Vectors with the same 
number of elements are said to be conformable for addition. A vector all of 
whose elements are 0 is the  additive identity for all conformable vectors. 
We overload the usual symbols for the operations on the reals to signify 
corresponding operations on vectors or matrices when those operations are 
deﬁned. Hence, “+” can mean addition of scalars, addition of conformable 
vectors, or addition of a scalar to a vector. This last meaning of “+” may 
not be used in many mathematical treatments of vectors, but it is consistent 
with the semantics of modern computer languages such as R, Fortran, and 
MATLAB. By the addition of a scalar and a vector, we mean the addition 
of the scalar to each element of the vector, resulting in a vector of the same 
number of elements. 
A scalar multiple of a vector (that is, the product of a real number and 
a vector) is the vector whose elements are the multiples of the corresponding 
elements of the original vector. Juxtaposition of a symbol for a scalar and a 
symbol for a vector indicates the multiplication of the scalar with each element 
of the vector, resulting in a vector of the same number of elements. 
The basic operation in working with vectors is the addition of a scalar 
multiple of one vector to another vector, 
z equals a x plus y commaz = ax + y,
(2.1) 
where a is a scalar, and x and y are vectors conformable for addition. Viewed 
as a single operation with three operands, this is called an axpy operation for 
obvious reasons. (Because the original Fortran versions of BLAS to perform 
this operation were called saxpy and daxpy, the operation is also sometimes 
called “saxpy” or “daxpy”. See Sect. 12.2.1 on page 624, for a description of 
the BLAS.) 
The axpy operation is a linear combination. Such linear combinations of 
vectors are the basic operations in most areas of linear algebra. The com-
position of axpy operations is also an axpy; that is, one linear combination 
followed by another linear combination is a linear combination. Furthermore, 
any linear combination can be decomposed into a sequence of axpy operations. 
A special linear combination is called a convex combination. For vectors x 
and y, it is the combination 
a x plus b y commaax + by,
(2.2) 
where a, b ≥ 0 and  a + b = 1. A set of vectors that is closed with respect to 
convex combinations is said to be convex. 
2.1.1 Linear Combinations and Linear Independence 
If a given vector can be formed by a linear combination of one or more vectors, 
the set of vectors (including the given one) is said to be linearly dependent; 
conversely, if in a set of vectors, no one vector can be represented as a linear 
combination of any of the others, the set of vectors is said to be linearly

2.1 Operations on Vectors
21
independent. Linear independence is a property of a set. In Eq. (2.1), for 
example, the set of vectors x, y, and  z is not linearly independent. It is possible, 
however, that any two of these vectors comprise a linearly independent set. 
Linear independence is one of the most important concepts in linear alge-
bra. 
Notice that linear (in)dependence is a property of a set of vectors, all of 
the same order; nevertheless, we often say that the elements in a linearly 
(in)dependent set are “linearly (in)dependent vectors.” 
If a set of vectors is linearly independent, then every pair of vectors in 
the set is a linearly independent set. On the other hand, it is possible that 
every pair in the set is linearly independent (that is, each doubleton subset is 
linearly independent), but the set of vectors is not linearly independent. (This 
is Exercise 2.1.) 
We can see that the deﬁnition of a linearly independent set of vectors 
{v1, . . . , vk} is equivalent to stating that if 
a 1 v 1 plus midline horizontal ellipsis a Subscript k Baseline v Subscript k Baseline equals 0 commaa1v1 + · · · akvk = 0,
(2.3) 
then a1 = · · · = ak = 0. This condition can serve as an alternate deﬁnition of 
linear independence. 
If the set of vectors {v1, . . . , vk} is not linearly independent, then it is 
possible to select a maximal linearly independent subset; that is, a subset of 
{v1, . . . , vk} that is linearly independent and has maximum cardinality. We 
do this by selecting an arbitrary vector, vi1 , and then seeking a vector that 
is independent of vi1 . If there are none in the set that is linearly independent 
of vi1 , then a maximum linearly independent subset is just the singleton, 
because all of the vectors must be a linear combination of just one vector 
(that is, a scalar multiple of that one vector). If there is a vector that is 
linearly independent of vi1 , say  vi2 , we next seek a vector in the remaining set 
that is independent of vi1 and vi2 . If one does not exist, then {vi1 , vi2} is a 
maximal subset because any other vector can be represented in terms of these 
two and hence, within any subset of three vectors, one can be represented 
in terms of the two others. Thus, we see how to form a maximal linearly 
independent subset, and we see that the maximum cardinality of any subset 
of linearly independent vectors is unique, however they are formed. 
It is easy to see that the maximum number of n-vectors that can form a 
set that is linearly independent is n. (We can see this by assuming n linearly 
independent vectors and then, for any (n + 1)th vector, showing that it is 
a linear combination of the others by building it up one by one from linear 
combinations of two of the given linearly independent vectors. In Exercise 2.2, 
you are asked to write out these steps.) 
Properties of a set of vectors are usually invariant to a permutation of the 
elements of the vectors if the same permutation is applied to all vectors in the 
set. In particular, if a set of vectors is linearly independent, the set remains 
linearly independent if the elements of each vector are permuted in the same 
way.

22
2 Vectors and Vector Spaces
If the elements of each vector in a set of vectors are separated into sub-
vectors, linear independence of any set of corresponding subvectors implies 
linear independence of the full vectors. To state this more precisely for a set 
of three n-vectors, let x = (x1, . . . , xn), y = (y1, . . . , yn), and z = (z1, . . . , zn). 
Now let {i1, . . . , ik} ⊆{1, . . . , n}, and  form  the  k-vectors ˜x = (xi1 , . . . , xik ), 
˜y = (yi1 , . . . , yik ), and ˜z = (zi1 , . . . , zik ). Then linear independence of ˜x, ˜y, and  
˜z implies linear independence of x, y, and  z. (This can be shown directly from 
the deﬁnition of linear independence. It is related to Eq. (2.19) on page 30, 
which you are asked to prove in Exercise 2.6.) 
2.1.2 Vector Spaces and Spaces of Vectors 
Let V be a set of n-vectors such that any linear combination of the vectors 
in V is also in V ; that is,  V is convex. Such a set together with the usual 
vector algebra is called a vector space. A vector space is a linear space. It is an  
algebraic structure consisting of a set together with the axpy operation, with 
the restriction that the set is closed under the operation. To indicate that it 
is a structure, rather than just a set, we use a calligraphic font to denote a 
vector space; V or W, for example, and we may denote its two components as 
script upper V equals left parenthesis upper V comma ring right parenthesis commaV = (V, ◦),
where V is just the set, and ◦ denotes the axpy operation. While it would be 
possible to allow a vector space to be deﬁned in terms of some other linear 
operation under which the set is closed, we will always assume that it is the 
axpy operation. 
A vector space necessarily includes the additive identity (the zero vector). 
(To see this, in the axpy operation, let a = −1 and  y = x.) 
The set consisting only of the additive identity, along with the axpy op-
eration, is a vector space. It is called the “null vector space.” Some people 
deﬁne “vector space” in a way that excludes it, because its properties do not 
conform to many general statements we can make about other vector spaces. 
Except for the null vector space, the cardinality of the set of vectors in 
the vector space is uncountable. This is true because the multiplier a in the 
linear combination can be any element of IR, which is uncountable. 
A vector space is necessarily convex (that is, the set of vectors comprising 
the space is convex; this follows from the deﬁnitions). A geometric perspective 
of convexity of a set means that any point on the line segment connecting any 
two points in the set is also in the set. We see this by letting v1 and v2 be any 
two vectors in the space and c be a real number in the interval [0, 1]. A point 
on the line segment connecting them can be written as cv1+(1−c)v2 for some 
c ∈ [0, 1]. Because the space is closed with respect to the axpy operation, we 
know that w = v1 − v2 is in the space and hence cw + v2 = cv1 + (1 − c)v2 is 
in the space. 
We often refer to the “usual algebra,” which is a linear algebra consist-
ing of two operations: vector addition and scalar times vector multiplication,

2.1 Operations on Vectors
23
which are the two operations comprising an axpy. It has closure of the space 
under the combination of those operations, commutativity and associativity 
of addition, an additive identity and inverses, a multiplicative identity, distri-
bution of multiplication over both vector addition and scalar addition, and 
associativity of scalar multiplication and scalar times vector multiplication. 
A vector space can also be composed of other objects, such as matrices, 
along with their appropriate operations. The key characteristic of a vector 
space is a linear algebra. 
Generating Sets 
Given a set G of vectors of the same order, a vector space can be formed from 
the set G together with all vectors that result from the axpy operation being 
applied to all combinations of vectors in G and all values of the real number 
a; that is, for all vi, vj ∈ G and all real a, 
left brace a v Subscript i Baseline plus v Subscript j Baseline right brace period{avi + vj}.
This set together with the axpy operation itself is a vector space. It is 
called the space generated by G. We denote this space as 
normal s normal p normal a normal n left parenthesis upper G right parenthesis periodspan(G).
We will discuss generating and spanning sets further in Sect. 2.1.3. 
The Order and the Dimension of a Vector Space 
The vector space consisting of all n-vectors with real elements is denoted IRn . 
(As mentioned earlier, the notation IRn can also refer to just the set of n-
vectors with real elements; that is, to the set over which the vector space is 
deﬁned.) 
The order of a vector space is the order of the vectors in the space (their 
“lengths”, as deﬁned on page 2). 
The dimension of a vector space is the maximum number of linearly inde-
pendent vectors in the vector space. We denote the dimension by 
normal d normal i normal m left parenthesis dot right parenthesis commadim(·),
which is a mapping into the positive integers, which we will denote by ZZ+. 
Because the maximum number of n-vectors that can form a linearly indepen-
dent set is n, as we showed above, the dimension of the vector space is no 
greater than the order of a vector space. 
Both the order and the dimension of IRn are n. A set of  m linearly inde-
pendent n-vectors with real elements can generate a vector space within IRn 
of order n and dimension m. 
We also may use the phrase dimension of a vector to mean the dimension 
of the vector space of which the vector is an element. This term is ambiguous, 
but its meaning is clear in speciﬁc contexts, such as dimension reduction, 
which we will discuss later.

24
2 Vectors and Vector Spaces
Vector Spaces with an Inﬁnite Number of Dimensions 
It is possible that no ﬁnite set of vectors spans a given vector space. In that 
case, the vector space is said to be of inﬁnite dimension. 
Many of the properties of vector spaces that we discuss hold for those with 
an inﬁnite number of dimensions; but not all do, such as the equivalence of 
norms (see page 39). 
Throughout this book, however, unless we state otherwise, we assume that 
the vector spaces have a ﬁnite number of dimensions. 
Some Special Vectors: Notation 
We denote the additive identity in a vector space of order n by 0n or sometimes 
by 0. This is the vector each of whose elements is the additive identity of the 
underlying ﬁeld: 
0 Subscript n Baseline equals left parenthesis 0 comma ellipsis comma 0 right parenthesis period0n = (0, . . . , 0).
(2.4) 
We call this the zero vector, or the  null vector. (A vector x /= 0 is called a 
“nonnull vector”.) This vector by itself is sometimes called the null vector 
space. It is not a vector space in the usual sense; it would have dimension 0. 
(All linear combinations are the same.) 
Likewise, we denote the vector consisting of all ones by 1n or sometimes 
by 1: 
1 Subscript n Baseline equals left parenthesis 1 comma ellipsis comma 1 right parenthesis period1n = (1, . . . , 1).
(2.5) 
We call this the one vector and also the “summing vector” (see page 44). This 
vector and all scalar multiples of it are vector spaces with dimension 1. (This 
is true of any single nonzero vector; all linear combinations are just scalar 
multiples.) Whether 0 and 1 without a subscript represent vectors or scalars 
is usually clear from the context. 
The zero vector and the one vector are both instances of constant vectors; 
that is, vectors all of whose elements are the same. In some cases, we may 
abuse the notation slightly, as we have done with “0” and “1” above, and use 
a single symbol to denote both a scalar and a vector all of whose elements are 
that constant; for example, if “c” denotes a scalar constant, we may refer to the 
vector all of whose elements are c as “c” also. These notational conveniences 
rarely result in any ambiguity. They also allow another interpretation of the 
deﬁnition of addition of a scalar to a vector that we mentioned at the beginning 
of the chapter. 
The ith unit vector, denoted by ei, has  a 1 in the  ith position and 0s in 
all other positions: 
e Subscript i Baseline equals left parenthesis 0 comma ellipsis comma 0 comma 1 comma 0 comma ellipsis comma 0 right parenthesis periodei = (0, . . . , 0, 1, 0, . . . , 0).
(2.6) 
Another useful vector is the sign vector, which is formed from signs of the 
elements of a given vector. It is denoted by “sign(·)” and for x = (x1, . . . , xn) 
is deﬁned by

2.1 Operations on Vectors
25
StartLayout 1st Row 1st Column normal s normal i normal g normal n left parenthesis x right parenthesis Subscript i 2nd Column equals 3rd Column 1 normal i normal f x Subscript i Baseline greater than 0 comma 2nd Row 1st Column Blank 2nd Column equals 3rd Column 0 normal i normal f x Subscript i Baseline equals 0 comma 3rd Row 1st Column Blank 2nd Column equals 3rd Column negative 1 normal i normal f x Subscript i Baseline less than 0 period EndLayout
sign(x)i =
1
if xi > 0,
=
0
if xi = 0,
= −1
if xi < 0.
(2.7) 
Ordinal Relations Among Vectors 
There are several possible ways to form a rank ordering of vectors of the same 
order, but no complete ordering is entirely satisfactory. (Note the unfortunate 
overloading of the words “order” and “ordering” here.) If x and y are vectors 
of the same order and for corresponding elements xi > yi, we say  x is greater 
than y and write 
x greater than y periodx > y.
(2.8) 
In particular, if all of the elements of x are positive, we write x >  0. 
If x and y are vectors of the same order and for corresponding elements 
xi ≥ yi, we say  x is greater than or equal to y and write 
x greater than or equals y periodx ≥y.
(2.9) 
This relationship is a partial ordering (see Exercise 8.2a on page 433 for the 
deﬁnition of partial ordering). 
The expression x ≥ 0 means that all of the elements of x are nonnegative. 
Set Operations on Vector Spaces 
The ordinary operations of subsetting, intersection, union, direct sum, and 
direct product for sets have analogs for vector spaces that are deﬁned in terms 
of the set of vectors in the vector space. For these operations to make sense 
for vector spaces, all spaces must be of the same order, and the operations 
must yield vector spaces. 
If V and W are vector spaces, and we have V1 ⊂V, then  V1 should be 
a vector space; V ∩W  should be a vector space; V ∪W  should be a vector 
space; and so on. Furthermore, the objects V ∩W  and V ∪W  make sense only 
if V and W are of the same order. If V and W are sets of vectors, V ∩ W 
and V ∪W make sense even if they contain vectors of diﬀerent orders (in that 
case, V ∩ W = ∅, of course). 
A diﬀerence in sets and vector spaces is that a vector space is a set together 
with an operation, which must be closed within the set. 
An operator, such as “union” or “∪’ should yield an object consistent 
with the type of objects of the operands; if the operands are vector spaces, 
the results in most cases should be vector spaces. For example, if V = (V, ◦) 
and W = (W, ◦) are vector spaces, then V ∪ U is the ordinary union of the 
sets; however, V ∪W  is the union of the vector spaces, and is not necessarily 
the same as (U ∪ W, ◦), which may not even be a vector space. 
We use some of the same notation to refer to vector spaces that we use to 
refer to sets. The set operations themselves are performed on the individual

26
2 Vectors and Vector Spaces
sets to yield a set of vectors, and the resulting vector space is the space 
generated by that set of vectors. Unfortunately, there are many inconsistencies 
in terminology used in the literature regarding operations on vector spaces. 
When I use a term and/or symbol, such as “union” or “∪,” for a structure 
such as a vector space, I use it in reference to the structure. For example, if 
V = (V, ◦) and  W = (W, ◦) are vector spaces, then V ∪ U is the ordinary 
union of the sets; however, V ∪ W  is the union of the vector spaces and 
is  not necessarily  the same as (U ∪ W, ◦), which may not even be a vector 
space. Occasionally in the following discussion, I will try to point out common 
variants in usage. 
Notice that union and intersection are binary operations that are both 
associative and commutative. 
The convention that I follow allows the well-known relationships among 
common set operations to hold for the corresponding operations on vector 
spaces; for example, if V and W are vector spaces, V ⊆V ∪W, just as for sets 
V and W. 
The properties of vector spaces are proven the same way that properties 
of sets are proven, after ﬁrst requiring that the axpy operation have the same 
meaning in the diﬀerent vector spaces. For example, to prove that one vector 
space is a subspace of another, we show that any given vector in the ﬁrst 
vector space is necessarily in the second. To prove that two vector spaces 
are equal, we show that each is a subspace of the other. Some properties of 
vector spaces and subspaces can be shown more easily using “basis sets” for 
the spaces, which we discuss in Sect. 2.1.3, beginning on page 31. 
Note that if (V, ◦) and  (W, ◦) are vector spaces of the same order and U is 
some set formed by an operation on V and W, then (U, ◦) may not be a vector 
space because it is not closed under the axpy operation, ◦. We sometimes refer 
to a set of vectors of the same order together with the axpy operator (whether 
or not the set is closed with respect to the operator) as a “space of vectors” 
(instead of a “vector space”). 
Essentially Disjoint Vector Spaces 
If the only element in common in a set of vector spaces of the same order is 
the additive identity, the spaces are said to be essentially disjoint. 
If the vector spaces V and W are essentially disjoint, it is clear that any 
element in V (except the additive identity) is linearly independent of any 
set of elements in W. (If  v1 ∈V  could be written as a1w1 + · · ·  + akwk for 
w1, . . . , wk ∈W, then  V and W would not be essentially disjoint.) 
Subpaces 
Given a vector space V = (V, ◦), if W is any subset of V , then the vector 
space W generated by W, that is, span(W), is said to be a subspace of V, and  
we denote this relationship by W ⊆V.

2.1 Operations on Vectors
27
If W ⊆ V  and W /= V, then  W is said to be a proper subspace of V. If  
W = V, then  W ⊆V  and V ⊆W, and  the converse is also true.  
The maximum number of linearly independent vectors in the subspace 
cannot be greater than the maximum number of linearly independent vectors 
in the original space; that is, if W ⊆V, then  
normal d normal i normal m left parenthesis script upper W right parenthesis less than or equals normal d normal i normal m left parenthesis script upper V right parenthesisdim(W) ≤dim(V)
(2.10) 
(Exercise 2.3). If W is a proper subspace of V, then dim(W) < dim(V). 
Intersections of Vector Spaces 
For two vector spaces V and W of the same order with vectors formed from 
the same ﬁeld, we deﬁne their intersection, denoted by V ∩W, to be the  set  
of vectors consisting of the intersection of the sets in the individual vector 
spaces together with the axpy operation. 
The intersection of two vector spaces of the same order that are not es-
sentially disjoint is a vector space, as we can see by letting x and y be any 
vectors in the intersection U = V ∩W, and showing, for any real number a, 
that ax + y ∈U. This is easy because both  x and y must be in both V and 
W. 
If V and W are essentially disjoint, then V ∩W  = (0, ◦), which, as we have 
said, is not a vector space in the usual sense. 
Note that 
normal d normal i normal m left parenthesis script upper V intersection script upper W right parenthesis less than or equals min left parenthesis normal d normal i normal m left parenthesis script upper V right parenthesis comma normal d normal i normal m left parenthesis script upper W right parenthesis right parenthesisdim(V ∩W) ≤min(dim(V), dim(W))
(2.11) 
(Exercise 2.3). 
Also note that the ∩ operation for vector spaces is both associative and 
commutative, just as it is for sets. 
Unions and Direct Sums of Vector Spaces 
Given two vector spaces V and W of the same order, we deﬁne their union, 
denoted by V ∪W, to be the vector space generated by the union of the sets 
in the individual vector spaces together with the axpy operation. If V = (V, ◦) 
and W = (W, ◦), this is the vector space generated by the set of vectors V ∪W; 
that is, 
script upper V union script upper W equals normal s normal p normal a normal n left parenthesis upper V union upper W right parenthesis periodV ∪W = span(V ∪W).
(2.12) 
Note that the ∪ operation for vector spaces is both associative and commu-
tative, just as it is for sets. 
The union of the sets of vectors in two vector spaces may not be closed 
under the axpy operation (Exercise 2.4b), but the union of vector spaces is a 
vector space by deﬁnition. 
The vector space generated by the union of the sets in the individual vector 
spaces is easy to form. Since (V, ◦) and  (W, ◦) are vector spaces (so for any

28
2 Vectors and Vector Spaces
vector x in either V or W, ax is in that set), all we need do is just include all 
simple sums of the vectors from the individual sets, that is, 
script upper V union script upper W equals StartSet v plus w comma normal s period normal t period v element of script upper V comma w element of script upper W EndSet periodV ∪W = {v + w, s.t. v ∈V, w ∈W}.
(2.13) 
It is easy to see that this is a vector space by showing that it is closed with 
respect to axpy. (As above, we show that for any x and y in V ∪W  and for 
any real number a, ax + y is in V ∪W.) 
(Because of the way the union of vector spaces can be formed from simple 
addition of the individual elements, some authors call the vector space in 
Eq. (2.13) the “sum” of V and W, and  write it as  V + W. Other authors, 
including myself, call this the direct sum, and denote it by V⊕W. Some authors 
deﬁne “direct sum” only in the cases of vector spaces that are essentially 
disjoint. Still other authors deﬁne “direct sum” to be what I will call a “direct 
product” below.) 
Despite the possible confusion with other uses of the notation, I often 
use the notation V ⊕W  because it points directly to the nice construction 
of Eq. (2.13). To be clear: to the extent that I use “direct sum” and “⊕” for  
vector spaces V and W, I will mean the direct sum 
script upper V circled plus script upper W identical to script upper V union script upper W commaV ⊕W ≡V ∪W,
(2.14) 
as deﬁned above. 
Notice that ⊕ is a binary operation and, inheriting the properties of ∪, is  
both associative and commutative. 
A further comment on overloaded notation: The meaning of an operator 
depends on the type of operand(s). Sometimes the notation for an operator for 
one type of operand may be the same as for an operator for a diﬀerent type of 
operand, in which case, the operations are actually diﬀerent. In this section, 
the operands are mostly vector spaces, and the operators are all overloaded 
for other types of operands. Two operators, ∩ and ∪, often take sets as their 
operands. The meanings of ∩ and ∪ for vector spaces are very similar as 
for sets, but they also involve the requirement that the algebraic structures 
be preserved in the operation; hence, even though the notation is the same, 
the operations are actually diﬀerent. Another operator for vector spaces, ⊕ 
(see below), often takes matrices as its operands (as we will see on page 82). 
Obviously, the meaning of ⊕ for matrices is diﬀerent from what we describe 
in this section. 
We also note that 
normal d normal i normal m left parenthesis script upper V circled plus script upper W right parenthesis equals normal d normal i normal m left parenthesis script upper V right parenthesis plus normal d normal i normal m left parenthesis script upper W right parenthesis minus normal d normal i normal m left parenthesis script upper V intersection script upper W right parenthesisdim(V ⊕W) = dim(V) + dim(W) −dim(V ∩W)
(2.15) 
(Exercise 2.5); therefore, 
normal d normal i normal m left parenthesis script upper V circled plus script upper W right parenthesis greater than or equals max left parenthesis normal d normal i normal m left parenthesis script upper V right parenthesis comma normal d normal i normal m left parenthesis script upper W right parenthesis right parenthesisdim(V ⊕W) ≥max(dim(V), dim(W))
and 
normal d normal i normal m left parenthesis script upper V circled plus script upper W right parenthesis less than or equals normal d normal i normal m left parenthesis script upper V right parenthesis plus normal d normal i normal m left parenthesis script upper W right parenthesis perioddim(V ⊕W) ≤dim(V) + dim(W).

2.1 Operations on Vectors
29
Direct Sum Decomposition of a Vector Space 
In some applications, given a vector space V, it is of interest to ﬁnd essentially 
disjoint vector spaces V1, . . . , Vn such that 
script upper V equals script upper V 1 circled plus midline horizontal ellipsis circled plus script upper V Subscript n Baseline periodV = V1 ⊕· · · ⊕Vn.
This is called a direct sum decomposition of V. 
A collection of essentially disjoint vector spaces V1, . . . , Vn such that V = 
V1 ⊕· · · ⊕Vn is said to be complementary with respect to V. 
It is clear that if V1, . . . , Vn is a direct sum decomposition of V, then  
normal d normal i normal m left parenthesis script upper V right parenthesis equals sigma summation Underscript i equals 1 Overscript n Endscripts normal d normal i normal m left parenthesis script upper V Subscript i Baseline right parenthesisdim(V) =
n
E
i=1
dim(Vi)
(2.16) 
(Exercise 2.5). 
An important property of a direct sum decomposition is that it allows 
a unique representation of a vector in the decomposed space in terms of a 
sum of vectors from the individual essentially disjoint spaces; that is, if V = 
V1 ⊕· · · ⊕Vn is a direct sum decomposition of V and v ∈V, then there exist 
unique vectors vi ∈Vi such that 
v equals v 1 plus midline horizontal ellipsis plus v Subscript n Baseline periodv = v1 + · · · + vn.
(2.17) 
We will prove this for the case n = 2. This is without loss, because the 
binary operation is associative, and additional spaces in the decomposition 
add nothing diﬀerent. 
Given the direct sum decomposition V = V1 ⊕V2, let  v be any vector in 
V. Because V1 ⊕V2 can be formed as in Eq. (2.13), there exist vectors v1 ∈V1 
and v2 ∈V2 such that v = v1 +v2. Now all we need to do is to show that they 
are unique. 
Let u1 ∈V1 and u2 ∈V2 be such that v = u1 +u2. Now  we  have  (v −u1) ∈ 
V2 and (v − v1) ∈V2; hence (v1 − u1) ∈V2. However, since v1, u1 ∈V1, 
(v1 − u1) ∈V1. Since  V1 and V2 are essentially disjoint, and (v1 − u1) is in  
both, it must be the case that (v1 − u1) = 0, or u1 = v1. In like manner, we 
show that u2 = v2; hence, the representation v = v1 + v2 is unique. 
An important fact is that for any vector space V with dimension 2 or 
greater, a direct sum decomposition exists; that is, there exist essentially dis-
joint vector spaces V1 and V2 such that V = V1 ⊕V2. 
This is easily shown by ﬁrst choosing a proper subspace V1 of V and then 
constructing an essentially disjoint subspace V2 such that V = V1 ⊕V2. The  
details of these steps are made simpler by use of basis sets, which we will 
discuss in Sect. 2.1.3, in particular the facts listed on page 32.

30
2 Vectors and Vector Spaces
Direct Products of Vector Spaces and Dimension Reduction 
The operations on vector spaces that we have mentioned so far require that 
the vector spaces be of a ﬁxed order. Sometimes in applications, it is useful 
to deal with vector spaces of diﬀerent orders. 
The direct product of the vector space V of order n and the vector space 
W of order m is the vector space of order n + m on the set of vectors 
StartSet left parenthesis v 1 comma ellipsis comma v Subscript n Baseline comma w 1 comma ellipsis comma w Subscript m Baseline right parenthesis comma normal s period normal t period left parenthesis v 1 comma ellipsis comma v Subscript n Baseline right parenthesis element of script upper V comma left parenthesis w 1 comma ellipsis comma w Subscript m Baseline right parenthesis element of script upper W EndSet comma{(v1, . . . , vn, w1, . . . , wm), s.t. (v1, . . . , vn) ∈V, (w1, . . . , wm) ∈W},
(2.18) 
together with the axpy operator deﬁned as the same operator in V and W 
applied separately to the ﬁrst n and the last m elements. The direct product 
of V and W is denoted by V × W. 
Note that the direct product of vector spaces is associative, but it is not 
commutative. (Recall that the direct sum operation is both associative and 
commutative.) 
Notice the possible diﬀerences in the interpretation of the direct product. 
In some cases, the direct product, which is analogous to the ordinary cross-
product (or Cartesian product) of sets, may be interpreted as nonassociative, 
so that the elements resulting from chained operations yield doubletons of 
doubletons, and so on; ((u, v), w) or (u, (v, w)), for example. The elements in 
the direct product deﬁned above are simple vectors. 
The vectors in V and W are sometimes called “subvectors” of the vectors 
in V × W. These subvectors are related to projections, which we will discuss 
in more detail in Sects. 2.2.2 (page 46) and  8.5.2 (page 396). 
We can see that the direct product is a vector space using the same method 
as above by showing that it is closed under the axpy operation. 
Note that 
normal d normal i normal m left parenthesis script upper V times script upper W right parenthesis equals normal d normal i normal m left parenthesis script upper V right parenthesis plus normal d normal i normal m left parenthesis script upper W right parenthesisdim(V × W) = dim(V) + dim(W)
(2.19) 
(Exercise 2.6). 
Note that for integers 0 < p < n, 
normal fraktur upper R Superscript n Baseline equals normal fraktur upper R Superscript p Baseline times normal fraktur upper R Superscript n minus p Baseline commaIRn = IRp × IRn−p,
(2.20) 
where the operations in the space IRn are the same as in the component vector 
spaces with the meaning adjusted to conform to the larger order of the vectors 
in IRn . (Recall that IRn represents the algebraic structure consisting of the 
set of n-tuples of real numbers plus the special axpy operator.) 
In statistical applications, we often want to do “dimension reduction.” 
This means to ﬁnd a smaller number of coordinates that cover the relevant 
regions of a larger-dimensional space. In other words, we are interested in 
ﬁnding a lower-dimensional vector space in which a given set of vectors in a 
higher-dimensional vector space can be approximated by vectors in the lower-
dimensional space. For a given set of vectors of the form x = (x1, . . . , xn) we  
seek a set of vectors of the form z = (z1, . . . , zp) that almost “cover the  same  
space”. (The transformation from x to z is called a projection.)

2.1 Operations on Vectors
31
2.1.3 Basis Sets for Vector Spaces 
If each vector in the vector space V can be expressed as a linear combination 
of the vectors in some set G, then  G is said to be a generating set or spanning 
set of V. The number of vectors in a generating set is at least as great as the 
dimension of the vector space. 
If all linear combinations of the elements of G are in V, the vector space 
is the space generated by G and is denoted by V(G) or by span(G), as we 
mentioned on page 23. We will use either notation interchangeably: 
script upper V left parenthesis upper G right parenthesis identical to normal s normal p normal a normal n left parenthesis upper G right parenthesis periodV(G) ≡span(G).
(2.21) 
Note that G is also a generating or spanning set for W where W ⊆span(G). 
A basis for a vector space is a set of linearly independent vectors that 
generate or span the space. For any vector space, a generating set consisting 
of the minimum number of vectors of any generating set for that space is a 
basis set for the space. A basis set is obviously not unique. 
Note that the linear independence implies that a basis set cannot contain 
the 0 vector. 
An important fact is 
• The representation of a given vector in terms of a given basis set is unique. 
To see this, let {v1, . . . , vk} be a basis for a vector space that includes the 
vector x, and  let  
x equals c 1 v 1 plus midline horizontal ellipsis c Subscript k Baseline v Subscript k Baseline periodx = c1v1 + · · · ckvk.
Now suppose 
x equals b 1 v 1 plus midline horizontal ellipsis b Subscript k Baseline v Subscript k Baseline commax = b1v1 + · · · bkvk,
so that we have 
0 equals left parenthesis c 1 minus b 1 right parenthesis v 1 plus midline horizontal ellipsis plus left parenthesis c Subscript k Baseline minus b Subscript k Baseline right parenthesis v Subscript k Baseline period0 = (c1 −b1)v1 + · · · + (ck −bk)vk.
Since {v1, . . . , vk} are independent, the only way this is possible is if ci = bi 
for each i. 
A related fact is that if {v1, . . . , vk} is a basis for a vector space of order 
n that includes the vector x and x = c1v1 + · · · ckvk, then  x = 0n if and only 
if ci = 0  for  each  i. 
For any vector space, the order of the vectors in a basis set is the same as 
the order of the vector space. 
Because the vectors in a basis set are independent, the number of vectors 
in a basis set is the same as the dimension of the vector space; that is, if B is 
a basis set of the vector space V, then  
normal d normal i normal m left parenthesis script upper V right parenthesis equals number sign left parenthesis upper B right parenthesis perioddim(V) = #(B).
(2.22) 
A simple basis set for the vector space IRn is the set of unit vectors 
{e1, . . . , en}, deﬁned on page  24.

32
2 Vectors and Vector Spaces
Properties of Basis Sets of Vector Subspaces 
There are several interesting facts about basis sets for vector spaces and var-
ious combinations of the vector spaces. Veriﬁcations of these facts all follow 
similar arguments, and some are left as exercises (Exercise 2.7, page 67.) 
• If B1 is a basis set for V1, B2 is a basis set for V2, and  V1 and V2 are 
essentially disjoint, then B1 ∩B2 = ∅. 
This fact is easily seen by assuming the contrary; that is, assume that 
b ∈ B1 ∩ B2. (Note that b cannot be the 0 vector.) This implies, however, 
that b is in both V1 and V2, contradicting the hypothesis that they are 
essentially disjoint. 
• Let V be a vector space and let V1 be a subspace, V1 ⊆V. Then there 
exists a basis B for V and a basis B1 for V1 such that B1 ⊆ B. 
• If B1 is a basis set for V1 and B2 is a basis set for V2, then  B1 ∪ B2 is a 
generating set for V1 ⊕V2. 
(We see this easily from the deﬁnition of ⊕ because any vector in V1 ⊕V2 
can be represented as a linear combination of vectors in B1 plus a linear 
combination of vectors in B2.) 
• If V1 and V2 are essentially disjoint, B1 is a basis set for V1, and  B2 is a 
basis set for V2, then  B1 ∪B2 is a basis set for V = V1 ⊕V2. 
This is the case that V1 ⊕V2 is a direct sum decomposition of V. 
• Suppose V1 is a real vector space of order n1 (that is, it is a subspace of 
IRn1 ) and  B1 is a basis set for V1. Now  let  V2 be a real vector space of 
order n2 and B2 be a basis  set for  V2. For each vector b1 in B1 form the 
vector 
b overTilde Subscript 1 Baseline equals left parenthesis b 1 vertical bar 0 comma ellipsis comma 0 right parenthesis where there are n 2 0 s comma˜b1 = (b1|0, . . . , 0)
where there are n2 0s,
and let -B1 be the set of all such vectors. (The order of each ˜b1 ∈-B1 is 
n1 + n2.) Likewise, for each vector b2 in B2 form the vector 
b overTilde Subscript 2 Baseline equals left parenthesis 0 comma ellipsis comma 0 vertical bar b 2 right parenthesis where there are n 1 0 s comma˜b2 = (0, . . . , 0|b2)
where there are n1 0s,
and let -B2 be the set of all such vectors. Then -B1∪-B2 is a basis for V1×V2. 
2.1.4 Inner Products 
Inner products are some of the most useful real-valued binary operators in 
real linear vector spaces. We ﬁrst consider inner products in general and then 
deﬁne the inner product for real vectors.

2.1 Operations on Vectors
33
Inner Products in General Real Vector Spaces 
An inner product is a mapping from a self-cross-product of a real vector space 
V ×V  to IR, which we denote by <·, ·>, and which has the following properties: 
1. Positivity: 
if x /= 0,  then <x, x> > 0. 
2. Commutativity:
<x, y> = <y, x>. 
3. Factoring of scalar multiplication in dot products:
<ax, y> = a<x, y> for real a. 
4. Relation of vector addition to addition of dot products:
<x + y, z> = <x, z> + <y, z>. 
These properties in fact deﬁne an inner product for mathematical objects for 
which an addition, an additive identity, and a multiplication by a scalar are 
deﬁned. From 2 and 3 above, we have the important properties of an inner 
product involving the additive identity: <0, x> = <x, 0> = <0, 0> = 0.  
Notice that the operation deﬁned in Eq. (2.24) is not an inner product for 
vectors over the complex ﬁeld because, if x is complex, we can have <x, x> = 0  
when x /= 0.  
A vector space together with an inner product is called an inner product 
space. 
Inner products are also deﬁned for matrices, as we will discuss on page 119. 
We should note in passing that there are two diﬀerent kinds of multiplication 
used in property 3. The ﬁrst multiplication is scalar multiplication, that is, 
an operation from IR × IRn to IRn , which we have deﬁned above, and the 
second multiplication is ordinary multiplication in IR, that is, an operation 
from IR × IR to IR. There are also two diﬀerent kinds of addition used in 
property 4. The ﬁrst addition is vector addition, deﬁned above, and the second 
addition is ordinary addition in IR. 
A useful property of inner products is the Cauchy–Schwarz inequality: 
left angle bracket x comma y right angle bracket less than or equals left angle bracket x comma x right angle bracket Superscript one half Baseline left angle bracket y comma y right angle bracket Superscript one half Baseline period<x, y> ≤<x, x>
1
2 <y, y>
1
2 .
(2.23) 
This relationship is also sometimes called the Cauchy–Bunyakovskii–Schwarz 
inequality. (Augustin-Louis Cauchy gave the inequality for the kind of dis-
crete inner products we are considering here, and Viktor Bunyakovskii and 
Hermann Schwarz independently extended it to more general inner products, 
deﬁned on functions, for example.) The inequality is easy to see, by ﬁrst ob-
serving that for every real number t, 
StartLayout 1st Row 1st Column 0 2nd Column less than or equals 3rd Column left angle bracket left parenthesis t x plus y right parenthesis comma left parenthesis t x plus y right parenthesis right angle bracket 2nd Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x comma x right angle bracket t squared plus 2 left angle bracket x comma y right angle bracket t plus left angle bracket y comma y right angle bracket 3rd Row 1st Column Blank 2nd Column equals 3rd Column a t squared plus b t plus c comma EndLayout0 ≤<(tx + y), (tx + y)>
= <x, x>t2 + 2<x, y>t + <y, y>
= at2 + bt + c,

34
2 Vectors and Vector Spaces
where the constants a, b, and  c correspond to the dot products in the preceding 
equation. This quadratic in t cannot have two distinct real roots. Hence, the 
discriminant, b2 − 4ac, must be less than or equal to zero; that is, 
left parenthesis one half b right parenthesis squared less than or equals a c period
(1
2b
)2
≤ac.
By substituting and taking square roots, we get the Cauchy–Schwarz inequal-
ity. Since both <x, x> and <y, y> are nonnegative, is also clear from this proof 
that equality holds only if x = 0 or  if  y = rx, for some scalar r (which of 
course includes 0). 
The Inner Product in Real Vector Spaces 
We now deﬁne a speciﬁc inner product of two real vectors. The inner product 
is a mapping 
normal fraktur upper R Superscript n Baseline times normal fraktur upper R Superscript n Baseline right arrow normal fraktur upper R periodIRn × IRn →IR.
Let x and y of the same order. The inner product of x and y, which  we  
denote by <x, y>, is deﬁned as 
left angle bracket x comma y right angle bracket equals sigma summation Underscript i Endscripts x Subscript i Baseline y Subscript i Baseline period<x, y> =
E
i
xiyi.
(2.24) 
Henceforth in this book, the notation <·, ·> without further qualiﬁcation for 
real vectors will mean the operation deﬁned as in Eq. (2.24). 
The inner product is also called the dot product or the scalar product. The  
dot product is actually a special type of inner product, and there is some 
ambiguity in the terminology. The dot product is the most commonly used 
inner product in the applications we consider, and so we will use the terms 
synonymously. 
The inner product is also sometimes written as x · y, hence the name dot 
product. Yet another notation for the inner product for real vectors is xT y, 
and we will see later that this notation is natural in the context of matrix 
multiplication. So for real vectors, we have the equivalent notations 
left angle bracket x comma y right angle bracket identical to x dot y identical to x Superscript normal upper T Baseline y period<x, y> ≡x · y ≡xTy.
(2.25) 
The inner product, or dot product, can reveal fundamental relationships be-
tween the two vectors, as we will see later. 
(I will mention one more notation that is equivalent for real vectors. This 
is the “bra·ket” notation originated by Paul Dirac and is still used in certain 
areas of application, especially in physics. Dirac referred to xT as the “bra x” 
and denoted it as <x|. He referred to an ordinary vector y as the “ket y” and  
denoted it as |y>. He then denoted the inner product of the vectors as <x||y>, 
or, omitting one vertical bar, as <x|y>.)

2.1 Operations on Vectors
35
Two vectors x and y such that <x, y> = 0 are said to be orthogonal. This  
term has such an intuitive meaning that we may use it prior to a careful 
deﬁnition and study, so I only introduce it here. We will discuss orthogonality 
more thoroughly in Sect. 2.1.8 beginning on page 43. 
2.1.5 Norms 
We consider a set of objects S that has an addition-type operator, +, a cor-
responding additive identity, 0, and a scalar multiplication; that is, a multi-
plication of the objects by a real (or complex) number. On such a set, a norm 
is a function, || · ||, from  S to IR that satisﬁes the following three conditions: 
1. Nonnegativity and mapping of the additive identity: 
if x /= 0,  then ||x|| > 0, and ||0|| = 0.  
2. Relation of scalar multiplication to real multiplication:
||ax|| = |a| ||x|| for real a. 
3. Triangle inequality:
||x + y|| ≤||x|| + ||y||. 
(If property 1 is relaxed to require only ||x|| ≥ 0 for  x /= 0, the function is 
called a seminorm.) Because a norm is a function whose argument is a vector, 
we also often use a functional notation such as ρ(x) to represent  a norm of  
the vector x. 
Sets of various types of objects (functions, for example) can have norms, 
but our interest in the present context is in norms for vectors and (later) 
for matrices. (The three properties above in fact deﬁne a more general norm 
for other kinds of mathematical objects for which an addition, an additive 
identity, and multiplication by a scalar are deﬁned. Norms are deﬁned for 
matrices, as we will discuss later. Note that there are two diﬀerent kinds of 
multiplication used in property 2 and two diﬀerent kinds of addition used in 
property 3.) 
A vector space together with a norm is called a normed space. 
For some types of objects, a norm of an object may be called its “length” 
or its “size.” (Recall the ambiguity of “length” of a vector that we mentioned 
previously.) 
Convexity 
A function f(·) over a convex domain S into a range R, where  both  S and 
R have an addition-type operator, +, corresponding additive identities, and 
scalar multiplication, is said to be convex, if, for any x and y in S, and  a such 
that 0 ≤ a ≤ 1, 
f left parenthesis a x plus left parenthesis 1 minus a right parenthesis y right parenthesis less than or equals a f left parenthesis x right parenthesis plus left parenthesis 1 minus a right parenthesis f left parenthesis y right parenthesis periodf(ax + (1 −a)y) ≤af(x) + (1 −a)f(y).
(2.26) 
If, for x /= y and a such that 0 < a <  1, the inequality in (2.26) is sharp, then 
the function is said to be strictly convex. 
It is clear from the triangle inequality that a norm is convex.

36
2 Vectors and Vector Spaces
Norms Induced by Inner Products 
There is a close relationship between a norm and an inner product. For any 
inner product space with inner product <·, ·>, a norm of an element of the 
space can be deﬁned in terms of the square root of the inner product of the 
element with itself: 
parallel to x parallel to equals StartRoot left angle bracket x comma x right angle bracket EndRoot period||x|| =
/
<x, x>.
(2.27) 
Any function || · || deﬁned in this way satisﬁes the properties of a norm. It is 
easy to see that ||x|| satisﬁes the ﬁrst two properties of a norm, nonnegativity 
and scalar equivariance. Now, consider the square of the right-hand side of 
the triangle inequality, ||x|| + ||y||: 
StartLayout 1st Row 1st Column left parenthesis parallel to x parallel to plus parallel to y parallel to right parenthesis squared 2nd Column equals 3rd Column left angle bracket x comma x right angle bracket plus 2 StartRoot left angle bracket x comma x right angle bracket left angle bracket y comma y right angle bracket EndRoot plus left angle bracket y comma y right angle bracket 2nd Row 1st Column Blank 2nd Column greater than or equals 3rd Column left angle bracket x comma x right angle bracket plus 2 left angle bracket x comma y right angle bracket plus left angle bracket y comma y right angle bracket 3rd Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x plus y comma x plus y right angle bracket 4th Row 1st Column Blank 2nd Column equals 3rd Column parallel to x plus y parallel to squared semicolon EndLayout(||x|| + ||y||)2 = <x, x> + 2
/
<x, x><y, y> + <y, y>
≥<x, x> + 2<x, y> + <y, y>
= <x + y, x + y>
= ||x + y||2;
(2.28) 
hence, the triangle inequality holds. Therefore, given an inner product, <x, y>, 
then
/
<x, x> is a norm. 
Equation (2.27) deﬁnes a norm given any inner product. It is called the 
norm induced by the inner product. 
Norms induced by inner products have some interesting properties. First of 
all, they have the Cauchy–Schwarz relationship (inequality (2.23)) with their 
associated inner product: 
StartAbsoluteValue left angle bracket x comma y right angle bracket EndAbsoluteValue less than or equals parallel to x parallel to parallel to y parallel to period|<x, y>| ≤||x||||y||.
(2.29) 
In the sequence of equations above for an induced norm of the sum of two 
vectors, one equation (expressed diﬀerently) stands out as particularly useful 
in later applications: 
parallel to x plus y parallel to squared equals parallel to x parallel to squared plus parallel to y parallel to squared plus 2 left angle bracket x comma y right angle bracket period||x + y||2 = ||x||2 + ||y||2 + 2<x, y>.
(2.30) 
If <x, y> = 0 (that is, the vectors are orthogonal), Eq. (2.30) becomes the 
Pythagorean theorem: 
parallel to x plus y parallel to squared equals parallel to x parallel to squared plus parallel to y parallel to squared period||x + y||2 = ||x||2 + ||y||2.
Another useful property of a norm induced by an inner product is the 
parallelogram equality: 
2 parallel to x parallel to squared plus 2 parallel to y parallel to squared equals parallel to x plus y parallel to squared plus parallel to x minus y parallel to squared period2||x||2 + 2||y||2 = ||x + y||2 + ||x −y||2.
(2.31) 
This is trivial to show, and you are asked to do so in Exercise 2.8. (It  is  also  
the case that if the parallelogram equality holds for every pair of vectors in 
the space, then the norm is necessarily induced by an inner product. This fact 
is both harder to show and less useful than its converse; I state it only because 
it is somewhat surprising.)

2.1 Operations on Vectors
37
A vector space whose norm is induced by an inner product has an inter-
esting structure; for example, the geometric properties such as projections, 
orthogonality, and angles between vectors that we discuss in Sect. 2.2 are de-
ﬁned in terms of inner products and the associated norm. 
For real vectors, we have deﬁned “the” inner product. We will deﬁne var-
ious norms for real vectors, but we will use the unqualiﬁed notation || · || to 
refer to the object in Eq. (2.27), with
/
<·, ·> as in Eq. (2.24). 
Lp Norms 
There are many norms that could be deﬁned for vectors. One type of norm is 
called an Lp norm, often denoted as || · ||p. For  p ≥ 1, it is deﬁned as 
parallel to x parallel to Subscript p Baseline equals left parenthesis sigma summation Underscript i Endscripts StartAbsoluteValue x Subscript i Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript StartFraction 1 Over p EndFraction Baseline period||x||p =
(E
i
|xi|p
) 1
p
.
(2.32) 
This is also sometimes called the Minkowski norm and also the H¨older norm. 
An Lp norm is also called a p-norm, or 1-norm, 2-norm, or ∞-norm (deﬁned 
by a limit) in those special cases. 
It is easy to see that the Lp norm satisﬁes the ﬁrst two conditions above. For 
general p ≥ 1, it is somewhat more diﬃcult to prove the triangular inequality 
(which for the Lp norms is also called the Minkowski inequality), but for some 
special cases it is straightforward, as we will see below. 
The most common Lp norms, and in fact the most commonly used vector 
norms, are: 
•
||x||1 = E
i |xi|, also called the Manhattan norm because it corresponds 
to sums of distances along coordinate axes, as one would travel along the 
rectangular street plan of Manhattan (except for Broadway and a few 
other streets and avenues). 
•
||x||2 =
/E
i x2 
i , also called the Euclidean norm, the  Euclidean length, 
or just the length of the vector. The L2 norm is induced by an inner 
product; it is the square root of the inner product of the vector with itself:
||x||2 =
/
<x, x>. It is the  only  Lp norm induced by an inner product. (See 
Exercise 2.10.) 
•
||x||∞ = maxi |xi|, also called the max norm or the Chebyshev norm. The  
L∞ norm is deﬁned by taking the limit in an Lp norm, and we see that it 
is indeed maxi |xi| by expressing it as 
parallel to x parallel to Subscript normal infinity Baseline equals limit Underscript p right arrow normal infinity Endscripts parallel to x parallel to Subscript p Baseline equals limit Underscript p right arrow normal infinity Endscripts left parenthesis sigma summation Underscript i Endscripts StartAbsoluteValue x Subscript i Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript StartFraction 1 Over p EndFraction Baseline equals m limit Underscript p right arrow normal infinity Endscripts left parenthesis sigma summation Underscript i Endscripts StartAbsoluteValue StartFraction x Subscript i Baseline Over m EndFraction EndAbsoluteValue Superscript p Baseline right parenthesis Superscript StartFraction 1 Over p EndFraction||x||∞= lim
p→∞||x||p = lim
p→∞
(E
i
|xi|p
) 1
p
= m lim
p→∞
(E
i
|||xi
m
|||
p
) 1
p
with m = maxi |xi|. Because the quantity of which we are taking the pth 
root is bounded above by the number of elements in x and below by 1, 
that factor goes to 1 as p goes to ∞.

38
2 Vectors and Vector Spaces
The Euclidean norm of a vector corresponds to the length of the vector in a 
natural way; that is, it agrees with our intuition regarding “length.” Although 
this is just one of many vector norms, in most applications it is the most 
useful one. (I must warn you, however, that occasionally I will carelessly but 
naturally use “length” to refer to the order of a vector; that is, the number of 
its elements. This usage is common in computer software packages such as R 
and SAS IML, and software necessarily shapes our vocabulary.) 
It is easy to see that, for any n-vector x, the  Lp norms have the relation-
ships 
parallel to x parallel to Subscript normal infinity Baseline less than or equals parallel to x parallel to Subscript 2 Baseline less than or equals parallel to x parallel to Subscript 1 Baseline period||x||∞≤||x||2 ≤||x||1.
(2.33) 
More generally, for given x and for p ≥ 1, we see that ||x||p is a nonincreasing 
function of p. 
We also have bounds that involve the number of elements in the vector: 
parallel to x parallel to Subscript normal infinity Baseline less than or equals parallel to x parallel to Subscript 2 Baseline less than or equals StartRoot n EndRoot parallel to x parallel to Subscript normal infinity Baseline comma||x||∞≤||x||2 ≤√n||x||∞,
(2.34) 
and 
parallel to x parallel to Subscript 2 Baseline less than or equals parallel to x parallel to Subscript 1 Baseline less than or equals StartRoot n EndRoot parallel to x parallel to Subscript 2 Baseline period||x||2 ≤||x||1 ≤√n||x||2.
(2.35) 
The triangle inequality obviously holds for the L1 and L∞ norms. For the 
L2 norm it can be seen by expanding E(xi +yi)2 and then using the Cauchy-
Schwarz inequality (2.23) on page 33. Rather than approaching it that way, 
however, we have shown that the L2 norm can be deﬁned in terms of an inner 
product, and then we established the triangle inequality for any norm deﬁned 
similarly by an inner product; see inequality (2.28). Showing that the triangle 
inequality holds for other Lp norms is more diﬃcult; see Exercise 2.12. 
A generalization of the Lp vector norm is the weighted Lp vector norm 
deﬁned by 
parallel to x parallel to Subscript w p Baseline equals left parenthesis sigma summation Underscript i Endscripts w Subscript i Baseline StartAbsoluteValue x Subscript i Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript StartFraction 1 Over p EndFraction Baseline comma||x||wp =
(E
i
wi|xi|p
) 1
p
,
(2.36) 
where wi ≥ 0 and E
i wi = 1.  
In the following, if we use the unqualiﬁed symbol || · || for a vector norm 
and do not state otherwise, we will mean the L2 norm; that is, the Euclidean 
norm, the induced norm. 
Basis Norms 
If {v1, . . . , vk} is a basis for a vector space that includes a vector x with 
x = c1v1 + · · · + ckvk, then  
rho left parenthesis x right parenthesis equals left parenthesis sigma summation Underscript i Endscripts c Subscript i Superscript 2 Baseline right parenthesis Superscript one halfρ(x) =
(E
i
c2
i
) 1
2
(2.37) 
is a norm. It is straightforward to see that ρ(x) is a norm by checking the 
following three conditions:

2.1 Operations on Vectors
39
• ρ(x) ≥ 0 and  ρ(x) = 0 if and only if x = 0 because x = 0  if  and  only  if  
ci = 0 for all i. 
• ρ(ax) =
(E
i a2 c2 
i
) 1 
2 = |a|
(E
i c2 
i
) 1 
2 = |a|ρ(x). 
• If also y = b1v1 + · · ·  + bkvk, then  
rho left parenthesis x plus y right parenthesis equals left parenthesis sigma summation Underscript i Endscripts left parenthesis c Subscript i Baseline plus b Subscript i Baseline right parenthesis squared right parenthesis Superscript one half Baseline less than or equals left parenthesis sigma summation Underscript i Endscripts c Subscript i Superscript 2 Baseline right parenthesis Superscript one half Baseline plus left parenthesis sigma summation Underscript i Endscripts b Subscript i Superscript 2 Baseline right parenthesis Superscript one half Baseline equals rho left parenthesis x right parenthesis plus rho left parenthesis y right parenthesis periodρ(x + y) =
(E
i
(ci + bi)2
) 1
2
≤
(E
i
c2
i
) 1
2
+
(E
i
b2
i
) 1
2
= ρ(x) + ρ(y).
The last inequality is just the triangle inequality for the L2 norm for the 
vectors (c1, · · ·  , ck) and  (b1, · · ·  , bk). 
In Sect. 2.2.5, we will consider special forms of basis sets in which the norm 
in Eq. (2.37) is identically the L2 norm. (This is called Parseval’s identity, 
Eq. (2.59) on page 51.) 
Equivalence of Norms 
There is an equivalence among any two norms over a normed ﬁnite-dimensional 
linear space in the sense that if ||·||a and ||·||b are norms, then there are positive 
numbers r and s such that for any x in the space, 
r parallel to x parallel to Subscript b Baseline less than or equals parallel to x parallel to Subscript a Baseline less than or equals s parallel to x parallel to Subscript b Baseline periodr||x||b ≤||x||a ≤s||x||b.
(2.38) 
Expressions (2.34) and  (2.35) are examples of this general equivalence for 
three Lp norms. 
We can prove inequality (2.38) by using the norm deﬁned in Eq. (2.37). 
We need only consider the case x /= 0, because the inequality is obviously 
true if x = 0.  Let || · ||a be any norm over a given normed linear space and let 
{v1, . . . , vk} be a basis for the space. (Here’s where the assumption of a vector 
space with ﬁnite dimensions comes in.) Any x in the space has a representation 
in terms of the basis, x = c1v1 + · · ·  + ckvk. Then  
parallel to x parallel to Subscript a Baseline equals double vertical bar sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline v Subscript i Baseline double vertical bar Subscript a Baseline less than or equals sigma summation Underscript i equals 1 Overscript k Endscripts StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue parallel to v Subscript i Baseline parallel to Subscript a Baseline period||x||a =
||||||||||
k
E
i=1
civi
||||||||||
a
≤
k
E
i=1
|ci| ||vi||a.
Applying the Cauchy–Schwarz inequality to the two vectors (c1, · · ·  , ck) and  
(||v1||a, · · ·  , ||vk||a), we have 
sigma summation Underscript i equals 1 Overscript k Endscripts StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue parallel to v Subscript i Baseline parallel to Subscript a Baseline less than or equals left parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 Baseline right parenthesis Superscript one half Baseline left parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts parallel to v Subscript i Baseline parallel to Subscript a Superscript 2 Baseline right parenthesis Superscript one half Baseline period
k
E
i=1
|ci| ||vi||a ≤
( k
E
i=1
c2
i
) 1
2 ( k
E
i=1
||vi||2
a
) 1
2
.
Hence, with ˜s = (E
i ||vi||2 
a) 
1 
2 , which must be positive, we have 
parallel to x parallel to Subscript a Baseline less than or equals s overTilde rho left parenthesis x right parenthesis period||x||a ≤˜sρ(x).

40
2 Vectors and Vector Spaces
Now, to establish a lower bound for ||x||a, let us deﬁne a subset C of the 
linear space consisting of all vectors (u1, . . . , uk) such that E |ui|2 = 1.  This  
set is obviously closed. Next, we deﬁne a function f(·) over this closed subset 
by 
f left parenthesis u right parenthesis equals double vertical bar sigma summation Underscript i equals 1 Overscript k Endscripts u Subscript i Baseline v Subscript i Baseline double vertical bar Subscript a Baseline periodf(u) =
||||||||||
k
E
i=1
uivi
||||||||||
a
.
Because f is continuous, it attains a minimum in this closed subset, say for 
the vector u∗; that is,  f(u∗) ≤ f(u) for any u such that E |ui|2 = 1.  Let  
r overTilde equals f left parenthesis u Subscript asterisk Baseline right parenthesis comma˜r = f(u∗),
which must be positive, and again consider any x in the normed linear space 
and express it in terms of the basis, x = c1v1 + · · · ckvk. If  x /= 0,  we  have  
StartLayout 1st Row 1st Column parallel to x parallel to Subscript a 2nd Column equals 3rd Column double vertical bar sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline v Subscript i Baseline double vertical bar Subscript a 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 Baseline right parenthesis Superscript one half Baseline double vertical bar sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis StartFraction c Subscript i Baseline Over left parenthesis sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 Baseline right parenthesis Superscript one half Baseline EndFraction right parenthesis v Subscript i Baseline double vertical bar Subscript a 3rd Row 1st Column Blank 2nd Column equals 3rd Column rho left parenthesis x right parenthesis f left parenthesis c overTilde right parenthesis comma EndLayout||x||a =
||||||||||
k
E
i=1
civi
||||||||||
a
=
( k
E
i=1
c2
i
) 1
2
||||||||||||||
k
E
i=1
⎛
⎜
⎝
ci
(Ek
i=1 c2
i
) 1
2
⎞
⎟
⎠vi
||||||||||||||
a
= ρ(x)f(˜c),
where ˜c = (c1, · · ·  , ck)/(Ek 
i=1 c2 
i )1/2 . Because ˜c is in the set C, f(˜c) ≥ r; 
hence, combining this with the inequality above, we have 
r overTilde rho left parenthesis x right parenthesis less than or equals parallel to x parallel to Subscript a Baseline less than or equals s overTilde rho left parenthesis x right parenthesis period˜rρ(x) ≤||x||a ≤˜sρ(x).
This expression holds for any norm ||·||a and so, after obtaining similar bounds 
for any  other norm ||·||b and then combining the inequalities for ||·||a and ||·||b, 
we have the bounds in the equivalence relation (2.38). (This is an equivalence 
relation because it is reﬂexive, symmetric, and transitive. Its transitivity is 
seen by the same argument that allowed us to go from the inequalities involv-
ing ρ(·) to ones involving || · ||b.) 
As we have mentioned, there are some diﬀerences in the properties of 
vector spaces that have an inﬁnite number of dimensions and those with ﬁnite 
dimensions. The equivalence of norms is one of those diﬀerences. The argument 
above fails in the properties of the continuous function f. (Recall, however, 
as we have mentioned, unless we state otherwise, we assume that the vector 
spaces we discuss have ﬁnite dimensions.) 
2.1.6 Normalized Vectors 
Dividing a given vector by its norm normalizes the vector, and the resulting 
vector whose norm is 1 is said to be normalized; thus,

2.1 Operations on Vectors
41
x overTilde equals StartFraction 1 Over parallel to x parallel to EndFraction x˜x =
1
||x||x
(2.39) 
is a normalized vector. The most common norm is the L2 norm or Euclidean 
norm, and a vector normalized by that norm has a Euclidean length of 1. 
Normalized vectors are sometimes referred to as “unit vectors,” although 
we will generally reserve this term for a special kind of normalized vector 
(see page 24). A normalized vector is also sometimes referred to as a “normal 
vector.” I use “normalized vector” for a vector such as ˜x in Eq. (2.39) and use 
“normal vector” to denote a vector that is orthogonal to a subspace (as on 
page 44). 
“Inverse” of a Vector 
Because the mapping of an inner product takes the elements of one space into 
a diﬀerent space (the inner product of vectors takes elements of IRn into IR), 
the concept of an inverse for the inner product does not make sense in the 
usual way. First of all, there is no identity with respect to the inner product. 
Often in applications, however, inner products are combined with the usual 
scalar–vector multiplication in the form <x, y>z; therefore, for given x, it may  
be of interest to determine y such that <x, y> = 1, the multiplicative identity 
in IR. For x in IRn such that x /= 0, the additive identity in IRn , 
y equals StartFraction 1 Over parallel to x parallel to squared EndFraction x equals StartFraction 1 Over parallel to x parallel to EndFraction x overTildey =
1
||x||2 x =
1
||x|| ˜x
(2.40) 
uniquely satisﬁes <x, y> = 1. Such a y is called the Samelson inverse of the vec-
tor x and is sometimes denoted as x−1 or as [x]−1 . It is also sometimes called 
the Moore–Penrose vector inverse because it satisﬁes the four properties of the 
deﬁnition of the Moore–Penrose inverse. (See page 151, where, for example, 
the ﬁrst property is interpreted both as <x, [x]−1>x and as x<[x]−1 , x>.) 
The norm in Eq. (2.40) is obviously the Euclidean norm (because of the 
way we deﬁned ˜x), but the idea of the inverse could also be extended to other 
norms associated with other inner products. 
The Samelson inverse has a nice geometric interpretation: it is the inverse 
point of x with respect to the unit sphere in IRn . This inverse arises in the 
vector e-algorithm used in accelerating convergence of vector sequences in 
numerical computations (see Wynn 1962). 
2.1.7 Metrics and Distances 
It is often useful to consider how far apart two objects are; that is, the “dis-
tance” between them. A reasonable distance measure would have to satisfy 
certain requirements, such as being a nonnegative real number. 
A function Δ that maps any two objects in a set S to IR is called a metric 
on S if, for all x, y, and  z in S, it satisﬁes the following three conditions:

42
2 Vectors and Vector Spaces
1. Δ(x, y) > 0 if  x /= y and Δ(x, y) = 0  if  x = y; 
2. Δ(x, y) = Δ(y, x); 
3. Δ(x, y) ≤ Δ(x, z) + Δ(z, y). 
These conditions correspond in an intuitive manner to the properties we ex-
pect of a distance between objects. 
A vector space together with a metric deﬁned on it is called a metric 
space. A normed vector space is a metric space because the norm can induce 
a metric. In the following, we may speak almost interchangeably of an inner 
product space, a normed space, or a metric space, but we must recognize that 
none is a special case of another. (Recall that a normed space whose norm is 
the L1 norm is not equivalent to an inner product space, for example.) 
Metrics Induced by Norms 
If subtraction and a norm are deﬁned for the elements of S, the most common 
way of forming a metric is by using the norm. If || · || is a norm, we can verify 
that 
upper Delta left parenthesis x comma y right parenthesis equals parallel to x minus y parallel toΔ(x, y) = ||x −y||
(2.41) 
is a metric by using the properties of a norm to establish the three properties 
of a metric above (Exercise 2.13). 
The norm in Eq. (2.41) may, of course, be induced by an inner product. 
The general inner products, norms, and metrics deﬁned above are relevant 
in a wide range of applications. The sets on which they are deﬁned can consist 
of various types of objects. In the context of real vectors, the most common 
inner product is the dot product; the most common norm is the Euclidean 
norm that arises from the dot product; and the most common metric is the 
one deﬁned by the Euclidean norm, called the Euclidean distance. 
Convergence of Sequences of Vectors 
A sequence of real numbers a1, a2, . . .  is said to converge to a ﬁnite number a 
if for any given e >  0 there is an integer M such that, for k >  M, |ak −a| < e, 
and we write limk→∞ ak = a, or we write  ak → a as k →∞. 
We deﬁne convergence of a sequence of vectors in a normed vector space in 
terms of the convergence of a sequence of their norms, which is a sequence of 
real numbers. We say that a sequence of vectors x1, x2, . . .  (of the same order) 
converges to the vector x with respect to the norm || · || if the sequence of real 
numbers ||x1 − x||, ||x2 − x||, . . .  converges to 0. Because of the bounds (2.38), 
the choice of the norm is irrelevant (for ﬁnite dimensional vector spaces), and 
so convergence of a sequence of vectors is well deﬁned without reference to a 
speciﬁc norm. (This is one reason that equivalence of norms is an important 
property.) 
A sequence of vectors x1, x2, . . .  in the metric space V that come arbitrarily 
close to one another (as measured by the given metric) is called a Cauchy

2.1 Operations on Vectors
43
sequence. (In a Cauchy sequence x1, x2, . . ., for any e >  0 there is a number 
N such that for i, j > N, Δ(xi, xj) < e.) Intuitively, such a sequence should 
converge to some ﬁxed vector in V, but this is not necessarily the case. A 
metric space in which every Cauchy sequence converges to an element in the 
space is said to be a  complete metric space or just a complete space. The  space  
IRn (with any norm) is complete. 
A complete normed space is called a Banach space, and a complete inner 
product space is called a Hilbert space. It is clear that a Hilbert space is a 
Banach space (because its inner product induces a norm). As we have indi-
cated, a space with a norm induced by an inner product, such as a Hilbert 
space, has an interesting structure. Most of the vector spaces encountered in 
statistical applications are Hilbert spaces. The space IRn with the L2 norm is 
a Hilbert space. 
2.1.8 Orthogonal Vectors and Orthogonal Vector Spaces 
Two vectors v1 and v2 such that 
left angle bracket v 1 comma v 2 right angle bracket equals 0<v1, v2> = 0
(2.42) 
are said to be orthogonal, and this condition is denoted by v1 ⊥ v2. (Sometimes 
we exclude the zero vector from this deﬁnition, but it is not important to 
do so.) Normalized vectors that are all orthogonal to each other are called 
orthonormal vectors. 
An Aside: Complex Vectors 
If the elements of the vectors are from the ﬁeld of complex numbers, 
orthogonality and normality are also deﬁned as above; however, the 
inner product in the deﬁnition (2.42) must be deﬁned as E
i ¯xiyi, and  
the expression xT y in Eq. (2.25) is not equivalent to the inner product. 
We will use a diﬀerent notation in this case: xH y. The relationship 
between the two notations is 
x Superscript normal upper H Baseline y equals x overbar Superscript normal upper T Baseline y periodxHy = ¯xTy.
With this interpretation of the inner product, all of the statements 
below about orthogonality hold for complex numbers as well as for 
real numbers. 
A set of nonzero vectors that are mutually orthogonal are necessarily lin-
early independent. To see this, we show it for any two orthogonal vectors and 
then indicate the pattern that extends to three or more vectors. First, sup-
pose v1 and v2 are nonzero and are orthogonal; that is, <v1, v2> = 0.  We  see  
immediately that if there is a scalar a such that v1 = av2, then  a must be 
nonzero, and we have a contradiction because <v1, v2> = a<v2, v2> /= 0. Hence, 
we conclude v1 and v2 are independent (there is no a such that v1 = av2). For

44
2 Vectors and Vector Spaces
three mutually orthogonal vectors, v1, v2, and  v3, we consider  v1 = av2 + bv3 
for a or b nonzero and arrive at the same contradiction. 
Two vector spaces V1 and V2 are said to be orthogonal, written V1 ⊥V2, 
if each vector in one is orthogonal to every vector in the other. If V1 ⊥V2 and 
V1 ⊕V2 = IRn , then  V2 is called the orthogonal complement of V1, and this is 
written as V2 = V⊥ 
1 . More generally, if V1 ⊥V2 and V1 ⊕V2 = V, then  V2 is 
called the orthogonal complement of V1 with respect to V. This is obviously 
a symmetric relationship; if V2 is the orthogonal complement of V1, then  V1 
is the orthogonal complement of V2. 
A vector that is orthogonal to all vectors in a given vector space is said to 
be orthogonal to that space or normal to that space. Such a vector is called a 
normal vector to that space. 
If B1 is a basis set for V1, B2 is a basis set for V2, and  V2 is the orthogonal 
complement of V1 with respect to V, then  B1 ∪ B2 is a basis set for V. It is  
a basis set because since V1 and V2 are orthogonal, it must be the case that 
B1 ∩B2 = ∅. (See the properties listed on page 32.) 
If V1 ⊂V, V2 ⊂V, V1 ⊥V2, and dim(V1) + dim(V2) = dim(V), then 
script upper V 1 circled plus script upper V 2 equals script upper V semicolonV1 ⊕V2 = V;
(2.43) 
that is, V2 is the orthogonal complement of V1. We see this by ﬁrst letting B1 
and B2 be bases for V1 and V2. Now  V1 ⊥V2 implies that B1 ∩ B2 = ∅ and 
dim(V1) + dim(V2) = dim(V) implies #(B1) + #(B2) = #(B), for any basis 
set B for V; hence, B1 ∪ B2 is a basis set for V. 
The intersection of two orthogonal vector spaces consists only of the zero 
vector (Exercise 2.15). 
A set of linearly independent vectors can be mapped to a set of mutu-
ally orthogonal (and orthonormal) vectors by means of the Gram-Schmidt 
transformations (see Sect. 2.2.4 and Eq. (2.55) below). 
2.1.9 The “One Vector” 
The vector with all elements equal to 1 that we mentioned previously is useful 
in various vector operations. We call this the “one vector” and denote it by 1 
or by 1n. The one vector can be used in the representation of the sum of the 
elements in a vector: 
1 Superscript normal upper T Baseline x equals sigma summation x Subscript i Baseline period1Tx =
E
xi.
(2.44) 
The one vector is also called the “summing vector.” 
The Mean and the Mean Vector 
Because the elements of x are real, they can be summed; however, in applica-
tions it may or may not make sense to add the elements in a vector, depending 
on what is represented by those elements. If the elements have some kind of

2.2 Cartesian Geometry
45
essential commonality, it may make sense to compute their sum as well as 
their arithmetic mean, which for the n-vector x is denoted by ¯x and deﬁned 
by 
x overbar equals 1 Subscript n Superscript normal upper T Baseline x divided by n period¯x = 1T
nx/n.
(2.45) 
We also refer to the arithmetic mean as just the “mean” because it is the most 
commonly used mean. 
It is often useful to think of the mean as an n-vector all of whose elements 
are ¯x. The  symbol  ¯x is also used to denote this vector; hence, we have 
x overbar equals x overbar 1 Subscript n Baseline comma¯x = ¯x1n,
(2.46) 
in which ¯x on the left-hand side is a vector and ¯x on the right-hand side is a 
scalar. We also have, for the two diﬀerent objects, 
parallel to x overbar parallel to squared equals n x overbar squared period||¯x||2 = n¯x2.
(2.47) 
The meaning, whether a scalar or a vector, is usually clear from the con-
text. In any event, an expression such as x − ¯x is unambiguous; the addition 
(subtraction) has the same meaning whether ¯x is interpreted as a vector or a 
scalar. (In some mathematical treatments of vectors, addition of a scalar to 
a vector is not deﬁned, but here we are following the conventions of modern 
computer languages.) 
2.2 Cartesian Coordinates and Geometrical 
Properties of Vectors 
Points in a Cartesian geometry can be identiﬁed with vectors, and several 
deﬁnitions and properties of vectors can be motivated by this geometric in-
terpretation. In this interpretation, vectors are directed line segments with a 
common origin. The geometrical properties can be seen most easily in terms 
of a Cartesian coordinate system, but the properties of vectors deﬁned in 
terms of a Cartesian geometry have analogues in Euclidean geometry without 
a coordinate system. In such a system, only length and direction are deﬁned, 
and two vectors are considered to be the same vector if they have the same 
length and direction. Generally, we will not assume that there is a “location” 
or “position” associated with a vector. 
2.2.1 Cartesian Geometry 
A Cartesian coordinate system in d dimensions is deﬁned by d unit vectors, ei 
in Eq. (2.6), each with d elements. A unit vector is also called a principal axis 
of the coordinate system. The set of unit vectors is orthonormal. (There is an 
implied number of elements of a unit vector that is inferred from the context. 
Also parenthetically, we remark that the phrase “unit vector” is sometimes

46
2 Vectors and Vector Spaces
used to refer to a vector the sum of whose squared elements is 1, that is, whose 
length, in the Euclidean distance sense, is 1. As we mentioned above, we refer 
to this latter type of vector as a “normalized vector.”) 
The sum of all of the unit vectors is the one vector: 
sigma summation Underscript i equals 1 Overscript d Endscripts e Subscript i Baseline equals 1 Subscript d Baseline period
d
E
i=1
ei = 1d.
(2.48) 
A point  x with Cartesian coordinates (x1, . . . , xd) is associated with a 
vector from the origin to the point, that is, the vector (x1, . . . , xd). The vector 
can be written as the linear combination 
x equals x 1 e 1 plus ellipsis plus x Subscript d Baseline e Subscript dx = x1e1 + . . . + xded
(2.49) 
or, equivalently, as 
x equals left angle bracket x comma e 1 right angle bracket e 1 plus ellipsis plus left angle bracket x comma e Subscript d Baseline right angle bracket e Subscript d Baseline periodx = <x, e1>e1 + . . . + <x, ed>ed.
(This is a Fourier expansion, Eq. (2.57) below.)  
2.2.2 Projections 
The projection of the vector y onto the nonnull vector x is the vector 
ModifyingAbove y With caret equals StartFraction left angle bracket x comma y right angle bracket Over parallel to x parallel to squared EndFraction x periodˆy = <x, y>
||x||2 x.
(2.50) 
This deﬁnition is consistent with a geometrical interpretation of vectors as 
directed line segments with a common origin. The projection of y onto x is 
the inner product of the normalized x and y times the normalized x; that is,
<˜x, y>˜x, where  ˜x = x/||x||. Notice that the order of y and x is the same. 
An important property of a projection is that when it is subtracted from 
the vector that was projected, the resulting vector, called the “residual”, is 
orthogonal to the projection; that is, if 
StartLayout 1st Row 1st Column r 2nd Column equals 3rd Column y minus StartFraction left angle bracket x comma y right angle bracket Over parallel to x parallel to squared EndFraction x 2nd Row 1st Column Blank 2nd Column equals 3rd Column y minus ModifyingAbove y With caret EndLayoutr = y −<x, y>
||x||2 x
= y −ˆy
(2.51) 
then r and ˆy are orthogonal, as we can easily see by taking their inner product 
(see Fig. 2.1). Notice also that the Pythagorean relationship holds: 
parallel to y parallel to squared equals parallel to ModifyingAbove y With caret parallel to squared plus parallel to r parallel to squared period||y||2 = ||ˆy||2 + ||r||2.
(2.52)

2.2 Cartesian Geometry
47
Figure 2.1. Projections and Angles 
As we mentioned on page 45, the  mean  ¯y can be interpreted either as a 
scalar or as a vector all of whose elements are ¯y. As a vector, it is the projection 
of y onto the one vector 1n, 
StartLayout 1st Row 1st Column StartFraction left angle bracket 1 Subscript n Baseline comma y right angle bracket Over parallel to 1 Subscript n Baseline parallel to squared EndFraction 1 Subscript n 2nd Column equals 3rd Column StartFraction 1 Subscript n Superscript normal upper T Baseline y Over n EndFraction 1 Subscript n 2nd Row 1st Column Blank 2nd Column equals 3rd Column y overbar 1 Subscript n Baseline comma EndLayout<1n, y>
||1n||2 1n = 1T
ny
n
1n
= ¯y 1n,
from equations (2.45) and  (2.50). 
We will consider more general projections (that is, projections onto planes 
or other subspaces) on page 391, and on page 450 we will view linear regression 
ﬁtting as a projection onto the space spanned by the independent variables. 
2.2.3 Angles Between Vectors 
The angle between the nonnull vectors x and y is determined by its cosine, 
which we can compute from the length of the projection of one vector onto 
the other. Hence, denoting the angle between the nonnull vectors x and y as 
angle(x, y), we deﬁne 
normal a normal n normal g normal l normal e left parenthesis x comma y right parenthesis equals cosine Superscript negative 1 Baseline left parenthesis StartFraction left angle bracket x comma y right angle bracket Over parallel to x parallel to parallel to y parallel to EndFraction right parenthesis commaangle(x, y) = cos−1
( <x, y>
||x||||y||
)
,
(2.53) 
with cos−1 (·) being taken in the interval [0, π).  The cosine is  ±||ˆy||/||y||, with  
the sign chosen appropriately; see Fig. 2.1. Because of this choice of cos−1 (·), 
we have that angle(y, x) = angle(x, y) — but see Exercise 2.21e on page 69; 
in some applications it is useful to give a “direction” to an angle, and in that 
case, the range of an angle is [0, 2π). 
The word “orthogonal” is appropriately deﬁned by Eq. (2.42) on page 43 
because orthogonality in that sense is equivalent to the corresponding geo-
metric property. (The cosine is 0.)

48
2 Vectors and Vector Spaces
Notice that the angle between two vectors is invariant to scaling of the 
vectors; that is, for any positive scalar a, angle(ax, y) = angle(x, y). 
A given vector can be deﬁned in terms of its length and the angles θi that 
it makes with the unit vectors. The cosines of these angles are just the scaled 
coordinates of the vector: 
StartLayout 1st Row 1st Column cosine left parenthesis theta Subscript i Baseline right parenthesis 2nd Column equals 3rd Column StartFraction left angle bracket x comma e Subscript i Baseline right angle bracket Over parallel to x parallel to parallel to e Subscript i Baseline parallel to EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction 1 Over parallel to x parallel to EndFraction x Subscript i Baseline period EndLayoutcos(θi) =
<x, ei>
||x||||ei||
=
1
||x|| xi.
(2.54) 
These quantities are called the direction cosines of the vector. 
Although geometrical intuition often helps us in understanding properties 
of vectors, sometimes it may lead us astray in high dimensions. Consider the 
direction cosines of an arbitrary vector in a vector space with large dimensions. 
If the elements of the arbitrary vector are nearly equal (that is, if the vector is 
a diagonal through an orthant of the coordinate system), the direction cosine 
goes to 0 as the dimension increases. In high dimensions, any two vectors are 
“almost orthogonal” to each other; see Exercise 2.17. 
The geometric property of the angle between vectors has important im-
plications for certain operations both because it may indicate that rounding 
in computations will have deleterious eﬀects and because it may indicate a 
deﬁciency in the understanding of the application. 
We will consider more general projections and angles between vectors and 
other subspaces on page 397. In Sect. 4.1.3, we will consider rotations of vectors 
onto other vectors or subspaces. Rotations are similar to projections, except 
that the length of the vector being rotated is preserved. 
2.2.4 Orthogonalization Transformations: Gram–Schmidt 
Given m nonnull, linearly independent vectors, x1, . . . , xm, it is easy to form 
m orthonormal vectors, ˜x1, . . . ,  ˜xm, that span the same space. A simple way 
to do this is sequentially. First normalize x1 and call this ˜x1. Next, project x2 
onto ˜x1 and subtract this projection from x2. The result is orthogonal to ˜x1; 
hence, normalize this and call it ˜x2. These ﬁrst two steps, which are illustrated 
in Fig. 2.2, are  
StartLayout 1st Row 1st Column x overTilde Subscript 1 2nd Column equals 3rd Column StartFraction 1 Over parallel to x 1 parallel to EndFraction x 1 comma 2nd Row 1st Column Blank 3rd Row 1st Column x overTilde Subscript 2 2nd Column equals 3rd Column StartFraction 1 Over parallel to x 2 minus left angle bracket x overTilde Subscript 1 Baseline comma x 2 right angle bracket x overTilde Subscript 1 Baseline parallel to EndFraction left parenthesis x 2 minus left angle bracket x overTilde Subscript 1 Baseline comma x 2 right angle bracket x overTilde Subscript 1 Baseline right parenthesis period EndLayout
˜x1 =
1
||x1|| x1,
˜x2 =
1
||x2 −<˜x1, x2>˜x1|| (x2 −<˜x1, x2>˜x1).
(2.55) 
These are called Gram–Schmidt transformations.

2.2 Cartesian Geometry
49
The Gram–Schmidt transformations have a close relationship to least 
squares ﬁtting of overdetermined systems of linear equations and to least 
squares ﬁtting of linear regression models. For example, if Eq. (5.33) on  
page 284 is merely the system of equations in one unknown x1b = x2 − r, 
and it is approximated by least squares, then the “residual vector” r is ˜x2 
above, and of course it has the orthogonality property shown in Eq. (5.37) for  
that problem. 
Figure 2.2. Orthogonalization of x1 and x2 
The Gram–Schmidt transformations can be continued with all of the vec-
tors in the linearly independent set. There are two straightforward ways equa-
tions (2.55) can be extended. One method generalizes the second equation in 
an obvious way: 
StartLayout 1st Row 1st Column normal f normal o normal r k equals 2 comma 3 ellipsis comma 2nd Row 1st Column Blank 2nd Column x overTilde Subscript k Baseline equals left parenthesis x Subscript k Baseline minus sigma summation Underscript i equals 1 Overscript k minus 1 Endscripts left angle bracket x overTilde Subscript i Baseline comma x Subscript k Baseline right angle bracket x overTilde Subscript i Baseline right parenthesis slash double vertical bar x Subscript k Baseline minus sigma summation Underscript i equals 1 Overscript k minus 1 Endscripts left angle bracket x overTilde Subscript i Baseline comma x Subscript k Baseline right angle bracket x overTilde Subscript i Baseline double vertical bar period EndLayoutfor k = 2, 3 . . . ,
˜xk =
(
xk −
k−1
E
i=1
<˜xi, xk>˜xi
) / ||||||||||xk −
k−1
E
i=1
<˜xi, xk>˜xi
|||||||||| .
(2.56) 
In this method, at the kth step, we orthogonalize the kth vector by comput-
ing its residual with respect to the plane formed by all the previous k −1 
orthonormal vectors. 
Another way of extending the transformation of equations (2.55) after 
normalizing the ﬁrst vector, for k = 2, . . ., at the  kth step, is to compute the 
residuals of all k = 2, . . .  vectors with respect just to the (k −1)st normalized 
vector and then to normalize the kth vector. If the initial set of vectors are 
linearly independent, the residuals at any stage will be nonzero. (This is fairly 
obvious, but you are asked to show it in Exercise 2.18.) We describe this 
method explicitly in Algorithm 2.1.

50
2 Vectors and Vector Spaces
Algorithm 2.1 Gram–Schmidt Orthonormalization of a Set of 
Linearly Independent Vectors, x1, . . . , xm 
0. For k = 1, . . . , m, 
{ 
set ˜xk = xk. 
} 
1. Ensure that ˜x1 /= 0;  
set ˜x1 = ˜x1/||˜x1||. 
2. If m >  1, for k = 2, . . . , m, 
{ 
for j = k, . . . , m, 
{ 
set ˜xj = ˜xj −<˜xk−1, ˜xj>˜xk−1. 
} 
ensure that ˜xk /= 0;  
set ˜xk = ˜xk/||˜xk||. 
} 
Although the method indicated in Eq. (2.56) is mathematically equivalent 
to this method, the use of Algorithm 2.1 is to be preferred for computations 
because it is less subject to rounding errors. (This may not be immediately 
obvious, although a simple numerical example can illustrate the fact — see 
Exercise 11.1c on page 604. We will not digress here to consider this further, 
but the diﬀerence in the two methods has to do with the relative magnitudes 
of the quantities in the subtraction. The method of Algorithm 2.1 is sometimes 
called the “modiﬁed Gram–Schmidt method,” although I call it the “Gram– 
Schmidt method.” I will discuss this method again in Sect. 11.2.1.) This is 
an instance of a principle that we will encounter repeatedly: the form of a 
mathematical expression and the way the expression should be evaluated in 
actual practice may be quite diﬀerent. 
These orthogonalizing transformations result in a set of orthogonal vectors 
that span the same space as the original set. They are not unique; if the order 
in which the vectors are processed is changed, a diﬀerent set of orthogonal 
vectors will result. 
Orthogonal vectors are useful for many reasons: perhaps to improve the 
stability of computations; or in data analysis to capture the variability most 
eﬃciently; or for dimension reduction as in principal components analysis 
(see Sect. 9.3 beginning on page 470); or in order to form more meaningful 
quantities as in a vegetative index in remote sensing. We will discuss various 
speciﬁc orthogonalizing transformations later. 
2.2.5 Orthonormal Basis Sets 
A basis for a vector space is often chosen to be an orthonormal set because it 
is easy to work with the vectors in such a set.

2.2 Cartesian Geometry
51
If u1, . . . , un is an orthonormal basis set for a space, then a vector x in 
that space can be expressed as 
x equals c 1 u 1 plus midline horizontal ellipsis plus c Subscript n Baseline u Subscript n Baseline commax = c1u1 + · · · + cnun,
(2.57) 
and because of orthonormality, we have 
c Subscript i Baseline equals left angle bracket x comma u Subscript i Baseline right angle bracket periodci = <x, ui>.
(2.58) 
(We see this by taking the inner product of both sides with ui.) A repre-
sentation of a vector as a linear combination of orthonormal basis vectors, 
as in Eq. (2.57), is called a Fourier expansion, and  the  ci are called Fourier 
coeﬃcients. 
By taking the inner product of each side of Eq. (2.57) with itself, we have 
Parseval’s identity: 
parallel to x parallel to squared equals sigma summation c Subscript i Superscript 2 Baseline period||x||2 =
E
c2
i .
(2.59) 
This shows that the L2 norm is the same as the norm in Eq. (2.37) (on page 38) 
for the case of an orthogonal basis. 
Although the Fourier expansion is not unique because a diﬀerent orthog-
onal basis set could be chosen, Parseval’s identity removes some of the arbi-
trariness in the choice; no matter what basis is used, the sum of the squares 
of the Fourier coeﬃcients is equal to the square of the norm that arises from 
the inner product. (“The inner product” here means the inner product used 
in deﬁning the orthogonality.) 
The expansion (2.57) is a special case of a very useful expansion in an 
orthogonal basis set. In the ﬁnite-dimensional vector spaces we consider here, 
the series is ﬁnite. In function spaces, the series is generally inﬁnite, and so 
issues of convergence are important. For diﬀerent types of functions, diﬀerent 
orthogonal basis sets may be appropriate. Polynomials are often used, and 
there are some standard sets of orthogonal polynomials, such as Jacobi, Her-
mite, and so on. For periodic functions especially, orthogonal trigonometric 
functions are useful. 
2.2.6 Approximation of Vectors 
In high-dimensional vector spaces, it is often useful to approximate a given 
vector in terms of vectors from a lower-dimensional space. Suppose, for exam-
ple, that V ⊆ IRn is a vector space of dimension k (necessarily, k ≤ n) and  x 
is a given n-vector, not necessarily in V. We wish to determine a vector ˜x in 
V that approximates x. Of course if  V = IRn , then  x ∈V, and so the problem 
is not very interesting. The interesting case is when V ⊂ IRn . 
Optimality of the Fourier Coeﬃcients 
The ﬁrst question, of course, is what constitutes a “good” approximation. One 
obvious criterion would be based on a norm of the diﬀerence of the given vector

52
2 Vectors and Vector Spaces
and the approximating vector. So now, choosing the norm as the Euclidean 
norm, we may pose the problem as one of ﬁnding ˜x ∈V  such that 
parallel to x minus x overTilde parallel to less than or equals parallel to x minus v parallel to for all v element of script upper V period||x −˜x|| ≤||x −v||
∀v ∈V.
(2.60) 
This diﬀerence is a truncation error. 
Let u1, . . . , uk be an orthonormal basis set for V, and  let  
x overTilde equals c 1 u 1 plus midline horizontal ellipsis plus c Subscript k Baseline u Subscript k Baseline comma˜x = c1u1 + · · · + ckuk,
(2.61) 
where the ci are the Fourier coeﬃcients of x, <x, ui>. 
Now let v = a1u1 + · · ·  + akuk be any other vector in V, and consider 
StartLayout 1st Row 1st Column parallel to x minus v parallel to squared 2nd Column equals 3rd Column double vertical bar x minus sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Baseline u Subscript i Baseline double vertical bar squared 2nd Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x minus sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Baseline u Subscript i Baseline comma x minus sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Baseline u Subscript i Baseline right angle bracket 3rd Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x comma x right angle bracket minus 2 sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Baseline left angle bracket x comma u Subscript i Baseline right angle bracket plus sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Superscript 2 4th Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x comma x right angle bracket minus 2 sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Baseline c Subscript i plus sigma summation Underscript i equals 1 Overscript k Endscripts a Subscript i Superscript 2 plus sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 minus sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 5th Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket x comma x right angle bracket plus sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis a Subscript i Baseline minus c Subscript i Baseline right parenthesis squared minus sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Superscript 2 6th Row 1st Column Blank 2nd Column equals 3rd Column double vertical bar x minus sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline u Subscript i Baseline double vertical bar squared plus sigma summation Underscript i equals 1 Overscript k Endscripts left parenthesis a Subscript i Baseline minus c Subscript i Baseline right parenthesis squared 7th Row 1st Column Blank 2nd Column greater than or equals 3rd Column double vertical bar x minus sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline u Subscript i Baseline double vertical bar squared period EndLayout||x −v||2 =
||||||||||x −
k
E
i=1
aiui
||||||||||
2
=
/
x −
k
E
i=1
aiui, x −
k
E
i=1
aiui
\
= <x, x> −2
k
E
i=1
ai<x, ui> +
k
E
i=1
a2
i
= <x, x> −2
k
E
i=1
aici +
k
E
i=1
a2
i +
k
E
i=1
c2
i −
k
E
i=1
c2
i
= <x, x> +
k
E
i=1
(ai −ci)2 −
k
E
i=1
c2
i
=
||||||||||x −
k
E
i=1
ciui
||||||||||
2
+
k
E
i=1
(ai −ci)2
≥
||||||||||x −
k
E
i=1
ciui
||||||||||
2
.
(2.62) 
Therefore, we have ||x − ˜x|| ≤||x −v||, and  so  ˜x formed by the Fourier coef-
ﬁcients is the best approximation of x with respect to the Euclidean norm in 
the k-dimensional vector space V. (For some other norm, this may not be the 
case.) 
Choice of the Best Basis Subset 
Now, posing the problem another way, we may seek the best k-dimensional 
subspace of IRn from which to choose an approximating vector. This question 
is not well posed (because the one-dimensional vector space determined by x 
is the solution), but we can pose a related interesting question: suppose we

2.2 Cartesian Geometry
53
have a Fourier expansion of x in terms of a set of n orthogonal basis vectors, 
u1, . . . , un, and we want to choose the “best” k basis vectors from this set and 
use them to form an approximation of x. (This restriction of the problem is 
equivalent to choosing a coordinate system.) We see the solution immediately 
from inequality (2.62): we choose the k uis corresponding to the k largest cis 
in absolute value, and we take 
x overTilde equals c Subscript i 1 Baseline u Subscript i 1 Baseline plus midline horizontal ellipsis plus c Subscript i Sub Subscript k Subscript Baseline u Subscript i Sub Subscript k Subscript Baseline comma˜x = ci1ui1 + · · · + cikuik,
(2.63) 
where min({|cij| : j = 1, . . . , k}) ≥ max({|cij| : j = k + 1, . . . , n}). 
2.2.7 Flats, Aﬃne Spaces, and Hyperplanes 
Given an n-dimensional vector space of order n, IRn for example, consider a 
system of m linear equations in the n-vector variable x, 
StartLayout 1st Row 1st Column c 1 Superscript normal upper T Baseline x 2nd Column equals 3rd Column b 1 2nd Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column vertical ellipsis 3rd Row 1st Column c Subscript m Superscript normal upper T Baseline x 2nd Column equals 3rd Column b Subscript m Baseline comma EndLayoutcT
1 x = b1
...
...
cT
mx = bm,
where c1, . . . , cm are linearly independent n-vectors (and hence m ≤ n). The 
set of points deﬁned by these linear equations is called a ﬂat. Although it is not 
necessarily a vector space, a ﬂat is also called an aﬃne space. An intersection 
of two ﬂats is a ﬂat. 
If the equations are homogeneous (that is, if b1 = · · ·  = bm = 0), then the 
point (0, . . . , 0) is included, and the ﬂat is an (n −m)-dimensional subspace 
(also a vector space, of course). Stating this another way, a ﬂat through the 
origin is a vector space, but other ﬂats are not vector spaces. 
If m = 1, the ﬂat is called a hyperplane. A hyperplane through the origin 
is an (n − 1)-dimensional vector space. 
If m = n−1, the ﬂat is a line. A line through the origin is a one-dimensional 
vector space. 
2.2.8 Cones 
A set of vectors that contains all nonnegative scalar multiples of any vector 
in the set is called a cone. A cone always contains the zero vector. (Some 
authors deﬁne a cone as a set that contains all positive scalar multiples, and 
in that case, the zero vector may not be included.) If a set of vectors contains 
all scalar multiples of any vector in the set, it is called a double cone. 
Geometrically, a cone is just a set, possibly a ﬁnite set, of lines or half-
lines. (A double cone is a set of lines.) In general, a cone may not be very 
interesting, but certain special cones are of considerable interest. 
Given two (double) cones over the same vector space, both their union 
and their intersection are (double) cones. A (double) cone is in general not a 
vector space.

54
2 Vectors and Vector Spaces
Convex Cones 
A set of vectors C in a vector space V is a convex cone if, for all v1, v2 ∈ C 
and all nonnegative real numbers a, b ≥ 0, av1 + bv2 ∈ C. Such a cone is 
called a homogeneous convex cone by some authors. (An equivalent deﬁnition 
requires that the set C be a cone, and then, more in keeping with the deﬁnition 
of convexity, includes the requirement a + b = 1 along with a, b ≥ 0 in the  
deﬁnition of a convex cone.) 
A convex cone C is in general not a vector space because, for example, for 
some v1, v2 ∈ C, v1 − v2 may not be in C. 
If C is a convex cone and if v ∈ C implies −v ∈ C, then  C is called a 
double convex cone. A double convex cone is a vector space; for all v1, v2 ∈ C 
and all real numbers a, b, av1 + bv2 ∈ C. 
It is clear that a (double) convex cone is a (double) cone. In common 
applications, a convex cone is the most important type of cone. A convex 
cone corresponds to a solid geometric object with a single ﬁnite vertex. 
Figure 2.3. A Set of Vectors {v1, v2, v3}, and the Corresponding Convex Cone C, 
the Dual Cone C∗, and the Polar Cone C0 
An important convex cone in an n-dimensional vector space with a Carte-
sian coordinate system is the positive orthant together with the zero vector. 
The closure of the open positive orthant (that is, the nonnegative orthant) is 
also a convex cone. (This is the cone C∗ in 2-space in Fig. 2.3 below.) 
A generating set or spanning set of a cone C is a set of vectors S = {vi} 
such that for any vector v in C there exists scalars ai ≥0 so that  v = E aivi. 
If, in addition, for any scalars bi ≥ 0 with E bivi = 0, it is necessary that 
bi = 0 for all i, then  S is a basis set for the cone. The concept of a generating 
set is of course more interesting in the case of a convex cone. 
If a generating set of a convex cone has a ﬁnite number of elements, the 
cone is a polyhedron. For the common geometric object in three dimensions 
with elliptical contours and which is the basis for “conic sections,” any gen-
erating set has an uncountable number of elements. Cones of this type are 
sometimes called “Lorentz cones.”

2.2 Cartesian Geometry
55
It is easy to see from the deﬁnition that if C1 and C2 are convex cones 
over the same vector space, then C1 ∩C2 is a convex cone. On the other hand, 
C1 ∪C2 is not necessarily a convex cone. Of course the union of two cones, as 
we have seen, is a cone. 
Dual Cones 
Given a set of vectors S in a given vector space (in cases of interest, S is 
usually a cone, but not necessarily), the dual cone of S, denoted C∗(S), is 
deﬁned as 
upper C Superscript asterisk Baseline left parenthesis upper S right parenthesis equals left brace v Superscript asterisk Baseline normal s period normal t period left angle bracket v Superscript asterisk Baseline comma v right angle bracket greater than or equals 0 normal f normal o normal r normal a normal l normal l v element of upper S right brace periodC∗(S) = {v∗s.t. <v∗, v> ≥0 for all v ∈S}.
See Fig. 2.3 in which S = {v1, v2, v3}. Clearly, the dual cone is a cone, and 
also S ⊆ C∗(S). 
If, as in the most common cases, the underlying set of vectors is a cone, 
say C, we generally drop the reference to an underlying set of vectors, and 
just denote the dual cone of C as C∗. 
Geometrically, the dual cone C∗ of S consists of all vectors that form 
nonobtuse angles with the vectors in S. 
Notice that for a given set of vectors S, if  −S represents the set of vec-
tors v such that −v ∈ S, then  C∗(−S) =  −(C∗(S)), or just −C∗(S), which 
represents the set of vectors v∗ such that −v∗ ∈C∗(S). 
Further, from the deﬁnition, we note that if S1 and S2 are sets of vectors 
in the same vector space such that S1 ⊆ S2 then C∗(S2) ⊆ C∗(S1), that is, 
C∗ 
2 ⊆ C∗ 
1. This is obvious if we realize that a dual cone is essentially a kind 
of complementary set. 
A dual cone C∗(S) is a closed convex cone. We see this by considering any 
v∗ 
1, v∗ 
2 ∈C∗ and real numbers a, b ≥ 0. For any v ∈ S, it must be the  case  that
<v∗ 
1, v> ≥0 and <v∗ 
2, v> ≥ 0; hence, <(av∗ 
1 +bv∗ 
2), v> ≥ 0, that is, av∗ 
1 +bv∗ 
2 ∈ C∗, 
so C∗ is a convex cone. The closure property comes from the ≥ condition in 
the deﬁnition. 
Polar Cones 
Given a set of vectors S in a given vector space (in cases of interest, S is 
usually a cone, but not necessarily), the polar cone of S, denoted C0 (S), is 
deﬁned as 
upper C Superscript 0 Baseline left parenthesis upper S right parenthesis equals left brace v Superscript 0 Baseline normal s period normal t period left angle bracket v Superscript 0 Baseline comma v right angle bracket less than or equals 0 normal f normal o normal r normal a normal l normal l v element of upper S right brace periodC0(S) = {v0 s.t. <v0, v> ≤0 for all v ∈S}.
See Fig. 2.3. 
We generally drop the reference to an underlying set of vectors and just 
denote the dual cone of the set C as C0 . 
From the deﬁnition, we note that if S1 and S2 are sets of vectors in the 
same vector space such that S1 ⊆ S2, then  C0 (S1) ⊆ C0 (S2), or C0 
1 ⊆ C0 
2. 
The polar cone and the dual cone of a double cone are clearly the same.

56
2 Vectors and Vector Spaces
From the deﬁnitions, it is clear in any case that the polar cone C0 can be 
formed by multiplying all of the vectors in the corresponding dual cone C∗ 
by −1, and so C0 = −C∗. 
The relationships of the polar cone to the dual cone and the properties we 
have established for a dual cone immediately imply that a polar cone is also 
a convex cone. 
Another interesting property of polar cones is that for any set of vectors 
S in a given vector space, S ⊆ (C0 )0 . We generally write (C0 )0 as just C00 . 
(The precise notation of course is C0 (C0 (S)).) We see this by ﬁrst taking 
any v ∈ S. Therefore, if v0 ∈ C0 then <v, v0> ≤ 0, which implies v ∈ (C0 )0 , 
because 
upper C Superscript 00 Baseline equals left brace v normal s period normal t period left angle bracket v comma v Superscript 0 Baseline right angle bracket less than or equals 0 normal f normal o normal r normal a normal l normal l v Superscript 0 Baseline element of upper C Superscript 0 Baseline right brace periodC00 = {v s.t. <v, v0> ≤0 for all v0 ∈C0}.
Additional Properties 
As noted above, a cone is a very loose and general structure. In my deﬁnition, 
the vectors in the set do not even need to be in the same vector space. A 
convex cone, on the other hand, is a useful structure, and the vectors in a 
convex cone must be in the same vector space. 
Most cones of interest, in particular, dual cones and polar cones are not 
necessarily vector spaces. 
Although the deﬁnitions of dual cones and polar cones can apply to any 
set of vectors, they are of the most interest in the case in which the underlying 
set of vectors is the cone in the nonnegative orthant of a Cartesian coordinate 
system on IRn (the set of n-vectors all of whose elements are nonnegative). 
In that case, the dual cone is just the full nonnegative orthant, and the polar 
cone is just the nonpositive orthant (the set of all vectors all of whose elements 
are nonpositive). 
The whole nonnegative orthant itself is a convex cone, and as we have seen 
for any convex cone within that orthant, the dual cone is the full nonnegative 
orthant. 
Because the nonnegative orthant is its own dual, and hence is said to be 
“self-dual.” (There is an extension of the property of self-duality that we will 
not discuss here.) 
Convex cones occur in many optimization problems. The feasible region 
in a linear programming problem is generally a convex polyhedral cone, for 
example. 
2.2.9 Vector Cross Products in IR3 
The vector space IR3 is especially interesting because it serves as a useful 
model of the real physical world, and many processes can be represented as 
vectors in it. 
For the special case of the vector space IR3 , another useful product of 
vectors is the vector cross-product, which is a mapping from IR3 × IR3 to IR3 .

2.2 Cartesian Geometry
57
Before proceeding, we note an overloading of the term “cross-product” and of 
the symbol “×” used to denote it. If A and B are sets, the set cross product 
or the set Cartesian product of A and B is the set consisting of all doubletons 
(a, b) where  a ranges over all elements of A, and  b ranges independently over 
all elements of B. Thus,  IR3 × IR3 is the set of all pairs of all real 3-vectors. 
The vector cross-product of the 3-vectors 
x equals left parenthesis x 1 comma x 2 comma x 3 right parenthesisx = (x1, x2, x3)
and 
y equals left parenthesis y 1 comma y 2 comma y 3 right parenthesis commay = (y1, y2, y3),
written x × y, is deﬁned as 
x times y equals parallel to x parallel to parallel to y parallel to sine left parenthesis normal a normal n normal g normal l normal e left parenthesis y comma x right parenthesis right parenthesis e commax × y = ||x||||y|| sin(angle(y, x))e,
(2.64) 
where e is a vector such that ||e|| = 1  and <e, x> = <e, y> = 0. (Statisticians 
also use the term “cross-products” in a diﬀerent context to refer to another 
type of product formed by inner products; see page 398.) Because x × y is 
a vector we sometimes refer to the cross-product as the vector product, as  
opposed to the scalar product. 
Notice the ambiguity about the sign of x × y, or its direction, “up” or 
“down.” This results from the ambiguity in the sign on e and an ambiguity  in  
functions of angle(y, x). We resolve this ambiguity by adopting a convention 
about the coordinate system. This convention also allows us to choose the 
sign on the normal vector e. 
In the deﬁnition of angles between vectors given on page 47, it is the  
case that angle(y, x) = angle(x, y). As we pointed out there, sometimes it is 
important to introduce the concept of direction of an angle, and this is the 
case in Eq. (2.64), as in many applications in IR3 , such as those involving 
electric or magnetic polarity. Special derivative operations useful in physics 
are based on these concepts (see page 333). 
The direction of angles in IR3 often is used to determine the orientation of 
the principal axes in a coordinate system. In a Cartesian coordinate system 
in IR3 , the principal axes correspond to the unit vectors e1 = (1, 0, 0), e2 = 
(0, 1, 0), and e3 = (0, 0, 1). This system has an indeterminate correspondence 
to a physical three-dimensional system; if the plane determined by e1 and e2 
is taken as horizontal, then e3 could “point upward” or “point downward”. 
A simple way that this indeterminacy can be resolved is to require that the 
principal axes have the orientation of the thumb, index ﬁnger, and middle 
ﬁnger of the right hand when those digits are spread in orthogonal directions, 
where e1 corresponds to the index ﬁnger, e2 corresponds to the middle ﬁnger, 
and e3 corresponds to the thumb. This is called a “right-hand” coordinate 
system. 
The cross-product has the following properties, which are immediately 
obvious from the deﬁnition:

58
2 Vectors and Vector Spaces
1. Self-nilpotency: 
x × x = 0, for all x. 
2. Anti-commutativity: 
x × y = −y × x. 
3. Factoring of scalar multiplication; 
ax × y = a(x × y) for real a. 
4. Relation of vector addition to addition of cross products: 
z × (x + y) = (z × x) + (z × x) and  (x + y) × z = (x × z) + (y × z), 
that is, the cross-product distributes over addition. 
Other properties of cross-products are shown in Exercise 2.21 on page 69. 
The cross-product has the important property in a right-hand system 
x times y equals left parenthesis x 2 y 3 minus x 3 y 2 comma x 3 y 1 minus x 1 y 3 comma x 1 y 2 minus x 2 y 1 right parenthesis periodx × y = (x2y3 −x3y2, x3y1 −x1y3, x1y2 −x2y1).
(2.65) 
This can be shown by writing x as x1e1 +x2e2 +x3e3 and y similarly, forming 
x × y using the distributivity property along with the facts that ei × ei = 0,  
e1 × e2 = e3, and so on, and ﬁnally collecting terms. You are asked to do go 
through these steps in Exercise 2.21g. 
The expansion (2.65) can be reexpressed in terms of the cofactors of a 
certain matrix, as we will show on page 91. We will then give an extension of 
the cofactor formula to obtain a k-vector that is orthogonal to a given set of 
k −1 vectors. 
The cross-product is useful in modeling natural phenomena, such as force 
ﬁelds and ﬂow of electricity. Physical ﬁelds and forces are naturally often 
represented as vectors in IR3 . The cross-product is also useful in “three-
dimensional” computer graphics for determining whether a given surface is 
visible from a given perspective and for simulating the eﬀect of lighting on a 
surface. 
2.3 Centered Vectors and Variances and 
Covariances of Vectors 
In this section, we deﬁne some scalar-valued functions of vectors that are 
analogous to functions of random variables averaged over their probabilities or 
probability density. The functions of vectors discussed here are the same as the 
ones that deﬁne sample statistics. This short section illustrates the properties 
of norms, inner products, and angles in terms that should be familiar to the 
reader. 
These functions, and transformations using them, are useful for appli-
cations in the data sciences. It is important to know the eﬀects of various 
transformations of data on data analysis.

2.3 Variances and Covariances
59
2.3.1 The Mean and Centered Vectors 
When the elements of a vector have some kind of common interpretation, the 
sum of the elements or the mean (Eq. (2.45)) of the vector may have meaning. 
In this case, it may make sense to center the vector; that is, to subtract 
the mean from each element. For a given vector x, we denote its centered 
counterpart as xc: 
x Subscript normal c Baseline equals x minus x overbar periodxc = x −¯x.
(2.66) 
We refer to any vector whose sum of elements is 0 as a centered vector; note, 
therefore, for any centered vector xc, 
1 Superscript normal upper T Baseline x Subscript normal c Baseline equals 0 semicolon1Txc = 0;
or, indeed, for any constant vector a, aT xc = 0.  
From the deﬁnitions, it is easy to see that 
left parenthesis x plus y right parenthesis Subscript normal c Baseline equals x Subscript normal c Baseline plus y Subscript normal c(x + y)c = xc + yc
(2.67) 
(see Exercise 2.22). Interpreting ¯x as a vector, and recalling that it is the 
projection of x onto the one vector, we see that xc is the residual in the sense 
of Eq. (2.51). Hence, we see that xc and ¯x are orthogonal, and the Pythagorean 
relationship holds: 
parallel to x parallel to squared equals parallel to x overbar parallel to squared plus parallel to x Subscript normal c Baseline parallel to squared period||x||2 = ||¯x||2 + ||xc||2.
(2.68) 
From this we see that the length of a centered vector is less than or equal to the 
length of the original vector. (Notice that Eq. (2.68) is just the formula familiar 
to data analysts, which with some rearrangement is E(xi−¯x)2 = E x2 
i −n¯x2 .) 
For any scalar a and n-vector x, expanding the terms, we see that 
parallel to x minus a parallel to squared equals parallel to x Subscript normal c Baseline parallel to squared plus n left parenthesis a minus x overbar right parenthesis squared comma||x −a||2 = ||xc||2 + n(a −¯x)2,
(2.69) 
where we interpret ¯x as a scalar here. An implication of this equation is that 
for all values of a, ||x − a|| is minimized if a = ¯x. 
Notice that a nonzero vector when centered may be the zero vector. This 
leads us to suspect that some properties that depend on a dot product are 
not invariant to centering. This is indeed the case. The angle between two 
vectors, for example, is not invariant to centering; that is, in general, 
normal a normal n normal g normal l normal e left parenthesis x Subscript normal c Baseline comma y Subscript normal c Baseline right parenthesis not equals normal a normal n normal g normal l normal e left parenthesis x comma y right parenthesisangle(xc, yc) /= angle(x, y)
(2.70) 
(see Exercise 2.23). 
2.3.2 The Standard Deviation, the Variance, and Scaled Vectors 
We also sometimes ﬁnd it useful to scale a vector by both its length (normalize 
the vector) and by a function of its number of elements. We denote this scaled 
vector as xs and deﬁne it as

60
2 Vectors and Vector Spaces
x Subscript normal s Baseline equals StartRoot n minus 1 EndRoot StartFraction x Over parallel to x Subscript normal c Baseline parallel to EndFraction periodxs =
√
n −1
x
||xc||.
(2.71) 
For comparing vectors, it is usually better to center the vectors prior to any 
scaling. We denote this centered and scaled vector as xcs and deﬁne it as 
x Subscript normal c normal s Baseline equals StartRoot n minus 1 EndRoot StartFraction x Subscript normal c Baseline Over parallel to x Subscript normal c Baseline parallel to EndFraction periodxcs =
√
n −1
xc
||xc||.
(2.72) 
Centering and scaling are also called standardizing. Note that the vector is 
centered before being scaled. The angle between two vectors is not changed 
by scaling (but, of course, it may be changed by centering). 
The multiplicative inverse of the scaling factor, 
s Subscript x Baseline equals parallel to x Subscript normal c Baseline parallel to divided by StartRoot n minus 1 EndRoot commasx = ||xc||/
√
n −1,
(2.73) 
is called the standard deviation of the vector x. The standard deviation of xc 
is the same as that of x; in fact, the standard deviation is invariant to the 
addition of any constant. The standard deviation is a measure of how much 
the elements of the vector vary. If all of the elements of the vector are the 
same, the standard deviation is 0 because in that case xc = 0.  
The square of the standard deviation is called the variance, denoted by V: 
StartLayout 1st Row 1st Column normal upper V left parenthesis x right parenthesis 2nd Column equals 3rd Column s Subscript x Superscript 2 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction parallel to x Subscript normal c Baseline parallel to squared Over n minus 1 EndFraction period EndLayoutV(x) = s2
x
= ||xc||2
n −1 .
(2.74) 
In perhaps more familiar notation, Eq. (2.74) is just  
normal upper V left parenthesis x right parenthesis equals StartFraction 1 Over n minus 1 EndFraction sigma summation left parenthesis x Subscript i Baseline minus x overbar right parenthesis squared periodV(x) =
1
n −1
E
(xi −¯x)2.
From Eq. (2.68), we see that 
normal upper V left parenthesis x right parenthesis equals StartFraction 1 Over n minus 1 EndFraction left parenthesis parallel to x parallel to squared minus parallel to x overbar parallel to squared right parenthesis periodV(x) =
1
n −1
(
||x||2 −||¯x||2)
.
(The terms “mean,” “standard deviation,” “variance,” and other terms we will 
mention below are also used in an analogous, but slightly diﬀerent, manner to 
refer to properties of random variables. In that context, the terms to refer to 
the quantities we are discussing here would be preceded by the word “sample,” 
and often for clarity, I will use the phrases “sample standard deviation” and 
“sample variance” to refer to what is deﬁned above, especially if the elements 
of x are interpreted as independent realizations of a random variable. Also, 
recall the two possible meanings of “mean”, or ¯x; one is a vector as above, 
and one is a scalar as in Eq. (2.46).) 
If a and b are scalars (or b is a vector with all elements the same), the 
deﬁnition, together with Eq. (2.69), immediately gives

2.3 Variances and Covariances
61
normal upper V left parenthesis a x plus b right parenthesis equals a squared normal upper V left parenthesis x right parenthesis periodV(ax + b) = a2V(x).
This implies that for the scaled vector xs, 
normal upper V left parenthesis x Subscript normal s Baseline right parenthesis equals 1 periodV(xs) = 1.
If a is a scalar and x and y are vectors with the same number of elements, 
from the equation above, and using Eq. (2.30) on page 36, we see that the 
variance following an axpy operation is given by 
normal upper V left parenthesis a x plus y right parenthesis equals a squared normal upper V left parenthesis x right parenthesis plus normal upper V left parenthesis y right parenthesis plus 2 a StartFraction left angle bracket x Subscript normal c Baseline comma y Subscript normal c Baseline right angle bracket Over n minus 1 EndFraction periodV(ax + y) = a2V(x) + V(y) + 2a<xc, yc>
n −1 .
(2.75) 
While Eq. (2.74) appears to be relatively simple, evaluating the expression 
for a given x may not be straightforward. We discuss computational issues 
for this expression on page 568. This is an instance of a principle that we will 
encounter repeatedly: the form of a mathematical expression and the way the 
expression should be evaluated in actual practice may be quite diﬀerent. 
2.3.3 Covariances and Correlations Between Vectors 
If x and y are n-vectors, the covariance between x and y is 
normal upper C normal o normal v left parenthesis x comma y right parenthesis equals StartFraction left angle bracket x minus x overbar comma y minus y overbar right angle bracket Over n minus 1 EndFraction periodCov(x, y) = <x −¯x, y −¯y>
n −1
.
(2.76) 
By representing x − ¯x as x − ¯x1 and  y − ¯y 
similarly, and expanding, we see that Cov(x, y) = (<x, y> − n¯x¯y)/(n − 1). 
Also, we see from the deﬁnition of covariance that Cov(x, x) is the  variance  
of the vector x, as deﬁned above. 
From the deﬁnition and the properties of an inner product given on 
page 33, if  x, y, and  z are conformable vectors, we see immediately that 
• Cov(x, y) = 0  
if V(x) = 0  or  V(y) = 0;  
• Cov(ax, y) =  aCov(x, y) 
for any scalar a; 
• Cov(y, x) = Cov(x, y); 
• Cov(y, y) = V(y); and 
• Cov(x + z, y) = Cov(x, y) + Cov(z, y), 
in particular, 
–
Cov(x + y, y) = Cov(x, y) + V(y), and 
–
Cov(x + a, y) = Cov(x, y), for any scalar a. 
Using the deﬁnition of the covariance, we can rewrite Eq. (2.75) as  
normal upper V left parenthesis a x plus y right parenthesis equals a squared normal upper V left parenthesis x right parenthesis plus normal upper V left parenthesis y right parenthesis plus 2 a normal upper C normal o normal v left parenthesis x comma y right parenthesis periodV(ax + y) = a2V(x) + V(y) + 2aCov(x, y).
(2.77)

62
2 Vectors and Vector Spaces
The covariance is a measure of the extent to which the vectors point in 
the same direction. A more meaningful measure of this is obtained by the 
covariance of the centered and scaled vectors. This is the correlation between 
the vectors, which if ||xc|| /= 0  and ||yc|| /= 0,  
StartLayout 1st Row 1st Column normal upper C normal o normal r left parenthesis x comma y right parenthesis 2nd Column equals 3rd Column normal upper C normal o normal v left parenthesis x Subscript normal c normal s Baseline comma y Subscript normal c normal s Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column left angle bracket StartFraction x Subscript normal c Baseline Over parallel to x Subscript normal c Baseline parallel to EndFraction comma StartFraction y Subscript normal c Baseline Over parallel to y Subscript normal c Baseline parallel to EndFraction right angle bracket 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction left angle bracket x Subscript normal c Baseline comma y Subscript normal c Baseline right angle bracket Over parallel to x Subscript normal c Baseline parallel to parallel to y Subscript normal c Baseline parallel to EndFraction period EndLayoutCor(x, y) = Cov(xcs, ycs)
=
/ xc
||xc||,
yc
||yc||
\
= <xc, yc>
||xc||||yc||.
(2.78) 
If ||xc|| = 0  or ||yc|| = 0, we deﬁne Cor(x, y) to be 0. We see  immediately from  
Eq. (2.53) that the correlation is the cosine of the angle between xc and yc: 
normal upper C normal o normal r left parenthesis x comma y right parenthesis equals cosine left parenthesis normal a normal n normal g normal l normal e left parenthesis x Subscript normal c Baseline comma y Subscript normal c Baseline right parenthesis right parenthesis periodCor(x, y) = cos(angle(xc, yc)).
(2.79) 
(Recall that this is not the same as the angle between x and y.) 
An equivalent expression for the correlation, so long as V(x) /= 0  and  
V(y) /= 0,  is  
normal upper C normal o normal r left parenthesis x comma y right parenthesis equals StartFraction normal upper C normal o normal v left parenthesis x comma y right parenthesis Over StartRoot normal upper V left parenthesis x right parenthesis normal upper V left parenthesis y right parenthesis EndRoot EndFraction periodCor(x, y) =
Cov(x, y)
/
V(x)V(y)
.
(2.80) 
It is clear that the correlation is in the interval [−1, 1] (from the Cauchy-
Schwarz inequality). A correlation of −1 indicates that the vectors point in 
opposite directions, a correlation of 1 indicates that the vectors point in the 
same direction, and a correlation of 0 indicates that the vectors are orthogonal. 
While the covariance is equivariant to scalar multiplication, the absolute 
value of the correlation is invariant to it; that is, the correlation changes only 
as the sign of the scalar multiplier, 
normal upper C normal o normal r left parenthesis a x comma y right parenthesis equals normal s normal i normal g normal n left parenthesis a right parenthesis normal upper C normal o normal r left parenthesis x comma y right parenthesis commaCor(ax, y) = sign(a)Cor(x, y),
(2.81) 
for any scalar a. 
Appendix: Vectors in R 
The way to form an object with multiple elements in R is by use of the c 
function. If, for example, x is the vector x = (1, 2, −2, 4, 2), an object with 
these elements could be formed by 
> x <- c(1,2,-2,4,2) 
The individual elements may be accessed with indexes that start at 1 and are 
enclosed in square brackets.

2.3 Variances and Covariances
63
> x[3] 
[1]
-2 
> x[c(2,3)] 
[1] 2
-2 
If the elements are numeric, as in this case, R treats the object as a vector. 
Scalar operations are performed on the vector elementwise: 
> x^2 
[1] 1 4 4 16 4 
and other numeric vector operations, such as axpy, are performed as appro-
priate: 
> a <- 2  
> x <- c(1,2,-2,4,2) 
> y <- c(2,1,4,5,0) 
> z <- a*x+y 
> z  
[1] 4 5 0 13 4 
Since the basic numeric object in R is a vector, a is a vector in the code above. 
For operations on vectors of diﬀerent order, as in multiplication of a and x, R  
follows the rule of cycling the elements of the shorter vector: 
> b <- c(2,3) 
> b*x 
[1] 2 6 -4 12 4 
Warning message: 
In b * x : longer object length is not a multiple of shorter 
object length 
In R, the object should be treated as a vector, and the individual elements 
should not be addressed, unless necessary. When working with vectors, it is 
very rare to need to use one of the program looping statements such as for 
or while. 
R usually interprets the ordinary arithmetic operators to apply element-
wise to arrays.

64
2 Vectors and Vector Spaces
> x+y 
[1] 3 3 2 9 2 
> x-y 
[1] -1 1 -6 -1 2 
> x*y 
[1] 2 2 -8 20 0 
> x/y 
[1] 0.5 2.0 0.5 0.8 Inf 
R provides simple functions to operate on the contents of a vector such 
as max, min, sum, with obvious meanings. There is no basic operator for the 
dot product, but it can be computed using elementwise multiplication and 
the sum function. The length function computes the number of elements in 
the vector. The which function determines which elements of a vector satisfy 
a logical relation. 
> min(x) 
[1] -2 
> sum(x*y)
# the dot product 
[1] 16 
> sqrt(sum(x*x)) # Euclidean length (the L2 norm) 
[1] 5.385165 
> length(x)
# number of elements 
[1] 5 
> which(x==2)
# which elements equal 2 
[1] 2 5 
A useful operator in building a vector is the “:” operator, for which there 
is a corresponding function, “seq.” The related “rep” is also useful in forming  
entries for a vector. An important property of the : operator is that it takes 
precedence over the usual arithmetic operators, *, +, and  ^. Some examples  
will clarify: 
> a <- -1  
> b <- 2  
> a:b 
[1] -1 0 1 2 
> seq(from=-1, to=2) 
[1] -1 0 1 2

2.3 Variances and Covariances
65
> seq(from=-1, to=2, by=2) 
[1] -1 1 
> 2*a:b+3 
[1] 1 3 5 7 
> (2*a):(b+3) 
[1] -2 -1 0 1 2 3 4 5 
> c(1:3,rep(4,2),3:1,seq(5,8)) 
[1]  1 2 3 4 4 3 2 1 5 6 7 8  
There are also simple functions to compute sample statistics from the 
contents of a vector: mean, var, sd, cov, cor, with obvious meanings. 
Everything is a Vector 
One of the most useful design aspects of R is that the atomic object is a 
vector. This can be illustrated by the hypot function we wrote on page 12 in 
Chap. 1: 
> hypot <- function(a, b=a) sqrt(a^2+b^2) 
> u <- c(3,5); v <- c(4,12) 
> hypot(u,v) 
[1] 5 13 
and by the %hyp% operator deﬁned in Chap. 1: 
> u %hyp% v 
[1] 5 13 
Lists 
A list is a special R object of class list that, like a vector, contains a sequence, 
but unlike a vector can hold objects of diﬀerent types. A list is a very ﬂexible 
structure, but I will discuss lists here only in the very simple context of vectors 
in a list, and I will only mention a few of the properties of a list. I will mention 
other features of lists on page 251. 
A list is constructed by the list function. Lists have two levels of indexing. 
The ﬁrst level can be accessed as a list, by a indexes enclosed in “[],” or as 
the class of the element itself, by an indexes enclosed in “[[]]”. If an element 
of the list has indexes, elements within the list can be accessed by a second 
level index.

66
2 Vectors and Vector Spaces
> x1 <- c(1,2) 
> y1 <- c(4,6,8) 
> xy <-list(x1,y1) 
> xy  
[[1]] 
[1] 1 2 
[[2]] 
[1] 4 6 8 
> xy[2] 
[[1]] 
[1] 4 6 8 
> xy[[2]] 
[1] 4 6 8 
> xy[[2]][3] 
[1] 8 
Two of the most useful functions for working with lists are the lapply and 
lapply functions. The ﬁrst argument in each function is a list, and the second 
argument is a function that is to be applied to each upper-level element in 
the list. The lapply function returns a list. If the list can be “simpliﬁed” as 
a vector, the sapply function can be used. 
> lapply(xy,mean) 
[[1]] 
[1] 1.5 
[[2]] 
[1] 6 
> sapply(xy,mean) 
[1] 1.5 6.0 
Exercises 
2.1. Linear independence. 
a) Using the property in Eq. (2.3) show that if a set of at least 2 vectors 
is linearly independent, then every pair of vectors in the set is a 
linearly independent set. 
b) Construct a counterexample in IR3 to show that if every pair of 
vectors in a set of vectors (every doubleton subset) is linearly inde-
pendent, the set is not necessarily linearly independent. 
2.2. Write out the step-by-step proof that the maximum number of n-vectors 
that can form a set that is linearly independent is n, as stated on page 21.

Exercises
67
2.3. Prove inequalities (2.10) and  (2.11). 
2.4. a) Give an example of a vector space and a subset of the set of vectors 
in it such that that subset together with the axpy operation is not 
a vector space. 
b) Give an example of two vector spaces such that the union of the sets 
of vectors in them together with the axpy operation is not a vector 
space. 
2.5. Prove the equalities (2.15) and  (2.16). 
Hint: Use of basis sets makes the details easier. 
2.6. Prove (2.19). 
2.7. a) Let V be a vector space and let V1 be a subspace, V1 ⊆V. 
Show that there exists a basis B for V and a basis B1 for V1 such 
that B1 ⊆B. 
b) Let V1 and V2 be essentially disjoint vector spaces. Let B1 be a basis  
set for V1 and B2 be a basis set for V2. 
Show that B1 ∪ B2 is a basis set for V = V1 ⊕V2. 
c) Let V1 be a real vector space of order n1 and let B1 be a basis set 
for V1. Let  V2 be a real vector space of order n2 and B2 be a basis 
set for V2. For each vector b1 in B1 form the vector 
b overTilde Subscript 1 Baseline equals left parenthesis b 1 vertical bar 0 comma ellipsis comma 0 right parenthesis where there are n 2 0 s comma˜b1 = (b1|0, . . . , 0)
where there are n2 0s,
and let -B1 be the set of all such vectors. Likewise, for each vector 
b2 in B2 form the vector 
b overTilde Subscript 2 Baseline equals left parenthesis 0 comma ellipsis comma 0 vertical bar b 2 right parenthesis where there are n 1 0 s comma˜b2 = (0, . . . , 0|b2)
where there are n1 0s,
and let -B2 be the set of all such vectors. 
Show that -B1 ∪-B2 is a basis for V1 ⊗V2. 
2.8. Show that if the norm is induced by an inner product that the parallel-
ogram equality, Eq. (2.31) holds.  
2.9. Let p = 1 
2 in Eq. (2.32); that is, let ρ(x) be deﬁned for the n-vector x as 
rho left parenthesis x right parenthesis equals left parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts StartAbsoluteValue x Subscript i Baseline EndAbsoluteValue Superscript 1 divided by 2 Baseline right parenthesis squared periodρ(x) =
( n
E
i=1
|xi|1/2
)2
.
Show that ρ(·) is not a norm. 
2.10. Show that the L1 norm is not induced by an inner product. 
Hint: Find a counterexample that does not satisfy the parallelogram 
equality (Eq. (2.31)). 
2.11. Prove Eq. (2.33) and show that the bounds are sharp by exhibiting in-
stances of equality. (Use the fact that ||x||∞ = maxi |xi|.) 
2.12. Prove the following inequalities. 
a) Prove H¨older’s inequality: for any p and q such that p ≥ 1 and  
p + q = pq, and for vectors x and y of the same order, 
left angle bracket x comma y right angle bracket less than or equals parallel to x parallel to Subscript p Baseline parallel to y parallel to Subscript q Baseline period<x, y> ≤||x||p||y||q.

68
2 Vectors and Vector Spaces
b) Prove the triangle inequality for any Lp norm. (This is sometimes 
called Minkowski’s inequality.) 
Hint: Use H¨older’s inequality. 
2.13. Show that the expression deﬁned in Eq. (2.41) on page 42 is a metric. 
2.14. Show that Eq. (2.52) on page 46 is correct. 
2.15. Show that the intersection of two orthogonal vector spaces consists only 
of the zero vector. 
2.16. From the deﬁnition of direction cosines in Eq. (2.54), it is easy to see 
that the sum of the squares of the direction cosines is 1. For the special 
case of IR3 , draw a sketch and use properties of right triangles to show 
this geometrically. 
2.17. In IR2 with a Cartesian coordinate system, the diagonal directed line 
segment through the positive quadrant (orthant) makes a 45◦ angle 
with each of the positive axes. In 3 dimensions, what is the angle be-
tween the diagonal and each of the positive axes? In 10 dimensions? In 
100 dimensions? In 1000 dimensions? We see that in higher dimensions 
any two lines are almost orthogonal. (That is, the angle between them 
approaches 90◦.) What are some of the implications of this for data 
analysis? 
2.18. Show that if the initial set of vectors are linearly independent, all resid-
uals in Algorithm 2.1 are nonzero. (For given k ≥ 2, all that is required 
is to show that 
x overTilde Subscript k Baseline minus left angle bracket x overTilde Subscript k minus 1 Baseline comma x overTilde Subscript k Baseline right angle bracket x overTilde Subscript k minus 1 Baseline not equals 0˜xk −<˜xk−1, ˜xk>˜xk−1 /= 0
if ˜xk and ˜xk−1 are linearly independent. Why?) 
2.19. Let {vi}n 
i=1 be an orthonormal basis for the n-dimensional vector space 
V. Let  x ∈V  have the representation 
x equals sigma summation b Subscript i Baseline v Subscript i Baseline periodx =
E
bivi.
Show that the Fourier coeﬃcients bi can be computed as 
b Subscript i Baseline equals left angle bracket x comma v Subscript i Baseline right angle bracket periodbi = <x, vi>.
2.20. Convex cones. 
a) I deﬁned a convex cone as a set of vectors (not necessarily a cone) 
such that for any two vectors v1, v2 in the set and for any nonneg-
ative real numbers a, b ≥ 0, av1 + bv2 is in the set. Then I stated 
that an equivalent deﬁnition requires ﬁrst that the set be a cone, 
and then includes the requirement a + b = 1 along with a, b ≥ 0 in  
the deﬁnition of a convex cone. Show that the two deﬁnitions are 
equivalent. 
b) The restriction that a + b = 1 in the deﬁnition of a convex cone 
is the kind of restriction that we usually encounter in deﬁnitions of 
convex objects. Without this restriction, it may seem that the linear 
combinations may get “outside” of the object. Show that this is not 
the case for convex cones.

Exercises
69
In particular in the two-dimensional case, show that if x = (x1, x2), 
y = (y1, y2), with x1/x2 < y1/y2 and a, b ≥ 0, then 
x 1 divided by x 2 less than or equals left parenthesis a x 1 plus b y 1 right parenthesis divided by left parenthesis a x 2 plus b y 2 right parenthesis less than or equals y 1 divided by y 2 periodx1/x2 ≤(ax1 + by1)/(ax2 + by2) ≤y1/y2.
This should also help to give a geometrical perspective on convex 
cones. 
c) Show that if C1 and C2 are convex cones over the same vector space, 
then C1 ∩ C2 is a convex cone. Give a counterexample to show that 
C1 ∪C2 is not necessarily a convex cone. 
2.21. IR3 and the cross product. 
a) Show by an example that the cross-product is not associative. 
b) For x, y ∈ IR3 , show that the area of the triangle with vertices 
(0, 0, 0), x, and  y is ||x × y||/2. 
c) For x, y, z ∈ IR3 , show that 
left angle bracket x comma y times z right angle bracket equals left angle bracket x times y comma z right angle bracket period<x, y × z> = <x × y, z>.
This is called the “triple scalar product”. 
d) For x, y, z ∈ IR3 , show that 
x times left parenthesis y times z right parenthesis equals left angle bracket x comma z right angle bracket y minus left angle bracket x comma y right angle bracket z periodx × (y × z) = <x, z>y −<x, y>z.
This is called the “triple vector product.” It is in the plane deter-
mined by y and z. 
e) The magnitude of the angle between two vectors is determined by 
the cosine, formed from the inner product. Show that in the spe-
cial case of IR3 , the angle is also determined by the sine and the 
cross-product, and show that this method can determine both the 
magnitude and the direction of the angle; that is, the way a partic-
ular vector is rotated into the other. 
f) Show that in a right-hand coordinate system, if we interpret the 
angle between ei and ej to be measured in the direction from ei to 
ej, then  e3 = e1 × e2 and e3 = −e2 × e1. 
g) Show that Eq. (2.65) is true using the hints given following the equa-
tion. 
2.22. Using equations (2.45) and  (2.66), establish Eq. (2.67). 
2.23. Show that the angle between the centered vectors xc and yc is not the 
same in general as the angle between the uncentered vectors x and y of 
the same order. 
2.24. Formally prove Eq. (2.75) (and hence Eq. (2.77)). 
2.25. Let x and y be any vectors of the same order over the same ﬁeld. 
a) Prove 
left parenthesis normal upper C normal o normal v left parenthesis x comma y right parenthesis right parenthesis squared less than or equals normal upper V left parenthesis x right parenthesis normal upper V left parenthesis y right parenthesis period(Cov(x, y))2 ≤V(x)V(y).
b) Hence, prove 
negative 1 less than or equals normal upper C normal o normal r left parenthesis x comma y right parenthesis right parenthesis less than or equals 1 period −1 ≤Cor(x, y)) ≤1.

70
2 Vectors and Vector Spaces
R Exercises 
These exercises are intended to illustrate R functions and commands and to 
show some of the steps in vector computations. In most cases, the intent is 
just to compute a quantity using the deﬁnitions given in this chapter and the 
R functions from pages 8 through 16 and pages 62 through 66. Do not  use a  
for or while loop in any exercise. 
2.26. Dot products in R. R does not have a built-in operator or function for 
the dot product of two vectors 
a) Let x = (1, 2, 3) and y = (−1, 2, −3). Use a single R statement to 
compute the dot product <x, y>. 
b) Write an R function Dot to compute the dot product of two vectors. 
Use the function deﬁnition 
Dot <- function(x, y){ 
(From Exercise 2.26a, we know this must be a very simple function!) 
Let x = (1, 2, 3) and y = (−1, 2, −3), and use your function Dot to 
compute the dot product <x, y>. 
c) Deﬁne a binary operator %.% to compute the dot product of two 
vectors. 
Let x = (1, 2, 3) and y = (−1, 2, −3), and compute the dot product 
using 
x %.% y. 
2.27. Let x = (1, 2, 3) and y = (−1, 2, −3). Use R to compute the vector cross 
product of x and y. Assume a right-hand coordinate system. 
2.28. Let x = (1, 2, 3). Use R to compute the L1, L2, and  L∞ norms of x 
directly using the deﬁnitions. 
2.29. Let x = (1, 2, 3). Use R to compute the Samelson inverse of x. 
2.30. Given x = (1, 2, 3) and y = (−1, 2, −3). 
a) Use R to determine the projection of y onto x. 
b) Use R to compute the vector cross product of x and y projected 
onto x. 
Explain your results. 
2.31. Given x = (1, 2, 3), y = (−1, 2, −3), and z = (1, 0, −3). 
a) Use R to determine three orthonormal vectors u1, u2, and  u3 that 
span the same space as x, y, and  z. 
For this exercise, just go through the steps separately for these three 
vectors. 
b) Express w = (5, 4, 3) in terms of the basis you determined in Exer-
cise 2.31a.

Exercises
71
2.32. Use R to generate a random sample of size 100 from a normal distribu-
tion with mean 0 and variance 1. Put that sample in a vector named u. 
Now generate another random sample of size 100 from the same distri-
bution into a vector named z. Next, form the vector v = u + z. 
a) Use R to compute the sample mean and variance of u and v. 
b) Use R to compute the covariance of u and v, and then use the 
covariance to compute the correlation of u and v (that is, do not 
use the cor function). 
2.33. Let x = (1, 2) and y = (4, 6, 8). Make an R list to hold these vectors. 
Use one R statement to determine the overall minimum value in both 
vectors and another statement to determine the maximum.

3 
Basic Properties of Matrices 
In this chapter, we build on the notions introduced beginning on page 4, and  
discuss a wide range of basic topics related to matrices with real elements. 
Some of the properties carry over to matrices with complex elements, but the 
reader should not assume this. Occasionally, for emphasis, we will refer to 
“real” matrices, but unless it is stated otherwise, we are assuming that the 
matrices are real. 
While reading about matrices and operations with them, it may be in-
structive to work with numerical examples. The R software system provides 
an excellent way of doing this. The appendix beginning on page 201 provides 
enough information to get started. 
The topics and the properties of matrices that we choose to discuss are 
motivated by applications in the data sciences. In Chap. 8, we will consider in 
more detail some special types of matrices that arise in regression analysis and 
multivariate data analysis, and then in Chap. 9 we will discuss some speciﬁc 
applications in statistics. 
3.1 Basic Deﬁnitions and Notation 
It is often useful to treat the rows or columns of a matrix as vectors. Terms 
such as linear independence that we have deﬁned for vectors also apply to 
rows and/or columns of a matrix. The vector space generated by the columns 
of the n times mn × m matrix A is of order n and of dimension min left parenthesis n comma m right parenthesismin(n, m) or less, and 
is called the column space of A, the  range of A, or the  manifold of A. This  
vector space is denoted by 
script upper V left parenthesis upper A right parenthesisV(A)
or 
normal s normal p normal a normal n left parenthesis upper A right parenthesis periodspan(A).
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 3 
73

74
3 Basic Properties of Matrices
I make no distinction between these two notations. The notation script upper V left parenthesis dot right parenthesisV(·) em-
phasizes that the result is a vector space. Note that if upper A element of normal upper I normal upper R Superscript n times mA ∈IRn×m, then  
script upper V left parenthesis upper A right parenthesis subset of or equal to normal upper I normal upper R Superscript nV(A) ⊆IRn. 
The argument of script upper V left parenthesis dot right parenthesisV(·) or normal s normal p normal a normal n left parenthesis dot right parenthesisspan(·) can also be a set of vectors instead of a 
matrix. Recall from Sect. 2.1.3 that if G is a set of vectors, the symbol normal s normal p normal a normal n left parenthesis upper G right parenthesisspan(G)
denotes the vector space generated by the vectors in G. 
We also deﬁne the row space of A to be the vector space of order m (and 
of dimension min left parenthesis n comma m right parenthesismin(n, m) or less) generated by the rows of A; notice, however, 
the preference given to the column space. 
Many of the properties of matrices that we discuss hold for matrices with 
an inﬁnite number of elements, but throughout this book, we will assume that 
the matrices have a ﬁnite number of elements, and hence, the vector spaces 
are of ﬁnite order and have a ﬁnite number of dimensions. 
3.1.1 Multiplication of a Matrix by a Scalar 
Similar to our deﬁnition of multiplication of a vector by a scalar, we deﬁne 
the multiplication of a matrix A by a scalar c as 
c upper A equals left parenthesis c a Subscript i j Baseline right parenthesis periodcA = (caij).
(3.1) 
We will later deﬁne multiplication of a matrix and a vector, and various kinds 
of multiplication of a matrix and a matrix. 
3.1.2 Symmetric and Hermitian Matrices 
In the matrix A with elements a Subscript i jaij, if  a Subscript i j Baseline equals a Subscript j iaij = aji for all i and j, A is said to be 
symmetric. A symmetric matrix is necessarily square. 
A matrix  A such that a Subscript i j Baseline equals minus a Subscript j iaij = −aji is said to be skew symmetric or antisym-
metric. (There is some ambiguity in the use of the terms “skew” and “anti”.) 
Obviously, the diagonal entries of a skew symmetric matrix must be 0. 
If a Subscript i j Baseline equals a overbar Subscript j iaij = ¯aji (where a overbar¯a represents the conjugate of the complex number a), A 
is said to be Hermitian. A Hermitian matrix is also necessarily square with real 
elements on the diagonal. A real symmetric matrix is Hermitian. A Hermitian 
matrix is also called a self-adjoint matrix. (“Adjoint” is an ambiguous term, 
but here refers to the conjugate transpose; see page 78.) 
3.1.3 Diagonal Elements 
The a Subscript i iaii elements of a matrix are called diagonal elements or principal diagonal 
elements. An element  a Subscript i jaij with i less than ji < j is said to be “above the diagonal,” and 
one with i greater than ji > j is said to be “below the diagonal.” The vector consisting of all 
of the a Subscript i iaiis is called the  principal diagonal or just the diagonal. 
Visualizing a matrix as a rectangle, the diagonal is from the top left to 
the bottom right. Another diagonal direction is from bottom left to top right.

3.1 Basic Deﬁnitions and Notation
75
These elements or the vector consisting of them is called the skew diagonal or, 
sometimes, the anti-diagonal. (As mentioned above, there is some ambiguity 
in the use of the terms “skew” and “anti.”) 
These terms, diagonal, principal diagonal, skew diagonal, or anti-diagonal 
apply whether or not the matrix is square, but they may require clariﬁcation 
if the matrix is not square. For the n times mn × m matrix A, for example, it is more 
common to deﬁne the skew principal diagonal elements as a Subscript n Baseline 1 Baseline comma a Subscript n minus 1 comma 2 Baseline comma ellipsis comma ban1, an−1,2, . . . , b, 
where b equals a Subscript 1 comma nb = a1,n if n less than or equals mn ≤m and b equals a Subscript n minus m plus 1 comma mb = an−m+1,m if n greater than mn > m. The ordering beginning 
with a Subscript n Baseline 1an1 is more common than an ordering beginning with a Subscript 1 ma1m and proceeding 
downward to a Subscript 2 comma m minus 1a2,m−1, and so on, which would involve diﬀerent elements if 
n not equals mn /= m. 
3.1.4 Diagonally Dominant Matrices 
An n times mn × m matrix A for which 
StartAbsoluteValue a Subscript i i Baseline EndAbsoluteValue greater than sigma summation Underscript j not equals i Overscript m Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue for each i equals 1 comma ellipsis comma n|aii| >
m
E
j/=i
|aij|
for each i = 1, . . . , n
(3.2) 
is said to be row diagonally dominant; and  a matrix  A for which StartAbsoluteValue a Subscript j j Baseline EndAbsoluteValue greater than sigma summation Underscript i not equals j Overscript n Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue|ajj| >
En
i/=j |aij| for each j equals 1 comma ellipsis comma mj = 1, . . . , m is said to be column diagonally dominant. 
(Some authors refer to this as strict diagonal dominance and use “diagonal 
dominance” without qualiﬁcation to allow the possibility that the inequalities 
in the deﬁnitions are not strict.) Most interesting properties of such matrices 
hold whether the dominance is by row or by column. 
If A is symmetric, row and column diagonal dominances are equivalent, so 
we refer to row or column diagonally dominant symmetric matrices without 
the qualiﬁcation; that is, as just diagonally dominant. 
3.1.5 Diagonal and Hollow Matrices 
If all except the principal diagonal elements of a matrix are 0, the matrix is 
called a diagonal matrix. A diagonal matrix is the most common and most 
important type of “sparse matrix.” If all of the principal diagonal elements 
of a matrix are 0, the matrix is called a hollow matrix. A skew symmetric, 
or antisymmetric, matrix is hollow. If all except the principal skew diagonal 
elements of a matrix are 0, the matrix is called a skew diagonal matrix. 
3.1.6 Matrices with Other Special Patterns of Zeroes 
If all elements below the diagonal are 0, the matrix is called an upper triangular 
matrix; and  a  lower triangular matrix is deﬁned similarly. If all elements of a 
column or row of a triangular matrix are zero, we still refer to the matrix as 
triangular, although sometimes we speak of its form as trapezoidal. Another

76
3 Basic Properties of Matrices
form called trapezoidal is one in which there are more columns than rows, 
and the additional columns are possibly nonzero. The four general forms of 
triangular or trapezoidal matrices are shown below, using an intuitive notation 
with X and 0 to indicate the pattern. 
Start 3 By 3 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X EndMatrix Start 3 By 3 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 EndMatrix Start 4 By 3 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 EndMatrix Start 3 By 4 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Column monospace upper X EndMatrix
⎡
⎣
X X X
0 X X
0 0 X
⎤
⎦
⎡
⎣
X X X
0 X X
0 0 0
⎤
⎦
⎡
⎢⎢⎣
X X X
0 X X
0 0 X
0 0 0
⎤
⎥⎥⎦
⎡
⎣
X X X X
0 X X X
0 0 X X
⎤
⎦
In this notation, X indicates that the element is possibly not zero. It does 
not mean each element is the same. In some cases, X and 0 may indicate 
“submatrices,” which we discuss in the section on partitioned matrices. 
If all elements are 0 except a Subscript i comma i plus c Sub Subscript k Subscriptai,i+ck for some small number of integers c Subscript kck, 
the matrix is called a band matrix (or banded matrix). In many applications, 
c Subscript k Baseline element of StartSet minus w Subscript l Baseline comma minus w Subscript l Baseline plus 1 comma ellipsis comma negative 1 comma 0 comma 1 comma ellipsis comma w Subscript u Baseline minus 1 comma w Subscript u Baseline EndSetck ∈{−wl, −wl + 1, . . . , −1, 0, 1, . . . , wu −1, wu}. In such a case,  w Subscript lwl is called 
the lower band width and w Subscript uwu is called the upper band width. These patterned 
matrices arise in time series and other stochastic process models as well as 
in solutions of diﬀerential equations, and so they are very important in cer-
tain applications. Computations generally involve special algorithms to take 
advantage of the structure. 
Although it is often the case that interesting band matrices are symmetric, 
or at least have the same number of codiagonals that are nonzero, neither of 
these conditions always occurs in applications of band matrices. If all elements 
below the principal skew diagonal elements of a matrix are 0, the matrix is 
called a skew upper triangular matrix. A common form of Hankel matrix, 
for example, is the skew upper triangular matrix (see page 427). Notice that 
the various terms deﬁned here, such as triangular and band, also apply to 
nonsquare matrices. 
Band matrices occur often in numerical solutions of partial diﬀerential 
equations. A band matrix with lower and upper band widths of 1 is a tridi-
agonal matrix. If all diagonal elements and all elements a Subscript i comma i plus or minus 1ai,i±1 are nonzero, a 
tridiagonal matrix is called a “matrix of type 2”. The inverse of a covariance 
matrix that occurs in common stationary time series models is a matrix of 
type 2 (see page 422). 
Using the intuitive notation of X and 0 as above, a band matrix may be 
written as 
Start 5 By 6 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 2nd Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 3rd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 4th Row 1st Column Blank 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column down right diagonal ellipsis 5th Column Blank 5th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X EndMatrix period
⎡
⎢⎢⎢⎢⎢⎣
X X 0 · · · 0 0
X X X · · · 0 0
0 X X · · · 0 0
...
...
0 0 0 · · · X X
⎤
⎥⎥⎥⎥⎥⎦
.
Computational methods for matrices may be more eﬃcient if the patterns are 
taken into account.

3.1 Basic Deﬁnitions and Notation
77
A matrix is in upper Hessenberg form and is called a Hessenberg matrix, if  
it is upper triangular except for the ﬁrst subdiagonal, which may be nonzero. 
That is, a Subscript i j Baseline equals 0aij = 0 for i greater than j plus 1i > j + 1: 
Start 6 By 6 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X 2nd Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X 5th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column Blank 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 6th Column vertical ellipsis 6th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X EndMatrix period
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
X X X · · · X X
X X X · · · X X
0 X X · · · X X
0 0 X · · · X X
... ...
... ... ...
0 0 0 · · · X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
.
A symmetric matrix that is in Hessenberg form is necessarily tridiagonal. 
Hessenberg matrices arise in some methods for computing eigenvalues (see 
Chap. 6). 
Many matrices of interest are sparse; that is, they have a large propor-
tion of elements that are 0. The matrices discussed above are generally not 
considered sparse. (“A large proportion” is subjective, but generally means 
more than 75%, and in many interesting cases is well over 95%.) Eﬃcient and 
accurate computations often require that the sparsity of a matrix be accom-
modated explicitly. 
3.1.7 Matrix Shaping Operators 
In order to perform certain operations on matrices and vectors, it is often 
useful ﬁrst to reshape a matrix. The most common reshaping operation is 
the transpose, which we deﬁne in this section. Sometimes we may need to 
rearrange the elements of a matrix or form a vector into a special matrix. In 
this section, we deﬁne three operators for doing this. 
Transpose 
Given an n times mn×m matrix A with elements a Subscript i jaij, the  m times nm×n matrix with elements a Subscript j iaji
is called the transpose of A. We use a superscript “normal upper TT” to denote the transpose 
of a matrix; thus, if upper A equals left parenthesis a Subscript i j Baseline right parenthesisA = (aij), then  
upper A Superscript normal upper T Baseline equals left parenthesis a Subscript j i Baseline right parenthesis periodAT = (aji).
(3.3) 
In other literature, especially on statistical applications, the transpose is often 
denoted by a prime, as in upper A prime equals left parenthesis a Subscript j i Baseline right parenthesisA' = (aji). 
The transpose of a matrix is the matrix whose i normal t normal hith row is the i normal t normal hith column 
of the original matrix and whose j normal t normal hjth column is the j normal t normal hjth row of the original 
matrix. 
We note immediately that 
left parenthesis upper A Superscript normal upper T Baseline right parenthesis Superscript normal upper T Baseline equals upper A period(AT)T = A.
(3.4)

78
3 Basic Properties of Matrices
Conjugate Transpose 
If the elements of the matrix have imaginary components, the conjugate trans-
pose or Hermitian transpose is more useful than the transpose. 
We use a superscript “normal upper HH” to denote the conjugate transpose of a matrix, 
and deﬁne the conjugate transpose of the matrix upper A equals left parenthesis a Subscript i j Baseline right parenthesisA = (aij) as 
upper A Superscript normal upper H Baseline equals left parenthesis a overbar Subscript j i Baseline right parenthesis periodAH = (¯aji).
(3.5) 
The conjugate transpose is often denoted by an asterisk, as in upper A Superscript asterisk Baseline equals left parenthesis a overbar Subscript j i Baseline right parenthesis equals upper A Superscript normal upper HA∗= (¯aji) =
AH. (This notation is more common if a prime is used to denote the transpose. 
We also sometimes use the notation upper A Superscript asteriskA∗to denote a g 12g12 inverse of the matrix 
A; see page 149.) We also use similar notations for vectors. 
The conjugate transpose is also called the adjoint. (“Adjoint” is also used 
to denote another type of matrix, so we will generally avoid using that term. 
This meaning of the word is the origin of another term for a Hermitian matrix, 
a “self-adjoint matrix”.) 
Properties 
As with the transpose, left parenthesis upper A Superscript normal upper H Baseline right parenthesis Superscript normal upper H Baseline equals upper A(AH)H = A. If (and only if) all of the elements of A 
are real, then upper A Superscript normal upper H Baseline equals upper A Superscript normal upper TAH = AT. 
If (and only if) A is symmetric, upper A equals upper A Superscript normal upper TA = AT; if (and only if)  A is skew sym-
metric, upper A Superscript normal upper T Baseline equals negative upper AAT = −A; and if (and only if) A is Hermitian, upper A equals upper A Superscript normal upper HA = AH (and, in that 
case, all of the diagonal elements are real). 
Diagonals of Matrices and Diagonal Vectors: vecdiag(·) or diag(·) 
We denote the principal diagonal of a matrix A by normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A right parenthesisvecdiag(A). The vecdiag 
function deﬁned here is a mapping normal upper I normal upper R Superscript n times m Baseline right arrow normal upper I normal upper R Superscript min left parenthesis n comma m right parenthesisIRn×m →IRmin(n,m). 
There is a related mapping normal upper I normal upper R Superscript n Baseline right arrow normal upper I normal upper R Superscript n times nIRn →IRn×n that is denoted by normal d normal i normal a normal g left parenthesis v right parenthesisdiag(v), for  
a vector v, which we deﬁne below in Eq. (3.8). Some authors and software 
systems (R, for example) use “normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·)” to denote both normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) and normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·)
as above, with the ambiguity being resolved by the type of the argument. I will 
generally use the notation “normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·)”, but occasionally I may use “normal d normal i normal a normal g left parenthesis upper A right parenthesisdiag(A)” 
when I mean “normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·)” because of my frequent use of R. The meaning is 
clear from the type of the argument. The argument of normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·) is a matrix 
and the argument of normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) is a vector usually, but if the argument in normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·)
is a matrix, then it should be interpreted as normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·). 
If A is an n times mn × m matrix, and k equals min left parenthesis n comma m right parenthesisk = min(n, m), 
normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A right parenthesis equals left parenthesis a 11 comma ellipsis comma a Subscript k k Baseline right parenthesis periodvecdiag(A) = (a11, . . . , akk).
(3.6)

3.1 Basic Deﬁnitions and Notation
79
Note from the deﬁnition that 
normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A right parenthesis commavecdiag(AT) = vecdiag(A),
(3.7) 
and this is true whether or not A is square. 
The diagonal begins in the ﬁrst row and ﬁrst column (that is, a 11a11), and 
ends at a Subscript k kakk, where  k is the minimum of the number of rows and the number 
of columns. 
For c equals plus or minus 1 comma ellipsisc = ±1, . . ., the elements a Subscript i comma i plus cai,i+c are called “codiagonals” or “minor 
diagonals.” The codiagonals a Subscript i comma i plus 1ai,i+1 are called “supradiagonals,” and the codi-
agonals a Subscript i comma i minus 1ai,i−1 are called “infradiagonals.” 
If the matrix has m columns, the a Subscript i comma m plus 1 minus iai,m+1−i elements of the matrix are called 
skew diagonal elements. We use terms similar to those for diagonal elements 
for elements above and below the skew diagonal elements. These phrases are 
used with both square and nonsquare matrices. 
The Diagonal Matrix Constructor Function diag(·) 
A square diagonal matrix can be speciﬁed by a constructor function that 
operates on a vector and forms a diagonal matrix with the elements of the 
vector along the diagonal. We denote that constructor function by normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·), 
normal d normal i normal a normal g left parenthesis left parenthesis d 1 comma d 2 comma ellipsis comma d Subscript n Baseline right parenthesis right parenthesis equals Start 4 By 4 Matrix 1st Row 1st Column d 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 2nd Row 1st Column 0 2nd Column d 2 3rd Column midline horizontal ellipsis 4th Column 0 3rd Row 1st Column Blank 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column Blank 4th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column d Subscript n Baseline EndMatrix perioddiag
(
(d1, d2, . . . , dn)
)
=
⎡
⎢⎢⎢⎣
d1 0 · · · 0
0 d2 · · · 0
...
0 0 · · · dn
⎤
⎥⎥⎥⎦.
(3.8) 
(Notice that the argument of diag here is a vector; that is why there are 
two sets of parentheses in the expression above, although sometimes we omit 
one set without loss of clarity.) The diag function deﬁned here is a mapping 
normal upper I normal upper R Superscript n Baseline right arrow normal upper I normal upper R Superscript n times nIRn →IRn×n. On page 82, we will extend this deﬁnition slightly to refer to 
matrices. 
As mentioned above, normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) is sometimes overloaded to be the same 
as normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·) in Eq. (3.6) with a single matrix argument. This means that 
normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·) is the same as normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) when the argument is a matrix. Both the 
R and MATLAB computing systems, for example, use this overloading; that 
is, they each provide a single function, called diag in each case. (There is a 
vecdiag function in the lazy.mat R package.) 
Note that normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) and normal v normal e normal c normal d normal i normal a normal g left parenthesis dot right parenthesisvecdiag(·) are the inverses of each other; that is, if 
v is a vector, 
normal v normal e normal c normal d normal i normal a normal g left parenthesis normal d normal i normal a normal g left parenthesis v right parenthesis right parenthesis equals v commavecdiag(diag(v)) = v,
(3.9) 
and if A is a square diagonal matrix, 
normal d normal i normal a normal g left parenthesis normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A right parenthesis right parenthesis equals upper A perioddiag(vecdiag(A)) = A.
(3.10) 
The overloaded normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) is its own inverse over square diagonal matrices of ﬁxed 
size.

80
3 Basic Properties of Matrices
Forming a Vector from the Elements of a Matrix: vec(·) and  
vech(·) 
It is sometimes useful to consider the elements of a matrix to be elements of 
a single vector. The most common way this is done is to string the columns 
of the matrix end-to-end into a vector. The normal v normal e normal c left parenthesis dot right parenthesisvec(·) function does this. For the 
n times mn × m matrix A, 
normal v normal e normal c left parenthesis upper A right parenthesis equals left parenthesis a 11 comma a 21 comma ellipsis comma a Subscript n Baseline 1 Baseline comma a 12 comma ellipsis comma a Subscript 1 m Baseline comma ellipsis comma a Subscript n m Baseline right parenthesis periodvec(A) = (a11, a21, . . . , an1, a12, . . . , a1m, . . . , anm).
(3.11) 
The vec function is a mapping normal upper I normal upper R Superscript n times m Baseline right arrow normal upper I normal upper R Superscript n mIRn×m →IRnm. The vec function is also some-
times called the “pack” function. 
For an n times nn × n symmetric matrix A with elements a Subscript i jaij, the “vech” function 
stacks the unique elements into a vector: 
normal v normal e normal c normal h left parenthesis upper A right parenthesis equals left parenthesis a 11 comma a 21 comma ellipsis comma a Subscript n Baseline 1 Baseline comma a 22 comma ellipsis comma a Subscript n Baseline 2 Baseline comma ellipsis comma a Subscript n n Baseline right parenthesis periodvech(A) = (a11, a21, . . . , an1, a22, . . . , an2, . . . , ann).
(3.12) 
There are other ways that the unique elements could be stacked that would 
be simpler and perhaps more useful (see the discussion of symmetric storage 
mode on page 617), but Eq. (3.12) is the standard deﬁnition of vechleft parenthesis dot right parenthesis(·). The  
vech function is a mapping normal upper I normal upper R Superscript n times n Baseline right arrow normal upper I normal upper R Superscript n left parenthesis n plus 1 right parenthesis divided by 2IRn×n →IRn(n+1)/2. 
3.1.8 Partitioned Matrices and Submatrices 
We often ﬁnd it useful to partition a matrix into submatrices. In many appli-
cations in data analysis, for example, it is convenient to work with submatrices 
of various types that represent diﬀerent subsets of the data. 
Notation 
We usually denote the submatrices with capital letters with subscripts indi-
cating the relative positions of the submatrices. Hence, we may write 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[
A11 A12
A21 A22
]
,
(3.13) 
where the matrices upper A 11A11 and upper A 12A12 have  the same number of rows,  upper A 21A21 and 
upper A 22A22 have  the same number of rows,  upper A 11A11 and upper A 21A21 have  the same number of  
columns, and upper A 12A12 and upper A 22A22 have the same number of columns. Of course, the 
submatrices in a partitioned matrix may be denoted by diﬀerent letters. Also, 
for clarity, sometimes we use a vertical bar to indicate a partition: 
upper A equals left bracket upper B vertical bar upper C right bracket periodA = [ B | C ].
(3.14) 
The vertical bar is used just for clarity and has no special meaning in this 
representation.

3.1 Basic Deﬁnitions and Notation
81
A submatrix may consist of nonadjacent rows and columns of the orig-
inal matrix, and we can adopt a notation that identiﬁes those rows and 
columns. For example, upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis left parenthesis j 1 comma ellipsis comma j Sub Subscript l Subscript right parenthesisA(i1,...,ik)(j1,...,jl) denotes the submatrix of A formed 
by rows i 1 comma ellipsis comma i Subscript k Baselinei1, . . . , ik and columns j 1 comma ellipsis comma j Subscript l Baselinej1, . . . , jl. When the entire rows are included, 
upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis left parenthesis asterisk right parenthesisA(i1,...,ik)(∗) denotes the submatrix of A formed from rows i 1 comma ellipsis comma i Subscript k Baselinei1, . . . , ik; and  
upper A Subscript left parenthesis asterisk right parenthesis left parenthesis j 1 comma ellipsis comma j Sub Subscript l Subscript right parenthesisA(∗)(j1,...,jl) denotes the submatrix formed from columns j 1 comma ellipsis comma j Subscript l Baselinej1, . . . , jl with el-
ements from all rows. 
We also use a related notation to denote a submatrix by listing the rows 
and columns that are deleted from the original matrix. Thus, 
upper A Subscript minus left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis left parenthesis j 1 comma ellipsis comma j Sub Subscript l Subscript right parenthesisA−(i1,...,ik)(j1,...,jl)
(3.15) 
denotes the submatrix of A formed by deleting rows i 1 comma ellipsis comma i Subscript k Baselinei1, . . . , ik and columns 
j 1 comma ellipsis comma j Subscript l Baselinej1, . . . , jl from A. In certain applications, it is common to remove a single row 
and column, and so this notation often occurs in the form upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesisA−(i)(j). This sub-
matrix is called the “complementary minor” of the a Subscript i jaij element of A. It occurs 
often in working with determinants, as we will see in the section beginning on 
page 89. 
As we have mentioned previously, we use a Subscript i asteriskai∗to denote the vector whose 
elements correspond to those in the i normal t normal hith row of the matrix A. We sometimes 
emphasize that it is a vector by writing it in the form a Subscript i asterisk Superscript normal upper TaT
i∗. Likewise, a Subscript asterisk ja∗j denotes 
the vector whose elements correspond to those in the j normal t normal hjth column of A. See  
page 663 for a summary of this notation. This kind of subsetting is often done 
in data analysis, for example, in variable selection in linear regression analysis. 
Block Diagonal Matrices 
A square submatrix whose principal diagonal elements are elements of the 
principal diagonal of the given matrix is called a principal submatrix. If  upper A 11A11
in the example above is square, it is a principal submatrix, and if upper A 22A22 is square, 
it is also a principal submatrix. Sometimes the term “principal submatrix” is 
restricted to square submatrices. If a matrix is diagonally dominant, then it 
is clear that any principal submatrix of it is also diagonally dominant. 
A principal submatrix that contains the left parenthesis 1 comma 1 right parenthesis(1, 1) element and whose rows 
and columns are contiguous in the original matrix is called a leading principal 
submatrix. If  upper A 11A11 is square, it is a leading principal submatrix in the example 
above. 
Partitioned matrices may have useful patterns. A “block diagonal” matrix 
is one of the form 
Start 4 By 4 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace 0 3rd Column midline horizontal ellipsis 4th Column monospace 0 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column midline horizontal ellipsis 4th Column monospace 0 3rd Row 1st Column Blank 2nd Column down right diagonal ellipsis 3rd Column Blank 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column midline horizontal ellipsis 4th Column monospace upper X EndMatrix comma
⎡
⎢⎢⎢⎣
X 0 · · · 0
0 X · · · 0
...
0 0 · · · X
⎤
⎥⎥⎥⎦,
where 0 represents a submatrix with all zeros and X represents a general 
submatrix with at least some nonzeros.

82
3 Basic Properties of Matrices
The diag(·) Matrix Function and the Direct Sum 
The normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) function previously introduced for a vector is also deﬁned for a 
list of matrices: 
normal d normal i normal a normal g left parenthesis upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript k Baseline right parenthesisdiag(A1, A2, . . . , Ak)
(3.16) 
denotes the block diagonal matrix with submatrices upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript k BaselineA1, A2, . . . , Ak along the 
diagonal and zeros elsewhere. A matrix formed in this way is sometimes called 
a direct sum of upper A 1 comma upper A 2 comma ellipsis comma upper A Subscript k BaselineA1, A2, . . . , Ak, and the operation is denoted by circled plus⊕: 
upper A 1 circled plus midline horizontal ellipsis circled plus upper A Subscript k Baseline equals normal d normal i normal a normal g left parenthesis upper A 1 comma ellipsis comma upper A Subscript k Baseline right parenthesis periodA1 ⊕· · · ⊕Ak = diag(A1, . . . , Ak).
(3.17) 
(The R and MATLAB functions named diag, referred to earlier, do not im-
plement the operation in (3.16) above.)  
Although the direct sum is a binary operation, we are justiﬁed in deﬁning 
it for a list of matrices because the operation is clearly associative. 
The upper A Subscript iAi may be of diﬀerent sizes, and they may not be square, although in 
most applications the matrices are square (and some authors deﬁne the direct 
sum only for square matrices). 
We will deﬁne vector spaces of matrices below and then recall the deﬁnition 
of a direct sum of vector spaces (page 28), which is diﬀerent from the direct 
sum deﬁned above in terms of normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·). 
Transposes of Partitioned Matrices 
The transpose of a partitioned matrix is formed in the obvious way; for ex-
ample, 
Start 2 By 3 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 3rd Column upper A 13 2nd Row 1st Column upper A 21 2nd Column upper A 22 3rd Column upper A 23 EndMatrix Superscript normal upper T Baseline equals Start 3 By 2 Matrix 1st Row 1st Column upper A 11 Superscript normal upper T Baseline 2nd Column upper A 21 Superscript normal upper T Baseline 2nd Row 1st Column upper A 12 Superscript normal upper T Baseline 2nd Column upper A 22 Superscript normal upper T Baseline 3rd Row 1st Column upper A 13 Superscript normal upper T Baseline 2nd Column upper A 23 Superscript normal upper T Baseline EndMatrix period
[
A11 A12 A13
A21 A22 A23
]T
=
⎡
⎣
AT
11 AT
21
AT
12 AT
22
AT
13 AT
23
⎤
⎦.
(3.18) 
3.1.9 Matrix Addition 
The sum of two matrices of the same shape is the matrix whose elements 
are the sums of the corresponding elements of the addends. As in the case of 
vector addition, we overload the usual symbols for the operations on the reals 
to signify the corresponding operations on matrices when the operations are 
deﬁned; hence, addition of matrices is also indicated by “plus+”, as with scalar 
addition and vector addition. We assume throughout that writing a sum of 
matrices upper A plus upper BA+B implies that they are of the same shape; that is, that they are 
conformable for addition. 
The “+” operator can also mean addition of a scalar to a matrix, as in 
upper A plus aA + a, where  A is a matrix and a is a scalar. We will use “+” to mean the 
addition of the scalar to each element of the matrix, resulting in a matrix 
of the same shape. This meaning is consistent with the semantics of modern 
computer languages such as Fortran and R.

3.1 Basic Deﬁnitions and Notation
83
The addition of two n times mn×m matrices or the addition of a scalar to an n times mn×m
matrix requires nm scalar additions. 
The matrix additive identity is a matrix with all elements zero. We some-
times denote such a matrix with n rows and m columns as 0 Subscript n times m0n×m, or just as 0.  
We may denote a square additive identity as 0 Subscript n0n. 
The Transpose of the Sum of Matrices 
The transpose of the sum of two matrices is the sum of the transposes: 
left parenthesis upper A plus upper B right parenthesis Superscript normal upper T Baseline equals upper A Superscript normal upper T Baseline plus upper B Superscript normal upper T Baseline period(A + B)T = AT + BT.
(3.19) 
The sum of two symmetric matrices is therefore symmetric. 
Rank Ordering Matrices 
There are several possible ways to form a rank ordering of matrices of the 
same shape, but no complete ordering is entirely satisfactory. If all of the 
elements of the matrix A are positive, we write 
upper A greater than 0 semicolonA > 0;
(3.20) 
if all of the elements are nonnegative, we write 
upper A greater than or equals 0 periodA ≥0.
(3.21) 
The terms “positive” and “nonnegative” and these symbols are not to be 
confused with the terms “positive deﬁnite” and “nonnegative deﬁnite” and 
similar symbols for important classes of matrices having diﬀerent properties 
(which we will introduce on page 113, and discuss further in Sect. 8.3.) 
Vector Spaces of Matrices 
Having deﬁned scalar multiplication and matrix addition (for conformable 
matrices), we can deﬁne a vector space of n times mn × m matrices as any set that is 
closed with respect to those operations. The individual operations of scalar 
multiplication and matrix addition allow us to deﬁne an axpy operation on 
the matrices, as in Eq. (2.1) on page 20. Closure of this space implies that 
it must contain the additive identity, just as we saw on page 22. The  matrix  
additive identity is the 0 matrix. 
As with any vector space, we have the concepts of linear independence, 
generating set or spanning set, basis set, essentially disjoint spaces, and direct 
sums of matrix vector spaces (as in Eq. (2.13), which is diﬀerent from the 
direct sum of matrices deﬁned in terms of normal d normal i normal a normal g left parenthesis dot right parenthesisdiag(·) as in Eq. (3.17)). 
An important vector space of matrices contains the full set of n times mn×m matri-
ces, that is, normal upper I normal upper R Superscript n times mIRn×m. For matrices upper X comma upper Y element of normal upper I normal upper R Superscript n times mX, Y ∈IRn×m and a element of normal upper I normal upper Ra ∈IR, the axpy operation 
is a upper X plus upper YaX + Y .

84
3 Basic Properties of Matrices
If n greater than or equals mn ≥m, a set of  nm n times mn×m matrices whose columns consist of all combina-
tions of a set of n n-vectors that span normal upper I normal upper R Superscript nIRn is a basis set for normal upper I normal upper R Superscript n times mIRn×m. If  n less than mn < m, 
we can likewise form a basis set for normal upper I normal upper R Superscript n times mIRn×m or for subspaces of normal upper I normal upper R Superscript n times mIRn×m in a 
similar way. If StartSet upper B 1 comma ellipsis comma upper B Subscript k Baseline EndSet{B1, . . . , Bk} is a basis set for normal upper I normal upper R Superscript n times mIRn×m, then any  n times mn × m matrix 
can be represented as sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline upper B Subscript iEk
i=1 ciBi. Subsets of a basis set generate subspaces of 
normal upper I normal upper R Superscript n times mIRn×m. 
Because the sum of two symmetric matrices is symmetric, and a scalar 
multiple of a symmetric matrix is likewise symmetric, we have a vector space 
of the n times nn × n symmetric matrices. This is clearly a subspace of the vector 
space normal upper I normal upper R Superscript n times nIRn×n. All vectors in any basis for this vector space must be symmetric. 
Using a process similar to our development of a basis for a general vector 
space of matrices, we see that there are n left parenthesis n plus 1 right parenthesis divided by 2n(n + 1)/2 matrices in the basis (see 
Exercise 3.2). 
3.1.10 The Trace of a Square Matrix 
There are several useful mappings from matrices to real numbers; that is, from 
normal upper I normal upper R Superscript n times mIRn×m to normal upper I normal upper RIR. Some important ones are various norms, which are similar to 
vector norms, and which we will consider in Sect. 3.11. In this section and the 
next, we deﬁne two scalar-valued operators that apply just to square matrices, 
the trace and the determinant. 
The Trace: tr(·) 
The sum of the diagonal elements of a square matrix is called the trace of the 
matrix. We use the notation “tr(A)” to denote the trace of the matrix A: 
normal t normal r left parenthesis upper A right parenthesis equals sigma summation Underscript i Endscripts a Subscript i i Baseline periodtr(A) =
E
i
aii.
(3.22) 
The Trace of the Transpose of Square Matrices 
From the deﬁnition, we see 
normal t normal r left parenthesis upper A right parenthesis equals normal t normal r left parenthesis upper A Superscript normal upper T Baseline right parenthesis periodtr(A) = tr(AT).
(3.23) 
The Trace of Scalar Products of Square Matrices 
For a scalar c and an n times nn × n matrix A, 
normal t normal r left parenthesis c upper A right parenthesis equals c normal t normal r left parenthesis upper A right parenthesis periodtr(cA) = c tr(A).
This follows immediately from the deﬁnition because for normal t normal r left parenthesis c upper A right parenthesistr(cA) each diagonal 
element is multiplied by c.

3.2 The Determinant
85
The Trace of Partitioned Square Matrices 
If the square matrix A is partitioned such that the diagonal blocks are square 
submatrices, that is, 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[
A11 A12
A21 A22
]
,
(3.24) 
where upper A 11A11 and upper A 22A22 are square, then from the deﬁnition, we see that 
normal t normal r left parenthesis upper A right parenthesis equals normal t normal r left parenthesis upper A 11 right parenthesis plus normal t normal r left parenthesis upper A 22 right parenthesis periodtr(A) = tr(A11) + tr(A22).
(3.25) 
The Trace of the Sum of Square Matrices 
If A and B are square matrices of the same order, a useful (and obvious) 
property of the trace is 
normal t normal r left parenthesis upper A plus upper B right parenthesis equals normal t normal r left parenthesis upper A right parenthesis plus normal t normal r left parenthesis upper B right parenthesis periodtr(A + B) = tr(A) + tr(B).
(3.26) 
3.2 The Determinant 
The determinant, like the trace, is a mapping from normal upper I normal upper R Superscript n times nIRn×n to normal upper I normal upper RIR. Although 
it may not be obvious from the deﬁnition below, the determinant has far-
reaching applications in matrix theory. 
3.2.1 Deﬁnition and Simple Properties 
For an n times nn × n (square) matrix A, consider the product a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baselinea1j1 · · · anjn, where  
pi Subscript j Baseline equals left parenthesis j 1 comma ellipsis comma j Subscript n Baseline right parenthesisπj = (j1, . . . , jn) is one of the n factorialn! permutations of the integers from 1 to n. 
Deﬁne a permutation to be even or odd according to the number of times 
that a smaller element follows a larger one in the permutation. For example, 
given the tuple left parenthesis 1 comma 2 comma 3 right parenthesis(1, 2, 3), then  left parenthesis 1 comma 3 comma 2 right parenthesis(1, 3, 2) is an odd permutation, and left parenthesis 3 comma 1 comma 2 right parenthesis(3, 1, 2) and 
left parenthesis 1 comma 2 comma 3 right parenthesis(1, 2, 3) are even permutations (0 is even). Let 
sigma left parenthesis pi right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column 1 2nd Column normal i normal f pi is an even permutation 2nd Row 1st Column negative 1 2nd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e period EndLayoutσ(π) =
(
1 if π is an even permutation
−1 otherwise.
(3.27) 
Then the determinant of A, denoted by normal d normal e normal t left parenthesis upper A right parenthesisdet(A), is deﬁned by 
normal d normal e normal t left parenthesis upper A right parenthesis equals sigma summation Underscript normal a normal l normal l normal p normal e normal r normal m normal u normal t normal a normal t normal i normal o normal n normal s pi Subscript j Baseline Endscripts sigma left parenthesis pi Subscript j Baseline right parenthesis a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baseline perioddet(A) =
E
all permutations πj
σ(πj)a1j1 · · · anjn.
(3.28) 
This simple function has many remarkable relationships to various prop-
erties of matrices. 
The deﬁnition of the determinant is not as daunting as it may appear 
at ﬁrst glance. Many properties become obvious when we realize that sigma left parenthesis dot right parenthesisσ(·)
is always plus or minus 1±1, and it can be built up by elementary exchanges of adjacent

86
3 Basic Properties of Matrices
elements. For example, consider sigma left parenthesis 3 comma 2 comma 1 right parenthesisσ(3, 2, 1). There are two ways we can use 
three elementary exchanges, each beginning with the natural ordering: 
left parenthesis 1 comma 2 comma 3 right parenthesis right arrow left parenthesis 2 comma 1 comma 3 right parenthesis right arrow left parenthesis 2 comma 3 comma 1 right parenthesis right arrow left parenthesis 3 comma 2 comma 1 right parenthesis comma(1, 2, 3) →(2, 1, 3) →(2, 3, 1) →(3, 2, 1),
or 
left parenthesis 1 comma 2 comma 3 right parenthesis right arrow left parenthesis 1 comma 3 comma 2 right parenthesis right arrow left parenthesis 3 comma 1 comma 2 right parenthesis right arrow left parenthesis 3 comma 2 comma 1 right parenthesis semicolon(1, 2, 3) →(1, 3, 2) →(3, 1, 2) →(3, 2, 1);
hence, either way, sigma left parenthesis 3 comma 2 comma 1 right parenthesis equals left parenthesis negative 1 right parenthesis cubed equals negative 1σ(3, 2, 1) = (−1)3 = −1. 
If pi Subscript jπj consists of the interchange of exactly two elements in the index vector 
left parenthesis 1 comma ellipsis comma n right parenthesis(1, . . . , n), say elements p and q with p less than qp < q, then after the interchange there 
are q minus pq −p elements before p that are larger than p, and there are now q minus p minus 1q −p −1
elements between q and p (inclusive) with exactly one larger element preceding 
it. The total number is 2 q minus 2 p minus 12q −2p −1, which is an odd number. Therefore, if 
pi Subscript jπj consists of the interchange of exactly two elements, then sigma left parenthesis pi Subscript j Baseline right parenthesis equals negative 1σ(πj) = −1. A  
similar argument applies to the interchange of exactly two elements any index 
vector left parenthesis i 1 comma ellipsis comma i Subscript n Baseline right parenthesis(i1, . . . , in). Counting in the same way, the number of additional or the 
number of fewer elements for which a smaller element follows a larger one in 
the new permutation is an odd number. We conclude that the new value of sigmaσ
has a diﬀerent sign from its original value. 
If the integers 1 comma ellipsis comma m1, . . . , m occur sequentially in a given permutation and are 
followed by m plus 1 comma ellipsis comma nm + 1, . . . , n which also occur sequentially in the permutation, 
they can be considered separately: 
sigma left parenthesis j 1 comma ellipsis comma j Subscript n Baseline right parenthesis equals sigma left parenthesis j 1 comma ellipsis comma j Subscript m Baseline right parenthesis sigma left parenthesis j Subscript m plus 1 Baseline comma ellipsis comma j Subscript n Baseline right parenthesis periodσ(j1, . . . , jn) = σ(j1, . . . , jm)σ(jm+1, . . . , jn).
(3.29) 
Furthermore, we see that the product a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baselinea1j1 · · · anjn has exactly one factor from 
each unique row–column pair. These observations facilitate the derivation of 
various properties of the determinant (although the details are sometimes 
quite tedious). 
We see immediately from the deﬁnition that the determinant of an upper 
or lower triangular matrix (or a diagonal matrix) is merely the product of the 
diagonal elements (because in each term of Eq. (3.28) there is a 0, except in 
the term in which the subscripts on each factor are the same). 
Notation 
The determinant is also sometimes written as StartAbsoluteValue upper A EndAbsoluteValue|A|. This notation has the advan-
tage of compactness, but I prefer the notation normal d normal e normal t left parenthesis upper A right parenthesisdet(A) because of the possible 
confusion between StartAbsoluteValue upper A EndAbsoluteValue|A| and the absolute value of some quantity. The determi-
nant of a matrix may be negative, and sometimes, as in measuring volumes 
(see page 95 for simple areas and page 338 for special volumes called Jaco-
bians), we need to specify the absolute value of the determinant, so we need 
something of the form StartAbsoluteValue normal d normal e normal t left parenthesis upper A right parenthesis EndAbsoluteValue|det(A)|.

3.2 The Determinant
87
3.2.2 Determinants of Various Types of Square Matrices 
The determinants of matrices with special patterns or of functions of matrices 
may have simple properties or relationships to other determinants. 
The Determinant of the Transpose of Square Matrices 
One important property we see immediately from a manipulation of the deﬁ-
nition of the determinant is 
normal d normal e normal t left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper A Superscript normal upper T Baseline right parenthesis perioddet(A) = det(AT).
(3.30) 
The Determinant of Scalar Products of Square Matrices 
For a scalar c and an n times nn × n matrix A, 
normal d normal e normal t left parenthesis c upper A right parenthesis equals c Superscript n Baseline normal d normal e normal t left parenthesis upper A right parenthesis perioddet(cA) = cndet(A).
(3.31) 
This follows immediately from the deﬁnition because, for normal d normal e normal t left parenthesis c upper A right parenthesisdet(cA), each factor 
in each term of Eq. (3.28) is multiplied by c. 
The Determinant of a Triangular Matrix, Upper or Lower 
If A is an n times nn × n (upper or lower) triangular matrix, then 
normal d normal e normal t left parenthesis upper A right parenthesis equals product Underscript i equals 1 Overscript n Endscripts a Subscript i i Baseline perioddet(A) =
n
||
i=1
aii.
(3.32) 
This follows immediately from the deﬁnition. It can be generalized, as in the 
next section. 
The Determinant of Square Block Triangular Matrices 
Consider a square matrix partitioned into square matrices: 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 21 2nd Row 1st Column upper A 12 2nd Column upper A 22 EndMatrix periodA =
[
A11 A21
A12 A22
]
.
If all submatrices are square, they all must be of the same dimensions. If just 
the diagonal submatrices are square, there are some simple relations between 
the determinant of A and the determinants of the diagonal submatrices. In 
Sect. 3.5, we will deﬁne the Schur complement in partitioned matrices and 
derive a relationship among the determinants based on it. In this section, we 
consider the case in which a submatrix is 0, resulting in a block diagonal or 
skew block diagonal matrix or a block triangular matrix.

88
3 Basic Properties of Matrices
If the diagonal submatrices are square and one of the oﬀ-diagonal matrices 
is 0, whether or not it is square, that is, if 
upper A 12 equals 0 normal o normal r upper A 21 equals 0 commaA12 = 0
or
A21 = 0,
then we have the simple relationship of the determinant of the full matrix to 
the determinants of the diagonal submatrices: 
normal d normal e normal t left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper A 11 right parenthesis normal d normal e normal t left parenthesis upper A 22 right parenthesis perioddet(A) = det(A11)det(A22).
(3.33) 
That is, the determinant of a square partitioned matrix that is block diagonal 
or upper or lower block triangular depends only on the diagonal partitions. 
If, similarly, the oﬀ-diagonal submatrices are square and one of the main 
diagonal matrices is 0, whether or not it is square, then there is a simple 
relationship of the determinant of the full matrix to the determinants of the 
oﬀ-diagonal submatrices. If 
upper A 11 equals 0 normal o normal r upper A 22 equals 0 commaA11 = 0
or
A22 = 0,
then 
normal d normal e normal t left parenthesis upper A right parenthesis equals minus normal d normal e normal t left parenthesis upper A 12 right parenthesis normal d normal e normal t left parenthesis upper A 21 right parenthesis perioddet(A) = −det(A12)det(A21).
(3.34) 
We can obtain Eq. (3.33) or (3.34) by considering the individual terms in 
the determinant, Eq. (3.28). 
Consider just Eq. (3.33). Suppose the full matrix is n times nn×n, and  upper A 11A11 is m times mm×m. 
Then upper A 22A22 is left parenthesis n minus m right parenthesis times left parenthesis n minus m right parenthesis(n−m)×(n−m), upper A 21A21 is left parenthesis n minus m right parenthesis times m(n−m)×m, and  
upper A 12A12 is m times left parenthesis n minus m right parenthesism×(n−m). 
In Eq. (3.28), any addend for which left parenthesis j 1 comma ellipsis comma j Subscript m Baseline right parenthesis(j1, . . . , jm) is not a permutation of the 
integers 1 comma ellipsis comma m1, . . . , m contains a factor a Subscript i jaij that is in a 0 diagonal block, and hence 
the addend is 0. The determinant consists only of those addends for which 
left parenthesis j 1 comma ellipsis comma j Subscript m Baseline right parenthesis(j1, . . . , jm) is a permutation of the integers 1 comma ellipsis comma m1, . . . , m, and hence left parenthesis j Subscript m plus 1 Baseline comma ellipsis comma j Subscript n Baseline right parenthesis(jm+1, . . . , jn)
is a permutation of the integers m plus 1 comma ellipsis comma nm + 1, . . . , n, 
normal d normal e normal t left parenthesis upper A right parenthesis equals sigma summation sigma summation sigma left parenthesis j 1 comma ellipsis comma j Subscript m Baseline comma j Subscript m plus 1 Baseline comma ellipsis comma j Subscript n Baseline right parenthesis a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript m j Sub Subscript m Subscript Baseline a Subscript m plus 1 comma j Sub Subscript m plus 1 Subscript Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baseline commadet(A) =
E E
σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 · · · amjmam+1,jm+1 · · · anjn,
where the ﬁrst sum is taken over all permutations that keep the ﬁrst m integers 
together while maintaining a ﬁxed ordering for the integers m plus 1m + 1 through n, 
and the second sum is taken over all permutations of the integers from m plus 1m + 1
through n while maintaining a ﬁxed ordering of the integers from 1 to m. 
Now, using Eq. (3.29), we therefore have for A of this special form 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper A right parenthesis 2nd Column equals 3rd Column sigma summation sigma summation sigma left parenthesis j 1 comma ellipsis comma j Subscript m Baseline comma j Subscript m plus 1 Baseline comma ellipsis comma j Subscript n Baseline right parenthesis a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript m j Sub Subscript m Subscript Baseline a Subscript m plus 1 comma j Sub Subscript m plus 1 Subscript Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baseline 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation sigma left parenthesis j 1 comma ellipsis comma j Subscript m Baseline right parenthesis a Subscript 1 j 1 Baseline midline horizontal ellipsis a Subscript m j Sub Subscript m Subscript Baseline sigma summation sigma left parenthesis j Subscript m plus 1 Baseline comma ellipsis comma j Subscript n Baseline right parenthesis a Subscript m plus 1 comma j Sub Subscript m plus 1 Subscript Baseline midline horizontal ellipsis a Subscript n j Sub Subscript n Subscript Baseline 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper A 11 right parenthesis normal d normal e normal t left parenthesis upper A 22 right parenthesis comma EndLayoutdet(A) =
E E
σ(j1, . . . , jm, jm+1, . . . , jn)a1j1 · · · amjmam+1,jm+1 · · · anjn
=
E
σ(j1, . . . , jm)a1j1 · · · amjm
E
σ(jm+1, . . . , jn)am+1,jm+1 · · · anjn
= det(A11)det(A22),
which is Eq. (3.33). 
The relationships shown in Eq. (3.33) and  (3.34) are even more useful if 
one of the submatrices is has a special structure.

3.2 The Determinant
89
We use these equations to evaluate the determinant of a Cayley product 
of square matrices on page 110 and to give an expression for the determinant 
of more general partitioned matrices in Sect. 3.5.2, beginning on page 144. 
We also use these equations in working out marginal distributions of mul-
tivariate distributions. 
The Determinant of the Sum of Square Matrices 
We note in general that 
normal d normal e normal t left parenthesis upper A plus upper B right parenthesis not equals normal d normal e normal t left parenthesis upper A right parenthesis plus normal d normal e normal t left parenthesis upper B right parenthesis commadet(A + B) /= det(A) + det(B),
which we can see easily by an example. (Consider matrices in normal upper I normal upper R Superscript 2 times 2IR2×2, for  ex-
ample, and let upper A equals upper IA = I and upper B equals Start 2 By 2 Matrix 1st Row 1st Column negative 1 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrixB =
[
−1 0
0 0
]
.) 
3.2.3 Minors, Cofactors, and Adjugate Matrices 
In this section, we consider matrices formed by deleting one row and one 
column from an n times nn×n matrix. The determinants of these n minus 1 times n minus 1n−1×n−1 matrices 
have some interesting and useful relationships with the determinant of the full 
matrix. 
An Expansion of the Determinant 
Consider the 2 times 22 × 2 matrix 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column a 11 2nd Column a 12 2nd Row 1st Column a 21 2nd Column a 22 EndMatrix periodA =
[
a11 a12
a21 a22
]
.
From the deﬁnition of the determinant, we see that 
normal d normal e normal t left parenthesis upper A right parenthesis equals a 11 a 22 minus a 12 a 21 perioddet(A) = a11a22 −a12a21.
(3.35) 
Now consider the 3 times 33 × 3 matrix 
upper A equals Start 3 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 12 3rd Column a 13 2nd Row 1st Column a 21 2nd Column a 22 3rd Column a 23 3rd Row 1st Column a 31 2nd Column a 32 3rd Column a 33 EndMatrix periodA =
⎡
⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
⎤
⎦.
In the deﬁnition of the determinant, identify all of the terms in which the 
elements of the ﬁrst row of A appear. With some manipulation of those terms, 
we can express the determinant in terms of determinants of submatrices as

90
3 Basic Properties of Matrices
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper A right parenthesis equals 2nd Column a 11 left parenthesis negative 1 right parenthesis Superscript 1 plus 1 Baseline normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column a 22 2nd Column a 23 2nd Row 1st Column a 32 2nd Column a 33 EndMatrix right parenthesis 2nd Row 1st Column Blank 3rd Row 1st Column plus 2nd Column a 12 left parenthesis negative 1 right parenthesis Superscript 1 plus 2 Baseline normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column a 21 2nd Column a 23 2nd Row 1st Column a 31 2nd Column a 33 EndMatrix right parenthesis 4th Row 1st Column Blank 5th Row 1st Column plus 2nd Column a 13 left parenthesis negative 1 right parenthesis Superscript 1 plus 3 Baseline normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column a 21 2nd Column a 22 2nd Row 1st Column a 31 2nd Column a 32 EndMatrix right parenthesis period EndLayout
det(A) = a11(−1)1+1det
([
a22 a23
a32 a33
])
+ a12(−1)1+2det
([
a21 a23
a31 a33
])
+ a13(−1)1+3det
([
a21 a22
a31 a32
])
.
(3.36) 
Notice that this expression with 2 times 22 × 2 submatrices is the same form as in 
Eq. (3.35) in which the submatrices in the determinants are 1 times 11 × 1. 
The manipulations of the terms in the determinant in Eq. (3.36) could  be  
carried out with other rows of A. 
In the notation adopted on page 81, the submatrices in Eq. (3.36) are  
denoted as upper A Subscript minus left parenthesis 1 right parenthesis left parenthesis 1 right parenthesisA−(1)(1), upper A Subscript minus left parenthesis 1 right parenthesis left parenthesis 2 right parenthesisA−(1)(2), and  upper A Subscript minus left parenthesis 1 right parenthesis left parenthesis 3 right parenthesisA−(1)(3). 
The manipulations leading to Eq. (3.36), though somewhat tedious, can 
be carried out for a square matrix of any size larger than 1 times 11×1. The extension 
of the expansion (3.36) to an expression involving a similar sum of signed 
products of determinants of left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1) submatrices of an n times nn × n matrix 
A is 
normal d normal e normal t left parenthesis upper A right parenthesis equals sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript i j Baseline left parenthesis negative 1 right parenthesis Superscript i plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesisdet(A) =
n
E
j=1
aij(−1)i+jdet
(
A−(i)(j)
)
(3.37) 
or, over the rows, 
normal d normal e normal t left parenthesis upper A right parenthesis equals sigma summation Underscript i equals 1 Overscript n Endscripts a Subscript i j Baseline left parenthesis negative 1 right parenthesis Superscript i plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesis perioddet(A) =
n
E
i=1
aij(−1)i+jdet
(
A−(i)(j)
)
.
(3.38) 
These expressions are called Laplace expansions. Each determinant 
normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesisdet
(
A−(i)(j)
)
can likewise be expressed recursively in a similar Laplace ex-
pansion. 
Laplace expansions could be used to compute the determinant, but the 
main value of these expansions is in proving the properties of determinants. 
For example, from the special Laplace expansion (3.37) or (3.38), we can 
quickly see that the determinant of a matrix with two rows that are the same 
is zero. We see this by recursively expanding all of the minors until we have 
only 2 times 22 × 2 matrices consisting of a duplicated row. The determinant of such a 
matrix is 0, so the expansion is 0. 
Deﬁnitions of Terms: Minors and Cofactors 
The determinants of the submatrices in Eqs. (3.37) or (3.38) are called minors 
or complementary minors of the associated elements; the minor associated 
with the a Subscript i jaij element is 
normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesis commadet
(
A−(i)(j)
)
,
(3.39)

3.2 The Determinant
91
in which upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesisA−(i)(j) denotes the submatrix that is formed from A by removing 
the i normal t normal hith row and the j normal t normal hjth column. 
The sign associated with the minor corresponding to a Subscript i jaij is left parenthesis negative 1 right parenthesis Superscript i plus j(−1)i+j. The  
minor together with its appropriate sign is called the cofactor of the associated 
element. We denote the cofactor of a Subscript i jaij as a Subscript left parenthesis i j right parenthesisa(ij): 
a Subscript left parenthesis i j right parenthesis Baseline equals left parenthesis negative 1 right parenthesis Superscript i plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesis perioda(ij) = (−1)i+jdet
(
A−(i)(j)
)
.
(3.40) 
In this notation, the expansion in Eq. (3.37) can be written as 
normal d normal e normal t left parenthesis upper A right parenthesis equals sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript i j Baseline a Subscript left parenthesis i j right parenthesis Baseline perioddet(A) =
n
E
j=1
aija(ij).
(3.41) 
Notice that both minors and cofactors are scalars. 
Adjugate Matrices 
A certain matrix formed from the cofactors has some interesting properties. 
We deﬁne the matrix here but defer further discussion. The adjugate of the 
n times nn × n matrix A is denoted as normal a normal d normal j left parenthesis upper A right parenthesisadj(A) and deﬁned as  
normal a normal d normal j left parenthesis upper A right parenthesis equals left parenthesis a Subscript left parenthesis j i right parenthesis Baseline right parenthesis commaadj(A) = (a(ji)),
(3.42) 
which is an n times nn × n matrix of the cofactors of the elements of the transposed 
matrix. (The adjugate is also called the adjoint or sometimes “classical ad-
joint,” but as we noted above, the term adjoint may also mean the conjugate 
transpose. To distinguish it from the conjugate transpose, the adjugate is also 
sometimes called the “classical adjoint.” We will generally avoid using the 
term “adjoint.”) Note the reversal of the subscripts; that is, 
normal a normal d normal j left parenthesis upper A right parenthesis equals left parenthesis a Subscript left parenthesis i j right parenthesis Baseline right parenthesis Superscript normal upper T Baseline periodadj(A) = (a(ij))T.
The adjugate has a number of useful properties, some of which we will 
encounter later, as in Eq. (3.98) and  in  Eq. (3.186). 
Cofactors and Orthogonal Vectors 
Notice an interesting property of the expansion in Eq. (3.37). If instead of the 
elements a Subscript i jaij from the i normal t normal hith row, we use elements from a diﬀerent row, say the 
k normal t normal hkth row, the sum is zero. That is, for k not equals ik /= i, 
StartLayout 1st Row 1st Column sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript k j Baseline left parenthesis negative 1 right parenthesis Superscript i plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis j right parenthesis Baseline right parenthesis 2nd Column equals 3rd Column sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript k j Baseline a Subscript left parenthesis i j right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column 0 period EndLayout
n
E
j=1
akj(−1)i+jdet
(
A−(i)(j)
)
=
n
E
j=1
akja(ij)
= 0.
(3.43)

92
3 Basic Properties of Matrices
This is true because such an expansion is exactly the same as an expansion for 
the determinant of a matrix whose k normal t normal hkth row has been replaced by its i normal t normal hith row; 
that is, a matrix with two identical rows. The determinant of such a matrix 
is 0, as we saw above. 
This suggests an interesting method for obtaining a vector orthogonal to 
each of a given set of linearly independent vectors. 
Let a 1 comma ellipsis comma a Subscript k Baselinea1, . . . , ak be linearly independent n-vectors, with k less than nk < n. Now  form  
the k times nk × n matrix, A, whose rows consist of the elements of the given vectors. 
(The elements in the ﬁrst row of A, for example, correspond to the elements 
of a 1a1, those in the second row to those of a 2a2, and so on.) A vector orthogonal 
to each of a 1 comma ellipsis comma a Subscript k Baselinea1, . . . , ak (or to the space normal s normal p normal a normal n left parenthesis StartSet a 1 comma ellipsis comma a Subscript k Baseline EndSet right parenthesisspan({a1, . . . , ak})) is  
v equals left parenthesis left parenthesis negative 1 right parenthesis Superscript i plus 1 Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis 1 right parenthesis Baseline right parenthesis comma ellipsis comma left parenthesis negative 1 right parenthesis Superscript i plus n Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis i right parenthesis left parenthesis n right parenthesis Baseline right parenthesis right parenthesis periodv =
(
(−1)i+1det
(
A−(i)(1)
)
, . . . , (−1)i+ndet
(
A−(i)(n)
))
.
(3.44) 
For k equals n minus 1k = n −1, another way of writing this vector is by a formal Laplace 
expansion of an n times nn × n “matrix” in which the ﬁrst row consists of the unit 
vectors in the coordinate directions. 
The vector product of the vectors x and y in normal upper I normal upper R cubedIR3 discussed on page 58 can 
be used to illustrate this formal expansion. The vector product x times yx × y is of 
course orthogonal to each of the vectors x and y. 
First, write the matrix 
upper A equals Start 3 By 3 Matrix 1st Row 1st Column normal e 1 2nd Column normal e 2 3rd Column normal e 3 2nd Row 1st Column x 1 2nd Column x 2 3rd Column x 3 3rd Row 1st Column y 1 2nd Column y 2 3rd Column y 3 EndMatrix commaA =
⎡
⎣
e1 e2 e3
x1 x2 x3
y1 y2 y3
⎤
⎦,
where normal e 1 comma normal e 2 comma normal e 3e1, e2, e3 represent the unit vectors in the three coordinate directions. 
Next, write the formal Laplace expansion: 
normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column x 2 2nd Column x 3 2nd Row 1st Column y 2 2nd Column y 3 EndMatrix right parenthesis normal e 1 minus normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column x 1 2nd Column x 3 2nd Row 1st Column y 1 2nd Column y 3 EndMatrix right parenthesis normal e 2 plus normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column x 1 2nd Column x 2 2nd Row 1st Column y 1 2nd Column y 2 EndMatrix right parenthesis normal e 3 perioddet
([
x2 x3
y2 y3
])
e1 −det
([
x1 x3
y1 y3
])
e2 + det
([
x1 x2
y1 y2
])
e3.
This is the vector product x times yx × y. 
The process of determining orthonormal vectors is called orthonormal 
completion, and is discussed in the context of linear systems of equations 
in Sect. 3.6.3. The Gram–Schmidt transformations for orthogonalizing a set 
of vectors discussed in Sect. 2.2.4 on page 48 can also be used to determine 
a vector orthogonal to each of a set of vectors. The method discussed here 
using Laplace expansions is not computationally competitive with the Gram– 
Schmidt process. This method is discussed here because of the interesting 
relationships to the topic of this section. 
A Diagonal Expansion of the Determinant 
As we have mentioned, in general there is no simple expression for normal d normal e normal t left parenthesis upper A plus upper B right parenthesisdet(A+B). 
The determinant of a particular sum of matrices is of interest, however. The

3.2 The Determinant
93
sum is one in which a diagonal matrix D is added to a square matrix A, and  
we are interested in normal d normal e normal t left parenthesis upper A plus upper D right parenthesisdet(A + D). Such a determinant arises in eigenanalysis, 
for example, as we see in Sect. 3.9.3. 
For evaluating the determinant normal d normal e normal t left parenthesis upper A plus upper D right parenthesisdet(A+D), we can develop another expan-
sion of the determinant by restricting our choice of minors to determinants of 
matrices formed by deleting the same rows and columns and then continuing 
to delete rows and columns recursively from the resulting matrices. The ex-
pansion is a polynomial in the elements of D; and for our purposes later, that 
is  the most useful form.  
Recall that upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesisA(i1,...,ik)(i1,...,ik) represents the matrix formed from rows 
i 1 comma ellipsis comma i Subscript k Baselinei1, . . . , ik and columns i 1 comma ellipsis comma i Subscript k Baselinei1, . . . , ik from a given matrix A. For square matri-
ces, we often use the simpler notation upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesisA(i1,...,ik). 
We denote the determinant of this k times kk × k matrix in the obvious way, 
normal d normal e normal t left parenthesis upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis Baseline right parenthesisdet(A(i1,...,ik)). Because the principal diagonal elements of this matrix are 
principal diagonal elements of A, we call  normal d normal e normal t left parenthesis upper A Subscript left parenthesis i 1 comma ellipsis comma i Sub Subscript k Subscript right parenthesis Baseline right parenthesisdet(A(i1,...,ik)) a principal minor of 
A. 
Now consider normal d normal e normal t left parenthesis upper A plus upper D right parenthesisdet(A + D) for the 2 times 22 × 2 case: 
normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column a 11 plus d 1 2nd Column a 12 2nd Row 1st Column a 21 2nd Column a 22 plus d 2 EndMatrix right parenthesis perioddet
([
a11 + d1
a12
a21
a22 + d2
])
.
Expanding this, we have 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper A plus upper D right parenthesis 2nd Column equals 3rd Column left parenthesis a 11 plus d 1 right parenthesis left parenthesis a 22 plus d 2 right parenthesis minus a 12 a 21 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column a 11 2nd Column a 12 2nd Row 1st Column a 21 2nd Column a 22 EndMatrix right parenthesis plus d 1 d 2 plus a 22 d 1 plus a 11 d 2 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 2 right parenthesis Baseline right parenthesis plus d 1 d 2 plus a 22 d 1 plus a 11 d 2 period EndLayoutdet(A + D) = (a11 + d1)(a22 + d2) −a12a21
= det
([
a11 a12
a21 a22
])
+ d1d2 + a22d1 + a11d2
= det(A(1,2)) + d1d2 + a22d1 + a11d2.
Of course, normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 2 right parenthesis Baseline right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesisdet(A(1,2)) = det(A), but we are writing it this way to develop the 
pattern. Now, for the 3 times 33 × 3 case, we have 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper A plus upper D right parenthesis 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 2 comma 3 right parenthesis Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column Blank 3rd Column plus normal d normal e normal t left parenthesis upper A Subscript left parenthesis 2 comma 3 right parenthesis Baseline right parenthesis d 1 plus normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 3 right parenthesis Baseline right parenthesis d 2 plus normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 2 right parenthesis Baseline right parenthesis d 3 3rd Row 1st Column Blank 2nd Column Blank 3rd Column plus a 33 d 1 d 2 plus a 22 d 1 d 3 plus a 11 d 2 d 3 4th Row 1st Column Blank 2nd Column Blank 3rd Column plus d 1 d 2 d 3 period EndLayoutdet(A + D) = det(A(1,2,3))
+ det(A(2,3))d1 + det(A(1,3))d2 + det(A(1,2))d3
+ a33d1d2 + a22d1d3 + a11d2d3
+ d1d2d3.
(3.45) 
In the applications of interest, the elements of the diagonal matrix D may be 
a single variable: d, say. In this case, the expression simpliﬁes to 
normal d normal e normal t left parenthesis upper A plus upper D right parenthesis equals normal d normal e normal t left parenthesis upper A Subscript left parenthesis 1 comma 2 comma 3 right parenthesis Baseline right parenthesis plus sigma summation Underscript i not equals j Endscripts normal d normal e normal t left parenthesis upper A Subscript left parenthesis i comma j right parenthesis Baseline right parenthesis d plus sigma summation Underscript i Endscripts a Subscript i comma i Baseline d squared plus d cubed perioddet(A + D) = det(A(1,2,3)) +
E
i/=j
det(A(i,j))d +
E
i
ai,id2 + d3.
(3.46) 
Consider the expansion in a single variable because that will prove most 
useful. The pattern persists; the constant term is StartAbsoluteValue upper A EndAbsoluteValue|A|, the coeﬃcient of the

94
3 Basic Properties of Matrices
ﬁrst-degree term is the sum of the left parenthesis n minus 1 right parenthesis(n −1)-order principal minors, and, at 
the other end, the coeﬃcient of the left parenthesis n minus 1 right parenthesis normal t normal h(n −1)th-degree term is the sum of the 
ﬁrst-order principal minors (that is, just the diagonal elements), and ﬁnally 
the coeﬃcient of the n normal t normal hnth-degree term is 1. 
This kind of representation is called a diagonal expansion of the determi-
nant because the coeﬃcients are principal minors. It has occasional use for 
matrices with large patterns of zeros, but its main application is in analysis 
of eigenvalues, which we consider in Sect. 3.9.3. 
3.2.4 A Geometrical Perspective of the Determinant 
In Sect. 2.2, we discussed a useful geometric interpretation of vectors in a 
linear space with a Cartesian coordinate system. The elements of a vec-
tor correspond to measurements along the respective axes of the coordinate 
system. When working with several vectors, or with a matrix in which the 
columns (or rows) are associated with vectors, we may designate a vector 
x Subscript ixi as x Subscript i Baseline equals left parenthesis x Subscript i Baseline 1 Baseline comma ellipsis comma x Subscript i d Baseline right parenthesisxi = (xi1, . . . , xid). A set of  d linearly independent d-vectors deﬁne a 
parallelotope in d dimensions. For example, in a two-dimensional space, the 
linearly independent 2-vectors x 1x1 and x 2x2 deﬁne a parallelogram, as shown in 
Fig. 3.1. 
Figure 3.1. Volume (area) of region determined by x 1x1 and x 2x2
The area of this parallelogram is the base times the height, bh, where, in  
this case, b is the length of the vector x 1x1, and  h is the length of x 2x2 times the 
sine of the angle thetaθ. Thus, making use of Eq. (2.53) on page 47 for the cosine 
of the angle, we have

3.3 Multiplication of Matrices
95
StartLayout 1st Row 1st Column normal a normal r normal e normal a 2nd Column equals 3rd Column b h 2nd Row 1st Column Blank 2nd Column equals 3rd Column parallel to x 1 parallel to parallel to x 2 parallel to sine left parenthesis theta right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column parallel to x 1 parallel to parallel to x 2 parallel to StartRoot 1 minus left parenthesis StartFraction left angle bracket x 1 comma x 2 right angle bracket Over parallel to x 1 parallel to parallel to x 2 parallel to EndFraction right parenthesis squared EndRoot 4th Row 1st Column Blank 2nd Column equals 3rd Column StartRoot parallel to x 1 parallel to squared parallel to x 2 parallel to squared minus left parenthesis left angle bracket x 1 comma x 2 right angle bracket right parenthesis squared EndRoot 5th Row 1st Column Blank 2nd Column equals 3rd Column StartRoot left parenthesis x 11 squared plus x 12 squared right parenthesis left parenthesis x 21 squared plus x 22 squared right parenthesis minus left parenthesis x 11 x 21 minus x 12 x 22 right parenthesis squared EndRoot 6th Row 1st Column Blank 2nd Column equals 3rd Column StartAbsoluteValue x 11 x 22 minus x 12 x 21 EndAbsoluteValue 7th Row 1st Column Blank 2nd Column equals 3rd Column StartAbsoluteValue normal d normal e normal t left parenthesis upper X right parenthesis EndAbsoluteValue comma EndLayoutarea = bh
= ||x1||||x2|| sin(θ)
= ||x1||||x2||
/
1 −
( <x1, x2>
||x1||||x2||
)2
=
/
||x1||2||x2||2 −(<x1, x2>)2
=
/
(x2
11 + x2
12)(x2
21 + x2
22) −(x11x21 −x12x22)2
= |x11x22 −x12x21|
= |det(X)|,
(3.47) 
where x 1 equals left parenthesis x 11 comma x 12 right parenthesisx1 = (x11, x12), x 2 equals left parenthesis x 21 comma x 22 right parenthesisx2 = (x21, x22), and  
StartLayout 1st Row 1st Column upper X 2nd Column equals 3rd Column left bracket x 1 vertical bar x 2 right bracket 2nd Row 1st Column Blank 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column x 11 2nd Column x 21 2nd Row 1st Column x 12 2nd Column x 22 EndMatrix period EndLayoutX = [x1 | x2]
=
[ x11 x21
x12 x22
]
.
Although we will not go through the details here, this equivalence of a 
volume of a parallelotope that has a vertex at the origin and the absolute 
value of the determinant of a square matrix whose columns correspond to the 
vectors that form the sides of the parallelotope extends to higher dimensions. 
In making a change of variables in integrals, as in Eq. (7.20) on page 338, we  
use the absolute value of the determinant of the Jacobian as a volume element. 
Another instance of the interpretation of the determinant as a volume is in 
the generalized variance, discussed on page 406. 
Computing the Determinant 
For an arbitrary matrix, the determinant is rather diﬃcult to compute. The 
method for computing a determinant is not the one that would arise directly 
from the deﬁnition or even from a Laplace expansion. The more eﬃcient meth-
ods involve ﬁrst factoring the matrix, as we discuss later. 
The determinant is not very often directly useful, but although it may not 
be obvious from its deﬁnition, the determinant, along with minors, cofactors, 
and adjoint matrices, is very useful in discovering and proving the properties 
of matrices. The determinant is particularly useful in developing properties of 
eigenvalues and eigenvectors (see Sect. 3.7). 
3.3 Multiplication of Matrices and Multiplication of 
Vectors and Matrices 
The elements of a vector or matrix are elements of a ﬁeld, and most matrix 
and vector operations are deﬁned in terms of the two operations of the ﬁeld. 
Of course, in this book, the ﬁeld of most interest is the ﬁeld of real numbers.

96
3 Basic Properties of Matrices
3.3.1 Matrix Multiplication (Cayley) 
There are various kinds of multiplication of matrices that may be useful. The 
most common kind of multiplication is Cayley multiplication. If the  number  
of columns of the matrix A, with elements a Subscript i jaij, and the number of rows of the 
matrix B, with elements b Subscript i jbij, are equal, then the (Cayley) product of A and B 
is deﬁned as the matrix C with elements 
c Subscript i j Baseline equals sigma summation Underscript k Endscripts a Subscript i k Baseline b Subscript k j Baseline periodcij =
E
k
aikbkj.
(3.48) 
This is the most common type of matrix product, and we refer to it by the 
unqualiﬁed phrase “matrix multiplication.” 
Cayley matrix multiplication is indicated by juxtaposition, with no inter-
vening symbol for the operation: AB. 
If the matrix A is n times mn × m and the matrix B is m times pm × p, the product upper A upper B equals upper CAB = C
is n times pn × p: 
StartLayout 1st Row 1st Column upper A 2nd Column upper B 3rd Column equals 4th Column upper C 2nd Row 1st Column Blank 3rd Row 1st Column left bracket right bracket Subscript n times m Baseline 2nd Column left bracket right bracket Subscript m times p Baseline 3rd Column equals 4th Column left bracket right bracket Subscript n times p Baseline EndLayout period
A
B
=
C
[ ]
n×m
[
]m×p =
[
]
n×p
.
(3.49) 
Cayley matrix multiplication is a mapping, 
normal upper I normal upper R Superscript n times m Baseline times normal upper I normal upper R Superscript m times p Baseline right arrow normal upper I normal upper R Superscript n times p Baseline periodIRn×m × IRm×p →IRn×p.
The multiplication of an n times mn × m matrix and an m times pm × p matrix requires 
nmp scalar multiplications and n p left parenthesis m minus 1 right parenthesisnp(m −1) scalar additions. Here, as always 
in numerical analysis, we must remember that the deﬁnition of an operation, 
such as matrix multiplication, does not necessarily deﬁne a good algorithm 
for evaluating the operation. 
It is obvious that while the product AB may be well-deﬁned, the product 
BA is deﬁned only if n equals pn = p; that is, if the matrices AB and BA are square. 
We assume throughout that writing a product of matrices AB implies that 
the number of columns of the ﬁrst matrix is the same as the number of rows of 
the second; that is, they are conformable for multiplication in the order given. 
It is easy to see from the deﬁnition of matrix multiplication (3.48) that in  
general, even for square matrices, upper A upper B not equals upper B upper AAB /= BA. 
It is also obvious that if AB exists, then upper B Superscript normal upper T Baseline upper A Superscript normal upper TBTAT exists and, in fact, 
upper B Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals left parenthesis upper A upper B right parenthesis Superscript normal upper T Baseline periodBTAT = (AB)T.
(3.50) 
The product of symmetric matrices is not, in general, symmetric. If (but not 
only if) A and B are symmetric, then upper A upper B equals left parenthesis upper B upper A right parenthesis Superscript normal upper TAB = (BA)T. 
Because matrix multiplication is not commutative, we often use the terms 
“premultiply” and “postmultiply” and the corresponding nominal forms of

3.3 Multiplication of Matrices
97
these terms. Thus, in the product AB, we may  say  B is premultiplied by A, 
or, equivalently, A is postmultiplied by B. 
Although matrix multiplication is not commutative, it is  associative; that  
is, if the matrices are conformable, 
upper A left parenthesis upper B upper C right parenthesis equals left parenthesis upper A upper B right parenthesis upper C periodA(BC) = (AB)C.
(3.51) 
It is also distributive over addition; that is, 
upper A left parenthesis upper B plus upper C right parenthesis equals upper A upper B plus upper A upper CA(B + C) = AB + AC
(3.52) 
and 
left parenthesis upper B plus upper C right parenthesis upper A equals upper B upper A plus upper C upper A period(B + C)A = BA + CA.
(3.53) 
These properties are obvious from the deﬁnition of matrix multiplication. 
(Note that left-sided distribution is not the same as right-sided distribution 
because the multiplication is not commutative.) 
An n times nn×n matrix consisting of 1s along the diagonal and 0s everywhere else is 
a multiplicative identity for the set of n times nn×n matrices and Cayley multiplication. 
Such a matrix is called the identity matrix of order n, and is denoted by upper I Subscript nIn, 
or just by I. The columns of the identity matrix are unit vectors. 
The identity matrix is a multiplicative identity for any matrix so long as 
the matrices are conformable for the multiplication. If A is n times mn × m, then  
upper I Subscript n Baseline upper A equals upper A upper I Subscript m Baseline equals upper A periodInA = AIm = A.
Another matrix of interest is a zero matrix, which is any matrix consisting 
of all zeros. We denote a zero matrix as 0, with its shape being implied by the 
context. Two properties for any matrix A and a zero matrix of the appropriate 
shape are immediately obvious: 
0 upper A equals 0 upper A Baseline 0 equals 0 left parenthesis the zero matrices may be different right parenthesis0A = 0
A0 = 0
(the zero matrices may be diﬀerent)
and 
0 plus upper A equals upper A plus 0 equals upper A period0 + A = A + 0 = A.
Factorization of Matrices 
For a given matrix C, it is often of interest to ﬁnd matrices upper C 1 comma ellipsis comma upper C Subscript k BaselineC1, . . . , Ck such 
that upper C 1 comma ellipsis comma upper C Subscript k BaselineC1, . . . , Ck have some useful properties and upper C equals upper C 1 midline horizontal ellipsis upper C Subscript k BaselineC = C1 · · · Ck. This is called  
a factorization or decomposition of C. (We will usually use these two words 
interchangeably; that is, by “decomposition,” we will usually mean “multi-
plicative decomposition.” Occasionally we will be interested in an additive 
decomposition of a matrix, as in Cochran’s theorem, discussed on page 393.) 
In most cases, the number of factors of interest in upper C equals upper C 1 midline horizontal ellipsis upper C Subscript k BaselineC = C1 · · · Ck is either 
2 or 3.

98
3 Basic Properties of Matrices
Obviously, the factorization is not unique unless some properties of the 
factors are speciﬁed. If either A or B In Eq. (3.49), for example, there are 
various matrices A and B that would yield a given C, but if either A or B 
is given, of it the those matrices must have some given property, then the 
problem of determining the factorization upper C equals upper A upper BC = AB may be well posed. 
Factorizations or decompositions play important roles both in theoretical 
development and in computations with matrices. We will encounter some ma-
trix factorization at various places in this chapter, and then in Chap. 4, we  
will discuss them more extensively. 
Powers of Square Matrices 
For a square matrix A, its product with itself is deﬁned, and so we will use the 
notation upper A squaredA2 to mean the Cayley product AA, with similar meanings for upper A Superscript kAk
for a positive integer k. As with the analogous scalar case, upper A Superscript kAk for a negative 
integer may or may not exist, and when it exists, it has a meaning for Cayley 
multiplication similar to the meaning in ordinary scalar multiplication. We 
will consider these issues later (in Sect. 3.4.6). 
For an n times nn × n matrix A, if  upper A Superscript kAk exists for negative integral values of k, we  
deﬁne upper A Superscript 0A0 by 
upper A Superscript 0 Baseline equals upper I Subscript n Baseline periodA0 = In.
(3.54) 
For a diagonal matrix upper D equals normal d normal i normal a normal g left parenthesis left parenthesis d 1 comma ellipsis comma d Subscript n Baseline right parenthesis right parenthesisD = diag ((d1, . . . , dn)), we have  
upper D Superscript k Baseline equals normal d normal i normal a normal g left parenthesis left parenthesis d 1 Superscript k Baseline comma ellipsis comma d Subscript n Superscript k Baseline right parenthesis right parenthesis periodDk = diag
(
(dk
1, . . . , dk
n)
)
.
(3.55) 
Idempotent and Nilpotent Matrices 
For an n times nn × n matrix A, it may be the case for some positive integer k that 
upper A Superscript k Baseline equals upper AAk = A or upper A Superscript k Baseline equals 0Ak = 0. Such a matrix would be of interest because of its possible 
applications. 
A square A such that 
upper A squared equals upper AA2 = A
(3.56) 
is said to be an idempotent matrix, and a square A such that 
upper A squared equals 0A2 = 0
(3.57) 
is said to be a nilpotent matrix. 
For larger powers, a square A such that for an integer k greater than or equals 3k ≥3, 
upper A Superscript k Baseline equals upper A comma but upper A Superscript k minus 1 Baseline not equals upper A commaAk = A,
but
Ak−1 /= A,
(3.58) 
is said to be an idempotent matrix of index k, and a square A such that for 
an integer k greater than or equals 3k ≥3, 
upper A Superscript k Baseline equals 0 comma but upper A Superscript k minus 1 Baseline not equals 0 commaAk = 0,
but
Ak−1 /= 0,
(3.59)

3.3 Multiplication of Matrices
99
is said to be a nilpotent matrix of index k. 
We will refer to idempotent matrices often, and will discuss them exten-
sively in Sect. 8.5 beginning on page 391. 
A simple example of a matrix that is nilpotent of index 3 is 
upper A equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 2nd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 EndMatrix periodA =
⎡
⎢⎢⎢⎢⎣
0 1 0 0 0
0 0 1 0 0
0 0 0 0 0
0 0 0 0 1
0 0 0 0 0
⎤
⎥⎥⎥⎥⎦
.
(3.60) 
The form of A in Eq. (3.60) is called a Jordan form. All nilpotent matrices have 
a certain relationship to matrices in the Jordan form, but we will not consider 
this fact here. (We discuss the Jordan form and the Jordan decomposition 
on page 174.) Nilpotent matrices have many interesting properties, some of 
which we will list on page 198. 
Matrix Polynomials 
Polynomials in square matrices are similar to the more familiar polynomials 
in scalars. We may consider 
p left parenthesis upper A right parenthesis equals b 0 upper I plus b 1 upper A plus midline horizontal ellipsis b Subscript k Baseline upper A Superscript k Baseline periodp(A) = b0I + b1A + · · · bkAk.
The value of this polynomial is a matrix. 
The theory of polynomials in general holds for matrix polynomials, and 
in particular, we have the useful factorizations of monomials: for any positive 
integer k, 
upper I minus upper A Superscript k Baseline equals left parenthesis upper I minus upper A right parenthesis left parenthesis upper I plus upper A plus midline horizontal ellipsis upper A Superscript k minus 1 Baseline right parenthesis commaI −Ak = (I −A)(I + A + · · · Ak−1),
(3.61) 
and for an odd positive integer k, 
upper I plus upper A Superscript k Baseline equals left parenthesis upper I plus upper A right parenthesis left parenthesis upper I minus upper A plus midline horizontal ellipsis upper A Superscript k minus 1 Baseline right parenthesis periodI + Ak = (I + A)(I −A + · · · Ak−1).
(3.62) 
3.3.2 Cayley Multiplication of Matrices with Special Patterns 
Various properties of matrices may or may not be preserved under matrix 
multiplication. We have seen already that the product of symmetric matrices 
is not in general symmetric. 
Many of the various patterns of zeroes in matrices discussed on page 75, 
however, are preserved under matrix multiplication. Zeroes are often preserved 
in operations with diagonal and triangular matrices. Assume A and B are 
square matrices of the same number of rows. 
• If A and B are diagonal, AB is diagonal and the left parenthesis i comma i right parenthesis(i, i) element of AB is 
a Subscript i i Baseline b Subscript i iaiibii;

100
3 Basic Properties of Matrices
• if A and B are block diagonal with conformable blocks, AB is block diag-
onal; 
• if A and B are upper triangular, AB is upper triangular and the left parenthesis i comma i right parenthesis(i, i)
element of AB is a Subscript i i Baseline b Subscript i iaiibii; 
• if A and B are lower triangular, AB is lower triangular and the left parenthesis i comma i right parenthesis(i, i)
element of AB is a Subscript i i Baseline b Subscript i iaiibii; 
• if A is upper triangular and B is lower triangular, in general, none of AB, 
BA, upper A Superscript normal upper T Baseline upper AATA, upper B Superscript normal upper T Baseline upper BBTB, upper A upper A Superscript normal upper TAAT, and  upper B upper B Superscript normal upper TBBT is triangular. 
Each of these statements can be easily proven using the deﬁnition of multi-
plication in Eq. (3.48). 
An important special case of diagonal and triangular matrices is one in 
which all diagonal elements are 1. Such a diagonal matrix is the identity, of 
course, so it a very special multiplicative property. Triangular matrices with 
1s on the diagonal are called “unit triangular” matrices. A unit triangular 
matrix is a unipotent matrix of index 1. A unipotent matrix of index k, is  
a square matrix A such that upper A minus upper IA −I is nilpotent of index k. Unit triangular 
matrices are often used in matrix factorizations, as we see later in this chapter 
and in Chap. 4. 
The products of banded matrices are generally banded with a wider band-
width. If the bandwidth is too great, obviously the matrix is no longer banded. 
Multiplication of Matrices and Vectors 
Although vectors and matrices are fundamentally diﬀerent kinds of mathe-
matical objects, it is often convenient to think of a vector as a matrix with 
only one element in one of its dimensions. 
In this scheme, we can immediately extend the concept of transpose of a 
matrix to transpose of a vector. 
This also provides for an immediate extension of Cayley matrix multipli-
cation to include vectors as either or both factors. In this scheme, we follow 
the convention that a vector corresponds to a column; that is,  if  x is a vector 
and A is a matrix, Ax or x Superscript normal upper T Baseline upper AxTA may be well-deﬁned, but neither xA nor upper A x Superscript normal upper TAxT
would represent anything, except in the case when all dimensions are 1. The 
alternative notation x Superscript normal upper T Baseline yxTy we introduced earlier for the dot product or inner 
product, left angle bracket x comma y right angle bracket<x, y>, of the vectors x and y is consistent with this paradigm. 
In general, it is not relevant to say that a vector is a “column” or a “row”; 
a vector is merely a one-dimensional (or rank 1) object. We will continue to 
write vectors as x equals left parenthesis x 1 comma ellipsis comma x Subscript n Baseline right parenthesisx = (x1, . . . , xn), but this does not imply that the vector is a 
“row vector.” 
Matrices with just one row or one column are diﬀerent objects from vectors. 
We represent a matrix with one row in a form such as upper Y equals left bracket y 11 ellipsis y Subscript 1 n Baseline right bracketY = [y11 . . . y1n], and  
we represent a matrix with one column in a form such as upper Z equals Start 3 By 1 Matrix 1st Row z 11 2nd Row vertical ellipsis 3rd Row z Subscript m Baseline 1 EndMatrixZ =
⎡
⎢⎣
z11
...
zm1
⎤
⎥⎦or as

3.3 Multiplication of Matrices
101
upper Z equals left bracket z 11 ellipsis z Subscript m Baseline 1 Baseline right bracket Superscript normal upper TZ = [z11 . . . zm1]T. 
(Compare the notation in Eqs. (1.1) and  (1.2) on page 2.) 
The Matrix/Vector Product as a Linear Combination 
If we represent the vectors formed by the columns of an n times mn × m matrix A 
as a 1 comma ellipsis comma a Subscript m Baselinea1, . . . , am, the matrix/vector product Ax is a linear combination of these 
columns of A: 
upper A x equals sigma summation Underscript i equals 1 Overscript m Endscripts x Subscript i Baseline a Subscript i Baseline periodAx =
m
E
i=1
xiai.
(3.63) 
(Here, each x Subscript ixi is a scalar, and each a Subscript iai is a vector.) 
Given the equation upper A x equals bAx = b, we have  b element of normal s normal p normal a normal n left parenthesis upper A right parenthesisb ∈span(A); that is,  the  n-vector b 
is in the k-dimensional column space of A, where  k less than or equals mk ≤m. 
The Matrix as a Mapping on Vector Spaces 
In this chapter, we have considered matrices to be fundamental objects. Only 
after deﬁning operations on matrices themselves have we deﬁned an operation 
by a matrix on a vector. Another way of thinking about matrices is as a class 
of functions or mappings on vector spaces. In this approach, we give primacy 
to the vector spaces. 
Let script upper V 1V1 and script upper V 2V2 be vector spaces of order m and n respectively. Then an 
n times mn × m matrix A is a function from script upper V 1V1 to script upper V 2V2 deﬁned for x element of script upper V 1x ∈V1 as 
x right arrow from bar upper A x periodx |→Ax.
(3.64) 
Matrices are “transformations” of vectors. There is nothing essentially 
diﬀerent in this development of concepts about matrices; it does, however, 
motivate terminology based in geometry that we will use from time to time 
(“rotations,” “projections,” and so on; see Sect. 4.1). 
A matrix changes the “direction” of a vector. The cosine of the angle 
between the x and Ax is 
normal c normal o normal s left parenthesis normal a normal n normal g normal l normal e left parenthesis x comma upper A x right parenthesis right parenthesis equals left parenthesis StartFraction left angle bracket x comma upper A x right angle bracket Over parallel to x parallel to parallel to upper A x parallel to EndFraction right parenthesiscos(angle(x, Ax)) =
( <x, Ax>
||x||||Ax||
)
(see page 47), which in general is not 1. (If A is square and symmetric, this 
expression is related to the Rayleigh quotient of A, upper R Subscript upper A Baseline left parenthesis x right parenthesisRA(x); see page 179.) 
Multiplication of Partitioned Matrices 
Multiplication and other operations with partitioned matrices are carried out 
with their submatrices in the obvious way. Thus, assuming the submatrices 
are conformable for multiplication,

102
3 Basic Properties of Matrices
Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column upper B 21 2nd Column upper B 22 EndMatrix equals Start 2 By 3 Matrix 1st Row 1st Column upper A 11 upper B 11 plus upper A 12 upper B 21 2nd Column upper A 11 upper B 12 plus upper A 12 upper B 22 2nd Row 1st Column upper A 21 upper B 11 plus upper A 22 upper B 21 2nd Column upper A 21 upper B 12 plus upper A 22 upper B 22 EndMatrix period
[
A11 A12
A21 A22
] [
B11 B12
B21 B22
]
=
[
A11B11 + A12B21
A11B12 + A12B22
A21B11 + A22B21
A21B12 + A22B22
]
.
It is clear that the product of conformable block diagonal matrices is block 
diagonal. 
Sometimes a matrix may be partitioned such that one partition is just a 
single column or row, that is, a vector or the transpose of a vector. In that 
case, we may use a notation such as 
left bracket upper X y right bracket[X y]
or 
left bracket upper X vertical bar y right bracket comma[X | y],
where X is a matrix and y is a vector. We develop the notation in the obvious 
fashion; for example, 
left bracket upper X y right bracket Superscript normal upper T Baseline left bracket upper X y right bracket equals Start 2 By 2 Matrix 1st Row 1st Column upper X Superscript normal upper T Baseline upper X 2nd Column upper X Superscript normal upper T Baseline y 2nd Row 1st Column y Superscript normal upper T Baseline upper X 2nd Column y Superscript normal upper T Baseline y EndMatrix period[X y]T [X y] =
[
XTX XTy
yTX yTy
]
.
(3.65) 
3.3.3 Elementary Operations on Matrices 
Many common computations involving matrices can be performed as a se-
quence of three simple types of operations on either the rows or the columns 
of the matrix: 
• the interchange of two rows (columns), 
• a scalar multiplication of a given row (column), and 
• the replacement of a given row (column) by the sum of that row (columns) 
and a scalar multiple of another row (column); that is, an axpy operation. 
Such an operation on the rows of a matrix can be performed by premultipli-
cation by a matrix in a standard form, and an operation on the columns of 
a matrix can be performed by postmultiplication by a matrix in a standard 
form. To repeat: 
• premultiplication: operation on rows; 
• postmultiplication: operation on columns. 
The matrix used to perform the operation is called an elementary trans-
formation matrix or elementary operator matrix. Such a matrix is the identity 
matrix transformed by the corresponding operation performed on its unit 
rows, e Subscript p Superscript normal upper TeT
p , or columns, e Subscript pep. 
In actual computations, we do not form the elementary transformation 
matrices explicitly, but their formulation allows us to discuss the operations 
in a systematic way and better understand the properties of the operations. 
Products of any of these elementary operator matrices can be used to eﬀect 
more complicated transformations. 
Operations on the rows are more common, and that is what we will dis-
cuss here, although operations on columns are completely analogous. These 
transformations of rows are called elementary row operations.

3.3 Multiplication of Matrices
103
Interchange of Rows or Columns: Permutation Matrices 
By ﬁrst interchanging the rows or columns of a matrix, it may be possible 
to partition the matrix in such a way that the partitions have interesting 
or desirable properties. Also, in the course of performing computations on a 
matrix, it is often desirable to interchange the rows or columns of the matrix. 
(This is an instance of “pivoting,” which will be discussed later, especially 
in Chap. 5.) In matrix computations, we almost never actually move data 
from one row or column to another; rather, the interchanges are eﬀected by 
changing the indexes to the data. 
Interchanging two rows of a matrix can be accomplished by premultiply-
ing the matrix by a matrix that is the identity with those same two rows 
interchanged; for example, 
Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 2nd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 3rd Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 EndMatrix Start 4 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 12 3rd Column a 13 2nd Row 1st Column a 21 2nd Column a 22 3rd Column a 23 3rd Row 1st Column a 31 2nd Column a 32 3rd Column a 33 4th Row 1st Column a 41 2nd Column a 42 3rd Column a 43 EndMatrix equals Start 4 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 12 3rd Column a 13 2nd Row 1st Column a 31 2nd Column a 32 3rd Column a 33 3rd Row 1st Column a 21 2nd Column a 22 3rd Column a 23 4th Row 1st Column a 41 2nd Column a 42 3rd Column a 43 EndMatrix period
⎡
⎢⎢⎣
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
⎤
⎥⎥⎦
⎡
⎢⎢⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43
⎤
⎥⎥⎦=
⎡
⎢⎢⎣
a11 a12 a13
a31 a32 a33
a21 a22 a23
a41 a42 a43
⎤
⎥⎥⎦.
The ﬁrst matrix in the expression above is called an elementary permutation 
matrix. It is the identity matrix with its second and third rows (or columns) 
interchanged. An elementary permutation matrix, which is the identity with 
the p normal t normal hpth and q normal t normal hqth rows interchanged, is denoted by upper E Subscript p qEpq. That is,  upper E Subscript p qEpq is the 
identity, except the p normal t normal hpth row is e Subscript q Superscript normal upper TeT
q and the q normal t normal hqth row is e Subscript p Superscript normal upper TeT
p . Note that  upper E Subscript p q Baseline equals upper E Subscript q pEpq = Eqp. 
Thus, for example, if the given matrix is 4 times m4×m, to interchange the second and 
third rows, we use 
upper E 23 equals upper E 32 equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 2nd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 3rd Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 EndMatrix periodE23 = E32 =
⎡
⎢⎢⎣
1 0 0 0
0 0 1 0
0 1 0 0
0 0 0 1
⎤
⎥⎥⎦.
(3.66) 
It is easy to see from the deﬁnition that an elementary permutation matrix 
is symmetric. Note that the notation upper E Subscript p qEpq does not indicate the order of the 
elementary permutation matrix; that must be speciﬁed in the context. 
Premultiplying a matrix A by a (conformable) upper E Subscript p qEpq results in an inter-
change of the p normal t normal hpth and q normal t normal hqth rows of A as we see above. Any permutation of 
rows of A can be accomplished by successive premultiplications by elemen-
tary permutation matrices. Note that the order of multiplication matters. 
Although a given permutation can be accomplished by diﬀerent elementary 
permutations, the number of elementary permutations that eﬀect a given per-
mutation is always either even or odd; that is, if an odd number of elementary 
permutations results in a given permutation, any other sequence of elemen-
tary permutations to yield the given permutation is also odd in number. Any 
given permutation can be eﬀected by successive interchanges of adjacent rows. 
Postmultiplying a matrix A by a (conformable) upper E Subscript p qEpq results in an inter-
change of the p normal t normal hpth and q normal t normal hqth columns of A:

104
3 Basic Properties of Matrices
Start 4 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 12 3rd Column a 13 2nd Row 1st Column a 21 2nd Column a 22 3rd Column a 23 3rd Row 1st Column a 31 2nd Column a 32 3rd Column a 33 4th Row 1st Column a 41 2nd Column a 42 3rd Column a 43 EndMatrix Start 3 By 3 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 2nd Row 1st Column 0 2nd Column 0 3rd Column 1 3rd Row 1st Column 0 2nd Column 1 3rd Column 0 EndMatrix equals Start 4 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 13 3rd Column a 12 2nd Row 1st Column a 21 2nd Column a 23 3rd Column a 22 3rd Row 1st Column a 31 2nd Column a 33 3rd Column a 32 4th Row 1st Column a 41 2nd Column a 43 3rd Column a 42 EndMatrix period
⎡
⎢⎢⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43
⎤
⎥⎥⎦
⎡
⎣
1 0 0
0 0 1
0 1 0
⎤
⎦=
⎡
⎢⎢⎣
a11 a13 a12
a21 a23 a22
a31 a33 a32
a41 a43 a42
⎤
⎥⎥⎦.
Note that 
upper A equals upper E Subscript p q Baseline upper E Subscript p q Baseline upper A equals upper A upper E Subscript p q Baseline upper E Subscript p q Baseline semicolonA = EpqEpqA = AEpqEpq;
(3.67) 
that is, as an operator, an elementary permutation matrix is its own inverse 
operator: upper E Subscript p q Baseline upper E Subscript p q Baseline equals upper IEpqEpq = I. 
Because all of the elements of a permutation matrix are 0 or 1, the trace 
of an n times nn × n elementary permutation matrix is n minus 2n −2. 
The product of elementary permutation matrices is also a permutation 
matrix in the sense that it permutes several rows or columns. For example, 
premultiplying A by the matrix upper Q equals upper E Subscript p q Baseline upper E Subscript q rQ = EpqEqr will yield a matrix whose p normal t normal hpth row 
is the r normal t normal hrth row of the original A, whose  q normal t normal hqth row is the p normal t normal hpth row of A, and  whose  
r normal t normal hrth row is the q normal t normal hqth row of A. We often use the notation upper E Subscript left parenthesis pi right parenthesisE(π) to denote a more 
general permutation matrix. This expression will usually be used generically, 
but sometimes we will specify the permutation, piπ. 
A general permutation matrix (that is, a product of elementary permuta-
tion matrices) is not necessarily symmetric, but its transpose is also a per-
mutation matrix. It is not necessarily its own inverse, but its permutations 
can be reversed by a permutation matrix formed by products of permutation 
matrices in the opposite order; that is, 
upper E Subscript left parenthesis pi right parenthesis Superscript normal upper T Baseline upper E Subscript left parenthesis pi right parenthesis Baseline equals upper I periodET
(π)E(π) = I.
As a prelude to other matrix operations, we often permute both rows and 
columns, so we often have a representation such as 
upper B equals upper E Subscript left parenthesis pi 1 right parenthesis Baseline upper A upper E Subscript left parenthesis pi 2 right parenthesis Baseline commaB = E(π1)AE(π2),
(3.68) 
where upper E Subscript left parenthesis pi 1 right parenthesisE(π1) is a permutation matrix to permute the rows and upper E Subscript left parenthesis pi 2 right parenthesisE(π2) is a per-
mutation matrix to permute the columns. We use these kinds of operations to 
form a full rank partitioning as in Eq. (3.145) on page 125, to obtain an equiv-
alent canonical form as in Eq. (3.173) on page 135 and LDU decomposition of 
a matrix as in Eq. (4.30) on page 236. These equations are used to determine 
the number of linearly independent rows and columns and to represent the 
matrix in a form with a maximal set of linearly independent rows and columns 
clearly identiﬁed. 
The Vec-Permutation Matrix 
A special permutation matrix is the matrix that transforms the vector normal v normal e normal c left parenthesis upper A right parenthesisvec(A)
into normal v normal e normal c left parenthesis upper A Superscript normal upper T Baseline right parenthesisvec(AT). If  A is n times mn × m, the  matrix  upper K Subscript n mKnm that does this is n m times n mnm × nm. We  
have 
normal v normal e normal c left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals upper K Subscript n m Baseline normal v normal e normal c left parenthesis upper A right parenthesis periodvec(AT) = Knmvec(A).
(3.69) 
The matrix upper K Subscript n mKnm is called the nm vec-permutation matrix.

3.3 Multiplication of Matrices
105
Scalar Row or Column Multiplication 
Often, numerical computations with matrices are more accurate if the rows 
have roughly equal norms. For this and other reasons, we often transform a 
matrix by multiplying one of its rows by a scalar. This transformation can also 
be performed by premultiplication by an elementary transformation matrix. 
For multiplication of the p normal t normal hpth row by the scalar, the elementary transformation 
matrix, which is denoted by upper E Subscript p Baseline left parenthesis a right parenthesisEp(a), is the identity matrix in which the p normal t normal hpth
diagonal element has been replaced by a. Thus, for example, if the given 
matrix is 4 times m4 × m, to multiply the second row by a, we use  
upper E 2 left parenthesis a right parenthesis equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 2nd Row 1st Column 0 2nd Column a 3rd Column 0 4th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 EndMatrix periodE2(a) =
⎡
⎢⎢⎣
1 0 0 0
0 a 0 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦.
(3.70) 
Postmultiplication of a given matrix by the multiplier matrix upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) results 
in the multiplication of the p normal t normal hpth column by the scalar. For this, upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) is a square 
matrix of order equal to the number of columns of the given matrix. 
Note that the notation upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) does not indicate the number of rows and 
columns. This must be speciﬁed in the context. 
Note that, if a not equals 0a /= 0, 
upper A equals upper E Subscript p Baseline left parenthesis 1 divided by a right parenthesis upper E Subscript p Baseline left parenthesis a right parenthesis upper A commaA = Ep(1/a)Ep(a)A,
(3.71) 
that is, as an operator, the inverse operator is a row multiplication matrix on 
the same row and with the reciprocal as the multiplier. 
Axpy Row or Column Transformations 
The other elementary operation is an axpy on two rows and a replacement of 
one of those rows with the result 
a Subscript p Baseline left arrow a a Subscript q Baseline plus a Subscript p Baseline periodap ←aaq + ap.
This operation also can be eﬀected by premultiplication by a matrix formed 
from the identity matrix by inserting the scalar in the left parenthesis p comma q right parenthesis(p, q) position. Such a 
matrix is denoted by upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a). Thus, for example, if the given matrix is 4 times m4 × m, 
to add a times the third row to the second row, we use 
upper E 23 left parenthesis a right parenthesis equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column a 4th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 EndMatrix periodE23(a) =
⎡
⎢⎢⎣
1 0 0 0
0 1 a 0
0 0 1 0
0 0 0 1
⎤
⎥⎥⎦.
(3.72) 
Premultiplication of a matrix A by such a matrix, 
upper E Subscript p q Baseline left parenthesis a right parenthesis upper A commaEpq(a)A,
(3.73)

106
3 Basic Properties of Matrices
yields a matrix whose p normal t normal hpth row is a times the q normal t normal hqth row plus the original row. 
Given the 4 times 34 × 3 matrix upper A equals left parenthesis a Subscript i j Baseline right parenthesisA = (aij), we have  
upper E 23 left parenthesis a right parenthesis upper A equals Start 4 By 3 Matrix 1st Row 1st Column a 11 2nd Column a 12 3rd Column a 13 2nd Row 1st Column a 21 plus a a 31 2nd Column a 22 plus a a 32 3rd Column a 23 plus a a 33 3rd Row 1st Column a 31 2nd Column a 32 3rd Column a 33 4th Row 1st Column a 41 2nd Column a 42 3rd Column a 43 EndMatrix periodE23(a)A =
⎡
⎢⎢⎣
a11
a12
a13
a21 + aa31 a22 + aa32 a23 + aa33
a31
a32
a33
a41
a42
a43
⎤
⎥⎥⎦.
(3.74) 
Postmultiplication of a matrix A by an axpy operator matrix, 
upper A upper E Subscript p q Baseline left parenthesis a right parenthesis commaAEpq(a),
yields a matrix whose q normal t normal hqth column is a times the p normal t normal hpth column plus the original 
column. For this, upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a) is a square matrix of order equal to the number of 
columns of the given matrix. Note that the column that is changed corresponds 
to the second subscript in upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a). 
Note that 
upper A equals upper E Subscript p q Baseline left parenthesis negative a right parenthesis upper E Subscript p q Baseline left parenthesis a right parenthesis upper A semicolonA = Epq(−a)Epq(a)A;
(3.75) 
that is, as an operator, the inverse operator is the same axpy elementary 
operator matrix with the negative of the multiplier. 
Gaussian Elimination: Gaussian Transformation Matrix 
A common use of axpy operator matrices is to form a matrix with zeros in 
all positions of a given column below a given position in the column. These 
operations usually follow an operation by a scalar row multiplier matrix that 
puts a 1 in the position of interest. For example, given an n times mn × m matrix A 
with a Subscript i j Baseline not equals 0aij /= 0, to put a 1 in  the  left parenthesis i comma j right parenthesis(i, j) position and 0s in all positions of the j normal t normal hjth
column below the i normal t normal hith row, we form the product 
upper E Subscript n i Baseline left parenthesis minus a Subscript n j Baseline right parenthesis midline horizontal ellipsis upper E Subscript i plus 1 comma i Baseline left parenthesis minus a Subscript i plus 1 comma j Baseline right parenthesis upper E Subscript i Baseline left parenthesis 1 divided by a Subscript i j Baseline right parenthesis upper A periodEni(−anj) · · · Ei+1,i(−ai+1,j)Ei(1/aij)A.
(3.76) 
This process is called Gaussian elimination, and  the matrix  
upper G Subscript i j Baseline equals upper E Subscript n i Baseline left parenthesis minus a Subscript n j Baseline right parenthesis midline horizontal ellipsis upper E Subscript i plus 1 comma i Baseline left parenthesis minus a Subscript i plus 1 comma j Baseline right parenthesis upper E Subscript i Baseline left parenthesis 1 divided by a Subscript i j Baseline right parenthesisGij = Eni(−anj) · · · Ei+1,i(−ai+1,j)Ei(1/aij)
(3.77) 
is called a Gaussian transformation matrix, or just a  Gaussian matrix. 
A Gaussian matrix of this form is for transformation by premultiplication. 
We can likewise zero out all elements in the i normal t normal hith row except the one in the 
left parenthesis i j right parenthesis normal t normal h(ij)th position by similar postmultiplications. 
Notice that it is lower triangular, and its inverse, also lower triangular, is 
upper G Subscript i j Superscript negative 1 Baseline equals upper E Subscript i Baseline left parenthesis a Subscript i j Baseline right parenthesis upper E Subscript i plus 1 comma i Baseline left parenthesis a Subscript i plus 1 comma j Baseline right parenthesis midline horizontal ellipsis upper E Subscript n i Baseline left parenthesis a Subscript n j Baseline right parenthesis periodG−1
ij = Ei(aij)Ei+1,i(ai+1,j) · · · Eni(anj).
(3.78) 
Gaussian elimination is often performed sequentially down the diagonal 
elements of a matrix (see its use in the LU factorization on page 234, for  
example).

3.3 Multiplication of Matrices
107
To form a matrix with zeros in all positions of a given column except one, 
we use additional matrices for the rows above the given element: 
upper G overTilde Subscript i j Baseline equals upper E Subscript n i Baseline left parenthesis minus a Subscript n j Baseline right parenthesis midline horizontal ellipsis upper E Subscript i plus 1 comma i Baseline left parenthesis minus a Subscript i plus 1 comma j Baseline right parenthesis upper E Subscript i minus 1 comma i Baseline left parenthesis minus a Subscript i minus 1 comma j Baseline right parenthesis midline horizontal ellipsis upper E Subscript 1 i Baseline left parenthesis minus a Subscript 1 j Baseline right parenthesis upper E Subscript i Baseline left parenthesis 1 divided by a Subscript i j Baseline right parenthesis period-Gij = Eni(−anj) · · · Ei+1,i(−ai+1,j)Ei−1,i(−ai−1,j) · · · E1i(−a1j)Ei(1/aij).
(3.79) 
This is also called a Gaussian transformation matrix. 
If at some point a Subscript i i Baseline equals 0aii = 0, the operations of Eq. (3.76) cannot be performed. 
In that case, we may ﬁrst interchange the i normal t normal hith row or column with the k normal t normal hkth
row, where k greater than ik > i and a Subscript k i Baseline not equals 0aki /= 0. Such an interchange is called pivoting. We will 
discuss pivoting in more detail on page 270 in Chap. 5. It may  be  the case  
that no row or column can supply a nonnegative replacement value. In that 
case, the Gaussian transformation matrix is not of full rank. 
As we mentioned above, in actual computations, we do not form the ele-
mentary transformation matrices explicitly, but their formulation allows us to 
discuss the operations in a systematic way and better understand the prop-
erties of the operations. 
This is an instance of a principle that we will encounter repeatedly: the 
form of a mathematical expression and the way the expression should be eval-
uated in actual practice may be quite diﬀerent. 
These elementary transformations are the basic operations in Gaussian 
elimination, which is discussed in Sects. 4.5 and 5.2.1. 
Elementary Operator Matrices: Summary of Notation and 
Properties 
Because we have introduced various notations for elementary operator matri-
ces, it may be worthwhile to review the notations. The notation is useful, and 
I will use it from time to time, but unfortunately, there is no general form for 
the notation. I will generally use an “E” as the root symbol for the matrix, 
but the speciﬁc type is indicated by various other symbols. 
Referring back to the listing of the types of operations on page 102, we  
have the various elementary operator matrices: 
up
per E Subscript p qEpq: the interchange of rows p and q (upper E Subscript p qEpq is the same as upper E Subscript q pEqp) 
upper E Subscript p q Baseline equals upper E Subscript q p Baseline equals Start 9 By 9 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 4th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 1 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 5th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 6th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 1 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 7th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 8th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 1 9th Column 0 9th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 1 EndMatrix StartLayout 1st Row Blank 2nd Row Blank 3rd Row p 4th Row Blank 5th Row Blank 6th Row q 7th Row Blank 8th Row Blank 9th Row Blank EndLayoutEpq = Eqp =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 1 · · · 0 0
... ... ... ... ... ... ... ... ...
0 0 · · · 1 · · · 0 · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
q
(3.80)

108
3 Basic Properties of Matrices
It is symmetric, 
upper E Subscript p q Superscript normal upper T Baseline equals upper E Subscript p q Baseline commaET
pq = Epq,
(3.81) 
and it is its own inverse, 
upper E Subscript p q Superscript negative 1 Baseline equals upper E Subscript p q Baseline commaE−1
pq = Epq,
(3.82) 
that is, it is orthogonal. 
upper E Subscript left parenthesis pi right parenthesisE(π): a general permutation of rows, where piπ denotes a permutation. We 
have 
upper E Subscript left parenthesis pi right parenthesis Baseline equals upper E Subscript p 1 q 1 Baseline midline horizontal ellipsis upper E Subscript p Sub Subscript k Subscript q Sub Subscript k Subscript Baseline comma for some p 1 comma ellipsis comma p Subscript k Baseline and q 1 comma ellipsis comma q Subscript k Baseline periodE(π) = Ep1q1 · · · Epkqk, for some p1, . . . , pk and q1, . . . , qk.
(3.83) 
up
per E Subscript p Baseline left parenthesis a right parenthesisEp(a): multiplication of row p by a. 
upper E Subscript p Baseline left parenthesis a right parenthesis equals Start 7 By 7 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column 0 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column vertical ellipsis 4th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column a 5th Column midline horizontal ellipsis 6th Column 0 7th Column 0 5th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column vertical ellipsis 6th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 1 7th Column 0 7th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column 1 EndMatrix StartLayout 1st Row Blank 2nd Row Blank 3rd Row Blank 4th Row p 5th Row Blank 6th Row Blank 7th Row Blank 8th Row Blank EndLayoutEp(a) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 0
...
... ... ... ... ...
...
0 0 · · · a · · · 0 0
...
... ... ... ... ...
...
0 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
(3.84) 
Its inverse is 
upper E Subscript p Superscript negative 1 Baseline left parenthesis a right parenthesis equals upper E Subscript p Baseline left parenthesis 1 divided by a right parenthesis periodE−1
p (a) = Ep(1/a).
(3.85) 
up
per E Subscript p q Baseline left parenthesis a right parenthesisEpq(a): the replacement of row p by the sum of row p and a times row q. 
If q greater than pq > p, 
upper E Subscript p q Baseline left parenthesis a right parenthesis equals Start 9 By 9 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 4th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 1 5th Column midline horizontal ellipsis 6th Column a 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 5th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 6th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 1 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 7th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column down right diagonal ellipsis 4th Column vertical ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 7th Column down right diagonal ellipsis 8th Column vertical ellipsis 9th Column vertical ellipsis 8th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 1 9th Column 0 9th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 1 EndMatrix StartLayout 1st Row Blank 2nd Row Blank 3rd Row p 4th Row Blank 5th Row Blank 6th Row q 7th Row Blank 8th Row Blank 9th Row Blank EndLayoutEpq(a) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 · · · 0 · · · 0 0
0 1 · · · 0 · · · 0 · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 1 · · · a · · · 0 0
...
... ... ... ... ... ... ...
...
0 0 · · · 0 · · · 1 · · · 0 0
... ... ... ... ... ... ... ... ...
0 0 · · · 0 · · · 0 · · · 1 0
0 0 · · · 0 · · · 0 · · · 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
p
q
(3.86) 
Its inverse is 
upper E Subscript p q Superscript negative 1 Baseline left parenthesis a right parenthesis equals upper E Subscript p q Baseline left parenthesis negative a right parenthesis periodE−1
pq (a) = Epq(−a).
(3.87) 
Recall that these operations are eﬀected by premultiplication. The same kinds 
of operations on the columns are eﬀected by postmultiplication.

3.3 Multiplication of Matrices
109
Determinants of Elementary Operator Matrices 
The determinant of an elementary permutation matrix upper E Subscript p qEpq has only one term 
in the sum that deﬁnes the determinant (Eq. (3.28), page 85), and that term 
is 1 times sigmaσ evaluated at the permutation that exchanges p and q. As we  
have seen (page 86), this is an odd permutation; hence, for an elementary 
permutation matrix upper E Subscript p qEpq, 
normal d normal e normal t left parenthesis upper E Subscript p q Baseline right parenthesis equals negative 1 perioddet(Epq) = −1.
(3.88) 
Because a general permutation matrix upper E Subscript left parenthesis pi right parenthesisE(π) can be formed as the product 
of elementary permutation matrices, which together form the permutation piπ, 
we have from Eq. (3.88) 
normal d normal e normal t left parenthesis upper E Subscript pi Baseline right parenthesis equals sigma left parenthesis pi right parenthesis commadet(Eπ) = σ(π),
(3.89) 
where sigma left parenthesis pi right parenthesis equals 1σ(π) = 1 if piπ is an even permutation and negative 1−1 otherwise, as deﬁned in 
Eq. (3.27). 
Because all terms in normal d normal e normal t left parenthesis upper E Subscript p q Baseline upper A right parenthesisdet(EpqA) are exactly the same terms as in normal d normal e normal t left parenthesis upper A right parenthesisdet(A)
but with one diﬀerent permutation in each term, we have 
normal d normal e normal t left parenthesis upper E Subscript p q Baseline upper A right parenthesis equals minus normal d normal e normal t left parenthesis upper A right parenthesis perioddet(EpqA) = −det(A).
More generally, if A and upper E Subscript left parenthesis pi right parenthesisE(π) are n times nn × n matrices, and upper E Subscript left parenthesis pi right parenthesisE(π) is any permuta-
tion matrix (that is, any product of upper E Subscript p qEpq matrices), then normal d normal e normal t left parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline upper A right parenthesisdet(E(π)A) is either 
normal d normal e normal t left parenthesis upper A right parenthesisdet(A) or minus normal d normal e normal t left parenthesis upper A right parenthesis−det(A) because all terms in normal d normal e normal t left parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline upper A right parenthesisdet(E(π)A) are exactly the same as 
the terms in normal d normal e normal t left parenthesis upper A right parenthesisdet(A) but possibly with diﬀerent signs because the permutations 
are diﬀerent. In fact, the diﬀerences in the permutations are exactly the same 
as the permutation of 1 comma ellipsis comma n1, . . . , n in upper E Subscript left parenthesis pi right parenthesisE(π); hence, 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline upper A right parenthesis 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline right parenthesis normal d normal e normal t left parenthesis upper A right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma left parenthesis pi right parenthesis normal d normal e normal t left parenthesis upper A right parenthesis period EndLayoutdet(E(π)A) = det(E(π)) det(A)
= σ(π)det(A).
(In Eq. (3.95) below, we will see that this equation holds more generally.) 
The determinant of an elementary row multiplication matrix upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) is 
normal d normal e normal t left parenthesis upper E Subscript p Baseline left parenthesis a right parenthesis right parenthesis equals a perioddet(Ep(a)) = a.
(3.90) 
If A and upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) are n times nn × n matrices, then 
normal d normal e normal t left parenthesis upper E Subscript p Baseline left parenthesis a right parenthesis upper A right parenthesis equals a normal d normal e normal t left parenthesis upper A right parenthesis commadet(Ep(a)A) = adet(A),
as we see from the deﬁnition of the determinant, Eq. (3.28). (Again, this also 
follows from the general result in Eq. (3.95) below.)  
The determinant of an elementary axpy matrix upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a) is 1, 
normal d normal e normal t left parenthesis upper E Subscript p q Baseline left parenthesis a right parenthesis right parenthesis equals 1 commadet(Epq(a)) = 1,
(3.91) 
because the term consisting of the product of the diagonals is the only term 
in the determinant.

110
3 Basic Properties of Matrices
Now consider normal d normal e normal t left parenthesis upper E Subscript p q Baseline left parenthesis a right parenthesis upper A right parenthesisdet(Epq(a)A) for an n times nn×n matrix A. Expansion in the minors 
(Eq. (3.37)) along the p normal t normal hpth row yields 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper E Subscript p q Baseline left parenthesis a right parenthesis upper A right parenthesis 2nd Column equals 3rd Column sigma summation Underscript j equals 1 Overscript n Endscripts left parenthesis a Subscript p j Baseline plus a a Subscript q j Baseline right parenthesis left parenthesis negative 1 right parenthesis Superscript p plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis p right parenthesis left parenthesis j right parenthesis Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript p j Baseline left parenthesis negative 1 right parenthesis Superscript p plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis p right parenthesis left parenthesis j right parenthesis Baseline right parenthesis plus a sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript q j Baseline left parenthesis negative 1 right parenthesis Superscript p plus j Baseline normal d normal e normal t left parenthesis upper A Subscript minus left parenthesis p right parenthesis left parenthesis j right parenthesis Baseline right parenthesis period EndLayoutdet(Epq(a)A) =
n
E
j=1
(apj + aaqj)(−1)p+jdet(A−(p)(j))
=
n
E
j=1
apj(−1)p+jdet(A−(p)(j)) + a
n
E
j=1
aqj(−1)p+jdet(A−(p)(j)).
From Eq. (3.43) on page 91, we see that the second term is 0, and since the 
ﬁrst term is just the determinant of A, we have  
normal d normal e normal t left parenthesis upper E Subscript p q Baseline left parenthesis a right parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis perioddet(Epq(a)A) = det(A).
(3.92) 
(Again, this also follows from the general result in Eq. (3.95) below. I have  
shown the steps in the speciﬁc case because I think they help to see the eﬀect 
of the elementary operator matrix.) 
3.3.4 The Trace of a Cayley Product that Is Square 
A useful property of the trace for the matrices A and B that are conformable 
for the multiplications AB and BA is 
normal t normal r left parenthesis upper A upper B right parenthesis equals normal t normal r left parenthesis upper B upper A right parenthesis periodtr(AB) = tr(BA).
(3.93) 
This is obvious from the deﬁnitions of matrix multiplication and the trace. 
Note that A and B may not be square (so the trace is not deﬁned for them), 
but if they are conformable for the multiplications, then both AB and BA 
are square. 
Because of the associativity of matrix multiplication, this relation can be 
extended as 
normal t normal r left parenthesis upper A upper B upper C right parenthesis equals normal t normal r left parenthesis upper B upper C upper A right parenthesis equals normal t normal r left parenthesis upper C upper A upper B right parenthesistr(ABC) = tr(BCA) = tr(CAB)
(3.94) 
for matrices A, B, and  C that are conformable for the multiplications indi-
cated. Notice that the individual matrices need not be square. This fact is 
very useful in working with quadratic forms, as in Eq. (3.103). 
3.3.5 The Determinant of a Cayley Product of Square Matrices 
An important property of the determinant is 
normal d normal e normal t left parenthesis upper A upper B right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis normal d normal e normal t left parenthesis upper B right parenthesisdet(AB) = det(A) det(B)
(3.95) 
if A and B are square matrices conformable for multiplication. We see this by 
ﬁrst forming 
normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column upper I 2nd Column upper A 2nd Row 1st Column 0 2nd Column upper I EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper A 2nd Column 0 2nd Row 1st Column negative upper I 2nd Column upper B EndMatrix right parenthesis equals normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column upper A upper B 2nd Row 1st Column negative upper I 2nd Column upper B EndMatrix right parenthesisdet
([ I A
0 I
] [ A 0
−I B
])
= det
([
0 AB
−I
B
])
(3.96)

3.3 Multiplication of Matrices
111
and then observing from Eq. (3.34) on page 88 that the right-hand side is 
normal d normal e normal t left parenthesis upper A upper B right parenthesisdet(AB). Now consider the left-hand side. The matrix that is the ﬁrst factor 
on the left-hand side is a product of elementary axpy transformation matrices; 
that is, it is a matrix that when postmultiplied by another matrix merely adds 
multiples of rows in the lower part of the matrix to rows in the upper part of 
the matrix. If A and B are n times nn × n (and so the identities are likewise n times nn × n), 
the full matrix is the product: 
Start 2 By 2 Matrix 1st Row 1st Column upper I 2nd Column upper A 2nd Row 1st Column 0 2nd Column upper I EndMatrix equals upper E Subscript 1 comma n plus 1 Baseline left parenthesis a 11 right parenthesis midline horizontal ellipsis upper E Subscript 1 comma 2 n Baseline left parenthesis a Subscript 1 n Baseline right parenthesis upper E Subscript 2 comma n plus 1 Baseline left parenthesis a 21 right parenthesis midline horizontal ellipsis upper E Subscript 2 comma 2 n Baseline left parenthesis a Subscript 2 comma n Baseline right parenthesis midline horizontal ellipsis upper E Subscript n comma 2 n Baseline left parenthesis a Subscript n n Baseline right parenthesis period
[ I A
0 I
]
= E1,n+1(a11) · · · E1,2n(a1n)E2,n+1(a21) · · · E2,2n(a2,n) · · · En,2n(ann).
Hence, applying Eq. (3.92) recursively, we have 
normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column upper I 2nd Column upper A 2nd Row 1st Column 0 2nd Column upper I EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper A 2nd Column 0 2nd Row 1st Column negative upper I 2nd Column upper B EndMatrix right parenthesis equals normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column upper A 2nd Column 0 2nd Row 1st Column negative upper I 2nd Column upper B EndMatrix right parenthesis commadet
([
I A
0 I
] [
A 0
−I B
])
= det
([
A 0
−I B
])
,
and from Eq. (3.33) we have  
normal d normal e normal t left parenthesis Start 2 By 2 Matrix 1st Row 1st Column upper A 2nd Column 0 2nd Row 1st Column negative upper I 2nd Column upper B EndMatrix right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis normal d normal e normal t left parenthesis upper B right parenthesis commadet
([ A 0
−I B
])
= det(A)det(B),
and so ﬁnally we have Eq. (3.95). 
From Eq. (3.95), we see that if A and B are square matrices conformable 
for multiplication, then 
StartLayout 1st Row normal d normal e normal t left parenthesis upper A upper B right parenthesis equals normal d normal e normal t left parenthesis upper B upper A right parenthesis period EndLayoutdet(AB) = det(BA).
(3.97) 
(Recall, in general, even in the case of square matrices, upper A upper B not equals upper B upper AAB /= BA.) This 
equation is to be contrasted with Eq. (3.93), normal t normal r left parenthesis upper A upper B right parenthesis equals normal t normal r left parenthesis upper B upper A right parenthesistr(AB) = tr(BA), which does 
not even require that the matrices be square. A simple counterexample for 
nonsquare matrices is normal d normal e normal t left parenthesis x x Superscript normal upper T Baseline right parenthesis not equals normal d normal e normal t left parenthesis x Superscript normal upper T Baseline x right parenthesisdet(xxT) /= det(xTx), where  x is a vector with at least 
two elements. (Here, think of the vector as an n times 1n × 1 matrix. This counterex-
ample can be seen in various ways. One way is to use a fact that we will 
encounter on page 139, and observe that normal d normal e normal t left parenthesis x x Superscript normal upper T Baseline right parenthesis equals 0det(xxT) = 0 for any x with at least 
two elements.) 
The Adjugate in Matrix Multiplication 
The adjugate has an interesting property involving matrix multiplication and 
the identity matrix: 
upper A normal a normal d normal j left parenthesis upper A right parenthesis equals normal a normal d normal j left parenthesis upper A right parenthesis upper A equals normal d normal e normal t left parenthesis upper A right parenthesis upper I periodA adj(A) = adj(A)A = det(A)I.
(3.98) 
To see this, consider the left parenthesis i comma j right parenthesis normal t normal h(i, j)th element of upper A normal a normal d normal j left parenthesis upper A right parenthesisA adj(A). By the deﬁnition of the 
multiplication of A and normal a normal d normal j left parenthesis upper A right parenthesisadj(A), that element is sigma summation Underscript k Endscripts a Subscript i k Baseline left parenthesis normal a normal d normal j left parenthesis upper A right parenthesis right parenthesis Subscript k jE
k aik(adj(A))kj. Now, noting  
the reversal of the subscripts in normal a normal d normal j left parenthesis upper A right parenthesisadj(A) in Eq. (3.42), and using Eqs. (3.37) 
and (3.43), we have

112
3 Basic Properties of Matrices
sigma summation Underscript k Endscripts a Subscript i k Baseline left parenthesis normal a normal d normal j left parenthesis upper A right parenthesis right parenthesis Subscript k j Baseline equals StartLayout Enlarged left brace 1st Row 1st Column normal d normal e normal t left parenthesis upper A right parenthesis 2nd Column normal i normal f i equals j 2nd Row 1st Column 0 2nd Column normal i normal f i not equals j semicolon EndLayout
E
k
aik(adj(A))kj =
(
det(A) if i = j
0
if i /= j;
that is, upper A normal a normal d normal j left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis upper IA adj(A) = det(A)I. 
3.3.6 Outer Products of Vectors 
The outer product of the vectors x and y is the matrix 
x y Superscript normal upper T Baseline periodxyT.
(3.99) 
Note that the deﬁnition of the outer product does not require the vectors to 
be of equal length. Note also that while the inner product is commutative, 
the outer product is not commutative (although it does have the property 
x y Superscript normal upper T Baseline equals left parenthesis y x Superscript normal upper T Baseline right parenthesis Superscript normal upper TxyT = (yxT)T). 
While the inner product is a mapping from normal upper I normal upper R Superscript n Baseline times normal upper I normal upper R Superscript nIRn × IRn to normal upper I normal upper RIR, the outer 
product of two vectors is a mapping 
normal upper I normal upper R Superscript n Baseline times normal upper I normal upper R Superscript m Baseline right arrow script upper M subset of or equal to normal upper I normal upper R Superscript n times m Baseline commaIRn × IRm →M ⊆IRn×m,
where script upper MM is the set of n times mn×m matrices of rank one. (We will deﬁne and discuss 
matrix rank in Sect. 3.4, beginning on page 121. Also, see Exercise 3.16.) 
A very common outer product is of a vector with itself: 
x x Superscript normal upper T Baseline periodxxT.
The outer product of a vector with itself is obviously a symmetric matrix. 
We should again note some subtleties of diﬀerences in the types of objects 
that result from operations. If A and B are matrices conformable for the 
operation, the product upper A Superscript normal upper T Baseline upper BATB is a matrix even if both A and B are n times 1n × 1 and 
so the result is 1 times 11 × 1. For the vectors x and y and matrix C, however, x Superscript normal upper T Baseline yxTy
and x Superscript normal upper T Baseline upper C yxTCy are scalars; hence, the dot product and a quadratic form are not 
the same as the result of a matrix multiplication. The dot product is a scalar, 
and the result of a matrix multiplication is a matrix. The outer product of 
vectors is a matrix, even if both vectors have only one element. Nevertheless, 
as we have mentioned before, we will treat a one-by-one matrix or a vector 
with only one element as a scalar whenever it is convenient to do so. 
3.3.7 Bilinear and Quadratic Forms: Deﬁniteness 
Given a matrix A of conformable shape, a variation of the vector dot product, 
x Superscript normal upper T Baseline upper A yxTAy, is called a bilinear form, and the special bilinear form x Superscript normal upper T Baseline upper A xxTAx is called 
a quadratic form. Note  
x Superscript normal upper T Baseline upper A Superscript normal upper T Baseline x equals x Superscript normal upper T Baseline upper A x normal a normal n normal d x Superscript normal upper T Baseline upper A Superscript normal upper T Baseline y equals y Superscript normal upper T Baseline upper A x not equals x Superscript normal upper T Baseline upper A y in general periodxTATx = xTAx
and
xTATy = yTAx /= xTAy
in general.

3.3 Multiplication of Matrices
113
Although in the deﬁnition of quadratic form we do not require A to be sym-
metric — because for a given value of x and a given value of the quadratic form 
x Superscript normal upper T Baseline upper A xxTAx there is a unique symmetric matrix upper A Subscript sAs such that x Superscript normal upper T Baseline upper A Subscript s Baseline x equals x Superscript normal upper T Baseline upper A xxTAsx = xTAx — we  
generally work only with symmetric matrices in dealing with quadratic forms. 
(The matrix upper A Subscript sAs is one half left parenthesis upper A plus upper A Superscript normal upper T Baseline right parenthesis 1
2(A + AT); see Exercise 3.5.) Quadratic forms correspond 
to sums of squares and hence play an important role in statistical applications. 
Nonnegative Deﬁnite and Positive Deﬁnite Matrices 
A symmetric matrix A such that for any (conformable and real) vector x the 
quadratic form x Superscript normal upper T Baseline upper A xxTAx is nonnegative, that is, 
x Superscript normal upper T Baseline upper A x greater than or equals 0 commaxTAx ≥0,
(3.100) 
is called a nonnegative deﬁnite matrix. (There is another term, “positive 
semideﬁnite matrix” and its acronym PSD, that is often used to mean “non-
negative deﬁnite matrix,” but the term is not used consistently in the litera-
ture. I will generally avoid the term “semideﬁnite.”) We denote the fact that 
A is nonnegative deﬁnite by 
upper A succeeds above single line equals sign 0 periodA > 0.
(Note that we consider  0 Subscript n times n0n×n to be nonnegative deﬁnite.) 
A symmetric matrix A such that for any (conformable) vector x not equals 0x /= 0 the 
quadratic form 
x Superscript normal upper T Baseline upper A x greater than 0xTAx > 0
(3.101) 
is called a positive deﬁnite matrix. We denote the fact that A is positive 
deﬁnite by 
upper A succeeds 0 periodA > 0.
(Recall that upper A greater than or equals 0A ≥0 and upper A greater than 0A > 0 mean, respectively, that all elements of A are 
nonnegative and positive.) 
Nonnegative and positive deﬁnite matrices are very important in applica-
tions. We will encounter them from time to time in this chapter, and then we 
will discuss more of their properties in Sect. 8.3. 
In this book, we use the terms “nonnegative deﬁnite” and “positive def-
inite” only for symmetric matrices. In other literature, these terms may be 
used more generally; that is, for any (square) matrix that satisﬁes (3.100) 
or (3.101). 
Ordinal Relations among Symmetric Matrices 
When A and B are symmetric matrices of the same order, we write upper A succeeds above single line equals sign upper BA > B to 
mean upper A minus upper B succeeds above single line equals sign 0A −B > 0 and upper A succeeds upper BA > B to mean upper A minus upper B succeeds 0A −B > 0. 
The succeeds above single line equals sign> relationship is a partial ordering and the succeeds> relationship is transi-
tive; that is, if for conformable matrices, upper A succeeds upper BA > B and upper B succeeds upper CB > C, then  upper A succeeds upper CA > C
(See Exercise 8.2 on page 433; also compare ordinal relations among vectors, 
page 25.)

114
3 Basic Properties of Matrices
The Trace of Inner and Outer Products 
The invariance of the trace to permutations of the factors in a product 
(Eq. (3.93)) is particularly useful in working with bilinear and quadratic forms. 
Let A be an n times mn × m matrix, x be an n-vector, and y be an m-vector. Because 
the bilinear form is a scalar (or a 1 times 11×1 matrix), and because of the invariance, 
we have the very useful fact 
StartLayout 1st Row 1st Column x Superscript normal upper T Baseline upper A y 2nd Column equals 3rd Column normal t normal r left parenthesis x Superscript normal upper T Baseline upper A y right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper A y x Superscript normal upper T Baseline right parenthesis period EndLayoutxTAy = tr(xTAy)
= tr(AyxT).
(3.102) 
A common instance is when A is square and x equals yx = y. We have for the quadratic 
form the equality 
x Superscript normal upper T Baseline upper A x equals normal t normal r left parenthesis upper A x x Superscript normal upper T Baseline right parenthesis periodxTAx = tr(AxxT).
(3.103) 
In Eq. (3.103), if A is the identity I, we have that the inner product of a 
vector with itself is the trace of the outer product of the vector with itself, 
that is, 
x Superscript normal upper T Baseline x equals normal t normal r left parenthesis x x Superscript normal upper T Baseline right parenthesis periodxTx = tr(xxT).
(3.104) 
Also, by letting A be the identity in Eq. (3.103), we have an alternative 
way of showing that for a given vector x and any scalar a, the norm parallel to x minus a parallel to||x −a||
is minimized when a equals x overbara = ¯x: 
left parenthesis x minus a right parenthesis Superscript normal upper T Baseline left parenthesis x minus a right parenthesis equals normal t normal r left parenthesis x Subscript normal c Baseline x Subscript normal c Superscript normal upper T Baseline right parenthesis plus n left parenthesis a minus x overbar right parenthesis squared period(x −a)T(x −a) = tr(xcxT
c ) + n(a −¯x)2.
(3.105) 
(Here, “x overbar¯x” denotes the mean of the elements in x, and  “x Subscript normal cxc” is  x minus x overbarx−¯x. Compare  
this with Eq. (2.69) on page 59.) 
3.3.8 Anisometric Spaces 
In Sect. 2.1, we considered various properties of vectors that depend on the 
inner product, such as orthogonality of two vectors, norms of a vector, angles 
between two vectors, and distances between two vectors. All of these prop-
erties and measures are invariant to the orientation of the vectors; the space 
is isometric with respect to a Cartesian coordinate system. Noting that for 
real vectors the inner product is the bilinear form x Superscript normal upper T Baseline upper I yxTIy, we have a heuristic 
generalization to an anisometric space. Suppose, for example, that the scales 
of the coordinates diﬀer; say, a given distance along one axis in the natural 
units of the axis is equivalent (in some sense depending on the application) to 
twice that distance along another axis, again measured in the natural units of 
the axis. The properties derived from the inner product, such as a norm and 
a metric, may correspond to the application better if we use a bilinear form in 
which the matrix reﬂects the diﬀerent eﬀective distances along the coordinate 
axes. A diagonal matrix whose entries have relative values corresponding to 
the inverses of the relative scales of the axes may be more useful. Instead of 
x Superscript normal upper T Baseline yxTy, we may  use  x Superscript normal upper T Baseline upper D yxTDy, where  D is this diagonal matrix.

3.3 Multiplication of Matrices
115
Rather than diﬀerences in scales being just in the directions of the co-
ordinate axes, more generally we may think of anisometries being measured 
by general (but perhaps symmetric) matrices. (The covariance and correla-
tion matrices deﬁned on page 404 come to mind.) Any such matrix to be 
used in this context should be positive deﬁnite because we will generalize the 
dot product, which is necessarily nonnegative, in terms of a quadratic form. 
A bilinear form x Superscript normal upper T Baseline upper A yxTAy may correspond more closely to the properties of the 
application than the standard inner product. 
Conjugacy 
We deﬁne orthogonality of two vectors real vectors x and y with respect to A 
by 
x Superscript normal upper T Baseline upper A y equals 0 periodxTAy = 0.
(3.106) 
In this case, we say x and y are A-conjugate. 
The normal upper L 2L2 norm of a vector is the square root of the quadratic form of the 
vector with respect to the identity matrix. A generalization of the normal upper L 2L2 vector 
norm, called an elliptic norm or a conjugate norm, is deﬁned for the vector 
x as the square root of the quadratic form x Superscript normal upper T Baseline upper A xxTAx for any symmetric positive 
deﬁnite matrix A. It is sometimes denoted by parallel to x parallel to Subscript upper A||x||A: 
parallel to x parallel to Subscript upper A Baseline equals StartRoot x Superscript normal upper T Baseline upper A x EndRoot period||x||A =
√
xTAx.
(3.107) 
It is easy to see that parallel to x parallel to Subscript upper A||x||A satisﬁes the deﬁnition of a norm given on page 35. If  
A is a diagonal matrix with elements w Subscript i Baseline greater than or equals 0wi ≥0, the elliptic norm is the weighted 
normal upper L 2L2 norm of Eq. (2.36). 
The elliptic norm yields an elliptic metric in the usual way of deﬁning a 
metric in terms of a norm. The distance between the real vectors x and y 
with respect to A is StartRoot left parenthesis x minus y right parenthesis Superscript normal upper T Baseline upper A left parenthesis x minus y right parenthesis EndRoot
/
(x −y)TA(x −y). It is easy to see that this satisﬁes 
the deﬁnition of a metric given on page 41. 
A metric that is widely useful in statistical applications is the Mahalanobis 
distance, which uses a covariance matrix as the scale for a given space. (The 
sample covariance matrix is deﬁned in Eq. (8.67) on page 404.) If S is the 
covariance matrix, the Mahalanobis distance, with respect to that matrix, 
between the vectors x and y is 
StartRoot left parenthesis x minus y right parenthesis Superscript normal upper T Baseline upper S Superscript negative 1 Baseline left parenthesis x minus y right parenthesis EndRoot period
/
(x −y)TS−1(x −y).
(3.108) 
3.3.9 The Hadamard Product 
The most common kind of product of two matrices is the Cayley product, 
and when we speak of matrix multiplication without qualiﬁcation, we mean 
the Cayley product. Three other types of matrix multiplication that are use-
ful are Hadamard multiplication, Kronecker multiplication, and  inner product 
multiplication, and we will consider them in this and the next two sections.

116
3 Basic Properties of Matrices
While the domains over which these multiplication operations are deﬁned 
are all sets of matrices, the operations have diﬀering requirements of con-
formability for the particular operation. 
Hadamard multiplication is deﬁned for matrices of the same shape. Hadamard 
multiplication is just the multiplication of each element of one matrix by the 
corresponding element of the other matrix. Hadamard multiplication is often 
denoted by circled doto; for two matrices upper A Subscript n times mAn×m and upper B Subscript n times mBn×m we have 
upper A circled dot upper B equals Start 3 By 3 Matrix 1st Row 1st Column a 11 b 11 2nd Column ellipsis 3rd Column a Subscript 1 m Baseline b Subscript 1 m Baseline 2nd Row 1st Column vertical ellipsis 2nd Column ellipsis 3rd Column vertical ellipsis 3rd Row 1st Column a Subscript n Baseline 1 Baseline b Subscript n Baseline 1 Baseline 2nd Column ellipsis 3rd Column a Subscript n m Baseline b Subscript n m Baseline EndMatrix periodA o B =
⎡
⎢⎣
a11b11 . . . a1mb1m
...
. . .
...
an1bn1 . . . anmbnm
⎤
⎥⎦.
Hadamard multiplication immediately inherits the commutativity, asso-
ciativity, and distribution over addition of the ordinary multiplication of the 
underlying ﬁeld of scalars. Hadamard multiplication is also called array mul-
tiplication and element-wise multiplication. Hadamard matrix multiplication 
is a mapping 
normal upper I normal upper R Superscript n times m Baseline times normal upper I normal upper R Superscript n times m Baseline right arrow normal upper I normal upper R Superscript n times m Baseline periodIRn×m × IRn×m →IRn×m.
The identity for Hadamard multiplication is the matrix of appropriate 
shape whose elements are all 1s. 
3.3.10 The Kronecker Product 
Kronecker multiplication, denoted by circled times⊗, is deﬁned for any two matrices upper A Subscript n times mAn×m
and upper B Subscript p times qBp×q as 
upper A circled times upper B equals Start 3 By 3 Matrix 1st Row 1st Column a 11 upper B 2nd Column ellipsis 3rd Column a Subscript 1 m Baseline upper B 2nd Row 1st Column vertical ellipsis 2nd Column ellipsis 3rd Column vertical ellipsis 3rd Row 1st Column a Subscript n Baseline 1 Baseline upper B 2nd Column ellipsis 3rd Column a Subscript n m Baseline upper B EndMatrix periodA ⊗B =
⎡
⎢⎣
a11B . . . a1mB
...
. . .
...
an1B . . . anmB
⎤
⎥⎦.
(3.109) 
The Kronecker product of A and B is n p times m qnp × mq; that is, Kronecker matrix 
multiplication is a mapping 
normal upper I normal upper R Superscript n times m Baseline times normal upper I normal upper R Superscript p times q Baseline right arrow normal upper I normal upper R Superscript n p times m q Baseline periodIRn×m × IRp×q →IRnp×mq.
Notice that we can extend the deﬁnition of Kronecker multiplication 
slightly so that either factor of a Kronecker product can be a scalar, in which 
case Kronecker multiplication is the same as multiplication by a scalar, as we 
deﬁned on page 74 
c circled times upper A equals upper A circled times c equals c upper A equals left parenthesis c a Subscript i j Baseline right parenthesis commac ⊗A = A ⊗c = cA = (caij),
for a scalar c and the matrix upper A equals left parenthesis a Subscript i j Baseline right parenthesisA = (aij). We can also extend the deﬁnition to 
allow either factor to be a vector, just as we did with Cayley multiplication 
on page 100, by considering a vector to be a matrix with only one column. 
The Kronecker product is also called the “right direct product” or just 
direct product. (A left direct product is a Kronecker product with the factors

3.3 Multiplication of Matrices
117
reversed. In some of the earlier literature, “Kronecker product” was used to 
mean a left direct product.) Note the similarity of the Kronecker product of 
matrices with the direct product of sets, deﬁned on page 3, in the sense that 
the result is formed from ordered pairs of elements from the two operands. 
Kronecker multiplication is not commutative, but it is associative, and it 
is distributive over addition, as we will see below. (Again, this parallels the 
direct product of sets.) The Kronecker multiplicative identity is the scalar 1 
(with the generalized deﬁnition above), but in general, matrices do not have 
Kronecker multiplicative inverses. 
The identity for Kronecker multiplication is the 1 times 11 × 1 matrix with the 
element 1; that is, it is the same as the scalar 1. 
We can understand the properties of the Kronecker product by expressing 
the left parenthesis i comma j right parenthesis(i, j) element of upper A circled times upper BA ⊗B in terms of the elements of A and B, 
left parenthesis upper A circled times upper B right parenthesis Subscript i comma j Baseline equals upper A Subscript left floor left parenthesis i minus 1 right parenthesis divided by p right floor plus 1 comma left floor left parenthesis j minus 1 right parenthesis divided by q right floor plus 1 Baseline upper B Subscript i minus p left floor left parenthesis i minus 1 right parenthesis divided by p right floor comma j minus q left floor left parenthesis j minus 1 right parenthesis divided by q right floor Baseline period(A ⊗B)i,j = A[(i−1)/p]+1, [(j−1)/q]+1Bi−p[(i−1)/p], j−q[(j−1)/q].
(3.110) 
Some additional properties of Kronecker products that are immediate re-
sults of the deﬁnition are, assuming that the matrices are conformable for the 
indicated operations, 
StartLayout 1st Row 1st Column left parenthesis a upper A right parenthesis circled times left parenthesis b upper B right parenthesis 2nd Column equals 3rd Column a b left parenthesis upper A circled times upper B right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis a b upper A right parenthesis circled times upper B 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper A circled times left parenthesis a b upper B right parenthesis comma normal f normal o normal r normal s normal c normal a normal l normal a normal r normal s a comma b comma 4th Row 1st Column Blank 5th Row 1st Column left parenthesis upper A plus upper B right parenthesis circled times left parenthesis upper C right parenthesis 2nd Column equals 3rd Column upper A circled times upper C plus upper B circled times upper C comma 6th Row 1st Column Blank 7th Row 1st Column left parenthesis upper A circled times upper B right parenthesis circled times upper C 2nd Column equals 3rd Column upper A circled times left parenthesis upper B circled times upper C right parenthesis comma 8th Row 1st Column Blank 9th Row 1st Column left parenthesis upper A circled times upper B right parenthesis Superscript normal upper T 2nd Column equals 3rd Column upper A Superscript normal upper T Baseline circled times upper B Superscript normal upper T Baseline comma 10th Row 1st Column Blank 11th Row 1st Column left parenthesis upper A circled times upper B right parenthesis left parenthesis upper C circled times upper D right parenthesis 2nd Column equals 3rd Column upper A upper C circled times upper B upper D period 12th Row 1st Column Blank 13th Row 1st Column upper I circled times upper A 2nd Column equals 3rd Column normal d normal i normal a normal g left parenthesis upper A comma ellipsis comma upper A right parenthesis period 14th Row 1st Column Blank 15th Row 1st Column upper A circled times upper I 2nd Column equals 3rd Column left parenthesis a Subscript i j Baseline upper I right parenthesis period EndLayout(aA) ⊗(bB) = ab(A ⊗B)
= (abA) ⊗B
= A ⊗(abB), for scalars a, b,
(3.111) 
(A + B) ⊗ (C) =  A ⊗ C + B ⊗C,
(3.112) 
(A ⊗ B) ⊗ C = A ⊗ (B ⊗ C),
(3.113) 
(A ⊗ B)T = AT ⊗ BT ,
(3.114) 
(A ⊗ B)(C ⊗ D) =  AC ⊗ BD.
(3.115) 
I ⊗ A = diag(A, . . . , A).
(3.116) 
A ⊗ I = (aijI) .
(3.117) 
These properties are all easy to see by using Eq. (3.110) to express the 
left parenthesis i comma j right parenthesis(i, j) element of the matrix on either side of the equation, taking into account 
the size of the matrices involved. For example, in the ﬁrst equation, if A is 
n times mn × m and B is p times qp × q, the  left parenthesis i comma j right parenthesis(i, j) element on the left-hand side is 
a upper A Subscript left bracket left parenthesis i minus 1 right parenthesis divided by p right bracket plus 1 comma left bracket left parenthesis j minus 1 right parenthesis divided by q right bracket plus 1 Baseline b upper B Subscript i minus p left bracket left parenthesis i minus 1 right parenthesis divided by p right bracket comma j minus q left bracket left parenthesis j minus 1 right parenthesis divided by q right bracketaA[(i−1)/p]+1, [(j−1)/q]+1bBi−p[(i−1)/p], j−q[(j−1)/q]

118
3 Basic Properties of Matrices
and that on the right-hand side is 
a b upper A Subscript left bracket left parenthesis i minus 1 right parenthesis divided by p right bracket plus 1 comma left bracket left parenthesis j minus 1 right parenthesis divided by q right bracket plus 1 Baseline upper B Subscript i minus p left bracket left parenthesis i minus 1 right parenthesis divided by p right bracket comma j minus q left bracket left parenthesis j minus 1 right parenthesis divided by q right bracket Baseline periodabA[(i−1)/p]+1, [(j−1)/q]+1Bi−p[(i−1)/p], j−q[(j−1)/q].
They are all this easy! Hence, they are given as Exercise 3.8. 
The determinant of the Kronecker product of two square matrices upper A Subscript n times nAn×n
and upper B Subscript m times mBm×m has a simple relationship to the determinants of the individual 
matrices: 
normal d normal e normal t left parenthesis upper A circled times upper B right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis Superscript m Baseline normal d normal e normal t left parenthesis upper B right parenthesis Superscript n Baseline perioddet(A ⊗B) = det(A)mdet(B)n.
(3.118) 
The proof of this, like many facts about determinants, is straightforward but 
involves tedious manipulation of cofactors. The manipulations in this case can 
be facilitated by using the vec-permutation matrix. See Harville (1997) for a 
detailed formal proof. 
From Eq. (3.114) we see that the Kronecker product of symmetric matrices 
is symmetric. 
Another property of the Kronecker product of square matrices is 
normal t normal r left parenthesis upper A circled times upper B right parenthesis equals normal t normal r left parenthesis upper A right parenthesis normal t normal r left parenthesis upper B right parenthesis periodtr(A ⊗B) = tr(A)tr(B).
(3.119) 
This is true because the trace of the product is merely the sum of all possible 
products of the diagonal elements of the individual matrices. 
The Kronecker product and the vec function often ﬁnd uses in the same 
application. For example, an n times mn × m normal random matrix X with parameters 
M, normal upper SigmaΣ, and  normal upper PsiΨ can be expressed in terms of an ordinary np-variate normal 
random variable upper Y equals normal v normal e normal c left parenthesis upper X right parenthesisY = vec(X) with parameters normal v normal e normal c left parenthesis upper M right parenthesisvec(M) and normal upper Sigma circled times normal upper PsiΣ⊗Ψ. (We discuss 
matrix random variables brieﬂy on page 343. 
A relationship between the vec function and Kronecker multiplication is 
normal v normal e normal c left parenthesis upper A upper B upper C right parenthesis equals left parenthesis upper C Superscript normal upper T Baseline circled times upper A right parenthesis normal v normal e normal c left parenthesis upper B right parenthesisvec(ABC) = (CT ⊗A)vec(B)
(3.120) 
for matrices A, B, and  C that are conformable for the multiplication indicated. 
This is easy to show and is left as an exercise. 
The Kronecker product is useful in representing the derivative of a function 
of matrices with respect to a matrix (see Table 7.2 on page 336, for example). 
Kronecker productions are also useful in the representation of statistical 
models with special variance–covariance structures. Statistical models consist-
ing of multiple groups often assume a common variance–covariance within the 
groups. (This is a generalization of the assumption of a common variance in a 
simple two-sample t test.) The variance–covariance in the overall model may 
then be written as normal d normal i normal a normal g left parenthesis normal upper Sigma comma ellipsis comma normal upper Sigma right parenthesis equals upper I circled times normal upper Sigmadiag(Σ, . . . , Σ) = I ⊗Σ. This is called Kronecker structure. 
3.3.11 The Inner Product of Matrices 
An inner product of two matrices of the same shape is deﬁned as the sum of 
the dot products of the vectors formed from the columns of one matrix with 
vectors formed from the corresponding columns of the other matrix; that is,

3.3 Multiplication of Matrices
119
if a 1 comma ellipsis comma a Subscript m Baselinea1, . . . , am are the columns of A and b 1 comma ellipsis comma b Subscript m Baselineb1, . . . , bm are the columns of B, then  
the inner product of A and B, denoted left angle bracket upper A comma upper B right angle bracket<A, B>, is  
left angle bracket upper A comma upper B right angle bracket equals sigma summation Underscript j equals 1 Overscript m Endscripts left angle bracket a Subscript j Baseline comma b Subscript j Baseline right angle bracket period<A, B> =
m
E
j=1
<aj, bj>.
(3.121) 
Similarly as for vectors (page 34), the inner product is sometimes called a “dot 
product,” and the notation upper A dot upper BA·B is sometimes used to denote the matrix inner 
product. (I generally try to avoid use of the term dot product for matrices 
because the term may be used diﬀerently by diﬀerent people. In MATLAB, for 
example, “dot product,” implemented in the dot function, can refer either to 
the 1 times m1 × m matrix consisting of the individual terms in the sum in Eq. (3.121), 
or to the n times 1n×1 matrix consisting of the dot products of the vectors formed from 
the rows of A and B. In the  NumPy linear algebra package, the dot function 
implements Cayley multiplication! This is probably because someone working 
with Python realized the obvious fact that the deﬁning equation of Cayley 
multiplication, Eq. (3.48) on page 96, is actually the dot product of the vector 
formed from the elements in the i normal t normal hith row in the ﬁrst matrix and the vector 
formed from the elements in the j normal t normal hjth column in the ﬁrst matrix.) 
For real matrices, Eq. (3.121) can be written as 
left angle bracket upper A comma upper B right angle bracket equals sigma summation Underscript j equals 1 Overscript m Endscripts a Subscript j Superscript normal upper T Baseline b Subscript j Baseline period<A, B> =
m
E
j=1
aT
j bj.
(3.122) 
As in the case of the dot product of vectors, the product of matrices deﬁned 
in Eq. (3.122) over the complex ﬁeld is not an inner product because the ﬁrst 
property (on page 33 or as listed below) does not hold. 
For conformable matrices A, B, and  C, we can easily conﬁrm that this 
product satisﬁes the general properties of an inner product listed on page 33: 
• If upper A not equals 0A /= 0, left angle bracket upper A comma upper A right angle bracket greater than 0<A, A> > 0, and  left angle bracket 0 comma upper A right angle bracket equals left angle bracket upper A comma 0 right angle bracket equals left angle bracket 0 comma 0 right angle bracket equals 0<0, A> = <A, 0> = <0, 0> = 0. 
le
ft angle bracket upper A comma upper B right angle bracket equals left angle bracket upper B comma upper A right angle bracket<A, B> = <B, A>. 
le
ft angle bracket s upper A comma upper B right angle bracket equals s left angle bracket upper A comma upper B right angle bracket<sA, B> = s<A, B>, for  a scalar  s. 
le
ft angle bracket left parenthesis upper A plus upper B right parenthesis comma upper C right angle bracket equals left angle bracket upper A comma upper C right angle bracket plus left angle bracket upper B comma upper C right angle bracket<(A + B), C> = <A, C> + <B, C>. 
As with any inner product (restricted to objects in the ﬁeld of the reals), 
its value is a real number. Thus, the matrix inner product is a mapping 
normal upper I normal upper R Superscript n times m Baseline times normal upper I normal upper R Superscript n times m Baseline right arrow normal upper I normal upper R periodIRn×m × IRn×m →IR.
We see from the deﬁnition above that the inner product of real matrices 
satisﬁes 
left angle bracket upper A comma upper B right angle bracket equals normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper B right parenthesis comma<A, B> = tr(ATB),
(3.123) 
which could alternatively be taken as the deﬁnition. 
Rewriting the deﬁnition of left angle bracket upper A comma upper B right angle bracket<A, B> as sigma summation Underscript j equals 1 Overscript m Endscripts sigma summation Underscript i equals 1 Overscript n Endscripts a Subscript i j Baseline b Subscript i jEm
j=1
En
i=1 aijbij, we see that for real 
matrices

120
3 Basic Properties of Matrices
left angle bracket upper A comma upper B right angle bracket equals left angle bracket upper A Superscript normal upper T Baseline comma upper B Superscript normal upper T Baseline right angle bracket period<A, B> = <AT, BT>.
(3.124) 
Like any inner product, inner products of matrices obey the Cauchy– 
Schwarz inequality (see inequality (2.23), page 33), 
left angle bracket upper A comma upper B right angle bracket less than or equals left angle bracket upper A comma upper A right angle bracket Superscript one half Baseline left angle bracket upper B comma upper B right angle bracket Superscript one half Baseline comma<A, B> ≤<A, A>
1
2 <B, B>
1
2 ,
(3.125) 
with equality holding only if upper A equals 0A = 0 or upper B equals s upper AB = sA for some scalar s. 
Orthonormal Matrices 
In Sect. 2.1.8, we deﬁned orthogonality and orthonormality of two or more 
vectors in terms of inner products. We can likewise deﬁne an orthogonal binary 
relationship between two matrices in terms of inner products of matrices. We 
say the matrices A and B of the same shape are orthogonal to each other if 
left angle bracket upper A comma upper B right angle bracket equals 0 period<A, B> = 0.
(3.126) 
We also use the term “orthonormal” to refer to matrices that are orthogonal 
to each other and for which each has an inner product with itself of 1. In 
Sect. 3.8, we will deﬁne orthogonality as a unary property of matrices. The 
term “orthogonal,” when applied to matrices, generally refers to that property 
rather than the binary property we have deﬁned here. “Orthonormal,” on the 
other hand, generally refers to the binary property. 
Orthonormal Basis: Fourier Expansion 
On page 83 we identiﬁed a vector space of matrices and deﬁned a basis for the 
space normal upper I normal upper R Superscript n times mIRn×m. If  StartSet upper U 1 comma ellipsis comma upper U Subscript k Baseline EndSet{U1, . . . , Uk} is a basis set for script upper M subset of or equal to normal upper I normal upper R Superscript n times mM ⊆IRn×m with the property 
that left angle bracket upper U Subscript i Baseline comma upper U Subscript j Baseline right angle bracket equals 0<Ui, Uj> = 0 for i not equals ji /= j and left angle bracket upper U Subscript i Baseline comma upper U Subscript i Baseline right angle bracket equals 1<Ui, Ui> = 1, then the set is an orthonormal 
basis set. 
If A is an n times mn × m matrix, with the Fourier expansion 
upper A equals sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline upper U Subscript i Baseline commaA =
k
E
i=1
ciUi,
(3.127) 
we have, analogous to Eq. (2.58) on page 51, 
c Subscript i Baseline equals left angle bracket upper A comma upper U Subscript i Baseline right angle bracket periodci = <A, Ui>.
(3.128) 
The c Subscript ici have the same properties (such as the Parseval identity, Eq. (2.59), 
for example) as the Fourier coeﬃcients in any orthonormal expansion. Best 
approximations within script upper MM can also be expressed as truncations of the sum in 
Eq. (3.127) as in Eq. (2.61). The objective of course is to reduce the truncation 
error, and the optimality of the Fourier expansion in this regard discussed on 
page 51 holds in the matrix case as well. (The norms in Parseval’s identity 
and in measuring the goodness of an approximation are matrix norms in this 
case. We discuss matrix norms in Sect. 3.11 beginning on page 187.)

3.4 Matrix Rank and the Inverse of a Matrix
121
3.4 Matrix Rank and the Inverse of a Matrix 
The linear dependence or independence of the vectors forming the rows or 
columns of a matrix is an important characteristic of the matrix. 
The maximum number of linearly independent vectors (those forming ei-
ther the rows or the columns) is called the rank of the matrix. We use the 
notation 
normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(A)
to denote the rank of the matrix A. (We have used the term “rank” before to 
denote dimensionality of an array. “Rank” as we have just deﬁned it applies 
only to a matrix or to a set of vectors, and this is by far the more common 
meaning of the word. The meaning is clear from the context, however.) 
Because multiplication by a nonzero scalar does not change the linear 
independence of vectors, for the scalar a with a not equals 0a /= 0, we have  
normal r normal a normal n normal k left parenthesis a upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(aA) = rank(A).
(3.129) 
From results developed in Sect. 2.1, we see that for the n times mn × m matrix A, 
normal r normal a normal n normal k left parenthesis upper A right parenthesis less than or equals min left parenthesis n comma m right parenthesis periodrank(A) ≤min(n, m).
(3.130) 
The rank of the zero matrix is 0, and the rank of any nonzero matrix is 
positive. 
A matrix can be transformed into any given matrix of the same shape 
and rank. Matrices of the same shape that have the same rank are said to be 
equivalent matrices. 
3.4.1 Row Rank and Column Rank 
We have deﬁned matrix rank in terms of number of linearly independent 
rows or columns. This is because the number of linearly independent rows 
is the same as the number of linearly independent columns. Although we 
may use the terms “row rank” or “column rank,” the single word “rank” 
is suﬃcient because they are the same. To see this, assume that we have 
an n times mn × m matrix A and that there are exactly p linearly independent rows 
and exactly q linearly independent columns. We can permute the rows and 
columns of the matrix so that the ﬁrst p rows are linearly independent rows 
and the ﬁrst q columns are linearly independent and the remaining rows or 
columns are linearly dependent on the ﬁrst ones. (Recall that applying the 
same permutation to all of the elements of each vector in a set of vectors does 
not change the linear dependencies over the set.) After these permutations, 
we have a matrix B with submatrices W, X, Y , and  Z, 
upper B equals Start 2 By 2 Matrix 1st Row 1st Column upper W Subscript p times q Baseline 2nd Column upper X Subscript p times m minus q Baseline 2nd Row 1st Column upper Y Subscript n minus p times q Baseline 2nd Column upper Z Subscript n minus p times m minus q Baseline EndMatrix commaB =
[
Wp×q
Xp×m−q
Yn−p×q Zn−p×m−q
]
,
(3.131)

122
3 Basic Properties of Matrices
where the rows of upper R equals left bracket upper W vertical bar upper X right bracketR = [W|X] correspond to p linearly independent m-vectors 
and the columns of upper C equals StartBinomialOrMatrix upper W Choose upper Y EndBinomialOrMatrixC =
[ W
Y
]
correspond to q linearly independent n-vectors. 
Without loss of generality, we can assume p less than or equals qp ≤q. Now, if  p less than qp < q, it must be  
the case that the  columns of  W are linearly dependent because there are q 
of them, but they have only p elements. Therefore, there is some q-vector 
a not equals 0a /= 0 such that upper W a equals 0Wa = 0. Now, since  the rows of  R are the full set of linearly 
independent rows, any row in left bracket upper Y vertical bar upper Z right bracket[Y |Z] can be expressed as a linear combination 
of the rows of R, and  any row  in  Y can be expressed as a linear combination 
of the rows of W. This means, for some n minus p times pn−p × p matrix T, that  upper Y equals upper T upper WY = TW. 
In this case, however, upper C a equals 0Ca = 0. But this contradicts the assumption that the 
columns of C are linearly independent; therefore, it cannot be the case that 
p less than qp < q. We conclude therefore that p equals qp = q; that is, that the maximum number 
of linearly independent rows is the same as the maximum number of linearly 
independent columns. 
Because the row rank, the column rank, and the rank of A are all the 
same, we have 
StartLayout 1st Row 1st Column normal r normal a normal n normal k left parenthesis upper A right parenthesis 2nd Column equals 3rd Column normal d normal i normal m left parenthesis script upper V left parenthesis upper A right parenthesis right parenthesis comma 2nd Row 1st Column Blank 3rd Row 1st Column normal r normal a normal n normal k left parenthesis upper A Superscript normal upper T Baseline right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A right parenthesis comma 4th Row 1st Column Blank 5th Row 1st Column normal d normal i normal m left parenthesis script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesis right parenthesis 2nd Column equals 3rd Column normal d normal i normal m left parenthesis script upper V left parenthesis upper A right parenthesis right parenthesis period EndLayoutrank(A) = dim(V(A)),
(3.132) 
rank(AT ) = rank(A),
(3.133) 
dim(V(AT )) = dim(V(A)).
(3.134) 
(Note, of course, that in general script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesis not equals script upper V left parenthesis upper A right parenthesisV(AT) /= V(A); the orders of the vector spaces 
are possibly diﬀerent.) 
3.4.2 Full Rank Matrices 
If the rank of a matrix is the same as its smaller dimension, we say the matrix 
is of full rank. In the case of a nonsquare matrix, we may say the matrix is 
of full row rank or full column rank just to emphasize which is the smaller 
number. 
If a matrix is not of full rank, we say it is rank deﬁcient and deﬁne the 
rank deﬁciency as the diﬀerence between its smaller dimension and its rank. 
A full rank matrix that is square is called nonsingular, and one that is not 
nonsingular is called singular. 
A square matrix that is either row or column diagonally dominant is non-
singular. The proof of this is Exercise 3.11. (It’s easy!) 
A positive deﬁnite matrix is nonsingular. The proof of this is Exercise 3.12. 
Later in this section, we will identify additional properties of square full 
rank matrices. (For example, they have inverses and their determinants are 
nonzero.)

3.4 Matrix Rank and the Inverse of a Matrix
123
3.4.3 Rank of Elementary Operator Matrices and Matrix 
Products Involving Them 
Because within any set of rows of an elementary operator matrix, for some 
given column, only one of those rows contains a nonzero element, the elemen-
tary operator matrices are all obviously of full rank, with the proviso that 
a not equals 0a /= 0 in upper E Subscript p Baseline left parenthesis a right parenthesisEp(a). (See Sect. 3.3.3 for discussion of elementary operator matrices 
and associated notation.) 
Furthermore, the rank of the product of any given matrix with an elemen-
tary operator matrix is the same as the rank of the given matrix. To see this, 
consider each type of elementary operator matrix in turn. For a given matrix 
A, the  set of rows of  upper E Subscript p q Baseline upper AEpqA is  the same as the  set of rows of  A; hence, the rank 
of upper E Subscript p q Baseline upper AEpqA is  the same as the  rank  of  A. Likewise, the set of columns of upper A upper E Subscript p qAEpq
is the same as the set of columns of A; hence, again, the rank of upper A upper E Subscript p qAEpq is the 
same as the rank of A. 
The set of rows of upper E Subscript p Baseline left parenthesis a right parenthesis upper AEp(a)A for a not equals 0a /= 0 is the same as the set of rows of A, 
except for one, which is a nonzero scalar multiple of the corresponding row 
of A; therefore, the rank of upper E Subscript p Baseline left parenthesis a right parenthesis upper AEp(a)A is  the same as the  rank  of  A. Likewise, 
the set of columns of upper A upper E Subscript p Baseline left parenthesis a right parenthesisAEp(a) is  the same as the  set of columns  of  A, except 
for one, which is a nonzero scalar multiple of the corresponding column of A; 
therefore, again, the rank of upper A upper E Subscript p Baseline left parenthesis a right parenthesisAEp(a) is the same as the rank of A. 
Finally, the set of rows of upper E Subscript p q Baseline left parenthesis a right parenthesis upper AEpq(a)A for a not equals 0a /= 0 is  the same as the  set of  
rows of A, except for one, which is a nonzero scalar multiple of some row of A 
added to the corresponding row of A (see Eq. (3.74), page 106, for example); 
therefore, the rank of upper E Subscript p q Baseline left parenthesis a right parenthesis upper AEpq(a)A is the same as the rank of A. Likewise, we 
conclude that the rank of upper A upper E Subscript p q Baseline left parenthesis a right parenthesisAEpq(a) is the same as the rank of A. 
We therefore have that if P and Q are the products of elementary operator 
matrices, 
normal r normal a normal n normal k left parenthesis upper P upper A upper Q right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(PAQ) = rank(A).
(3.135) 
On page 132, we will extend this result to products by any full rank matrices. 
3.4.4 The Rank of Partitioned Matrices, Products of 
Matrices, and Sums of Matrices 
The partitioning in Eq. (3.131) leads us to consider partitioned matrices in 
more detail. 
Rank of Partitioned Matrices and Submatrices 
Let the matrix A be partitioned as 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[
A11 A12
A21 A22
]
,
where any pair of submatrices in a column or row may be null (that is, where 
for example, it may  be  the case that  upper A equals left bracket upper A 11 vertical bar upper A 12 right bracketA = [A11|A12]). Then, the number

124
3 Basic Properties of Matrices
of linearly independent rows of A must be at least as great as the number of 
linearly independent rows of left bracket upper A 11 vertical bar upper A 12 right bracket[A11|A12] and the number of linearly independent 
rows of left bracket upper A 21 vertical bar upper A 22 right bracket[A21|A22]. By the properties of subvectors in Sect. 2.1.1, the  number  
of linearly independent rows of left bracket upper A 11 vertical bar upper A 12 right bracket[A11|A12] must be at least as great as the 
number of linearly independent rows of upper A 11A11 or upper A 12A12. We could go through a 
similar argument relating to the number of linearly independent columns and 
arrive at the inequality 
normal r normal a normal n normal k left parenthesis upper A Subscript i j Baseline right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(Aij) ≤rank(A).
(3.136) 
Furthermore, we see that 
normal r normal a normal n normal k left parenthesis upper A right parenthesis less than or equals normal r normal a normal n normal k left parenthesis left bracket upper A 11 vertical bar upper A 12 right bracket right parenthesis plus normal r normal a normal n normal k left parenthesis left bracket upper A 21 vertical bar upper A 22 right bracket right parenthesisrank(A) ≤rank([A11|A12]) + rank([A21|A22])
(3.137) 
because normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(A) is the number of linearly independent columns of A, which  
is less than or equal to the number of linearly independent rows of left bracket upper A 11 vertical bar upper A 12 right bracket[A11|A12]
plus the number of linearly independent rows of left bracket upper A 21 vertical bar upper A 22 right bracket[A21|A22]. Likewise, we have 
normal r normal a normal n normal k left parenthesis upper A right parenthesis less than or equals normal r normal a normal n normal k left parenthesis StartBinomialOrMatrix upper A 11 Choose upper A 21 EndBinomialOrMatrix right parenthesis plus normal r normal a normal n normal k left parenthesis StartBinomialOrMatrix upper A 12 Choose upper A 22 EndBinomialOrMatrix right parenthesis periodrank(A) ≤rank
([
A11
A21
])
+ rank
([
A12
A22
])
.
(3.138) 
In a similar manner, by merely counting the number of independent rows, 
we see that, if 
script upper V left parenthesis left bracket upper A 11 vertical bar upper A 12 right bracket Superscript normal upper T Baseline right parenthesis perpendicular script upper V left parenthesis left bracket upper A 21 vertical bar upper A 22 right bracket Superscript normal upper T Baseline right parenthesis commaV
(
[A11|A12]T)
⊥V
(
[A21|A22]T)
,
then 
normal r normal a normal n normal k left parenthesis upper A right parenthesis equals normal r normal a normal n normal k left parenthesis left bracket upper A 11 vertical bar upper A 12 right bracket right parenthesis plus normal r normal a normal n normal k left parenthesis left bracket upper A 21 vertical bar upper A 22 right bracket right parenthesis semicolonrank(A) = rank([A11|A12]) + rank([A21|A22]);
(3.139) 
and, if 
script upper V left parenthesis StartBinomialOrMatrix upper A 11 Choose upper A 21 EndBinomialOrMatrix right parenthesis perpendicular script upper V left parenthesis StartBinomialOrMatrix upper A 12 Choose upper A 22 EndBinomialOrMatrix right parenthesis commaV
([
A11
A21
])
⊥V
([
A12
A22
])
,
then 
normal r normal a normal n normal k left parenthesis upper A right parenthesis equals normal r normal a normal n normal k left parenthesis StartBinomialOrMatrix upper A 11 Choose upper A 21 EndBinomialOrMatrix right parenthesis plus normal r normal a normal n normal k left parenthesis StartBinomialOrMatrix upper A 12 Choose upper A 22 EndBinomialOrMatrix right parenthesis periodrank(A) = rank
([ A11
A21
])
+ rank
([ A12
A22
])
.
(3.140) 
An Upper Bound on the Rank of Products of Matrices 
Because the columns of the product AB are linear combinations of the columns 
of A, it is clear that 
script upper V left parenthesis upper A upper B right parenthesis subset of or equal to script upper V left parenthesis upper A right parenthesis periodV(AB) ⊆V(A).
(3.141) 
The rank of the product of two matrices is less than or equal to the lesser 
of the ranks of the two: 
normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis less than or equals min left parenthesis normal r normal a normal n normal k left parenthesis upper A right parenthesis comma normal r normal a normal n normal k left parenthesis upper B right parenthesis right parenthesis periodrank(AB) ≤min(rank(A), rank(B)).
(3.142) 
This follows from Eq. (3.141). We can also show this by separately considering 
two cases for the n times kn × k matrix A and the k times mk × m matrix B. In one  case, we  
assume k is at least as large as n and n less than or equals mn ≤m, and in the other case we

3.4 Matrix Rank and the Inverse of a Matrix
125
assume k less than n less than or equals mk < n ≤m. In both cases, we represent the rows of AB as k linear 
combinations of the rows of B. 
From inequality (3.142), we see that the rank of a nonzero outer product 
matrix (that is, a matrix formed as the outer product of two nonzero vectors) 
is 1. 
The bound in inequality (3.142) is sharp, as we can see by exhibiting 
matrices A and B such that normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis equals min left parenthesis normal r normal a normal n normal k left parenthesis upper A right parenthesis comma normal r normal a normal n normal k left parenthesis upper B right parenthesis right parenthesisrank(AB) = min(rank(A), rank(B)), as you are 
asked to do in Exercise  3.14a. 
Inequality (3.142) provides a useful upper bound on normal r normal a normal n normal k left parenthesis upper A upper B right parenthesisrank(AB). In  
Sect. 3.4.12, we will develop a lower bound on normal r normal a normal n normal k left parenthesis upper A upper B right parenthesisrank(AB). 
An Upper and a Lower Bound on the Rank of Sums of Matrices 
The rank of the sum of two matrices is less than or equal to the sum of their 
ranks; that is, 
normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesis periodrank(A + B) ≤rank(A) + rank(B).
(3.143) 
We can see this by observing that 
upper A plus upper B equals left bracket upper A vertical bar upper B right bracket StartBinomialOrMatrix upper I Choose upper I EndBinomialOrMatrix commaA + B = [A|B]
[
I
I
]
,
and so normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis left bracket upper A vertical bar upper B right bracket right parenthesisrank(A+B) ≤rank([A|B]) by Eq. (3.142),  which in turn is less than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesis≤rank(A)+
rank(B) by Eq. (3.138). 
The bound in inequality (3.143) is sharp, as we can see by exhibiting 
matrices A and B such that normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(A + B) = rank(A) + rank(B), as you are 
asked to do in Exercise  3.14c. 
Using inequality (3.143) and the fact that normal r normal a normal n normal k left parenthesis negative upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(−B) = rank(B), we write  
normal r normal a normal n normal k left parenthesis upper A minus upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(A−B) ≤rank(A)+rank(B), and  so, replacing  A in (3.143) by  upper A plus upper BA+B, we  
have normal r normal a normal n normal k left parenthesis upper A right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(A) ≤rank(A+B)+rank(B), or  normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis greater than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis minus normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(A+B) ≥rank(A)−rank(B). 
By a similar procedure, we get normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis greater than or equals normal r normal a normal n normal k left parenthesis upper B right parenthesis minus normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(A + B) ≥rank(B) −rank(A), or  
normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis greater than or equals StartAbsoluteValue normal r normal a normal n normal k left parenthesis upper A right parenthesis minus normal r normal a normal n normal k left parenthesis upper B right parenthesis EndAbsoluteValue periodrank(A + B) ≥|rank(A) −rank(B)|.
(3.144) 
The bound in inequality (3.144) is sharp, as we can see by exhibiting 
matrices A and B such that normal r normal a normal n normal k left parenthesis upper A plus upper B right parenthesis equals StartAbsoluteValue normal r normal a normal n normal k left parenthesis upper A right parenthesis minus normal r normal a normal n normal k left parenthesis upper B right parenthesis EndAbsoluteValuerank(A + B) = |rank(A) −rank(B)|, as you are 
asked to do in Exercise  3.14d. 
3.4.5 Full Rank Partitioning 
On page 121, given  an  n times mn×m matrix A of rank r, we showed how, if necessary,  
its rows and columns can be permuted to form an n times mn × m matrix B that can 
be partitioned in such a way that the principal block diagonal submatrix is 
square and of full rank: 
upper B equals Start 2 By 2 Matrix 1st Row 1st Column upper W Subscript r times r Baseline 2nd Column upper X Subscript r times m minus r Baseline 2nd Row 1st Column upper Y Subscript n minus r times r Baseline 2nd Column upper Z Subscript n minus r times m minus r Baseline EndMatrix periodB =
[ Wr×r
Xr×m−r
Yn−r×r Zn−r×m−r
]
.
(3.145)

126
3 Basic Properties of Matrices
Either of the matrices X and Y may be null (in which case Z is also null). 
This is called a full rank partitioning of B. 
The matrix B in Eq. (3.145) has a very special property: the full set of 
linearly independent rows is the ﬁrst r rows, and the full set of linearly inde-
pendent columns is the ﬁrst r columns. 
Any rank r matrix can be put in the form of Eq. (3.145) by using permuta-
tion matrices as in Eq. (3.68), assuming that r greater than or equals 1r ≥1. That is,  if  A is a nonzero 
matrix, there is a matrix of the form of B above that has the same rank. For 
some permutation matrices upper E Subscript left parenthesis pi 1 right parenthesisE(π1) and upper E Subscript left parenthesis pi 2 right parenthesisE(π2), 
upper B equals upper E Subscript left parenthesis pi 1 right parenthesis Baseline upper A upper E Subscript left parenthesis pi 2 right parenthesis Baseline periodB = E(π1)AE(π2).
(3.146) 
The inverses of these permutations coupled with the full rank partitioning of 
B form a full rank partitioning of the original matrix A. 
For a square matrix of rank r, this kind of partitioning implies that there 
is a full rank r times rr × r principal submatrix, and the principal submatrix formed 
by including any of the remaining diagonal elements is singular. The princi-
pal minor formed from the full rank principal submatrix is nonzero, but if 
the order of the matrix is greater than r, a principal minor formed from a 
submatrix larger than r times rr × r is zero. 
The partitioning in Eq. (3.145) is of general interest, and we will use this 
type of partitioning often. We express an equivalent partitioning of a trans-
formed matrix in Eq. (3.173) below.  
The same methods as above can be used to form a full rank square subma-
trix of any order less than or equal to the rank. That is, if the n times mn × m matrix 
A is of rank r and q less than or equals rq ≤r, we can  form  
upper E Subscript left parenthesis pi Sub Subscript r Subscript right parenthesis Baseline upper A upper E Subscript left parenthesis pi Sub Subscript c Subscript right parenthesis Baseline equals Start 2 By 2 Matrix 1st Row 1st Column upper S Subscript q times q Baseline 2nd Column upper T Subscript q times m minus q Baseline 2nd Row 1st Column upper U Subscript n minus q times q Baseline 2nd Column upper V Subscript n minus q times m minus q Baseline EndMatrix commaE(πr)AE(πc) =
[
Sq×q
Tq×m−q
Un−q×q Vn−q×m−q
]
,
(3.147) 
where S is of rank q. 
It is obvious that the rank of a matrix can never exceed its smaller dimen-
sion (see the discussion of linear independence on page 20). Whether or not 
a matrix has more rows than columns, the rank of the matrix is the same as 
the dimension of the column space of the matrix. (As we have just seen, the 
dimension of the column space is necessarily the same as the dimension of the 
row space, but the order of the column space is diﬀerent from the order of the 
row space unless the matrix is square.) 
3.4.6 Full Rank Matrices and Matrix Inverses 
We have already seen that full rank matrices have some important properties. 
In this section, we consider full rank matrices and matrices that are their 
Cayley multiplicative inverses.

3.4 Matrix Rank and the Inverse of a Matrix
127
Solutions of Linear Equations 
Important applications of vectors and matrices involve systems of linear equa-
tions. We will discuss this topic again in Sect. 3.6, and more fully in Chap. 5, 
but here we will introduce some terms and relate the problem to the current 
discussion. 
A system of linear equations is of the form 
StartLayout 1st Row 1st Column a 11 x 1 2nd Column plus midline horizontal ellipsis plus 3rd Column a Subscript 1 m Baseline x Subscript m 4th Column ModifyingAbove equals With question mark 5th Column b 1 2nd Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column vertical ellipsis 4th Column Blank 5th Column vertical ellipsis 3rd Row 1st Column a Subscript n Baseline 1 Baseline x 1 2nd Column plus midline horizontal ellipsis plus 3rd Column a Subscript n m Baseline x Subscript m 4th Column ModifyingAbove equals With question mark 5th Column b Subscript n EndLayout
a11x1 + · · · + a1mxm
?= b1
...
...
...
an1x1 + · · · + anmxm
?= bn
(3.148) 
or 
upper A x ModifyingAbove equals With question mark b periodAx
?= b.
(3.149) 
In this system, A is called the coeﬃcient matrix. An x that satisﬁes this 
system of equations is called a solution to the system. For given A and b, a  
solution may or may not exist. From Eq. (3.63), a solution exists if and only 
if the n-vector b is in the k-dimensional column space of A, where  k less than or equals mk ≤m. 
A system for which a solution exists is said to be consistent; otherwise, it is 
inconsistent. 
We note that if upper A x equals bAx = b, for any conformable y, 
y Superscript normal upper T Baseline upper A x equals 0 long left right double arrow y Superscript normal upper T Baseline b equals 0 periodyTAx = 0 ⇐⇒yTb = 0.
(3.150) 
Consistent Systems 
A linear system upper A Subscript n times m Baseline x equals bAn×mx = b is consistent if and only if 
normal r normal a normal n normal k left parenthesis left bracket upper A vertical bar b right bracket right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank([A | b]) = rank(A).
(3.151) 
We can see this following the argument above that b element of script upper V left parenthesis upper A right parenthesisb ∈V(A); that is, the space 
spanned by the columns of A is the same as that spanned by the columns of 
A and the vector b. Therefore, b must be a linear combination of the columns 
of A, and furthermore, the linear combination is a solution to the system 
upper A x equals bAx = b. (Note, of course, that it is not necessary that it be a unique linear 
combination.) 
Equation (3.151) implies the equivalence of the conditions 
left bracket upper A vertical bar b right bracket y equals 0 for some y not equals 0 long left right double arrow upper A x equals 0 for some x not equals 0 period[A | b]y = 0 for some y /= 0
⇐⇒
Ax = 0 for some x /= 0.
(3.152) 
A special case that yields Eq. (3.151) for any b is 
normal r normal a normal n normal k left parenthesis upper A Subscript n times m Baseline right parenthesis equals n commarank(An×m) = n,
(3.153) 
and so if A is of full row rank, the system is consistent regardless of the value 
of b. In this case, of course, the number of rows of A must be no greater than 
the number of columns (by inequality (3.130)). A square system in which A is 
nonsingular is clearly consistent. (The condition of consistency is also called 
“compatibility” of the system; that is, the linear system upper A x equals bAx = b is said to be 
compatible if it is consistent.)

128
3 Basic Properties of Matrices
Multiple Consistent Systems 
A generalization of the linear system upper A x equals bAx = b is 
upper A upper X equals upper B commaAX = B,
(3.154) 
where B is an n times kn×k matrix. This is the  same  as  k systems upper A x 1 equals b 1 comma ellipsis comma upper A x Subscript k Baseline equals b Subscript k BaselineAx1 = b1, . . . , Axk =
bk, where  the  x Subscript ixi and the b Subscript ibi are the columns of the respective matrices. Con-
sistency of upper A upper X equals upper BAX = B, as above, is the condition for a solution in X to exist, 
and in that case the system is also said to be compatible. 
It is clear that the system upper A upper X equals upper BAX = B is consistent if each of the upper A x Subscript i Baseline equals b Subscript iAxi = bi
systems is consistent. Furthermore, if the system is consistent, then every 
linear relationship among the rows of A exists among the rows of B; that is,  
for any c such that c Superscript normal upper T Baseline upper A equals 0cTA = 0, then  c Superscript normal upper T Baseline upper B equals 0cTB = 0. To see this, let c be such that 
c Superscript normal upper T Baseline upper A equals 0cTA = 0. We then have  c Superscript normal upper T Baseline upper A upper X equals c Superscript normal upper T Baseline upper B equals 0cTAX = cTB = 0, and so the same linear relationship 
that exists among the rows of A exists among the rows of B. 
As above for upper A x equals bAx = b, we also see that the system upper A upper X equals upper BAX = B is consistent if 
and only if any of the following conditions hold: 
StartLayout 1st Row 1st Column script upper V left parenthesis upper B right parenthesis 2nd Column subset of or equal to 3rd Column script upper V left parenthesis upper A right parenthesis 2nd Row 1st Column script upper V left parenthesis left bracket upper A vertical bar upper B right bracket right parenthesis 2nd Column equals 3rd Column script upper V left parenthesis upper A right parenthesis 3rd Row 1st Column normal r normal a normal n normal k left parenthesis left bracket upper A vertical bar upper B right bracket right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A right parenthesis period EndLayoutV(B) ⊆V(A)
(3.155) 
V([A | B]) = V(A)
(3.156) 
rank([A | B]) = rank(A).
(3.157) 
These relations imply that if upper A upper X equals upper BAX = B is consistent, then for any con-
formable vector c, 
c Superscript normal upper T Baseline upper A equals 0 long left right double arrow c Superscript normal upper T Baseline upper B equals 0 periodcTA = 0 ⇐⇒cTB = 0.
(3.158) 
We discuss methods for solving consistent linear systems in Sect. 3.6 and 
in Chap. 5. In the next section, we consider a special case of n times nn × n (square) A 
when Eq. (3.153) is satisﬁed (that is, when A is nonsingular). 
3.4.7 Matrix Inverses 
The (Cayley) multiplicative inverse of a matrix, if it exists, is of interest. We 
have encountered the idea of a matrix inverse in our discussions of elementary 
transformation matrices in Sect. 3.3.3. The matrix that performs the inverse 
of the elementary operation is the inverse matrix. 
The Inverse of a Square Nonsingular Matrix 
Let A be an n times nn × n nonsingular matrix, and consider the linear systems 
upper A x Subscript i Baseline equals e Subscript i Baseline commaAxi = ei,
where e Subscript iei is the i normal t normal hith unit vector. For each e Subscript iei, this is a consistent system by 
Eq. (3.151).

3.4 Matrix Rank and the Inverse of a Matrix
129
We can represent all n such systems as 
upper A left bracket x 1 StartAbsoluteValue midline horizontal ellipsis EndAbsoluteValue x Subscript n Baseline right bracket equals left bracket e 1 StartAbsoluteValue midline horizontal ellipsis EndAbsoluteValue e Subscript n Baseline right bracketA
[
x1| · · · |xn
]
=
[
e1| · · · |en
]
or 
upper A upper X equals upper I Subscript n Baseline commaAX = In,
and this full system must have a solution; that is, there must be an X such 
that upper A upper X equals upper I Subscript nAX = In. Because upper A upper X equals upper IAX = I, we call  X a “right inverse” of A. The  matrix  
X must be n times nn × n and nonsingular (because I is); hence, it also has a right 
inverse, say Y , and  upper X upper Y equals upper IXY = I. From  upper A upper X equals upper IAX = I, we have  upper A upper X upper Y equals upper YAXY = Y , so  upper A equals upper YA = Y , 
and so ﬁnally upper X upper A equals upper IXA = I; that is, the right inverse of A is also the “left inverse”. 
We will therefore just call it the inverse of A and denote it as upper A Superscript negative 1A−1. This is  
the Cayley multiplicative inverse. Hence, for an n times nn × n nonsingular matrix A, 
we have a matrix upper A Superscript negative 1A−1 such that 
upper A Superscript negative 1 Baseline upper A equals upper A upper A Superscript negative 1 Baseline equals upper I Subscript n Baseline periodA−1A = AA−1 = In.
(3.159) 
The inverse of the nonsingular square matrix A is unique. (This follows from 
an argument similar to that above about a “right inverse” and a “left inverse.”) 
The Inverse and the Solution to a Linear System 
In the linear system (3.149), if A is square and nonsingular, the system is 
consistent and the solution is 
x equals upper A Superscript negative 1 Baseline b periodx = A−1b.
(3.160) 
For scalars, the combined operations of inversion and multiplication are 
equivalent to the single operation of division. From the analogy with scalar op-
erations, we sometimes denote upper A upper B Superscript negative 1AB−1 by upper A divided by upper BA/B. Because matrix multiplication 
is not commutative, we often use the notation “\” to indicate the combined 
operations of inversion and multiplication on the left; that is, B\A is the same 
as upper B Superscript negative 1 Baseline upper AB−1A. The solution given in Eq. (3.160) is also sometimes represented as 
A\b. 
We discuss the solution of systems of equations in Chap. 5, but here we 
will point out that when we write an expression that involves computations to 
evaluate it, such as upper A Superscript negative 1 Baseline bA−1b or A\b, the form of the expression does not specify 
how to do the computations. This is an instance of a principle that we will 
encounter repeatedly: the form of a mathematical expression and the way the 
expression should be evaluated in actual practice may be quite diﬀerent. 
Inverses and Transposes 
From the deﬁnitions of the inverse and the transpose, we see that 
left parenthesis upper A Superscript negative 1 Baseline right parenthesis Superscript normal upper T Baseline equals left parenthesis upper A Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline comma(A−1)T = (AT)−1,
(3.161)

130
3 Basic Properties of Matrices
and because in applications we often encounter the inverse of a transpose of 
a matrix, we adopt the notation 
upper A Superscript negative normal upper TA−T
to denote the inverse of the transpose. 
Nonsquare Full Rank Matrices: Right and Left Inverses 
Suppose A is n times mn × m and normal r normal a normal n normal k left parenthesis upper A right parenthesis equals nrank(A) = n; that is,  n less than or equals mn ≤m and A is of full row 
rank. Then normal r normal a normal n normal k left parenthesis left bracket upper A vertical bar e Subscript i Baseline right bracket right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesisrank([A | ei]) = rank(A), where  e Subscript iei is the i normal t normal hith unit vector of order 
n; hence the system 
upper A x Subscript i Baseline equals e Subscript iAxi = ei
is consistent for each e Subscript iei, and, as before, we can represent all n such systems 
as 
upper A left bracket x 1 StartAbsoluteValue midline horizontal ellipsis EndAbsoluteValue x Subscript n Baseline right bracket equals left bracket e 1 StartAbsoluteValue midline horizontal ellipsis EndAbsoluteValue e Subscript n Baseline right bracketA
[
x1| · · · |xn
]
=
[
e1| · · · |en
]
or 
upper A upper X equals upper I Subscript n Baseline periodAX = In.
As above, there must be an X such that upper A upper X equals upper I Subscript nAX = In, and  we  call  X a right 
inverse of A. The  matrix  X must be m times nm × n, and it must be of rank n (because 
I is). This matrix is not necessarily the inverse of A, however, because A and 
X may not be square. We denote the right inverse of A as 
upper A Superscript negative normal upper R Baseline periodA−R.
Furthermore, we could only have solved the system AX if A was of full row 
rank because n less than or equals mn ≤m and n equals normal r normal a normal n normal k left parenthesis upper I right parenthesis equals normal r normal a normal n normal k left parenthesis upper A upper X right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesisn = rank(I) = rank(AX) ≤rank(A). To summarize, 
A has a right inverse if and only if A is of full row rank. 
Now, suppose A is n times mn × m and normal r normal a normal n normal k left parenthesis upper A right parenthesis equals mrank(A) = m; that is,  m less than or equals nm ≤n and A is of 
full column rank. Writing upper Y upper A equals upper I Subscript mY A = Im and reversing the roles of the coeﬃcient 
matrix and the solution matrix in the argument above, we have that Y exists 
and is a left inverse of A. We denote the left inverse of A as 
upper A Superscript negative normal upper L Baseline periodA−L.
Also, using a similar argument as above, we see that the matrix A has a left 
inverse if and only if A is of full column rank. 
We also note that if upper A upper A Superscript normal upper TAAT is of full rank, the right inverse of A is 
upper A Superscript negative normal upper R Baseline equals upper A Superscript normal upper T Baseline left parenthesis upper A upper A Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline periodA−R = AT(AAT)−1.
(3.162) 
Likewise, if upper A Superscript normal upper T Baseline upper AATA is of full rank, the left inverse of A is 
upper A Superscript negative normal upper L Baseline equals left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis Superscript negative 1 Baseline upper A Superscript normal upper T Baseline periodA−L = (ATA)−1AT.
(3.163)

3.4 Matrix Rank and the Inverse of a Matrix
131
3.4.8 Full Rank Factorization 
Factorizations or decompositions play important roles both in theoretical de-
velopment and in computations with matrices. We will discuss some factor-
izations as they arise naturally in the development here, and then in Chap. 4 
we will discuss factorizations in more detail. 
The full rank partitioning of an n times mn×m matrix as in Eq. (3.145) on page 125 
leads to an interesting factorization of a matrix. Recall that we had an n times mn × m
matrix B partitioned as 
upper B equals Start 2 By 2 Matrix 1st Row 1st Column upper W Subscript r times r Baseline 2nd Column upper X Subscript r times m minus r Baseline 2nd Row 1st Column upper Y Subscript n minus r times r Baseline 2nd Column upper Z Subscript n minus r times m minus r Baseline EndMatrix commaB =
[ Wr×r
Xr×m−r
Yn−r×r Zn−r×m−r
]
,
where r is the rank of B, W is of full rank, the rows of upper R equals left bracket upper W vertical bar upper X right bracketR = [W|X] span the 
full row space of B, and the columns of upper C equals StartBinomialOrMatrix upper W Choose upper Y EndBinomialOrMatrixC =
[ W
Y
]
span the full column space 
of B. 
Therefore, for some T, we have  left bracket upper Y vertical bar upper Z right bracket equals upper T upper R[Y |Z] = TR, and  for some  S, we have  
StartBinomialOrMatrix upper X Choose upper Z EndBinomialOrMatrix equals upper C upper S
[ X
Z
]
= CS. From this, we have upper Y equals upper T upper WY = TW, upper Z equals upper T upper XZ = TX, upper X equals upper W upper SX = WS, and  upper Z equals upper Y upper SZ = Y S, 
so upper Z equals upper T upper W upper SZ = TWS. Since  W is nonsingular, we have upper T equals upper Y upper W Superscript negative 1T = Y W −1 and upper S equals upper W Superscript negative 1 Baseline upper XS = W −1X, 
so upper Z equals upper Y upper W Superscript negative 1 Baseline upper XZ = Y W −1X. 
We can therefore write the partitions as 
StartLayout 1st Row 1st Column upper B 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column upper W 2nd Column upper X 2nd Row 1st Column upper Y 2nd Column upper Y upper W Superscript negative 1 Baseline upper X EndMatrix 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartBinomialOrMatrix upper I Choose upper Y upper W Superscript negative 1 Baseline EndBinomialOrMatrix upper W left bracket upper I vertical bar upper W Superscript negative 1 Baseline upper X right bracket period EndLayoutB =
[ W
X
Y
Y W −1X
]
=
[
I
Y W −1
]
W
[
I | W −1X
]
.
(3.164) 
From this, we can form two equivalent factorizations of B: 
upper B equals StartBinomialOrMatrix upper W Choose upper Y EndBinomialOrMatrix left bracket upper I vertical bar upper W Superscript negative 1 Baseline upper X right bracket equals StartBinomialOrMatrix upper I Choose upper Y upper W Superscript negative 1 Baseline EndBinomialOrMatrix left bracket upper W vertical bar upper X right bracket periodB =
[
W
Y
] [
I | W −1X
]
=
[
I
Y W −1
] [
W | X
]
.
The matrix B has a very special property: the full set of linearly indepen-
dent rows are the ﬁrst r rows, and the full set of linearly independent columns 
are the ﬁrst r columns. We have seen, however, that any matrix A of rank 
r can be put in this form, and upper A equals upper E Subscript left parenthesis pi 2 right parenthesis Baseline upper B upper E Subscript left parenthesis pi 1 right parenthesisA = E(π2)BE(π1) for an n times nn × n permutation 
matrix upper E Subscript left parenthesis pi 2 right parenthesisE(π2) and an m times mm × m permutation matrix upper E Subscript left parenthesis pi 1 right parenthesisE(π1). 
We therefore have, for the n times mn × m matrix A with rank r, two equivalent 
factorizations, 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column StartBinomialOrMatrix upper Q 1 upper W Choose upper Q 2 upper Y EndBinomialOrMatrix left bracket upper P 1 vertical bar upper W Superscript negative 1 Baseline upper X upper P 2 right bracket 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartBinomialOrMatrix upper Q 1 Choose upper Q 2 upper Y upper W Superscript negative 1 Baseline EndBinomialOrMatrix left bracket upper W upper P 1 vertical bar upper X upper P 2 right bracket comma EndLayout
A =
[
Q1W
Q2Y
] [
P1 | W −1XP2
]
=
[
Q1
Q2Y W −1
] [
WP1 | XP2
]
,
(3.165)

132
3 Basic Properties of Matrices
where upper Q 1Q1, upper Q 2Q2, upper P 1P1, and  upper P 2P2 are products of elementary permutation matrices, 
and hence of full rank. 
Both of the factorizations in (3.165) are in the general form 
upper A Subscript n times m Baseline equals upper L Subscript n times r Baseline upper R Subscript r times m Baseline commaAn×m = Ln×r Rr×m,
(3.166) 
where L is of full column rank and R is of full row rank. This is called a full 
rank factorization of the matrix A. 
A full rank factorization is useful in proving various properties of matrices. 
We will consider other factorizations later in this chapter and in Chap. 4 that 
have more practical uses in computations. 
3.4.9 Multiplication by Full Rank Matrices 
We have seen that a matrix has an inverse if it is square and of full rank. 
Conversely, it has an inverse only if it is square and of full rank. We see that 
a matrix that has an inverse must be square because upper A Superscript negative 1 Baseline upper A equals upper A upper A Superscript negative 1A−1A = AA−1, and  
we see that it must be full rank by the inequality (3.142). In this section, we 
consider other properties of full rank matrices. In some cases, we require the 
matrices to be square, but in other cases, these properties hold whether or 
not they are square. 
Using matrix inverses allows us to establish important properties of prod-
ucts of matrices in which at least one factor is a full rank matrix. 
Products with a Nonsingular Matrix 
It is easy to see that if A is a square full rank matrix (that is, A is nonsingular), 
and if B and C are conformable matrices for the multiplications AB and CA, 
respectively, then 
normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(AB) = rank(B)
(3.167) 
and 
normal r normal a normal n normal k left parenthesis upper C upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper C right parenthesis periodrank(CA) = rank(C).
(3.168) 
This is true because, for a given conformable matrix B, by the inequal-
ity (3.142), we have normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(AB) ≤rank(B). Forming  upper B equals upper A Superscript negative 1 Baseline upper A upper BB = A−1AB, and again 
applying the inequality, we have normal r normal a normal n normal k left parenthesis upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper A upper B right parenthesisrank(B) ≤rank(AB); hence, normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(AB) =
rank(B). Likewise, for a square full rank matrix A, we have  normal r normal a normal n normal k left parenthesis upper C upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper C right parenthesisrank(CA) =
rank(C). (Here, we should recall that all matrices are real.) 
Products with a General Full Rank Matrix 
If C is a full column rank matrix and if B is a matrix conformable for the 
multiplication CB, then  
normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesis periodrank(CB) = rank(B).
(3.169)

3.4 Matrix Rank and the Inverse of a Matrix
133
To see this, consider a full rank n times mn × m matrix C with normal r normal a normal n normal k left parenthesis upper C right parenthesis equals mrank(C) = m (that is, 
m less than or equals nm ≤n) and  let  B be conformable for the multiplication CB. Because C is 
of full column rank, it has a left inverse (see page 130); call it upper C Superscript negative normal upper LC−L, and  so  
upper C Superscript negative normal upper L Baseline upper C equals upper I Subscript mC−LC = Im. From inequality (3.142), we have normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(CB) ≤rank(B), and  ap-
plying the inequality again, we have normal r normal a normal n normal k left parenthesis upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper C Superscript negative normal upper L Baseline upper C upper B right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper C upper B right parenthesisrank(B) = rank(C−LCB) ≤rank(CB); 
hence normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(CB) = rank(B). 
If R is a full row rank matrix and if B is a matrix conformable for the 
multiplication BR, then  
normal r normal a normal n normal k left parenthesis upper B upper R right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesis periodrank(BR) = rank(B).
(3.170) 
To see this, consider a full rank n times mn × m matrix R with normal r normal a normal n normal k left parenthesis upper R right parenthesis equals nrank(R) = n (that is, 
n less than or equals mn ≤m) and  let  B be conformable for the multiplication BR. Because R is of 
full row rank, it has a right inverse; call it upper R Superscript negative normal upper RR−R, and  so  upper R upper R Superscript negative normal upper R Baseline equals upper I Subscript nRR−R = In. From  
inequality (3.142), we have normal r normal a normal n normal k left parenthesis upper B upper R right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(BR) ≤rank(B), and applying the inequality 
again, we have normal r normal a normal n normal k left parenthesis upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B upper R upper R Superscript negative normal upper L Baseline right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper B upper R right parenthesisrank(B) = rank(BRR−L) ≤rank(BR); hence normal r normal a normal n normal k left parenthesis upper B upper R right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(BR) =
rank(B). 
To state this more simply: 
• Premultiplication of a given matrix by a full column rank matrix yields a 
product with the same rank as the given matrix, and postmultiplication 
of a given matrix by a full row rank matrix yields a product with the same 
rank as the given matrix. 
From this we see that, given any matrix B, if  A is a square matrix of full 
rank that is compatible for the multiplication upper A upper B equals upper DAB = D, then  B and D are 
equivalent matrices. (And, of course, a similar statement for postmultiplica-
tion by a full rank matrix holds.) 
Furthermore, if the matrix B is square and A is a square matrix of the 
same order that is full rank, then 
normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesis periodrank(AB) = rank(BA) = rank(B).
(3.171) 
Preservation of Positive Deﬁniteness 
A certain type of product of a full rank matrix and a positive deﬁnite matrix 
preserves not only the rank but also the positive deﬁniteness: if A is n times nn × n
and positive deﬁnite, and C is n times mn × m and of rank m (hence, m less than or equals nm ≤n), then 
upper C Superscript normal upper T Baseline upper A upper CCTAC is positive deﬁnite. (Recall from inequality (3.101) that a matrix  A is 
positive deﬁnite if it is symmetric and for any x not equals 0x /= 0, x Superscript normal upper T Baseline upper A x greater than 0xTAx > 0.) 
To see this, assume matrices A and C as described. Let x be any m-vector 
such that x not equals 0x /= 0, and  let  y equals upper C xy = Cx. Because C is of full column rank, y not equals 0y /= 0. We  
have 
StartLayout 1st Row 1st Column x Superscript normal upper T Baseline left parenthesis upper C Superscript normal upper T Baseline upper A upper C right parenthesis x 2nd Column equals 3rd Column left parenthesis upper C x right parenthesis Superscript normal upper T Baseline upper A left parenthesis upper C x right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column y Superscript normal upper T Baseline upper A y 3rd Row 1st Column Blank 2nd Column greater than 3rd Column 0 period EndLayoutxT(CTAC)x = (Cx)TA(Cx)
= yTAy
> 0.
(3.172)

134
3 Basic Properties of Matrices
Therefore, since upper C Superscript normal upper T Baseline upper A upper CCTAC is symmetric, 
• if A is positive deﬁnite and C is of full column rank, then upper C Superscript normal upper T Baseline upper A upper CCTAC is positive 
deﬁnite. 
Furthermore, we have the converse: 
• if upper C Superscript normal upper T Baseline upper A upper CCTAC is positive deﬁnite, then C is of full column rank, 
for otherwise there exists an x not equals 0x /= 0 such that upper C x equals 0Cx = 0, and  so  x Superscript normal upper T Baseline left parenthesis upper C Superscript normal upper T Baseline upper A upper C right parenthesis x equals 0xT(CTAC)x = 0. 
The General Linear Group 
Consider the set of all square n times nn × n full rank matrices together with the usual 
(Cayley) multiplication. As we have seen, this set is closed under multiplica-
tion. (The product of two square matrices of full rank is of full rank, and of 
course the product is also square.) Furthermore, the (multiplicative) identity 
is a member of this set, and each matrix in the set has a (multiplicative) 
inverse in the set; therefore, the set together with the usual multiplication is 
a mathematical structure called a group. (See any text on modern algebra.) 
This group is called the general linear group and is denoted by script upper G upper L left parenthesis n right parenthesisGL(n). The  
order of the group is n, the order of the square matrices in the group. General 
group-theoretic properties can be used in the derivation of properties of these 
full-rank matrices. Note that this group is not commutative. 
We note that all matrices in the general linear group of order n are equiv-
alent. 
As we mentioned earlier (before we had considered inverses in general), if 
A is an n times nn × n matrix and if upper A Superscript negative 1A−1 exists, we deﬁne upper A Superscript 0A0 to be upper I Subscript nIn (otherwise, upper A Superscript 0A0
does not exist). 
The n times nn×n elementary operator matrices are members of the general linear 
group script upper G upper L left parenthesis n right parenthesisGL(n). 
The elements in the general linear group are matrices and, hence, can be 
viewed as transformations or operators on n-vectors. Another set of linear 
operators on n-vectors are the doubletons left parenthesis upper A comma v right parenthesis(A, v), where  A is an n times nn × n full 
rank matrix and v is an n-vector. As an operator on x element of normal upper I normal upper R Superscript nx ∈IRn, left parenthesis upper A comma v right parenthesis(A, v) is the 
transformation upper A x plus vAx + v, which preserves aﬃne spaces. Two such operators, 
left parenthesis upper A comma v right parenthesis(A, v) and left parenthesis upper B comma w right parenthesis(B, w), are combined by composition: left parenthesis upper A comma v right parenthesis left parenthesis left parenthesis upper B comma w right parenthesis left parenthesis x right parenthesis right parenthesis equals upper A upper B x plus upper A w plus v(A, v)((B, w)(x)) = ABx +
Aw + v. The set of such doubletons together with composition forms a group, 
called the aﬃne group. It is denoted by script upper A upper L left parenthesis n right parenthesisAL(n). A subset of the elements of 
the aﬃne group with the same ﬁrst element, together with the axpy operator, 
constitute a quotient space. 
3.4.10 Nonfull Rank and Equivalent Matrices 
There are several types of nonfull rank matrices that are of interest, such 
as nilpotent (page 98) and idempotent (pages 98 and 391) matrices, which 
are square matrices. The only idempotent matrix that is of full rank is the

3.4 Matrix Rank and the Inverse of a Matrix
135
identity matrix. All other idempotent matrices, and all nilpotent matrices are 
necessarily singular. 
Equivalent Canonical Forms 
For any n times mn×m matrix A with normal r normal a normal n normal k left parenthesis upper A right parenthesis equals r greater than 0rank(A) = r > 0, by combining the permutations 
that yield Eq. (3.145) with other operations, we have, for some matrices P and 
Q that are products of various elementary operator matrices, 
upper P upper A upper Q equals Start 2 By 2 Matrix 1st Row 1st Column upper I Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix periodPAQ =
[ Ir 0
0 0
]
.
(3.173) 
This is called an equivalent canonical form of A, and it exists for any matrix 
A that has at least one nonzero element (which is the same as requiring 
normal r normal a normal n normal k left parenthesis upper A right parenthesis greater than 0rank(A) > 0). 
We can see by construction that an equivalent canonical form exists for 
any n times mn × m matrix A that has a nonzero element. First, assume a Subscript i j Baseline not equals 0aij /= 0. By  
two successive permutations, we move a Subscript i jaij to the left parenthesis 1 comma 1 right parenthesis(1, 1) position; speciﬁcally, 
left parenthesis upper E Subscript i Baseline 1 Baseline upper A upper E Subscript 1 j Baseline right parenthesis Subscript 11 Baseline equals a Subscript i j(Ei1AE1j)11 = aij. We then divide the ﬁrst row by a Subscript i jaij; that is,  we  form  
upper E 1 left parenthesis 1 divided by a Subscript i j Baseline right parenthesis upper E Subscript i Baseline 1 Baseline upper A upper E Subscript 1 jE1(1/aij)Ei1AE1j. We then proceed with a sequence of n minus 1n −1 premultipli-
cations by axpy matrices to zero out the ﬁrst column of the matrix, as in 
expression (3.76), followed by a sequence of left parenthesis m minus 1 right parenthesis(m −1) postmultiplications by 
axpy matrices to zero out the ﬁrst row. We then have a matrix of the form 
Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 2nd Row 1st Column 0 2nd Column Blank 3rd Column Blank 4th Column Blank 3rd Row 1st Column vertical ellipsis 2nd Column left bracket 3rd Column upper X 4th Column right bracket 4th Row 1st Column 0 2nd Column Blank 3rd Column Blank 4th Column Blank EndMatrix period
⎡
⎢⎢⎢⎣
1 0 · · · 0
0
... [ X ]
0
⎤
⎥⎥⎥⎦.
(3.174) 
If upper X equals 0X = 0, we are ﬁnished; otherwise, we perform the same kinds of operations 
on the left parenthesis n minus 1 right parenthesis times left parenthesis m minus 1 right parenthesis(n −1) × (m −1) matrix X and continue until we have the form of 
Eq. (3.173). 
The matrices P and Q in Eq. (3.173) are not unique. The order in which 
they are built from elementary operator matrices can be very important in 
preserving the accuracy of the computations. 
Although the matrices P and Q in Eq. (3.173) are not unique, the equiva-
lent canonical form itself (the right-hand side) is obviously unique because the 
only thing that determines it, aside from the shape, is the r in upper I Subscript rIr, and  that  
is just the rank of the matrix. There are two other, more general, equivalent 
forms that are often of interest. These equivalent forms, “row echelon form” 
and “Hermite form,” are not unique. A matrix R is said to be in row echelon 
form, or just  echelon form, if  
r 
Subscript i j Baseline equals 0rij = 0 for i greater than ji > j, and  
• if k is such that r Subscript i k Baseline not equals 0rik /= 0 and r Subscript i l Baseline equals 0ril = 0 for l less than kl < k, then  r Subscript i plus 1 comma j Baseline equals 0ri+1,j = 0 for j less than or equals kj ≤k.

136
3 Basic Properties of Matrices
A matrix in echelon form is upper triangular. An upper triangular matrix H 
is said to be in Hermite form if 
h 
Subscript i i Baseline equals 0hii = 0 or 1, 
• if h Subscript i i Baseline equals 0hii = 0, then  h Subscript i j Baseline equals 0hij = 0 for all j, and  
• if h Subscript i i Baseline equals 1hii = 1, then  h Subscript k i Baseline equals 0hki = 0 for all k not equals ik /= i. 
If H is in Hermite form, then upper H squared equals upper HH2 = H, as is easily veriﬁed. (Hence, H is 
idempotent.) Another, more speciﬁc, equivalent form, called the Jordan form, 
is a special row echelon form based on eigenvalues, which we show on page 174. 
Any of these equivalent forms is useful in determining the rank of a ma-
trix. Each form may have special uses in proving properties of matrices. We 
will often make use of the equivalent canonical form in other sections of this 
chapter. 
A Factorization Based on an Equivalent Canonical Form 
Elementary operator matrices and products of them are of full rank and thus 
have inverses. When we introduced the matrix operations that led to the 
deﬁnitions of the elementary operator matrices in Sect. 3.3.3, we mentioned 
the inverse operations, which would then deﬁne the inverses of the matrices. 
The matrices P and Q in the equivalent canonical form of the matrix 
A, PAQ in Eq. (3.173), have inverses. From an equivalent canonical form of a 
matrix A with rank r, we therefore have the equivalent canonical factorization 
of A: 
upper A equals upper P Superscript negative 1 Baseline Start 2 By 2 Matrix 1st Row 1st Column upper I Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper Q Superscript negative 1 Baseline periodA = P −1
[
Ir 0
0 0
]
Q−1.
(3.175) 
A factorization based on an equivalent canonical form is also a full rank fac-
torization and could be written in the same form as Eq. (3.166). 
Equivalent Forms of Symmetric Matrices 
If A is symmetric, the equivalent form in Eq. (3.173) can be written as 
upper P upper A upper P Superscript normal upper T Baseline equals normal d normal i normal a normal g left parenthesis upper I Subscript r Baseline comma 0 right parenthesisPAP T = diag(Ir, 0) and the equivalent canonical factorization of A in 
Eq. (3.175) can be written as 
upper A equals upper P Superscript negative 1 Baseline Start 2 By 2 Matrix 1st Row 1st Column upper I Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper P Superscript negative normal upper T Baseline periodA = P −1
[
Ir 0
0 0
]
P −T.
(3.176) 
These facts follow from the same process that yielded Eq. (3.173) for a general 
matrix.

3.4 Matrix Rank and the Inverse of a Matrix
137
3.4.11 Gramian Matrices: Products of the Form AT A 
Given a real matrix A, an important matrix product is upper A Superscript normal upper T Baseline upper AATA. (This is called 
a Gramian matrix, or just a  Gram matrix. This kind of matrix occurs often 
in the analysis of linear models, and we will discuss these matrices in more 
detail beginning on page 397 and again in Chap. 9. I should note here that this 
is not a deﬁnition of “Gramian” or “Gram”; these terms have more general 
meanings, but they do include any matrix expressible as upper A Superscript normal upper T Baseline upper AATA.) 
We ﬁrst note that upper A upper A Superscript normal upper TAAT is also a Gramian matrix and has the same prop-
erties as upper A Superscript normal upper T Baseline upper AATA with any dependencies on A being replaced with dependencies 
on upper A Superscript normal upper TAT. 
General Properties of Gramian Matrices 
Gramian matrices have several interesting properties. First of all, we note that 
for any A, because 
left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis Subscript i j Baseline equals a Subscript asterisk i Superscript normal upper T Baseline a Subscript asterisk j Baseline equals a Subscript asterisk j Superscript normal upper T Baseline a Subscript asterisk i Baseline equals left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis Subscript j i Baseline left parenthesis normal r normal e normal c normal a normal l normal l normal n normal o normal t normal a normal t normal i normal o normal n comma normal p normal a normal g normal e(ATA)ij = aT
∗ia∗j = aT
∗ja∗i = (ATA)ji
(recall notation, page
(664),) 
upper A Superscript normal upper T Baseline upper AATA is symmetric and hence has all of the useful properties of symmetric 
matrices. (These properties are shown in various places in this book, but are 
summarized conveniently in Sect. 8.2 beginning on page 378.) 
We see that upper A Superscript normal upper T Baseline upper AATA is nonnegative deﬁnite by observing that for any y, 
y Superscript normal upper T Baseline left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis y equals left parenthesis upper A y right parenthesis Superscript normal upper T Baseline left parenthesis upper A y right parenthesis greater than or equals 0yT(ATA)y = (Ay)T(Ay) ≥0. 
Another interesting property of a Gramian matrix is that, for any matrices 
C and D (that are conformable for the operations indicated), 
upper C upper A Superscript normal upper T Baseline upper A equals upper D upper A Superscript normal upper T Baseline upper A long left right double arrow upper C upper A Superscript normal upper T Baseline equals upper D upper A Superscript normal upper T Baseline periodCATA = DATA
⇐⇒
CAT = DAT.
(3.177) 
The implication from right to left is obvious, and we can see the left to right 
implication by writing 
left parenthesis upper C upper A Superscript normal upper T Baseline upper A minus upper D upper A Superscript normal upper T Baseline upper A right parenthesis left parenthesis upper C Superscript normal upper T Baseline minus upper D Superscript normal upper T Baseline right parenthesis equals left parenthesis upper C upper A Superscript normal upper T Baseline minus upper D upper A Superscript normal upper T Baseline right parenthesis left parenthesis upper C upper A Superscript normal upper T Baseline minus upper D upper A Superscript normal upper T Baseline right parenthesis Superscript normal upper T Baseline comma(CATA −DATA)(CT −DT) = (CAT −DAT)(CAT −DAT)T,
and then observing that if the left-hand side is null, then so is the right-
hand side, and if the right-hand side is null, then upper C upper A Superscript normal upper T Baseline minus upper D upper A Superscript normal upper T Baseline equals 0CAT −DAT = 0 because 
upper A Superscript normal upper T Baseline upper A equals 0 long right double arrow upper A equals 0ATA = 0 =⇒A = 0, as above.  
Similarly, we have 
upper A Superscript normal upper T Baseline upper A upper C equals upper A Superscript normal upper T Baseline upper A upper D long left right double arrow upper A upper C equals upper A upper D periodATAC = ATAD
⇐⇒
AC = AD.
(3.178) 
Rank of AT A 
Consider the linear system upper A Superscript normal upper T Baseline upper A upper X equals upper A Superscript normal upper T Baseline upper BATAX = ATB. Suppose that c is such that 
c Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A equals 0cTATA = 0. Then by (3.177), c Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals 0cTAT = 0, which  by  (3.158) on page 128, implies 
that upper A Superscript normal upper T Baseline upper A upper X equals upper A Superscript normal upper T Baseline upper BATAX = ATB is consistent. Letting upper B equals upper IB = I, we have that  upper A Superscript normal upper T Baseline upper A upper X equals upper A Superscript normal upper TATAX = AT
is consistent.

138
3 Basic Properties of Matrices
Now if upper A Superscript normal upper T Baseline upper A upper X equals upper A Superscript normal upper TATAX = AT, for any conformable matrix K, 
script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline right parenthesis equals script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A upper X right parenthesis periodV(KTAT) = V(KTATAX).
By (3.141) on page 124, script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A upper X right parenthesis subset of or equal to script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A right parenthesisV(KTATAX) ⊆V(KTATA) and script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A right parenthesis subset of or equal to script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline right parenthesisV(KTATA) ⊆
V(KTAT); hence script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A right parenthesis equals script upper V left parenthesis upper K Superscript normal upper T Baseline upper A Superscript normal upper T Baseline right parenthesisV(KTATA) = V(KTAT). By similar arguments applied to 
the transposes we have  script upper V left parenthesis upper A Superscript normal upper T Baseline upper A upper K right parenthesis equals script upper V left parenthesis upper A upper K right parenthesisV(ATAK) = V(AK). 
With upper K equals upper IK = I, this yields 
normal r normal a normal n normal k left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(ATA) = rank(A).
(3.179) 
In a similar manner, we have normal r normal a normal n normal k left parenthesis upper A upper A Superscript normal upper T Baseline right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(AAT) = rank(A), and hence, 
normal r normal a normal n normal k left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper A upper A Superscript normal upper T Baseline right parenthesis periodrank(ATA) = rank(AAT).
(3.180) 
It is clear from the statements above that upper A Superscript normal upper T Baseline upper AATA is of full rank if and only 
if A is of full column rank. 
We also see further that upper A Superscript normal upper T Baseline upper AATA is positive deﬁnite, that is, for any y not equals 0y /= 0
y Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A y greater than 0yTATAy > 0, if and only if A is of full column rank. This follows from (3.183), 
and if A is of full column rank, upper A y equals 0 right double arrow y equals 0Ay = 0 ⇒y = 0. 
From Eq. (3.179), we have another useful fact for Gramian matrices. The 
system 
upper A Superscript normal upper T Baseline upper A x equals upper A Superscript normal upper T Baseline bATAx = ATb
(3.181) 
is consistent for any A and b. 
Zero Matrices and Equations Involving Gramians 
First of all, for any n times mn×m matrix A, we have the fact that upper A Superscript normal upper T Baseline upper A equals 0ATA = 0 if and only 
if upper A equals 0A = 0. We see this by noting that if upper A equals 0A = 0, then  normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals 0tr(ATA) = 0. Conversely, 
if normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals 0tr(ATA) = 0, then  a Subscript i j Superscript 2 Baseline equals 0a2
ij = 0 for all i comma ji, j, and  so  a Subscript i j Baseline equals 0aij = 0, that is,  upper A equals 0A = 0. 
Summarizing, we have 
normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals 0 left right double arrow upper A equals 0tr(ATA) = 0 ⇔A = 0
(3.182) 
and 
upper A Superscript normal upper T Baseline upper A equals 0 left right double arrow upper A equals 0 periodATA = 0 ⇔A = 0.
(3.183) 
3.4.12 A Lower Bound on the Rank of a Matrix Product 
Equation (3.142) gives an upper bound on the rank of the product of two 
matrices; the rank cannot be greater than the rank of either of the factors. 
Now, using Eq. (3.175), we develop a lower bound on the rank of the product 
of two matrices if one of them is square. 
If A is n times nn × n (that is, square), and B is a matrix with n rows, then 
normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis greater than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesis minus n periodrank(AB) ≥rank(A) + rank(B) −n.
(3.184)

3.4 Matrix Rank and the Inverse of a Matrix
139
We see this by ﬁrst letting r equals normal r normal a normal n normal k left parenthesis upper A right parenthesisr = rank(A), letting P and Q be matrices that 
form an equivalent canonical form of A (see Eq. (3.175)),  and then forming  
upper C equals upper P Superscript negative 1 Baseline Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 0 2nd Row 1st Column 0 2nd Column upper I Subscript n minus r Baseline EndMatrix upper Q Superscript negative 1 Baseline commaC = P −1
[
0 0
0 In−r
]
Q−1,
so that upper A plus upper C equals upper P Superscript negative 1 Baseline upper Q Superscript negative 1A + C = P −1Q−1. Because upper P Superscript negative 1P −1 and upper Q Superscript negative 1Q−1 are of full rank, normal r normal a normal n normal k left parenthesis upper C right parenthesis equals normal r normal a normal n normal k left parenthesis upper I Subscript n minus r Baseline right parenthesis equals n minus normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(C) =
rank(In−r) = n −rank(A). We now develop an upper bound on normal r normal a normal n normal k left parenthesis upper B right parenthesisrank(B), 
StartLayout 1st Row 1st Column normal r normal a normal n normal k left parenthesis upper B right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper P Superscript negative 1 Baseline upper Q Superscript negative 1 Baseline upper B right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B plus upper C upper B right parenthesis 3rd Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 4th Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 5th Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus n minus normal r normal a normal n normal k left parenthesis upper A right parenthesis comma EndLayoutrank(B) = rank(P −1Q−1B)
= rank(AB + CB)
≤rank(AB) + rank(CB), by equation (3.143St
artLayout 1st Row 1st Column normal r normal a normal n normal k left parenthesis upper B right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper P Superscript negative 1 Baseline upper Q Superscript negative 1 Baseline upper B right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B plus upper C upper B right parenthesis 3rd Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 4th Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 5th Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus n minus normal r normal a normal n normal k left parenthesis upper A right parenthesis comma EndLayout3.142St
artLayout 1 st  Row 1st Column normal r normal a normal n normal k left parenthesis upper B right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper P Superscript negative 1 Baseline upper Q Superscript negative 1 Baseline upper B right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B plus upper C upper B right parenthesis 3rd Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C upper B right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 4th Row 1st Column Blank 2nd Column less than or equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus normal r normal a normal n normal k left parenthesis upper C right parenthesis comma normal b normal y normal e normal q normal u normal a normal t normal i normal o normal n left parenthesis question mark question mark question mark right parenthesis 5th Row 1st Column Blank 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis plus n minus normal r normal a normal n normal k left parenthesis upper A right parenthesis comma EndLayout
yielding (3.184), a lower bound on normal r normal a normal n normal k left parenthesis upper A upper B right parenthesisrank(AB). 
The inequality (3.184) is called Sylvester’s law of nullity. It provides a 
lower bound on normal r normal a normal n normal k left parenthesis upper A upper B right parenthesisrank(AB) to go with the upper bound of inequality (3.142), 
min left parenthesis normal r normal a normal n normal k left parenthesis upper A right parenthesis comma normal r normal a normal n normal k left parenthesis upper B right parenthesis right parenthesismin(rank(A), rank(B)). The bound in inequality (3.184) is also sharp, as we  
can see by exhibiting matrices A and B such that normal r normal a normal n normal k left parenthesis upper A upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper B right parenthesis minus nrank(AB) = rank(A) +
rank(B) −n, as you are asked to do in Exercise 3.14b. 
3.4.13 Determinants of Inverses 
From the relationship normal d normal e normal t left parenthesis upper A upper B right parenthesis equals normal d normal e normal t left parenthesis upper A right parenthesis normal d normal e normal t left parenthesis upper B right parenthesisdet(AB) = det(A) det(B) for square matrices men-
tioned earlier, it is easy to see that for nonsingular square A, 
normal d normal e normal t left parenthesis upper A Superscript negative 1 Baseline right parenthesis equals left parenthesis normal d normal e normal t left parenthesis upper A right parenthesis right parenthesis Superscript negative 1 Baseline commadet(A−1) = (det(A))−1,
(3.185) 
and so 
no
rmal d normal e normal t left parenthesis upper A right parenthesis equals 0det(A) = 0 if  and only if  A is singular. 
(From the deﬁnition of the determinant in Eq. (3.28), we see that the de-
terminant of any ﬁnite-dimensional matrix with ﬁnite elements is ﬁnite. We 
implicitly assume that the elements are ﬁnite.) 
For an n times nn × n matrix with n greater than or equals 2n ≥2 whose determinant is nonzero, from 
Eq. (3.98) we have  
upper A Superscript negative 1 Baseline equals StartFraction 1 Over normal d normal e normal t left parenthesis upper A right parenthesis EndFraction normal a normal d normal j left parenthesis upper A right parenthesis periodA−1 =
1
det(A)adj(A).
(3.186) 
If normal d normal e normal t left parenthesis upper A right parenthesis equals 1det(A) = 1, this obviously implies 
upper A Superscript negative 1 Baseline equals normal a normal d normal j left parenthesis upper A right parenthesis periodA−1 = adj(A).
See Exercise 3.17 on page 211 for an interesting consequence of this.

140
3 Basic Properties of Matrices
3.4.14 Inverses of Products and Sums of Nonsingular Matrices 
In linear regression analysis and other applications, we sometimes need in-
verses of various sums or products of matrices. In regression analysis, this 
may be because we wish to update regression estimates based on additional 
data or because we wish to delete some observations. 
There is no simple relationship between the inverses of factors in a 
Hadamard product and the product matrix, but there are simple relation-
ships between the inverses of factors in Cayley and Kronecker products and 
the product matrices. 
Inverses of Cayley Products of Matrices 
The inverse of the Cayley product of two nonsingular matrices of the same 
size is particularly easy to form. If A and B are square full rank matrices of 
the same size, 
left parenthesis upper A upper B right parenthesis Superscript negative 1 Baseline equals upper B Superscript negative 1 Baseline upper A Superscript negative 1 Baseline period(AB)−1 = B−1A−1.
(3.187) 
We can see this by multiplying upper B Superscript negative 1 Baseline upper A Superscript negative 1B−1A−1 and left parenthesis upper A upper B right parenthesis(AB). This, of course, generalizes 
to 
left parenthesis upper A 1 midline horizontal ellipsis upper A Subscript n Baseline right parenthesis Superscript negative 1 Baseline equals upper A Subscript n Superscript negative 1 Baseline midline horizontal ellipsis upper A 1 Superscript negative 1(A1 · · · An)−1 = A−1
n · · · A−1
1
if upper A 1 comma midline horizontal ellipsis comma upper A Subscript n BaselineA1, · · · , An are all full rank and conformable. 
Inverses of Kronecker Products of Matrices 
If A and B are square full rank matrices, then 
left parenthesis upper A circled times upper B right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline circled times upper B Superscript negative 1 Baseline period(A ⊗B)−1 = A−1 ⊗B−1.
(3.188) 
We can see this by multiplying upper A Superscript negative 1 Baseline circled times upper B Superscript negative 1A−1 ⊗B−1 and upper A circled times upper BA ⊗B using Eq. (3.115) on  
page 117. 
Inverses of Sums of Matrices and Their Inverses 
The inverse of the sum of two nonsingular matrices is somewhat more com-
plicated. The ﬁrst question of course is whether the sum is nonsingular. We 
can develop several useful relationships of inverses of sums and the sums and 
products of the individual matrices. 
The simplest case to get started is upper I plus upper AI + A. Let  A and upper I plus upper AI + A be nonsingular. 
Then it is easy to derive left parenthesis upper I plus upper A right parenthesis Superscript negative 1(I + A)−1 by use of upper I equals upper A upper A Superscript negative 1I = AA−1 and Eq. (3.187). We 
get 
left parenthesis upper I plus upper A right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline left parenthesis upper I plus upper A Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline period(I + A)−1 = A−1(I + A−1)−1.
(3.189) 
If A and B are full rank matrices of the same size and such sums as upper I plus upper AI +A, 
upper A plus upper BA + B, and so on, are full rank, the following relationships are easy to show

3.4 Matrix Rank and the Inverse of a Matrix
141
(and are easily proven in the order given, using Eqs. (3.187) and  (3.189); see 
Exercise 3.18): 
StartLayout 1st Row 1st Column upper A left parenthesis upper I plus upper A right parenthesis Superscript negative 1 2nd Column equals 3rd Column left parenthesis upper I plus upper A Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline comma 2nd Row 1st Column Blank 3rd Row 1st Column left parenthesis upper A plus upper B right parenthesis Superscript negative 1 2nd Column equals 3rd Column upper A Superscript negative 1 Baseline minus upper A Superscript negative 1 Baseline left parenthesis upper A Superscript negative 1 Baseline plus upper B Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline upper A Superscript negative 1 Baseline comma 4th Row 1st Column Blank 5th Row 1st Column left parenthesis upper A plus upper B upper B Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline upper B 2nd Column equals 3rd Column upper A Superscript negative 1 Baseline upper B left parenthesis upper I plus upper B Superscript normal upper T Baseline upper A Superscript negative 1 Baseline upper B right parenthesis Superscript negative 1 Baseline comma 6th Row 1st Column Blank 7th Row 1st Column left parenthesis upper A Superscript negative 1 Baseline plus upper B Superscript negative 1 Baseline right parenthesis Superscript negative 1 2nd Column equals 3rd Column upper A left parenthesis upper A plus upper B right parenthesis Superscript negative 1 Baseline upper B comma 8th Row 1st Column Blank 9th Row 1st Column upper A minus upper A left parenthesis upper A plus upper B right parenthesis Superscript negative 1 Baseline upper A 2nd Column equals 3rd Column upper B minus upper B left parenthesis upper A plus upper B right parenthesis Superscript negative 1 Baseline upper B comma 10th Row 1st Column Blank 11th Row 1st Column upper A Superscript negative 1 Baseline plus upper B Superscript negative 1 2nd Column equals 3rd Column upper A Superscript negative 1 Baseline left parenthesis upper A plus upper B right parenthesis upper B Superscript negative 1 Baseline comma 12th Row 1st Column Blank 13th Row 1st Column left parenthesis upper I plus upper A upper B right parenthesis Superscript negative 1 2nd Column equals 3rd Column upper I minus upper A left parenthesis upper I plus upper B upper A right parenthesis Superscript negative 1 Baseline upper B comma 14th Row 1st Column Blank 15th Row 1st Column left parenthesis upper I plus upper A upper B right parenthesis Superscript negative 1 Baseline upper A 2nd Column equals 3rd Column upper A left parenthesis upper I plus upper B upper A right parenthesis Superscript negative 1 Baseline period EndLayoutA(I + A)−1 = (I + A−1)−1,
(3.190) 
(A + B)−1 = A−1 − A−1 (A−1 + B−1 )−1 A−1 ,
(3.191) 
(A + BBT )−1 B = A−1 B(I + BT A−1 B)−1 ,
(3.192) 
(A−1 + B−1 )−1 = A(A + B)−1 B,
(3.193) 
A −A(A + B)−1 A = B −B(A + B)−1 B,
(3.194) 
A−1 + B−1 = A−1 (A + B)B−1 ,
(3.195) 
(I + AB)−1 = I −A(I + BA)−1 B,
(3.196) 
(I + AB)−1 A = A(I + BA)−1 .
(3.197) 
When A and B are not of full rank, the inverses may not exist, but in that 
case these equations may or may not hold for a generalized inverse, which we 
will discuss in Sect. 3.7. 
Another simple general result, this time involving some non-square matri-
ces, is that if A is a full rank n times nn × n matrix, B is a full rank m times mm × m matrix, C 
is any n times mn × m matrix, and D is any m times nm × n matrix such that upper A plus upper C upper B upper DA + CBD is full 
rank, then 
left parenthesis upper A plus upper C upper B upper D right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline minus upper A Superscript negative 1 Baseline upper C left parenthesis upper B Superscript negative 1 Baseline plus upper D upper A Superscript negative 1 Baseline upper C right parenthesis Superscript negative 1 Baseline upper D upper A Superscript negative 1 Baseline period(A + CBD)−1 = A−1 −A−1C(B−1 + DA−1C)−1DA−1.
(3.198) 
This can be derived from Eq. (3.190), which is a special case of it. We can 
verify this by multiplication (Exercise 3.19). 
From this it also follows that if A is a full rank n times nn × n matrix and b and c 
are n-vectors such that left parenthesis upper A plus b c Superscript normal upper T Baseline right parenthesis(A + bcT) is full rank, then 
left parenthesis upper A plus b c Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline minus StartFraction upper A Superscript negative 1 Baseline b c Superscript normal upper T Baseline upper A Superscript negative 1 Baseline Over 1 plus c Superscript normal upper T Baseline upper A Superscript negative 1 Baseline b EndFraction period(A + bcT)−1 = A−1 −A−1bcTA−1
1 + cTA−1b .
(3.199) 
This fact has application in adding an observation to a least squares linear 
regression problem (page 453). 
An Expansion of a Matrix Inverse 
There is also an analog to the expansion of the inverse of left parenthesis 1 minus a right parenthesis(1 −a) for a scalar 
a:

142
3 Basic Properties of Matrices
left parenthesis 1 minus a right parenthesis Superscript negative 1 Baseline equals 1 plus a plus a squared plus a cubed plus midline horizontal ellipsis comma if StartAbsoluteValue a EndAbsoluteValue less than 1 period(1 −a)−1 = 1 + a + a2 + a3 + · · · ,
if |a| < 1.
This expansion for the scalar a comes from a factorization of the binomial 
1 minus a Superscript k1 −ak and the fact that a Superscript k Baseline right arrow 0ak →0 if StartAbsoluteValue a EndAbsoluteValue less than 1|a| < 1. 
To extend this to left parenthesis upper I plus upper A right parenthesis Superscript negative 1(I + A)−1 for a matrix A, we need a similar condition 
on upper A Superscript kAk as k increases without bound. In Sect. 3.11 on page 187, we will discuss 
conditions that ensure the convergence of upper A Superscript kAk for a square matrix A. We will 
deﬁne a norm parallel to upper A parallel to||A|| on A and show that if parallel to upper A parallel to less than 1||A|| < 1, then  upper A Superscript k Baseline right arrow 0Ak →0. Then, 
analogous to the scalar series, using Eq. (3.61) on page 99 for a square matrix 
A, we have  
left parenthesis upper I minus upper A right parenthesis Superscript negative 1 Baseline equals upper I plus upper A plus upper A squared plus upper A cubed plus midline horizontal ellipsis comma if parallel to upper A parallel to less than 1 period(I −A)−1 = I + A + A2 + A3 + · · · ,
if ||A|| < 1.
(3.200) 
We include this equation here because of its relation to Eqs. (3.190) through 
(3.196). We will discuss it further on page 195, after we have introduced and 
discussed parallel to upper A parallel to||A|| and other conditions that ensure convergence. This expression 
and the condition that determines it are very important in the analysis of 
time series and other stochastic processes. 
Also, looking ahead, we have another expression similar to Eqs. (3.190) 
through (3.196) and  (3.200) for a special type of matrix. If upper A squared equals upper AA2 = A, for any 
a not equals negative 1a /= −1, 
left parenthesis upper I plus a upper A right parenthesis Superscript negative 1 Baseline equals upper I minus StartFraction a Over a plus 1 EndFraction upper A(I + aA)−1 = I −
a
a + 1A
(see page 392). 
3.4.15 Inverses of Matrices with Special Forms 
Matrices with various special patterns may have inverses with similar patterns. 
• The inverse of a nonsingular symmetric matrix is symmetric. 
• The inverse of a diagonal matrix with nonzero entries is a diagonal matrix 
consisting of the reciprocals of those elements. 
• The inverse of a block diagonal matrix with nonsingular submatrices along 
the diagonal is a block diagonal matrix consisting of the inverses of the 
submatrices. 
• The inverse of a nonsingular triangular matrix is a triangular matrix with 
the same pattern; furthermore, the diagonal elements in the inverse are 
the reciprocals of the diagonal elements in the original matrix. 
Each of these statements can be easily proven by multiplication (using the 
fact that the inverse is unique). See also Exercise 3.21. 
The inverses of other matrices with special patterns, such as banded ma-
trices, may not have those patterns. 
In Chap. 8, we discuss inverses of various other special matrices that arise 
in applications in statistics.

3.5 The Schur Complement in Partitioned Square Matrices
143
3.4.16 Determining the Rank of a Matrix 
Although the equivalent canonical form (3.173) immediately gives the rank 
of a matrix, in practice the numerical determination of the rank of a matrix 
is not an easy task. The problem is that rank is a mapping normal upper I normal upper R Superscript n times m Baseline right arrow normal upper Z normal upper Z Subscript plusIRn×m →ZZ+, 
where normal upper Z normal upper Z Subscript plusZZ+ represents the positive integers. Such a function is often diﬃcult to 
compute because the domain is dense, and the range is sparse. Small changes 
in the domain may result in large discontinuous changes in the function value. 
(In Hadamard’s sense, the problem is ill-posed.) The common way that the 
rank of a matrix is evaluated is by use of the QR decomposition; see page 241. 
It is not even always clear whether a matrix is nonsingular. Because of 
rounding on the computer, a matrix that is mathematically nonsingular may 
appear to be singular. We sometimes use the phrase “nearly singular” or 
“algorithmically singular” to describe such a matrix. In Sects. 5.1 and 11.4, 
we consider this kind of problem in more detail. 
3.5 The Schur Complement in Partitioned Square 
Matrices 
A square matrix A that can be partitioned as 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[ A11 A12
A21 A22
]
,
(3.201) 
where upper A 11A11 is nonsingular, has interesting properties that depend on the matrix 
upper Z equals upper A 22 minus upper A 21 upper A 11 Superscript negative 1 Baseline upper A 12Z = A22 −A21A−1
11 A12
(3.202) 
The matrix Z in Eq. (3.202) is called the Schur complement of upper A 11A11 in A. 
If A is symmetric, then the Schur complement is upper A 22 minus upper A 12 Superscript normal upper T Baseline upper A 11 Superscript negative 1 Baseline upper A 12A22 −AT
12A−1
11 A12. 
Notice from Eq. (3.164) that if Eq. (3.201) represents a full rank partition-
ing (that is, if the rank of upper A 11A11 is the same as the rank of A), then upper Z equals 0Z = 0, 
and 
upper A equals Start 2 By 3 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 21 upper A 11 Superscript negative 1 Baseline upper A 12 EndMatrix periodA =
[ A11
A12
A21
A21A−1
11 A12
]
.
(3.203) 
This is sometimes a useful simpliﬁcation. 
The Schur complement yields important relationships between the inverses 
of the submatrices and the full matrix and between determinants of the sub-
matrices and the full matrix. 
The simpliﬁcations of the inverse and the determinant of patterned ma-
trices allow us to work out properties of the multivariate normal distribution 
and distributions derived from the normal, such as the Wishart distribution. 
We will discuss these distributions in Sects. 7.4.3 through 7.4.6.

144
3 Basic Properties of Matrices
3.5.1 Inverses of Partitioned Matrices 
Suppose A is nonsingular and can be partitioned as above with both upper A 11A11 and 
upper A 22A22 nonsingular. It is easy to see (Exercise 3.22, page 211) that the  inverse of  
A is given by 
upper A Superscript negative 1 Baseline equals Start 3 By 3 Matrix 1st Row 1st Column upper A 11 Superscript negative 1 Baseline plus upper A 11 Superscript negative 1 Baseline upper A 12 upper Z Superscript negative 1 Baseline upper A 21 upper A 11 Superscript negative 1 Baseline 2nd Column minus upper A 11 Superscript negative 1 Baseline upper A 12 upper Z Superscript negative 1 Baseline 2nd Row 1st Column Blank 2nd Column Blank 3rd Row 1st Column minus upper Z Superscript negative 1 Baseline upper A 21 upper A 11 Superscript negative 1 Baseline 2nd Column upper Z Superscript negative 1 Baseline EndMatrix commaA−1 =
⎡
⎣
A−1
11 + A−1
11 A12Z−1A21A−1
11
−A−1
11 A12Z−1
−Z−1A21A−1
11
Z−1
⎤
⎦,
(3.204) 
where upper Z equals upper A 22 minus upper A 21 upper A 11 Superscript negative 1 Baseline upper A 12Z = A22 −A21A−1
11 A12, that is,  Z is the Schur complement of upper A 11A11. 
If 
upper A equals left bracket upper X y right bracket Superscript normal upper T Baseline left bracket upper X y right bracketA = [X y]T [X y]
and is partitioned as in Eq. (3.65) on page 102 and X is of full column rank, 
then the Schur complement of upper X Superscript normal upper T Baseline upper XXTX in left bracket upper X y right bracket Superscript normal upper T Baseline left bracket upper X y right bracket[X y]T [X y] is 
y Superscript normal upper T Baseline y minus y Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline y periodyTy −yTX(XTX)−1XTy.
(3.205) 
This particular partitioning is useful in linear regression analysis (see, for ex-
ample, page 401), where this Schur complement is the residual sum of squares, 
and the more general Wishart distribution mentioned above reduces to a chi-
squared distribution. (Although the expression is useful, this is an instance 
of a principle that we will encounter repeatedly: the form of a mathematical 
expression and the way the expression should be evaluated in actual practice 
may be quite diﬀerent.) 
3.5.2 Determinants of Partitioned Matrices 
If the square matrix A is partitioned as 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[ A11 A12
A21 A22
]
,
and upper A 11A11 is square and nonsingular, then 
normal d normal e normal t left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper A 11 right parenthesis normal d normal e normal t left parenthesis upper A 22 minus upper A 21 upper A 11 Superscript negative 1 Baseline upper A 12 right parenthesis semicolondet(A) = det(A11) det
(
A22 −A21A−1
11 A12
)
;
(3.206) 
that is, the determinant is the product of the determinant of the principal 
submatrix and the determinant of its Schur complement. 
This result is obtained by using Eq. (3.33) on page 88 and the factorization 
Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix equals Start 2 By 3 Matrix 1st Row 1st Column upper A 11 2nd Column 0 2nd Row 1st Column upper A 21 2nd Column upper A 22 minus upper A 21 upper A 11 Superscript negative 1 Baseline upper A 12 EndMatrix Start 2 By 3 Matrix 1st Row 1st Column upper I 2nd Column upper A 11 Superscript negative 1 Baseline upper A 12 2nd Row 1st Column 0 2nd Column upper I EndMatrix period
[
A11 A12
A21 A22
]
=
[ A11
0
A21
A22 −A21A−1
11 A12
] [
I
A−1
11 A12
0
I
]
.
(3.207) 
The factorization in Eq. (3.207) is often useful in other contexts as well.

3.6 Linear Systems of Equations
145
3.6 Linear Systems of Equations 
Some of the most important applications of matrices are in representing and 
solving systems of n linear equations in m unknowns, 
upper A x equals b commaAx = b,
where A is an n times mn × m matrix, x is an m-vector, and b is an n-vector. As 
we observed in Eq. (3.63), the product Ax in the linear system is a linear 
combination of the columns of A; that is,  if  a Subscript jaj is the j normal t normal hjth column of A, upper A x equals sigma summation Underscript j equals 1 Overscript m Endscripts x Subscript j Baseline a Subscript jAx =
Em
j=1 xjaj. 
If b equals 0b = 0, the system is said to be homogeneous. In this case, unless x equals 0x = 0, 
the columns of A must be linearly dependent. This follows from the condition 
expressed in Eq. (2.3). 
3.6.1 Solutions of Linear Systems 
When in the linear system upper A x equals bAx = b, A is square and nonsingular, the solution is 
obviously x equals upper A Superscript negative 1 Baseline bx = A−1b. We will not discuss this simple but common case further 
here. Rather, we will discuss it in detail in Chap. 5 after we have discussed 
matrix factorizations later in this chapter and in Chap. 4. 
When A is not square or is singular, the system may not have a solution 
or may have more than one solution. A consistent system (see Eq. (3.151)) 
has a solution. For consistent systems that are singular or not square, the 
generalized inverse is an important concept. We introduce it in this section 
but defer its discussion to Sect. 3.7. 
Underdetermined Systems 
A consistent system in which normal r normal a normal n normal k left parenthesis upper A right parenthesis less than mrank(A) < m is said to be underdetermined. 
An underdetermined system may have fewer equations than variables, or the 
coeﬃcient matrix may just not be of full rank. For such a system there is 
more than one solution, which would correspond to diﬀerent completions of a 
spanning set of the m-dimensional vector space that contains the rows of A. 
In fact, there are inﬁnitely many solutions because if the vectors x 1x1 and x 2x2
are diﬀerent solutions, the vector w x 1 plus left parenthesis 1 minus w right parenthesis x 2wx1 + (1 −w)x2 is likewise a solution for 
any scalar w. 
Underdetermined systems arise in analysis of variance in statistics, and it 
is useful to have a compact method of representing the solution to the system. 
It is also desirable to identify a unique solution that has some kind of optimal 
properties. Below, we will discuss types of solutions and the number of linearly 
independent solutions and then describe a unique solution of a particular type.

146
3 Basic Properties of Matrices
Overdetermined Systems 
Often in mathematical modeling applications, the number of equations in the 
system upper A x equals bAx = b is not equal to the number of variables; that is the coeﬃcient 
matrix A is n times mn×m and n not equals mn /= m. If n greater than mn > m and normal r normal a normal n normal k left parenthesis left bracket upper A vertical bar b right bracket right parenthesis greater than normal r normal a normal n normal k left parenthesis upper A right parenthesisrank([A | b]) > rank(A), the system 
is said to be overdetermined. There is no x that satisﬁes such a system, but 
approximate solutions are useful. We discuss approximate solutions of such 
systems in Sect. 5.6 on page 282 and in Sect. 9.1 on page 438. 
Solutions in Consistent Systems: Generalized Inverses 
Consider the consistent system upper A x equals bAx = b. Let  G be a matrix such that upper A upper G upper A equals upper AAGA =
A. Then  
x equals upper G bx = Gb
(3.208) 
is a solution to the system. This is the case because if upper A upper G upper A equals upper AAGA = A, then upper A upper G upper A x equals upper A xAGAx =
Ax; and since upper A x equals bAx = b, 
upper A upper G b equals b semicolonAGb = b;
(3.209) 
that is, Gb is a solution. We see that this also holds for multiple consistent 
systems with the same coeﬃcient matrix, as upper A upper X equals upper BAX = B in Eq. (3.154): 
upper A upper G upper B equals upper B periodAGB = B.
(3.210) 
A matrix  G such that upper A upper G upper A equals upper AAGA = A is called a generalized inverse of A and 
is denoted by upper A Superscript minusA−: 
upper A upper A Superscript minus Baseline upper A equals upper A periodAA−A = A.
(3.211) 
Note that if A is n times mn × m, then  upper A Superscript minusA−is m times nm × n. 
Furthermore, if x equals upper G bx = Gb is any solution to the system upper A x equals bAx = b, then  upper A upper G upper A equals upper AAGA =
A; that is,  G is a generalized inverse of A. This can be seen by the following 
argument. Let a Subscript jaj be the j normal t normal hjth column of A. The  m systems of n equations, 
upper A x equals a Subscript jAx = aj, j equals 1 comma ellipsis comma mj = 1, . . . , m, all have solutions. (Each solution is a vector with 0s 
in all positions except the j normal t normal hjth position, which is a 1.) Now, if Gb is a solution 
to the original system, then by Eq. (3.210), upper G a Subscript jGaj is a solution to the system 
upper A x equals a Subscript jAx = aj. So  upper A upper G a Subscript j Baseline equals a Subscript jAGaj = aj for all j; hence upper A upper G upper A equals upper AAGA = A. 
If upper A x equals bAx = b is consistent, not only is upper A Superscript minus Baseline bA−b a solution but also, for any z, 
upper A Superscript minus Baseline b plus left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis zA−b + (I −A−A)z
(3.212) 
is a solution because upper A left parenthesis upper A Superscript minus Baseline b plus left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis z right parenthesis equals upper A upper A Superscript minus Baseline b plus left parenthesis upper A minus upper A upper A Superscript minus Baseline upper A right parenthesis z equals bA(A−b + (I −A−A)z) = AA−b + (A −AA−A)z = b. 
Furthermore, any solution to upper A x equals bAx = b can be represented as upper A Superscript minus Baseline b plus left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis zA−b+(I −A−A)z
for some z. This is because if y is any solution (that is, if upper A y equals bAy = b), we have 
y equals upper A Superscript minus Baseline b minus upper A Superscript minus Baseline upper A y plus y equals upper A Superscript minus Baseline b minus left parenthesis upper A Superscript minus Baseline upper A minus upper I right parenthesis y equals upper A Superscript minus Baseline b plus left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis z periody = A−b −A−Ay + y = A−b −(A−A −I)y = A−b + (I −A−A)z.
The number of linearly independent solutions arising from left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis z(I −A−A)z is 
just the rank of left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis(I −A−A), which  from  Eq. (3.229) is  m minus normal r normal a normal n normal k left parenthesis upper A right parenthesism −rank(A). 
We will discuss generalized inverses more extensively in Sect. 3.7.

3.6 Linear Systems of Equations
147
3.6.2 Null Space: The Orthogonal Complement 
The solutions of a consistent system upper A x equals bAx = b, which we characterized in 
Eq. (3.212) as  upper A Superscript minus Baseline b plus left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis zA−b + (I −A−A)z for any z, are formed as a given solution to 
upper A x equals bAx = b plus all solutions to upper A z equals 0Az = 0. 
For an n times mn × m matrix A, the set of vectors generated by all solutions, z, of  
the homogeneous system 
upper A z equals 0Az = 0
(3.213) 
is called the null space of A. We denote the null space of A by 
script upper N left parenthesis upper A right parenthesis periodN(A).
The null space is either the single 0 vector (in which case we say the null 
space is empty or null), or it is a vector space. (It is actually a vector space 
in either case, but recall our ambiguity about the null vector space, page 22.) 
We see that script upper N left parenthesis upper A right parenthesisN(A) is a vector space (if it is not empty) because the zero 
vector is in script upper N left parenthesis upper A right parenthesisN(A), and  if  x and y are in script upper N left parenthesis upper A right parenthesisN(A) and a is any scalar, a x plus yax + y is 
also a solution of upper A z equals 0Az = 0, and hence in script upper N left parenthesis upper A right parenthesisN(A). We call the dimension of script upper N left parenthesis upper A right parenthesisN(A)
the nullity of A. The nullity of A is 
StartLayout 1st Row 1st Column normal d normal i normal m left parenthesis script upper N left parenthesis upper A right parenthesis right parenthesis 2nd Column equals 3rd Column normal r normal a normal n normal k left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column m minus normal r normal a normal n normal k left parenthesis upper A right parenthesis EndLayoutdim(N(A)) = rank(I −A−A)
= m −rank(A)
(3.214) 
from Eq. (3.229). 
If upper A x equals bAx = b is consistent, any solution can be represented as upper A Superscript minus Baseline b plus zA−b + z, for  
some z in the null space of A, because if y is some solution, upper A y equals b equals upper A upper A Superscript minus Baseline bAy = b = AA−b
from Eq. (3.209), and so upper A left parenthesis y minus upper A Superscript minus Baseline b right parenthesis equals 0A(y −A−b) = 0; that is,  z equals y minus upper A Superscript minus Baseline bz = y −A−b is in the null 
space of A. If  A is nonsingular, then there is no such z, and the solution is 
unique. The number of linearly independent solutions to upper A z equals 0Az = 0, is the  same  
as the nullity of A. 
The order of script upper N left parenthesis upper A right parenthesisN(A) is m. (Recall that the order of script upper V left parenthesis upper A right parenthesisV(A) is n. The order of 
script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesisV(AT) is m.) 
If A is square, we have 
script upper N left parenthesis upper A right parenthesis subset of or equal to script upper N left parenthesis upper A squared right parenthesis subset of or equal to script upper N left parenthesis upper A cubed right parenthesis subset of or equal to midline horizontal ellipsisN(A) ⊆N(A2) ⊆N(A3) ⊆· · ·
(3.215) 
and 
script upper V left parenthesis upper A right parenthesis superset of or equal to script upper V left parenthesis upper A squared right parenthesis superset of or equal to script upper V left parenthesis upper A cubed right parenthesis superset of or equal to midline horizontal ellipsis periodV(A) ⊇V(A2) ⊇V(A3) ⊇· · · .
(3.216) 
(We see this easily from the inequality (3.142) on page 124.) 
If a is in script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesisV(AT) and b is in script upper N left parenthesis upper A right parenthesisN(A), we have  b Superscript normal upper T Baseline a equals b Superscript normal upper T Baseline upper A Superscript normal upper T Baseline x equals 0bTa = bTATx = 0. In other 
words, the null space of A is orthogonal to the row space of A; that is,  script upper N left parenthesis upper A right parenthesis perpendicular script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesisN(A) ⊥
V(AT). This is because upper A Superscript normal upper T Baseline x equals aATx = a for some x, and  upper A b equals 0Ab = 0 or b Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals 0bTAT = 0. For  
any matrix B whose columns are in script upper N left parenthesis upper A right parenthesisN(A), upper A upper B equals 0AB = 0, and  upper B Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals 0BTAT = 0. 
Because normal d normal i normal m left parenthesis script upper N left parenthesis upper A right parenthesis right parenthesis plus normal d normal i normal m left parenthesis script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesis right parenthesis equals mdim(N(A))+dim(V(AT)) = m and script upper N left parenthesis upper A right parenthesis perpendicular script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesisN(A) ⊥V(AT), by Eq. (2.43) 
we have

148
3 Basic Properties of Matrices
script upper N left parenthesis upper A right parenthesis circled plus script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals normal upper I normal upper R Superscript m Baseline semicolonN(A) ⊕V(AT) = IRm;
(3.217) 
that is, the null space of A is the orthogonal complement of script upper V left parenthesis upper A Superscript normal upper T Baseline right parenthesisV(AT). All vectors 
in the null space of the matrix upper A Superscript normal upper TAT are orthogonal to all vectors in the column 
space of A. 
3.6.3 Orthonormal Completion 
Consider a consistent system upper A x equals bAx = b in which the vectors formed by the 
rows of A, a Subscript i asteriskai∗, are linearly independent. In this case, in the n times mn × m matrix 
A, n less than or equals mn ≤m, that is, the number of equations is possibly less than the number 
of variables so the system is undetermined. Suppose, if n less than mn < m, we want to  
augment the system with additional equations so as to have a system with a 
unique solution. A reason to do this can be seen on page 348, where we need 
a full rank transformation. 
The problem is to ﬁnd an n times nn × n nonsingular matrix B such that 
upper B equals StartBinomialOrMatrix upper A Choose upper C EndBinomialOrMatrix periodB =
[ A
C
]
.
(3.218) 
The m minus nm −n vectors formed by the rows of C span script upper N left parenthesis upper A right parenthesisN(A). Geometrically, all 
of these vectors c Subscript i asteriskci∗are perpendicular to a hyperplane in script upper V left parenthesis upper A right parenthesisV(A), and  so  each  
satisﬁes 
upper A c Subscript i asterisk Baseline equals 0 periodAci∗= 0.
(3.219) 
The set of m minus nm −n vectors is not unique, so we will choose them so that 
they have desirable properties. A set that forms an orthonormal basis of script upper N left parenthesis upper A right parenthesisN(A)
would yield the result upper C Superscript normal upper T Baseline upper C equals upper I Subscript m minus nCTC = Im−n. 
The row of C can be found easily using Eq. (3.44) and the method indicated 
on page 92. See also Exercise 3.3. 
3.7 Generalized Inverses 
On page 146, we deﬁned a generalized inverse of a matrix A as a matrix upper A Superscript minusA−
such that 
upper A upper A Superscript minus Baseline upper A equals upper A periodAA−A = A.
(3.220) 
We will now consider some interesting properties of generalized inverses. 
Some examples and applications of generalized inverses are in Sect. 5.6.3, be-
ginning on page 287, and Sect. 9.1.3, beginning on page 444. 
3.7.1 Immediate Properties of Generalized Inverses 
First, note that if A is n times mn × m, then  upper A Superscript minusA−is m times nm × n. If  A is nonsingular (square 
and of full rank), then obviously upper A Superscript minus Baseline equals upper A Superscript negative 1A−= A−1. For any matrix A, a matrix  upper A Superscript minusA−
with the property (3.220) exists, as we will show on page 152.

3.7 Generalized Inverses
149
Without additional restrictions on A, the generalized inverse is not unique. 
Various types of generalized inverses can be deﬁned by adding restrictions to 
the deﬁnition of the inverse. Here we will consider some properties of any 
generalized inverse. 
From Eq. (3.220), we see that 
upper A Superscript normal upper T Baseline left parenthesis upper A Superscript minus Baseline right parenthesis Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals upper A Superscript normal upper T Baseline semicolonAT(A−)TAT = AT;
(3.221) 
thus, if upper A Superscript minusA−is a generalized inverse of A, then  left parenthesis upper A Superscript minus Baseline right parenthesis Superscript normal upper T(A−)T is a generalized inverse 
of upper A Superscript normal upper TAT. 
From this, we also see that if A is symmetric, then left parenthesis upper A Superscript minus Baseline right parenthesis Superscript normal upper T(A−)T is a generalized 
inverse of A. It is not the case, however, that left parenthesis upper A Superscript minus Baseline right parenthesis Superscript normal upper T Baseline equals upper A Superscript minus(A−)T = A−; that is, the gener-
alized inverse of a symmetric matrix is not necessarily symmetric. (Recall from 
page 142 that the inverse of a nonsingular symmetric matrix is symmetric.) 
Rank and the Reﬂexive Generalized Inverse 
From Eq. (3.142) together with the fact that upper A upper A Superscript minus Baseline upper A equals upper AAA−A = A, we see that 
normal r normal a normal n normal k left parenthesis upper A Superscript minus Baseline upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis commarank(A−A) = rank(A),
(3.222) 
which further implies that 
normal r normal a normal n normal k left parenthesis upper A Superscript minus Baseline right parenthesis greater than or equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(A−) ≥rank(A).
(3.223) 
Extending the same reasoning, we see that if (and only if) normal r normal a normal n normal k left parenthesis upper A Superscript minus Baseline right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesisrank(A−) =
rank(A), then  
upper A Superscript minus Baseline upper A upper A Superscript minus Baseline equals upper A Superscript minus Baseline periodA−AA−= A−.
(3.224) 
A generalized inverse that satisﬁes the condition (3.224) is called a reﬂexive 
generalized inverse or an outer pseudoinverse. It is sometimes denoted by upper A Superscript asteriskA∗. 
(The terminology and notation distinguishing the various types of generalized 
inverses is not used consistently in the literature.) 
A reﬂexive generalized inverse, as in Eq. (3.224), is also called a g 12g12 inverse, 
and the “general” generalized inverse as in Eq. (3.220), is called a g 1g1 inverse. 
A g 1g1 inverse is also called an inner pseudoinverse, or a  conditional inverse. 
Both g 1g1 and g 12g12 inverses are easily formed using Gaussian elimination or 
the sweep operator (page 447). 
The m times mm × m square matrices upper A Superscript minus Baseline upper AA−A and left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis(I −A−A) are often of interest. 
From the deﬁnition (3.220), we see that 
left parenthesis upper A Superscript minus Baseline upper A right parenthesis left parenthesis upper A Superscript minus Baseline upper A right parenthesis equals upper A Superscript minus Baseline upper A semicolon(A−A)(A−A) = A−A;
(3.225) 
that is, upper A Superscript minus Baseline upper AA−A is idempotent. 
By multiplication as above, we see that 
upper A left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis equals 0 commaA(I −A−A) = 0,
(3.226)

150
3 Basic Properties of Matrices
that 
left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis left parenthesis upper A Superscript minus Baseline upper A right parenthesis equals 0 comma(I −A−A)(A−A) = 0,
(3.227) 
and that left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis(I −A−A) is also idempotent: 
left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis equals left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis period(I −A−A)(I −A−A) = (I −A−A).
(3.228) 
The fact that  left parenthesis upper A Superscript minus Baseline upper A right parenthesis left parenthesis upper A Superscript minus Baseline upper A right parenthesis equals upper A Superscript minus Baseline upper A(A−A)(A−A) = A−A yields the useful fact that 
normal r normal a normal n normal k left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis equals m minus normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(I −A−A) = m −rank(A).
(3.229) 
This follows from Eqs. (3.227), (3.184), and (3.222), which yield 
0 greater than or equals normal r normal a normal n normal k left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper A right parenthesis minus m comma0 ≥rank(I −A−A) + rank(A) −m,
and from Eq. (3.143), which gives 
m equals normal r normal a normal n normal k left parenthesis upper I right parenthesis less than or equals normal r normal a normal n normal k left parenthesis upper I minus upper A Superscript minus Baseline upper A right parenthesis plus normal r normal a normal n normal k left parenthesis upper A right parenthesis periodm = rank(I) ≤rank(I −A−A) + rank(A).
The two inequalities result in the equality of Eq. (3.229). 
Properties of Generalized Inverses Useful in Analysis of Linear 
Models 
In the analysis of a linear statistical model of the form y equals upper X beta plus epsilony = Xβ + e, an  
important matrix is the Gramian upper X Superscript normal upper T Baseline upper XXTX. (See Sect. 3.4.11 for general properties 
of Gramian matrices.) If X is of full (column) rank, the inverse left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1(XTX)−1
plays a major role in the analysis (but not in the computations!). If X is not 
of full rank, then generalized inverses of upper X Superscript normal upper T Baseline upper XXTX play that role. We will make 
extensive use of the properties of the inverse or the generalized inverses of 
upper X Superscript normal upper T Baseline upper XXTX in Sect. 9.2 beginning on page 458. 
A generalized inverse of upper X Superscript normal upper T Baseline upper XXTX of course has all of the interesting properties 
discussed above, but left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus(XTX)−also has some additional useful properties. 
Consider left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T(XTX)−XT, for example. Applying Eq. (3.178) on page 137 to 
the basic expression upper X Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X equals upper X Superscript normal upper T Baseline upper XXTX(XTX)−XTX = XTX yields 
upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X equals upper X semicolonX(XTX)−XTX = X;
(3.230) 
that is, left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T(XTX)−XT is a generalized inverse of X. 
Multiplying both sides of Eq. (3.230) on the  right by  left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus(XTX)−and again 
using Eq. (3.178), we get 
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline semicolon(XTX)−XTX(XTX)−= (XTX)−;
(3.231) 
that is, any generalized inverse of upper X Superscript normal upper T Baseline upper XXTX is a reﬂexive, or g 12g12, inverse of upper X Superscript normal upper T Baseline upper XXTX. 
An expression of the form upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper TX(XTX)−XT occurs in the variance of certain 
least-squares estimators in linear models. An important property of these 
expressions is that they are invariant to the choice of the generalized inverse.

3.7 Generalized Inverses
151
To see this, let G be any generalized inverse of upper X Superscript normal upper T Baseline upper XXTX. Then from (3.230), we 
have 
upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X equals upper X upper G upper X Superscript normal upper T Baseline upper X commaX(XTX)−XTX = XGXTX,
which again as in the derivation of Eq. (3.230) yields 
upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline equals upper X upper G upper X Superscript normal upper T Baseline semicolonX(XTX)−XT = XGXT;
(3.232) 
that is, upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper TX(XTX)−XT is invariant to choice of generalized inverse of upper X Superscript normal upper T Baseline upper XXTX. 
3.7.2 The Moore–Penrose Inverse 
We sometimes impose additional conditions on a solution to a linear system, 
possibly to determine a unique solution with some desirable property. We do 
this by imposing additional requirements on the generalized inverse. 
If we impose two additional conditions on a g 12g12 inverse, we have a unique 
matrix, denoted by upper A Superscript plusA+, that yields a solution upper A Superscript plus Baseline bA+b that has the minimum 
length of any solution to the linear system upper A x equals bAx = b. We deﬁne this matrix and 
discuss some of its properties below, and in Sect. 5.6 we discuss properties of 
the solution upper A Superscript plus Baseline bA+b. 
Deﬁnitions and Terminology 
The Moore–Penrose inverse is a generalized inverse that has the four prop-
erties below. The Moore–Penrose inverse of the matrix A is denoted by upper A Superscript plusA+. 
The Moore–Penrose inverse is also called the g 1234g1234 or just g 4g4 inverse. 
upp er A upper A Superscript plus Baseline upper A equals upper AAA+A = A. 
upp er A Superscript plus Baseline upper A upper A Superscript plus Baseline equals upper A Superscript plusA+AA+ = A+. 
upp er A Superscript plus Baseline upper AA+A is symmetric. 
upp er A upper A Superscript plusAA+ is symmetric. 
The Moore–Penrose inverse is also called the pseudoinverse, the  p-inverse, and  
the normalized generalized inverse. (My current preferred term is “Moore– 
Penrose inverse,” but out of habit, I often use the term “pseudoinverse” for 
this special generalized inverse. I generally avoid using any of the other alter-
native terms introduced above. I use the term “generalized inverse” to mean 
the “general generalized inverse,” that is, the g 1g1 inverse.) The name Moore– 
Penrose derives from the preliminary work of Moore (1920) and  the more  
thorough later work of Penrose (1955), who laid out the conditions above and 
proved existence and uniqueness.

152
3 Basic Properties of Matrices
Existence 
We can see by construction that the Moore–Penrose inverse exists for any 
matrix A. First, if  upper A equals 0A = 0, note that  upper A Superscript plus Baseline equals 0A+ = 0. If  upper A not equals 0A /= 0, it has a full rank 
factorization, upper A equals upper L upper RA = LR, as in Eq. (3.166), so upper L Superscript normal upper T Baseline upper A upper R Superscript normal upper T Baseline equals upper L Superscript normal upper T Baseline upper L upper R upper R Superscript normal upper TLTART = LTLRRT. Because the 
n times rn × r matrix L is of full column rank and the r times mr × m matrix R is of full row 
rank, upper L Superscript normal upper T Baseline upper LLTL and upper R upper R Superscript normal upper TRRT are both of full rank, and hence upper L Superscript normal upper T Baseline upper L upper R upper R Superscript normal upper TLTLRRT is of full rank. 
Furthermore, upper L Superscript normal upper T Baseline upper A upper R Superscript normal upper T Baseline equals upper L Superscript normal upper T Baseline upper L upper R upper R Superscript normal upper TLTART = LTLRRT, so it is of full rank, and left parenthesis upper L Superscript normal upper T Baseline upper A upper R Superscript normal upper T Baseline right parenthesis Superscript negative 1(LTART)−1 exists. 
Now, form upper R Superscript normal upper T Baseline left parenthesis upper L Superscript normal upper T Baseline upper A upper R Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline upper L Superscript normal upper TRT(LTART)−1LT. By checking properties 1 through 4 above, we 
see that 
upper A Superscript plus Baseline equals upper R Superscript normal upper T Baseline left parenthesis upper L Superscript normal upper T Baseline upper A upper R Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline upper L Superscript normal upper TA+ = RT(LTART)−1LT
(3.233) 
is a Moore–Penrose inverse of A. This expression for the Moore–Penrose in-
verse based on a full rank decomposition of A is not as useful as another 
expression we will consider later, based on QR decomposition (Eq. (4.43) on  
page 241). 
Uniqueness 
We can see that the Moore–Penrose inverse is unique by considering any 
matrix G that satisﬁes the properties 1 through 4 for upper A not equals 0A /= 0. (The Moore–  
Penrose inverse of upper A equals 0A = 0 (that is, upper A Superscript plus Baseline equals 0A+ = 0) is clearly unique, as there could be 
no other matrix satisfying property 2.) By applying the properties and using 
upper A Superscript plusA+ given above, we have the following sequence of equations: 
StartLayout 1st Row upper G equals 2nd Row upper G upper A upper G equals left parenthesis upper G upper A right parenthesis Superscript normal upper T Baseline upper G equals upper A Superscript normal upper T Baseline upper G Superscript normal upper T Baseline upper G equals left parenthesis upper A upper A Superscript plus Baseline upper A right parenthesis Superscript normal upper T Baseline upper G Superscript normal upper T Baseline upper G equals left parenthesis upper A Superscript plus Baseline upper A right parenthesis Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper G Superscript normal upper T Baseline upper G equals 3rd Row upper A Superscript plus Baseline upper A upper A Superscript normal upper T Baseline upper G Superscript normal upper T Baseline upper G equals upper A Superscript plus Baseline upper A left parenthesis upper G upper A right parenthesis Superscript normal upper T Baseline upper G equals upper A Superscript plus Baseline upper A upper G upper A upper G equals upper A Superscript plus Baseline upper A upper G equals upper A Superscript plus Baseline upper A upper A Superscript plus Baseline upper A upper G equals 4th Row upper A Superscript plus Baseline left parenthesis upper A upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline left parenthesis upper A upper G right parenthesis Superscript normal upper T Baseline equals upper A Superscript plus Baseline left parenthesis upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper G Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals upper A Superscript plus Baseline left parenthesis upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline left parenthesis upper A upper G upper A right parenthesis Superscript normal upper T Baseline equals 5th Row upper A Superscript plus Baseline left parenthesis upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline upper A Superscript normal upper T Baseline equals upper A Superscript plus Baseline left parenthesis upper A upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline equals upper A Superscript plus Baseline upper A upper A Superscript plus Baseline 6th Row equals upper A Superscript plus Baseline period EndLayout
G =
GAG = (GA)TG = ATGTG = (AA+A)TGTG = (A+A)TATGTG =
A+AATGTG = A+A(GA)TG = A+AGAG = A+AG = A+AA+AG =
A+(AA+)T(AG)T = A+(A+)TATGTAT = A+(A+)T(AGA)T =
A+(A+)TAT = A+(AA+)T = A+AA+
= A+.
Other Properties 
Similarly to the property for inverses expressed in Eq. (3.161), we have 
left parenthesis upper A Superscript plus Baseline right parenthesis Superscript normal upper T Baseline equals left parenthesis upper A Superscript normal upper T Baseline right parenthesis Superscript plus Baseline period(A+)T = (AT)+.
(3.234) 
This is easily seen from the deﬁning properties of the Moore–Penrose inverse. 
If A is nonsingular, then obviously upper A Superscript plus Baseline equals upper A Superscript negative 1A+ = A−1, just as for any generalized 
inverse. 
Because upper A Superscript plusA+ is a generalized inverse, all of the properties for a generalized 
inverse upper A Superscript minusA−discussed above hold; in particular, upper A Superscript plus Baseline bA+b is a solution to the linear 
system upper A x equals bAx = b (see Eq. (3.208)). In Sect. 5.6, we will show that this unique 
solution has a kind of optimality.

3.7 Generalized Inverses
153
3.7.3 Generalized Inverses of Products and Sums of Matrices 
We often need to perform various operations on a matrix that is expressed 
as sums or products of various other matrices. Some operations are rather 
simple, for example, the transpose of the sum of two matrices is the sum of 
the transposes (Eq. (3.19)), and the transpose of the product is the product of 
the transposes in reverse order (Eq. (3.50)). Once we know the relationships 
for a single sum and a single product, we can extend those relationships to 
various sums and products of more than just two matrices. 
In Sect. 3.4.14, beginning on page 140, we gave a number  of  relationships  
between inverses of sums and/or products and sums and/or products of sums 
in Eqs. (3.187) through (3.196). Some of these carry over directly to general-
ized inverses; for example, analogous to Eqs. (3.187) and  (3.189), we have 
left parenthesis upper A upper B right parenthesis Superscript minus Baseline equals upper B Superscript minus Baseline upper A Superscript minus(AB)−= B−A−
and 
left parenthesis upper I plus upper A right parenthesis Superscript minus Baseline equals upper A Superscript minus Baseline left parenthesis upper I plus upper A Superscript minus Baseline right parenthesis Superscript minus Baseline period(I + A)−= A−(I + A−)−.
We often can relax the conditions on nonsingularity of A, B, upper I plus upper AI + A and so 
on, but because of the nonuniqueness of generalized inverses, in some cases 
we must interpret the equations as “holding for some generalized inverse.” 
With the relaxation on the nonsingularity of constituent matrices, 
Eqs. (3.190) through (3.196) do not necessarily hold for generalized inverses 
of general matrices, but some do. For example, Eq. (3.190), 
upper A left parenthesis upper I plus upper A right parenthesis Superscript minus Baseline equals left parenthesis upper I plus upper A Superscript minus Baseline right parenthesis Superscript minus Baseline periodA(I + A)−= (I + A−)−.
(Again, the true relationships are easily proven if taken in the order given on 
page 141, and in Exercise 3.20 you are asked to determine which are true for 
generalized inverses of general matrices and to prove that those are.) 
3.7.4 Generalized Inverses of Partitioned Matrices 
If A is partitioned as 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[
A11 A12
A21 A22
]
,
(3.235) 
then, similar to Eq. (3.204), a generalized inverse of A is given by 
upper A Superscript minus Baseline equals Start 3 By 2 Matrix 1st Row 1st Column upper A 11 Superscript minus Baseline plus upper A 11 Superscript minus Baseline upper A 12 upper Z Superscript minus Baseline upper A 21 upper A 11 Superscript minus Baseline 2nd Column minus upper A 11 Superscript minus Baseline upper A 12 upper Z Superscript minus Baseline 2nd Row 1st Column Blank 2nd Column Blank 3rd Row 1st Column minus upper Z Superscript minus Baseline upper A 21 upper A 11 Superscript minus Baseline 2nd Column upper Z Superscript minus Baseline EndMatrix commaA−=
⎡
⎣
A−
11 + A−
11A12Z−A21A−
11 −A−
11A12Z−
−Z−A21A−
11
Z−
⎤
⎦,
(3.236) 
where upper Z equals upper A 22 minus upper A 21 upper A 11 Superscript minus Baseline upper A 12Z = A22 −A21A−
11A12 (see Exercise 3.24, page 211). 
If the inverses on the right-hand side of Eq. (3.236) are Moore–Penrose 
inverses, then the result is the Moore–Penrose inverse of A.

154
3 Basic Properties of Matrices
If the partitioning in (3.235) happens to be such that upper A 11A11 is of full rank 
and of the same rank as A, a generalized inverse of A is given by 
upper A Superscript minus Baseline equals Start 3 By 2 Matrix 1st Row 1st Column upper A 11 Superscript negative 1 Baseline 2nd Column 0 2nd Row 1st Column Blank 2nd Column Blank 3rd Row 1st Column 0 2nd Column 0 EndMatrix commaA−=
⎡
⎣
A−1
11 0
0
0
⎤
⎦,
(3.237) 
where two or all three of the 0s may not be present, but if present 0 represents a 
zero matrix of the appropriate shape. The fact that it is a generalized inverse is 
easy to establish by using the deﬁnition of generalized inverse and Eq. (3.203). 
3.8 Orthogonality 
In Sect. 2.1.8, we deﬁned orthogonality and orthonormality of two or more 
vectors in terms of inner products. On page 120, in Eq. (3.126), we also de-
ﬁned the orthogonal binary relationship between two matrices in terms of 
inner products of matrices. Now we deﬁne the orthogonal unary property of a 
matrix. This is the more important property and is what is commonly meant 
when we speak of orthogonality of matrices. We use the orthonormality prop-
erty of vectors, which is a binary relationship, to deﬁne orthogonality of a 
single matrix. 
3.8.1 Orthogonal Matrices: Deﬁnition and Simple Properties 
A matrix whose rows or columns constitute a set of orthonormal vectors is 
said to be an orthogonal matrix. If Q is an n times mn × m orthogonal matrix, then 
upper Q upper Q Superscript normal upper T Baseline equals upper I Subscript nQQT = In if n less than or equals mn ≤m, and  upper Q Superscript normal upper T Baseline upper Q equals upper I Subscript mQTQ = Im if n greater than or equals mn ≥m. 
If Q is a square orthogonal matrix, then upper Q upper Q Superscript normal upper T Baseline equals upper Q Superscript normal upper T Baseline upper Q equals upper IQQT = QTQ = I. The deﬁnition 
of orthogonality is sometimes made more restrictive to require that the matrix 
be square. 
For matrices, orthogonality is both a type of binary relationship and a 
unary property. The unary property of orthogonality is deﬁned in terms of a 
transpose. 
The determinant of a square orthogonal matrix is plus or minus 1±1 (because the deter-
minant of the product is the product of the determinants and the determinant 
of I is 1). 
When n greater than or equals mn ≥m, the matrix inner product of an n times mn × m orthogonal matrix Q 
with itself is its number of columns: 
left angle bracket upper Q comma upper Q right angle bracket equals m period<Q, Q> = m.
(3.238) 
This is because upper Q Superscript normal upper T Baseline upper Q equals upper I Subscript mQTQ = Im. If  n less than or equals mn ≤m, the matrix inner product of Q with 
itself is its number of rows. 
Recalling the deﬁnition of the orthogonal binary relationship from page 120, 
we note that if Q is an orthogonal matrix, then Q is not orthogonal to itself 
in that sense.

3.8 Orthogonality
155
A permutation matrix (see page 103) is orthogonal. We can see this by 
building the permutation matrix as a product of elementary permutation ma-
trices, and it is easy to see that they are all orthogonal. 
One further property we see by simple multiplication is that if A and B 
are orthogonal, then upper A circled times upper BA ⊗B is orthogonal. 
3.8.2 Unitary Matrices 
A square matrix is said to be unitary if the matrix times its conjugate trans-
pose is the identity; that is, if upper U upper U Superscript normal upper H Baseline equals upper U Superscript normal upper H Baseline upper U equals upper IUU H = U HU = I. An orthogonal matrix is also 
a unitary matrix if only if it is square and real. 
Transformations using unitary matrices are analogous in many ways to 
transformations using orthogonal matrices, but there are important diﬀer-
ences. 
The deﬁnition of orthogonality of vectors is the same for complex vectors as 
it is for real vectors; in both cases, it is that the inner product is 0. Because of 
our emphasis on real vectors and matrices, we often think of orthogonality of 
vectors in terms of x Superscript normal upper T Baseline yxTy, but this only applies to real vectors. In general, x and 
y are orthogonal if x Superscript normal upper H Baseline y equals 0xHy = 0, which is the inner product. The corresponding 
binary relationship of orthogonality for matrices, as deﬁned in Eq. (3.126) on  
page 120, likewise depends on an inner product, which is given in Eq. (3.121). 
The relationship in Eq. (3.122) may not be correct if the elements are not real. 
3.8.3 Orthogonal and Orthonormal Columns 
The deﬁnition given above for orthogonal matrices is sometimes relaxed to re-
quire only that the columns or rows be orthogonal (rather than orthonormal). 
If orthonormality is not required, the determinant is not necessarily plus or minus 1±1. If  Q 
is a matrix that is “orthogonal” in this weaker sense of the deﬁnition, and Q 
has more rows than columns, then 
upper Q Superscript normal upper T Baseline upper Q equals Start 4 By 4 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace 0 3rd Column midline horizontal ellipsis 4th Column monospace 0 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column midline horizontal ellipsis 4th Column monospace 0 3rd Row 1st Column Blank 2nd Column down right diagonal ellipsis 3rd Column Blank 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column midline horizontal ellipsis 4th Column monospace upper X EndMatrix periodQTQ =
⎡
⎢⎢⎢⎣
X 0 · · · 0
0 X · · · 0
...
0 0 · · · X
⎤
⎥⎥⎥⎦.
Unless stated otherwise, I use the term “orthogonal matrix” to refer to a 
matrix whose columns are orthonormal; that is, for which upper Q Superscript normal upper T Baseline upper Q equals upper IQTQ = I. 
3.8.4 The Orthogonal Group 
The set of n times mn×m orthogonal matrices for which n greater than or equals mn ≥m is called an left parenthesis n comma m right parenthesis(n, m) Stiefel 
manifold, and an left parenthesis n comma n right parenthesis(n, n) Stiefel manifold together with Cayley multiplication 
is a group, sometimes called the orthogonal group and denoted as script upper O left parenthesis n right parenthesisO(n). The

156
3 Basic Properties of Matrices
orthogonal group script upper O left parenthesis n right parenthesisO(n) is a subgroup of the general linear group script upper G upper L left parenthesis n right parenthesisGL(n), deﬁned 
on page 134. The orthogonal group is useful in multivariate analysis because of 
the invariance of the so-called Haar measure over this group (see Sect. 7.3.1). 
Because the Euclidean norm of any column of an n times mn×m orthogonal matrix 
with n greater than or equals mn ≥m is 1, no element in the matrix can be greater than 1 in absolute 
value. We therefore have an analog of the Bolzano–Weierstrass theorem for 
sequences of orthogonal matrices. The standard Bolzano–Weierstrass theorem 
for real numbers states that if a sequence a Subscript iai is bounded, then there exists a 
subsequence a Subscript i Sub Subscript jaij that converges. (See any text on real analysis.) From this, we 
conclude that if upper Q 1 comma upper Q 2 comma ellipsisQ1, Q2, . . . is a sequence of n times mn × m (with n greater than or equals mn ≥m) orthogonal 
matrices, then there exists a subsequence upper Q Subscript i 1 Baseline comma upper Q Subscript i 2 Baseline comma ellipsisQi1, Qi2, . . ., such that 
limit Underscript j right arrow normal infinity Endscripts upper Q Subscript i Sub Subscript j Subscript Baseline equals upper Q comma lim
j→∞Qij = Q,
(3.239) 
where Q is some ﬁxed matrix. The limiting matrix Q must also be orthogonal 
because upper Q Subscript i Sub Subscript j Superscript normal upper T Baseline upper Q Subscript i Sub Subscript j Baseline equals upper IQT
ijQij = I, and so, taking limits, we have upper Q Superscript normal upper T Baseline upper Q equals upper IQTQ = I. The  set of  n times mn×m
orthogonal matrices is therefore compact. 
3.8.5 Conjugacy 
Instead of deﬁning orthogonality of vectors in terms of dot products as in 
Sect. 2.1.8, we could deﬁne it more generally in terms of a bilinear form as in 
Sect. 3.3.8. If the bilinear form x Superscript normal upper T Baseline upper A y equals 0xTAy = 0, we say  x and y are orthogonal with 
respect to the matrix A. We also often use a diﬀerent term and say that the 
vectors are conjugate with respect to A, as in Eq. (3.106). The usual deﬁnition 
of orthogonality in terms of a dot product is equivalent to the deﬁnition in 
terms of a bilinear form in the identity matrix. 
Likewise, but less often, orthogonality of matrices is generalized to conju-
gacy of two matrices with respect to a third matrix: upper Q Superscript normal upper T Baseline upper A upper Q equals upper IQTAQ = I. 
3.9 Eigenanalysis: Canonical Factorizations 
Throughout this section on eigenanalysis, we will generally implicitly assume 
that the matrices we discuss are square, unless we state otherwise. 
Multiplication of a given vector by a square matrix may result in a scalar 
multiple of the vector. Stating this more formally, and giving names to such 
a special vector and scalar, if A is an n × n (square) matrix, v is a vector not 
equal to 0, and c is a scalar such that 
upper A v equals c v commaAv = cv,
(3.240) 
we say v is an eigenvector of the matrix A, and  c is an eigenvalue of the matrix 
A. We refer to the pair c and v as an associated eigenvector and eigenvalue 
or as an eigenpair.

3.9 Eigenanalysis: Canonical Factorizations
157
We immediately note that if v is an eigenvector of A, then for any scalar, 
b, because A(bv) =  c(bv), bv is also an eigenvector of A. (We will exclude the 
case b = 0, so that we do not consider the 0 vector to be an eigenvector of A.) 
That is, any vector in the double cone generated by an eigenvector, except 
the 0 vector, is an eigenvector (see discussion of cones, beginning on page 53). 
While we restrict an eigenvector to be nonzero (or else we would have 0 as 
an eigenvector associated with any number being an eigenvalue), an eigenvalue 
can be 0; in that case, of course, the matrix must be singular. (Some authors 
restrict the deﬁnition of an eigenvalue to real values that satisfy (3.240), and 
there is an important class of matrices for which it is known that all eigenvalues 
are real. In this book, we do not want to restrict ourselves to that class; hence, 
we do not require c or v in Eq. (3.240) to be real.)  
We use the term “eigenanalysis” or “eigenproblem” to refer to the gen-
eral theory, applications, or computations related to either eigenvectors or 
eigenvalues. 
There are various other terms used for eigenvalues and eigenvectors. An 
eigenvalue is also called a characteristic value (that is why I use a “c” to  
represent an eigenvalue), a latent root (that is why I also might use a “λ” to  
represent an eigenvalue), or a proper value, and similar synonyms exist for an 
eigenvector. An eigenvalue is also sometimes called a singular value, but the 
latter term has a diﬀerent meaning that we will use in this book (see page 183; 
the absolute value of an eigenvalue is a singular value, and singular values are 
also deﬁned for nonsquare matrices). 
Although generally throughout this chapter we have assumed that vectors 
and matrices are real, in eigenanalysis, even if A is real, it may be the case 
that c and v are complex. Therefore, in this section, we must be careful about 
the nature of the eigenpairs, even though we will continue to assume that the 
basic matrices are real. 
3.9.1 Eigenvalues and Eigenvectors Are Remarkable 
Before proceeding to consider properties of eigenvalues and eigenvectors, we 
should note how remarkable the relationship upper A v equals c vAv = cv is. 
The eﬀect of a matrix multiplication of an eigenvector is the same as a 
scalar multiplication of the eigenvector. 
The eigenvector is an invariant of the transformation in the sense that its 
direction does not change. This would seem to indicate that the eigenvalue 
and eigenvector depend on some kind of deep properties of the matrix, and 
indeed this is the case, as we will see. 
Of course, the ﬁrst question is, for a given matrix, do such special vectors 
and scalars exist? 
The answer is yes. 
The next question is, for a given matrix, what is the formula for the eigen-
values (or what is a ﬁnite sequence of steps to compute the eigenvalues)?

158
3 Basic Properties of Matrices
The answer is that a formula does not exist, and there is no ﬁnite 
sequence of steps, in general, for determining the eigenvalues (if the 
matrix is bigger than 4 times 44 × 4). 
Before considering these and other more complicated issues, we will state 
some simple properties of any scalar and vector that satisfy upper A v equals c vAv = cv and intro-
duce some additional terminology. In Sect. 3.9.3, we will show that eigenvalues 
and eigen vectors mus exist for a square matrix. 
3.9.2 Basic Properties of Eigenvalues and Eigenvectors 
If c is an eigenvalue and v is a corresponding eigenvector for a real matrix 
A, we see immediately from the deﬁnition of eigenvector and eigenvalue in 
Eq. (3.240) the following properties. (In Exercise 3.25, you are asked to supply 
the simple proofs for these properties, or you can see the proofs in a text such 
as Harville, 1997, for example.) 
Assume that upper A v equals c vAv = cv and that all elements of A are real. 
1. bv is an eigenvector of A, where  b is any nonzero scalar. 
It is often desirable to scale an eigenvector v so that v Superscript normal upper T Baseline v equals 1vTv = 1. Such 
an eigenvector is also called a “unit eigenvector,” but I prefer the term 
“normalized eigenvector” because of the use of the phrase “unit vector” 
to refer to the special vectors e Subscript iei. 
For a given eigenvector, there is always a particular eigenvalue associated 
with it, but for a given eigenvalue there is a space of associated eigen-
vectors. (The space is a vector space if we consider the zero vector to 
be a member.) It is therefore not appropriate to speak of “the” eigen-
vector associated with a given eigenvalue — although we do use this term 
occasionally. (We could interpret it as referring to the normalized eigen-
vector.) There is, however, another sense in which an eigenvalue does not 
determine a unique eigenvector, as we discuss below. 
2. bc is an eigenvalue of bA, where  b is any nonzero scalar. 
1 d ivided by c1/c and v are an eigenpair of upper A Superscript negative 1A−1 (if A is nonsingular). 
1 d ivided by c1/c and v are an eigenpair of upper A Superscript plusA+ if A (and hence upper A Superscript plusA+) is symmetric (and 
square) and c is nonzero. 
5. If A is diagonal or triangular with elements a Subscript i iaii, the eigenvalues are a Subscript i iaii, and  
for diagonal A the corresponding eigenvectors are e Subscript iei (the unit vectors). 
c s quaredc2 and v are an eigenpair of upper A squaredA2. More generally, c Superscript kck and v are an eigenpair 
of upper A Superscript kAk for k equals 1 comma 2 comma ellipsisk = 1, 2, . . .. 
c m inus dc −d and v are an eigenpair of upper A minus d upper IA −dI. This obvious fact is useful in 
computing eigenvalues (see Sect. 6.1.5). 
8. If A and B are conformable for the multiplications AB and BA, the  
nonzero eigenvalues of AB are the same as the nonzero eigenvalues of 
BA. (Note that A and B are not necessarily square.) All of eigenvalues 
are the same if A and B are square. (Note, however, that if A and B are

3.9 Eigenanalysis: Canonical Factorizations
159
square and d is an eigenvalue of B, d is not necessarily an eigenvalue of 
AB.) 
9. If A and B are square and of the same order and if upper B Superscript negative 1B−1 exists, then the 
eigenvalues of upper B upper A upper B Superscript negative 1BAB−1 are the same as the eigenvalues of A. (This is called 
a similarity transformation; see page 168.) 
List continued on page 162. 
Eigenvalues of Elementary Operator Matrices 
For a matrix with a very simple pattern, such as a diagonal matrix, whose 
determinant is just the product of the elements, we can determine the eigen-
values by inspection. For example, it is clear immediately that all eigenvalues 
of the identity matrix are 1s. (Although they are all the same, we still say 
there are n of them, if n is the order of the identity matrix. Multiplicity 
of eigenvalues is an important property, which we will discuss beginning on 
page 166.) 
Because of their simple patterns, we can also easily determine the eigen-
values of elementary operator matrices, possibly by considering one or two 
adjugates that arise from submatrices that are identity matrices. 
The eigenvalues of the 2 times 22 × 2 permutation 
Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 1 2nd Row 1st Column 1 2nd Column 0 EndMatrix
[
0 1
1 0
]
are just the two square roots of 1; that is, 1 and negative 1−1. From this, using partitions 
of an elementary permutation matrix upper E Subscript p qEpq of order n to form adjugates have 
1s on the skew diagonals and 0s elsewhere, we see that the eigenvalues of an 
elementary permutation matrix upper E Subscript p qEpq are n minus 1n −1 1s and one negative 1−1. 
With a little more eﬀort we can determine the eigenvalues of general per-
mutation matrices. Following the preceding approach, we immediately see that 
the eigenvalues of the matrix 
Start 3 By 3 Matrix 1st Row 1st Column 0 2nd Column 0 3rd Column 1 2nd Row 1st Column 0 2nd Column 1 3rd Column 0 3rd Row 1st Column 1 2nd Column 0 3rd Column 0 EndMatrix
⎡
⎣
0 0 1
0 1 0
1 0 0
⎤
⎦
are the three cube roots of 1, two of which contain imaginary components. In 
Chap. 8, on page 425, we describe the full set of eigenvalues for a permutation 
matrix in which all rows are moved. 
By inspection of the determinant, we see that the eigenvalues of an el-
ementary row-multiplication matrix upper E Subscript p Baseline left parenthesis a right parenthesisEp(a) of order n are n minus 1n −1 1s and one 
a. 
Again by inspection of the determinant, we see that the eigenvalues of an 
elementary axpy matrix upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a) of order n are n 1s, the same as the identity 
itself.

160
3 Basic Properties of Matrices
Left Eigenvectors 
In the following, when we speak of an eigenvector or eigenpair without qual-
iﬁcation, we will mean the objects deﬁned by Eq. (3.240). There is another 
type of eigenvector for A, however, a left eigenvector, deﬁned as a nonzero w 
in 
w Superscript normal upper T Baseline upper A equals c w Superscript normal upper T Baseline periodwTA = cwT.
(3.241) 
For emphasis, we sometimes refer to the eigenvector of Eq. (3.240), upper A v equals c vAv = cv, 
as a right eigenvector. 
We see from the deﬁnition of a left eigenvector, that if a matrix is sym-
metric, each left eigenvector is an eigenvector (a right eigenvector). 
If v is an eigenvector of A and w is a left eigenvector of A with a diﬀerent 
associated eigenvalue, then v and w are orthogonal; that is, if upper A v equals c 1 vAv = c1v, 
w Superscript normal upper T Baseline upper A equals c 2 w Superscript normal upper TwTA = c2wT, and  c 1 not equals c 2c1 /= c2, then  w Superscript normal upper T Baseline v equals 0wTv = 0. We see this by multiplying both 
sides of w Superscript normal upper T Baseline upper A equals c 2 w Superscript normal upper TwTA = c2wT by v to get w Superscript normal upper T Baseline upper A v equals c 2 w Superscript normal upper T Baseline vwTAv = c2wTv and multiplying both sides 
of upper A v equals c 1 vAv = c1v by w Superscript normal upper TwT to get w Superscript normal upper T Baseline upper A v equals c 1 w Superscript normal upper T Baseline vwTAv = c1wTv. Hence, we have c 1 w Superscript normal upper T Baseline v equals c 2 w Superscript normal upper T Baseline vc1wTv = c2wTv, 
and because c 1 not equals c 2c1 /= c2, we have  w Superscript normal upper T Baseline v equals 0wTv = 0. 
3.9.3 The Characteristic Polynomial 
From the equation left parenthesis upper A minus c upper I right parenthesis v equals 0(A −cI)v = 0 that deﬁnes eigenvalues and eigenvectors, 
we see that in order for v to be nonnull, left parenthesis upper A minus c upper I right parenthesis(A −cI) must be singular, and hence 
normal d normal e normal t left parenthesis upper A minus c upper I right parenthesis equals normal d normal e normal t left parenthesis c upper I minus upper A right parenthesis equals 0 perioddet(A −cI) = det(cI −A) = 0.
(3.242) 
Equation (3.242) is sometimes taken as the deﬁnition of an eigenvalue c. It is  
deﬁnitely a fundamental relation and, as we will see, allows us to identify a 
number of useful properties. 
For the n times nn × n matrix A, the determinant in Eq. (3.242) is a polynomial 
of degree n in c, p Subscript upper A Baseline left parenthesis c right parenthesispA(c), called the characteristic polynomial, and when it is 
equated to 0, it is called the characteristic equation: 
p Subscript upper A Baseline left parenthesis c right parenthesis equals s 0 plus s 1 c plus midline horizontal ellipsis plus s Subscript n Baseline c Superscript n Baseline equals 0 periodpA(c) = s0 + s1c + · · · + sncn = 0.
(3.243) 
From the expansion of the determinant normal d normal e normal t left parenthesis c upper I minus upper A right parenthesisdet(cI −A), as in Eq. (3.46) on  
page 93, we see that s 0 equals left parenthesis negative 1 right parenthesis Superscript n Baseline normal d normal e normal t left parenthesis upper A right parenthesiss0 = (−1)ndet(A) and s Subscript n Baseline equals 1sn = 1, and, in general, s Subscript k Baseline equals left parenthesis negative 1 right parenthesis Superscript n minus ksk =
(−1)n−k times the sums of all principal minors of A of order n minus kn−k. (Note that 
the signs of the s Subscript isi are diﬀerent depending on whether we use normal d normal e normal t left parenthesis c upper I minus upper A right parenthesisdet(cI −A) or 
normal d normal e normal t left parenthesis upper A minus c upper I right parenthesisdet(A −cI).) 
How Many Eigenvalues Does a Matrix Have? 
From above, we see that an eigenvalue of A is a root of the characteristic 
polynomial, and a root of the characteristic polynomial is an eigenvalue. The

3.9 Eigenanalysis: Canonical Factorizations
161
existence of n roots of the polynomial (by the Fundamental Theorem of Al-
gebra) allows the characteristic polynomial to be written in factored form 
as 
p Subscript upper A Baseline left parenthesis c right parenthesis equals left parenthesis negative 1 right parenthesis Superscript n Baseline left parenthesis c minus c 1 right parenthesis midline horizontal ellipsis left parenthesis c minus c Subscript n Baseline right parenthesis commapA(c) = (−1)n(c −c1) · · · (c −cn),
(3.244) 
and establishes the existence of n eigenvalues. Some may be complex, some 
may be zero, and some may be equal to others. We call the set of all eigen-
values the spectrum of the matrix. The “number of eigenvalues” must be 
distinguished from the cardinality of the spectrum, which is the number of 
unique values. 
A real matrix may have complex eigenvalues (and, hence, eigenvectors), 
just as a polynomial with real coeﬃcients can have complex roots. Clearly, the 
eigenvalues of a real matrix must occur in conjugate pairs just as in the case of 
roots of polynomials with real coeﬃcients. (As mentioned above, some authors 
restrict the deﬁnition of an eigenvalue to real values that satisfy (3.240). We 
will see below that the eigenvalues of a real symmetric matrix are always real, 
and this is a case that we will emphasize, but in this book we do not restrict 
the deﬁnition.) 
Properties of the Characteristic Polynomial 
The characteristic polynomial has many interesting properties. One, stated in 
the Cayley-Hamilton theorem, is that the matrix itself is a root of the matrix 
polynomial formed by the characteristic polynomial; that is, 
p Subscript upper A Baseline left parenthesis upper A right parenthesis equals s 0 upper I plus s 1 upper A plus midline horizontal ellipsis plus s Subscript n Baseline upper A Superscript n Baseline equals 0 Subscript n Baseline periodpA(A) = s0I + s1A + · · · + snAn = 0n.
(3.245) 
We see this by using Eq. (3.98) to write the matrix in Eq. (3.242) as  
left parenthesis upper A minus c upper I right parenthesis normal a normal d normal j left parenthesis upper A minus c upper I right parenthesis equals p Subscript upper A Baseline left parenthesis c right parenthesis upper I period(A −cI)adj(A −cI) = pA(c)I.
(3.246) 
Hence, normal a normal d normal j left parenthesis upper A minus c upper I right parenthesisadj(A −cI) is a polynomial in c of degree less than or equal to n minus 1n −1, 
so we can write it as 
normal a normal d normal j left parenthesis upper A minus c upper I right parenthesis equals upper B 0 plus upper B 1 c plus midline horizontal ellipsis plus upper B Subscript n minus 1 Baseline c Superscript n minus 1 Baseline commaadj(A −cI) = B0 + B1c + · · · + Bn−1cn−1,
where the upper B Subscript iBi are n times nn × n matrices. Now, equating the coeﬃcients of c on the 
two sides of Eq. (3.246), we have 
StartLayout 1st Row 1st Column upper A upper B 0 2nd Column equals 3rd Column s 0 upper I 2nd Row 1st Column upper A upper B 1 minus upper B 0 2nd Column equals 3rd Column s 1 upper I 3rd Row 1st Column Blank 2nd Column vertical ellipsis 3rd Column Blank 4th Row 1st Column upper A upper B Subscript n minus 1 minus upper B Subscript n minus 2 2nd Column equals 3rd Column s Subscript n minus 1 Baseline upper I 5th Row 1st Column upper B Subscript n minus 1 2nd Column equals 3rd Column s Subscript n Baseline upper I period EndLayoutAB0 = s0I
AB1 −B0 = s1I
...
ABn−1 −Bn−2 = sn−1I
Bn−1 = snI.

162
3 Basic Properties of Matrices
Now, multiply the second equation by A, the third equation by upper A squaredA2, and  the  i normal t normal hith
equation by upper A Superscript i minus 1Ai−1, and add all equations. We get the desired result: p Subscript upper A Baseline left parenthesis upper A right parenthesis equals 0pA(A) = 0. 
See also Exercise  3.26. 
Another interesting fact is that any given n normal t normal hnth-degree polynomial, p, is the  
characteristic polynomial of an n times nn × n matrix, A, of particularly simple form. 
Consider the polynomial 
p left parenthesis c right parenthesis equals s 0 plus s 1 c plus midline horizontal ellipsis plus s Subscript n minus 1 Baseline c Superscript n minus 1 Baseline plus c Superscript np(c) = s0 + s1c + · · · + sn−1cn−1 + cn
and the matrix 
upper A equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 0 2nd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column midline horizontal ellipsis 5th Column 0 3rd Row 1st Column Blank 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 1 5th Row 1st Column minus s 0 2nd Column minus s 1 3rd Column minus s 2 4th Column midline horizontal ellipsis 5th Column minus s Subscript n minus 1 Baseline EndMatrix periodA =
⎡
⎢⎢⎢⎢⎢⎣
0
1
0
· · ·
0
0
0
1
· · ·
0
...
0
0
0
· · ·
1
−s0 −s1 −s2 · · · −sn−1
⎤
⎥⎥⎥⎥⎥⎦
.
(3.247) 
The matrix A is called the companion matrix of the polynomial p, and it is 
easy to see (by a tedious expansion) that the characteristic polynomial of A 
is p. This, of course, shows that a characteristic polynomial does not uniquely 
determine a matrix, although the converse is true (within signs). 
Additional Properties of Eigenvalues and Eigenvectors 
Using the characteristic polynomial yields the following properties. This is a 
continuation of the list we began on page 158. We assume A is a real matrix 
with eigenpair left parenthesis c comma v right parenthesis(c, v). 
10. c is an eigenvalue of upper A Superscript normal upper TAT (because normal d normal e normal t left parenthesis upper A Superscript normal upper T Baseline minus c upper I right parenthesis equals normal d normal e normal t left parenthesis upper A minus c upper I right parenthesisdet(AT−cI) = det(A−cI) for any c). The 
eigenvectors of upper A Superscript normal upper TAT, which are left eigenvectors of A, are not necessarily 
the same as the eigenvectors of A, however. 
11. There is a left eigenvector such that c is the associated eigenvalue. 
left  parenthesis c overbar comma v overbar right parenthesis(¯c, ¯v) is an eigenpair of A, where  c overbar¯c and v overbar¯v are the complex conjugates 
and A, as usual, consists of real elements.  (If  c and v are real, this is a 
tautology.) 
13. If A is symmetric or if A is triangular, then c is real. (The elements of A 
are assumed to be real,  of  course.)  
c c overbarc¯c is an eigenvalue of the Gramian matrix upper A Superscript normal upper T Baseline upper AATA, and, since upper A Superscript normal upper T Baseline upper AATA is sym-
metric, c c overbarc¯c is real and nonnegative (as is any x x overbarx¯x that is real). 
15. The nonzero eigenvalues of the Gramian matrix upper A Superscript normal upper T Baseline upper AATA are the same as 
the nonzero eigenvalues of the Gramian matrix upper A upper A Superscript normal upper TAAT. (This is actually 
property 8 on page 158.) 
In Exercise 3.27, you are asked to supply the simple proofs for these properties, 
or you can see the proofs in a text such as Harville (1997), for example. 
A further comment on property 12 may be worthwhile. Throughout this 
book, we assume we begin with real numbers. There are some times, however,

3.9 Eigenanalysis: Canonical Factorizations
163
when standard operations in the real domain carry us outside the reals. The 
simplest situations, which of course are related, are roots of polynomial equa-
tions with real coeﬃcients and eigenpairs of matrices with real elements. In 
both of these situations, because sums must be real, the complex values occur 
in conjugate pairs. 
There are many additional interesting properties of eigenvalues and eigen-
vectors that we will encounter in later sections, but there is one more that I 
want to list here with these very basic and important properties: 
Star tAbsoluteValue c EndAbsoluteValue less than or equals parallel to upper A parallel to|c| ≤||A||, where  parallel to dot parallel to|| · || is any consistent matrix norm. 
(We will discuss matrix norms in Sect. 3.11 beginning on page 187, and  this  
particular bound is given in Eq. (3.340) in that section. In my deﬁnition of 
matrix norm, all norms are required to be consistent.) 
Eigenvalues and the Trace and the Determinant 
If the eigenvalues of the matrix A are c 1 comma ellipsis comma c Subscript n Baselinec1, . . . , cn, because they are the roots 
of the characteristic polynomial, we can readily form that polynomial as 
StartLayout 1st Row 1st Column p Subscript upper A Baseline left parenthesis c right parenthesis 2nd Column equals 3rd Column left parenthesis c minus c 1 right parenthesis midline horizontal ellipsis left parenthesis c minus c Subscript n Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis negative 1 right parenthesis Superscript n Baseline product c Subscript i Baseline plus midline horizontal ellipsis plus left parenthesis negative 1 right parenthesis Superscript 1 Baseline sigma summation c Subscript i Baseline c Superscript n minus 1 Baseline plus c Superscript n Baseline period EndLayoutpA(c) = (c −c1) · · · (c −cn)
= (−1)n ||
ci + · · · + (−1)1 E
cicn−1 + cn.
(3.248) 
Because this is the same polynomial as obtained by the expansion of the 
determinant in Eq. (3.243), the coeﬃcients must be equal. In particular, by 
simply equating the corresponding coeﬃcients of the constant terms and left parenthesis n minus 1 right parenthesis normal t normal h(n−
1)th-degree terms, we have the two very important facts: 
normal d normal e normal t left parenthesis upper A right parenthesis equals product c Subscript idet(A) =
||
ci
(3.249) 
and 
normal t normal r left parenthesis upper A right parenthesis equals sigma summation c Subscript i Baseline periodtr(A) =
E
ci.
(3.250) 
It might be worth recalling that we have assumed that A is real, and 
therefore, normal d normal e normal t left parenthesis upper A right parenthesisdet(A) and normal t normal r left parenthesis upper A right parenthesistr(A) are real, but the eigenvalues c Subscript ici may not be real. 
Nonreal eigenvalues, however, occur in conjugate pairs (property 12 above); 
hence product c Subscript i|| ci and sigma summation c Subscript iE ci are real even though the individual elements may not be. 
3.9.4 The Spectrum 
Although, for an n times nn × n matrix, from the characteristic polynomial we have 
n roots, and hence n eigenvalues, some of these roots may be the same. It 
may also be the case that more than one eigenvector corresponds to a given 
eigenvalue. As we mentioned above, the set of all the distinct eigenvalues of a 
matrix is called the spectrum of the matrix.

164
3 Basic Properties of Matrices
Each eigenvalue has a corresponding eigenvector, of course, so by arranging 
the spectrum into a diagonal matrix and the eigenvectors into the columns 
of a matrix, we form a matrix equation corresponding to the deﬁnition of 
eigenvalues and eigenvectors (3.240). We usually arrange the eigenvalues so 
that StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue greater than or equals StartAbsoluteValue c Subscript i plus 1 Baseline EndAbsoluteValue|ci| ≥|ci+1|. 
upper A upper V equals normal d normal i normal a normal g left parenthesis c 1 comma ellipsis comma c Subscript n Baseline right parenthesis upper V periodAV = diag(c1, . . . , cn)V.
(3.251) 
Notation 
Sometimes it is convenient to refer to the distinct eigenvalues and sometimes 
we wish to refer to all eigenvalues, as in referring to the number of roots of the 
characteristic polynomial. To refer to the distinct eigenvalues in a way that 
allows us to be consistent in the subscripts, we will call the distinct eigenvalues 
lamda 1 comma ellipsis comma lamda Subscript k Baselineλ1, . . . , λk. The set of these constitutes the spectrum. 
We denote the spectrum of the matrix A by sigma left parenthesis upper A right parenthesisσ(A): 
sigma left parenthesis upper A right parenthesis equals StartSet lamda 1 comma ellipsis comma lamda Subscript k Baseline EndSet periodσ(A) = {λ1, . . . , λk}.
(3.252) 
We see immediately that sigma left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals sigma left parenthesis upper A right parenthesisσ(AT) = σ(A) (property 10 above). 
In terms of the spectrum, Eq. (3.244) becomes 
p Subscript upper A Baseline left parenthesis c right parenthesis equals left parenthesis negative 1 right parenthesis Superscript n Baseline left parenthesis c minus lamda 1 right parenthesis Superscript m 1 Baseline midline horizontal ellipsis left parenthesis c minus lamda Subscript k Baseline right parenthesis Superscript m Super Subscript k Superscript Baseline commapA(c) = (−1)n(c −λ1)m1 · · · (c −λk)mk,
(3.253) 
for m Subscript i Baseline greater than or equals 1mi ≥1. 
We label the c Subscript ici and v Subscript ivi so that 
StartAbsoluteValue c 1 EndAbsoluteValue greater than or equals midline horizontal ellipsis greater than or equals StartAbsoluteValue c Subscript n Baseline EndAbsoluteValue period|c1| ≥· · · ≥|cn|.
(3.254) 
We likewise label the lamda Subscript iλi so that 
StartAbsoluteValue lamda 1 EndAbsoluteValue greater than midline horizontal ellipsis greater than StartAbsoluteValue lamda Subscript k Baseline EndAbsoluteValue period|λ1| > · · · > |λk|.
(3.255) 
With this notation, we have 
StartAbsoluteValue lamda 1 EndAbsoluteValue equals StartAbsoluteValue c 1 EndAbsoluteValue|λ1| = |c1|
and 
StartAbsoluteValue lamda Subscript k Baseline EndAbsoluteValue equals StartAbsoluteValue c Subscript n Baseline EndAbsoluteValue comma|λk| = |cn|,
but we cannot say anything about the other lamdaλs and  cs. 
The Spectral Radius 
For the matrix A with these eigenvalues, StartAbsoluteValue c 1 EndAbsoluteValue|c1| is called the spectral radius and 
is denoted by rho left parenthesis upper A right parenthesisρ(A): 
rho left parenthesis upper A right parenthesis equals max StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue equals StartAbsoluteValue c 1 EndAbsoluteValue equals StartAbsoluteValue lamda 1 EndAbsoluteValue periodρ(A) = max |ci| = |c1| = |λ1|.
(3.256) 
We immediately note that rho left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals rho left parenthesis upper A right parenthesisρ(AT) = ρ(A).

3.9 Eigenanalysis: Canonical Factorizations
165
The set of complex numbers 
StartSet z colon StartAbsoluteValue z EndAbsoluteValue equals rho left parenthesis upper A right parenthesis EndSet{z : |z| = ρ(A)}
(3.257) 
is called the spectral circle of A. 
An eigenvalue equal to plus or minus max StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue± max |ci| (that  is, equal to  plus or minus c 1±c1) is called a domi-
nant eigenvalue. We are more often interested in the absolute value (or mod-
ulus) of a dominant eigenvalue rather than the eigenvalue itself; that is, rho left parenthesis upper A right parenthesisρ(A)
(or StartAbsoluteValue c 1 EndAbsoluteValue|c1|) is more often of interest than c 1c1. 
Interestingly, we have for all i 
StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue less than or equals max Underscript j Endscripts sigma summation Underscript k Endscripts StartAbsoluteValue a Subscript k j Baseline EndAbsoluteValue|ci| ≤max
j
E
k
|akj|
(3.258) 
and 
StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue less than or equals max Underscript k Endscripts sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript k j Baseline EndAbsoluteValue period|ci| ≤max
k
E
j
|akj|.
(3.259) 
The inequalities of course also hold for rho left parenthesis upper A right parenthesisρ(A) on the left-hand side. Rather 
than proving this here, we show this fact in a more general setting relating 
to matrix norms in inequality (3.340) on page 194. (The two bounds above 
relate to the normal upper L 1L1 and normal upper L Subscript normal infinityL∞matrix norms, respectively, as we will see.) 
The spectral radius gives a very simple indication of the region in the 
complex plane in which the entire spectrum lies. Consider, for example, the 
matrix 
upper A equals Start 3 By 5 Matrix 1st Row 1st Column 9 2nd Column negative 6 3rd Column 1 2nd Row 1st Column negative 2 2nd Column Blank 3rd Column 9 4th Column Blank 5th Column negative 5 3rd Row 1st Column 10 2nd Column Blank 3rd Column negative 2 4th Column Blank 5th Column 4 EndMatrix periodA =
⎡
⎣
9
−6
1
−2
9
−5
10
−2
4
⎤
⎦.
(3.260) 
(See Exercise 3.46 for comments on the origins of this matrix.) 
From Eq. (3.258), we see that all eigenvalues are less than or equal to 16 in 
modulus. In fact, the eigenvalues are sigma left parenthesis upper A right parenthesis equals StartSet 16 comma 3 plus 4 normal i comma 3 minus 4 normal i EndSetσ(A) = {16, 3+4i, 3−4i}, and  rho left parenthesis upper A right parenthesis equals 16ρ(A) = 16. 
On page 167, we will discuss other regions of the complex plane in which 
all eigenvalues necessarily lie. 
A matrix may have all eigenvalues equal to 0, but yet the matrix itself 
may not be 0. (The matrix must be singular, however.) A nilpotent matrix 
(see page 98) and any upper triangular matrix with all 0s on the diagonal are 
examples. 
Because, as we saw on page 158, if  c is an eigenvalue of A, then  bc is an 
eigenvalue of bA where b is any nonzero scalar, we can scale a matrix with a 
nonzero eigenvalue so that its spectral radius is 1. The scaled matrix is simply 
upper S equals upper A divided by StartAbsoluteValue c 1 EndAbsoluteValueS = A/|c1|, and  rho left parenthesis upper S right parenthesis equals 1ρ(S) = 1. 
The spectral radius is one of the most important properties of a matrix. 
As we will see in Sect. 3.11.1, it is the  the  normal upper L Subscript pLp norm for a symmetric matrix. 
From Eqs. (3.258) and  (3.259), we have seen in any event that it is bounded 
from above by the normal upper L 1L1 and normal upper L Subscript normal infinityL∞matrix norms (which we will deﬁne formally in 
Sect. 3.11.1), and, in fact, in Eq. (3.340) we will see that the spectral radius is

166
3 Basic Properties of Matrices
bounded from above by any matrix norm. We will discuss the spectral radius 
further in Sects. 3.11.5 and 3.11.6. In Sect. 3.11.6, we will see that the spectral 
radius determines the convergence of a matrix power series (and this fact is 
related to the behavior of autoregressive processes). 
Linear Independence of Eigenvectors Associated with 
Distinct Eigenvalues 
Suppose that StartSet lamda 1 comma ellipsis comma lamda Subscript k Baseline EndSet{λ1, . . . , λk} is a set of distinct eigenvalues of the matrix A 
and StartSet x 1 comma ellipsis comma x Subscript k Baseline EndSet{x1, . . . , xk} is a set of eigenvectors such that left parenthesis lamda Subscript i Baseline comma x Subscript i Baseline right parenthesis(λi, xi) is an eigenpair. 
Then x 1 comma ellipsis comma x Subscript k Baselinex1, . . . , xk are linearly independent; that is, eigenvectors associated with 
distinct eigenvalues are linearly independent. 
We can see that this must be the case by assuming that the eigenvectors 
are not linearly independent. In that case, let StartSet y 1 comma ellipsis comma y Subscript j Baseline EndSet subset of StartSet x 1 comma ellipsis comma x Subscript k Baseline EndSet{y1, . . . , yj} ⊂{x1, . . . , xk}, for  
some j less than kj < k, be a maximal linearly independent subset. Let the corresponding 
eigenvalues be StartSet mu 1 comma ellipsis comma mu Subscript j Baseline EndSet subset of StartSet lamda 1 comma ellipsis comma lamda Subscript k Baseline EndSet{μ1, . . . , μj} ⊂{λ1, . . . , λk}. Then, for some eigenvector y Subscript j plus 1yj+1, 
we have 
y Subscript j plus 1 Baseline equals sigma summation Underscript i equals 1 Overscript j Endscripts t Subscript i Baseline y Subscript iyj+1 =
j
E
i=1
tiyi
for some t Subscript iti. Now, multiplying both sides of the equation by upper A minus mu Subscript j plus 1 Baseline upper IA −μj+1I, where  
mu Subscript j plus 1μj+1 is the eigenvalue corresponding to y Subscript j plus 1yj+1, we have  
0 equals sigma summation Underscript i equals 1 Overscript j Endscripts t Subscript i Baseline left parenthesis mu Subscript i Baseline minus mu Subscript j plus 1 Baseline right parenthesis y Subscript i Baseline period0 =
j
E
i=1
ti(μi −μj+1)yi.
If the eigenvalues are distinct (that is, for each i less than or equals ji ≤j), we have mu Subscript i Baseline not equals mu Subscript j plus 1μi /= μj+1, 
then the assumption that the eigenvalues are not linearly independent is con-
tradicted because otherwise we would have a linear combination with nonzero 
coeﬃcients equal to zero. 
The Eigenspace and Geometric Multiplicity 
Rewriting the deﬁnition (3.240) for  the  i normal t normal hith eigenvalue and associated eigen-
vector of the n times nn × n matrix A as 
left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis v Subscript i Baseline equals 0 comma(A −ciI)vi = 0,
(3.261) 
we see that the eigenvector v Subscript ivi is in script upper N left parenthesis upper A minus c Subscript i Baseline upper I right parenthesisN(A −ciI), the  null  space of  left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis(A −ciI). 
For such a nonnull vector to exist, of course, left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis(A −ciI) must be singular; that 
is, normal r normal a normal n normal k left parenthesis upper A minus c Subscript i Baseline upper I right parenthesisrank(A −ciI) must be less than n. This null space is called the eigenspace 
of the eigenvalue c Subscript ici. 
It is possible that a given eigenvalue may have more than one associated 
eigenvector that are linearly independent of each other. For example, we eas-
ily see that the identity matrix has only one distinct eigenvalue, namely 1,

3.9 Eigenanalysis: Canonical Factorizations
167
but any vector is an eigenvector, and so the number of linearly independent 
eigenvectors is equal to the number of rows or columns of the identity. If u 
and v are eigenvectors corresponding to the same eigenvalue lamdaλ, then any lin-
ear combination of u and v is an eigenvector corresponding to lamdaλ; that is,  if  
upper A u equals lamda uAu = λu and upper A v equals lamda vAv = λv, for any scalars a and b, 
upper A left parenthesis a u plus b v right parenthesis equals lamda left parenthesis a u plus b v right parenthesis periodA(au + bv) = λ(au + bv).
The dimension of the eigenspace corresponding to the eigenvalue c Subscript ici is called 
the geometric multiplicity of c Subscript ici; that is, the geometric multiplicity of c Subscript ici is the 
nullity of upper A minus c Subscript i Baseline upper IA −ciI. If  g Subscript igi is the geometric multiplicity of c Subscript ici, an eigenvalue of the 
n times nn× n matrix A, then we can see from Eq. (3.214) that  normal r normal a normal n normal k left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis plus g Subscript i Baseline equals nrank(A−ciI) + gi = n. 
The multiplicity of 0 as an eigenvalue is just the nullity of A. If  A is of full 
rank, the multiplicity of 0 will be 0, but, in this case, we do not consider 0 to 
be an eigenvalue. If A is singular, however, we consider 0 to be an eigenvalue, 
and the multiplicity of the 0 eigenvalue is the rank deﬁciency of A. 
Multiple linearly independent eigenvectors corresponding to the same 
eigenvalue can be chosen to be orthogonal to each other using, for exam-
ple, the Gram–Schmidt transformations, as in Eq. (2.55) on page 48. These  
orthogonal eigenvectors span the same eigenspace. They are not unique, of 
course, as any sequence of Gram–Schmidt transformations could be applied. 
Algebraic Multiplicity 
A single value that occurs as a root of the characteristic equation m times 
is said to have algebraic multiplicity m. Although we sometimes refer to this 
as just the multiplicity, algebraic multiplicity should be distinguished from 
geometric multiplicity, deﬁned above. These are not the same, as we will see 
in an example later (page 173). The algebraic multiplicity of a given eigenvalue 
is at least as great as its geometric multiplicity (exercise). 
An eigenvalue whose algebraic multiplicity and geometric multiplicity are 
the same is called a  semisimple eigenvalue. An eigenvalue with algebraic multi-
plicity 1 is called a simple eigenvalue (hence, a simple eigenvalue is necessarily 
a semisimple eigenvalue). 
Because the determinant that deﬁnes the eigenvalues of an n times nn×n matrix is 
an n normal t normal hnth-degree polynomial, we see that the sum of the multiplicities of distinct 
eigenvalues is n. 
Gershgorin Disks 
In addition to the spectral circle, there is another speciﬁcation of a region in 
the complex plane that contains the spectrum of an n times nn × n matrix A. This is  
the union of the n Gershgorin disks, where  for  i equals 1 comma ellipsis comma ni = 1, . . . , n, the  i normal t normal hith of which 
is the disk 
StartAbsoluteValue z minus a Subscript i i Baseline EndAbsoluteValue less than or equals r Subscript i Baseline normal w normal h normal e normal r normal e r Subscript i Baseline equals sigma summation Underscript 1 less than or equals j less than or equals n semicolon j not equals i Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue period|z −aii| ≤ri
where ri =
E
1≤j≤n; j/=i
|aij|.
(3.262)

168
3 Basic Properties of Matrices
(“Gershgorin” is often spelled as “Gerschgorin” or “Gersgorin” or even 
“Gerˇsgorin”; he was Russian.) 
To see that this is the case, let left parenthesis c comma v right parenthesis(c, v) be an arbitrary eigenpair of A with 
v normalized by the normal upper L Subscript normal infinityL∞norm (that is, max left parenthesis StartAbsoluteValue v EndAbsoluteValue right parenthesis equals 1max(|v|) = 1). Let k be such that 
StartAbsoluteValue v Subscript k Baseline EndAbsoluteValue equals 1|vk| = 1. Then  
c v Subscript k Baseline equals left parenthesis upper A v right parenthesis Subscript k Baseline equals sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript k j Baseline v Subscript j Baseline semicoloncvk = (Av)k =
n
E
j=1
akjvj;
hence 
left parenthesis c minus a Subscript k k Baseline right parenthesis v Subscript k Baseline equals sigma summation Underscript 1 less than or equals j less than or equals n semicolon j not equals k Endscripts a Subscript k j Baseline v Subscript j Baseline period(c −akk)vk =
E
1≤j≤n; j/=k
akjvj.
Now introduce the modulus, and we get the desired inequality: 
StartLayout 1st Row 1st Column StartAbsoluteValue c minus a Subscript k k Baseline EndAbsoluteValue 2nd Column equals 3rd Column StartAbsoluteValue c minus a Subscript k k Baseline EndAbsoluteValue StartAbsoluteValue v Subscript k Baseline EndAbsoluteValue 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartAbsoluteValue sigma summation Underscript 1 less than or equals j less than or equals n semicolon j not equals k Endscripts a Subscript k j Baseline v Subscript j Baseline EndAbsoluteValue 3rd Row 1st Column Blank 4th Row 1st Column Blank 2nd Column less than or equals 3rd Column sigma summation Underscript 1 less than or equals j less than or equals n semicolon j not equals k Endscripts StartAbsoluteValue a Subscript k j Baseline EndAbsoluteValue StartAbsoluteValue v Subscript j Baseline EndAbsoluteValue 5th Row 1st Column Blank 6th Row 1st Column Blank 2nd Column less than or equals 3rd Column sigma summation Underscript 1 less than or equals j less than or equals n semicolon j not equals k Endscripts StartAbsoluteValue a Subscript k j Baseline EndAbsoluteValue 7th Row 1st Column Blank 8th Row 1st Column Blank 2nd Column equals 3rd Column r Subscript k Baseline period EndLayout|c −akk| = |c −akk||vk|
=
||||||
E
1≤j≤n; j/=k
akjvj
||||||
≤
E
1≤j≤n; j/=k
|akj||vj|
≤
E
1≤j≤n; j/=k
|akj|
= rk.
We conclude that every eigenvalue lies in some similar disk; that is, the spec-
trum lies in the union of such disks. 
Since sigma left parenthesis upper A Superscript normal upper T Baseline right parenthesis equals sigma left parenthesis upper A right parenthesisσ(AT) = σ(A), using the same argument as above, we can deﬁne 
another collection of n Gershgorin disks based on column sums: 
StartAbsoluteValue z minus a Subscript j j Baseline EndAbsoluteValue less than or equals s Subscript j Baseline normal w normal h normal e normal r normal e s Subscript j Baseline equals sigma summation Underscript 1 less than or equals i less than or equals n semicolon i not equals j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue period|z −ajj| ≤sj
where sj =
E
1≤i≤n; i/=j
|aij|.
(3.263) 
All eigenvalues of A lie within the union of these disks. 
Combining the two restrictions, we see that all eigenvalues of A lie within 
the intersection of these two unions of Gershgorin disks. 
3.9.5 Similarity Transformations 
Two n times nn×n matrices, A and B, are  said  to  be  similar if there exists a nonsingular 
matrix P such that 
upper B equals upper P Superscript negative 1 Baseline upper A upper P periodB = P −1AP.
(3.264) 
The transformation in Eq. (3.264) is called a similarity transformation. (Com-
pare similar matrices with equivalent matrices on page 134. The matrices

3.9 Eigenanalysis: Canonical Factorizations
169
A and B in Eq. (3.264) are also equivalent, as we see using Eqs. (3.167) 
and (3.168).) 
It is clear from the deﬁnition that the similarity relationship is both com-
mutative and transitive. 
If A and B are similar, as in Eq. (3.264), then for any scalar c 
StartLayout 1st Row 1st Column normal d normal e normal t left parenthesis upper A minus c upper I right parenthesis 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper P Superscript negative 1 Baseline right parenthesis normal d normal e normal t left parenthesis upper A minus c upper I right parenthesis normal d normal e normal t left parenthesis upper P right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper P Superscript negative 1 Baseline upper A upper P minus c upper P Superscript negative 1 Baseline upper I upper P right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal e normal t left parenthesis upper B minus c upper I right parenthesis comma EndLayoutdet(A −cI) = det(P −1)det(A −cI)det(P)
= det(P −1AP −cP −1IP)
= det(B −cI),
and, hence, A and B have the same eigenvalues. (This simple fact was stated 
as property 9 on page 159.) 
Orthogonally and Unitarily Similar Transformations 
An important type of similarity transformation is based on an orthogonal 
matrix in Eq. (3.264). If Q is orthogonal and 
upper B equals upper Q Superscript normal upper T Baseline upper A upper Q commaB = QTAQ,
(3.265) 
A and B are said to be orthogonally similar. 
If there is an orthogonal Q, such that B in Eq. (3.265) is a diagonal matrix, 
then A is said to be orthogonally diagonalizable, and  
upper A equals upper Q upper B upper Q Superscript normal upper TA = QBQT
(3.266) 
is called the orthogonally diagonal factorization or orthogonally similar fac-
torization of A. 
The concepts of orthogonally similar and orthogonal diagonalization are 
very important, but for matrices with complex entries or for real matrices 
with complex eigenvalues, generalizations of the concepts based on unitary 
matrices are more useful. If U is unitary and 
upper B equals upper U Superscript normal upper H Baseline upper A upper U commaB = U HAU,
(3.267) 
A and B are said to be unitarily similar. Since an orthogonal matrix is unitary, 
two matrices that are orthogonally similar are also unitarily similar. 
If B in Eq. (3.267) upper B equals upper U Superscript normal upper H Baseline upper A upper UB = U HAU is a diagonal matrix, A is said to be uni-
tarily diagonalizable, and  upper U upper B upper U Superscript normal upper HUBU H is called the unitarily diagonal factorization 
or unitarily similar factorization of A. A matrix that is orthogonally diago-
nalizable is also unitarily diagonalizable. 
We will discuss characteristics of orthogonally diagonalizable matrices in 
Sects. 3.9.7 through 3.9.9 below. The signiﬁcant fact that we will see there is 
that a matrix is orthogonally diagonalizable if and only if it is symmetric. 
We will discuss characteristics of unitarily diagonalizable matrices in 
Sect. 8.2.3 on page 383. The signiﬁcant fact that we will see there is that 
a matrix is unitarily diagonalizable if and only if it is normal (which includes 
symmetric matrices).

170
3 Basic Properties of Matrices
Uses of Similarity Transformations 
Similarity transformations are very useful in establishing properties of matri-
ces, such as convergence properties of sequences (see, for example, Sect. 3.11.6). 
Similarity transformations are also used in algorithms for computing eigenval-
ues (see, for example, Sect. 6.3). In an orthogonally similar factorization, the 
elements of the diagonal matrix are the eigenvalues. Although the diagonals 
in the upper triangular matrix of the Schur factorization are the eigenvalues, 
that particular factorization is rarely used in computations. 
Although similar matrices have the same eigenvalues, they do not neces-
sarily have the same eigenvectors. If A and B are similar, for some nonzero 
vector v and some scalar c, upper A v equals c vAv = cv implies that there exists a nonzero vector 
u such that upper B u equals c uBu = cu, but it does not imply that u equals vu = v (see Exercise 3.29b). 
3.9.6 Schur Factorization 
If B in Eq. (3.265) is an upper triangular matrix, upper Q upper B upper Q Superscript normal upper TQBQT is called the Schur 
factorization of A: 
upper A equals upper Q upper B upper Q Superscript normal upper T Baseline periodA = QBQT.
(3.268) 
This is also called the “Schur form” of A. 
For any square matrix, the Schur factorization exists. Although the ma-
trices in the factorization are not unique, the diagonal elements of the upper 
triangular matrix B are the eigenvalues of A. 
There are various forms of the Schur factorization. Because in general, the 
eigenvalues and eigenvectors may contain imaginary components, the orthog-
onal matrices in Eq. (3.268) may contain imaginary components, and further-
more, the Schur factorization is particularly useful in studying factorizations 
involving unitary matrices, we will describe the Schur factorization that use 
unitary matrices. 
The general form of the Schur factorization for a square matrix A is 
upper A equals upper U upper T upper U Superscript normal upper H Baseline commaA = UTU H,
(3.269) 
where U is a unitary matrix and T is an upper triangular matrix whose 
diagonal entries are the eigenvalues of A. 
The Schur factorization exists for any square matrix, which we can see by 
induction. It clearly exists in the degenerate case of a 1 times 11×1 matrix. To see that 
it exists for any n times nn × n matrix A, let  left parenthesis c comma v right parenthesis(c, v) be an arbitrary eigenpair of A with 
v normalized, and form a unitary matrix upper U 1U1 with v as its ﬁrst column. Let upper U 2U2
be the matrix consisting of the remaining columns; that is, upper U 1U1 is partitioned 
as left bracket v vertical bar upper U 2 right bracket[v | U2].

3.9 Eigenanalysis: Canonical Factorizations
171
StartLayout 1st Row 1st Column upper U 1 Superscript normal upper H Baseline upper A upper U 1 2nd Column equals 3rd Column Start 2 By 3 Matrix 1st Row 1st Column v Superscript normal upper H Baseline upper A v 2nd Column v Superscript normal upper H Baseline upper A upper U 2 2nd Row 1st Column upper U 2 Superscript normal upper H Baseline upper A v 2nd Column Blank 3rd Column upper U 2 Superscript normal upper H Baseline upper A upper U 2 EndMatrix 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column Start 2 By 3 Matrix 1st Row 1st Column c 2nd Column v Superscript normal upper H Baseline upper A upper U 2 2nd Row 1st Column 0 2nd Column Blank 3rd Column upper U 2 Superscript normal upper H Baseline upper A upper U 2 EndMatrix 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals 3rd Column upper T comma EndLayoutU H
1 AU1 =
[ vHAv
vHAU2
U H
2 Av
U H
2 AU2
]
=
[
c
vHAU2
0
U H
2 AU2
]
= T,
where upper U 2 Superscript normal upper H Baseline upper A upper U 2U H
2 AU2 is an left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1) matrix. Now the eigenvalues of upper U 2 Superscript normal upper H Baseline upper A upper U 2U H
2 AU2
are all eigenvalues of A; hence, if n equals 2n = 2, then  upper U 2 Superscript normal upper H Baseline upper A upper U 2U H
2 AU2 is a scalar and must 
equal the other eigenvalue of A, and so the statement is proven for a 2 times 22 × 2
matrix. 
We now use induction on n to establish the general case. Assume that the 
factorization exists for any left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1) matrix, and let A be any n times nn × n
matrix. We let left parenthesis c comma v right parenthesis(c, v) be an arbitrary eigenpair of A (with v normalized), follow 
the same procedure as in the preceding paragraph, and get 
upper U 1 Superscript normal upper H Baseline upper A upper U 1 equals Start 2 By 3 Matrix 1st Row 1st Column c 2nd Column v Superscript normal upper H Baseline upper A upper U 2 2nd Row 1st Column 0 2nd Column Blank 3rd Column upper U 2 Superscript normal upper H Baseline upper A upper U 2 EndMatrix periodU H
1 AU1 =
[ c
vHAU2
0
U H
2 AU2
]
.
Now, since upper U 2 Superscript normal upper H Baseline upper A upper U 2U H
2 AU2 is an left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n−1)×(n−1) matrix, by the induction hypothesis 
there exists an left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n−1)×(n−1) unitary matrix V such that upper V Superscript normal upper H Baseline left parenthesis upper U 2 Superscript normal upper H Baseline upper A upper U 2 right parenthesis upper V equals upper T 1V H(U H
2 AU2)V = T1, 
where upper T 1T1 is upper triangular. Now let 
upper U equals upper U 1 Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st Column 0 2nd Column upper V EndMatrix periodU = U1
[ 1 0
0 V
]
.
By multiplication, we see that upper U Superscript normal upper H Baseline upper U equals upper IU HU = I (that is, Q is unitary). Now form 
upper U Superscript normal upper H Baseline upper A upper U equals Start 2 By 3 Matrix 1st Row 1st Column c 2nd Column v Superscript normal upper H Baseline upper A upper U 2 upper V 2nd Row 1st Column 0 2nd Column Blank 3rd Column upper V Superscript normal upper H Baseline upper U 2 Superscript normal upper H Baseline upper A upper U 2 upper V EndMatrix equals Start 2 By 3 Matrix 1st Row 1st Column c 2nd Column v Superscript normal upper H Baseline upper A upper U 2 upper V 2nd Row 1st Column 0 2nd Column Blank 3rd Column upper T 1 EndMatrix equals upper T periodU HAU =
[
c
vHAU2V
0
V HU H
2 AU2V
]
=
[
c
vHAU2V
0
T1
]
= T.
We see that T is upper triangular because upper T 1T1 is, and so by induction the Schur 
factorization exists for any n times nn × n matrix. 
The steps in the induction did not necessarily involve unique choices except 
for the eigenvalues on the diagonal of T. 
Note that the Schur factorization is also based on unitarily similar trans-
formations, but the term “unitarily similar factorization” is generally used 
only to refer to the diagonal factorization. 
3.9.7 Similar Canonical Factorization: Diagonalizable Matrices 
If V is a matrix whose columns correspond to the eigenvectors of A, and  C 
is a diagonal matrix whose entries are the eigenvalues corresponding to the 
columns of V , using the deﬁnition (Eq. (3.240)) we can write 
upper A upper V equals upper V upper C periodAV = V C.
(3.270)

172
3 Basic Properties of Matrices
Now, if V is nonsingular, we have 
upper A equals upper V upper C upper V Superscript negative 1 Baseline periodA = VCV −1.
(3.271) 
Expression (3.271) represents a  diagonal factorization of the matrix A. We see  
that a matrix A with eigenvalues c 1 comma ellipsis comma c Subscript n Baselinec1, . . . , cn that can be factorized this way is 
similar to the matrix normal d normal i normal a normal g left parenthesis left parenthesis c 1 comma ellipsis comma c Subscript n Baseline right parenthesis right parenthesisdiag((c1, . . . , cn)), and this representation is sometimes 
called the similar canonical form of A or the similar canonical factorization 
of A. 
Not all matrices can be factored as in Eq. (3.271). It obviously depends on 
V being nonsingular; that is, that the eigenvectors are linearly independent. 
If a matrix can be factored as in (3.271), it is called a diagonalizable matrix, 
a simple matrix, or a  regular matrix (the terms are synonymous, and we will 
generally use the term “diagonalizable”); a matrix that cannot be factored 
in that way is called a deﬁcient matrix or a defective matrix (the terms are 
synonymous). 
Any matrix all of whose eigenvalues are unique (that is, distinct) is di-
agonalizable (because, as we saw on page 166, in that case the eigenvectors 
are linearly independent), but uniqueness of the eigenvalues is not a necessary 
condition. 
A necessary and suﬃcient condition for a matrix to be diagonalizable can 
be stated in terms of the unique eigenvalues and their multiplicities: suppose 
for the n times nn × n matrix A that the distinct eigenvalues lamda 1 comma ellipsis comma lamda Subscript k Baselineλ1, . . . , λk have algebraic 
multiplicities m 1 comma ellipsis comma m Subscript k Baselinem1, . . . , mk. If, for l equals 1 comma ellipsis comma kl = 1, . . . , k, 
normal r normal a normal n normal k left parenthesis upper A minus lamda Subscript l Baseline upper I right parenthesis equals n minus m Subscript lrank(A −λlI) = n −ml
(3.272) 
(that is, if all eigenvalues are semisimple), then A is diagonalizable, and this 
condition is also necessary for A to be diagonalizable. This fact is called the 
“diagonalizability theorem.” Recall that A being diagonalizable is equivalent 
to V in upper A upper V equals upper V upper CAV = V C (Eq. (3.270)) being nonsingular. 
To see that the condition is suﬃcient, assume, for each i, normal r normal a normal n normal k left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis equals n minus m Subscript irank(A −ciI) =
n −mi, and so the equation left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis x equals 0(A −ciI)x = 0 has exactly n minus left parenthesis n minus m Subscript i Baseline right parenthesisn −(n −mi) linearly 
independent solutions, which are by deﬁnition eigenvectors of A associated 
with c Subscript ici. (Note the somewhat complicated notation. Each c Subscript ici is  the same as  
some lamda Subscript lλl, and  for each  lamda Subscript lλl, we have  lamda Subscript l Baseline equals c Subscript l 1 Baseline equals c Subscript l Sub Subscript m Sub Sub Subscript lλl = cl1 = clml for 1 less than or equals l 1 less than midline horizontal ellipsis less than l Subscript m Sub Subscript l Baseline less than or equals n1 ≤l1 < · · · < lml ≤n.) 
Let w 1 comma ellipsis comma w Subscript m Sub Subscript i Subscript Baselinew1, . . . , wmi be a set of linearly independent eigenvectors associated with 
c Subscript ici, and  let  u be an eigenvector associated with c Subscript jcj and c Subscript j Baseline not equals c Subscript icj /= ci. (The vectors 
w 1 comma ellipsis comma w Subscript m Sub Subscript i Subscript Baselinew1, . . . , wmi and u are columns of V .) We have already seen on page 166 that 
u must be linearly independent of the other eigenvectors, but we can also 
use a slightly diﬀerent argument here. Now if u is not linearly independent of 
w 1 comma ellipsis comma w Subscript m Sub Subscript i Subscript Baselinew1, . . . , wmi, we write  u equals sigma summation b Subscript k Baseline w Subscript ku = E bkwk, and  so  upper A u equals upper A sigma summation b Subscript k Baseline w Subscript k Baseline equals c Subscript i Baseline sigma summation b Subscript k Baseline w Subscript k Baseline equals c Subscript i Baseline uAu = A E bkwk = ci
E bkwk =
ciu, contradicting the assumption that u is not an eigenvector associated with 
c Subscript ici. Therefore, the eigenvectors associated with diﬀerent eigenvalues are linearly 
independent, and so V is nonsingular. 
Now, to see that the condition is necessary, assume V is nonsingular; that 
is, upper V Superscript negative 1V −1 exists. Because C is a diagonal matrix of all n eigenvalues, the matrix

3.9 Eigenanalysis: Canonical Factorizations
173
left parenthesis upper C minus c Subscript i Baseline upper I right parenthesis(C −ciI) has exactly m Subscript imi zeros on the diagonal, and hence, normal r normal a normal n normal k left parenthesis upper C minus c Subscript i Baseline upper I right parenthesis equals n minus m Subscript irank(C −ciI) =
n−mi. Because upper V left parenthesis upper C minus c Subscript i Baseline upper I right parenthesis upper V Superscript negative 1 Baseline equals left parenthesis upper A minus c Subscript i Baseline upper I right parenthesisV (C −ciI)V −1 = (A−ciI), and multiplication by a full rank 
matrix does not change the rank (see page 132), we have normal r normal a normal n normal k left parenthesis upper A minus c Subscript i Baseline upper I right parenthesis equals n minus m Subscript irank(A −ciI) =
n −mi. 
Symmetric Matrices 
A symmetric matrix is a diagonalizable matrix. We see this by ﬁrst letting A 
be any n times nn × n symmetric matrix with eigenvalue c of multiplicity m. We need 
to show that normal r normal a normal n normal k left parenthesis upper A minus c upper I right parenthesis equals n minus mrank(A −cI) = n −m. Let  upper B equals upper A minus c upper IB = A −cI, which is symmetric 
because A and I are. First, we note that c is real, and therefore B is real. Let 
r equals normal r normal a normal n normal k left parenthesis upper B right parenthesisr = rank(B). From Eq. (3.179), we have 
normal r normal a normal n normal k left parenthesis upper B squared right parenthesis equals normal r normal a normal n normal k left parenthesis upper B Superscript normal upper T Baseline upper B right parenthesis equals normal r normal a normal n normal k left parenthesis upper B right parenthesis equals r periodrank
(
B2)
= rank
(
BTB
)
= rank(B) = r.
In the full rank partitioning of B, there is at least one r times rr×r principal submatrix 
of full rank. The r-order principal minor in upper B squaredB2 corresponding to any full rank 
r times rr × r principal submatrix of B is therefore positive. Furthermore, any j-order 
principal minor in upper B squaredB2 for j greater than rj > r is zero. Now, rewriting the characteristic 
polynomial in Eq. (3.243) slightly by attaching the sign to the variable w, we  
have 
p Subscript upper B squared Baseline left parenthesis w right parenthesis equals t Subscript n minus r Baseline left parenthesis negative w right parenthesis Superscript n minus r Baseline plus midline horizontal ellipsis plus t Subscript n minus 1 Baseline left parenthesis negative w right parenthesis Superscript n minus 1 Baseline plus left parenthesis negative w right parenthesis Superscript n Baseline equals 0 commapB2(w) = tn−r(−w)n−r + · · · + tn−1(−w)n−1 + (−w)n = 0,
where t Subscript n minus jtn−j is the sum of all j-order principal minors. Because t Subscript n minus r Baseline not equals 0tn−r /= 0, w equals 0w = 0
is a root of multiplicity n minus rn−r. It is likewise an eigenvalue of B with multiplicity 
n minus rn −r. Because upper A equals upper B plus c upper IA = B + cI, 0 plus c0 + c is an eigenvalue of A with multiplicity n minus rn −r; 
hence, m equals n minus rm = n −r. Therefore, n minus m equals r equals normal r normal a normal n normal k left parenthesis upper A minus c upper I right parenthesisn −m = r = rank(A −cI). 
As we will see below in Sect. 3.9.9, a symmetric matrix A is not only 
diagonalizable in the form (3.271), upper A equals upper V upper C upper V Superscript negative 1A = VCV −1, the  matrix  V can be chosen as 
an orthogonal matrix, so we have upper A equals upper U upper C upper U Superscript normal upper TA = UCU T. We will say that the symmetric 
matrix is orthogonally diagonalizable. 
A Defective Matrix 
Although most matrices encountered in statistics applications are diagonal-
izable, it may be of interest to consider an example of a matrix that is not 
diagonalizable. Consider the matrix 
upper A equals Start 3 By 3 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 2 2nd Row 1st Column 2 2nd Column 3 3rd Column 0 3rd Row 1st Column 0 2nd Column 4 3rd Column 5 EndMatrix periodA =
⎡
⎣
0 1 2
2 3 0
0 4 5
⎤
⎦.
The three strategically placed 0s make this matrix easy to work with, and the 
determinant of left parenthesis c upper I minus upper A right parenthesis(cI −A) yields the characteristic polynomial equation 
c cubed minus 8 c squared plus 13 c minus 6 equals 0 periodc3 −8c2 + 13c −6 = 0.

174
3 Basic Properties of Matrices
This can be factored as left parenthesis c minus 6 right parenthesis left parenthesis c minus 1 right parenthesis squared(c−6)(c−1)2, hence, we have eigenvalues c 1 equals 6c1 = 6 with 
algebraic multiplicity m 1 equals 1m1 = 1, and  c 2 equals 1c2 = 1 with algebraic multiplicity m 2 equals 2m2 = 2. 
Now, consider upper A minus c 2 upper IA −c2I: 
upper A minus upper I equals Start 3 By 3 Matrix 1st Row 1st Column negative 1 2nd Column 1 3rd Column 2 2nd Row 1st Column 2 2nd Column 2 3rd Column 0 3rd Row 1st Column 0 2nd Column 4 3rd Column 4 EndMatrix periodA −I =
⎡
⎣
−1 1 2
2 2 0
0 4 4
⎤
⎦.
This is clearly of rank 2; hence, the rank of the null space of upper A minus c 2 upper IA−c2I (that is, the 
geometric multiplicity of c 2c2) is  3 minus 2 equals 13 −2 = 1. The  matrix  A is not diagonalizable. 
(This example is due to Searle, 1982.) 
The Jordan Decomposition 
Although not all matrices can be diagonalized in the form of Eq. (3.271), 
upper V Superscript negative 1 Baseline upper A upper V equals upper C equals normal d normal i normal a normal g left parenthesis c Subscript i Baseline right parenthesisV −1AV = C = diag(ci), any square matrix A can be expressed in the form 
upper X Superscript negative 1 Baseline upper A upper X equals normal d normal i normal a normal g left parenthesis upper J Subscript j Baseline right parenthesis commaX−1AX = diag(Jj),
(3.273) 
where the upper J Subscript jJj are Jordan blocks associated with a single eigenvalue lamda Subscript jλj, of the  
form 
upper J Subscript j Baseline left parenthesis lamda Subscript j Baseline right parenthesis equals Start 6 By 6 Matrix 1st Row 1st Column lamda Subscript j Baseline 2nd Column 1 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 0 6th Column 0 2nd Row 1st Column 0 2nd Column lamda Subscript j Baseline 3rd Column 1 4th Column midline horizontal ellipsis 5th Column 0 6th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column lamda Subscript j Baseline 4th Column midline horizontal ellipsis 5th Column 0 6th Column 0 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column down right diagonal ellipsis 6th Column vertical ellipsis 5th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column lamda Subscript j Baseline 6th Column 1 6th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column lamda Subscript j Baseline EndMatrix commaJj(λj) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎣
λj 1
0 · · · 0
0
0 λj 1 · · · 0
0
0
0 λj · · · 0
0
...
...
...
... ... ...
0
0 · · · 0 λj 1
0
0 · · · 0
0 λj
⎤
⎥⎥⎥⎥⎥⎥⎥⎦
,
or, in the degenerate case, upper J Subscript j Baseline left parenthesis lamda Subscript j Baseline right parenthesis equals left bracket lamda Subscript j Baseline right bracketJj(λj) = [λj], where  lamda Subscript jλj is a speciﬁc distinct eigen-
value (that is, lamda Subscript j Baseline not equals lamda Subscript kλj /= λk if j not equals kj /= k). (Compare this with the Jordan form of a 
nilpotent matrix following Eq. (3.60) on page 99, in which the diagonal ele-
ments are 0s.) If each Jordan block upper J Subscript jJj is 1 times 11 × 1, the Jordan decomposition is 
a diagonal decomposition. 
There are some interesting facts about the Jordan decomposition. If there 
are g Subscript jgj Jordan blocks associated with the eigenvalue lamda Subscript jλj, then  lamda Subscript jλj has geometric 
multiplicity g Subscript jgj. The algebraic multiplicity of lamda Subscript jλj is the total number of diagonal 
elements in all the Jordan blocks associated with lamda Subscript jλj; hence, if each Jordan 
block upper J Subscript jJj is 1 times 11 × 1, then all eigenvalues are semisimple. While these two facts 
appear rather profound, they are of little interest for our purposes, and we 
will not give proofs. (Proofs can be found in Horn and Johnson (1991).) The 
problem of computing a Jordan decomposition is ill-conditioned because slight 
perturbations in the elements of A can obviously result in completely diﬀerent 
sets of Jordan blocks. 
3.9.8 Properties of Diagonalizable Matrices 
If the matrix A has the similar canonical factorization upper V upper C upper V Superscript negative 1VCV −1 of Eq. (3.271), 
some important properties are immediately apparent. First of all, this factor-

3.9 Eigenanalysis: Canonical Factorizations
175
ization implies that the eigenvectors of a diagonalizable matrix are linearly 
independent. 
Other properties are easy to derive or to show because of this factorization. 
For example, the general Eqs. (3.249) and  (3.250) concerning the product and 
the sum of eigenvalues follow easily from 
normal d normal e normal t left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper V upper C upper V Superscript negative 1 Baseline right parenthesis equals normal d normal e normal t left parenthesis upper V right parenthesis normal d normal e normal t left parenthesis upper C right parenthesis normal d normal e normal t left parenthesis upper V Superscript negative 1 Baseline right parenthesis equals normal d normal e normal t left parenthesis upper C right parenthesisdet(A) = det(VCV −1) = det(V ) det(C) det(V −1) = det(C)
and 
normal t normal r left parenthesis upper A right parenthesis equals normal t normal r left parenthesis upper V upper C upper V Superscript negative 1 Baseline right parenthesis equals normal t normal r left parenthesis upper V Superscript negative 1 Baseline upper V upper C right parenthesis equals normal t normal r left parenthesis upper C right parenthesis periodtr(A) = tr(VCV −1) = tr(V −1VC) = tr(C).
One important fact is that the number of nonzero eigenvalues of a diago-
nalizable matrix A is equal to the rank of A. This must be the case because 
the rank of the diagonal matrix C is its number of nonzero elements, and the 
rank of A must  be  the same as the  rank  of  C. Another way of saying this is 
that the sum of the multiplicities of the unique nonzero eigenvalues is equal 
to the rank of the matrix; that is, sigma summation Underscript i equals 1 Overscript k Endscripts m Subscript i Baseline equals normal r normal a normal n normal k left parenthesis upper A right parenthesisEk
i=1 mi = rank(A), for the matrix A with 
k distinct eigenvalues with multiplicities m Subscript imi. 
Matrix Functions 
We can use the diagonal factorization (3.271) of the  matrix  upper A equals upper V upper C upper V Superscript negative 1A = VCV −1 to 
deﬁne a function of the matrix that corresponds to a scalar-valued function 
of a scalar, f left parenthesis x right parenthesisf(x), 
f left parenthesis upper A right parenthesis equals upper V normal d normal i normal a normal g left parenthesis left parenthesis f left parenthesis c 1 right parenthesis comma ellipsis comma f left parenthesis c Subscript n Baseline right parenthesis right parenthesis right parenthesis upper V Superscript negative 1 Baseline commaf(A) = V diag((f(c1), . . . , f(cn)))V −1,
(3.274) 
if f left parenthesis dot right parenthesisf(·) is deﬁned for each eigenvalue c Subscript ici. (Notice the relationship of this deﬁni-
tion to the Cayley–Hamilton theorem, page 161, and to Exercise 3.26.) 
Another useful feature of the diagonal factorization of the matrix A in 
Eq. (3.271) is that it allows us to study functions of powers of A because 
upper A Superscript k Baseline equals upper V upper C Superscript k Baseline upper V Superscript negative 1Ak = VCkV −1. In particular, we may assess the convergence of a function of 
a power  of  A, 
limit Underscript k right arrow normal infinity Endscripts g left parenthesis k comma upper A right parenthesis period lim
k→∞g(k, A).
Functions deﬁned by elementwise operations have limited applications. 
Functions of real numbers that have power series expansions may be deﬁned 
for matrices in terms of power series expansions in A, which are eﬀectively 
power series in the diagonal elements of C. For example, using the power 
series expansion of normal e Superscript x Baseline equals sigma summation Underscript k equals 0 Overscript normal infinity Endscripts StartFraction x Superscript k Baseline Over k factorial EndFractionex = E∞
k=0
xk
k! , we can deﬁne the matrix exponential for 
the square matrix A as the matrix 
normal e Superscript upper A Baseline equals sigma summation Underscript k equals 0 Overscript normal infinity Endscripts StartFraction upper A Superscript k Baseline Over k factorial EndFraction commaeA =
∞
E
k=0
Ak
k! ,
(3.275) 
where upper A Superscript 0 Baseline divided by 0 factorialA0/0! is deﬁned as I. (Recall that we did not deﬁne upper A Superscript 0A0 if A is singular.) 
If A is represented as upper V upper C upper V Superscript negative 1VCV −1, this expansion becomes

176
3 Basic Properties of Matrices
StartLayout 1st Row 1st Column normal e Superscript upper A 2nd Column equals 3rd Column upper V sigma summation Underscript k equals 0 Overscript normal infinity Endscripts StartFraction upper C Superscript k Baseline Over k factorial EndFraction upper V Superscript negative 1 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper V normal d normal i normal a normal g left parenthesis left parenthesis normal e Superscript c 1 Baseline comma ellipsis comma normal e Superscript c Super Subscript n Superscript Baseline right parenthesis right parenthesis upper V Superscript negative 1 Baseline period EndLayouteA = V
∞
E
k=0
Ck
k! V −1
= V diag ((ec1, . . . , ecn)) V −1.
This is called the matrix exponential for the square matrix A. 
The expression exp left parenthesis upper A right parenthesisexp(A) is generally interpreted as exp left parenthesis upper A right parenthesis equals left parenthesis exp left parenthesis a Subscript i j Baseline right parenthesis right parenthesisexp(A) = (exp(aij)), 
while the expression normal e Superscript upper AeA is interpreted as in Eq. (3.275), but often each ex-
pression is used in the opposite way. As mentioned above, the standard exp 
function in software systems, when evaluated for a matrix A, yields left parenthesis exp left parenthesis a Subscript i j Baseline right parenthesis right parenthesis(exp(aij)). 
Both R and MATLAB have a function expm for the matrix exponential. (In 
R, expm is in the Matrix package.) 
3.9.9 Eigenanalysis of Symmetric Matrices 
The eigenvalues and eigenvectors of symmetric matrices have some interesting 
properties. First of all, as we have already observed, for a real symmetric 
matrix, the eigenvalues are all real. We have also seen that symmetric matrices 
are diagonalizable; therefore, all of the properties of diagonalizable matrices 
carry over to symmetric matrices. 
Orthogonality of Eigenvectors: Orthogonal Diagonalization 
In the case of a symmetric matrix A, any eigenvectors corresponding to dis-
tinct eigenvalues are orthogonal. This is easily seen by assuming that c 1c1 and 
c 2c2 are unequal eigenvalues with corresponding eigenvectors v 1v1 and v 2v2. Now  
consider v 1 Superscript normal upper T Baseline v 2vT
1 v2. Multiplying this by c 2c2, we get  
c 2 v 1 Superscript normal upper T Baseline v 2 equals v 1 Superscript normal upper T Baseline upper A v 2 equals v 2 Superscript normal upper T Baseline upper A v 1 equals c 1 v 2 Superscript normal upper T Baseline v 1 equals c 1 v 1 Superscript normal upper T Baseline v 2 periodc2vT
1 v2 = vT
1 Av2 = vT
2 Av1 = c1vT
2 v1 = c1vT
1 v2.
Because c 1 not equals c 2c1 /= c2, we have  v 1 Superscript normal upper T Baseline v 2 equals 0vT
1 v2 = 0. 
Now, consider two eigenvalues c Subscript i Baseline equals c Subscript jci = cj, that is, an eigenvalue of multiplicity 
greater than 1 and distinct associated eigenvectors v Subscript ivi and v Subscript jvj. By what we  
just saw, an eigenvector associated with c Subscript k Baseline not equals c Subscript ick /= ci is orthogonal to the space 
spanned by v Subscript ivi and v Subscript jvj. Assume v Subscript ivi is normalized and apply a Gram–Schmidt 
transformation to form 
v overTilde Subscript j Baseline equals StartFraction 1 Over parallel to v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline parallel to EndFraction left parenthesis v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline right parenthesis comma˜vj =
1
||vj −<vi, vj>vi|| (vj −<vi, vj>vi),
as in Eq. (2.55) on page 48, yielding a vector orthogonal to v Subscript ivi. Now, we have 
StartLayout 1st Row 1st Column upper A v overTilde Subscript j 2nd Column equals 3rd Column StartFraction 1 Over parallel to v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline parallel to EndFraction left parenthesis upper A v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket upper A v Subscript i Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction 1 Over parallel to v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline parallel to EndFraction left parenthesis c Subscript j Baseline v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket c Subscript i Baseline v Subscript i Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column c Subscript j Baseline StartFraction 1 Over parallel to v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline parallel to EndFraction left parenthesis v Subscript j Baseline minus left angle bracket v Subscript i Baseline comma v Subscript j Baseline right angle bracket v Subscript i Baseline right parenthesis 4th Row 1st Column Blank 2nd Column equals 3rd Column c Subscript j Baseline v overTilde Subscript j Baseline semicolon EndLayoutA˜vj =
1
||vj −<vi, vj>vi|| (Avj −<vi, vj>Avi)
=
1
||vj −<vi, vj>vi|| (cjvj −<vi, vj>civi)
= cj
1
||vj −<vi, vj>vi|| (vj −<vi, vj>vi)
= cj˜vj;

3.9 Eigenanalysis: Canonical Factorizations
177
hence, v overTilde Subscript j˜vj is an eigenvector of A associated with c Subscript jcj. We conclude therefore that 
the eigenvectors of a symmetric matrix can be chosen to be orthogonal. 
A symmetric matrix is orthogonally diagonalizable, because the V in 
Eq. (3.271) can be chosen to be orthogonal, and can be written as 
upper A equals upper V upper C upper V Superscript normal upper T Baseline commaA = VCV T,
(3.276) 
where upper V upper V Superscript normal upper T Baseline equals upper V Superscript normal upper T Baseline upper V equals upper IV V T = V TV = I, and so we also have 
upper V Superscript normal upper T Baseline upper A upper V equals upper C periodV TAV = C.
(3.277) 
Such a matrix is orthogonally similar to a diagonal matrix formed from its 
eigenvalues. 
Not only is a symmetric matrix orthogonally diagonalizable, any matrix 
that is orthogonally diagonalizable is symmetric. This is easy to see. Suppose 
upper B equals upper V upper C upper V Superscript normal upper TB = VCV T, where  V is orthogonal and C is diagonal. Then, 
upper B Superscript normal upper T Baseline equals left parenthesis upper V upper C upper V Superscript normal upper T Baseline right parenthesis Superscript normal upper T Baseline equals upper V upper C upper V Superscript normal upper T Baseline equals upper B semicolonBT = (VCV T)T = V CV T = B;
(3.278) 
hence, B is symmetric. 
Spectral Decomposition 
When A is symmetric and the eigenvectors v Subscript ivi are chosen to be orthonormal, 
upper I equals sigma summation Underscript i Endscripts v Subscript i Baseline v Subscript i Superscript normal upper T Baseline commaI =
E
i
vivT
i ,
(3.279) 
so 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column upper A sigma summation Underscript i Endscripts v Subscript i Baseline v Subscript i Superscript normal upper T 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i Endscripts upper A v Subscript i Baseline v Subscript i Superscript normal upper T 3rd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i Endscripts c Subscript i Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline period EndLayoutA = A
E
i
vivT
i
=
E
i
AvivT
i
=
E
i
civivT
i .
(3.280) 
This representation is called the spectral decomposition of the symmetric ma-
trix A. It is essentially the same as Eq. (3.276), so 
upper A equals upper V upper C upper V Superscript normal upper TA = VCV T
(3.281) 
is also called the spectral decomposition. 
The representation is unique except for the ordering and the choice of 
eigenvectors for eigenvalues with multiplicities greater than 1. If the rank of 
the matrix is r, we arrange the elements so that StartAbsoluteValue c 1 EndAbsoluteValue greater than or equals midline horizontal ellipsis greater than or equals StartAbsoluteValue c Subscript r Baseline EndAbsoluteValue greater than 0|c1| ≥· · · ≥|cr| > 0, and  if  
r less than nr < n, then  c Subscript r plus 1 Baseline equals midline horizontal ellipsis equals c Subscript n Baseline equals 0cr+1 = · · · = cn = 0.

178
3 Basic Properties of Matrices
Note that the matrices in the spectral decomposition are projection matri-
ces that are orthogonal to each other (but they are not orthogonal matrices), 
and they sum to the identity. Let 
upper P Subscript i Baseline equals v Subscript i Baseline v Subscript i Superscript normal upper T Baseline periodPi = vivT
i .
(3.282) 
Then, we have 
StartLayout 1st Row 1st Column upper P Subscript i Baseline upper P Subscript i 2nd Column equals 3rd Column upper P Subscript i Baseline comma 2nd Row 1st Column upper P Subscript i Baseline upper P Subscript j 2nd Column equals 3rd Column 0 for i not equals j comma 3rd Row 1st Column sigma summation Underscript i Endscripts upper P Subscript i 2nd Column equals 3rd Column upper I comma EndLayoutPiPi = Pi,
(3.283) 
PiPj = 0  for  i /= j,
(3.284)
E
i 
Pi = I,
(3.285) 
and the spectral decomposition, 
upper A equals sigma summation Underscript i Endscripts c Subscript i Baseline upper P Subscript i Baseline periodA =
E
i
ciPi.
(3.286) 
The upper P Subscript iPi are called spectral projectors. 
The spectral decomposition also applies to powers of A, 
upper A Superscript k Baseline equals sigma summation Underscript i Endscripts c Subscript i Superscript k Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline commaAk =
E
i
ck
i vivT
i ,
(3.287) 
where k is an integer. If A is nonsingular, k can be negative in the expression 
above. 
The spectral decomposition is one of the most important tools in working 
with symmetric matrices. 
Although we will not prove it here, all diagonalizable matrices have a spec-
tral decomposition in the form of Eq. (3.286) with projection matrices that 
satisfy properties (3.283) through (3.285). These projection matrices cannot 
necessarily be expressed as outer products of eigenvectors, however. The eigen-
values and eigenvectors of a nonsymmetric matrix might not be real, the left 
and right eigenvectors might not be the same, and two eigenvectors might 
not be mutually orthogonal. In the spectral representation upper A equals sigma summation Underscript i Endscripts c Subscript i Baseline upper P Subscript iA = E
i ciPi, how-
ever, if c Subscript jcj is a simple eigenvalue with associated left and right eigenvectors y Subscript jyj
and x Subscript jxj, respectively, then the projection matrix upper P Subscript jPj is x Subscript j Baseline y Subscript j Superscript normal upper H divided by y Subscript j Superscript normal upper H Baseline x Subscript jxjyH
j /yH
j xj. (Note that 
because the eigenvectors may not be real, we take the conjugate transpose.) 
This is Exercise 3.30. 
Kronecker Products of Symmetric Matrices: Orthogonal 
Diagonalization 
If A and B are symmetric, then upper A circled times upper BA ⊗B is symmetric. (We have already men-
tioned this fact, but it is easy to see using Eq. (3.114) on page 117.) 
Now if A and B are symmetric, we can orthogonally diagonalize them 
as in Eq. (3.276): upper A equals upper V upper C upper V Superscript normal upper TA = VCV T and upper B equals upper U upper D upper U Superscript normal upper TB = UDU T. This immediately yields an 
orthogonal diagonalization of the symmetric matrix upper A circled times upper BA ⊗B:

3.9 Eigenanalysis: Canonical Factorizations
179
StartLayout 1st Row 1st Column upper A circled times upper B 2nd Column equals 3rd Column upper V upper C upper V Superscript normal upper T circled times upper U upper D upper U Superscript normal upper T 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis upper V circled times upper U right parenthesis left parenthesis upper C circled times upper D right parenthesis left parenthesis upper V Superscript normal upper T Baseline circled times upper U Superscript normal upper T Baseline right parenthesis comma EndLayoutA ⊗B = VCV T ⊗UDU T
= (V ⊗U)(C ⊗D)(V T ⊗U T),
(3.288) 
which we obtain by using Eq. (3.115) twice. Using Eq. (3.115) again, we have 
left parenthesis upper V circled times upper U right parenthesis left parenthesis upper V Superscript normal upper T Baseline circled times upper U Superscript normal upper T Baseline right parenthesis equals left parenthesis upper V upper V Superscript normal upper T Baseline circled times upper U upper U Superscript normal upper T Baseline right parenthesis equals upper I(V ⊗U)(V T⊗U T) = (V V T⊗UU T) = I and since upper C circled times upper DC⊗D is obviously diagonal, 
Eq. (3.288) is in the orthogonally diagonalized form of Eq. (3.276). 
Quadratic Forms and the Rayleigh Quotient 
Equation (3.280) yields important facts about quadratic forms in A. Because 
V is of full rank, an arbitrary vector x can be written as V b  for some vector 
b. Therefore, for the quadratic form x Superscript normal upper T Baseline upper A xxTAx we have 
StartLayout 1st Row 1st Column x Superscript normal upper T Baseline upper A x 2nd Column equals 3rd Column x Superscript normal upper T Baseline sigma summation Underscript i Endscripts c Subscript i Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline x 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i Endscripts b Superscript normal upper T Baseline upper V Superscript normal upper T Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline upper V b c Subscript i 3rd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i Endscripts b Subscript i Superscript 2 Baseline c Subscript i Baseline period EndLayoutxTAx = xT E
i
civivT
i x
=
E
i
bTV TvivT
i V bci
=
E
i
b2
i ci.
This immediately gives the inequality 
x Superscript normal upper T Baseline upper A x less than or equals max left brace c Subscript i Baseline right brace b Superscript normal upper T Baseline b periodxTAx ≤max{ci}bTb.
(Notice that max left brace c Subscript i Baseline right bracemax{ci} here is not necessarily c 1c1; in the important case when 
all of the eigenvalues are nonnegative, it is, however.) Furthermore, if x not equals 0x /= 0, 
b Superscript normal upper T Baseline b equals x Superscript normal upper T Baseline xbTb = xTx, and we have the important inequality 
StartFraction x Superscript normal upper T Baseline upper A x Over x Superscript normal upper T Baseline x EndFraction less than or equals max left brace c Subscript i Baseline right brace periodxTAx
xTx ≤max{ci}.
(3.289) 
Equality is achieved if x is the eigenvector corresponding to max left brace c Subscript i Baseline right bracemax{ci}, so we  
have 
max Underscript x not equals 0 Endscripts StartFraction x Superscript normal upper T Baseline upper A x Over x Superscript normal upper T Baseline x EndFraction equals max left brace c Subscript i Baseline right brace period max
x/=0
xTAx
xTx = max{ci}.
(3.290) 
If c 1 greater than 0c1 > 0, this is the spectral radius, rho left parenthesis upper A right parenthesisρ(A). 
The expression on the left-hand side in (3.289) as a function of x is called 
the Rayleigh quotient of the symmetric matrix A and is denoted by upper R Subscript upper A Baseline left parenthesis x right parenthesisRA(x): 
StartLayout 1st Row 1st Column upper R Subscript upper A Baseline left parenthesis x right parenthesis 2nd Column equals 3rd Column StartFraction x Superscript normal upper T Baseline upper A x Over x Superscript normal upper T Baseline x EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction left angle bracket x comma upper A x right angle bracket Over left angle bracket x comma x right angle bracket EndFraction period EndLayoutRA(x) = xTAx
xTx
= <x, Ax>
<x, x> .
(3.291) 
Because if x not equals 0x /= 0, x Superscript normal upper T Baseline x greater than 0xTx > 0, it is clear that the Rayleigh quotient is nonnegative 
for all x if and only if A is nonnegative deﬁnite, and it is positive for all x if 
and only if  A is positive deﬁnite.

180
3 Basic Properties of Matrices
The Fourier Expansion 
The v Subscript i Baseline v Subscript i Superscript normal upper TvivT
i matrices in Eq. (3.280) have the property that left angle bracket v Subscript i Baseline v Subscript i Superscript normal upper T Baseline comma v Subscript j Baseline v Subscript j Superscript normal upper T Baseline right angle bracket equals 0<vivT
i , vjvT
j > = 0 for 
i not equals ji /= j and left angle bracket v Subscript i Baseline v Subscript i Superscript normal upper T Baseline comma v Subscript i Baseline v Subscript i Superscript normal upper T Baseline right angle bracket equals 1<vivT
i , vivT
i > = 1, and so the spectral decomposition is a Fourier 
expansion as in Eq. (3.127), and the eigenvalues are Fourier coeﬃcients. From 
Eq. (3.128), we see that the eigenvalues can be represented as the inner product 
c Subscript i Baseline equals left angle bracket upper A comma v Subscript i Baseline v Subscript i Superscript normal upper T Baseline right angle bracket periodci = <A, vivT
i >.
(3.292) 
The eigenvalues c Subscript ici have the same properties as the Fourier coeﬃcients 
in any orthonormal expansion. In particular, the best approximating matrices 
within the subspace of n times nn×n symmetric matrices spanned by StartSet v 1 v 1 Superscript normal upper T Baseline comma ellipsis comma v Subscript n Baseline v Subscript n Superscript normal upper T Baseline EndSet{v1vT
1 , . . . , vnvT
n }
are partial sums of the form of Eq. (3.280). In Sect. 3.12, however, we will 
develop a stronger result for approximation of matrices that does not rely 
on the restriction to this subspace and which applies to general, nonsquare 
matrices. 
Powers of a Symmetric Matrix 
If left parenthesis c comma v right parenthesis(c, v) is an eigenpair of the symmetric matrix A with v Superscript normal upper T Baseline v equals 1vTv = 1, then for any 
k equals 1 comma 2 comma ellipsisk = 1, 2, . . ., 
left parenthesis upper A minus c v v Superscript normal upper T Baseline right parenthesis Superscript k Baseline equals upper A Superscript k Baseline minus c Superscript k Baseline v v Superscript normal upper T Baseline period
(
A −cvvT)k = Ak −ckvvT.
(3.293) 
This follows from induction on k, for it clearly is true for k equals 1k = 1, and if for a 
given k it is true that for k minus 1k −1
left parenthesis upper A minus c v v Superscript normal upper T Baseline right parenthesis Superscript k minus 1 Baseline equals upper A Superscript k minus 1 Baseline minus c Superscript k minus 1 Baseline v v Superscript normal upper T Baseline comma
(
A −cvvT)k−1 = Ak−1 −ck−1vvT,
then by multiplying both sides by left parenthesis upper A minus c v v Superscript normal upper T Baseline right parenthesis(A −cvvT), we see it is true for k: 
StartLayout 1st Row 1st Column left parenthesis upper A minus c v v Superscript normal upper T Baseline right parenthesis Superscript k 2nd Column equals 3rd Column left parenthesis upper A Superscript k minus 1 Baseline minus c Superscript k minus 1 Baseline v v Superscript normal upper T Baseline right parenthesis left parenthesis upper A minus c v v Superscript normal upper T Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper A Superscript k Baseline minus c Superscript k minus 1 Baseline v v Superscript normal upper T Baseline upper A minus c upper A Superscript k minus 1 Baseline v v Superscript normal upper T plus c Superscript k Baseline v v Superscript normal upper T 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper A Superscript k Baseline minus c Superscript k Baseline v v Superscript normal upper T minus c Superscript k Baseline v v Superscript normal upper T plus c Superscript k Baseline v v Superscript normal upper T 4th Row 1st Column Blank 2nd Column equals 3rd Column upper A Superscript k Baseline minus c Superscript k Baseline v v Superscript normal upper T Baseline period EndLayout
(
A −cvvT)k =
(
Ak−1 −ck−1vvT)
(A −cvvT)
= Ak −ck−1vvTA −cAk−1vvT + ckvvT
= Ak −ckvvT −ckvvT + ckvvT
= Ak −ckvvT.
There is a similar result for nonsymmetric square matrices, where w and 
v are left and right eigenvectors, respectively, associated with the same eigen-
value c that can be scaled so that w Superscript normal upper T Baseline v equals 1wTv = 1. (Recall that an eigenvalue of A 
is also an eigenvalue of upper A Superscript normal upper TAT, and  if  w is a left eigenvector associated with the 
eigenvalue c, then  upper A Superscript normal upper T Baseline w equals c wATw = cw.) The only property of symmetry used above 
was that we could scale v Superscript normal upper T Baseline vvTv to be 1; hence, we just need w Superscript normal upper T Baseline v not equals 0wTv /= 0. This is  
clearly true for a diagonalizable matrix (from the deﬁnition). It is also true if c 
is simple (which is somewhat harder to prove). It is thus true for the dominant 
eigenvalue, which is simple, in two important classes of matrices we will con-
sider in Sects. 8.7.1 and 8.7.2, positive matrices and irreducible nonnegative 
matrices.

3.9 Eigenanalysis: Canonical Factorizations
181
If w and v are left and right eigenvectors of A associated with the same 
eigenvalue c and w Superscript normal upper T Baseline v equals 1wTv = 1, then for  k equals 1 comma 2 comma ellipsisk = 1, 2, . . ., 
left parenthesis upper A minus c v w Superscript normal upper T Baseline right parenthesis Superscript k Baseline equals upper A Superscript k Baseline minus c Superscript k Baseline v w Superscript normal upper T Baseline period
(
A −cvwT)k = Ak −ckvwT.
(3.294) 
We can prove this by induction as above. 
The Trace and the Sum of the Eigenvalues 
For a general n times nn × n matrix A with eigenvalues c 1 comma ellipsis comma c Subscript n Baselinec1, . . . , cn, we have  normal t normal r left parenthesis upper A right parenthesis equals sigma summation Underscript i equals 1 Overscript n Endscripts c Subscript itr(A) =
En
i=1 ci. (This is Eq. (3.250).) This is particularly easy to see for symmetric 
matrices because of Eq. (3.276), rewritten as upper V Superscript normal upper T Baseline upper A upper V equals upper CV TAV = C, the diagonal matrix 
of the eigenvalues. For a symmetric matrix, however, we have a stronger result. 
If A is an n times nn × n symmetric matrix with eigenvalues c 1 greater than or equals midline horizontal ellipsis greater than or equals c Subscript nc1 ≥· · · ≥cn, and  U 
is an n times kn × k orthogonal matrix, with k less than or equals nk ≤n, then  
normal t normal r left parenthesis upper U Superscript normal upper T Baseline upper A upper U right parenthesis less than or equals sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript i Baseline periodtr(U TAU) ≤
k
E
i=1
ci.
(3.295) 
To see this, we represent U in terms of the columns of V , which  span  normal upper I normal upper R Superscript nIRn, as  
upper U equals upper V upper XU = V X. Hence, 
StartLayout 1st Row 1st Column normal t normal r left parenthesis upper U Superscript normal upper T Baseline upper A upper U right parenthesis 2nd Column equals 3rd Column normal t normal r left parenthesis upper X Superscript normal upper T Baseline upper V Superscript normal upper T Baseline upper A upper V upper X right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper X Superscript normal upper T Baseline upper C upper X right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i Superscript normal upper T Baseline x Subscript i Baseline c Subscript i Baseline comma EndLayouttr(U TAU) = tr(XTV TAV X)
= tr(XTCX)
=
n
E
i=1
xT
i xi ci,
(3.296) 
where x Subscript i Superscript normal upper TxT
i is the i normal t normal hith row of X. 
Now upper X Superscript normal upper T Baseline upper X equals upper X Superscript normal upper T Baseline upper V Superscript normal upper T Baseline upper V upper X equals upper U Superscript normal upper T Baseline upper U equals upper I Subscript kXTX = XTV TV X = U TU = Ik, so either  x Subscript i Superscript normal upper T Baseline x Subscript i Baseline equals 0xT
i xi = 0 or x Subscript i Superscript normal upper T Baseline x Subscript i Baseline equals 1xT
i xi = 1, 
and sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i Superscript normal upper T Baseline x Subscript i Baseline equals kEn
i=1 xT
i xi = k. Because c 1 greater than or equals midline horizontal ellipsis greater than or equals c Subscript nc1 ≥· · · ≥cn, therefore sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i Superscript normal upper T Baseline x Subscript i Baseline c Subscript i Baseline less than or equals sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript iEn
i=1 xT
i xi ci ≤Ek
i=1 ci, 
and so from Eq. (3.296) we have  normal t normal r left parenthesis upper U Superscript normal upper T Baseline upper A upper U right parenthesis less than or equals sigma summation Underscript i equals 1 Overscript k Endscripts c Subscript itr(U TAU) ≤Ek
i=1 ci. 
3.9.10 Generalized Eigenvalues and Eigenvectors 
The characterization of an eigenvalue as a root of the determinant Eq. (3.242) 
can be extended to deﬁne a generalized eigenvalue of the square matrices A 
and B to be a root in c of the equation 
normal d normal e normal t left parenthesis upper A minus c upper B right parenthesis equals 0det(A −cB) = 0
(3.297) 
if a root exists. 
Equation (3.297) is equivalent to upper A minus c upper BA −cB being singular; that is, for some 
c and some nonzero, ﬁnite v, 
upper A v equals c upper B v periodAv = cBv.
(3.298)

182
3 Basic Properties of Matrices
Such a c (if it exists) is called a generalized eigenvalue of A and B, and  such  
a v (if it exists) is called a generalized eigenvector of A and B. In contrast 
to the existence of eigenvalues of any square matrix with ﬁnite elements, the 
generalized eigenvalues of two matrices may not exist; that is, they may be 
inﬁnite. Notice that if (and only if) c is nonzero and ﬁnite, the roles of A and 
B can be interchanged in Eq. (3.298), and the generalized eigenvalue of B and 
A is 1 divided by c1/c. 
If A and B are n times nn × n (that is, square) and B is nonsingular, then all 
n generalized eigenvalues of A and B exist (and are ﬁnite). These general-
ized eigenvalues are the eigenvalues of upper A upper B Superscript negative 1AB−1 or upper B Superscript negative 1 Baseline upper AB−1A. We see this because 
normal d normal e normal t left parenthesis upper B right parenthesis not equals 0det(B) /= 0, and  so  if  c 0c0 is any of the n (ﬁnite) eigenvalues of upper A upper B Superscript negative 1AB−1 or upper B Superscript negative 1 Baseline upper AB−1A, 
then 0 equals normal d normal e normal t left parenthesis upper A upper B Superscript negative 1 Baseline minus c 0 upper I right parenthesis equals normal d normal e normal t left parenthesis upper B Superscript negative 1 Baseline upper A minus c 0 upper I right parenthesis equals normal d normal e normal t left parenthesis upper A minus c 0 upper B right parenthesis equals 00 = det(AB−1 −c0I) = det(B−1A −c0I) = det(A −c0B) = 0. Likewise, 
we see that any eigenvector of upper A upper B Superscript negative 1AB−1 or upper B Superscript negative 1 Baseline upper AB−1A is a generalized eigenvector of 
A and B. 
In the case of ordinary eigenvalues, we have seen that symmetry of the 
matrix induces some simpliﬁcations. In the case of generalized eigenvalues, 
symmetry together with positive deﬁniteness also yields some useful proper-
ties, which we will discuss in Sect. 6.6. 
Generalized eigenvalue problems often arise in multivariate statistical ap-
plications. Roy’s maximum root statistic, for example, is the largest general-
ized eigenvalue of two matrices that result from operations on a partitioned 
matrix of sums of squares. 
Matrix Pencils 
As c ranges over the reals (or, more generally, the complex numbers), the set 
of matrices of the form upper A minus c upper BA −cB is called the matrix pencil, or just the pencil, 
generated by A and B, denoted as 
left parenthesis upper A comma upper B right parenthesis period(A, B).
(In this deﬁnition, A and B do not need to be square.) A generalized eigenvalue 
of the square matrices A and B is called an eigenvalue of the pencil. 
A pencil is said to be regular if normal d normal e normal t left parenthesis upper A minus c upper B right parenthesisdet(A −cB) is not identically 0 (assuming, 
of course, that normal d normal e normal t left parenthesis upper A minus c upper B right parenthesisdet(A −cB) is deﬁned, meaning A and B are square). An 
interesting special case of a regular pencil is when B is nonsingular. As we 
have seen, in that case, eigenvalues of the pencil left parenthesis upper A comma upper B right parenthesis(A, B) exist (and are ﬁnite) 
and are the same as the ordinary eigenvalues of upper A upper B Superscript negative 1AB−1 or upper B Superscript negative 1 Baseline upper AB−1A, and  the  
ordinary eigenvectors of upper A upper B Superscript negative 1AB−1 or upper B Superscript negative 1 Baseline upper AB−1A are eigenvectors of the pencil left parenthesis upper A comma upper B right parenthesis(A, B). 
3.9.11 Singular Values and the Singular Value Decomposition 
(SVD) 
One of the most useful matrix factorizations in all areas of application of linear 
algebra is the singular value decomposition or SVD. For an n times mn × m matrix A, 
the SVD is

3.9 Eigenanalysis: Canonical Factorizations
183
upper A equals upper U upper D upper V Superscript normal upper T Baseline commaA = UDV T,
(3.299) 
where U is an n times nn × n orthogonal matrix, V is an m times mm × m orthogonal matrix, 
and D is an n times mn × m diagonal matrix with nonnegative entries, arranged in 
nonascending order. (An n times mn×m diagonal matrix has min left parenthesis n comma m right parenthesismin(n, m) elements on the 
diagonal, and all other entries are zero.) The SVD is also called the canonical 
singular value factorization. 
The elements on the diagonal of D, d Subscript idi, are called the singular values of A. 
There are min left parenthesis n comma m right parenthesismin(n, m) singular values, which are arranged so that d 1 greater than or equals midline horizontal ellipsis greater than or equals d Subscript min left parenthesis n comma m right parenthesisd1 ≥· · · ≥
dmin(n,m). 
We can see that the SVD exists for any matrix by forming the correspond-
ing nonnegative deﬁnite Gramian matrix and then using the decomposition 
in Eq. (3.276) on page 177. 
Let A be an n times mn × m matrix, with the n times nn × n Gramian upper A Superscript normal upper T Baseline upper AATA. Let  
upper A Superscript normal upper T Baseline upper A equals upper V upper C upper V Superscript normal upper TATA = V CV T
(3.300) 
be its spectral decomposition (Eq. (3.281)), with the eigenvalues arranged as 
c 1 greater than or equals midline horizontal ellipsis greater than or equals c Subscript n Baseline greater than or equals 0c1 ≥· · · ≥cn ≥0. 
If all eigenvalues are 0, upper A equals 0A = 0 and the decomposition (3.299) exists, but is 
degenerate. Assume that there are r greater than or equals 1r ≥1 positive eigenvalues. If n greater than rn > r, then  
c Subscript r plus 1 Baseline equals midline horizontal ellipsis equals c Subscript n Baseline equals 0cr+1 = · · · = cn = 0. Let  upper D Subscript rDr be the r times rr × r full rank diagonal matrix whose 
elements are d Subscript i i Baseline equals c Subscript i Superscript one halfdii = c
1
2
i if c Subscript i Baseline greater than 0ci > 0. 
Let upper V 1V1 be the n times rn × r matrix consisting of the ﬁrst r columns of V , which  
are the orthonormal eigenvectors corresponding to the nonzero eigenvalues of 
upper A Superscript normal upper T Baseline upper AATA. Note that if  n greater than rn > r, and  if  V is partitioned as left bracket upper V 1 vertical bar upper V 2 right bracket[V1|V2], then  upper A Superscript normal upper T Baseline upper A upper V 2 equals 0ATAV2 = 0
and hence upper V 2 Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A upper V 2 equals 0V T
2 ATAV2 = 0, yielding the result upper A upper V 2 equals 0AV2 = 0. From (3.300), since 
the columns of upper V 1V1 are orthonormal, we have upper A Superscript normal upper T Baseline upper A upper V 1 equals upper V 1 upper D Subscript r Superscript 2ATAV1 = V1D2
r, and hence, 
upper V 1 Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A upper V 1 equals upper D Subscript r Superscript 2 Baseline periodV T
1 ATAV1 = D2
r.
(3.301) 
From Eq. (3.301), form 
upper D Subscript r Superscript negative 1 Baseline upper V 1 Superscript normal upper T Baseline upper A Superscript normal upper T Baseline upper A upper V 1 upper D Subscript r Superscript negative 1 Baseline equals upper I commaD−1
r V T
1 ATAV1D−1
r
= I,
(3.302) 
and let 
upper U 1 equals upper A upper V 1 upper D Subscript r Superscript negative 1 Baseline commaU1 = AV1D−1
r ,
(3.303) 
which, as we have just seen, is an n times rn × r orthonormal matrix. 
Now, if n greater than rn > r, determine matrix upper U 2U2 whose columns are orthonormal to the 
columns of upper U 1U1, and  let  upper U equals left bracket upper U 1 vertical bar upper U 2 right bracketU = [U1|U2]; otherwise, if n equals rn = r, let  upper U equals upper U 1U = U1. Either  
way, U is an orthonormal n times nn×n matrix. Since upper U 2 Superscript normal upper T Baseline upper U 1 equals 0U T
2 U1 = 0, we have  upper U 2 Superscript normal upper T Baseline upper A upper V 1 equals 0U T
2 AV1 = 0. 
Now, consider upper U Superscript normal upper T Baseline upper A upper VU TAV . Depending on whether or not n greater than rn > r, we have  U and 
V as partitioned matrices; otherwise, the operations are the same without the 
necessity of dealing with the submatrices. In the case of n greater than rn > r, we have  
upper U Superscript normal upper T Baseline upper A upper V equals StartBinomialOrMatrix upper U 1 Superscript normal upper T Baseline Choose upper U 2 Superscript normal upper T Baseline EndBinomialOrMatrix upper A left bracket upper V 1 vertical bar upper V 2 right bracket equals Start 2 By 2 Matrix 1st Row 1st Column upper U 1 Superscript normal upper T Baseline upper A upper V 1 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column upper D Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix commaU TAV =
[ U T
1
U T
2
]
A[V1|V2] =
[
U T
1 AV1 0
0
0
]
=
[ Dr 0
0 0
]
,
(3.304)

184
3 Basic Properties of Matrices
and if n equals rn = r, we have the same result except upper D Subscript rDr is the full matrix. Let D be 
upper D Subscript rDr with or without the 0 submatrix partitions, as appropriate, and we have 
upper U Superscript normal upper T Baseline upper A upper V equals upper DU TAV = D, from which  we  get  
upper A equals upper U upper D upper V Superscript normal upper T Baseline commaA = UDV T,
as desired. 
The number of positive entries in D is the same as the rank of A. (We  see  
this by ﬁrst recognizing that the number of nonzero entries of D is obviously 
the rank of D, and multiplication by the full rank matrices U and upper V Superscript normal upper TV T yields 
a product with the same rank from Eqs. (3.169) and  (3.170).) 
From this development, we see that the squares of the singular values of A 
are the eigenvalues of upper A Superscript normal upper T Baseline upper AATA and of upper A upper A Superscript normal upper TAAT, which are necessarily nonnegative. To 
state this more clearly (and using some additional facts developed previously, 
including property 14 on page 162), let A be an n times mn × m matrix with rank r, 
and let d be a singular value of A. We have  
c 
equals d squaredc = d2 is an eigenvalue of upper A Superscript normal upper T Baseline upper AATA; 
c 
equals d squaredc = d2 is an eigenvalue of upper A upper A Superscript normal upper TAAT; 
• if c is a nonzero eigenvalue of upper A Superscript normal upper T Baseline upper AATA, then there is a singular value d of A 
such that d squared equals cd2 = c; and  
• there are r nonzero singular values of A, and  r nonzero eigenvalues of upper A Superscript normal upper T Baseline upper AATA
and of upper A upper A Superscript normal upper TAAT. 
These relationships between singular values and eigenvalues are some of 
the most important properties of singular values and the singular value de-
composition. In particular, from these we can see that the singular value 
decomposition is unique (with the same qualiﬁcations attendant to unique-
ness of eigenvalues and eigenvectors, relating to ordering of the elements and 
selection of vectors corresponding to nonunique values). 
An additional observation is that if A is symmetric, the singular values of 
A are the absolute values of the eigenvalues of A. 
From the factorization (3.299) deﬁning the singular values, we see that 
• the singular values of upper A Superscript normal upper TAT are the same as those of A. 
As pointed out above, for a matrix with more rows than columns, in an 
alternate deﬁnition of the singular value decomposition, the matrix U is n times mn×m
with orthogonal columns, and D is an m times mm×m diagonal matrix with nonnegative 
entries. Likewise, for a matrix with more columns than rows, the singular value 
decomposition can be deﬁned as above but with the matrix V being m times nm × n
with orthogonal columns and D being n times nn × n and diagonal with nonnegative 
entries. 
We often partition D to separate the zero singular values. If the rank of 
the matrix is r, we have  d 1 greater than or equals midline horizontal ellipsis greater than or equals d Subscript r Baseline greater than 0d1 ≥· · · ≥dr > 0 (with the common indexing), and 
if r less than min left parenthesis n comma m right parenthesisr < min(n, m), then  d Subscript r plus 1 Baseline equals midline horizontal ellipsis equals d Subscript min left parenthesis n comma m right parenthesis Baseline equals 0dr+1 = · · · = dmin(n,m) = 0. In this case  
upper D equals Start 2 By 2 Matrix 1st Row 1st Column upper D Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix commaD =
[ Dr 0
0 0
]
,

3.10 Positive Deﬁnite and Nonnegative Deﬁnite Matrices
185
where upper D Subscript r Baseline equals normal d normal i normal a normal g left parenthesis left parenthesis d 1 comma ellipsis comma d Subscript r Baseline right parenthesis right parenthesisDr = diag((d1, . . . , dr)). 
The Fourier Expansion in Terms of the Singular Value 
Decomposition 
From Eq. (3.299), we see that the general matrix A with rank r also has a 
Fourier expansion, similar to Eq. (3.280), in terms of the singular values and 
outer products of the columns of the U and V matrices: 
upper A equals sigma summation Underscript i equals 1 Overscript r Endscripts d Subscript i Baseline u Subscript i Baseline v Subscript i Superscript normal upper T Baseline periodA =
r
E
i=1
diuivT
i .
(3.305) 
This is also called a spectral decomposition. The u Subscript i Baseline v Subscript i Superscript normal upper TuivT
i matrices in Eq. (3.305) 
have the property that left angle bracket u Subscript i Baseline v Subscript i Superscript normal upper T Baseline comma u Subscript j Baseline v Subscript j Superscript normal upper T Baseline right angle bracket equals 0<uivT
i , ujvT
j > = 0 for i not equals ji /= j and left angle bracket u Subscript i Baseline v Subscript i Superscript normal upper T Baseline comma u Subscript i Baseline v Subscript i Superscript normal upper T Baseline right angle bracket equals 1<uivT
i , uivT
i > = 1, and  
so the spectral decomposition is a Fourier expansion as in Eq. (3.127), and the 
singular values are Fourier coeﬃcients. 
The singular values d Subscript idi have the same properties as the Fourier coeﬃcients 
in any orthonormal expansion. For example, from Eq. (3.128), we see that the 
singular values can be represented as the inner products of A with the outer 
products of the corresponding columns in U and V 
d Subscript i Baseline equals left angle bracket upper A comma u Subscript i Baseline v Subscript i Superscript normal upper T Baseline right angle bracket perioddi = <A, uivT
i >.
After we have discussed matrix norms in the next section, we will formulate 
Parseval’s identity for this Fourier expansion. 
The spectral decomposition is a rank-one decomposition, since each of the 
matrices u Subscript i Baseline v Subscript i Superscript normal upper TuivT
i has rank one. 
The Singular Value Decomposition and the Orthogonally Diagonal 
Factorization 
If the matrix A has an orthogonally diagonal factorization, that is, if A is 
symmetric, then from Eqs. (3.299) and  (3.271), it is clear that the singular 
value decomposition of A is similar to its orthogonally diagonal factorization, 
except for possible diﬀerences in signs. Likewise, the spectral decomposition 
in Eq. (3.305) is the same as the spectral decomposition in Eq. (3.280), except 
for possible diﬀerences in signs. 
3.10 Positive Deﬁnite and Nonnegative Deﬁnite Matrices 
The factorization of symmetric matrices in Eq. (3.276) yields some useful prop-
erties of positive deﬁnite and nonnegative deﬁnite matrices (introduced on 
page 113). We will brieﬂy discuss these properties here and then return to 
the subject in Sect. 8.3 and discuss more properties of positive deﬁnite and 
nonnegative deﬁnite matrices.

186
3 Basic Properties of Matrices
3.10.1 Eigenvalues of Positive and Nonnegative Deﬁnite Matrices 
In this book, we use the terms “nonnegative deﬁnite” and “positive deﬁnite” 
only for real symmetric matrices, so the eigenvalues of nonnegative deﬁnite or 
positive deﬁnite matrices are real. 
Any real symmetric matrix is positive (nonnegative) deﬁnite if and only 
if all of its eigenvalues are positive (nonnegative). We can see this using the 
factorization (3.276) of a symmetric matrix. One factor is the diagonal matrix 
C of the eigenvalues, and the other factors are orthogonal. Hence, for any x, 
we have x Superscript normal upper T Baseline upper A x equals x Superscript normal upper T Baseline upper V upper C upper V Superscript normal upper T Baseline x equals y Superscript normal upper T Baseline upper C yxTAx = xTVCV Tx = yTCy, where  y equals upper V Superscript normal upper T Baseline xy = V Tx, and  so  
x Superscript normal upper T Baseline upper A x greater than left parenthesis greater than or equals right parenthesis 0xTAx > (≥) 0
if and only if 
y Superscript normal upper T Baseline upper C y greater than left parenthesis greater than or equals right parenthesis 0 periodyTCy > (≥) 0.
This, together with the resulting inequality (3.172) on page 133 implies 
that if P is a nonsingular matrix and D is a diagonal matrix, upper P Superscript normal upper T Baseline upper D upper PP TDP is positive 
(nonnegative) if and only if the elements of D are positive (nonnegative). 
We have seen on page 162 that the eigenvalues of Gramian matrices, which 
are symmetric, are nonnegative; hence, a Gramian matrix is nonnegative def-
inite. 
A matrix (whether symmetric or not and whether real or not) all of whose 
eigenvalues have positive real parts is said to be positive stable. Positive stabil-
ity is an important property in some applications, such as numerical solution 
of systems of nonlinear diﬀerential equations. Clearly, a positive deﬁnite ma-
trix is positive stable. 
3.10.2 Inverse of Positive Deﬁnite Matrices 
If (and only if) A is positive deﬁnite, then upper A Superscript negative 1A−1 is positive deﬁnite. We see 
this by writing upper A equals upper V upper C upper V Superscript normal upper TA = VCV T as in Eq. (3.276), giving upper A Superscript negative 1 Baseline equals upper V upper C Superscript negative 1 Baseline upper V Superscript normal upper TA−1 = VC−1V T; hence, 
upper A Superscript negative 1A−1 is positive deﬁnite because the elements of upper C Superscript negative 1C−1 are positive. 
3.10.3 Diagonalization of Positive Deﬁnite Matrices 
If A is positive deﬁnite, the elements of the diagonal matrix C in Eq. (3.276) 
are positive, and so their square roots can be absorbed into V to form a 
nonsingular matrix P. The diagonalization in Eq. (3.277), upper V Superscript normal upper T Baseline upper A upper V equals upper CV TAV = C, can  
therefore be reexpressed as 
upper P Superscript normal upper T Baseline upper A upper P equals upper I periodP TAP = I.
(3.306)

3.11 Matrix Norms
187
3.10.4 Square Roots of Positive and Nonnegative Deﬁnite Matrices 
The factorization (3.276) together with the nonnegativity of the eigenvalues 
of positive and nonnegative deﬁnite matrices allows us to deﬁne a square root 
of such a matrix. 
Let A be a nonnegative deﬁnite matrix and let V and C be as in 
Eq. (3.276); that is, upper A equals upper V upper C upper V Superscript normal upper TA = VCV T. Now, let  S be a diagonal matrix whose 
elements are the nonnegative square roots of the corresponding elements of 
C. Then  left parenthesis upper V upper S upper V Superscript normal upper T Baseline right parenthesis squared equals upper A(VSV T)2 = A; hence, we write 
upper A Superscript one half Baseline equals upper V upper S upper V Superscript normal upper TA
1
2 = VSV T
(3.307) 
and call this matrix the square root of A. 
Note that we deﬁne square root of a matrix only for nonnegative deﬁnite 
matrices, and the deﬁnition is in terms of Eq. (3.307); it does not apply to 
any matrix M, such that MM equals the given matrix. This deﬁnition of the 
square root of a matrix is an instance of Eq. (3.274) with  f left parenthesis x right parenthesis equals StartRoot x EndRootf(x) = √x. We also  
can similarly deﬁne upper A Superscript StartFraction 1 Over r EndFractionA
1
r for r greater than 0r > 0. 
We see immediately that upper A Superscript one halfA
1
2 is symmetric because A is symmetric. 
Notice that if upper A equals upper I 2A = I2 (the identity) and if upper J equals Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 1 2nd Row 1st Column 1 2nd Column 0 EndMatrixJ =
[
0 1
1 0
]
, then  upper J squared equals upper AJ2 = A, but 
by the deﬁnition, J is not a square root of A. 
If A is positive deﬁnite, upper A Superscript negative 1A−1 exists and is positive deﬁnite. It therefore has 
a square root, which we denote as upper A Superscript negative one halfA−1
2 . 
The square roots are nonnegative, and so upper A Superscript one halfA
1
2 is nonnegative deﬁnite. Fur-
thermore, upper A Superscript one halfA
1
2 and upper A Superscript negative one halfA−1
2 are positive deﬁnite if A is positive deﬁnite. 
In Sect. 4.7.1, we will show that this upper A Superscript one halfA
1
2 is unique, so our reference to it as 
the square root is appropriate. (There is occasionally some ambiguity in the 
terms “square root” and “second root” and the symbols used to denote them. 
If x is a nonnegative scalar, the usual meaning of its square root, denoted by 
StartRoot x EndRoot√x, is a nonnegative number, while its second roots, which may be denoted by 
x Superscript one halfx
1
2 , are usually considered to be either of the numbers plus or minus StartRoot x EndRoot±√x. In our notation 
upper A Superscript one halfA
1
2 , we mean  the square root; that is, the matrix above, if it exists. Otherwise, 
we say the square root of the matrix does not exist.) 
3.11 Matrix Norms 
Norms on matrices are scalar functions of matrices with the three properties 
on page 35 that deﬁne a norm in general. Matrix norms are often required 
to have another property, called the consistency property, in addition to the 
properties listed on page 35, which we repeat here for convenience. Assume A 
and B are matrices conformable for the operations shown. 
1. Nonnegativity and mapping of the identity: 
if upper A not equals 0A /= 0, then  parallel to upper A parallel to greater than 0||A|| > 0, and  parallel to 0 parallel to equals 0||0|| = 0.

188
3 Basic Properties of Matrices
2. Relation of scalar multiplication to real multiplication: 
parallel to a upper A parallel to equals StartAbsoluteValue a EndAbsoluteValue parallel to upper A parallel to||aA|| = |a| ||A|| for real a. 
3. Triangle inequality: 
parallel to upper A plus upper B parallel to less than or equals parallel to upper A parallel to plus parallel to upper B parallel to||A + B|| ≤||A|| + ||B||. 
4. Consistency property: 
parallel to upper A upper B parallel to less than or equals parallel to upper A parallel to parallel to upper B parallel to||AB|| ≤||A|| ||B||. 
Some people do not require the consistency property for a matrix norm. Most 
useful matrix norms have the property, however, and we will consider it to be 
a requirement in the deﬁnition. The consistency property for multiplication is 
similar to the triangular inequality for addition. 
Any function from normal upper I normal upper R Superscript n times mIRn×m to normal upper I normal upper RIR that satisﬁes these four properties is a 
matrix norm. 
A matrix norm, as any norm, is necessarily convex. (See page 35.) 
We note that the four properties of a matrix norm do not imply that it 
is invariant to transposition of a matrix, and in general, parallel to upper A Superscript normal upper T Baseline parallel to not equals parallel to upper A parallel to||AT|| /= ||A||. Some  
matrix norms are the same for the transpose of a matrix as for the original 
matrix. For instance, because of the property of the matrix inner product 
given in Eq. (3.124), we see that a norm deﬁned by that inner product would 
be invariant to transposition. 
For a square matrix A, the consistency property for a matrix norm yields 
parallel to upper A Superscript k Baseline parallel to less than or equals parallel to upper A parallel to Superscript k||Ak|| ≤||A||k
(3.308) 
for any positive integer k. 
A matrix norm parallel to dot parallel to||·|| is orthogonally invariant if A and B being orthogonally 
similar implies parallel to upper A parallel to equals parallel to upper B parallel to||A|| = ||B||. 
3.11.1 Matrix Norms Induced from Vector Norms 
Some matrix norms are deﬁned in terms of vector norms. For clarity, we will 
denote a vector norm as parallel to dot parallel to Subscript normal v|| · ||v and a matrix norm as parallel to dot parallel to Subscript normal upper M|| · ||M. (This notation is 
meant to be generic; that is, parallel to dot parallel to Subscript normal v||·||v represents any vector norm.) For the matrix 
upper A element of normal upper I normal upper R Superscript n times mA ∈IRn×m, the matrix norm parallel to dot parallel to Subscript normal upper M|| · ||M induced by parallel to dot parallel to Subscript normal v|| · ||v is deﬁned by 
parallel to upper A parallel to Subscript normal upper M Baseline equals max Underscript x not equals 0 Endscripts StartFraction parallel to upper A x parallel to Subscript normal v Baseline Over parallel to x parallel to Subscript normal v Baseline EndFraction period||A||M = max
x/=0
||Ax||v
||x||v
.
(3.309) 
(Note that there are some minor subtleties here; upper A x element of normal upper I normal upper R Superscript nAx ∈IRn while x element of normal upper I normal upper R Superscript mx ∈IRm, so  
the two vector norms are actually diﬀerent. Of course, in practice, an induced 
norm is deﬁned in terms of vector norms of the same “type,” for example normal upper L Subscript pLp
norms with the same p.) 
An induced matrix norm is also sometimes called an operator norm. 
It is easy to see that an induced norm is indeed a matrix norm. The ﬁrst 
three properties of a norm are immediate, and the consistency property can 
be veriﬁed by applying the deﬁnition (3.309) to  AB and replacing Bx with y; 
that is, using Ay.

3.11 Matrix Norms
189
We usually drop the v or M subscript, and the notation parallel to dot parallel to|| · || is overloaded 
to mean either a vector or matrix norm. (Overloading of symbols occurs in 
many contexts, and we usually do not even recognize that the meaning is 
context-dependent. In computer language design, overloading must be recog-
nized explicitly because the language speciﬁcations must be explicit.) 
The induced norm of A given in Eq. (3.309) is sometimes called the maxi-
mum magniﬁcation by A. The expression looks very similar to the maximum 
eigenvalue, and indeed it is in some cases. 
For any vector norm and its induced matrix norm, we see from Eq. (3.309) 
that 
parallel to upper A x parallel to less than or equals parallel to upper A parallel to parallel to x parallel to||Ax|| ≤||A|| ||x||
(3.310) 
because parallel to x parallel to greater than or equals 0||x|| ≥0. 
Lp Matrix Norms 
The matrix norms that correspond to the normal upper L Subscript pLp vector norms are deﬁned for the 
n times mn × m matrix A as 
parallel to upper A parallel to Subscript p Baseline equals max Underscript parallel to x parallel to Subscript p Baseline equals 1 Endscripts parallel to upper A x parallel to Subscript p Baseline period||A||p = max
||x||p=1 ||Ax||p.
(3.311) 
(Notice that the restriction on parallel to x parallel to Subscript p||x||p makes this an induced norm as deﬁned 
in Eq. (3.309). Notice also the overloading of the symbols; the norm on the 
left that is being deﬁned is a matrix norm, whereas those on the right of the 
equation are vector norms.) It is clear that the normal upper L Subscript pLp matrix norms satisfy the 
consistency property, because they are induced norms. 
The normal upper L 1L1 and normal upper L Subscript normal infinityL∞norms have interesting simpliﬁcations of Eq. (3.309): 
parallel to upper A parallel to Subscript 1 Baseline equals max Underscript j Endscripts sigma summation Underscript i Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue comma||A||1 = max
j
E
i
|aij|,
(3.312) 
so the normal upper L 1L1 is also called the column-sum norm; and  
parallel to upper A parallel to Subscript normal infinity Baseline equals max Underscript i Endscripts sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue comma||A||∞= max
i
E
j
|aij|,
(3.313) 
so the normal upper L Subscript normal infinityL∞is also called the row-sum norm. We see these relationships by 
considering the normal upper L Subscript pLp norm of the vector v equals upper A xv = Ax, 
v equals left parenthesis a Subscript 1 asterisk Superscript normal upper T Baseline x comma ellipsis comma a Subscript n asterisk Superscript normal upper T Baseline x right parenthesis commav = (aT
1∗x, . . . , aT
n∗x),
where a Subscript i asteriskai∗is the i normal t normal hith row of A, with the restriction that parallel to x parallel to Subscript p Baseline equals 1||x||p = 1. (This is a 
“column” vector; recall my convention for writing vectors.) 
The normal upper L Subscript pLp norm of this vector is based on the absolute values of the elements; 
that is, on StartAbsoluteValue a Subscript i asterisk Superscript normal upper T Baseline x EndAbsoluteValue equals StartAbsoluteValue sigma summation Underscript j Endscripts a Subscript i j Baseline x Subscript j Baseline EndAbsoluteValue|aT
i∗x| = | E
j aijxj| for i equals 1 comma ellipsis comma ni = 1, . . . , n. Because we are free to choose 
x (subject to the restriction that parallel to x parallel to Subscript p Baseline equals 1||x||p = 1), for a given i, we can  choose  the  
sign of each x Subscript jxj to maximize the overall expression. For example, for a ﬁxed i,

190
3 Basic Properties of Matrices
we can choose each x Subscript jxj to have the same sign as a Subscript i jaij, and  so  StartAbsoluteValue sigma summation Underscript j Endscripts a Subscript i j Baseline x Subscript j Baseline EndAbsoluteValue| E
j aijxj| is the 
same as sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue StartAbsoluteValue x Subscript j Baseline EndAbsoluteValueE
j |aij| |xj|. 
For the column-sum norm, the normal upper L 1L1 norm of v is sigma summation Underscript i Endscripts StartAbsoluteValue a Subscript i asterisk Superscript normal upper T Baseline x EndAbsoluteValueE
i |aT
i∗x|. The elements 
of x are chosen to maximize this under the restriction that sigma summation StartAbsoluteValue x Subscript j Baseline EndAbsoluteValue equals 1E |xj| = 1. The  
maximum of the expression is attained by setting x Subscript k Baseline equals 1xk = 1, where  k is such 
that sigma summation Underscript i Endscripts StartAbsoluteValue a Subscript i k Baseline EndAbsoluteValue greater than or equals sigma summation Underscript i Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValueE
i |aik| ≥E
i |aij|, for  j equals 1 comma ellipsis comma mj = 1, . . . , m, and  x Subscript q Baseline equals 0xq = 0 for q equals 1 comma ellipsis mq = 1, . . . m and 
q not equals kq /= k. (If there is no unique k, any choice will yield the same result.) This 
yields Eq. (3.312). 
For the row-sum norm, the normal upper L Subscript normal infinityL∞norm of v is 
max Underscript i Endscripts StartAbsoluteValue a Subscript i asterisk Superscript normal upper T Baseline x EndAbsoluteValue equals max Underscript i Endscripts sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue StartAbsoluteValue x Subscript j Baseline EndAbsoluteValuemax
i
|aT
i∗x| = max
i
E
j
|aij| |xj|
when the sign of x Subscript jxj is chosen appropriately (for a given i). The elements of 
x must be chosen so that max StartAbsoluteValue x Subscript j Baseline EndAbsoluteValue equals 1max |xj| = 1; hence, each x Subscript jxj is chosen as plus or minus 1±1. The  
maximum StartAbsoluteValue a Subscript i asterisk Superscript normal upper T Baseline x EndAbsoluteValue|aT
i∗x| is attained by setting x Subscript j Baseline equals normal s normal i normal g normal n left parenthesis a Subscript k j Baseline right parenthesisxj = sign(akj), for  j equals 1 comma ellipsis mj = 1, . . . m, where  
k is such that sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript k j Baseline EndAbsoluteValue greater than or equals sigma summation Underscript j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValueE
j |akj| ≥E
j |aij|, for  i equals 1 comma ellipsis comma ni = 1, . . . , n. This yields Eq. (3.313). 
From Eqs. (3.312) and  (3.313), we see that 
parallel to upper A Superscript normal upper T Baseline parallel to Subscript normal infinity Baseline equals parallel to upper A parallel to Subscript 1 Baseline period||AT||∞= ||A||1.
(3.314) 
Alternative formulations of the normal upper L 2L2 norm of a matrix are not so obvious 
from Eq. (3.311). It is related to the eigenvalues (or the singular values) of the 
matrix. The normal upper L 2L2 matrix norm is related to the spectral radius (page 164): 
parallel to upper A parallel to Subscript 2 Baseline equals StartRoot rho left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis EndRoot comma||A||2 =
/
ρ(ATA),
(3.315) 
(see Exercise 3.34, page 212). Because of this relationship, the normal upper L 2L2 matrix norm 
is also called the spectral norm. 
From the invariance of the singular values to matrix transposition, we 
see that positive eigenvalues of upper A Superscript normal upper T Baseline upper AATA are the same as those of upper A upper A Superscript normal upper TAAT; hence, 
parallel to upper A Superscript normal upper T Baseline parallel to Subscript 2 Baseline equals parallel to upper A parallel to Subscript 2||AT||2 = ||A||2. 
For Q orthogonal, the normal upper L 2L2 vector norm has the important property 
parallel to upper Q x parallel to Subscript 2 Baseline equals parallel to x parallel to Subscript 2||Qx||2 = ||x||2
(3.316) 
(see Exercise 3.35a, page 212). For this reason, an orthogonal matrix is some-
times called an isometric matrix. By the proper choice of x, it is easy to see  
from Eq. (3.316) that  
parallel to upper Q parallel to Subscript 2 Baseline equals 1 period||Q||2 = 1.
(3.317) 
Also from this we see that if A and B are orthogonally similar, then parallel to upper A parallel to Subscript 2 Baseline equals parallel to upper B parallel to Subscript 2||A||2 =
||B||2; hence, the spectral matrix norm is orthogonally invariant. 
The normal upper L 2L2 matrix norm is a Euclidean-type norm since it is induced by the 
Euclidean vector norm (but it is not called the Euclidean matrix norm; see 
below).

3.11 Matrix Norms
191
L1, L2, and  L∞ Norms of Symmetric Matrices 
For a symmetric matrix A, we have the obvious relationships 
parallel to upper A parallel to Subscript 1 Baseline equals parallel to upper A parallel to Subscript normal infinity||A||1 = ||A||∞
(3.318) 
and, from Eq. (3.315), 
parallel to upper A parallel to Subscript 2 Baseline equals rho left parenthesis upper A right parenthesis period||A||2 = ρ(A).
(3.319) 
3.11.2 The Frobenius Norm—The “Usual” Norm 
The Frobenius norm is deﬁned as 
parallel to upper A parallel to Subscript normal upper F Baseline equals StartRoot sigma summation Underscript i comma j Endscripts a Subscript i j Superscript 2 Baseline EndRoot period||A||F =
/E
i,j
a2
ij.
(3.320) 
It is easy to see that this measure has the consistency property (Exercise 3.37), 
as a norm must. The Frobenius norm is sometimes called the Euclidean matrix 
norm and denoted by parallel to dot parallel to Subscript normal upper E|| · ||E, although the normal upper L 2L2 matrix norm is more directly 
based on the Euclidean vector norm, as we mentioned above. We will usually 
use the notation parallel to dot parallel to Subscript normal upper F|| · ||F to denote the Frobenius norm. Occasionally, we use 
parallel to dot parallel to|| · || without the subscript to denote the Frobenius norm, but usually the 
symbol without the subscript indicates that any norm could be used in the 
expression. The Frobenius norm is also often called the “usual norm,” which 
emphasizes the fact that it is one of the most useful matrix norms. Other 
names sometimes used to refer to the Frobenius norm are Hilbert–Schmidt 
norm and Schur norm. 
From the deﬁnition, we have parallel to upper A Superscript normal upper T Baseline parallel to Subscript normal upper F Baseline equals parallel to upper A parallel to Subscript normal upper F||AT||F = ||A||F. We have seen that the normal upper L 2L2
matrix norm also has this property. 
Another important property of the Frobenius norm that is obvious from 
the deﬁnition is 
StartLayout 1st Row 1st Column parallel to upper A parallel to Subscript normal upper F 2nd Column equals 3rd Column StartRoot normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis EndRoot 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartRoot left angle bracket upper A comma upper A right angle bracket EndRoot semicolon EndLayout||A||F =
/
tr(ATA)
(3.321) 
=
/
<A, A>;
(3.322) 
that is, 
• the Frobenius norm is the norm that arises from the matrix inner product 
(see page 118). 
The complete vector space normal upper I normal upper R Superscript n times mIRn×m with the Frobenius norm is therefore a 
Hilbert space. 
Another thing worth noting for a square A is the relationship of the Frobe-
nius norm to the eigenvalues c Subscript ici of A: 
parallel to upper A parallel to Subscript normal upper F Baseline equals StartRoot sigma summation c Subscript i Baseline c overbar Subscript i Baseline EndRoot comma||A||F =
/E
ci¯ci,
(3.323)

192
3 Basic Properties of Matrices
and if A is also symmetric, 
parallel to upper A parallel to Subscript normal upper F Baseline equals StartRoot sigma summation c Subscript i Superscript 2 Baseline EndRoot comma||A||F =
/E
c2
i ,
(3.324) 
These follow from Eq. (3.321) and  Eq. (3.250) on page 163. 
Similar to deﬁning the angle between two vectors in terms of the inner 
product and the norm arising from the inner product, we deﬁne the angle 
between two matrices A and B of the same size and shape as 
normal a normal n normal g normal l normal e left parenthesis upper A comma upper B right parenthesis equals cosine Superscript negative 1 Baseline left parenthesis StartFraction left angle bracket upper A comma upper B right angle bracket Over parallel to upper A parallel to Subscript normal upper F Baseline parallel to upper B parallel to Subscript normal upper F Baseline EndFraction right parenthesis periodangle(A, B) = cos−1
(
<A, B>
||A||F||B||F
)
.
(3.325) 
If Q is an n times mn × m orthogonal matrix, then 
parallel to upper Q parallel to Subscript normal upper F Baseline equals StartRoot min left parenthesis n comma m right parenthesis EndRoot||Q||F =
/
min(n, m)
(3.326) 
(see Eq. (3.238)). 
If A and B are orthogonally similar (see Eq. (3.265)), then 
parallel to upper A parallel to Subscript normal upper F Baseline equals parallel to upper B parallel to Subscript normal upper F Baseline semicolon||A||F = ||B||F;
that is, the Frobenius norm is an orthogonally invariant norm. To see this, let 
upper A equals upper Q Superscript normal upper T Baseline upper B upper QA = QTBQ, where  Q is an orthogonal matrix. Then, 
StartLayout 1st Row 1st Column parallel to upper A parallel to Subscript normal upper F Superscript 2 2nd Column equals 3rd Column normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper Q Superscript normal upper T Baseline upper B Superscript normal upper T Baseline upper Q upper Q Superscript normal upper T Baseline upper B upper Q right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper B Superscript normal upper T Baseline upper B upper Q upper Q Superscript normal upper T Baseline right parenthesis 4th Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper B Superscript normal upper T Baseline upper B right parenthesis 5th Row 1st Column Blank 2nd Column equals 3rd Column parallel to upper B parallel to Subscript normal upper F Superscript 2 Baseline period EndLayout||A||2
F = tr(ATA)
= tr(QTBTQQTBQ)
= tr(BTBQQT)
= tr(BTB)
= ||B||2
F.
(The norms are nonnegative, of course, and so equality of the squares is suf-
ﬁcient.) 
The Frobenius Norm and the Singular Values 
Several important properties result because the Frobenius norm arises from 
an inner product. For example, following the Fourier expansion in terms of 
the singular value decomposition, Eq. (3.305), we mentioned that the singular 
values have the general properties of Fourier coeﬃcients; for example, they 
satisfy Parseval’s identity, Eq. (2.59), on page 51. This identity states that the 
sum of the squares of the Fourier coeﬃcients is equal to the square of the norm 
that arises from the inner product used in the Fourier expansion. Hence, we 
have the important property of the Frobenius norm that it is the normal upper L 2L2 norm of 
the vector of singular values of the matrix. For the n times mn × m matrix A, let  d be 
the min left parenthesis n comma m right parenthesismin(n, m)-vector of singular values of A. Then  
parallel to upper A parallel to Subscript normal upper F Superscript 2 Baseline equals parallel to d parallel to Subscript 2 Baseline period||A||2
F = ||d||2.
(3.327) 
Compare Eqs. (3.323) and  (3.324) for square matrices.

3.11 Matrix Norms
193
3.11.3 Other Matrix Norms 
There are two diﬀerent ways of generalizing the Frobenius norm. One is a 
simple generalization of the deﬁnition in Eq. (3.320). For p greater than or equals 1p ≥1, it is the  
Frobenius p norm: 
parallel to upper A parallel to Subscript normal upper F Sub Subscript p Subscript Baseline equals left parenthesis sigma summation Underscript i comma j Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript 1 divided by p Baseline period||A||Fp =
⎛
⎝E
i,j
|aij|p
⎞
⎠
1/p
.
(3.328) 
Some people refer to this as the normal upper L Subscript pLp norm of the matrix. As we have seen, the 
normal upper L Subscript pLp matrix norm is diﬀerent, but there is a simple relationship of the Frobenius 
p matrix norm to the normal upper L Subscript pLp vector norm: 
parallel to upper A parallel to Subscript normal upper F Sub Subscript p Subscript Baseline equals parallel to normal v normal e normal c left parenthesis upper A right parenthesis parallel to Subscript p Baseline period||A||Fp = ||vec(A)||p.
(3.329) 
This relationship of the matrix norm to a vector norm sometimes makes com-
putational problems easier. 
The Frobenius 2 norm is the ordinary Frobenius norm. 
Another generalization of the Frobenius norm arises from its relation to 
the singular values given in Eq. (3.327). For p greater than or equals 1p ≥1, it is the  Schatten p norm: 
parallel to upper A parallel to Subscript normal upper S Sub Subscript p Subscript Baseline equals parallel to d parallel to Subscript p Baseline comma||A||Sp = ||d||p,
(3.330) 
where d is the vector of singular values of A. 
The Schatten 2 norm is the ordinary Frobenius norm. 
The Schatten 1 norm is called the nuclear norm (because of its relation-
ship to “nuclear operators,” which are linear operators that preserve local 
convexity). It is also sometimes called the trace norm, because 
parallel to d parallel to Subscript 1 Baseline equals normal t normal r left parenthesis left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis Superscript one half Baseline right parenthesis period||d||1 = tr
(
(ATA)
1
2
)
.
(3.331) 
The Schatten normal infinity∞norm is the spectral norm. 
3.11.4 Matrix Norm Inequalities 
There is an equivalence among any two matrix norms similar to that of expres-
sion (2.38) for vector norms (over ﬁnite-dimensional vector spaces). If parallel to dot parallel to Subscript a|| · ||a
and parallel to dot parallel to Subscript b|| · ||b are matrix norms, then there are positive numbers r and s such 
that, for any matrix A, 
r parallel to upper A parallel to Subscript b Baseline less than or equals parallel to upper A parallel to Subscript a Baseline less than or equals s parallel to upper A parallel to Subscript b Baseline periodr||A||b ≤||A||a ≤s||A||b.
(3.332) 
We will not prove this result in general but, in Exercise 3.39, ask the reader 
to do so for matrix norms induced by vector norms. These induced norms 
include the matrix normal upper L Subscript pLp norms of course. 
If A is an n times mn × m real matrix, we have some speciﬁc instances of (3.332):

194
3 Basic Properties of Matrices
StartLayout 1st Row 1st Column parallel to upper A parallel to Subscript normal infinity 2nd Column less than or equals 3rd Column StartRoot m EndRoot parallel to upper A parallel to Subscript normal upper F Baseline comma 2nd Row 1st Column Blank 3rd Row 1st Column parallel to upper A parallel to Subscript normal upper F 2nd Column less than or equals 3rd Column StartRoot min left parenthesis n comma m right parenthesis EndRoot parallel to upper A parallel to Subscript 2 Baseline comma 4th Row 1st Column Blank 5th Row 1st Column parallel to upper A parallel to Subscript 2 2nd Column less than or equals 3rd Column StartRoot m EndRoot parallel to upper A parallel to Subscript 1 Baseline comma 6th Row 1st Column Blank 7th Row 1st Column parallel to upper A parallel to Subscript 1 2nd Column less than or equals 3rd Column StartRoot n EndRoot parallel to upper A parallel to Subscript 2 Baseline comma 8th Row 1st Column Blank 9th Row 1st Column parallel to upper A parallel to Subscript 2 2nd Column less than or equals 3rd Column parallel to upper A parallel to Subscript normal upper F Baseline comma 10th Row 1st Column Blank 11th Row 1st Column parallel to upper A parallel to Subscript normal upper F 2nd Column less than or equals 3rd Column StartRoot n EndRoot parallel to upper A parallel to Subscript normal infinity Baseline period EndLayout||A||∞≤√m ||A||F,
(3.333)
||A||F ≤
/
min(n, m) ||A||2,
(3.334)
||A||2 ≤√m ||A||1,
(3.335)
||A||1 ≤√n ||A||2,
(3.336)
||A||2 ≤||A||F,
(3.337)
||A||F ≤√n ||A||∞.
(3.338) 
See Exercise 3.40 on page 213. 
Compare these inequalities with those for normal upper L Subscript pLp vector norms on page 38. 
Recall speciﬁcally that for vector normal upper L Subscript pLp norms we had the useful fact that for a 
given x and for p greater than or equals 1p ≥1, parallel to x parallel to Subscript p||x||p is a nonincreasing function of p; and speciﬁcally 
we had inequality (2.33): 
parallel to x parallel to Subscript normal infinity Baseline less than or equals parallel to x parallel to Subscript 2 Baseline less than or equals parallel to x parallel to Subscript 1 Baseline period||x||∞≤||x||2 ≤||x||1.
There is a related inequality involving matrices: 
parallel to upper A parallel to Subscript 2 Superscript 2 Baseline less than or equals parallel to upper A parallel to Subscript 1 Baseline parallel to upper A parallel to Subscript normal infinity Baseline period||A||2
2 ≤||A||1||A||∞.
(3.339) 
3.11.5 The Spectral Radius 
The spectral radius is the appropriate measure of the condition of a square ma-
trix for certain iterative algorithms. Except in the case of symmetric matrices, 
as shown in Eq. (3.319), the spectral radius is not a norm (see Exercise 3.41a). 
We have for any norm parallel to dot parallel to|| · || and any square matrix A that 
rho left parenthesis upper A right parenthesis less than or equals parallel to upper A parallel to periodρ(A) ≤||A||.
(3.340) 
To see this, we consider the associated eigenvalue and eigenvector, c Subscript ici and v Subscript ivi, 
and form the matrix upper V equals left bracket v Subscript i Baseline StartAbsoluteValue 0 EndAbsoluteValue midline horizontal ellipsis vertical bar 0 right bracketV = [vi|0| · · · |0]. This yields c Subscript i Baseline upper V equals upper A upper VciV = AV , and  by  the  
consistency property of any matrix norm, 
StartLayout 1st Row 1st Column StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue parallel to upper V parallel to 2nd Column equals 3rd Column parallel to c Subscript i Baseline upper V parallel to 2nd Row 1st Column Blank 2nd Column equals 3rd Column parallel to upper A upper V parallel to 3rd Row 1st Column Blank 2nd Column less than or equals 3rd Column parallel to upper A parallel to parallel to upper V parallel to comma EndLayout|ci|||V || = ||ciV ||
= ||AV ||
≤||A|| ||V ||,
or 
StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue less than or equals parallel to upper A parallel to comma|ci| ≤||A||,

3.11 Matrix Norms
195
(see also Exercise 3.41b). 
The inequality (3.340) and  the  normal upper L 1L1 and normal upper L Subscript normal infinityL∞norms yield useful bounds on 
the eigenvalues and the maximum absolute row and column sums of matrices: 
the modulus of any eigenvalue is no greater than the largest sum of absolute 
values of the elements in any row or column. (These were inequalities (3.258) 
and (3.259) on page 165.) 
The inequality (3.340) and  Eq. (3.319) also yield a minimum property of 
the normal upper L 2L2 norm of a symmetric matrix A: 
parallel to upper A parallel to Subscript 2 Baseline less than or equals parallel to upper A parallel to period||A||2 ≤||A||.
3.11.6 Convergence of a Matrix Power Series 
We deﬁne the convergence of a sequence of matrices in terms of the conver-
gence of a sequence of their norms, just as we did for a sequence of vectors (on 
page 42). We say that a sequence of matrices upper A 1 comma upper A 2 comma ellipsisA1, A2, . . . (of the same shape) 
converges to the matrix A with respect to the norm parallel to dot parallel to||·|| if the sequence of real 
numbers parallel to upper A 1 minus upper A parallel to comma parallel to upper A 2 minus upper A parallel to comma ellipsis||A1 −A||, ||A2 −A||, . . . converges to 0. Because of the equivalence 
property of norms, the choice of the norm is irrelevant. 
Conditions for Convergence of a Sequence of Powers to 0 
For a square matrix A, we have the  important fact that  
upper A Superscript k Baseline right arrow 0 comma normal i normal f parallel to upper A parallel to less than 1 commaAk →0,
if ||A|| < 1,
(3.341) 
where 0 is the square zero matrix of the same order as A and parallel to dot parallel to||·|| is any matrix 
norm. (The consistency property is required.) This convergence follows from 
inequality (3.308) because that yields limit Underscript k right arrow normal infinity Endscripts parallel to upper A Superscript k Baseline parallel to less than or equals limit Underscript k right arrow normal infinity Endscripts parallel to upper A parallel to Superscript klimk→∞||Ak|| ≤limk→∞||A||k, and  so  
if parallel to upper A parallel to less than 1||A|| < 1, then  limit Underscript k right arrow normal infinity Endscripts parallel to upper A Superscript k Baseline parallel to equals 0limk→∞||Ak|| = 0. 
Now consider the spectral radius. Because of the spectral decomposition, 
we would expect the spectral radius to be related to the convergence of a 
sequence of powers of a matrix. If upper A Superscript k Baseline right arrow 0Ak →0, then for any conformable vector 
x, upper A Superscript k Baseline x right arrow 0Akx →0; in particular, for the eigenvector v 1 not equals 0v1 /= 0 corresponding to the 
dominant eigenvalue c 1c1, we have  upper A Superscript k Baseline v 1 equals c 1 Superscript k Baseline v 1 right arrow 0Akv1 = ck
1v1 →0. For  c 1 Superscript k Baseline v 1ck
1v1 to converge to 
zero, we must have StartAbsoluteValue c 1 EndAbsoluteValue less than 1|c1| < 1; that is,  rho left parenthesis upper A right parenthesis less than 1ρ(A) < 1. We can also show the converse: 
upper A Superscript k Baseline right arrow 0 normal i normal f rho left parenthesis upper A right parenthesis less than 1 periodAk →0
if ρ(A) < 1.
(3.342) 
We will do this by deﬁning a norm parallel to dot parallel to Subscript d|| · ||d in terms of the normal upper L 1L1 matrix norm in 
such a way that rho left parenthesis upper A right parenthesis less than 1ρ(A) < 1 implies parallel to upper A parallel to Subscript d Baseline less than 1||A||d < 1. Then, we can use Eq. (3.341) to  
establish the convergence. 
Let upper A equals upper Q upper T upper Q Superscript normal upper TA = QTQT be the Schur factorization of the n times nn × n matrix A, 
where Q is orthogonal, and T is upper triangular with the same eigen-
values as A, c 1 comma ellipsis comma c Subscript n Baselinec1, . . . , cn. Now  for any  d greater than 0d > 0, form the diagonal matrix

196
3 Basic Properties of Matrices
upper D equals normal d normal i normal a normal g left parenthesis left parenthesis d Superscript 1 Baseline comma ellipsis comma d Superscript n Baseline right parenthesis right parenthesisD = diag((d1, . . . , dn)). Notice that  upper D upper T upper D Superscript negative 1DTD−1 is an upper triangular matrix 
and its diagonal elements (which are its eigenvalues) are the same as the 
eigenvalues of T and A. Consider the column sums of the absolute values of 
the elements of upper D upper T upper D Superscript negative 1DTD−1: 
StartAbsoluteValue c Subscript j Baseline EndAbsoluteValue plus sigma summation Underscript i equals 1 Overscript j minus 1 Endscripts d Superscript minus left parenthesis j minus i right parenthesis Baseline StartAbsoluteValue t Subscript i j Baseline EndAbsoluteValue period|cj| +
j−1
E
i=1
d−(j−i)|tij|.
Now, because StartAbsoluteValue c Subscript j Baseline EndAbsoluteValue less than or equals rho left parenthesis upper A right parenthesis|cj| ≤ρ(A) for given epsilon greater than 0e > 0, by choosing  d large enough, we have 
StartAbsoluteValue c Subscript j Baseline EndAbsoluteValue plus sigma summation Underscript i equals 1 Overscript j minus 1 Endscripts d Superscript minus left parenthesis j minus i right parenthesis Baseline StartAbsoluteValue t Subscript i j Baseline EndAbsoluteValue less than rho left parenthesis upper A right parenthesis plus epsilon comma|cj| +
j−1
E
i=1
d−(j−i)|tij| < ρ(A) + e,
or 
parallel to upper D upper T upper D Superscript negative 1 Baseline parallel to Subscript 1 Baseline equals max Underscript j Endscripts left parenthesis StartAbsoluteValue c Subscript j Baseline EndAbsoluteValue plus sigma summation Underscript i equals 1 Overscript j minus 1 Endscripts d Superscript minus left parenthesis j minus i right parenthesis Baseline StartAbsoluteValue t Subscript i j Baseline EndAbsoluteValue right parenthesis less than rho left parenthesis upper A right parenthesis plus epsilon period||DTD−1||1 = max
j
(
|cj| +
j−1
E
i=1
d−(j−i)|tij|
)
< ρ(A) + e.
Now deﬁne parallel to dot parallel to Subscript d|| · ||d for any n times nn × n matrix X, where  Q is the orthogonal matrix 
in the Schur factorization, and D is as deﬁned above, as 
parallel to upper X parallel to Subscript d Baseline equals parallel to left parenthesis upper Q upper D Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline upper X left parenthesis upper Q upper D Superscript negative 1 Baseline right parenthesis parallel to Subscript 1 Baseline period||X||d = ||(QD−1)−1X(QD−1)||1.
(3.343) 
It is clear that parallel to dot parallel to Subscript d|| · ||d is a norm (Exercise 3.42). Furthermore, 
StartLayout 1st Row 1st Column parallel to upper A parallel to Subscript d 2nd Column equals 3rd Column parallel to left parenthesis upper Q upper D Superscript negative 1 Baseline right parenthesis Superscript negative 1 Baseline upper A left parenthesis upper Q upper D Superscript negative 1 Baseline right parenthesis parallel to Subscript 1 2nd Row 1st Column Blank 2nd Column equals 3rd Column parallel to upper D upper T upper D Superscript negative 1 Baseline parallel to Subscript 1 3rd Row 1st Column Blank 2nd Column less than 3rd Column rho left parenthesis upper A right parenthesis plus epsilon comma EndLayout||A||d = ||(QD−1)−1A(QD−1)||1
= ||DTD−1||1
< ρ(A) + e,
and so if rho left parenthesis upper A right parenthesis less than 1ρ(A) < 1, epsilone and d can be chosen so that  parallel to upper A parallel to Subscript d Baseline less than 1||A||d < 1, and  by  Eq. (3.341) 
above, we have upper A Superscript k Baseline right arrow 0Ak →0; hence, we conclude that 
upper A Superscript k Baseline right arrow 0 if and only if rho left parenthesis upper A right parenthesis less than 1 periodAk →0
if and only if ρ(A) < 1.
(3.344) 
Informally, we see that upper A Superscript kAk goes to 0 more rapidly the smaller is rho left parenthesis upper A right parenthesisρ(A). 
We will discuss convergence of a sequence of powers of an important special 
class of matrices with spectral radii possibly greater than or equal to 1 on 
page 415. 
Another Perspective on the Spectral Radius: Relation to Norms 
From inequality (3.340) and the fact that rho left parenthesis upper A Superscript k Baseline right parenthesis equals rho left parenthesis upper A right parenthesis Superscript kρ(Ak) = ρ(A)k, we have  
rho left parenthesis upper A right parenthesis less than or equals parallel to upper A Superscript k Baseline parallel to Superscript 1 divided by k Baseline commaρ(A) ≤||Ak||1/k,
(3.345) 
where parallel to dot parallel to|| · || is any matrix norm. Now, for any epsilon greater than 0e > 0, rho left parenthesis upper A divided by left parenthesis rho left parenthesis upper A right parenthesis plus epsilon right parenthesis right parenthesis less than 1ρ
(
A/(ρ(A) + e)
)
< 1 and 
so

3.11 Matrix Norms
197
limit Underscript k right arrow normal infinity Endscripts left parenthesis upper A divided by left parenthesis rho left parenthesis upper A right parenthesis plus epsilon right parenthesis right parenthesis Superscript k Baseline equals 0 lim
k→∞
(
A/(ρ(A) + e)
)k = 0
from expression (3.344); hence, 
limit Underscript k right arrow normal infinity Endscripts StartFraction parallel to upper A Superscript k Baseline parallel to Over left parenthesis rho left parenthesis upper A right parenthesis plus epsilon right parenthesis Superscript k Baseline EndFraction equals 0 period lim
k→∞
||Ak||
(ρ(A) + e)k = 0.
There is therefore a positive integer upper M Subscript epsilonMe such that parallel to upper A Superscript k Baseline parallel to divided by left parenthesis rho left parenthesis upper A right parenthesis plus epsilon right parenthesis Superscript k Baseline less than 1||Ak||/(ρ(A) + e)k < 1 for 
all k greater than upper M Subscript epsilonk > Me, and hence parallel to upper A Superscript k Baseline parallel to Superscript 1 divided by k Baseline less than left parenthesis rho left parenthesis upper A right parenthesis plus epsilon right parenthesis||Ak||1/k < (ρ(A) + e) for k greater than upper M Subscript epsilonk > Me. We have therefore, 
for any epsilon greater than 0e > 0, 
rho left parenthesis upper A right parenthesis less than or equals parallel to upper A Superscript k Baseline parallel to Superscript 1 divided by k Baseline less than rho left parenthesis upper A right parenthesis plus epsilon for k greater than upper M Subscript epsilon Baseline commaρ(A) ≤||Ak||1/k < ρ(A) + e
for k > Me,
and thus 
limit Underscript k right arrow normal infinity Endscripts parallel to upper A Superscript k Baseline parallel to Superscript 1 divided by k Baseline equals rho left parenthesis upper A right parenthesis period lim
k→∞||Ak||1/k = ρ(A).
(3.346) 
Compare this with the inequality (3.340). 
Convergence of a Power Series: Inverse of I −A 
Consider the power series in an n times nn×n matrix such as in Eq. (3.200) on page 142, 
upper I plus upper A plus upper A squared plus upper A cubed plus midline horizontal ellipsis periodI + A + A2 + A3 + · · · .
In the standard fashion for dealing with series, we form the partial sum 
upper S Subscript k Baseline equals upper I plus upper A plus upper A squared plus upper A cubed plus midline horizontal ellipsis upper A Superscript kSk = I + A + A2 + A3 + · · · Ak
and consider limit Underscript k right arrow normal infinity Endscripts upper S Subscript klimk→∞Sk. We ﬁrst note that  
left parenthesis upper I minus upper A right parenthesis upper S Subscript k Baseline equals upper I minus upper A Superscript k plus 1(I −A)Sk = I −Ak+1
and observe that if upper A Superscript k plus 1 Baseline right arrow 0Ak+1 →0, then  upper S Subscript k Baseline right arrow left parenthesis upper I minus upper A right parenthesis Superscript negative 1Sk →(I −A)−1, which  is  Eq. (3.200). 
Therefore, 
left parenthesis upper I minus upper A right parenthesis Superscript negative 1 Baseline equals upper I plus upper A plus upper A squared plus upper A cubed plus midline horizontal ellipsis if parallel to upper A parallel to less than 1 period(I −A)−1 = I + A + A2 + A3 + · · ·
if ||A|| < 1.
(3.347) 
Nilpotent Matrices 
As we discussed on page 98, for some nonzero square matrices, upper A Superscript k Baseline equals 0Ak = 0 for 
a ﬁnite integral value of k. If  upper A squared equals 0A2 = 0, such a matrix is a nilpotent matrix 
(otherwise, it is nilpotent with an index greater than 2). A matrix such as we 
discussed above for which upper A Superscript k Baseline right arrow 0Ak →0, but for any ﬁnite k, upper A Superscript k Baseline not equals 0Ak /= 0, is not called 
a nilpotent matrix. 
We have seen in Eq. (3.344) that  upper A Superscript k Baseline right arrow 0Ak →0 if and only if rho left parenthesis upper A right parenthesis less than 1ρ(A) < 1. The  
condition in Eq. (3.341) on any norm is not necessary, however; that is, if 
upper A Superscript k Baseline right arrow 0Ak →0, it may be the case that, for some norm, parallel to upper A parallel to greater than 1||A|| > 1. In fact, even for

198
3 Basic Properties of Matrices
an idempotent matrix (for which upper A Superscript k Baseline equals 0Ak = 0 for ﬁnite k), it may be the case that 
parallel to upper A parallel to greater than 1||A|| > 1. A simple  example is  
upper A equals Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 2 2nd Row 1st Column 0 2nd Column 0 EndMatrix periodA =
[
0 2
0 0
]
.
For this matrix, upper A squared equals 0A2 = 0, yet  parallel to upper A parallel to Subscript 1 Baseline equals parallel to upper A parallel to Subscript 2 Baseline equals parallel to upper A parallel to Subscript normal infinity Baseline equals parallel to upper A parallel to Subscript normal upper F Baseline equals 2||A||1 = ||A||2 = ||A||∞= ||A||F = 2. 
At this point, I list some more properties of nilpotent matrices that involve 
concepts we had not introduced when we ﬁrst discussed nilpotent matrices. 
It is easy to see that if upper A Subscript n times nAn×n is nilpotent, then 
normal t normal r left parenthesis upper A right parenthesis equals 0 commatr(A) = 0,
(3.348) 
normal d normal e normal t left parenthesis upper A right parenthesis equals 0 commadet(A) = 0,
(3.349) 
rho left parenthesis upper A right parenthesis equals 0 commaρ(A) = 0,
(3.350) 
(that is, all eigenvalues of A are 0), and 
normal r normal a normal n normal k left parenthesis upper A right parenthesis less than or equals n minus 1 periodrank(A) ≤n −1.
(3.351) 
You are asked to supply the proofs of these statements in Exercise 3.43b. 
In applications, for example in time series or other stochastic processes, 
because of expression (3.344), the spectral radius is often the most useful. 
Stochastic processes may be characterized by whether the absolute value of 
the dominant eigenvalue (spectral radius) of a certain matrix is less than 1. 
Interesting special cases occur when the dominant eigenvalue is equal to 1. 
3.12 Approximation of Matrices 
In Sect. 2.2.6, we discussed the problem of approximating a given vector in 
terms of vectors from a lower-dimensional space. Likewise, it is often of interest 
to approximate one matrix by another. 
In statistical applications, we may wish to ﬁnd a matrix of smaller rank 
that contains a large portion of the information content of a matrix of larger 
rank (“dimension reduction” as on page 473; or variable selection as in 
Sect. 9.4.2, for example), or we may want to impose conditions on an esti-
mate that it have properties known to be possessed by the estimand (positive 
deﬁniteness of the correlation matrix, for example, as in Sect. 9.4.6). 
In numerical linear algebra, we may wish to ﬁnd a matrix that is easier to 
compute or that has properties that ensure more stable computations. 
Finally, we may wish to represent a matrix as a sum or a product of 
other matrices with restrictions on those matrices that do not allow an exact 
representation. (A nonnegative factorization as discussed in Sect. 4.8.1 is an 
example.)

3.12 Approximation of Matrices
199
3.12.1 Measures of the Diﬀerence between Two Matrices 
A natural way to assess the goodness of the approximation is by a norm 
of the diﬀerence (that is, by a metric induced by a norm), as discussed on 
page 42. If  upper A overTilde-A is an approximation to A, we could measure the quality of 
the approximation by upper Delta left parenthesis upper A comma upper A overTilde right parenthesis equals parallel to upper A minus upper A overTilde parallel toΔ(A, -A) = ||A −-A|| for some norm parallel to dot parallel to|| · ||. The measure 
upper Delta left parenthesis upper A comma upper A overTilde right parenthesisΔ(A, -A) is a metric, as deﬁned on page 41, and is a common way of measuring 
the “distance” between two matrices. 
Other ways of measuring the diﬀerence between two matrices may be based 
on how much the entropy of one divergences from that of the other. This may 
make sense if all elements in the matrices are positive. The Kullback–Leibler 
divergence between distributions is based on this idea. Because one distri-
bution is used to normalize the other one, the Kullback–Leibler divergence 
is not a metric. If all elements of the matrices upper A overTilde-A and A are positive, the 
Kullback–Leibler divergence measure for how much the matrix upper A overTilde-A diﬀers from 
A is 
d Subscript normal upper K normal upper L Baseline left parenthesis upper A minus upper A overTilde right parenthesis equals sigma summation Underscript i j Endscripts left parenthesis a overTilde Subscript i j Baseline log left parenthesis StartFraction a overTilde Subscript i j Baseline Over a Subscript i j Baseline EndFraction right parenthesis minus a overTilde Subscript i j Baseline plus a Subscript i j Baseline right parenthesis perioddKL(A −-A) =
E
ij
(
˜aij log
(˜aij
aij
)
−˜aij + aij
)
.
(3.352) 
The most commonly used measure of the goodness of an approximation 
uses the norm that arises from the inner product (the Frobenius norm). 
3.12.2 Best Approximation with a Matrix of Given Rank 
Suppose we want the best approximation to an n times mn × m matrix A of rank r by 
a matrix  upper A overTilde-A in normal upper I normal upper R Superscript n times mIRn×m but with smaller rank, say k; that is, we want to ﬁnd upper A overTilde-A
of rank k such that 
parallel to upper A minus upper A overTilde parallel to Subscript normal upper F||A −-A||F
(3.353) 
is a minimum for all upper A overTilde element of normal upper I normal upper R Superscript n times m -A ∈IRn×m of rank k. 
We have an orthogonal basis in terms of the singular value decomposition, 
Eq. (3.305), for some subspace of normal upper I normal upper R Superscript n times mIRn×m, and we know that the Fourier coef-
ﬁcients provide the best approximation for any subset of k basis matrices, as 
in Eq. (2.63). This Fourier ﬁt would have rank k as required, but it would be 
the best only within that set of expansions. (This is the limitation imposed in 
Eq. (2.63).) Another approach to determine the best ﬁt could be developed by 
representing the columns of the approximating matrix as linear combinations 
of the given matrix A and then expanding parallel to upper A minus upper A overTilde parallel to Subscript normal upper F Superscript 2||A −-A||2
F. Neither the Fourier 
expansion nor the restriction script upper V left parenthesis upper A overTilde right parenthesis subset of script upper V left parenthesis upper A right parenthesisV( -A) ⊂V(A) permits us to address the question 
of what is the overall best approximation of rank k within normal upper I normal upper R Superscript n times mIRn×m. As we see  
below, however, there is a minimum of expression (3.353) that occurs within 
script upper V left parenthesis upper A right parenthesisV(A), and a minimum is at the truncated Fourier expansion in the singular 
values (Eq. (3.305)). 
To state this more precisely, let A be an n times mn × m matrix of rank r with 
singular value decomposition

200
3 Basic Properties of Matrices
upper A equals upper U Start 2 By 2 Matrix 1st Row 1st Column upper D Subscript r Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper V Superscript normal upper T Baseline commaA = U
[
Dr 0
0 0
]
V T,
where upper D Subscript r Baseline equals normal d normal i normal a normal g left parenthesis left parenthesis d 1 comma ellipsis comma d Subscript r Baseline right parenthesis right parenthesisDr = diag((d1, . . . , dr)), and the singular values are indexed so that 
d 1 greater than or equals midline horizontal ellipsis greater than or equals d Subscript r Baseline greater than 0d1 ≥· · · ≥dr > 0. Then, for all n times mn × m matrices X with rank k less than rk < r, 
parallel to upper A minus upper X parallel to Subscript normal upper F Superscript 2 Baseline greater than or equals sigma summation Underscript i equals k plus 1 Overscript r Endscripts d Subscript i Superscript 2 Baseline comma||A −X||2
F ≥
r
E
i=k+1
d2
i ,
(3.354) 
and this minimum occurs for upper X equals upper A overTildeX = -A, where  
upper A overTilde equals upper U Start 2 By 2 Matrix 1st Row 1st Column upper D Subscript k Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper V Superscript normal upper T Baseline period -A = U
[
Dk 0
0 0
]
V T.
(3.355) 
To see this, for any X, let  Q be an n times kn × k matrix whose columns are an 
orthonormal basis for script upper V left parenthesis upper X right parenthesisV(X), and  let  upper X equals upper Q upper YX = QY , where  Y is a k times mk × m matrix, 
also of rank k. The minimization problem now is 
min Underscript upper Y Endscripts parallel to upper A minus upper Q upper Y parallel to Subscript normal upper Fmin
Y
||A −QY ||F
with the restriction normal r normal a normal n normal k left parenthesis upper Y right parenthesis equals krank(Y ) = k. 
Now, expanding, completing the Gramian and using its nonnegative deﬁ-
niteness, and permuting the factors within a trace, we have 
StartLayout 1st Row 1st Column parallel to upper A minus upper Q upper Y parallel to Subscript normal upper F Superscript 2 2nd Column equals 3rd Column normal t normal r left parenthesis left parenthesis upper A minus upper Q upper Y right parenthesis Superscript normal upper T Baseline left parenthesis upper A minus upper Q upper Y right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis plus normal t normal r left parenthesis upper Y Superscript normal upper T Baseline upper Y minus upper A Superscript normal upper T Baseline upper Q upper Y minus upper Y Superscript normal upper T Baseline upper Q Superscript normal upper T Baseline upper A right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis plus normal t normal r left parenthesis left parenthesis upper Y minus upper Q Superscript normal upper T Baseline upper A right parenthesis Superscript normal upper T Baseline left parenthesis upper Y minus upper Q Superscript normal upper T Baseline upper A right parenthesis right parenthesis minus normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper Q upper Q Superscript normal upper T Baseline upper A right parenthesis 4th Row 1st Column Blank 2nd Column greater than or equals 3rd Column normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis minus normal t normal r left parenthesis upper Q Superscript normal upper T Baseline upper A upper A Superscript normal upper T Baseline upper Q right parenthesis period EndLayout||A −QY ||2
F = tr
(
(A −QY )T(A −QY )
)
= tr
(
ATA
)
+ tr
(
Y TY −ATQY −Y TQTA
)
= tr
(
ATA
)
+ tr
(
(Y −QTA)T(Y −QTA)
)
−tr
(
ATQQTA
)
≥tr
(
ATA
)
−tr
(
QTAATQ
)
.
The squares of the singular values of A are the eigenvalues of upper A Superscript normal upper T Baseline upper AATA, and  so  
normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis equals sigma summation Underscript i equals 1 Overscript r Endscripts d Subscript i Superscript 2tr(ATA) = Er
i=1 d2
i . The eigenvalues of upper A Superscript normal upper T Baseline upper AATA are also the eigenvalues of upper A upper A Superscript normal upper TAAT, 
and so, from inequality (3.295), normal t normal r left parenthesis upper Q Superscript normal upper T Baseline upper A upper A Superscript normal upper T Baseline upper Q right parenthesis less than or equals sigma summation Underscript i equals 1 Overscript k Endscripts d Subscript i Superscript 2tr(QTAATQ) ≤Ek
i=1 d2
i , and  so  
parallel to upper A minus upper X parallel to Subscript normal upper F Superscript 2 Baseline greater than or equals sigma summation Underscript i equals 1 Overscript r Endscripts d Subscript i Superscript 2 Baseline minus sigma summation Underscript i equals 1 Overscript k Endscripts d Subscript i Superscript 2 Baseline semicolon||A −X||2
F ≥
r
E
i=1
d2
i −
k
E
i=1
d2
i ;
hence, we have inequality (3.354). (This technique of “completing the Gramian” 
when an orthogonal matrix is present in a sum is somewhat similar to the tech-
nique of completing the square; it results in the diﬀerence of two Gramian 
matrices, which are deﬁned in Sect. 3.4.11.) 
Direct expansion of parallel to upper A minus upper A overTilde parallel to Subscript normal upper F Superscript 2||A −-A||2
F yields 
normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis minus 2 normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A overTilde right parenthesis plus normal t normal r left parenthesis upper A overTilde Superscript normal upper T Baseline upper A overTilde right parenthesis equals sigma summation Underscript i equals 1 Overscript r Endscripts d Subscript i Superscript 2 Baseline minus sigma summation Underscript i equals 1 Overscript k Endscripts d Subscript i Superscript 2 Baseline commatr
(
ATA
)
−2tr
(
AT -A
)
+ tr
(
-AT -A
)
=
r
E
i=1
d2
i −
k
E
i=1
d2
i ,
and hence upper A overTilde-A is the best rank k approximation to A under the Frobenius norm.

3.12 Approximation of Matrices
201
Equation (3.355) can be stated another way: the best approximation of A 
of rank k is 
upper A overTilde equals sigma summation Underscript i equals 1 Overscript k Endscripts d Subscript i Baseline u Subscript i Baseline v Subscript i Superscript normal upper T Baseline period -A =
k
E
i=1
diuivT
i .
(3.356) 
This result for the best approximation of a given matrix by one of lower 
rank was ﬁrst shown by Eckart and Young (1936). On page 380, we will 
discuss a bound on the diﬀerence between two symmetric matrices whether 
of the same or diﬀerent ranks. 
In applications, the rank k may be stated a priori, or we examine a sequence 
k equals r minus 1 comma r minus 2 comma ellipsisk = r −1, r −2, . . ., and determine the norm of the best ﬁt at each rank. If 
s Subscript ksk is the norm of the best approximating matrix, the sequence s Subscript r minus 1 Baseline comma s Subscript r minus 2 Baseline comma ellipsissr−1, sr−2, . . .
may suggest a value of k for which the reduction in rank is suﬃcient for 
our purposes and the loss in closeness of the approximation is not too great. 
Principal components analysis is a special case of this process (see Sect. 9.3). 
Appendix: Matrices in R 
R is an object-oriented system, and one of the most important classes of 
objects in R is a matrix. The basic R system includes many useful functions for 
operating on matrices. There are also several standard packages that provide 
additional functions for work with matrices. One of the most useful packages 
is MASS by Bill Venables and Brian Ripley. This package generally comes with 
the basic R distribution. Two other useful packages for work with vectors 
and matrices are Matrix originally developed by Doug Bates and matlib by 
Michael Friendly. 
In this appendix, we will discuss some of the basic features of R for working 
with matrices. In the appendix for Chap. 4, beginning on page 251, we will 
discuss additional R functions for matrices. 
A matrix of class  matrix can be constructed in R by use of the matrix 
function with an argument that supplies the matrix elements and an argument 
that speciﬁes the number of rows or columns. All rows must have the same 
number of elements, and all columns must have the same number of elements 
(the array is rectangular). The data in the matrix function are speciﬁed by 
columns, unless the argument byrows is true. Columns and rows can be given 
names by the colnames and rownames functions. The Matrix function in the 
Matrix package constructs matrices with more options, for example, with 
banded or sparse storage. The Matrix function produces an object of class 
Matrix, which inherits from class matrix.

202
3 Basic Properties of Matrices
Transpose is performed by the t function. 
> A <- matrix(c(0,1,2,3),nrow=2) 
> A  
[,1] [,2] 
[1,]
0
2 
[2,]
1
3 
> B <- matrix(c(0,1,2,3),nrow=2,byrow=TRUE) 
> colnames(B)= c("y1","y2") 
> B  
y1 y2 
[1,] 0 1 
[2,] 2 3 
> t(B) 
[,1] [,2] 
y1
0
2 
y2
1
3 
An important basic class in R is class array, which is a container object 
holding other objects organized in a hyperrectangular array. An array ob-
jected is characterized by its number of dimensions, which together index the 
objects in the array. The R matrix class inherits from the class array, and  the  
indexing scheme of a matrix is the same as that of a two-dimensional array. 
The ﬁrst dimension in a matrix corresponds to its row. 
Individual elements in an array are referenced by index numbers, indicated 
by “[ ]”. Indexing of arrays starts at 1. Indexes in diﬀerent dimensions are 
separated by commas; thus, for a two-dimensional array, or matrix, A[1,2] 
refers to the element of the matrix A in the ﬁrst row and second column. An 
absence of an index in a dimension indicates all indexes in that dimension. 
> A[2,1] 
[1] 1 
> A[2,] 
[1] 1 3 
See page 206 on subarrays for more examples of indexing. 
The function dim returns lengths of the dimensions in an array; for a 
matrix, dim is a vector of length 2 whose ﬁrst element is the number of rows 
and whose second element is the number of columns. The function length 
returns the number of elements in an array or a vector regardless of the 
number of dimensions.

3.12 Approximation of Matrices
203
> dim(A) 
[1] 2 2 
> length(A) 
[1] 4 
Operations on Matrices in R 
The standard matrix operators and functions are available in R. Matrix ad-
dition is indicated by +, which of course is an elementwise operation. For two 
matrices of the same shape, other elementwise operations are indicated by -, 
*, *, and  /; hence, the symbol “*” indicates the Hadamard product of two 
matrices. Likewise, ^ indicates elementwise exponentiation. 
For conformable matrices, Cayley multiplication is indicated by %*%. Kro-
necker multiplication is indicated by %x%. (There is also a useful function, 
kronecker, that allows more general combinations of the elements of two 
matrices.) 
> A^2
# elementwise exponentiation 
[,1] [,2] 
[1,]
0
4 
[2,]
1
9 
> A*B
# Hadamard multiplication 
[,1] [,2] 
[1,]
0
2 
[2,]
2
9 
> A%*%B
# Cayley multiplication 
[,1] [,2] 
[1,]
4
6 
[2,]
6
10 
> A%x%B
# Kronecker multiplication 
[,1] [,2] [,3] [,4] 
[1,]
0
0
0
2
 
[2,]
0
0
4
6
 
[3,]
0
1
0
3
 
[4,]
2
3
6
9
 
Vectors and Matrices in R 
Vectors and matrices can be adjoined by the cbind (“column bind”) and 
rbind (“row bind”) functions.

204
3 Basic Properties of Matrices
> x <- c(1,3) 
> rbind(x,x) 
[,1] [,2] 
x
1
3
 
x
1
3
 
> cbind(B,x) 
y1 y2 x 
[1,] 0 1 1 
[2,] 2 3 3 
> cbind(x,B,x,x) 
x y1 y2 x x  
[1,] 1 0 1 1 1 
[2,] 3 2 3 3 3 
> rbind(B,x) 
y1 y2 
0
1
 
2
3
 
x
1
3
 
Vectors are generally treated as “column vectors” in vector/vector or vec-
tor/matrix multiplication. R, however, interprets Cayley multiplication in 
the only reasonable way; hence, a vector may be considered a “row vector.” 
For example, x%*%A is interpreted as t(x)%*%A, and  x%*%x is interpreted as 
t(x)%*%x. It is good programming practice to treat vectors as “column vec-
tors,” however, and to use t(x)%*%A instead of x%*%A. 
The vector outer product is formed by the outer product operator %o%, 
which is of course the same as %*%t(dot·). (There is also a useful function, 
outer, that allows more general combinations of the elements of two vec-
tors. For example, if func is a scalar function of two scalar variables, 
outer(x,y,FUN=func) forms a matrix with the rows corresponding to x and 
the columns corresponding to y, and  whose  left parenthesis i j right parenthesis normal t normal h(ij)th element corresponds to 
func(x[i],y[j]).) 
> x <- c(1,3) 
> A%*%x 
[,1] 
[1,]
6 
[2,]
10 
> t(x)%*%A 
[,1] [,2] 
[1,]
3
11

3.12 Approximation of Matrices
205
> x%*%A 
[,1] [,2] 
[1,]
3
11 
> t(x)%*%x 
[,1] 
[1,]
10 
> x%*%x 
[,1] 
[1,]
10 
> x%o%x 
[,1] [,2] 
[1,]
1
3 
[2,]
3
9 
> x%*%t(x) 
[,1] [,2] 
[1,]
1
3 
[2,]
3
9 
To the extent that R distinguishes between row vectors and column vec-
tors, a vector is considered to be a column vector. In many cases, however, it 
does not distinguish. 
Like many other software systems for array manipulation, R usually does 
not distinguish between scalars and arrays of size 1. For example, if 
x <- c(1,3) 
y <- c(1,2) 
z <- c(1,2,3) 
the expression x %*% y %*% z yields the same value as 7*z because the ex-
pression x %*% y %*% z is interpreted as (x %*% y) %*% z and (x %*% y) 
is a scalar: 
>
x %*% y %*% z 
[,1] [,2] [,3] 
[1,]
7
14
21 
>
7*z 
[1] 7 14 21 
The expression above depends on the order of the the operations, which is left 
to right. It is best never to make assumptions about the order; hence, the ex-
pression (x %*% y) %*% z would be preferred. The expression x %*% (y %*% 
z) would not be valid because y and z are not conformable for multiplication.

206
3 Basic Properties of Matrices
Notice that it appears that (x %*% y) %*% z has two dimensions, while 
7*z has only one. In the case of one-dimensional and two-dimensional arrays, 
R sometimes allows the user to treat them in a general fashion, and sometimes 
it makes a hard distinction. 
Subarrays in R 
Individual elements in a vector or a matrix, or groups of elements, can be ref-
erenced by index numbers. Groups of indexes can be formed by the c function 
or the seq function. Any set of valid indexes can be speciﬁed; for example, 
x[c(2,1,2)] refers to the second, the ﬁrst, and again the second elements of 
the one-dimensional array x. Notice that  x[c(2,1,2)] contains three elements 
whereas x itself only has three elements. 
Negative values can be used to indicate removal of speciﬁed elements; 
for example, z[c(-1,-2)] refers to the same one-dimensional array z with 
the ﬁrst and second elements removed. The order of negative indexes or the 
repetition of negative indexes has no eﬀect; for example, z[c(-2,-1,-2)] is 
the same as z[c(-1,-2)]. Positive and negative values cannot be mixed as 
indexes. 
A missing index indicates that the entire corresponding dimension is to be 
used. Suppose, for example, that M is a 3 times 43 × 4 matrix. Then 
• M[c(2,3),c(1,3)] is the 2 times 22 × 2 submatrix in rows 2 and 3 and columns 1 
to 3 of M. It is of class  matrix. M[-1,c(1,3)] is the same 2 times 22 × 2 submatrix 
as above. It is of class matrix. 
• M[c(3,2),c(1,3)] is the 2 times 22 × 2 submatrix in rows 3 and 2 and columns 1 
to 3 of M. It is of class  matrix. 
• M[,seq(1,4,2)] is the submatrix with all 3 rows and the 1 Superscript normal s normal t1st and 3 Superscript normal r normal d3rd
columns of M. It is of class  matrix. 
• M[,4] is the column vector that is the 4 normal t normal h4th column of M. It is not of class 
matrix. 
Subarrays can be used directly in expressions; thus, if C is a matrix with 
2 columns, then 
M[,seq(1,4,2)]%*%C 
is the Cayley product of the submatrix of M and the matrix C. 
Functions for Manipulating Matrices 
R provides several functions for simple manipulations of matrices and vectors, 
as  shown in Table  3.1.

3.12 Approximation of Matrices
207
Table 3.1. Some R functions for forming and manipulating matrices 
as.vector 
normal v normal e normal c left parenthesis dot right parenthesisvec(·). 
matrix
Build a matrix from given elements. 
Matrix {Matrix} Build a matrix with more options. 
cbind
Bind vectors and matrices by columns. 
rbind
Bind vectors and matrices by rows. 
vecdiag {matlib} Principal diagonal of a matrix. 
diag
Diagonal matrix formed from a vector, 
or the principal diagonal of a matrix, 
or the identity. 
t
Transpose of a matrix or vector. 
Vectors Are Not Matrices 
Matrices are of the R class array with two dimensions. An object of class 
array with only one dimension is essentially the same as a vector, but vectors 
are ordinarily not of class array but are of the simpler class numeric. 
Objects of class array have methods that R vectors do not have; for ex-
ample, the function dim, which returns the number of elements in each of an 
array, returns NULL for a numeric vector. 
The basic atomic numeric type in R is a vector, and a 1 times n1 × n matrix or an 
n times 1n × 1 matrix may be cast as vectors (that is, one-dimensional arrays). This 
is often exactly what we would want. There are, however, cases where this 
casting is not desired, for example, if we use the dim function. 
> M <- matrix(c(1,2,3,4,5,6,7,8,9,10,11,12), nrow=3) 
> dim(M) 
[1] 3 4 
> b <- M[,2] 
> dim(b) 
NULL 
> length(b) 
[1] 2 
To prevent this casting, we can use the drop keyword in the subsetting 
operator. 
> C <- M[,2, drop=FALSE] 
> dim(C) 
[1] 2 1

208
3 Basic Properties of Matrices
The apply Functions in R 
In addition to the matrix functions, R functions can be applied to the rows or 
columns of a matrix. The functions are not just the functions for vectors such 
as min, max, and  mean, discussed in Chap. 2, but any function that accepts 
a single vector argument. This is done in a way similar to the lapply and 
sapply functions for lists, discussed on page 65. The basic function is apply. 
Its form is 
apply(X, MARGIN, FUN, ..., simplify = TRUE) 
The apply function can be used on various arguments. Here, we will only 
discuss and illustrate its use on matrices. In that case, X is a matrix; MARGIN is 1 
to indicate operation on rows, 2 to indicate columns, or c(1,2) to indicate rows 
and columns; FUN and ... represent the function along with any additional 
arguments to the function; and simplify speciﬁes that the output is to be 
“simpliﬁed” in much the same way that sapply simpliﬁes lapply (page 66). 
> M <- matrix(c(1,3,-1 , -1,1,0),nrow=3) 
> M  
[,1] [,2] 
[1,]
1
-1 
[2,]
3
1 
[3,]
-1
0 
> apply(M, 1, mean) 
[1] 0.0 2.0 -0.5 
> apply(M, 2, mean) 
[1] 1 0 
Some functions whose arguments can be either vectors or matrices, espe-
cially statistical functions, can have diﬀerent interpretations, depending on 
how it is used. For example, mean and sd compute the mean and standard 
deviation of all elements in the vector or matrix, while var, cov, and  cor 
compute the variance–covariance or correlation matrix if the argument is a 
matrix, treating the columns as the variables. The cov and cor functions re-
quire a matrix as their argument, but var accepts a vector as the argument, 
in which case it computes the ordinary variance. 
> mean(M) 
[1] 0.5 
> sd(M) 
[1] 1.516575

Exercises
209
> apply(M, 2, sd) 
[1] 2 1 
> var(M) 
[,1] [,2] 
[1,]
4
1 
[2,]
1
1 
> apply(M, 2, var) 
[1] 4 1 
Exercises 
3.1. Vector space generated by a matrix. 
Why is the dimension of the vector space generated by an n×m matrix 
no greater than min(n, m)? 
3.2. Vector spaces of matrices. 
a) Show that a vector space of matrices is convex. 
b) Exhibit a basis set for IRn×m for n ≥m. 
c) n × m diagonal matrices 
i. Show the set of n × m diagonal matrices forms a vector space. 
ii. Exhibit a basis set for this vector space (assuming n ≥ m). 
d) n × n symmetric matrices 
i. Show the set of n × n symmetric matrices forms a vector space. 
ii. Exhibit a basis set for the vector space of n × n symmetric ma-
trices. 
iii. Show that the cardinality of any basis set for the vector space 
of n × n symmetric matrices is n(n + 1)/2. 
3.3. Orthogonal vectors and cofactors. 
a) Let a1 = (1, 2). Using Eq. (3.44) and the method indicated on 
page 92, ﬁnd a vector orthogonal to a1. 
b) Let x1 = (1, 2, 3, 4) and x2 = (4, 3, 2, 1). Find a normalized vector 
x3 who ﬁrst element is 0 and that is orthogonal to both x1 and x2. 
c) Find a normalized vector x4 that is orthogonal to x1, x2, and  x3 in 
Exercise 3.3b. 
3.4. By expanding the expression on the left-hand side, derive Eq. (3.105) on  
page 114. 
3.5. Show that for any quadratic form xT Ax there is a symmetric matrix 
As such that xT Asx = xT Ax. (The proof is by construction, with As = 
1 
2(A+AT ), ﬁrst showing As is symmetric and then that xT Asx = xT Ax.) 
3.6. For a, b, c ∈IR, give conditions on a, b, and  c for the matrix below to be 
positive deﬁnite. 
Start 2 By 2 Matrix 1st Row 1st Column a 2nd Column b 2nd Row 1st Column b 2nd Column c EndMatrix period
[
a b
b c
]
.

210
3 Basic Properties of Matrices
3.7. Show that the Mahalanobis distance deﬁned in Eq. (3.108) is a metric 
(that is, show that it satisﬁes the properties listed on page 41). 
3.8. Verify the relationships for Kronecker products shown in Eqs. (3.111) 
through (3.117) on page 117. 
Hint: Make liberal use of Eq. (3.110) to show that corresponding ele-
ments on either side of the equation are the same, and use previously 
veriﬁed equations. 
3.9. Verify the relationship between the vec function and Kronecker multi-
plication given in Eq. (3.120), vec(ABC) = (CT ⊗ A)vec(B). 
Hint: Just determine an expression for the ith term in the vector on 
either side of the equation. 
3.10. Cauchy–Schwarz inequalities for matrices. 
a) Prove the Cauchy–Schwarz inequality for the dot product of matri-
ces (inequality (3.125), page 120), which can also be written as 
left parenthesis normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper B right parenthesis right parenthesis squared less than or equals normal t normal r left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis normal t normal r left parenthesis upper B Superscript normal upper T Baseline upper B right parenthesis period(tr(ATB))2 ≤tr(ATA)tr(BTB).
b) Prove the Cauchy–Schwarz inequality for determinants of matrices 
A and B of the same shape: 
normal d normal e normal t left parenthesis upper A Superscript normal upper T Baseline upper B right parenthesis squared less than or equals normal d normal e normal t left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis normal d normal e normal t left parenthesis upper B Superscript normal upper T Baseline upper B right parenthesis perioddet(ATB)2 ≤det(ATA)det(BTB).
Under what conditions is equality achieved? 
c) Let A and B be matrices of the same shape, and deﬁne 
p left parenthesis upper A comma upper B right parenthesis equals normal d normal e normal t left parenthesis upper A Superscript normal upper T Baseline upper B right parenthesis periodp(A, B) = det(ATB).
Is p(·, ·) an inner product? Why or why not? 
3.11. Prove that a square matrix that is either row or column (strictly) diag-
onally dominant is nonsingular. 
3.12. Prove that a positive deﬁnite matrix is nonsingular. 
3.13. Let A be an n × m matrix. 
a) Under what conditions does A have a Hadamard multiplicative in-
verse? 
b) If A has a Hadamard multiplicative inverse, what is it? 
3.14. Bounds on ranks. 
a) Show that the bound in inequality (3.142) is sharp by ﬁnding two 
matrices A and B such that rank(AB) = min(rank(A), rank(B)). 
b) Show that the bound in inequality (3.184) is sharp by ﬁnding an 
n × n matrix A and a matrix B with n rows such that rank(AB) =  
rank(A) + rank(B) − n. 
c) Show that the bound in inequality (3.143) is sharp by ﬁnding two 
matrices A and B such that rank(A + B) = rank(A) + rank(B). 
d) Show that the bound in inequality (3.144) is sharp by ﬁnding two 
matrices A and B such that rank(A + B) =  |rank(A) −rank(B)|. 
3.15. The aﬃne group AL(n).

Exercises
211
a) What is the identity in AL(n)? 
b) Let (A, v) be an element  of  AL(n). What is the inverse of (A, v)? 
3.16. Let A be an n × m matrix of rank one. Show that A can be written as 
an outer product 
upper A equals x y Superscript normal upper T Baseline commaA = xyT,
where x is some n-vector and y is some m-vector. 
3.17. In computational explorations involving matrices, it is often convenient 
to work with matrices whose elements are integers. If an inverse is in-
volved, it would be nice to know that the elements of the inverse are also 
integers. Equation (3.186) on page 139 provides us a way of ensuring 
this. 
Show that if the elements of the square matrix A are integers and if 
det(A) =  ±1, then (A−1 exists and) the elements of A−1 are integers. 
3.18. Verify the relationships shown in Eqs. (3.190) through (3.197) on page 141. 
Do this by multiplying the appropriate matrices. For example, Eq. (3.190) 
is veriﬁed by the equations 
left parenthesis upper I plus upper A Superscript negative 1 Baseline right parenthesis upper A left parenthesis upper I plus upper A right parenthesis Superscript negative 1 Baseline equals left parenthesis upper A plus upper I right parenthesis left parenthesis upper I plus upper A right parenthesis Superscript negative 1 Baseline equals left parenthesis upper I plus upper A right parenthesis left parenthesis upper I plus upper A right parenthesis Superscript negative 1 Baseline equals upper I period(I + A−1)A(I + A)−1 = (A + I)(I + A)−1 = (I + A)(I + A)−1 = I.
Make liberal use of Eq. (3.187) and previously veriﬁed equations. 
Of course, it is much more interesting to derive relationships such as 
these rather than merely to verify them. The veriﬁcation, however, often 
gives an indication of how the relationship would arise naturally. 
3.19. Verify Eq. (3.198). 
3.20. In Eqs. (3.190) through (3.197) on page 141, drop the assumptions of 
nonsingularity of matrices, and assume only that the matrices are con-
formable for the operations in the expressions. Replace the inverse with 
the Moore–Penrose inverse. 
Now, determine which of these relationships are true. For those that are 
true, show that they are (for general but conformable matrices). If the 
relationship does not hold, give a counterexample. 
3.21. Prove that if A is nonsingular and lower triangular, then A−1 is lower 
triangular. 
3.22. By writing AA−1 = I, derive the expression for the inverse of a parti-
tioned matrix given in Eq. (3.204). 
3.23. Show that the expression given in Eq. (3.233) on page 152 is a Moore– 
Penrose inverse of A. (Show that properties 1 through 4 hold.) 
3.24. Show that the expression given for the generalized inverse in Eq. (3.236) 
on page 153 is correct. 
3.25. Write formal proofs of the properties of eigenvalues/vectors listed on 
page 158. 
3.26. Let A be a square matrix with an eigenvalue c and corresponding eigen-
vector v. Consider the matrix polynomial in A 
p left parenthesis upper A right parenthesis equals b 0 upper I plus b 1 upper A plus midline horizontal ellipsis plus b Subscript k Baseline upper A Superscript k Baseline periodp(A) = b0I + b1A + · · · + bkAk.

212
3 Basic Properties of Matrices
Show that if (c, v) is an eigenpair of A, then  p(c), that is, 
b 0 plus b 1 c plus midline horizontal ellipsis plus b Subscript k Baseline c Superscript k Baseline commab0 + b1c + · · · + bkck,
is an eigenvalue of p(A) with corresponding eigenvector v. (Technically, 
the symbol p(·) is overloaded in these two instances.) 
3.27. Write formal proofs of the properties of eigenvalues/vectors listed on 
page 162. 
3.28. Prove that for any square matrix, the algebraic multiplicity of a given 
eigenvalue is at least as great as the geometric multiplicity of that eigen-
value. 
3.29. a) Show that the unit vectors are eigenvectors of a diagonal matrix. 
b) Give an example of two similar matrices whose eigenvectors are not 
the same. 
Hint: In Eq. (3.264), let A be a 2 × 2 diagonal matrix (so you know 
its eigenvalues and eigenvectors) with unequal values along the di-
agonal, and let P be a 2  × 2 upper triangular matrix, so that you 
can invert it. Form B and check the eigenvectors. 
3.30. Let A be a diagonalizable matrix (not necessarily symmetric) with a 
spectral decomposition of the form of Eq. (3.286), A = E
i ciPi. Let  cj 
be a simple eigenvalue with associated left and right eigenvectors yj 
and xj, respectively. (Note that because A is not symmetric, it may 
have nonreal eigenvalues and eigenvectors.) 
a) Show that yH 
j xj /= 0.  
b) Show that the projection matrix Pj is xjyH 
j /yH 
j xj. 
3.31. If A is nonsingular, show that for any (conformable) vector x 
left parenthesis x Superscript normal upper T Baseline upper A x right parenthesis left parenthesis x Superscript normal upper T Baseline upper A Superscript negative 1 Baseline x right parenthesis greater than or equals left parenthesis x Superscript normal upper T Baseline x right parenthesis squared period(xTAx)(xTA−1x) ≥(xTx)2.
Hint: Use the square roots and the Cauchy-Schwarz inequality. 
3.32. Prove that the induced norm (page 188) is a matrix norm; that is, prove 
that it satisﬁes the consistency property. 
3.33. Prove the inequality (3.310) for an induced matrix norm on page 189: 
parallel to upper A x parallel to less than or equals parallel to upper A parallel to parallel to x parallel to period||Ax|| ≤||A|| ||x||.
3.34. Prove that, for the square matrix A, 
parallel to upper A parallel to Subscript 2 Superscript 2 Baseline equals rho left parenthesis upper A Superscript normal upper T Baseline upper A right parenthesis period||A||2
2 = ρ(ATA).
Hint: Show that ||A||2 
2 = max xT AT Ax for any normalized vector x. 
3.35. Let Q be an n × n orthogonal matrix, and let x be an n-vector. 
a) Prove Eq. (3.316): 
parallel to upper Q x parallel to Subscript 2 Baseline equals parallel to x parallel to Subscript 2 Baseline period||Qx||2 = ||x||2.
Hint: Write ||Qx||2 as
/
(Qx)TQx. 
b) Give examples to show that this does not hold for other norms.

Exercises
213
3.36. The triangle inequality for matrix norms: ||A + B|| ≤||A|| + ||B||. 
a) Prove the triangle inequality for the matrix L1 norm. 
b) Prove the triangle inequality for the matrix L∞ norm. 
c) Prove the triangle inequality for the matrix Frobenius norm. 
3.37. Prove that the Frobenius norm satisﬁes the consistency property. 
3.38. The Frobenius p norm and the Shatten p norm. 
a) Prove that the expression in Eq. (3.328), the “Frobenius p norm,” is 
indeed a norm. 
b) Prove that the expression in Eq. (3.330), the “Shatten p norm,” is 
indeed a norm. 
c) Prove Eq. (3.331). 
3.39. If || · ||a and || · ||b are matrix norms induced respectively by the vector 
norms || · ||va and || · ||vb , prove inequality (3.332); that is, show that 
there are positive numbers r and s such that, for any A, 
r parallel to upper A parallel to Subscript b Baseline less than or equals parallel to upper A parallel to Subscript a Baseline less than or equals s parallel to upper A parallel to Subscript b Baseline periodr||A||b ≤||A||a ≤s||A||b.
3.40. Prove inequalities (3.333) through (3.339), and show that the bounds 
are sharp by exhibiting instances of equality. 
3.41. The spectral radius, ρ(A). 
a) We have seen by an example that ρ(A) = 0  does  not  imply  A = 0.  
What about other properties of a matrix norm? For each, either 
show that the property holds for the spectral radius or, by means 
of an example, that it does not hold. 
b) Use the outer product of an eigenvector and the one vector to show 
that for any norm || · || and any matrix A, ρ(A) ≤||A||. 
3.42. Show that the function || · ||d deﬁned in Eq. (3.343) is a norm.  
Hint: Just verify the properties on page 187 that deﬁne a norm. 
3.43. Nilpotent matrices. 
a) Prove that a nilpotent matrix is singular without using the proper-
ties listed on page 198. 
b) Prove Eqs. (3.348) through (3.351). 
3.44. Prove Eqs. (3.354) and  (3.355) under the restriction that V(X) ⊆V(A); 
that is, where X = BL for a matrix B whose columns span V(A). 
R Exercises 
These exercises are intended to illustrate R functions and commands, rather 
than to follow eﬃcient or proper computational methods. You would almost 
never explicitly create elementary matrices or Gaussian matrices, for example. 
In most cases, the intent is just to compute a quantity using the deﬁnitions 
given in this chapter and the R functions from pages 8 through 16, pages 62 
through 66, and pages 201 through 209. 
3.45. Elementary operator (square) matrices. 
The function in this exercise is somewhat complicated because the type 
of output depends on the values of the arguments.

214
3 Basic Properties of Matrices
a) Write a single R function to create an elementary operator ma-
trix of either the form Epq to interchange rows or columns p and 
q (Eq. (3.66)), the form Ep(a) to multiply row or column p by the 
scalar a (Eq. (3.70)), or the form Epq(a) to perform an axpy opera-
tion on rows or columns p and q (Eq. (3.72)). 
Use the function deﬁnition with initialized arguments 
Epqa <- function(n,p=0,q=0,a=0){ 
where n is the number of rows (for premultiplication) or columns (for 
postmultiplication), p and q are indexes of the rows or columns, and 
a is the multiplier. 
The type of operation is indicated by the values of p, q, and  a. 
• If p>0, q>0, and  a=0, then rows or columns p and q are to be 
interchanged. 
• If p>0 and q=0, then row  or  column  p is to be multiplied by a. 
• If p>0, q>0, and  a/= 0,  then  row  or  column  q is to be multiplied 
by a and added to row  or  column  p. 
• If none of the conditions apply, Epqa returns NA. 
The resulting elementary operator matrix is n × n in all cases. 
b) Choose values of n, p, and  q, and for your chosen values, show that 
Epq is its own inverse. 
c) Choose values of n, p, and  a, with  a/= 0. For your chosen values, 
show that Ep(1/a)Ep(a) =  I. 
d) Choose values of n, p, q, and  a, and for your chosen values, show 
that Epq(−a)Epq(a) =  I. 
3.46. In computational explorations involving matrices, it is often convenient 
to work with matrices whose elements are integers. If eigenvalues are 
involved, it would be nice to know that the eigenvalues are also inte-
gers. This is similar in spirit to matrices with integral elements whose 
inverses also have integral elements, as was the problem considered in 
Exercise 3.17. Matrices like this also provide convenient test problems 
for algorithms or software. 
The use of the companion matrix (Eq. (3.247)) gives us a convenient 
way of obtaining “nice” matrices for numerically exploring properties 
of eigenvalues. Using other properties of eigenvalues/vectors such as 
those listed on page 158 and with similarity transforms, we can generate 
“interesting” matrices that have nice eigenvalues. 
For instance, a 3 × 3 matrix in Eq. (3.260) was generated by choosing a 
set of eigenvalues {a, 1 + i, 1 − i}. 
Next, I used the relationship between the eigenvalues of A and A − dI, 
and ﬁnally, I squared the matrix, so that the eigenvalues are squared. 
The resulting spectrum is {(a−d)2 , (1−d+i)2 , (1−d−i}. After initializing 
a and d, the R statements are

Exercises
215
B <- matrix(c(-d,0,2*a, 1,-d,-2*a+2, 0,1,a+2-d), 
nrow=3,byrow=TRUE) 
A <- B%*%B 
eigen(A) 
Can you tell what values of a and d were used in generating the matrix 
in Eq. (3.260) following these steps? 
a) Using R, construct two diﬀerent 3 × 3 matrices whose elements are 
all integers and whose eigenvalues are {7, 5, 3}. 
b) Determine the six Gershgorin disks for each of your matrices. 
(Are they the same?) 
3.47. Let 
upper X equals Start 4 By 3 Matrix 1st Row 1st Column 3 2nd Column 2 3rd Column 4 2nd Row 1st Column 5 2nd Column 3 3rd Column 2 3rd Row 1st Column 2 2nd Column 8 3rd Column 1 4th Row 1st Column 2 2nd Column 1 3rd Column 5 EndMatrix periodX =
⎡
⎢⎢⎣
3 2 4
5 3 2
2 8 1
2 1 5
⎤
⎥⎥⎦.
Form the centered matrix Xc by subtracting the mean of each column 
from the column elements. Next form the centered and scaled matrix 
Xcs by dividing the elements of the columns of Xc by the standard 
deviations of the columns. 
What are the mean and the standard deviation of each column of Xcs?

4 
Matrix Transformations and Factorizations 
In most applications of linear algebra, problems are solved by transforma-
tions of matrices. A given matrix (which represents some transformation of 
a vector) is itself transformed. The simplest example of this is in solving the 
linear system upper A x equals bAx = b, where the matrix A represents a transformation of the 
vector x to the vector b. The  matrix  A is transformed through a succession of 
linear operations until x is determined easily by the transformed A and the 
transformed b. Each operation in the transformation of A is a pre- or post-
multiplication by some other matrix. Each matrix formed as a product must 
be equivalent to A; therefore, in order to ensure this in general, each trans-
formation matrix must be of full rank. In eigenproblems, we likewise perform 
a sequence of pre- or postmultiplications. In this case, each matrix formed as 
a product must be similar to A; therefore, each transformation matrix must 
be orthogonal. We develop transformations of matrices by transformations on 
the individual rows or columns. 
While reading about matrices and operations with them, it may be in-
structive to work with numerical examples. The R software system provides 
an excellent way of doing this. The appendices on page 201 in Chap. 3 and 
page 251 in this chapter provide enough information to get started. 
Factorizations 
Given a matrix A, it is often useful to decompose A into the product of other 
matrices, that is, to form a factorization upper A equals upper B upper CA = BC, where  B and C are ma-
trices. We refer to this as “matrix factorization,” or sometimes as “matrix 
decomposition,” although this latter term includes more general representa-
tions of the matrix, such as the spectral decomposition (page 177). 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 4 
217

218
4 Transformations and Factorizations
Most methods for eigenanalysis and for solving linear systems proceed by 
factoring the matrix, as we see in Chaps. 5 and 6. 
In Chap. 3, we discussed some factorizations including: 
• The full rank factorization (Eq. (3.166)) of a general matrix 
• The equivalent canonical factorization (Eq. (3.175)) of a general matrix 
• The Schur factorization (Eq. (3.268)) of a square matrix 
• The similar canonical factorization (Eq. (3.271)) or “diagonal factoriza-
tion” of a diagonalizable matrix (which is necessarily square) 
• The orthogonally similar canonical factorization (Eq. (3.276)) of a sym-
metric matrix (which is necessarily diagonalizable) 
• The square root (Eq. (3.307)) of a nonnegative deﬁnite matrix (which is 
necessarily symmetric) 
• The singular value factorization (Eq. (3.299)) of a general matrix 
In this chapter, we describe three additional factorizations: 
• The LU (and LR and LDU) factorization of a general matrix 
• The QR factorization of a general matrix 
• The Cholesky factorization of a nonnegative deﬁnite matrix 
These factorizations are useful both in theory and in practice. 
Computational Methods: Direct and Iterative 
In our previous discussions of matrix factorizations and other operations, we 
have shown derivations that may indicate computational methods, but we 
have not speciﬁed the computational details. There are many important com-
putational issues, some of which we will discuss in Part III. At this point, again 
without getting into the details, we want to note a fundamental diﬀerence in 
the types of computational methods. 
The developments of the full rank factorization (Eq. (3.166)) and the 
equivalent canonical factorization (Eq. (3.175)) were constructive, and indeed, 
those factorizations could be computed following those constructions. On the 
other hand, our developments of the diagonalizing transformations, such as 
the orthogonally similar canonical factorization (Eq. (3.276)), and other fac-
torizations related to eigenvalues, such as the singular value factorization 
(Eq. (3.299)), were not constructive. 
As it turns out, the factorizations involving eigenvalues or singular values 
cannot in general be computed using a ﬁnite set of arithmetic operations. If 
they could be, then the characteristic polynomial equation could be solved 
in a ﬁnite set of arithmetic operations, but the Abel-Ruﬃni theorem states 
that such a solution does not exist for polynomials of degree ﬁve or higher. 
For factorizations of this type, we must use iterative methods, at least to 
get the eigenvalues or singular values. I discuss one factorization based on 
an eigendecomposition in this chapter. It is the square root, described in

4.1 Linear Geometric Transformations
219
Sect. 4.7.1. This is because it so naturally goes with a Cholesky factorization of 
a nonnegative deﬁnite matrix. We will describe some of the other factorizations 
based on iterative methods in Chap. 6. 
In this chapter, we ﬁrst discuss some transformations and important fac-
torizations that can be carried out in a ﬁnite number of arithmetic steps, that 
is, by the use of direct methods. The factorizations themselves can be used 
iteratively; indeed, as we will discuss in Chap. 6, the  QR  is  the most important  
factorization used iteratively to obtain eigenvalues or singular values. 
A factorization of a given matrix A is generally eﬀected by a series of 
pre- or postmultiplications by transformation matrices with simple and de-
sirable properties. One such transformation matrix is the Gaussian matrix, 
upper G Subscript i jGij of Eq. (3.77) or  upper G overTilde Subscript i j -Gij of Eq. (3.79) on page 107. Another important class of 
transformation matrices are orthogonal matrices. Orthogonal transformation 
matrices have some desirable properties. In this chapter, before discussing fac-
torizations, we ﬁrst consider some general properties of various types of trans-
formations, and then we describe two speciﬁc types of orthogonal transforma-
tions, Householder reﬂections (Sect. 4.2) and Givens rotations (Sect. 4.3). As 
we will see, the Householder reﬂections are very similar to the Gram-Schmidt 
transformations that we discussed beginning on page 48. 
4.1 Linear Geometric Transformations 
In many important applications of linear algebra, a vector represents a point 
in space, with each element of the vector corresponding to an element of a 
coordinate system, usually a Cartesian system. A set of vectors describes a 
geometric object, such as a polyhedron or a Lorentz cone, as on page 54. Alge-
braic operations can be thought of as geometric transformations that rotate, 
deform, or translate the object. While these transformations are often used in 
the two or three dimensions that correspond to the easily perceived physical 
space, they have similar applications in higher dimensions. Thinking about 
operations in linear algebra in terms of the associated geometric operations 
often provides useful intuition. 
A linear transformation of a vector x is eﬀected by multiplication by a 
matrix A. Any  n times mn × m matrix A is a function or transformation from script upper V 1V1 to script upper V 2V2, 
where script upper V 1V1 is a vector space of order m and script upper V 2V2 is a vector space of order n. 
4.1.1 Invariance Properties of Linear Transformations 
An important characteristic of a transformation is what it leaves unchanged, 
that is, its invariance properties (see Table 4.1). All of the transformations we 
will discuss are linear transformations because they preserve straight lines. A 
set of points that constitute a straight line is transformed into a set of points 
that constitute a straight line.

220
4 Transformations and Factorizations
Table 4.1. Invariance properties of transformations 
Transformation Preserves 
Linear
Lines 
Projective
Lines 
Aﬃne
Lines, collinearity 
Shearing
Lines, collinearity 
Scaling
Lines, angles (and, hence, collinearity) 
Rotation
Lines, angles, lengths 
Reﬂection
Lines, angles, lengths 
Translation
Lines, angles, lengths 
As mentioned above, reﬂections and rotations are orthogonal transforma-
tions, and we have seen that an orthogonal transformation preserves lengths 
of vectors (Eq. (3.316)). We will also see that an orthogonal transformation 
preserves angles between vectors (Eq. (4.1)). A transformation that preserves 
lengths and angles is called an isometric transformation. Such a transforma-
tion also preserves areas and volumes. 
Another isometric transformation is a translation, which is essentially the 
addition of another vector (see Sect. 4.1.5). 
A transformation that preserves angles is called an isotropic transforma-
tion. An example of an isotropic transformation that is not isometric is a 
uniform scaling or dilation transformation, x overTilde equals a x˜x = ax, where  a is a scalar. 
The transformation x overTilde equals upper A x˜x = Ax, where  A is a diagonal matrix with not all ele-
ments the same, does not preserve angles; it is an anisotropic scaling. Another 
anisotropic transformation is a shearing transformation, x overTilde equals upper A x˜x = Ax, where  A is 
the same as an identity matrix, except for a single row or column that has 
a one on the diagonal but nonzero, possibly constant, elements in the other 
positions, for example, 
Start 3 By 3 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column a 1 2nd Row 1st Column 0 2nd Column 1 3rd Column a 1 3rd Row 1st Column 0 2nd Column 0 3rd Column 1 EndMatrix period
⎡
⎣
1 0 a1
0 1 a1
0 0 1
⎤
⎦.
Although they do not preserve angles, both anisotropic scaling and shear-
ing transformations preserve parallel lines. A transformation that preserves 
parallel lines is called an aﬃne transformation. Preservation of parallel lines 
is equivalent to preservation of collinearity, and so an alternative character-
ization of an aﬃne transformation is one that preserves collinearity. More 
generally, we can combine nontrivial scaling and shearing transformations to 
see that the transformation Ax for any nonsingular matrix A is aﬃne. It is 
easy to see that addition of a constant vector to all vectors in a set pre-
serves collinearity within the set, so a more general aﬃne transformation is 
x overTilde equals upper A x plus t˜x = Ax + t for a nonsingular matrix A and a vector t. 
A projective transformation, which uses the homogeneous coordinate sys-
tem of the projective plane (see Sect. 4.1.5), preserves straight lines, but does

4.1 Linear Geometric Transformations
221
not preserve parallel lines. Projective transformations are very useful in com-
puter graphics. In those applications, we do not always want parallel lines to 
project onto the display plane as parallel lines. 
4.1.2 Transformations by Orthogonal Matrices 
We deﬁned orthogonal matrices and considered some basic properties on 
page 154. Orthogonal matrices are not necessarily square; they may have 
more rows than columns or may have fewer. In the following, we will consider 
only orthogonal matrices with at least as many rows as columns; that is, if Q 
is an orthogonal transformation matrix, then upper Q Superscript normal upper T Baseline upper Q equals upper IQTQ = I. This means that an 
orthogonal matrix is of full rank. Of course, many useful orthogonal matrices 
are square (and, obviously, nonsingular). There are many types of orthogo-
nal transformation matrices. As noted previously, permutation matrices are 
square orthogonal matrices, and we have used them extensively in rearranging 
the columns and/or rows of matrices. 
As we stated, transformations by orthogonal matrices preserve lengths of 
vectors. Orthogonal transformations also preserve angles between vectors, as 
we can easily see. If Q is an orthogonal matrix, then, for vectors x and y, we  
have 
left angle bracket upper Q x comma upper Q y right angle bracket equals left parenthesis upper Q x right parenthesis Superscript normal upper T Baseline left parenthesis upper Q y right parenthesis equals x Superscript normal upper T Baseline upper Q Superscript normal upper T Baseline upper Q y equals x Superscript normal upper T Baseline y equals left angle bracket x comma y right angle bracket comma<Qx, Qy> = (Qx)T(Qy) = xTQTQy = xTy = <x, y>,
and hence, 
arc cosine left parenthesis StartFraction left angle bracket upper Q x comma upper Q y right angle bracket Over parallel to upper Q x parallel to Subscript 2 Baseline parallel to upper Q y parallel to Subscript 2 Baseline EndFraction right parenthesis equals arc cosine left parenthesis StartFraction left angle bracket x comma y right angle bracket Over parallel to x parallel to Subscript 2 Baseline parallel to y parallel to Subscript 2 Baseline EndFraction right parenthesis period arccos
(
<Qx, Qy>
||Qx||2 ||Qy||2
)
= arccos
(
<x, y>
||x||2 ||y||2
)
.
(4.1) 
Thus, orthogonal transformations preserve angles. 
We have seen that if Q is an orthogonal matrix and 
upper B equals upper Q Superscript normal upper T Baseline upper A upper Q commaB = QTAQ,
then A and B have the same eigenvalues (and A and B are said to be or-
thogonally similar). By forming the transpose, we see immediately that the 
transformation upper Q Superscript normal upper T Baseline upper A upper QQTAQ preserves symmetry; that is, if A is symmetric, then B 
is symmetric. 
From Eq. (3.317), we see that parallel to upper Q Superscript negative 1 Baseline parallel to Subscript 2 Baseline equals 1||Q−1||2 = 1. This has important implica-
tions for the accuracy of numerical computations. (Using computations with 
orthogonal matrices will not make problems more “ill-conditioned.”) 
We often use orthogonal transformations that preserve lengths and an-
gles while rotating normal upper I normal upper R Superscript nIRn or reﬂecting regions of normal upper I normal upper R Superscript nIRn. The transformations are 
appropriately called rotators and reﬂectors, respectively. 
4.1.3 Rotations 
The simplest rotation of a vector can be thought of as the rotation of a plane 
deﬁned by two coordinates about the other principal axes. Such a rotation

222
4 Transformations and Factorizations
changes two elements of all vectors in that plane and leaves all the other 
elements, representing the other coordinates, unchanged. This rotation can 
be described in a two-dimensional space deﬁned by the coordinates being 
changed, without reference to the other coordinates. 
Consider the rotation of the vector x through the angle thetaθ into the vector 
x overTilde˜x. The length is preserved, so we have parallel to x overTilde parallel to equals parallel to x parallel to||˜x|| = ||x||. Referring to Fig. 4.1, we  
can write 
StartLayout 1st Row 1st Column x overTilde Subscript 1 2nd Column equals 3rd Column parallel to x parallel to cosine left parenthesis phi plus theta right parenthesis comma 2nd Row 1st Column x overTilde Subscript 2 2nd Column equals 3rd Column parallel to x parallel to sine left parenthesis phi plus theta right parenthesis period EndLayout˜x1 = ||x|| cos(φ + θ),
˜x2 = ||x|| sin(φ + θ).
Figure 4.1. Rotation of x 
Now, from elementary trigonometry, we know 
StartLayout 1st Row 1st Column cosine left parenthesis phi plus theta right parenthesis 2nd Column equals 3rd Column cosine phi cosine theta minus sine phi sine theta comma 2nd Row 1st Column sine left parenthesis phi plus theta right parenthesis 2nd Column equals 3rd Column sine phi cosine theta plus cosine phi sine theta period EndLayoutcos(φ + θ) = cos φ cos θ −sin φ sin θ,
sin(φ + θ) = sin φ cos θ + cos φ sin θ.
Because cosine phi equals x 1 divided by parallel to x parallel tocos φ = x1/||x|| and sine phi equals x 2 divided by parallel to x parallel tosin φ = x2/||x||, we can combine these equations 
to get 
StartLayout 1st Row 1st Column x overTilde Subscript 1 2nd Column equals 3rd Column x 1 cosine theta minus x 2 sine theta comma 2nd Row 1st Column x overTilde Subscript 2 2nd Column equals 3rd Column x 1 sine theta plus x 2 cosine theta period EndLayout ˜x1 = x1 cos θ −x2 sin θ,
˜x2 = x1 sin θ + x2 cos θ.
(4.2) 
Hence, multiplying x by the orthogonal matrix 
Start 2 By 2 Matrix 1st Row 1st Column cosine theta 2nd Column minus sine theta 2nd Row 1st Column sine theta 2nd Column cosine theta EndMatrix
[cos θ −sin θ
sin θ
cos θ
]
(4.3) 
performs the rotation of x. 
This idea easily extends to the rotation of a plane formed by two coordi-
nates about all of the other (orthogonal) principal axes. By convention, we 
assume clockwise rotations for axes that increase in the direction from which

4.1 Linear Geometric Transformations
223
the system is viewed. For example, if there were an x 3x3 axis in Fig. 4.1, it would  
point toward the viewer. (This is called a “right-hand” coordinate system, be-
cause if the viewer’s right-hand ﬁngers point in the direction of the rotation, 
the thumb points toward the viewer.) 
The rotation matrix about principal axes is the same as an identity ma-
trix with two diagonal elements changed to cosine thetacos θ and the corresponding oﬀ-
diagonal elements changed to sine thetasin θ and minus sine theta−sin θ. 
To rotate a 3-vector, x, about  the  x 2x2 axis in a right-hand coordinate sys-
tem, we would use the rotation matrix 
Start 3 By 3 Matrix 1st Row 1st Column cosine theta 2nd Column 0 3rd Column sine theta 2nd Row 1st Column 0 2nd Column 1 3rd Column 0 3rd Row 1st Column minus sine theta 2nd Column 0 3rd Column cosine theta EndMatrix period
⎡
⎣
cos θ
0 sin θ
0
1
0
−sin θ
0 cos θ
⎤
⎦.
A rotation of any hyperplane in n-space can be formed by n successive 
rotations of hyperplanes formed by two principal axes. (In 3-space, this fact is 
known as Euler’s rotation theorem. We can see this to be the case, in 3-space 
or in general, by construction.) 
A rotation of an arbitrary plane can be deﬁned in terms of the direction 
cosines of a vector in the plane before and after the rotation. In a coordinate 
geometry, rotation of a plane can be viewed equivalently as a rotation of the 
coordinate system in the opposite direction. This is accomplished by rotating 
the unit vectors e Subscript iei into e overTilde Subscript i˜ei. 
A special type of transformation that rotates a vector to be perpendicular 
to a principal axis is called a Givens rotation. We discuss the use of this type 
of transformation in Sect. 4.3 on page 228. Another special rotation is the 
“reﬂection” of a vector about another vector. We discuss this kind of rotation 
next. 
4.1.4 Reﬂections 
Let u and v be orthonormal vectors, and let x be a vector in the space spanned 
by u and v, so  
x equals c 1 u plus c 2 vx = c1u + c2v
for some scalars c 1c1 and c 2c2. The vector 
x overTilde equals minus c 1 u plus c 2 v˜x = −c1u + c2v
(4.4) 
is a reﬂection of x through the line deﬁned by the vector v, or  u Superscript perpendicularu⊥. This  
reﬂection is a rotation in the plane deﬁned by u and v through an angle  of  
twice the size of the angle between x and v. 
The form of x overTilde˜x of course depends on the vector v and its relationship to x. 
In a common application of reﬂections in linear algebraic computations, we 
wish to rotate a given vector into a vector collinear with a coordinate axis; 
that is, we seek a reﬂection that transforms a vector 
x equals left parenthesis x 1 comma x 2 comma ellipsis comma x Subscript n Baseline right parenthesisx = (x1, x2, . . . , xn)

224
4 Transformations and Factorizations
into a vector collinear with a unit vector, 
StartLayout 1st Row 1st Column x overTilde 2nd Column equals 3rd Column left parenthesis 0 comma ellipsis comma 0 comma x overTilde Subscript i Baseline comma 0 comma ellipsis comma 0 right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column plus or minus parallel to x parallel to Subscript 2 Baseline e Subscript i Baseline period EndLayout˜x = (0, . . . , 0, ˜xi, 0, . . . , 0)
= ±||x||2ei.
(4.5) 
Geometrically, in two dimensions, we have the picture shown in Fig. 4.2, 
where i equals 1i = 1. Which vector that x is rotated through (i.e., which is u and 
which is v) depends on the choice of the sign in plus or minus parallel to x parallel to Subscript 2±||x||2. The choice that was 
made yields the x overTilde˜x shown in the ﬁgure, and from the ﬁgure, this can be seen 
to be correct. Note that 
v equals StartFraction 1 Over StartAbsoluteValue 2 c 2 EndAbsoluteValue EndFraction left parenthesis x plus x overTilde right parenthesisv =
1
|2c2|(x + ˜x)
If the opposite choice is made, we get the x overTilde overTilde˜˜x shown. In the simple two-
dimensional case, this is equivalent to reversing our choice of u and v. 
Figure 4.2. Reﬂections of x about v (or u Superscript perpendicularu⊥) and about u 
To accomplish this special rotation of course, we ﬁrst choose an appropriate 
vector about which to reﬂect our given vector and then perform the rotation. 
We will describe this process in Sect. 4.2. 
4.1.5 Translations; Homogeneous Coordinates 
A translation of a vector is a relatively simple transformation in which the 
vector is transformed into a parallel vector. It involves a type of addition of 
vectors. Rotations, as we have seen, and other geometric transformations such 
as shearing, as we have indicated, involve multiplication by an appropriate 
matrix. In applications where several geometric transformations are to be 
made, it would be convenient if translations could also be performed by matrix 
multiplication. This can be done by using homogeneous coordinates. 
Homogeneous coordinates, which form the natural coordinate system for 
projective geometry, have a very simple relationship to Cartesian coordinates. 
The point with Cartesian coordinates left parenthesis x 1 comma x 2 comma ellipsis comma x Subscript d Baseline right parenthesis(x1, x2, . . . , xd) is represented in homo-
geneous coordinates as left parenthesis x 0 Superscript normal h Baseline comma x 1 Superscript normal h Baseline comma ellipsis comma x Subscript d Superscript normal h Baseline right parenthesis(xh
0, xh
1, . . . , xh
d), where, for arbitrary x 0 Superscript normal hxh
0 not equal to

4.2 Householder Transformations (Reﬂections)
225
zero, x 1 Superscript normal h Baseline equals x 0 Superscript normal h Baseline x 1xh
1 = xh
0x1, and so on. Because the point is the same, the two diﬀerent 
symbols represent the same thing, and we have 
left parenthesis x 1 comma ellipsis comma x Subscript d Baseline right parenthesis equals left parenthesis x 0 Superscript normal h Baseline comma x 1 Superscript normal h Baseline comma ellipsis comma x Subscript d Superscript normal h Baseline right parenthesis period(x1, . . . , xd) = (xh
0, xh
1, . . . , xh
d).
(4.6a) 
Alternatively, the hyperplane coordinate may be added at the end, and we 
have 
left parenthesis x 1 comma ellipsis comma x Subscript d Baseline right parenthesis equals left parenthesis x 1 Superscript normal h Baseline comma ellipsis comma x Subscript d Superscript normal h Baseline comma x 0 Superscript normal h Baseline right parenthesis period(x1, . . . , xd) = (xh
1, . . . , xh
d, xh
0).
(4.6b) 
Each value of x 0 Superscript normal hxh
0 corresponds to a hyperplane in the ordinary Cartesian co-
ordinate system. The most common choice is x 0 Superscript normal h Baseline equals 1xh
0 = 1, and  so  x Subscript i Superscript normal h Baseline equals x Subscript ixh
i = xi. The  
special plane x 0 Superscript normal h Baseline equals 0xh
0 = 0 does not have a meaning in the Cartesian system, but in 
projective geometry, it corresponds to a hyperplane at inﬁnity. 
We can easily eﬀect the translation x overTilde equals x plus t˜x = x + t by ﬁrst representing the 
point x as left parenthesis 1 comma x 1 comma ellipsis comma x Subscript d Baseline right parenthesis(1, x1, . . . , xd) and then multiplying by the left parenthesis d plus 1 right parenthesis times left parenthesis d plus 1 right parenthesis(d + 1) × (d + 1) matrix 
upper T equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 2nd Row 1st Column t 1 2nd Column 1 3rd Column midline horizontal ellipsis 4th Column 0 3rd Row 1st Column midline horizontal ellipsis 4th Row 1st Column t Subscript d Baseline 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 1 EndMatrix periodT =
⎡
⎢⎢⎣
1 0 · · · 0
t1 1 · · · 0
· · ·
td 0 · · · 1
⎤
⎥⎥⎦.
We will use the symbol x Superscript normal hxh to represent the vector of corresponding homoge-
neous coordinates: 
x Superscript normal h Baseline equals left parenthesis 1 comma x 1 comma ellipsis comma x Subscript d Baseline right parenthesis periodxh = (1, x1, . . . , xd).
We must be careful to distinguish the point x from the vector that represents 
the point. In Cartesian coordinates, there is a natural correspondence, and 
the symbol x representing a point may also represent the vector left parenthesis x 1 comma ellipsis comma x Subscript d Baseline right parenthesis(x1, . . . , xd). 
The vector of homogeneous coordinates of the result upper T x Superscript normal hTxh corresponds to the 
Cartesian coordinates of x overTilde˜x, left parenthesis x 1 plus t 1 comma ellipsis comma x Subscript d Baseline plus t Subscript d Baseline right parenthesis(x1 + t1, . . . , xd + td), which is the desired result. 
Homogeneous coordinates are used extensively in computer graphics not 
only for the ordinary geometric transformations but also for projective trans-
formations, which model visual properties. See Exercise 4.10 for a simple ex-
ample. 
4.2 Householder Transformations (Reﬂections) 
We have brieﬂy discussed geometric transformations that reﬂect a vector 
through another vector. We now consider some properties and uses of these 
transformations. 
Consider the problem of reﬂecting x through the vector v. As before, we 
assume that u and v are orthonormal vectors and that x lies in a space spanned 
by u and v and x equals c 1 u plus c 2 vx = c1u + c2v. Form the  matrix  
upper H equals upper I minus 2 u u Superscript normal upper T Baseline commaH = I −2uuT,
(4.7)

226
4 Transformations and Factorizations
and note that 
StartLayout 1st Row 1st Column upper H x 2nd Column equals 3rd Column c 1 u plus c 2 v minus 2 c 1 u u Superscript normal upper T Baseline u minus 2 c 2 u u Superscript normal upper T Baseline v 2nd Row 1st Column Blank 2nd Column equals 3rd Column c 1 u plus c 2 v minus 2 c 1 u Superscript normal upper T Baseline u u minus 2 c 2 u Superscript normal upper T Baseline v u 3rd Row 1st Column Blank 2nd Column equals 3rd Column minus c 1 u plus c 2 v 4th Row 1st Column Blank 2nd Column equals 3rd Column x overTilde comma EndLayoutHx = c1u + c2v −2c1uuTu −2c2uuTv
= c1u + c2v −2c1uTuu −2c2uTvu
= −c1u + c2v
= ˜x,
as in Eq. (4.4). The matrix H is a reﬂector; it has transformed x into its 
reﬂection x overTilde˜x about v. 
A reﬂection is also called a Householder reﬂection or a Householder trans-
formation, and the matrix H is called a Householder matrix or a Householder 
reﬂector. The following properties of H are immediate: 
up
per H u equals negative uHu = −u. 
up
per H v equals vHv = v for any v orthogonal to u. 
up
per H equals upper H Superscript normal upper TH = HT (symmetric). 
up
per H Superscript normal upper T Baseline equals upper H Superscript negative 1HT = H−1 (orthogonal). 
Because H is orthogonal, if upper H x equals x overTildeHx = ˜x, then  parallel to x parallel to Subscript 2 Baseline equals parallel to x overTilde parallel to Subscript 2||x||2 = ||˜x||2 (see Eq. (3.316)), so 
x overTilde Subscript 1 Baseline equals plus or minus parallel to x parallel to Subscript 2˜x1 = ±||x||2. 
The matrix u u Superscript normal upper TuuT is symmetric, idempotent, and of rank 1. A transformation 
by a matrix of the form upper A minus v w Superscript normal upper TA−vwT is often called a “rank-one” update, because 
v w Superscript normal upper TvwT is of rank 1. Thus, a Householder reﬂection is a special rank-one update. 
4.2.1 Zeroing All Elements but One in a Vector 
The usefulness of Householder reﬂections results from the fact that it is easy 
to construct a reﬂection that will transform a vector x into a vector x overTilde˜x that 
has zeros in all but one position, as in Eq. (4.5). To construct the reﬂector of 
x into x overTilde˜x, we ﬁrst need to determine a vector v as in Fig. 4.2 about which to 
reﬂect x. That vector is merely 
x plus x overTilde periodx + ˜x.
Because parallel to x overTilde parallel to Subscript 2 Baseline equals parallel to x parallel to Subscript 2||˜x||2 = ||x||2, we know x overTilde˜x to within the sign; that is, 
x overTilde equals left parenthesis 0 comma ellipsis comma 0 comma plus or minus parallel to x parallel to Subscript 2 Baseline comma 0 comma ellipsis comma 0 right parenthesis period˜x = (0, . . . , 0, ±||x||2, 0, . . . , 0).
We choose the sign so as not to add quantities of diﬀerent signs and possibly 
similar magnitudes. (See the discussions of catastrophic cancellation below 
and beginning on page 554, in Chap. 10.) Hence, we have 
q equals left parenthesis x 1 comma ellipsis comma x Subscript i minus 1 Baseline comma x Subscript i Baseline plus normal s normal i normal g normal n left parenthesis x Subscript i Baseline right parenthesis parallel to x parallel to Subscript 2 Baseline comma x Subscript i plus 1 Baseline comma ellipsis comma x Subscript n Baseline right parenthesis periodq = (x1, . . . , xi−1, xi + sign(xi)||x||2, xi+1, . . . , xn).
(4.8) 
We normalize this to obtain 
u equals q divided by parallel to q parallel to Subscript 2u = q/||q||2
(4.9)

4.2 Householder Transformations (Reﬂections)
227
and ﬁnally form 
upper H equals upper I minus 2 u u Superscript normal upper T Baseline periodH = I −2uuT.
(4.10) 
Consider, for example, the vector 
x equals left parenthesis 3 comma 1 comma 2 comma 1 comma 1 right parenthesis commax = (3, 1, 2, 1, 1),
which we wish to transform into 
x overTilde equals left parenthesis x overTilde Subscript 1 Baseline comma 0 comma 0 comma 0 comma 0 right parenthesis period˜x = (˜x1, 0, 0, 0, 0).
We have 
parallel to x parallel to equals 4 comma||x|| = 4,
so we form the vector 
u equals StartFraction 1 Over StartRoot 56 EndRoot EndFraction left parenthesis 7 comma 1 comma 2 comma 1 comma 1 right parenthesisu =
1
√
56(7, 1, 2, 1, 1)
and the Householder reﬂector 
StartLayout 1st Row 1st Column upper H 2nd Column equals 3rd Column upper I minus 2 u u Superscript normal upper T 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column Start 5 By 5 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 0 5th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 EndMatrix minus one twenty eighth Start 5 By 5 Matrix 1st Row 1st Column 49 2nd Column 7 3rd Column 14 4th Column 7 5th Column 7 2nd Row 1st Column 7 2nd Column 1 3rd Column 2 4th Column 1 5th Column 1 3rd Row 1st Column 14 2nd Column 2 3rd Column 4 4th Column 2 5th Column 2 4th Row 1st Column 7 2nd Column 1 3rd Column 2 4th Column 1 5th Column 1 5th Row 1st Column 7 2nd Column 1 3rd Column 2 4th Column 1 5th Column 1 EndMatrix 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 6th Row 1st Column Blank 2nd Column equals 3rd Column one twenty eighth Start 5 By 5 Matrix 1st Row 1st Column negative 21 2nd Column negative 7 3rd Column negative 14 4th Column negative 7 5th Column negative 7 2nd Row 1st Column negative 7 2nd Column 27 3rd Column negative 2 4th Column negative 1 5th Column negative 1 3rd Row 1st Column negative 14 2nd Column negative 2 3rd Column 24 4th Column negative 2 5th Column negative 2 4th Row 1st Column negative 7 2nd Column negative 1 3rd Column negative 2 4th Column 27 5th Column negative 1 5th Row 1st Column negative 7 2nd Column negative 1 3rd Column negative 2 4th Column negative 1 5th Column 27 EndMatrix EndLayoutH = I −2uuT
=
⎡
⎢⎢⎢⎢⎣
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
⎤
⎥⎥⎥⎥⎦
−1
28
⎡
⎢⎢⎢⎢⎣
49 7 14 7 7
7 1
2 1 1
14 2
4 2 2
7 1
2 1 1
7 1
2 1 1
⎤
⎥⎥⎥⎥⎦
= 1
28
⎡
⎢⎢⎢⎢⎣
−21 −7 −14 −7 −7
−7 27
−2 −1 −1
−14 −2
24 −2 −2
−7 −1
−2 27 −1
−7 −1
−2 −1 27
⎤
⎥⎥⎥⎥⎦
to yield upper H x equals left parenthesis negative 4 comma 0 comma 0 comma 0 comma 0 right parenthesisHx = (−4, 0, 0, 0, 0). 
The procedure, described by Eqs. (4.8), (4.9), and (4.10), zeroes out all but 
the i normal t normal hith element of the given vector. We could of course modify the reﬂector 
matrix so that certain elements of the reﬂected vector are unchanged. 
We will consider these reﬂections further in Sect. 4.6.6, beginning on 
page 242. 
4.2.2 Computational Considerations 
Notice that if we had chosen x overTilde˜x as left parenthesis negative 4 comma 0 comma 0 comma 0 comma 0 right parenthesis(−4, 0, 0, 0, 0), then  u would have been 
left parenthesis negative 1 comma 1 comma 2 comma 1 comma 1 right parenthesis divided by StartRoot 8 EndRoot(−1, 1, 2, 1, 1)/
√
8, and  Hx would have been left parenthesis 4 comma 0 comma 0 comma 0 comma 0 right parenthesis(4, 0, 0, 0, 0), and our objective 
would also have been achieved. In this case, there would have been no nu-
merical rounding problems. If, however, x were such that x 1 almost equals minus parallel to x parallel to Subscript 2x1 ≈−||x||2 in

228
4 Transformations and Factorizations
the addition x 1 plus parallel to x parallel to Subscript 2x1 + ||x||2, “catastrophic cancellation” would occur. For exam-
ple, if x 1 equals negative 3x1 = −3, and  parallel to x parallel to||x|| is computed as 3.0000002, the computation of 
x 1 plus parallel to x parallel to Subscript 2x1 + ||x||2 would lose seven signiﬁcant digits. Of course, it can be the case 
that x 1 almost equals minus parallel to x parallel to Subscript 2x1 ≈−||x||2, only if  x 2 almost equals x 3 almost equals midline horizontal ellipsis almost equals 0x2 ≈x3 ≈· · · ≈0. Nevertheless, we should perform 
computations in such a way as to protect against the worst cases, especially 
if it is easy to do so. 
Standard Householder computations are performed generally as indicated 
above, but there may be minor variations in the order of performing the 
computations that take advantage of speciﬁc computer architectures. There 
are variants of the Householder transformations that are more eﬃcient by 
taking advantage of such architectures as a cache memory or a bank of ﬂoating-
point registers whose contents are immediately available to the computational 
unit. 
4.3 Givens Transformations (Rotations) 
We have brieﬂy discussed geometric transformations that rotate a vector in 
such a way that a speciﬁed element becomes 0 and only one other element in 
the vector is changed. Such a method may be particularly useful if only part 
of the matrix to be transformed is available. These transformations are called 
Givens transformations, or  Givens rotations, or sometimes Jacobi transforma-
tions. 
The basic idea of the rotation, which is a special case of the rotations 
discussed on page 221, can be seen in the case of a vector of length 2. Given 
the vector x equals left parenthesis x 1 comma x 2 right parenthesisx = (x1, x2), we wish to rotate it to  x overTilde equals left parenthesis x overTilde Subscript 1 Baseline comma 0 right parenthesis˜x = (˜x1, 0). As with a reﬂection, 
in the rotation, we also have x overTilde Subscript 1 Baseline equals parallel to x parallel to˜x1 = ||x||. Geometrically, we have the picture 
shown in Fig. 4.3. 
Figure 4.3. Rotation of x onto a coordinate axis 
It is easy to see that the orthogonal matrix 
upper Q equals Start 2 By 2 Matrix 1st Row 1st Column cosine theta 2nd Column sine theta 2nd Row 1st Column minus sine theta 2nd Column cosine theta EndMatrixQ =
[
cos θ sin θ
−sin θ cos θ
]
(4.11)

4.3 Givens Transformations (Rotations)
229
will perform this rotation of x if cosine theta equals x 1 divided by rcos θ = x1/r and sine theta equals x 2 divided by rsin θ = x2/r, where  r equals parallel to x parallel to equals StartRoot x 1 squared plus x 2 squared EndRootr =
||x|| =
/
x2
1 + x2
2. (This is the same matrix as in Eq. (4.3), except that the 
rotation is in the opposite direction.) Notice that thetaθ is not relevant; we only 
need real numbers c and s such that c squared plus s squared equals 1c2 + s2 = 1. 
We have 
StartLayout 1st Row 1st Column x overTilde Subscript 1 2nd Column equals 3rd Column StartFraction x 1 squared Over r EndFraction plus StartFraction x 2 squared Over r EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column parallel to x parallel to comma EndLayout˜x1 = x2
1
r + x2
2
r
= ||x||,
StartLayout 1st Row 1st Column x overTilde Subscript 2 2nd Column equals 3rd Column minus StartFraction x 2 x 1 Over r EndFraction plus StartFraction x 1 x 2 Over r EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column 0 semicolon EndLayout˜x2 = −x2x1
r
+ x1x2
r
= 0;
that is, 
upper Q StartBinomialOrMatrix x 1 Choose x 2 EndBinomialOrMatrix equals StartBinomialOrMatrix parallel to x parallel to Choose 0 EndBinomialOrMatrix periodQ
(
x1
x2
)
=
(
||x||
0
)
.
4.3.1 Zeroing One Element in a Vector 
As with the Householder reﬂection that transforms a vector 
x equals left parenthesis x 1 comma x 2 comma x 3 comma ellipsis comma x Subscript n Baseline right parenthesisx = (x1, x2, x3, . . . , xn)
into a vector 
x overTilde Subscript upper H Baseline equals left parenthesis x overTilde Subscript upper H Baseline 1 Baseline comma 0 comma 0 comma ellipsis comma 0 right parenthesis comma˜xH = (˜xH1, 0, 0, . . . , 0),
it is easy to construct a Givens rotation that transforms x into 
x overTilde Subscript upper G Baseline equals left parenthesis x overTilde Subscript upper G Baseline 1 Baseline comma 0 comma x 3 comma ellipsis comma x Subscript n Baseline right parenthesis period˜xG = (˜xG1, 0, x3, . . . , xn).
We can construct an orthogonal matrix upper G Subscript p qGpq similar to that shown in 
Eq. (4.11) that will transform the vector 
x equals left parenthesis x 1 comma ellipsis comma x Subscript p Baseline comma ellipsis comma x Subscript q Baseline comma ellipsis comma x Subscript n Baseline right parenthesisx = (x1, . . . , xp, . . . , xq, . . . , xn)
into 
x overTilde equals left parenthesis x 1 comma ellipsis comma x overTilde Subscript p Baseline comma ellipsis comma 0 comma ellipsis comma x Subscript n Baseline right parenthesis period˜x = (x1, . . . , ˜xp, . . . , 0, . . . , xn).
The orthogonal matrix that will do this is

230
4 Transformations and Factorizations
upper G Subscript p q Baseline left parenthesis theta right parenthesis equals Start 12 By 12 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 2nd Row 1st Column 0 2nd Column 1 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 3rd Row 1st Column Blank 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank 11th Column Blank 12th Column Blank 4th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 1 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 5th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column c 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column s 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 6th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 1 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 7th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column down right diagonal ellipsis 8th Column Blank 9th Column Blank 10th Column Blank 11th Column Blank 12th Column Blank 8th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 1 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 9th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column negative s 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column c 10th Column 0 11th Column midline horizontal ellipsis 12th Column 0 10th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 1 11th Column midline horizontal ellipsis 12th Column 0 11th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank 11th Column down right diagonal ellipsis 12th Column Blank 12th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 6th Column 0 7th Column midline horizontal ellipsis 8th Column 0 9th Column 0 10th Column 0 11th Column midline horizontal ellipsis 12th Column 1 EndMatrix commaGpq(θ) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 0 · · · 0 0 0 · · · 0 0 0 · · · 0
0 1 · · · 0 0 0 · · · 0 0 0 · · · 0
...
0 0 · · · 1 0 0 · · · 0 0 0 · · · 0
0 0 · · · 0 c 0 · · · 0 s 0 · · · 0
0 0 · · · 0 0 1 · · · 0 0 0 · · · 0
...
0 0 · · · 0 0 0 · · · 1 0 0 · · · 0
0 0 · · · 0 −s 0 · · · 0 c 0 · · · 0
0 0 · · · 0 0 0 · · · 0 0 1 · · · 0
...
0 0 · · · 0 0 0 · · · 0 0 0 · · · 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
,
(4.12) 
where the entries in the p normal t normal hpth and q normal t normal hqth rows and columns are 
c equals StartFraction x Subscript p Baseline Over r EndFractionc = xp
r
and 
s equals StartFraction x Subscript q Baseline Over r EndFraction commas = xq
r ,
where r equals StartRoot x Subscript p Superscript 2 Baseline plus x Subscript q Superscript 2 Baseline EndRootr =
/
x2p + x2q. A rotation matrix is the same as an identity matrix 
with four elements changed. 
Considering x to be the p normal t normal hpth column in a matrix X, we can easily see that 
upper G Subscript p q Baseline upper XGpqX results in a matrix with a zero as the q normal t normal hqth element of the p normal t normal hpth column 
and all except the p normal t normal hpth and q normal t normal hqth rows and columns of upper G Subscript p q Baseline upper XGpqX are the same as 
those of X. 
4.3.2 Givens Rotations That Preserve Symmetry 
If X is a symmetric matrix, we can preserve the symmetry by a transformation 
of the form upper Q Superscript normal upper T Baseline upper X upper QQTXQ, where  Q is any orthogonal matrix. The elements of a 
Givens rotation matrix that is used in this way and with the objective of 
forming zeros in two positions in X simultaneously would be determined in 
the same way as above, but the elements themselves would not be the same. 
We illustrate that below while at the same time considering the problem of 
transforming a value into something other than zero. 
4.3.3 Givens Rotations to Transform to Other Values 
Consider a symmetric matrix X that we wish to transform to the symmetric 
matrix upper X overTilde-
X that has all rows and columns except the p normal t normal hpth and q normal t normal hqth the same as 
those in X and we want a speciﬁed value in the left parenthesis p comma p right parenthesis normal t normal h(p, p)th position of upper X overTilde-
X, say  
x overTilde Subscript p p Baseline equals a-xpp = a. We seek a rotation matrix G such that upper X overTilde equals upper G Superscript normal upper T Baseline upper X upper G -
X = GTXG. We have

4.3 Givens Transformations (Rotations)
231
Start 2 By 2 Matrix 1st Row 1st Column c 2nd Column s 2nd Row 1st Column negative s 2nd Column c EndMatrix Superscript normal upper T Baseline Start 2 By 2 Matrix 1st Row 1st Column x Subscript p p Baseline 2nd Column x Subscript p q Baseline 2nd Row 1st Column x Subscript p q Baseline 2nd Column x Subscript q q Baseline EndMatrix Start 2 By 2 Matrix 1st Row 1st Column c 2nd Column s 2nd Row 1st Column negative s 2nd Column c EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column a 2nd Column x overTilde Subscript p q Baseline 2nd Row 1st Column x overTilde Subscript p q Baseline 2nd Column x overTilde Subscript q q EndMatrix
[
c s
−s c
]T [
xpp xpq
xpq xqq
] [
c s
−s c
]
=
[
a ˜xpq
˜xpq ˜xqq
]
(4.13) 
and 
c squared plus s squared equals 1 periodc2 + s2 = 1.
Hence, 
a equals c squared x Subscript p p Baseline minus 2 c s x Subscript p q Baseline plus s squared x Subscript q q Baseline perioda = c2xpp −2csxpq + s2xqq.
(4.14) 
Writing t equals s divided by ct = s/c (the tangent), we have the quadratic 
left parenthesis x Subscript q q Baseline minus a right parenthesis t squared minus 2 x Subscript p q Baseline t plus x Subscript p p Baseline minus a equals 0(xqq −a)t2 −2xpqt + xpp −a = 0
(4.15) 
with roots 
t equals StartFraction x Subscript p q Baseline plus or minus StartRoot x Subscript p q Superscript 2 Baseline minus left parenthesis x Subscript p p Baseline minus a right parenthesis left parenthesis x Subscript q q Baseline minus a right parenthesis EndRoot Over left parenthesis x Subscript q q Baseline minus a right parenthesis EndFraction periodt =
xpq ±
/
x2pq −(xpp −a)(xqq −a)
(xqq −a)
.
(4.16) 
The roots are real if and only if 
x Subscript p q Superscript 2 Baseline greater than or equals left parenthesis x Subscript p p Baseline minus a right parenthesis left parenthesis x Subscript q q Baseline minus a right parenthesis periodx2
pq ≥(xpp −a)(xqq −a).
If the roots in Eq. (4.16) are real, we choose the nonnegative one. (See the 
discussion of Eq. (10.3) on page 554.) We then form 
c equals StartFraction 1 Over StartRoot 1 plus t squared EndRoot EndFractionc =
1
√
1 + t2
(4.17) 
and 
s equals c t periods = ct.
(4.18) 
The rotation matrix G formed from c and s will transform X into upper X overTilde-
X. 
4.3.4 Fast Givens Rotations 
Often in applications, we need to perform a succession of Givens transforma-
tions. The overall number of computations can be reduced using a succession 
of “fast Givens rotations.” We write the matrix Q in Eq. (4.11) as  CT, 
Start 2 By 2 Matrix 1st Row 1st Column cosine theta 2nd Column sine theta 2nd Row 1st Column minus sine theta 2nd Column cosine theta EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column cosine theta 2nd Column 0 2nd Row 1st Column 0 2nd Column cosine theta EndMatrix Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column tangent theta 2nd Row 1st Column minus tangent theta 2nd Column 1 EndMatrix comma
[
cos θ sin θ
−sin θ cos θ
]
=
[
cos θ
0
0
cos θ
] [
1
tan θ
−tan θ
1
]
,
(4.19) 
and instead of working with matrices such as Q, which require four multipli-
cations and two additions, we work with matrices such as T, involving the 
tangents, which require only two multiplications and two additions. After a 
number of computations with such matrices, the diagonal matrices of the form 
of C are accumulated and multiplied together. 
The diagonal elements in the accumulated C matrices in the fast Givens 
rotations can become widely diﬀerent in absolute values, so to avoid excessive 
loss of accuracy, it is usually necessary to rescale the elements periodically.

232
4 Transformations and Factorizations
4.4 Factorization of Matrices 
It is often useful to represent a matrix A in a factored form, 
upper A equals upper B upper C commaA = BC,
where B and C have some speciﬁed desirable properties, such as being orthog-
onal or being triangular. We generally seek B and C such that B and C have 
useful properties for some particular aspect of the problem being addressed. 
Most direct methods of solving linear systems (discussed in Chap. 5) are  
based on factorizations (or, equivalently, “decompositions”) of the matrix of 
coeﬃcients. Matrix factorizations are also performed for reasons other than 
to solve a linear system, such as in eigenanalysis (discussed in Chap. 6). 
Notice an indeterminacy in the factorization upper A equals upper B upper CA = BC; if  B and C are 
factors of A, then so are  negative upper B−B and negative upper C−C. This indeterminacy includes not only 
the negatives of the matrices themselves but also the negatives of various 
rows and columns properly chosen. More generally, if D and E are matrices 
such that upper D upper E equals upper I Subscript mDE = Im, where  m is the number of columns in B and rows in C, 
then upper A equals upper B upper D upper E upper CA = BDEC, and  so  A can be factored as the product of BD and EC. 
Hence, in general, a factorization is not unique. If restrictions are placed on 
certain properties of the factors, however, then under those restrictions, the 
factorizations may be unique. Also, if one factor is given, the other factor may 
be unique. (For example, in the case of nonsingular matrices, we can see this 
by taking the inverse.) 
Invertible transformations result in a factorization of a matrix. For an n times kn×k
matrix B, if  D is a k times nk × n matrix such that upper B upper D equals upper I Subscript nBD = In, then a given  n times mn × m
matrix A can be factorized as upper A equals upper B upper D upper A equals upper B upper CA = BDA = BC, where  upper C equals upper D upper AC = DA. 
Some important matrix factorizations were listed at the beginning of this 
chapter. Of those, we have already discussed the full rank and the diagonal 
canonical factorizations in Chap. 3. Also in Chap. 3, we have brieﬂy described 
the orthogonally similar canonical factorization and the SVD. We will discuss 
these factorizations, which require iterative methods, further in Chap. 6. In  
the next few sections, we will introduce the LU, LDU, QR, and Cholesky 
factorizations. We will also describe the square root factorization, even though 
it uses eigenvalues, which require the iterative methods. 
Matrix factorizations are generally performed by a sequence of full-rank 
transformations and their inverses. 
4.5 LU and LDU Factorizations 
For any matrix (whether square or not) that can be expressed as LU, where L 
is lower triangular (or lower trapezoidal) and U is upper triangular (or upper 
trapezoidal), the product LU is called the LU factorization. We also generally 
restrict either L or U to have 0s or 1s on the diagonal. If an LU factorization

4.5 LU and LDU Factorizations
233
exists, it is clear that either L or U (but not necessarily both) can be made 
to have only 1s and 0s on its diagonal. 
If an LU factorization exists, both the lower triangular matrix, L, and  
the upper triangular matrix, U, can be made to have only 1s or 0s on their 
diagonals (i.e., be made to be unit lower triangular or unit upper triangular) 
by putting the products of any non-unit diagonal elements into a diagonal 
matrix D and then writing the factorization as LDU, where now L and U are 
unit triangular matrices (i.e., matrices with 1s on the diagonal). This is called 
the LDU factorization. 
If a matrix is not square, or if the matrix is not of full rank, in its LU 
decomposition, L and/or U may have zero diagonal elements or will be of 
trapezoidal form. An example of a singular matrix and its LU factorization is 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column Start 2 By 3 Matrix 1st Row 1st Column 0 2nd Column 3 3rd Column 2 2nd Row 1st Column 0 2nd Column 0 3rd Column 0 EndMatrix 2nd Row 1st Column Blank 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st Column 0 2nd Column 1 EndMatrix Start 2 By 3 Matrix 1st Row 1st Column 0 2nd Column 3 3rd Column 2 2nd Row 1st Column 0 2nd Column 0 3rd Column 0 EndMatrix 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper L upper U period EndLayoutA =
[
0 3 2
0 0 0
]
=
[ 1 0
0 1
] [ 0 3 2
0 0 0
]
= LU.
(4.20) 
In this case, U is an upper trapezoidal matrix. 
4.5.1 Properties: Existence 
Existence and uniqueness of an LU factorization (or LDU factorization) are 
interesting questions. It is neither necessary nor suﬃcient that a matrix be 
nonsingular for it to have an LU factorization. The example above shows the 
LU factorization for a matrix not of full rank. Furthermore, a full rank matrix 
does not necessarily have an LU factorization, as we see next. 
An example of a nonsingular matrix that does not have an LU factorization 
is an identity matrix with permuted rows or columns: 
upper B equals Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 1 2nd Row 1st Column 1 2nd Column 0 EndMatrix periodB =
[
0 1
1 0
]
.
(4.21) 
The conditions for the existence of an LU factorization are not so easy to state 
(see Harville 1997, e.g.), but in practice, as we will see, the question is not 
very relevant. First, however, we will consider a matrix that is guaranteed to 
have an LU factorization and show one method of obtaining it. 
A suﬃcient condition for an n times mn × m matrix A to have an LU factorization 
is that for k equals 1 comma 2 comma ellipsis comma min left parenthesis n comma m right parenthesisk = 1, 2, . . . , min(n, m), each  k times kk × k principal submatrix of A be 
nonsingular. 
The proof is by construction. We assume that all principal submatrices are 
nonsingular. This means that a 11 not equals 0a11 /= 0, and so the Gaussian matrix upper G 11G11 exists. 
(See Eq. (3.77) on page 106, where we also set the notation.) We multiply A 
by upper G 11G11, obtaining

234
4 Transformations and Factorizations
upper G 11 upper A equals upper A Superscript left parenthesis 1 right parenthesis Baseline commaG11A = A(1),
in which a Subscript i Baseline 1 Superscript left parenthesis 1 right parenthesis Baseline equals 0a(1)
i1 = 0 for i equals 2 comma ellipsis comma ni = 2, . . . , n and a 22 Superscript left parenthesis 1 right parenthesis Baseline not equals 0a(1)
22 /= 0 (otherwise, the 2 times 22 × 2 principal 
submatrix would be singular, which by assumption it is not). 
Since a 22 Superscript left parenthesis 1 right parenthesis Baseline not equals 0a(1)
22 /= 0, the Gaussian matrix upper G 22G22 exists, and now we multiply upper G 11 upper AG11A
by upper G 22G22, obtaining 
upper G 22 upper G 11 upper A equals upper A Superscript left parenthesis 2 right parenthesis Baseline commaG22G11A = A(2),
in which a Subscript i Baseline 2 Superscript left parenthesis 2 right parenthesis Baseline equals 0a(2)
i2
= 0 for i equals 3 comma ellipsis comma ni = 3, . . . , n and a 33 Superscript left parenthesis 2 right parenthesis Baseline not equals 0a(2)
33
/= 0 as before. (All a Subscript i Baseline 1 Superscript left parenthesis 1 right parenthesisa(1)
i1
are 
unchanged.) 
We continue in this way for k equals min left parenthesis n comma m right parenthesisk = min(n, m) steps, to obtain 
upper G Subscript k k Baseline midline horizontal ellipsis upper G 22 upper G 11 upper A equals upper A Superscript left parenthesis k right parenthesis Baseline commaGkk · · · G22G11A = A(k),
(4.22) 
in which upper A Superscript left parenthesis k right parenthesisA(k) is upper triangular and the matrix upper G Subscript k k Baseline midline horizontal ellipsis upper G 22 upper G 11Gkk · · · G22G11 is lower 
triangular because each matrix in the product is lower triangular. (Note that 
if m greater than nm > n, upper G Subscript k k Baseline equals upper E Subscript n Baseline left parenthesis 1 divided by a Subscript n n Superscript left parenthesis n minus 1 right parenthesisGkk = En(1/a(n−1)
nn
).) 
Furthermore, each matrix in the product upper G Subscript k k Baseline midline horizontal ellipsis upper G 22 upper G 11Gkk · · · G22G11 is nonsingu-
lar, and the matrix upper G 11 Superscript negative 1 Baseline upper G 22 Superscript negative 1 Baseline midline horizontal ellipsis upper G Subscript k k Superscript negative 1G−1
11 G−1
22 · · · G−1
kk is lower triangular (see Eq. (3.78) on  
page 106). We complete the factorization by multiplying both sides of Eq. (4.22) 
by upper G 11 Superscript negative 1 Baseline upper G 22 Superscript negative 1 Baseline midline horizontal ellipsis upper G Subscript k k Superscript negative 1G−1
11 G−1
22 · · · G−1
kk : 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column upper G 11 Superscript negative 1 Baseline upper G 22 Superscript negative 1 Baseline midline horizontal ellipsis upper G Subscript k k Superscript negative 1 Baseline upper A Superscript left parenthesis k right parenthesis Baseline 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper L upper A Superscript left parenthesis k right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper L upper U period EndLayoutA = G−1
11 G−1
22 · · · G−1
kk A(k)
= LA(k)
= LU.
(4.23) 
Hence, we see that an n times mn × m matrix A has an LU factorization if for k equals 1 comma 2 comma ellipsis comma min left parenthesis n comma m right parenthesisk =
1, 2, . . . , min(n, m), each  k times kk × k principal submatrix of A is nonsingular. 
The elements on the diagonal of upper A Superscript left parenthesis k right parenthesisA(k), that is,  U, are all 1s; hence, we note 
a useful property for square matrices. If the matrix is square (k equals nk = n), then 
normal d normal e normal t left parenthesis upper A right parenthesis equals normal d normal e normal t left parenthesis upper L right parenthesis normal d normal e normal t left parenthesis upper U right parenthesis equals l 11 l 22 midline horizontal ellipsis l Subscript n n Baseline perioddet(A) = det(L)det(U) = l11l22 · · · lnn.
(4.24) 
An alternate construction leaves 1s on the diagonal of L, and then the 
determinant of A is the product of the diagonal elements of U. One  way of  
achieving this factorization, again in the case in which the principal subma-
trices are all nonsingular, is to form the matrices 
upper L Subscript j Baseline equals upper E Subscript n comma j Baseline left parenthesis minus a Subscript n comma j Superscript left parenthesis j minus 1 right parenthesis Baseline divided by a Subscript j j Superscript left parenthesis j minus 1 right parenthesis Baseline right parenthesis midline horizontal ellipsis upper E Subscript j plus 1 comma j Baseline left parenthesis minus a Subscript j plus 1 comma j Superscript left parenthesis j minus 1 right parenthesis Baseline divided by a Subscript j j Superscript left parenthesis j minus 1 right parenthesis Baseline right parenthesis semicolonLj = En,j
(
−a(j−1)
n,j
/a(j−1)
jj
)
· · · Ej+1,j
(
−a(j−1)
j+1,j/a(j−1)
jj
)
;
(4.25) 
that is,

4.5 LU and LDU Factorizations
235
upper L Subscript j Baseline equals Start 7 By 6 Matrix 1st Row 1st Column 1 2nd Column midline horizontal ellipsis 3rd Column 0 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 2nd Row 1st Column Blank 2nd Column down right diagonal ellipsis 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 3rd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 4th Row 1st Column 0 2nd Column midline horizontal ellipsis 3rd Column 1 4th Column 0 5th Column midline horizontal ellipsis 6th Column 0 5th Row 1st Column 0 2nd Column midline horizontal ellipsis 3rd Column minus StartFraction a Subscript j plus 1 comma j Superscript left parenthesis j minus 1 right parenthesis Baseline Over a Subscript j j Superscript left parenthesis j minus 1 right parenthesis Baseline EndFraction 4th Column 1 5th Column midline horizontal ellipsis 6th Column 0 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column down right diagonal ellipsis 6th Column Blank 7th Row 1st Column 0 2nd Column midline horizontal ellipsis 3rd Column minus StartFraction a Subscript n j Superscript left parenthesis j minus 1 right parenthesis Baseline Over a Subscript j j Superscript left parenthesis j minus 1 right parenthesis Baseline EndFraction 4th Column 0 5th Column midline horizontal ellipsis 6th Column 1 EndMatrix periodLj =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 · · ·
0
0 · · · 0
...
0 · · ·
1
0 · · · 0
0 · · · −
a(j−1)
j+1,j
a(j−1)
jj
1 · · · 0
...
0 · · · −
a(j−1)
nj
a(j−1)
jj
0 · · · 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(4.26) 
(See page 108 for the notation “upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a),” representing the elementary axpy 
matrix.) 
Each upper L Subscript jLj is nonsingular, with a determinant of 1. The whole process of 
forward reduction can be expressed as a matrix product, 
upper U equals upper L Subscript k minus 1 Baseline upper L Subscript k minus 2 Baseline ellipsis upper L 2 upper L 1 upper A commaU = Lk−1Lk−2 . . . L2L1A,
(4.27) 
and by the way we have performed the forward reduction, U is an upper 
triangular matrix. The matrix upper L Subscript k minus 1 Baseline upper L Subscript k minus 2 Baseline ellipsis upper L 2 upper L 1Lk−1Lk−2 . . . L2L1 is nonsingular and is unit 
lower triangular (all 1s on the diagonal). Its inverse therefore is also unit lower 
triangular. Call its inverse L; that is,  
upper L equals left parenthesis upper L Subscript k minus 1 Baseline upper L Subscript k minus 2 Baseline ellipsis upper L 2 upper L 1 right parenthesis Superscript negative 1 Baseline periodL = (Lk−1Lk−2 . . . L2L1)−1.
(4.28) 
Thus, the forward reduction is equivalent to expressing A as LU, 
upper A equals upper L upper U periodA = LU.
(4.29) 
In this case, the diagonal elements of the lower triangular matrix L in the LU 
factorization are all 1s by the method of construction, and if A is square, 
normal d normal e normal t left parenthesis upper A right parenthesis equals u 11 u 22 midline horizontal ellipsis u Subscript n n Baseline perioddet(A) = u11u22 · · · unn.
The method of constructing the LU factorization described above is guar-
anteed to work in the case of matrices with all nonsingular principal sub-
matrices. Even if the principal submatrices of a matrix are not nonsingular, 
the matrix may have an LU decomposition, and it may be computable us-
ing a sequence of Gaussian matrices as we did above. As we indicated above, 
the suﬃcient conditions are rather complicated, but not very important in 
practice. 
If the matrix is not of full rank, as the Gaussian matrices are being formed, 
at some point, the diagonal element a Subscript i i Superscript left parenthesis k right parenthesisa(k)
ii will be zero, so the matrix upper G Subscript k kGkk cannot 
be formed. For such cases, we merely form a row of zeros in the lower triangular 
matrix and proceed to the next diagonal element. Even if the matrix is of full 
rank, but not all principal submatrices are of full rank, we would encounter 
this same kind of problem. In applications, we may address these two problems 
similarly, using a technique of pivoting.

236
4 Transformations and Factorizations
4.5.2 Pivoting 
As we have seen, the suﬃcient condition of nonsingularity of all principal 
submatrices is not a necessary requirement for the existence of an LU fac-
torization. We have also seen that, at least in some cases, if it exists, the 
factorization can be performed using Gaussian steps. There are matrices such 
as B in Eq. (4.21), however, for which no LU factorization exists. 
Does this matter? Obviously, it depends on the application; that is, it 
depends on the purpose of using an LU factorization. 
One of the most common applications of an LU factorization is to solve a 
system of linear equations upper A x equals bAx = b. In such applications, interchange of rows 
or columns does not change the problem, so long as the interchanges are made 
appropriately over the entire system. Such an interchange is called pivoting. 
Pivoting is often done not just to yield a matrix with an LU decomposi-
tion; it is routinely done in computations to improve the numerical accuracy. 
Pivoting is generally eﬀected by premultiplication by an elementary permu-
tation matrix upper E Subscript p qEpq (see Eq. (3.80) on page 107 for notation and deﬁnitions). 
Hence, instead of factoring A, we factor an equivalent matrix upper E Subscript left parenthesis pi right parenthesis Baseline upper AE(π)A: 
upper E Subscript left parenthesis pi right parenthesis Baseline upper A equals upper L upper U commaE(π)A = LU,
where L and U are lower triangular or trapezoidal and upper triangular or 
trapezoidal, respectively, and satisfying other restrictions, we wish to impose 
on the LU factorization. In an LDU decomposition, we often choose a permu-
tation matrix so that the diagonal elements of D are nonincreasing. Depending 
on the shape and other considerations of numerical computations, we may also 
permute the columns of the matrix, by postmultiplying by a permutation ma-
trix. In its most general form, we may express the LDU decomposition of the 
matrix A as 
upper E Subscript left parenthesis pi 1 right parenthesis Baseline upper A upper E Subscript left parenthesis pi 2 right parenthesis Baseline equals upper L upper D upper U periodE(π1)AE(π2) = LDU.
(4.30) 
We will discuss pivoting in more detail on page 270 in Chap. 5. 
As we mentioned in Chap. 3, in actual computations, we do not form the 
elementary transformation matrices or the Gaussian matrices explicitly, but 
their formulation in the text allows us to discuss the operations in a systematic 
way and better understand the properties of the operations. 
This is an instance of a principle that we will encounter repeatedly: the 
form of a mathematical expression and the way the expression should 
be evaluated in actual practice may be quite diﬀerent. 
4.5.3 Use of Inner Products 
The use of the elementary matrices described above is eﬀectively a series of 
outer products (columns of the elementary matrices with rows of the matrices 
being operated on).

4.5 LU and LDU Factorizations
237
The LU factorization can also be performed by using inner products. From 
Eq. (4.29), we see 
a Subscript i j Baseline equals sigma summation Underscript k equals 1 Overscript i minus 1 Endscripts l Subscript i k Baseline u Subscript k j Baseline plus u Subscript i j Baseline commaaij =
i−1
E
k=1
likukj + uij,
so 
l Subscript i j Baseline equals StartFraction a Subscript i j Baseline minus sigma summation Underscript k equals 1 Overscript j minus 1 Endscripts l Subscript i k Baseline u Subscript k j Baseline Over u Subscript j j Baseline EndFraction normal f normal o normal r i equals j plus 1 comma j plus 2 comma ellipsis comma n periodlij = aij −Ej−1
k=1 likukj
ujj
for i = j + 1, j + 2, . . . , n.
(4.31) 
The use of computations implied by Eq. (4.31) is called the Doolittle method 
or the Crout method. (There is a slight diﬀerence between the Doolittle 
method and the Crout method: the Crout method yields a decomposition in 
which the 1s are on the diagonal of the U matrix rather than the L matrix.) 
Whichever method is used to form the LU decomposition, n cubed divided by 3n3/3 multiplications 
and additions are required. 
4.5.4 Properties: Uniqueness 
There are clearly many ways indeterminacies can occur in L, D, or  U in an LU 
or LDU factorization in general. (Recall the simple replacement of L and U by 
negative upper L−L and negative upper U−U.) In some cases, indeterminacies can be eliminated or reduced by 
putting restrictions on the factors, but any uniqueness of an LU factorization 
is rather limited. 
If a nonsingular matrix has an LU factorization, the factorization itself in 
general is not unique, but given either L or U, the other factor is unique, as 
we can see by use of inverses. (Recall the form of the inverse of a triangular 
matrix, page 142.) 
For the LDU factorization of a general square matrix, if L and U are 
restricted to be unit triangular matrices, then D is unique. To see this, let 
A be an n times nn × n matrix for which an LDU factorization, and let upper A equals upper L upper D upper UA = LDU, 
with L a lower unit triangular matrix and U an upper unit triangular matrix 
and D a diagonal matrix. (All matrices are n times nn × n.) Now, suppose upper A equals upper L overTilde upper D overTilde upper U overTildeA = -L -D -U, 
where upper L overTilde-L, upper D overTilde-D, and  upper U overTilde-U have the same patterns as L, D, and  U. All of these unit 
triangular matrices have inverses of the same type (see page 142). Now, since 
upper L upper D upper U equals upper L overTilde upper D overTilde upper U overTildeLDU = -L -D -U, premultiplying by upper L overTilde Superscript negative 1-L−1 and postmultiplying by upper U Superscript negative 1U −1, we have  
upper L overTilde Superscript negative 1 Baseline upper L upper D equals upper D overTilde upper U overTilde upper U Superscript negative 1 Baseline semicolon-L−1LD = -D -UU −1;
that is, the diagonal elements of upper L overTilde Superscript negative 1 Baseline upper L upper D-L−1LD and upper D overTilde upper U overTilde upper U Superscript negative 1 -D -UU −1 are the same. By the 
properties of unit triangular matrices given on page 99, we see that those 
diagonal elements are the diagonal elements of D and upper D overTilde-D. Since, therefore, 
upper D equals upper D overTildeD = -D, D in the LDU factorization of a general square matrix is unique. 
4.5.5 Properties of the LDU Factorization of a Square Matrix 
The uniqueness of D in the LDU factorization of a general square matrix is an 
important fact. A related useful fact about the LDU factorization of a general

238
4 Transformations and Factorizations
square matrix A is 
normal d normal e normal t left parenthesis upper A right parenthesis equals product d Subscript i i Baseline commadet(A) =
||
dii,
(4.32) 
which we see from Eqs. (3.95) and  (3.32) on pages 110 and 87, respectively. 
There are other useful properties of the LDU factorization of square ma-
trices with special properties, such as positive deﬁniteness, but we will not 
pursue them here. 
4.6 QR Factorization 
A very useful factorization of a matrix is the product of an orthogonal matrix 
and an upper triangular matrix with nonnegative diagonal elements. Depend-
ing on the shape of A, the shapes of the factors may vary, and even the 
deﬁnition of the factors themselves may be stated diﬀerently. 
Let A be an n times mn × m matrix, and suppose 
upper A equals upper Q upper R commaA = QR,
(4.33) 
where Q is an orthogonal matrix and R is an upper triangular or trapezoidal 
matrix with nonnegative diagonal elements. This is called the QR factorization 
of A. In most applications, n greater than or equals mn ≥m, but if this is not the case, we still have a 
factorization into similar matrices. 
The QR factorization is useful for many tasks in linear algebra. It can be 
used to determine the rank of a matrix (see page 241), to extract eigenvalues 
and eigenvectors (see page 314), to form the singular value decomposition 
(see page 318), and to show various theoretical properties of matrices (see, 
e.g., Exercise 4.4 on page 254). The QR factorization is particularly useful 
in computations for overdetermined systems, as we will see in Sect. 5.6 on 
page 282, and in other computations involving nonsquare matrices. 
If A is square, both factors are square, but when A is not square, there 
are some variations in the form of the factorization. I will consider only the 
case in which the number of rows in A is at least as great as the number of 
columns. The other case is logically similar. 
If n greater than mn > m, there are two diﬀerent forms of the QR factorization. In one 
form, Q is an n times nn × n matrix, and R is an n times mn × m upper trapezoidal matrix 
with zeros in the lower rows. In the other form, Q is an n times mn × m matrix with 
orthonormal columns, and R is an m times mm×m upper triangular matrix. This latter 
form is sometimes called a “skinny” QR factorization. When n greater than mn > m, the  
skinny QR factorization is more commonly used than one with a square Q. 
The two factorizations are essentially the same. If upper R 1R1 is the matrix in the 
skinny factorization and R is the matrix in the full form, they are related as 
upper R equals StartBinomialOrMatrix upper R 1 Choose 0 EndBinomialOrMatrix periodR =
[
R1
0
]
.
(4.34)

4.6 QR Factorization
239
Likewise, the square Q can be partitioned as left bracket upper Q 1 vertical bar upper Q 2 right bracket[Q1 | Q2], and the skinny factor-
ization written as 
upper A equals upper Q 1 upper R 1 periodA = Q1R1.
(4.35) 
In the full form, upper Q Superscript normal upper T Baseline upper Q equals upper I Subscript nQTQ = In, and in the skinny form, upper Q 1 Superscript normal upper T Baseline upper Q 1 equals upper I Subscript mQT
1 Q1 = Im. 
The existence of the QR factorization can be shown by construction using, 
for example, Householder reﬂections, as in Sect. 4.6.6. 
4.6.1 Related Matrix Factorizations 
For the n times mn × m matrix, similar to the factorization in Eq. (4.33), we may have 
upper A equals upper R upper Q commaA = RQ,
where Q is an orthogonal matrix and R is an upper triangular or trapezoidal 
matrix with nonnegative diagonal elements. This is called the RQ factorization 
of A. 
Two other related factorizations, with obvious names, are 
upper A equals upper Q upper LA = QL
and 
upper A equals upper L upper Q commaA = LQ,
where Q is an orthogonal matrix and L is a lower triangular or trapezoidal 
matrix with nonnegative diagonal elements. 
4.6.2 Matrices of Full Column Rank 
If the matrix A is of full column rank (meaning that there are at least as 
many rows as columns and the columns are linearly independent), as in many 
applications in statistics, the R matrix in the QR factorization is full rank. 
Furthermore, in the skinny QR factorization, upper A equals upper Q 1 upper R 1A = Q1R1, upper R 1R1 is nonsingular, 
and the factorization is unique. (Recall that the diagonal elements are required 
to be nonnegative.) 
We see that the factorization is unique by forming upper A equals upper Q 1 upper R 1A = Q1R1 and then 
letting upper Q overTilde Subscript 1 -Q1 and upper R overTilde Subscript 1 -R1 be the skinny QR factorization 
upper A equals upper Q overTilde Subscript 1 Baseline upper R overTilde Subscript 1A = -Q1 -R1
and showing that upper Q overTilde Subscript 1 Baseline equals upper Q 1 -Q1 = Q1 and upper R overTilde Subscript 1 Baseline equals upper R 1 -R1 = R1. Since  upper Q 1 upper R 1 equals upper Q overTilde Subscript 1 Baseline upper R overTilde Subscript 1Q1R1 = -Q1 -R1, we have  
upper Q 1 equals upper Q overTilde Subscript 1 Baseline upper R overTilde Subscript 1 Baseline upper R 1 Superscript negative 1Q1 = -Q1 -R1R−1
1
and so upper R overTilde Subscript 1 Baseline upper R 1 Superscript negative 1 Baseline equals upper Q overTilde Subscript 1 Superscript normal upper T Baseline upper Q 1 -R1R−1
1
= -QT
1 Q1. As we saw on page 142, since  upper R 1R1 is 
upper triangular, upper R 1 Superscript negative 1R−1
1
is upper triangular; and as we saw on page 99, since  upper R overTilde Subscript 1-R1
is upper triangular, upper R overTilde Subscript 1 Baseline upper R 1 Superscript negative 1 -R1R−1
1
is upper triangular. Let T be this upper triangular 
matrix, 
upper T equals upper R overTilde Subscript 1 Baseline upper R 1 Superscript negative 1 Baseline periodT = -R1R−1
1 .
Now consider upper T Superscript normal upper T Baseline upper TT TT. (This is a Cholesky factorization; see Sect. 4.7.2.)

240
4 Transformations and Factorizations
Since upper Q overTilde Subscript 1 Superscript normal upper T Baseline upper Q overTilde Subscript 1 Baseline equals upper I Subscript m -QT
1 -Q1 = Im, we have  upper T Superscript normal upper T Baseline upper T equals upper T Superscript normal upper T Baseline upper Q overTilde Subscript 1 Superscript normal upper T Baseline upper Q overTilde Subscript 1 Baseline upper TT TT = T T -QT
1 -Q1T. Now, because upper Q 1 equals upper Q overTilde Subscript 1 Baseline upper TQ1 = -Q1T, 
we have 
upper T Superscript normal upper T Baseline upper T equals upper Q 1 Superscript normal upper T Baseline upper Q 1 equals upper I Subscript m Baseline periodT TT = QT
1 Q1 = Im.
The only upper triangular matrix T such that upper T Superscript normal upper T Baseline upper T equals upper IT TT = I is the identity I itself. 
(This is from the deﬁnition of matrix multiplication. First, we see that t 11 equals 1t11 = 1, 
and since all oﬀ-diagonal elements in the ﬁrst row of I are 0, all oﬀ-diagonal 
elements in the ﬁrst row of upper T Superscript normal upper TT T must be 0. Continuing in this way, we see that 
upper T equals upper IT = I.) Hence, 
upper R overTilde Subscript 1 Baseline upper R 1 Superscript negative 1 Baseline equals upper I comma-R1R−1
1
= I,
and so 
upper R overTilde Subscript 1 Baseline equals upper R 1 period-R1 = R1.
Now, 
upper Q overTilde Subscript 1 Baseline equals upper Q overTilde Subscript 1 Baseline upper T equals upper Q 1 semicolon-Q1 = -Q1T = Q1;
hence, the skinny factorization is unique. 
Relation to the Moore-Penrose Inverse for Matrices of Full 
Column Rank 
If the matrix A is of full column rank, the Moore-Penrose inverse of A is 
immediately available from the QR factorization: 
upper A Superscript plus Baseline equals Start 1 By 2 Matrix 1st Row 1st Column upper R 1 Superscript negative 1 Baseline 2nd Column 0 EndMatrix upper Q Superscript normal upper T Baseline periodA+ =
[
R−1
1
0
]
QT.
(4.36) 
(The four properties of a Moore-Penrose inverse listed on page 151 are easily 
veriﬁed, and you are asked to do so in Exercise 4.7.) 
4.6.3 Nonfull Rank Matrices 
If A is square but not of full rank, R has the form 
Start 3 By 3 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 EndMatrix period
⎡
⎣
X X X
0 X X
0 0 0
⎤
⎦.
(4.37) 
In the common case in which A has more rows than columns, if A is not of 
full (column) rank, upper R 1R1 in Eq. (4.34) will have the form shown in matrix (4.37). 
If A is not of full rank, we apply permutations to the columns of A by 
multiplying on the right by a permutation matrix. The permutations can be 
taken out by a second multiplication on the right. If A is of rank r (less than or equals m≤m), 
the resulting decomposition consists of three matrices: an orthogonal Q, a  T 
with an r times rr × r upper triangular submatrix, and a permutation matrix upper E Subscript left parenthesis pi right parenthesis Superscript normal upper TET
(π), 
upper A equals upper Q upper T upper E Subscript left parenthesis pi right parenthesis Superscript normal upper T Baseline periodA = QTET
(π).
(4.38)

4.6 QR Factorization
241
The matrix T has the form 
upper T equals Start 2 By 2 Matrix 1st Row 1st Column upper T 1 2nd Column upper T 2 2nd Row 1st Column 0 2nd Column 0 EndMatrix commaT =
[
T1 T2
0
0
]
,
(4.39) 
where upper T 1T1 is upper triangular and is r times rr × r. The decomposition in Eq. (4.38) is  
not unique because of the permutation matrix. The choice of the permuta-
tion matrix is the same as the pivoting that we discussed in connection with 
Gaussian elimination. A generalized inverse of A is immediately available from 
Eq. (4.38): 
upper A Superscript minus Baseline equals upper P Start 2 By 2 Matrix 1st Row 1st Column upper T 1 Superscript negative 1 Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper Q Superscript normal upper T Baseline periodA−= P
[
T −1
1
0
0
0
]
QT.
(4.40) 
Additional orthogonal transformations can be applied from the right-hand 
side of the n times mn × m matrix A in the form of Eq. (4.38) to yield 
upper A equals upper Q upper R upper U Superscript normal upper T Baseline commaA = QRU T,
(4.41) 
where R has the form 
upper R equals Start 2 By 2 Matrix 1st Row 1st Column upper R 1 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix commaR =
[ R1 0
0 0
]
,
(4.42) 
where upper R 1R1 is r times rr × r upper triangular, Q is n times nn × n and as in Eq. (4.38), and 
upper U Superscript normal upper TU T is n times mn × m and orthogonal. (The permutation matrix in Eq. (4.38) is also  
orthogonal, of course.) 
Relation to the Moore-Penrose Inverse 
The decomposition (4.41) is unique, and it provides the unique Moore-Penrose 
generalized inverse of A: 
upper A Superscript plus Baseline equals upper U Start 2 By 2 Matrix 1st Row 1st Column upper R 1 Superscript negative 1 Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper Q Superscript normal upper T Baseline periodA+ = U
[
R−1
1
0
0
0
]
QT.
(4.43) 
4.6.4 Determining the Rank of a Matrix 
It is often of interest to know the rank of a matrix. Given a decomposition of 
the form of Eq. (4.38), the rank is obvious, and in practice, this QR decom-
position with pivoting is a good way to determine the rank of a matrix. The 
QR decomposition is said to be “rank-revealing.” 
For many matrices, the computations are quite sensitive to rounding. Piv-
oting is often required, and even so, the pivoting must be done with some 
care (see Hong and Pan 1992; Section 2.7.3 of Bj¨orck 1996; and Bischof and 
Quintana-Ort´ı 1998a,b). (As we pointed out on page 143, the problem itself is 
ill-posed in Hadamard’s sense because the rank is not a continuous function 
of any of the quantities that determine it. For a given matrix, the problem 
can also be ill-conditioned in the computational sense. Ill-conditioning is a 
major concern, and we will discuss it often in latter chapters of this book. We 
introduce some of the concepts of ill-conditioning formally in Sect. 5.1.)

242
4 Transformations and Factorizations
4.6.5 Formation of the QR Factorization 
There are three good methods for obtaining the QR factorization: Householder 
transformations or reﬂections, Givens transformations or rotations, and the 
(modiﬁed) Gram-Schmidt procedure. Diﬀerent situations may make one of 
these procedures better than the other two. The Householder transforma-
tions described in the next section are probably the most commonly used. 
If the data are available only one row at a time, the Givens transformations 
discussed in Sect. 4.6.7 are very convenient. Whichever method is used to com-
pute the QR decomposition, at least 2 n cubed divided by 32n3/3 multiplications and additions are 
required. The operation count is therefore about twice as great as that for an 
LU decomposition. 
4.6.6 Householder Reﬂections to Form the QR Factorization 
To use reﬂectors to compute a QR factorization, we form in sequence the 
reﬂector for the i normal t normal hith column that will produce 0s below the left parenthesis i comma i right parenthesis(i, i) element. 
For a convenient example, consider the matrix 
upper A equals Start 9 By 5 Matrix 1st Row 1st Column 3 2nd Column negative StartFraction 98 Over 28 EndFraction 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 2nd Row 1st Column Blank 3rd Row 1st Column 1 2nd Column StartFraction 122 Over 28 EndFraction 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 4th Row 1st Column Blank 5th Row 1st Column 2 2nd Column negative eight twenty eighths 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Row 1st Column Blank 7th Row 1st Column 1 2nd Column StartFraction 66 Over 28 EndFraction 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 8th Row 1st Column Blank 9th Row 1st Column 1 2nd Column StartFraction 10 Over 28 EndFraction 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X EndMatrix periodA =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
3 −98
28 X X X
1
122
28 X X X
2 −8
28 X X X
1
66
28 X X X
1
10
28 X X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The ﬁrst transformation would be determined so as to transform left parenthesis 3 comma 1 comma 2 comma 1 comma 1 right parenthesis(3, 1, 2, 1, 1)
to left parenthesis monospace upper X comma 0 comma 0 comma 0 comma 0 right parenthesis(X, 0, 0, 0, 0). We use Eqs. (4.8) through (4.10) to do this.  Call  this  ﬁrst  
Householder matrix upper P 1P1. We have  
upper P 1 upper A equals Start 5 By 5 Matrix 1st Row 1st Column negative 4 2nd Column 1 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 2nd Row 1st Column 0 2nd Column 5 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 3rd Row 1st Column 0 2nd Column 1 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 4th Row 1st Column 0 2nd Column 3 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 5th Row 1st Column 0 2nd Column 1 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X EndMatrix periodP1A =
⎡
⎢⎢⎢⎢⎣
−4 1 X X X
0 5 X X X
0 1 X X X
0 3 X X X
0 1 X X X
⎤
⎥⎥⎥⎥⎦
.
We now choose a reﬂector to transform left parenthesis 5 comma 1 comma 3 comma 1 right parenthesis(5, 1, 3, 1) to left parenthesis negative 6 comma 0 comma 0 comma 0 right parenthesis(−6, 0, 0, 0).  We do not  
want to disturb the ﬁrst column in upper P 1 upper AP1A shown above, so we form upper P 2P2 as 
upper P 2 equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 0 3rd Column ellipsis 4th Column 0 2nd Row 1st Column 0 2nd Column Blank 3rd Column Blank 4th Column Blank 3rd Row 1st Column vertical ellipsis 2nd Column upper H 2 4th Row 1st Column 0 2nd Column Blank 3rd Column Blank 4th Column Blank EndMatrix periodP2 =
⎡
⎢⎢⎢⎣
1 0 . . . 0
0
...
H2
0
⎤
⎥⎥⎥⎦.

4.6 QR Factorization
243
Forming the vector left parenthesis 11 comma 1 comma 3 comma 1 right parenthesis divided by StartRoot 132 EndRoot(11, 1, 3, 1)/
√
132 and proceeding as before, we get the 
reﬂector 
StartLayout 1st Row 1st Column upper H 2 2nd Column equals 3rd Column upper I minus one sixty sixth left parenthesis 11 comma 1 comma 3 comma 1 right parenthesis left parenthesis 11 comma 1 comma 3 comma 1 right parenthesis Superscript normal upper T 2nd Row 1st Column Blank 2nd Column equals 3rd Column one sixty sixth Start 4 By 4 Matrix 1st Row 1st Column negative 55 2nd Column negative 11 3rd Column negative 33 4th Column negative 11 2nd Row 1st Column negative 11 2nd Column 65 3rd Column negative 3 4th Column negative 1 3rd Row 1st Column negative 33 2nd Column negative 3 3rd Column 57 4th Column negative 3 4th Row 1st Column negative 11 2nd Column negative 1 3rd Column negative 3 4th Column 65 EndMatrix period EndLayoutH2 = I −1
66(11, 1, 3, 1)(11, 1, 3, 1)T
= 1
66
⎡
⎢⎢⎣
−55 −11 −33 −11
−11
65 −3 −1
−33 −3
57 −3
−11 −1
−3
65
⎤
⎥⎥⎦.
Now we have 
upper P 2 upper P 1 upper A equals Start 5 By 5 Matrix 1st Row 1st Column negative 4 2nd Column 1 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 2nd Row 1st Column 0 2nd Column negative 6 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 3rd Row 1st Column 0 2nd Column 0 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 4th Row 1st Column 0 2nd Column 0 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 5th Row 1st Column 0 2nd Column 0 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X EndMatrix periodP2P1A =
⎡
⎢⎢⎢⎢⎣
−4
1 X X X
0 −6 X X X
0
0 X X X
0
0 X X X
0
0 X X X
⎤
⎥⎥⎥⎥⎦
.
Continuing in this way for three more steps, we would have the QR decom-
position of A with upper Q Superscript normal upper T Baseline equals upper P 5 upper P 4 upper P 3 upper P 2 upper P 1QT = P5P4P3P2P1. 
The number of computations for the QR factorization of an n times nn × n matrix 
using Householder reﬂectors is 2 n cubed divided by 32n3/3 multiplications and 2 n cubed divided by 32n3/3 additions. 
4.6.7 Givens Rotations to Form the QR Factorization 
Just as we built the QR factorization by applying a succession of Householder 
reﬂections, we can also apply a succession of Givens rotations to achieve the 
factorization. If the Givens rotations are applied directly, the number of com-
putations is about twice as many as for the Householder reﬂections, but if 
fast Givens rotations are used and accumulated cleverly, the number of com-
putations for Givens rotations is not much greater than that for Householder 
reﬂections. As mentioned on page 231, it is necessary to monitor the diﬀer-
ences in the magnitudes of the elements in the C matrix and often necessary 
to rescale the elements. This additional computational burden is excessive 
unless done carefully (see Bindel et al. 2002, for a description of an eﬃcient 
method). 
4.6.8 Gram-Schmidt Transformations to Form the 
QR Factorization 
Gram-Schmidt transformations yield a set of orthonormal vectors that span 
the same space as a given set of linearly independent vectors, StartSet x 1 comma x 2 comma ellipsis comma x Subscript m Baseline EndSet{x1, x2, . . . , xm}. 
Application of these transformations is called the Gram-Schmidt orthogonal-
ization. If the given linearly independent vectors are the columns of a matrix 
A, the Gram-Schmidt transformations ultimately yield the QR factorization of 
A. The basic Gram-Schmidt transformation is shown in Eq. (2.55) on page 48. 
The Gram-Schmidt algorithm for forming the QR factorization is just a 
simple extension of Eq. (2.55); see Exercise 4.9 on page 255.

244
4 Transformations and Factorizations
4.7 Factorizations of Nonnegative Deﬁnite Matrices 
There are factorizations that may not exist except for nonnegative deﬁnite 
matrices or may exist only for such matrices. The LU decomposition, for 
example, exists and is unique for a nonnegative deﬁnite matrix, but may not 
exist for general matrices (without permutations). In this section, we discuss 
two important factorizations for nonnegative deﬁnite matrices, the square root 
and the Cholesky factorization. 
4.7.1 Square Roots 
On page 187, we deﬁned the square root of a nonnegative deﬁnite matrix in 
the natural way and introduced the notation upper A Superscript one halfA
1
2 as the square root of the 
nonnegative deﬁnite n times nn × n matrix A: 
upper A equals left parenthesis upper A Superscript one half Baseline right parenthesis squared periodA =
(
A
1
2
)2
.
(4.44) 
Just as the computation of a square root of a general real number requires 
iterative methods, the computation of the square root of a matrix requires 
iterative methods. In this case, the iterative methods are required for the 
evaluation of the eigenvalues (as we will describe in Chap. 6). Once the eigen-
values are available, the computations are simple, as we describe below. 
Because A is symmetric, it has a diagonal factorization, and because it 
is nonnegative deﬁnite, the elements of the diagonal matrix are nonnegative. 
In terms of the orthogonal diagonalization of A, as on page 187, we write  
upper A Superscript one half Baseline equals upper V upper C Superscript one half Baseline upper V Superscript normal upper TA
1
2 = VC
1
2 V T. 
We now show that this square root of a nonnegative deﬁnite matrix is 
unique among nonnegative deﬁnite matrices. Let A be a (symmetric) nonneg-
ative deﬁnite matrix and upper A equals upper V upper C upper V Superscript normal upper TA = VCV T, and  let  B be a symmetric nonnegative 
deﬁnite matrix such that upper B squared equals upper AB2 = A. We want to show that  upper B equals upper V upper C Superscript one half Baseline upper V Superscript normal upper TB = VC
1
2 V T or that 
upper B minus upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline equals 0B −VC
1
2 V T = 0. Form  
StartLayout 1st Row 1st Column left parenthesis upper B minus upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline right parenthesis left parenthesis upper B minus upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline right parenthesis 2nd Column equals 3rd Column upper B squared minus upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper B minus upper B upper V upper C Superscript one half Baseline upper V Superscript normal upper T plus left parenthesis upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline right parenthesis squared 2nd Row 1st Column Blank 2nd Column equals 3rd Column 2 upper A minus upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper B minus left parenthesis upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper B right parenthesis Superscript normal upper T Baseline period EndLayout
(
B −VC
1
2 V T) (
B −VC
1
2 V T)
= B2 −VC
1
2 V TB −BVC
1
2 V T +
(
VC
1
2 V T)2
= 2A −VC
1
2 V TB −
(
VC
1
2 V TB
)T
.
(4.45) 
Now, we want to show that upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper B equals upper AVC
1
2 V TB = A. The argument below follows 
(Harville 1997). Because B is nonnegative deﬁnite, we can write upper B equals upper U upper D upper U Superscript normal upper TB = UDU T
for an orthogonal n times nn × n matrix U and a diagonal matrix D with nonnegative 
elements, d 1 comma ellipsis d Subscript n Baselined1, . . . dn. We ﬁrst want to show that  upper V Superscript normal upper T Baseline upper U upper D equals upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper UV TUD = C
1
2 V TU. We have

4.7 Factorizations of Nonnegative Deﬁnite Matrices
245
StartLayout 1st Row 1st Column upper V Superscript normal upper T Baseline upper U upper D squared 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline upper U upper D upper U Superscript normal upper T Baseline upper U upper D upper U Superscript normal upper T Baseline upper U 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline upper B squared upper U 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline upper A upper U 4th Row 1st Column Blank 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline left parenthesis upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline right parenthesis squared upper U 5th Row 1st Column Blank 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper U 6th Row 1st Column Blank 2nd Column equals 3rd Column upper C upper V Superscript normal upper T Baseline upper U period EndLayoutV TUD2 = V TUDU TUDU TU
= V TB2U
= V TAU
= V T(VC
1
2 V T)2U
= V TVC
1
2 V TVC
1
2 V TU
= CV TU.
Now consider the individual elements in these matrices. Let z Subscript i jzij be the left parenthesis i j right parenthesis normal t normal h(ij)th
element of upper V Superscript normal upper T Baseline upper UV TU, and since upper D squaredD2 and C are diagonal matrices, the left parenthesis i j right parenthesis normal t normal h(ij)th element 
of upper V Superscript normal upper T Baseline upper U upper D squaredV TUD2 is d Subscript j Superscript 2 Baseline z Subscript i jd2
jzij and the corresponding element of upper C upper V Superscript normal upper T Baseline upper UCV TU is c Subscript i Baseline z Subscript i jcizij, and these 
two elements are equal, so d Subscript j Baseline z Subscript i j Baseline equals StartRoot c Subscript i Baseline EndRoot z Subscript i jdjzij = √cizij. These, however, are the left parenthesis i j right parenthesis normal t normal h(ij)th
elements of upper V Superscript normal upper T Baseline upper U upper DV TUD and upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper UC
1
2 V TU, respectively; hence, upper V Superscript normal upper T Baseline upper U upper D equals upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper UV TUD = C
1
2 V TU. We  
therefore have 
upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper B equals upper V upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper U upper D upper U Superscript normal upper T Baseline equals upper V upper C Superscript one half Baseline upper C Superscript one half Baseline upper V Superscript normal upper T Baseline upper U upper U Superscript normal upper T Baseline equals upper V upper C upper V Superscript normal upper T Baseline equals upper A periodV C
1
2 V TB = V C
1
2 V TUDU T = V C
1
2 C
1
2 V TUU T = V CV T = A.
We conclude that upper V upper C Superscript one half Baseline upper V Superscript normal upper TVC
1
2 V T is the unique square root of A. 
If A is positive deﬁnite, it has an inverse, and the unique square root of 
the inverse is denoted as upper A Superscript negative one halfA−1
2 . 
4.7.2 Cholesky Factorization 
If the matrix A is symmetric and nonnegative deﬁnite, another important 
factorization is the Cholesky decomposition. In this factorization, 
upper A equals upper T Superscript normal upper T Baseline upper T commaA = T TT,
(4.46) 
where T is an upper triangular matrix with nonnegative diagonal elements. We 
occasionally denote the Cholesky factor of A (i.e., T in the expression above) 
as upper A Subscript normal upper CAC. (Notice on page 59 and later on page 403 that we use a lowercase c 
subscript to represent a centered vector or matrix.) 
The factor T in the Cholesky decomposition is sometimes called the square 
root, but we have deﬁned a diﬀerent matrix as the square root, upper A Superscript one halfA
1
2 (page 187 
and Sect. 4.7.1). The Cholesky factor is more useful in practice, but the square 
root has more applications in the development of the theory. 
We ﬁrst consider the Cholesky decomposition of a positive deﬁnite matrix 
A. In that case, a factor of the form of T in Eq. (4.46) is unique up to the 
sign, just as a square root is. To make the Cholesky factor unique, we require 
that the diagonal elements be positive. The elements along the diagonal of T 
will be square roots. Notice, for example, that t 11t11 is StartRoot a 11 EndRoot√a11. 
Algorithm 4.1 is a method for constructing the Cholesky factorization of 
a positive deﬁnite matrix A. The algorithm serves as the basis for a construc-
tive proof of the existence and uniqueness of the Cholesky factorization (see 
Exercise 4.5 on page 254). The uniqueness is seen by factoring the principal 
square submatrices.

246
4 Transformations and Factorizations
Algorithm 4.1 Cholesky Factorization of a Positive Deﬁnite Matrix 
1. Let t 11 equals StartRoot a 11 EndRoott11 = √a11. 
2. For j equals 2 comma ellipsis comma nj = 2, . . . , n, let  t Subscript 1 j Baseline equals a Subscript 1 j Baseline divided by t 11t1j = a1j/t11. 
3. For i equals 2 comma ellipsis comma ni = 2, . . . , n, 
{ 
let t Subscript i i Baseline equals StartRoot a Subscript i i Baseline minus sigma summation Underscript k equals 1 Overscript i minus 1 Endscripts t Subscript k i Superscript 2 Baseline EndRoottii =
/
aii −Ei−1
k=1 t2
ki, and  
for j equals i plus 1 comma ellipsis comma nj = i + 1, . . . , n, 
{ 
let t Subscript i j Baseline equals left parenthesis a Subscript i j Baseline minus sigma summation Underscript k equals 1 Overscript i minus 1 Endscripts t Subscript k i Baseline t Subscript k j Baseline right parenthesis divided by t Subscript i itij = (aij −Ei−1
k=1 tkitkj)/tii. 
} 
} 
There are other algorithms for computing the Cholesky decomposition. 
The method given in Algorithm 4.1 is sometimes called the inner product 
formulation because the sums in step 3 are inner products. The algorithms 
for computing the Cholesky decomposition are numerically stable. Although 
the order of the number of computations is the same, there are only about half 
as many computations in the Cholesky factorization as in the LU factorization. 
Another advantage of the Cholesky factorization is that there are only n left parenthesis n plus 1 right parenthesis divided by 2n(n +
1)/2 unique elements as opposed to n squared plus nn2 + n in the LU factorization. 
The Cholesky decomposition can also be formed as upper T overTilde Superscript normal upper T Baseline upper D upper T overTilde -T TD -T, where  D is a di-
agonal matrix that allows the diagonal elements of upper T overTilde-T to be computed without 
taking square roots. This modiﬁcation is sometimes called a Banachiewicz 
factorization or root-free Cholesky. The Banachiewicz factorization can be 
formed in essentially the same way as the Cholesky factorization shown in 
Algorithm 4.1; just put 1s along the diagonal of T, and store the squared 
quantities in a vector d. 
Cholesky Decomposition of Singular Nonnegative Deﬁnite 
Matrices 
Any symmetric nonnegative deﬁnite matrix has a decomposition similar to 
the Cholesky decomposition for a positive deﬁnite matrix. If A is n times nn × n with 
rank r, there exists a unique matrix T such that upper A equals upper T Superscript normal upper T Baseline upper TA = T TT, where  T is an 
upper triangular matrix with r positive diagonal elements and n minus rn −r rows 
containing all zeros. The algorithm is the same as Algorithm 4.1, except that 
in step 3 if t Subscript i i Baseline equals 0tii = 0, the entire row is set to zero. The algorithm serves as a 
constructive proof of the existence and uniqueness. 
Relations to Other Factorizations 
For a symmetric matrix, the LDU factorization is upper U Superscript normal upper T Baseline upper D upper UU TDU; hence, we have for 
the Cholesky factor 
upper T equals upper D Superscript one half Baseline upper U commaT = D
1
2 U,

4.7 Factorizations of Nonnegative Deﬁnite Matrices
247
where upper D Superscript one halfD
1
2 is the matrix whose elements are the square roots of the correspond-
ing elements of D. (This is consistent with our notation above for Cholesky 
factors; upper D Superscript one halfD
1
2 is the Cholesky factor of D, and it is symmetric.) 
The LU and Cholesky decompositions generally are applied to square ma-
trices. However, many of the linear systems that occur in scientiﬁc applications 
are overdetermined; that is, there are more equations than there are variables, 
resulting in a nonsquare coeﬃcient matrix. 
For the n times mn × m matrix A with n greater than or equals mn ≥m, we can  write  
StartLayout 1st Row 1st Column upper A Superscript normal upper T Baseline upper A 2nd Column equals 3rd Column upper R Superscript normal upper T Baseline upper Q Superscript normal upper T Baseline upper Q upper R 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper R Superscript normal upper T Baseline upper R comma EndLayoutATA = RTQTQR
= RTR,
(4.47) 
so we see that the matrix R in the QR factorization is (or at least can be) 
the same as the  matrix  T in the Cholesky factorization of upper A Superscript normal upper T Baseline upper AATA. There  is  
some ambiguity in the Q and R matrices, but if the diagonal entries of R are 
required to be nonnegative, the ambiguity disappears, and the matrices in the 
QR decomposition are unique. 
An overdetermined system may be written as 
upper A x almost equals b commaAx ≈b,
where A is n times mn × m (n greater than or equals mn ≥m), or it may be written as 
upper A x equals b plus e commaAx = b + e,
where e is an n-vector of possibly arbitrary “errors.” Because not all equations 
can be satisﬁed simultaneously, we must deﬁne a meaningful “solution.” A 
useful solution is an x such that e has a small norm. The most common 
deﬁnition is an x such that e has the least Euclidean norm, that is, such that 
the sum of squares of the e Subscript ieis is minimized. 
It is easy to show that such an x satisﬁes the square system upper A Superscript normal upper T Baseline upper A x equals upper A Superscript normal upper T Baseline bATAx = ATb, 
the “normal equations.” This expression is important and allows us to analyze 
the overdetermined system (not just to solve for the x but to gain some better 
understanding of the system). It is easy to show that if A is of full rank (i.e., 
of rank m, all of its columns are linearly independent or, redundantly, “full 
column rank”), then upper A Superscript normal upper T Baseline upper AATA is positive deﬁnite. Therefore, we could apply either 
Gaussian elimination or the Cholesky decomposition to obtain the solution. 
As we have emphasized many times before, however, useful conceptual 
expressions are not necessarily useful as computational formulations. That is 
sometimes true in this case also. In Sect. 5.1, we will discuss issues relating to 
the expected accuracy in the solutions of linear systems. There we will deﬁne 
a “condition number.” Larger values of the condition number indicate that 
the expected accuracy is less. We will see that the condition number of upper A Superscript normal upper T Baseline upper AATA is 
the square of the condition number of A. Given these facts, we conclude that 
it may be better to work directly on A rather than on upper A Superscript normal upper T Baseline upper AATA, which appears 
in the normal equations. We discuss solutions of overdetermined systems in

248
4 Transformations and Factorizations
Sect. 5.6, beginning on page 282, and in Sect. 5.7, beginning on page 290. 
Overdetermined systems are also a main focus of the statistical applications 
in Chap. 9. 
4.7.3 Factorizations of a Gramian Matrix 
The sums of squares and cross products matrix, the Gramian matrix upper X Superscript normal upper T Baseline upper XXTX, 
formed from a given matrix X, arises often in linear algebra. We discuss 
properties of the sums of squares and cross products matrix beginning on 
page 397. Now we consider some additional properties relating to various 
factorizations. 
First we observe that upper X Superscript normal upper T Baseline upper XXTX is symmetric and hence has an orthogonally 
similar canonical factorization, 
upper X Superscript normal upper T Baseline upper X equals upper V upper C upper V Superscript normal upper T Baseline periodXTX = V CV T.
We have already observed that upper X Superscript normal upper T Baseline upper XXTX is nonnegative deﬁnite, and so it has 
the LU factorization 
upper X Superscript normal upper T Baseline upper X equals upper L upper U commaXTX = LU,
with L lower triangular and U upper triangular, and it has the Cholesky 
factorization 
upper X Superscript normal upper T Baseline upper X equals upper T Superscript normal upper T Baseline upper TXTX = T TT
with T upper triangular. With upper L equals upper T Superscript normal upper TL = T T and upper U equals upper TU = T, both factorizations are 
the same. In the LU factorization, the diagonal elements of either L or U 
are often constrained to be 1, and hence, the two factorizations are usually 
diﬀerent. 
It is instructive to relate the factors of the m times mm × m matrix upper X Superscript normal upper T Baseline upper XXTX to the 
factors of the n times mn × m matrix X. Consider the QR factorization 
upper X equals upper Q upper R commaX = QR,
where R is upper triangular. Then upper X Superscript normal upper T Baseline upper X equals left parenthesis upper Q upper R right parenthesis Superscript normal upper T Baseline upper Q upper R equals upper R Superscript normal upper T Baseline upper RXTX = (QR)TQR = RTR, so  R is the 
Cholesky factor T because the factorizations are unique (again, subject to the 
restrictions that the diagonal elements be nonnegative). 
Consider the SVD factorization 
upper X equals upper U upper D upper V Superscript normal upper T Baseline periodX = UDV T.
We have upper X Superscript normal upper T Baseline upper X equals left parenthesis upper U upper D upper V Superscript normal upper T Baseline right parenthesis Superscript normal upper T Baseline upper U upper D upper V Superscript normal upper T Baseline equals upper V upper D squared upper V Superscript normal upper TXTX = (UDV T)TUDV T = V D2V T, which is the orthogonally sim-
ilar canonical factorization of upper X Superscript normal upper T Baseline upper XXTX. The eigenvalues of upper X Superscript normal upper T Baseline upper XXTX are the squares 
of the singular values of X, and the condition number of upper X Superscript normal upper T Baseline upper XXTX (which we 
deﬁne in Sect. 5.1) is the square of the condition number of X.

4.8 Approximate Matrix Factorization
249
4.8 Approximate Matrix Factorization 
It is occasionally of interest to form a factorization that approximates a given 
matrix. For a given matrix A, we may have factors B and C, where  
upper A overTilde equals upper B upper C comma-A = BC,
and upper A overTilde-A is an approximation to A. (See Sect. 3.12, beginning on page 198, for  
discussions of approximation of matrices.) 
The approximate factorization 
upper A almost equals upper B upper CA ≈BC
may be useful for various reasons. The computational burden of an exact 
factorization may be excessive. Alternatively, the matrices B and C may have 
desirable properties that no exact factors of A possess. 
In this section, we discuss two kinds of approximate factorizations, one 
motivated by the properties of the matrices and the other merely an incom-
plete factorization, which may be motivated by computational expediency or 
by other considerations. 
4.8.1 Nonnegative Matrix Factorization 
If A is an n times mn × m matrix all of whose elements are nonnegative, it may be of 
interest to approximate A as 
upper A almost equals upper W upper H commaA ≈WH,
where W is n times rn × r and H is r times mr × m, and  both  W and H have only nonnegative 
elements. Such matrices are called nonnegative matrices. If all elements of a 
matrix are positive, the matrix is called a positive matrix. Nonnegative and 
positive matrices arise often in applications and have a number of interesting 
properties. These kinds of matrices are the subject of Sect. 8.7, beginning on 
page 409. 
Clearly, if r greater than or equals min left parenthesis n comma m right parenthesisr ≥min(n, m), the factorization upper A equals upper W upper HA = WH exists exactly, for 
if r equals min left parenthesis n comma m right parenthesisr = min(n, m), then  upper A equals upper I Subscript r Baseline upper WA = IrW, which is not unique since for a greater than 0a > 0, upper A equals left parenthesis StartFraction 1 Over a EndFraction right parenthesis upper I Subscript r Baseline left parenthesis a upper H right parenthesisA =
( 1
a)Ir(aH). If,  however,  r less than min left parenthesis n comma m right parenthesisr < min(n, m), the factorization may not exist. 
A nonnegative matrix factorization (NMF) of the nonnegative matrix A 
for a given r is the expression WH, where  the  n times rn × r matrix W and the r times mr × m
matrix H are nonnegative, and the diﬀerence upper A minus upper W upper HA −WH is minimum according 
to some criterion (see page 199); that is, given r, the NMF factorization of 
the n times mn × m nonnegative matrix A are the matrices W and H satisfying 
StartLayout 1st Row 1st Column min Underscript upper W element of upper R Superscript n times r Baseline comma upper H element of upper R Superscript r times m Baseline Endscripts 2nd Column rho left parenthesis upper A minus upper W upper H right parenthesis 2nd Row 1st Column normal s period normal t period 2nd Column upper W comma upper H greater than or equals 0 comma EndLayout
min
W ∈Rn×r,H∈Rr×m ρ(A −WH)
s.t. W, H ≥0,
(4.48) 
where rhoρ is a measure of the size of upper A minus upper W upper HA −WH. Interest in this factorization 
arose primarily in the problem of analysis of text documents (see page 376).

250
4 Transformations and Factorizations
Most methods for solving the optimization problem (4.48) follow the al-
ternating variables approach: for ﬁxed upper W Superscript left parenthesis 0 right parenthesisW (0), determine an optimal upper H Superscript left parenthesis 1 right parenthesisH(1); then  
given optimal upper H Superscript left parenthesis k right parenthesisH(k), determine an optimal upper W Superscript left parenthesis k plus 1 right parenthesisW (k+1); and for optimal upper W Superscript left parenthesis k plus 1 right parenthesisW (k+1), 
determine an optimal upper H Superscript left parenthesis k plus 1 right parenthesisH(k+1). 
The ease of solving the optimization problem (4.48), whether or not the 
alternating variables approach is used, depends on the nature of rhoρ. Generally, 
rhoρ is a norm, but the Kullback-Leibler divergence (see page 199) may  also  be  
used. Often, rhoρ is chosen to be a Frobenius p norm, because that matrix norm 
can be expressed as a normal upper L Subscript pLp vector norm, as shown in Eq. (3.329) on page 193. 
Furthermore, if the ordinary Frobenius norm is chosen (i.e., p equals 2p = 2), then each 
subproblem is just a constrained linear least squares problem (as discussed on 
page 507). 
Often in applications, the matrix A to be factored is sparse. Computational 
methods for the factorization that take advantage of the sparsity can lead to 
improvements in the computational eﬃciency of orders of magnitude. 
4.8.2 Incomplete Factorizations 
Often instead of an exact factorization, an approximate or “incomplete” fac-
torization may be more useful because of its computational eﬃciency. This 
may be the case in the context of an iterative algorithm in which a matrix 
is being successively transformed, and, although a factorization is used in 
each step, the factors from a previous iteration are adequate approximations. 
Another common situation is in working with sparse matrices. Many exact 
operations on a sparse matrix yield a dense matrix; however, we may want to 
preserve the sparsity, even at the expense of losing exact equalities. When a 
zero position in a sparse matrix becomes nonzero, this is called “ﬁll-in,” and 
we want to avoid that. 
For example, instead of an LU factorization of a sparse matrix A, we may  
seek lower and upper triangular factors upper L overTilde-L and upper U overTilde-U, such that 
upper A almost equals upper L overTilde upper U overTilde commaA ≈-L-U,
(4.49) 
and if a Subscript i j Baseline equals 0aij = 0, then  l overTilde Subscript i j Baseline equals u overTilde Subscript i j Baseline equals 0˜lij = ˜uij = 0. This approximate factorization is easily 
accomplished by modifying the Gaussian elimination step that leads to the 
outer product algorithm of Eqs. (4.27) and  (4.28). 
More generally, we may choose a set of indices upper S equals left brace left parenthesis p comma q right parenthesis right braceS = {(p, q)} and modify 
the elimination step, for m greater than or equals im ≥i, to be  
a Subscript i j Superscript left parenthesis k plus 1 right parenthesis Baseline left arrow StartLayout Enlarged left brace 1st Row 1st Column a Subscript i j Superscript left parenthesis k right parenthesis Baseline minus a Subscript m j Superscript left parenthesis k right parenthesis Baseline a Subscript i j Superscript left parenthesis k right parenthesis Baseline divided by a Subscript j j Superscript left parenthesis k right parenthesis Baseline 2nd Column Blank 3rd Column if left parenthesis i comma j right parenthesis element of upper S 2nd Row 1st Column a Subscript i j Baseline 2nd Column Blank 3rd Column otherwise period EndLayouta(k+1)
ij
←
(
a(k)
ij −a(k)
mja(k)
ij /a(k)
jj
if (i, j) ∈S
aij
otherwise.
(4.50) 
Note that a Subscript i jaij does not change unless left parenthesis i comma j right parenthesis(i, j) is in S. This allows us to preserve 
0s in L and U corresponding to given positions in A.

4.8 Approximate Matrix Factorization
251
Appendix: R Functions for Matrix Computations and 
for Graphics 
In addition to the operators such as +, *, ^, and  %*%, and the vector functions 
of Chap. 2 that can be applied to the rows or columns of a matrix by use of the 
apply function discussed in the appendix to Chap. 3, R provides an extensive 
set of functions for operations on matrices. 
Some of the R functions for operations on matrices are shown in Table 4.2. 
Online documentation for these functions can be obtained in R by use of “?” 
followed by the name of the function. 
Table 4.2. Some R functions for matrix computations 
tr {matlib} 
Trace of a square matrix 
norm
Matrix norm ( normal upper L 1L1, normal upper L 2L2, normal upper L Subscript normal infinityL∞, and  Frobenius)  
rcond
Matrix condition number 
(more options in Matrix package) 
det
Determinant 
lu {Matrix} 
LU decomposition 
qr
QR decomposition (with auxiliary functions qr.Q and qr.R) 
chol
Cholesky factorization 
svd
Singular value decomposition 
eigen
Eigenvalues and eigenvectors 
expm {Matrix} Matrix exponential 
ginv {MASS} 
Compute the Moore-Penrose matrix inverse 
Ginv {matlib} Compute a generalized matrix inverse using Gaussian elimination 
Lists in R Output 
R is an object-oriented system, and it has an extensive set of pre-deﬁned 
objects and methods. One of the most useful types of R objects is the list, 
which is a one-dimensional grouping of other R objects that can be accessed 
by their index within the list (starting at 1) or by their names. 
Individual named components of a list can be extracted by the $ operator. 
The name of a component of a list is  
name of list$name of component in list 
In contrast to many software packages that are designed just to print the 
results of an analysis, R is a functional programming system. While it may 
print some results, the general design is built on the idea that the user may 
want to continue computing; the results of one step in the analysis will lead 
to further steps, involving more computations. The user normally stores the 
output of an R function in an R object for subsequent use. The class of that 
object depends on the function. 
Lists are very useful for the representing the output of the many R func-
tions that produce rather complicated results, such as chol or svd. The  result  
of the function is a list. The components of the output list may be any types of 
R objects. See page 65 for introductory comments on the list class, including 
indexing in lists.

252
4 Transformations and Factorizations
The results of a singular value decomposition, for example, consists of 
three parts, two orthogonal matrices and a diagonal matrix. The R function 
svd returns those results in a single list object whose components have ﬁxed 
names. The names of a list object can be determined by names function. 
The following example illustrates some simple manipulations with the list 
udv, which is the name I gave to the output of the function svd. 
> M <- matrix(c(1,0, -1,1, -2,-1),nrow=2) 
> udv <- svd(M) 
> names(udv) 
[1] "d" "u" "v" 
> udv 
$d 
[1] 2.497212 1.328131 
$u
[,1]
[,2] 
[1,] -0.9732490 -0.2297529 
[2,] -0.2297529 0.9732490 
$v
[,1]
[,2] 
[1,] -0.3897342 -0.1729896 
[2,] 0.2977305 0.9057856 
[3,] 0.8714722 -0.3868166 
> udv$d 
[1] 2.497212 1.328131 
> udv$d[2] 
[1] 1.328131 
> udv[1] 
$d 
[1] 2.497212 1.328131 
> udv[[1]][2] 
[1] 1.328131 
> udv[2] 
$u
[,1]
[,2] 
[1,] -0.9732490 -0.2297529 
[2,] -0.2297529 0.9732490 
> udv$u
[,1]
[,2] 
[1,] -0.9732490 -0.2297529 
[2,] -0.2297529 0.9732490 
> class(udv) 
[1] "list" 
> class(udv$d) 
[1] "numeric" 
> class(udv$u) 
[1] "matrix" "array"

4.8 Approximate Matrix Factorization
253
Graphics in R 
R has extensive graphics capabilities. Most of the graphics functions have a 
very simple interface with default settings, but they also allow speciﬁcation 
of several graphical characteristics, such as line color and thickness, labeling 
of the axes, size of characters, and so on. The R function par allows the user 
to set many graphical parameters, such as margins, colors, line types, and so 
on. (Just type ?par to see all of these options.) 
I recommend use of the ggplot2 package for graphics in R (see Wickham 
2016). 
Two-dimensional graphs display the relationship between two variables, 
or else they display the frequency distribution of one or more variables. An 
R function that displays two variables in a simple coordinate system is plot. 
Functions that represent the frequency distribution are hist and density. 
A display surface is essentially two-dimensional, so to display a third di-
mension, either we attempt to simulate a third dimension by means of per-
spective, or we show the third dimension by separate symbols, such as contour 
lines. An R function useful for viewing a three-dimensional surface is persp, 
which shows a surface from an angle that the user speciﬁes. The R function 
contour produces contour plots. 
The R function expression is useful in labeling graphical displays. 
Figure 2.2 on page 49 was produced by the following code. 
library(MASS) 
eqscplot(c(-.5,1.1),c(-.1,1.1),axes=F,xlab="",ylab="", 
type="n") 
arrows(0,0,.4,.8,length=.1,lwd=2) # scaled x2, slope = 2 
text(.415,.825,expression(italic(x)[2])) 
arrows(0,0,1.0,.5,length=.1) # x1, slope = 1/2 
text(1.05,.51,expression(italic(x)[1])) 
arrows(0,0,.8,.4,length=.1) # x1, slope = 1/2 
text(.78,.47,cex=1.5,expression(tilde(italic(x))[1])) 
lines(c(.4,.64),c(.8,.32)) # perpendicular, slope = -2 
arrows(0,0,.64,.32,length=.1,lwd=2) # projection 
lines(c(.52,.6),c(.36,.4)) ; lines(c(.52,.56),c(.36,.28)) 
text(.64,.25,expression(italic(p))) 
text(.65,.18,"projection onto") 
text(.64,.10, expression(tilde(italic(x))[1])) 
arrows(0,0,-.24,.48,length=.1)
# x2 - projection 
text(-.22,.48,pos=4,expression(italic(x)[2]-italic(p))) 
sqlengthx2<-.8^2+.47^2 
x21 <- -sqrt(sqlengthx2/5) 
x22 <- -2*x21 
arrows(0,0,x21,x22,length=.1)
# x2tilde 
text(x21-.01,x22+.1,cex=1.5,expression(tilde(italic(x))[2]))

254
4 Transformations and Factorizations
Exercises 
4.1. Consider the transformation of the 3-vector x that ﬁrst rotates the vector 
30◦ about the x1 axis, then rotates the vector 45◦ about the x2 axis, and 
then translates the vector by adding the 3-vector y. Find the matrix 
A that eﬀects these transformations by a single multiplication. Use the 
vector xh of homogeneous coordinates that corresponds to the vector x. 
(Thus, A is 4 × 4.) 
4.2. Determine the rotation matrix that rotates 3-vectors through an angle of 
30◦ in the plane x1 + x2 + x3 = 0.  
4.3. Let A = LU be the LU decomposition of the n × n matrix A. 
a) Suppose we multiply the jth column of A by cj, j = 1, 2, . . . n, to  
form the matrix Ac. What is the LU decomposition of Ac? Try  to  
express your answer in a compact form. 
b) Suppose we multiply the ith row of A by ci, i = 1, 2, . . . n, to form  
the matrix Ar. What is the LU decomposition of Ar? Try to express 
your answer in a compact form. 
c) What application might these relationships have? 
4.4. Use the QR decomposition to prove Hadamard’s inequality: 
StartAbsoluteValue normal d normal e normal t left parenthesis upper A right parenthesis EndAbsoluteValue less than or equals product Underscript j equals 1 Overscript n Endscripts parallel to a Subscript j Baseline parallel to Subscript 2 Baseline comma|det(A)| ≤
n
||
j=1
||aj||2,
where A is an n × n matrix, whose columns are the same as the vectors 
aj. Equality holds if and only if either the aj are mutually orthogonal or 
some aj is zero. 
4.5. Show that if A is positive deﬁnite, there exists a unique upper triangular 
matrix T with positive diagonal elements such that 
upper A equals upper T Superscript normal upper T Baseline upper T periodA = T TT.
Hint: Show that aii > 0. Show that if A is partitioned into square sub-
matrices A11 and A22, 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix commaA =
[ A11 A12
A21 A22
]
,
A11 and A22 are positive deﬁnite. Use Algorithm 4.1 (page 246) to show  
the existence of a T, and ﬁnally show that T is unique. 
4.6. Let X1, X2, and  X3 be independent random variables identically dis-
tributed as standard normals. 
a) Determine a matrix A such that the random vector 
upper A Start 3 By 1 Matrix 1st Row upper X 1 2nd Row upper X 2 3rd Row upper X 3 EndMatrixA
⎡
⎣
X1
X2
X3
⎤
⎦

Exercises
255
has a multivariate normal distribution with variance-covariance ma-
trix 
Start 3 By 3 Matrix 1st Row 1st Column 4 2nd Column 2 3rd Column 8 2nd Row 1st Column 2 2nd Column 10 3rd Column 7 3rd Row 1st Column 8 2nd Column 7 3rd Column 21 EndMatrix period
⎡
⎣
4 2
8
2 10
7
8 7 21
⎤
⎦.
b) Is your solution unique? (The answer is no.) Determine a diﬀerent 
solution. 
4.7. Generalized inverses. 
a) Prove Eq. (4.36) on page 240 (Moore-Penrose inverse of a full column 
rank matrix). 
b) Prove Eq. (4.40) on page 241 (generalized inverse of a nonfull rank 
matrix). 
c) Prove Eq. (4.43) on page 241 (Moore-Penrose inverse of a nonfull rank 
matrix). 
4.8. Determine the Givens transformation matrix that will rotate the matrix 
upper A equals Start 4 By 3 Matrix 1st Row 1st Column 3 2nd Column 5 3rd Column 6 2nd Row 1st Column 6 2nd Column 1 3rd Column 2 3rd Row 1st Column 8 2nd Column 6 3rd Column 7 4th Row 1st Column 2 2nd Column 3 3rd Column 1 EndMatrixA =
⎡
⎢⎢⎣
3 5 6
6 1 2
8 6 7
2 3 1
⎤
⎥⎥⎦
so that the second column becomes (5, ˜a22, 6, 0) (see also Exercise 12.5). 
4.9. Gram-Schmidt transformations. 
a) Use Gram-Schmidt transformations to determine an orthonormal ba-
sis for the space spanned by the vectors 
StartLayout 1st Row 1st Column v 1 2nd Column equals 3rd Column left parenthesis 3 comma 6 comma 8 comma 2 right parenthesis comma 2nd Row 1st Column v 2 2nd Column equals 3rd Column left parenthesis 5 comma 1 comma 6 comma 3 right parenthesis comma 3rd Row 1st Column v 3 2nd Column equals 3rd Column left parenthesis 6 comma 2 comma 7 comma 1 right parenthesis period EndLayoutv1 = (3, 6, 8, 2),
v2 = (5, 1, 6, 3),
v3 = (6, 2, 7, 1).
b) Write out a formal algorithm for computing the QR factorization of 
the n × m full rank matrix A. Assume n ≥m. 
R Exercises 
These exercises are intended to illustrate R functions and commands, rather 
than to follow eﬃcient or proper computational methods. In most cases, the 
intent is just to compute a quantity using the deﬁnitions given in this chapter 
and the R functions from pages 8 through 16, pages 62 through 66, pages 201 
through 209, and pages 251 through 253. 
4.10. Homogeneous coordinates are often used in mapping three-dimensional 
graphics to two dimensions. The perspective plot function persp in R, 
for example, produces a 4 × 4 matrix for projecting three-dimensional 
points represented in homogeneous coordinates onto two-dimensional

256
4 Transformations and Factorizations
points in the displayed graphic. R uses homogeneous coordinates in the 
form of Eq. (4.6b) rather than Eq. (4.6a). If the matrix produced is T 
and if ah is the representation of a point (xa, ya, za) in homogeneous 
coordinates, in the form of Eq. (4.6b), then ah T yields transformed 
homogeneous coordinates that correspond to the projection onto the 
two-dimensional coordinate system of the graphical display. Consider 
the two graphs in Fig. 4.4. 
Figure 4.4. Illustration of the use of homogeneous coordinates to locate three-
dimensional points on a two-dimensional graph 
The graph on the left in the unit cube was produced by the simple R 
statements 
x<-c(0,1) 
y<-c(0,1) 
z<-matrix(c(0,0,1,1),nrow=2) 
persp(x, y, z, theta = 45, phi = 30) 
(The angles theta and phi are the azimuthal and latitudinal viewing 
angles, respectively, in degrees.) The graph on the right is the same 
with a heavy line going down the middle of the surface, that is, from the 
point (0.5, 0.0, 0.0) to (0.5, 1.0, 1.0). Obtain the transformation matrix 
necessary to identify the rotated points and produce the graph on the 
right. 
4.11. Write an R function 
Kron <- function(A,B){ 
that accepts two matrices A and B and computes their Kronecker prod-
uct. (The purpose of this exercise is to perform various useful com-
putations in R; it is not to write a useful function. As mentioned in 
the text, both the built-in operator %x% and the function kronecker 
perform Kronecker multiplication.)

Exercises
257
4.12. Gaussian elimination. 
a) Write an R function Gaus to create a Gaussian transformation ma-
trix for a given n × m matrix, as in Eq. 3.77 on page 106. If the  
(p, p) element of the given matrix is nonzero, premultiplication by 
the Gaussian matrix will make the (p, p) element 1 and all other el-
ements in the pth column 0. (We must know the elements in that 
column of the given matrix.) If the (p, p) element of the given ma-
trix is zero, Gaus returns NA. The Gaussian transformation matrix 
is m × m. 
Use the function deﬁnition 
Gaus <- function(n,p,ap){ 
where n is the number of rows in the given matrix, p is the index of 
the pivotal element to be transformed to 1, and ap is the n-vector 
containing the elements in the pth column. 
Use the Epqa function from Exercise 3.45. 
b) Let 
upper A equals Start 3 By 4 Matrix 1st Row 1st Column 1 2nd Column 2 3rd Column 2 4th Column 4 2nd Row 1st Column 3 2nd Column 3 3rd Column 1 4th Column 2 3rd Row 1st Column 2 2nd Column 1 3rd Column 2 4th Column 1 EndMatrix periodA =
⎡
⎣
1 2 2 4
3 3 1 2
2 1 2 1
⎤
⎦.
Use Gaus to zero out the second column except for a 1 in the (2,2) 
position. 
c) Use Gaus to determine a generalized inverse of A, and  show  that  it  
is a generalized inverse. Is your generalized inverse a Moore-Penrose 
inverse? 
Compute a generalized inverse by adjoining an identity matrix to the 
given matrix, and zero matrix to the Gaussian matrix. 
d) The exercise above is not meant to illustrate good computing prac-
tices, but rather just to illustrate simple manipulations in R. 
What are some of the issues that would arise in similar computations 
that would need to be addressed if the values in the matrix were not 
known ahead of time? 
4.13. Singular value decomposition: svd. 
a) For the matrix A in Exercise 4.12b, compute the SVD, A = UDV T . 
(Use the R function svd.) Next, show that your matrices do indeed 
constitute the SVD by explicitly multiplying them to recapture A. 
b) Use the SVD to compute a generalized inverse of A. Show that your 
result is indeed a generalized inverse. Check the other properties. Is 
your generalized inverse a Moore-Penrose inverse? 
4.14. Square roots of nonnegative deﬁnite matrices. 
a) Write a function in R to ﬁnd the square root of a symmetric nonneg-
ative deﬁnite matrix. 
b) Let

258
4 Transformations and Factorizations
upper A equals Start 3 By 4 Matrix 1st Row 1st Column 1 2nd Column 2 3rd Column 2 4th Column 4 2nd Row 1st Column 3 2nd Column 3 3rd Column 1 4th Column 2 3rd Row 1st Column 2 2nd Column 1 3rd Column 2 4th Column 1 EndMatrix commaA =
⎡
⎣
1 2 2 4
3 3 1 2
2 1 2 1
⎤
⎦,
and let B = AT A. Is  B nonnegative deﬁnite? If so, use your function 
from Exercise 4.14a to compute its square root, B 
1 
2 . 
c) Let 
upper C equals Start 3 By 3 Matrix 1st Row 1st Column 4 2nd Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column 3 3rd Column 1 3rd Row 1st Column 2 2nd Column 1 3rd Column 4 EndMatrix commaC =
⎡
⎣
4 2 2
2 3 1
2 1 4
⎤
⎦,
and let D = CC. Is  D nonnegative deﬁnite? If so, compute its square 
root, D 
1 
2 . Is  C the square root of D? 
d) Let 
upper E equals Start 3 By 3 Matrix 1st Row 1st Column 4 2nd Column 2 3rd Column 2 2nd Row 1st Column 2 2nd Column negative 3 3rd Column 1 3rd Row 1st Column 2 2nd Column 1 3rd Column 4 EndMatrix commaE =
⎡
⎣
4
2 2
2 −3 1
2
1 4
⎤
⎦,
and let F = EE. Is  F nonnegative deﬁnite? If so, compute its square 
root, F 
1 
2 . Is  E the square root of F? 
4.15. QR decomposition. 
a) Write an R function to implement the algorithm for computing the 
QR factorization of an n × m full rank matrix A, with  n ≥ m, that  
you described in Exercise 4.9b. 
b) Generate a random 5×5 matrix  A using matrix(runif(25),nrow=5). 
Determine the QR factorization of A using your function from Exer-
cise 4.15a. 
Next determine the QR factorization using the R function qr. Use  
qr.Q and qr.R to identify the matrices. Are they the same as you 
got from your function? 
c) Generate a random 5 × 5 matrix  B using 
x1 <- rnorm(5) 
x2 <- 2*rnorm(5) 
x3 <- 3*rnorm(5) 
x4 <- x1+x2 
x5 <- 3*x1+2*x2+x1 
B <- rbind(x1,x2,x3,x4,x5) 
Use the R function qr to determine the rank of B.

5 
Solution of Linear Systems 
One of the most common problems in numerical computing is to solve the 
linear system 
upper A x equals b commaAx = b,
that is, for given A and b, to ﬁnd x such that the equation holds. The system 
is said to be consistent if there exists such an x, and in that case, a solution 
x may be written as A−b, where  A− is some inverse of A. If  A is square and 
of full rank, we can write the solution as A−1 b. 
It is important to distinguish the expression A−1 b or A−b, which represents 
the solution, from the method of computing the solution. We would never 
compute A−1 just so we could multiply it by b to form the solution A−1 b. 
Two topics we discuss in this chapter have wider relevance in computations 
for scientiﬁc applications. The ﬁrst is how to assess the expected numerical 
diﬃculties in solving a speciﬁc problem. A speciﬁc problem comprises a task 
and data or input to the task. Some tasks are naturally harder than others; 
for example, it is generally more diﬃcult to solve a constrained optimization 
problem than it is to solve a system of linear equations. Some constrained 
optimization problems, however, may be very “small” and even have a simple 
closed-form solution, while a system of linear equations may be very large 
and be subject to severe rounding errors (for reasons we will discuss). For 
any speciﬁc task, there may be ways of assessing the “diﬃculty” of a speciﬁc 
input. One measure of diﬃculty is a “condition number,” and we discuss a 
condition number for the task of solving a linear system in Sect. 5.1. 
Another topic that is illustrated by the techniques discussed in this chapter 
is the diﬀerence between direct methods and iterative methods. The methods 
for factoring a matrix discussed in Chap. 4 are direct methods; that is, there 
is a ﬁxed sequence of operations (possibly with some pivoting along the way) 
that yields the solution. Iterative methods do not have a ﬁxed, predetermined 
number of steps; rather, they must have a stopping rule that depends on the 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 5 
259

260
5 Solution of Linear Systems
steps themselves. Most optimization methods discussed in Sect. 9.6 are exam-
ples of iterative methods. In Chap. 6, we will see that all methods for obtaining 
eigenvalues (for general square matrices larger than 4 × 4) are necessarily it-
erative. 
An iterative method is characterized by a sequence of partial or ap-
proximate solutions, which I index by a superscript enclosed in parentheses: 
s(0) , s(1) , s(2) , . . .. A starting point is necessary. I usually index it as “(0) .” The 
iterative algorithm is essentially a rule for computing s(k) given s(k−1) . 
A stopping criterion is also necessary. The partial solution at any point in 
the sequence often consists of multiple parts; for example, in an optimization 
problem, there are two parts, the value of the decision variable and the value 
of the objective function. The stopping rule may be based on the magnitude 
of the change in one or more parts of the solution. Of course, a stopping 
criterion based simply on the number of iterations is also possible and, in 
fact, is usually a good idea in addition to any other stopping criterion. 
We discuss direct methods for solving systems of linear equations in 
Sect. 5.2. A direct method uses a ﬁxed number of computations that would 
in exact arithmetic lead to the solution. We may also refer to the steps in 
a direct method as “iterations”; but there is a ﬁxed limit on the number of 
them that depends only on the size of the input. 
We discuss iterative methods for solving systems of linear equations in 
Sect. 5.3. Iterative methods often work well for very large and/or sparse ma-
trices. 
Another general principle in numerical computations is that once an ap-
proximate solution has been obtained (in numerical computing, almost all 
solutions are approximate), it is often useful to see if the solution can be 
improved. This principle is well-illustrated by “iterative reﬁnement” of a so-
lution to a linear system, which we discuss in Sect. 5.4. Iterative reﬁnement 
is an iterative method, but the method yielding the starting approximation 
may be a direct method. 
5.1 Condition of Matrices 
Data are said to be “ill-conditioned” for a particular problem or computation 
if the data are likely to cause diﬃculties in the computations, such as severe 
loss of precision. More generally, the term “ill-conditioned” is applied to a 
problem in which small changes to the input result in large changes in the 
output. In the case of a linear system 
upper A x equals b commaAx = b,
the problem of solving the system is ill-conditioned if small changes to some 
elements of A or b will cause large changes in the solution x. The concept 
of ill-conditionedness is a heuristic generalization of Hadamard’s property of 
ill-posedness (see page 143).

5.1 Condition of Matrices
261
Consider, for example, the system of equations 
StartLayout 1st Row 1st Column 1.000 x 1 plus 0.500 x 2 2nd Column equals 3rd Column 1.500 comma 2nd Row 1st Column 0.667 x 1 plus 0.333 x 2 2nd Column equals 3rd Column 1.000 period EndLayout1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.333x2 = 1.000.
(5.1) 
The solution is easily seen to be x1 = 1.000 and x2 = 1.000. 
Now consider a small change in the right-hand side: 
StartLayout 1st Row 1st Column 1.000 x 1 plus 0.500 x 2 2nd Column equals 3rd Column 1.500 comma 2nd Row 1st Column 0.667 x 1 plus 0.333 x 2 2nd Column equals 3rd Column 0.999 period EndLayout1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.333x2 = 0.999.
(5.2) 
This system has solution x1 = 0.000 and x2 = 3.000. That is a relatively large 
change. 
Alternatively, consider a small change in one of the elements of the coeﬃ-
cient matrix: 
StartLayout 1st Row 1st Column 1.000 x 1 plus 0.500 x 2 2nd Column equals 3rd Column 1.500 comma 2nd Row 1st Column 0.667 x 1 plus 0.334 x 2 2nd Column equals 3rd Column 1.000 period EndLayout1.000x1 + 0.500x2 = 1.500,
0.667x1 + 0.334x2 = 1.000.
(5.3) 
The solution now is x1 = 2.000 and x2 = −1.000; again, that is a relatively 
large change. 
In both cases, small changes of the order of 10−3 in the input (the el-
ements of the coeﬃcient matrix or the right-hand side) result in relatively 
large changes (of the order of 1) in the output (the solution). Solving the 
system (either one of them) is an ill-conditioned problem when our relevant 
scales are of order 3. 
The nature of the data that cause ill-conditioning depends on the type 
of problem. The case above can be considered as a geometric problem of 
determining the point of intersection of two lines. The problem is that the 
lines represented by the equations are almost parallel, as seen in Fig. 5.1, 
and so their point of intersection is very sensitive to slight changes in the 
coeﬃcients deﬁning the lines. 
The problem can also be described in terms of the angle between the lines. 
When the angle is small, but not necessarily 0, we refer to the condition as 
“collinearity.” (This term is somewhat misleading because, strictly speaking, 
it should indicate that the angle is exactly 0.) In this example, the cosine 
of the angle between the lines, from Eq. (2.53), is 1 − 2 × 10−7 . In general, 
collinearity (or “multicollinearity”) exists whenever the angle between any line 
(i.e., vector) and the subspace spanned by any other set of vectors is small. 
5.1.1 Condition Number 
For a speciﬁc problem such as solving a system of equations, we may quantify 
the condition of the matrix by a condition number. To develop this quantiﬁ-
cation for the problem of solving linear equations, consider a linear system 
Ax = b, with  A nonsingular and b /= 0, as above. Now perturb the system 
slightly by adding a small amount, δb, to  b, and  let  ˜b = b + δb. The system

262
5 Solution of Linear Systems
Figure 5.1. Almost parallel lines: ill-conditioned coeﬃcient matrices, Eqs. (5.1) 
and (5.2) 
upper A x overTilde equals b overTildeA˜x = ˜b
has a solution ˜x = δx + x = A−1˜b. (Notice that δb and δx do not necessarily 
represent scalar multiples of the respective vectors.) If the system is well-
conditioned, for any reasonable norm, if ||δb||/||b|| is small, then ||δx||/||x|| is 
likewise small. 
From δx = A−1 δb and the inequality (3.310) (page 189), for an induced 
norm on A, we have  
parallel to delta x parallel to less than or equals parallel to upper A Superscript negative 1 Baseline parallel to parallel to delta b parallel to period||δx|| ≤||A−1|| ||δb||.
(5.4) 
Likewise, because b = Ax, we have  
StartFraction 1 Over parallel to x parallel to EndFraction less than or equals parallel to upper A parallel to StartFraction 1 Over parallel to b parallel to EndFraction comma 1
||x|| ≤||A|| 1
||b||,
(5.5) 
and Eqs. (5.4) and  (5.5) together imply 
StartFraction parallel to delta x parallel to Over parallel to x parallel to EndFraction less than or equals parallel to upper A parallel to parallel to upper A Superscript negative 1 Baseline parallel to StartFraction parallel to delta b parallel to Over parallel to b parallel to EndFraction period||δx||
||x|| ≤||A|| ||A−1||||δb||
||b|| .
(5.6)

5.1 Condition of Matrices
263
This provides a bound on the change in the solution ||δx||/||x|| in terms of the 
perturbation ||δb||/||b||. 
The bound in Eq. (5.6) motivates us to deﬁne the condition number with 
respect to inversion, denoted by κ(·), as 
kappa left parenthesis upper A right parenthesis equals parallel to upper A parallel to parallel to upper A Superscript negative 1 Baseline parallel toκ(A) = ||A|| ||A−1||
(5.7) 
for nonsingular A. In the context of linear algebra, the condition number with 
respect to inversion is so dominant in importance that we generally just refer 
to it as the “condition number.” This condition number also provides a bound 
on changes in eigenvalues due to perturbations of the matrix, as we will see in 
inequality (6.1). (That particular bound is of more theoretical interest than 
of practical value.) 
A condition number is a useful measure of the condition of A for the 
problem of solving a linear system of equations. There are other condition 
numbers useful in numerical analysis, however, such as the condition number 
for computing the sample variance (see Eq. (10.4.1) on page 569) or a condition 
number for a root of a function. 
We can write Eq. (5.6) as  
StartFraction parallel to delta x parallel to Over parallel to x parallel to EndFraction less than or equals kappa left parenthesis upper A right parenthesis StartFraction parallel to delta b parallel to Over parallel to b parallel to EndFraction||δx||
||x|| ≤κ(A)||δb||
||b||
(5.8) 
and, following a development similar to that above, write 
StartFraction parallel to delta b parallel to Over parallel to b parallel to EndFraction less than or equals kappa left parenthesis upper A right parenthesis StartFraction parallel to delta x parallel to Over parallel to x parallel to EndFraction period||δb||
||b|| ≤κ(A)||δx||
||x|| .
(5.9) 
These inequalities, as well as the other ones we write in this section, are sharp, 
as we can see by letting A = I. 
Because the condition number is an upper bound on a quantity that we 
would not want to be large, a large condition number is “bad.” 
Notice that our deﬁnition of the condition number does not specify the 
norm; it only requires that the norm be an induced norm. (An equivalent 
deﬁnition does not rely on the norm being an induced norm.) We sometimes 
specify a condition number with regard to a particular norm, and just as we 
sometimes denote a speciﬁc norm by a special symbol, we may use a special 
symbol to denote a speciﬁc condition number. For example, κp(A) may denote 
the condition number of A in terms of an Lp norm. Most of the properties of 
condition numbers (but not their actual values) are independent of the norm 
used. 
The coeﬃcient matrix in Eqs. (5.1) and  (5.2) is  
upper A equals Start 2 By 2 Matrix 1st Row 1st Column 1.000 2nd Column 0.500 2nd Row 1st Column 0.667 2nd Column 0.333 EndMatrix commaA =
[ 1.000 0.500
0.667 0.333
]
,
and its inverse is

264
5 Solution of Linear Systems
upper A Superscript negative 1 Baseline equals Start 2 By 2 Matrix 1st Row 1st Column negative 666 2nd Column 1000 2nd Row 1st Column 1344 2nd Column negative 2000 EndMatrix periodA−1 =
[
−666 1000
1344 −2000
]
.
It is easy to see that 
parallel to upper A parallel to Subscript 1 Baseline equals 1.667||A||1 = 1.667
and 
parallel to upper A Superscript negative 1 Baseline parallel to Subscript 1 Baseline equals 3000 semicolon||A−1||1 = 3000;
hence, 
kappa 1 left parenthesis upper A right parenthesis equals 5001 periodκ1(A) = 5001.
Likewise, 
parallel to upper A parallel to Subscript normal infinity Baseline equals 1.500||A||∞= 1.500
and 
parallel to upper A Superscript negative 1 Baseline parallel to Subscript normal infinity Baseline equals 3344 semicolon||A−1||∞= 3344;
hence, 
kappa Subscript normal infinity Baseline left parenthesis upper A right parenthesis equals 5016 periodκ∞(A) = 5016.
Notice that the condition numbers are not exactly the same, but they are 
close. Notice also that the condition numbers are of the order of magnitude 
of the ratio of the output perturbation to the input perturbation in those 
equations. 
Although we used this matrix in an example of ill-conditioning, these con-
dition numbers, although large, are not so large as to cause undue concern 
for numerical computations. Indeed, solving the systems of Eqs. (5.1), (5.2), 
and (5.3) would not cause problems for a computer program. 
An interesting relationship for the L2 condition number is 
kappa 2 left parenthesis upper A right parenthesis equals StartStartFraction max Underscript x not equals 0 Endscripts StartFraction parallel to upper A x parallel to Over parallel to x parallel to EndFraction OverOver min Underscript x not equals 0 Endscripts StartFraction parallel to upper A x parallel to Over parallel to x parallel to EndFraction EndEndFractionκ2(A) =
maxx/=0
||Ax||
||x||
minx/=0
||Ax||
||x||
(5.10) 
(see Exercise 5.1, page 300). The numerator and denominator in Eq. (5.10) 
look somewhat like the maximum and minimum eigenvalues, as we have sug-
gested. Indeed, the L2 condition number is just the ratio of the largest eigen-
value in absolute value to the smallest (see page 190). The L2 condition num-
ber is also called the spectral condition number. 
The eigenvalues of the coeﬃcient matrix in Eqs. (5.1) and  (5.2) are  
1.333375 and −0.0003750, and so 
kappa 2 left parenthesis upper A right parenthesis equals 3555.67 commaκ2(A) = 3555.67,
which is the same order of magnitude as κ∞(A) and  κ1(A) computed above. 
Some useful facts about condition numbers are: 
• κ(A) =  κ(A−1 ). 
• κ(cA) =  κ(A), 
for c /= 0.

5.1 Condition of Matrices
265
• κ(A) ≥1. 
• κ1(A) =  κ∞(AT ). 
• κ2(AT ) =  κ2(A). 
• κ2(AT A) =  κ2 
2(A) 
≥κ2(A). 
• If A and B are orthogonally similar (Eq. (3.265)), then 
parallel to upper A parallel to Subscript 2 Baseline equals parallel to upper B parallel to Subscript 2||A||2 = ||B||2
and 
kappa 2 left parenthesis upper A right parenthesis equals kappa 2 left parenthesis upper B right parenthesisκ2(A) = κ2(B)
(see Eq. (3.316)). 
Even though the condition number provides a very useful indication of the 
condition of the problem of solving a linear system of equations, it can be 
misleading at times. Consider, for example, the coeﬃcient matrix 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st Column 0 2nd Column epsilon EndMatrix commaA =
[ 1 0
0 e
]
,
where 0 < e <  1. The condition numbers are 
kappa 1 left parenthesis upper A right parenthesis equals kappa 2 left parenthesis upper A right parenthesis equals kappa Subscript normal infinity Baseline left parenthesis upper A right parenthesis equals StartFraction 1 Over epsilon EndFraction commaκ1(A) = κ2(A) = κ∞(A) = 1
e ,
and so if e is small, the condition number is large. It is easy to see, however, 
that small changes to the elements of A or b in the system Ax = b do not cause 
undue changes in the solution (our heuristic deﬁnition of ill-conditioning). In 
fact, the simple expedient of multiplying the second row of A by 1/e (i.e., 
multiplying the second equation, a21x1 + a22x2 = b2, by 1/e) yields a linear 
system that is very well-conditioned. 
This kind of apparent ill-conditioning is called artiﬁcial ill-conditioning. 
It is due to the diﬀerent rows (or columns) of the matrix having a very dif-
ferent scale; the condition number can be changed just by scaling the rows or 
columns. This usually does not make a linear system any better or any worse 
conditioned. 
In Sect. 5.1.3, we relate the condition number to bounds on the numerical 
accuracy of the solution of a linear system of equations. 
The relationship between the size of the matrix and its condition number 
is interesting. In general, we would expect the condition number to increase 
as the size increases. This is the case, but the nature of the increase depends 
on the type of elements in the matrix. If the elements are randomly and 
independently distributed as normal or uniform with a mean of zero and 
variance of one, the increase in the condition number is approximately linear 
in the size of the matrix (see Exercise 10.22, page 588). 
Our deﬁnition of condition number given above is for nonsingular matri-
ces. We can formulate a useful alternate deﬁnition that extends to singular

266
5 Solution of Linear Systems
matrices and to nonsquare matrices: the condition number of a matrix is the 
ratio of the largest singular value to the smallest nonzero singular value, and 
of course, this is the same as the deﬁnition for square nonsingular matrices. 
This is also called the spectral condition number. 
The condition number, like the determinant, is not easy to compute (see 
page 603 in Sect. 11.4). 
5.1.2 Improving the Condition Number 
Sometimes, the condition number reﬂects an essential characteristic of the 
problem being addressed. In other cases, it is only an artifact of the model 
formulation for the problem, as in the case of artiﬁcial ill-conditioning, in 
which a simple change of units removes the ill-conditioning. 
In other cases, we change the problem slightly. For example, we can im-
prove the condition number of a matrix by adding a diagonal matrix. Although 
this fact holds in general, we consider only one special case. Let A be positive 
deﬁnite, and let d >  0. Then, for the spectral condition number, we have 
kappa 2 left parenthesis upper A plus d upper I right parenthesis less than kappa 2 left parenthesis upper A right parenthesis periodκ2(A + dI) < κ2(A).
(5.11) 
To see this, let ci be an eigenvalue of A (which is positive). We have 
StartFraction max left parenthesis c Subscript i Baseline plus d right parenthesis Over min left parenthesis c Subscript i Baseline plus d right parenthesis EndFraction less than StartFraction max left parenthesis c Subscript i Baseline right parenthesis Over min left parenthesis c Subscript i Baseline right parenthesis EndFractionmax(ci + d)
min(ci + d) < max(ci)
min(ci)
for d >  0. 
Ridge Regression and the Condition Number 
Ridge regression, which we will discuss brieﬂy beginning on page 401, and  
again beginning on page 477, is a form  of  regularization of an inverse prob-
lem for which we have developed a set of approximate linear equations in the 
variable β, y ≈Xβ. One possible solution to the underlying problem is char-
acterized by the equations XT Xβ = XT y. This solution satisﬁes a criterion 
that in many cases is not an essential aspect of the inverse problem itself. (I 
am using “inverse problem” here in its usual sense; here, “inverse” does not 
refer to an inverse of a matrix.) 
The matrix XT X may be ill-conditioned or even singular, but its eigenval-
ues are nonnegative. The matrix XT X +λI for λ >  0, however, is nonsingular 
and has a smaller condition number, 
kappa 2 left parenthesis upper X Superscript normal upper T Baseline upper X plus lamda upper I right parenthesis less than kappa 2 left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis commaκ2(XTX + λI) < κ2(XTX),
(5.12) 
as we have seen above. 
In applications, the ill-conditioning of XT X may imply other issues relat-
ing to the statistical questions being addressed in the inverse problem.

5.1 Condition of Matrices
267
5.1.3 Numerical Accuracy 
The condition numbers we have deﬁned are useful indicators of the accuracy 
we may expect when solving a linear system Ax = b. Suppose the entries of 
the matrix A and the vector b are accurate to approximately p decimal digits, 
so we have the system 
left parenthesis upper A plus delta upper A right parenthesis left parenthesis x plus delta x right parenthesis equals b plus delta b(A + δA) (x + δx) = b + δb
with 
StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction almost equals 10 Superscript negative p||δA||
||A|| ≈10−p
and 
StartFraction parallel to delta b parallel to Over parallel to b parallel to EndFraction almost equals 10 Superscript negative p Baseline period||δb||
||b|| ≈10−p.
Assume A is nonsingular, and suppose that the condition number with respect 
to inversion, κ(A), is approximately 10t , so  
kappa left parenthesis upper A right parenthesis StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction almost equals 10 Superscript t minus p Baseline periodκ(A)||δA||
||A|| ≈10t−p.
Ignoring the approximation of b (i.e., assuming δb = 0), we can write 
delta x equals minus upper A Superscript negative 1 Baseline delta upper A left parenthesis x plus delta x right parenthesis commaδx = −A−1δA(x + δx),
which, together with the triangular inequality and inequality (3.310) on  
page 189, yields the bound 
parallel to delta x parallel to less than or equals parallel to upper A Superscript negative 1 Baseline parallel to parallel to delta upper A parallel to left parenthesis parallel to x parallel to plus parallel to delta x parallel to right parenthesis period||δx|| ≤||A−1|| ||δA||
(
||x|| + ||δx||
)
.
Using Eq. (5.7) with this, we have 
parallel to delta x parallel to less than or equals kappa left parenthesis upper A right parenthesis StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction left parenthesis parallel to x parallel to plus parallel to delta x parallel to right parenthesis||δx|| ≤κ(A)||δA||
||A||
(
||x|| + ||δx||
)
or 
left parenthesis 1 minus kappa left parenthesis upper A right parenthesis StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction right parenthesis parallel to delta x parallel to less than or equals kappa left parenthesis upper A right parenthesis StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction parallel to x parallel to period
(
1 −κ(A)||δA||
||A||
)
||δx|| ≤κ(A)||δA||
||A|| ||x||.
If the condition number is not too large relative to the precision (i.e., if 
10t−p << 1), then we have 
StartLayout 1st Row 1st Column StartFraction parallel to delta x parallel to Over parallel to x parallel to EndFraction 2nd Column almost equals 3rd Column kappa left parenthesis upper A right parenthesis StartFraction parallel to delta upper A parallel to Over parallel to upper A parallel to EndFraction 2nd Row 1st Column Blank 2nd Column almost equals 3rd Column 10 Superscript t minus p Baseline period EndLayout||δx||
||x|| ≈κ(A)||δA||
||A||
≈10t−p.
(5.13) 
Expression (5.13) provides a rough bound on the accuracy of the solution 
in terms of the precision of the data and the condition number of the coeﬃcient

268
5 Solution of Linear Systems
matrix. This result must be used with some care, however, and there are cases 
where the bound is violated or where it is not very tight. 
Another consideration in the practical use of expression (5.13) is the  fact  
that the condition number is usually not known and methods for computing 
it suﬀer from the same rounding problems as the solution of the linear system 
itself. In Sect. 11.4, we describe ways of estimating the condition number, but 
as the discussion there indicates, these estimates are often not very reliable. 
We would expect the norms in the expression (5.13) to be larger for larger-
size problems. The approach taken above addresses a type of “total” error. 
It may be appropriate to scale the norms to take into account the number of 
elements. Another approach to determining the accuracy of a solution is to 
use random perturbations of A and/or b and then to estimate the eﬀects of 
the perturbations on x. 
Higher accuracy in numerical computations can be achieved in various 
ways: multiple precision, interval arithmetic, and residue arithmetic (see 
pages 559 and following). Another way of improving the accuracy in com-
putations for solving linear systems is by using iterative reﬁnement, which we 
discuss in Sect. 5.4. 
5.2 Direct Methods for Consistent Systems 
There are two general approaches to solving the linear system Ax = b, direct 
and iterative. In this section, we discuss direct methods, which usually proceed 
by a factorization of the coeﬃcient matrix. 
5.2.1 Gaussian Elimination and Matrix Factorizations 
The most common direct method for the solution of linear systems is Gaus-
sian elimination. The basic idea in this method is to form equivalent sets of 
equations, beginning with the system to be solved, Ax = b, or  
StartLayout 1st Row 1st Column a Subscript 1 asterisk Superscript normal upper T Baseline x 2nd Column equals 3rd Column b 1 2nd Row 1st Column a Subscript 2 asterisk Superscript normal upper T Baseline x 2nd Column equals 3rd Column b 2 3rd Row 1st Column ellipsis 2nd Column equals 3rd Column ellipsis 4th Row 1st Column a Subscript n asterisk Superscript normal upper T Baseline x 2nd Column equals 3rd Column b Subscript n Baseline comma EndLayoutaT
1∗x = b1
aT
2∗x = b2
. . . = . . .
aT
n∗x = bn,
where aj∗ is the jth row of A. An equivalent set of equations can be formed 
by a sequence of elementary operations on the equations in the given set. 
These elementary operations on equations are essentially the same as the 
elementary operations on the rows of matrices discussed in Sect. 3.3.3 and in 
Sect. 4.5. There are three kinds of elementary operations:

5.2 Direct Methods for Consistent Systems
269
• An interchange of two equations, 
StartLayout 1st Row 1st Column a Subscript j asterisk Superscript normal upper T Baseline x equals b Subscript j Baseline 2nd Column left arrow 3rd Column a Subscript k asterisk Superscript normal upper T Baseline x equals b Subscript k Baseline comma 2nd Row 1st Column a Subscript k asterisk Superscript normal upper T Baseline x equals b Subscript k Baseline 2nd Column left arrow 3rd Column a Subscript j asterisk Superscript normal upper T Baseline x equals b Subscript j Baseline comma EndLayoutaT
j∗x = bj ←aT
k∗x = bk,
aT
k∗x = bk ←aT
j∗x = bj,
which aﬀects two equations simultaneously. 
• A scalar multiplication of a given equation, 
a Subscript j asterisk Superscript normal upper T Baseline x equals b Subscript j Baseline left arrow c a Subscript j asterisk Superscript normal upper T Baseline x equals c b Subscript j Baseline periodaT
j∗x = bj
←
caT
j∗x = cbj.
• A replacement of a single equation by an axpy operation (a sum of it and 
a scalar multiple of another equation), 
a Subscript j asterisk Superscript normal upper T Baseline x equals b Subscript j Baseline left arrow a Subscript j asterisk Superscript normal upper T Baseline x plus c a Subscript k asterisk Superscript normal upper T Baseline x equals b Subscript j Baseline plus c b Subscript k Baseline periodaT
j∗x = bj
←
aT
j∗x + caT
k∗x = bj + cbk.
The interchange operation can be accomplished by premultiplication by 
an elementary permutation matrix (see page 103): 
upper E Subscript j k Baseline upper A x equals upper E Subscript j k Baseline b periodEjkAx = Ejkb.
The scalar multiplication can be performed by premultiplication by an ele-
mentary transformation matrix Ej(c), and the axpy operation can be eﬀected 
by premultiplication by an Ejk(c) elementary transformation matrix. 
The elementary operation on the equation 
a Subscript 2 asterisk Superscript normal upper T Baseline x equals b 2aT
2∗x = b2
in which the ﬁrst equation is combined with it using c1 = −a21/a11 and 
c2 = 1 will yield an equation with a zero coeﬃcient for x1. Generalizing this, 
we perform elementary operations on the second through the nth equations 
to yield a set of equivalent equations in which all but the ﬁrst have zero 
coeﬃcients for x1. 
Next, we perform elementary operations using the second equation with 
the third through the nth equations, so that the new third through the nth 
equations have zero coeﬃcients for x2. This is the kind of sequence of multi-
plications by elementary operator matrices shown in Eq. (3.76) on page 106 
and grouped together as Lk in Eq. (4.25) on page 234. 
The sequence of equivalent equations, beginning with Ax = b, is  
StartLayout 1st Row 1st Column Blank 2nd Column a 11 x 1 3rd Column plus 4th Column a 12 x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 1 n Baseline x Subscript n Baseline 9th Column equals 10th Column b 1 2nd Row 1st Column Blank 2nd Column a 21 x 1 3rd Column plus 4th Column a 22 x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 2 n Baseline x Subscript n Baseline 9th Column equals 10th Column b 2 3rd Row 1st Column left parenthesis 0 right parenthesis 2nd Column vertical ellipsis 3rd Column plus 4th Column vertical ellipsis 5th Column Blank 6th Column Blank 7th Column Blank 8th Column vertical ellipsis 9th Column Blank 10th Column vertical ellipsis 4th Row 1st Column Blank 2nd Column a Subscript n Baseline 1 Baseline x 1 3rd Column plus 4th Column a Subscript n Baseline 2 Baseline x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript n n Baseline x Subscript n Baseline 9th Column equals 10th Column b Subscript n Baseline 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank EndLayout overbar comma
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a21x1 + a22x2 +
· · ·
+
a2nxn
=
b2
(0)
...
+
...
...
...
an1x1 + an2x2 +
· · ·
+
annxn
=
bn
,
(5.14) 
then A(1) x = b(1) , or  L1Ax = L1b,

270
5 Solution of Linear Systems
StartLayout 1st Row 1st Column Blank 2nd Column a 11 x 1 3rd Column plus 4th Column a 12 x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 1 n Baseline x Subscript n Baseline 9th Column equals 10th Column b 1 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column a 22 Superscript left parenthesis 1 right parenthesis Baseline x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 2 n Superscript left parenthesis 1 right parenthesis Baseline x Subscript n Baseline 9th Column equals 10th Column b 2 Superscript left parenthesis 1 right parenthesis Baseline 3rd Row 1st Column left parenthesis 1 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column vertical ellipsis 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column vertical ellipsis 9th Column Blank 10th Column vertical ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column a Subscript n Baseline 2 Superscript left parenthesis 1 right parenthesis Baseline x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript n n Superscript left parenthesis 1 right parenthesis Baseline x Subscript n Baseline 9th Column equals 10th Column b Subscript n Superscript left parenthesis 1 right parenthesis Baseline 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank EndLayout overbar comma
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a(1)
22 x2 +
· · ·
+
a(1)
2n xn
=
b(1)
2
(1)
...
+
· · ·
+
...
...
a(1)
n2 x2 +
· · ·
+
a(1)
nnxn
=
b(1)
n
,
(5.15) 
vertical ellipsis vertical ellipsis...
...
and ﬁnally A(n) x = b(n) , or  Ln−1 · · · L1Ax = Ln−1 · · · L1b, or  Ux = Ln−1 
· · · L1b, 
StartLayout 1st Row 1st Column Blank 2nd Column a 11 x 1 3rd Column plus 4th Column a 12 x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 1 n Baseline x Subscript n Baseline 9th Column equals 10th Column b 1 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank 3rd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column a 22 Superscript left parenthesis 1 right parenthesis Baseline x 2 5th Column plus 6th Column midline horizontal ellipsis 7th Column plus 8th Column a Subscript 2 n Superscript left parenthesis 1 right parenthesis Baseline x Subscript n Baseline 9th Column equals 10th Column b 2 Superscript left parenthesis 1 right parenthesis Baseline 4th Row 1st Column left parenthesis n minus 1 right parenthesis 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column vertical ellipsis 8th Column vertical ellipsis 9th Column Blank 10th Column vertical ellipsis 5th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column a Subscript n minus 1 comma n minus 1 Superscript left parenthesis n minus 2 right parenthesis Baseline x Subscript n minus 1 Baseline 7th Column plus 8th Column a Subscript n minus 1 comma n Superscript left parenthesis n minus 2 right parenthesis Baseline x Subscript n Baseline 9th Column equals 10th Column b Subscript n minus 1 Superscript left parenthesis n minus 2 right parenthesis Baseline 6th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column Blank 9th Column Blank 10th Column Blank 7th Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column Blank 7th Column Blank 8th Column a Subscript n n Superscript left parenthesis n minus 1 right parenthesis Baseline x Subscript n Baseline 9th Column equals 10th Column b Subscript n Superscript left parenthesis n minus 1 right parenthesis Baseline 8th Row 1st Column Blank EndLayout overbar period
a11x1 + a12x2 +
· · ·
+
a1nxn
=
b1
a(1)
22 x2 +
· · ·
+
a(1)
2n xn
=
b(1)
2
(n −1)
...
...
...
a(n−2)
n−1,n−1xn−1 + a(n−2)
n−1,nxn = b(n−2)
n−1
a(n−1)
nn
xn = b(n−1)
n
.
(5.16) 
Recalling Eq. (4.28), we see that the last system is Ux = L−1 b. This system 
is easy to solve because the coeﬃcient matrix is upper triangular. The last 
equation in the system yields 
x Subscript n Baseline equals StartFraction b Subscript n Superscript left parenthesis n minus 1 right parenthesis Baseline Over a Subscript n n Superscript left parenthesis n minus 1 right parenthesis Baseline EndFraction periodxn = b(n−1)
n
a(n−1)
nn
.
By back substitution, we get 
x Subscript n minus 1 Baseline equals StartFraction left parenthesis b Subscript n minus 1 Superscript left parenthesis n minus 2 right parenthesis Baseline minus a Subscript n minus 1 comma n Superscript left parenthesis n minus 2 right parenthesis Baseline x Subscript n Baseline right parenthesis Over a Subscript n minus 1 comma n minus 1 Superscript left parenthesis n minus 2 right parenthesis Baseline EndFraction commaxn−1 = (b(n−2)
n−1
−a(n−2)
n−1,nxn)
a(n−2)
n−1,n−1
,
and we obtain the rest of the xs in a similar manner. This back substitution 
is equivalent to forming 
x equals upper U Superscript negative 1 Baseline upper L Superscript negative 1 Baseline bx = U −1L−1b
(5.17) 
or x = A−1 b with A = LU. 
Gaussian elimination consists of two steps: the forward reduction, which 
is of order O(n3 ), and the back substitution, which is of order O(n2 ). 
Pivoting 
The only obvious problem with this method arises if some of the a (k−1) 
kk 
s used  
as divisors are zero (or very small in magnitude). These divisors are called 
“pivot elements.”

5.2 Direct Methods for Consistent Systems
271
Suppose, for example, we have the equations 
StartLayout 1st Row 1st Column 0.0001 x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 1 comma 2nd Row 1st Column x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 2 period EndLayout0.0001x1 + x2 = 1,
x1 + x2 = 2.
The solution is x1 = 1.0001 and x2 = 0.9999. Suppose we are working with 
three digits of precision (so our solution is x1 = 1.00 and x2 = 1.00). After 
the ﬁrst step in Gaussian elimination, we have 
StartLayout 1st Row 1st Column 0.0001 x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 1 comma 2nd Row 1st Column Blank 2nd Column Blank 3rd Column negative 10 comma 000 x 2 4th Column equals 5th Column negative 10 comma 000 comma EndLayout0.0001x1 +
x2 =
1,
−10, 000x2 = −10, 000,
and so the solution by back substitution is x2 = 1.00 and x1 = 0.000. The 
L2 condition number of the coeﬃcient matrix is 2.618, so even though the 
coeﬃcients vary greatly in magnitude, we certainly would not expect any 
diﬃculty in solving these equations. 
A simple solution to this potential problem is to interchange the equation 
having the small leading coeﬃcient with an equation below it. Thus, in our 
example, we ﬁrst form 
StartLayout 1st Row 1st Column x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 2 comma 2nd Row 1st Column 0.0001 x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 1 comma EndLayout
x1 + x2 = 2,
0.0001x1 + x2 = 1,
so that after the ﬁrst step, we have 
StartLayout 1st Row 1st Column x 1 2nd Column plus 3rd Column x 2 4th Column equals 5th Column 2 comma 2nd Row 1st Column Blank 2nd Column Blank 3rd Column x 2 4th Column equals 5th Column 1 comma EndLayoutx1 + x2 = 2,
x2 = 1,
and the solution is x2 = 1.00 and x1 = 1.00, which is correct to three digits. 
Another strategy would be to interchange the column having the small 
leading coeﬃcient with a column to its right. Both the row interchange and the 
column interchange strategies could be used simultaneously, of course. These 
processes, which obviously do not change the solution, are called pivoting. The  
equation or column to move into the active position may be chosen in such a 
way that the magnitude of the new diagonal element is the largest possible. 
Performing only row interchanges, so that at the kth stage the equation 
with 
max Underscript i equals k Overscript n Endscripts StartAbsoluteValue a Subscript i k Superscript left parenthesis k minus 1 right parenthesis Baseline EndAbsoluteValue
n
max
i=k |a(k−1)
ik
|
is moved into the kth row, is called partial pivoting. Performing both row 
interchanges and column interchanges, so that 
max Underscript i equals k semicolon j equals k Overscript n semicolon n Endscripts StartAbsoluteValue a Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline EndAbsoluteValue
n;n
max
i=k;j=k |a(k−1)
ij
|
is moved into the kth diagonal position, is called complete pivoting. See  Ex-
ercises 5.2a and 5.2b. 
It is always important to distinguish descriptions of eﬀects of actions from 
the actions that are actually carried out in the computer. Pivoting is “inter-
changing” rows or columns. We would usually do something like that in the

272
5 Solution of Linear Systems
computer only when we are ﬁnished and want to produce some output. In the 
computer, a row or a column is determined by the index identifying the row 
or column. All we do for pivoting is to keep track of the indices that we have 
permuted. 
There are many more computations required in order to perform complete 
pivoting than are required to perform partial pivoting. Gaussian elimination 
with complete pivoting can be shown to be stable; that is, the algorithm yields 
an exact solution to a slightly perturbed system, (A + δA)x = b. (We discuss 
stability on page 567.) For Gaussian elimination with partial pivoting, there 
are examples that show that it is not stable. These examples are somewhat 
contrived, however, and experience over many years has indicated that Gaus-
sian elimination with partial pivoting is stable for most problems occurring in 
practice. For this reason, together with the computational savings, Gaussian 
elimination with partial pivoting is one of the most commonly used methods 
for solving linear systems. 
There are two modiﬁcations of partial pivoting that result in stable algo-
rithms. One is to add one step of iterative reﬁnement (see Sect. 5.4, page 279) 
following each pivot. It can be shown that Gaussian elimination with partial 
pivoting together with one step of iterative reﬁnement is unconditionally sta-
ble. Another modiﬁcation is to consider two columns for possible interchange 
in addition to the rows to be interchanged. This does not require nearly as 
many computations as complete pivoting does. This method is used in LIN-
PACK and LAPACK (see page 627), and it is stable. 
Nonfull Rank and Nonsquare Systems 
The existence of an x that solves the linear system Ax = b depends on that 
system being consistent; it does not depend on A being square or of full rank. 
The methods discussed above also apply in this case. (See the discussion of LU 
and QR factorizations for nonfull rank and nonsquare matrices on pages 233 
and 240.) In applications, it is often annoying that many software developers 
do not provide capabilities for handling such systems. Many of the standard 
programs for solving systems provide solutions only if A is square and of full 
rank. This is because the algorithm is applied initially only to the coeﬃcient 
matrix and stops if it is not square and of full rank. The R function solve 
has this drawback. 
5.2.2 Choice of Direct Method 
Direct methods of solving linear systems all use some form of matrix factor-
ization, as discussed in Chap. 4. The LU factorization is the most commonly 
used method to solve a linear system. 
An important consideration is how easily an algorithm lends itself to im-
plementation on advanced computer architectures. Many of the algorithms for 
linear algebra can be vectorized easily. It is now becoming more important to

5.3 Iterative Methods for Consistent Systems
273
be able to parallelize the algorithms. The iterative methods discussed in the 
next section can often be parallelized more easily. 
For certain patterned matrices, other direct methods may be more eﬃcient. 
If a given matrix initially has a large number of zeros, it is important to 
preserve the zeros in the same positions (or in other known positions) in the 
matrices that result from operations on the given matrix. This helps to avoid 
unnecessary computations. The iterative methods discussed in the next section 
are often more useful for matrices with many zeros. (Such matrices are said 
to be “sparse.” We discuss computer storage of sparse matrices beginning on 
page 618 and mention some R software for dealing with them on page 644.) 
5.3 Iterative Methods for Consistent Systems 
In iterative methods for solving the linear system Ax = b, we begin with 
starting point x(0) , which we consider to be an approximate solution, and 
then move through a sequence of successive approximations x(1) , x(2) , . . ., that  
ultimately (it is hoped!) converge to a solution. The user must specify a con-
vergence criterion to determine when the approximation is close enough to 
the solution. The criterion may be based on successive changes in the solution 
x(k) − x(k−1) or on the diﬀerence ||Ax(k) − b||. 
Iterative methods may be particularly useful for very large systems because 
it may not be necessary to have the entire A matrix available for computa-
tions in each step. These methods are also useful for sparse systems. Also, as 
mentioned above, the iterative algorithms can often be parallelized. 
5.3.1 The Gauss-Seidel Method with Successive Overrelaxation 
One of the simplest iterative procedures is the Gauss-Seidel method. In this  
method, ﬁrst rearrange the equations if necessary so that no diagonal element 
of the coeﬃcient matrix is 0. We then begin with an initial approximation to 
the solution, x(0) . We next compute an update for the ﬁrst element of x: 
x 1 Superscript left parenthesis 1 right parenthesis Baseline equals StartFraction 1 Over a 11 EndFraction left parenthesis b 1 minus sigma summation Underscript j equals 2 Overscript n Endscripts a Subscript 1 j Baseline x Subscript j Superscript left parenthesis 0 right parenthesis Baseline right parenthesis periodx(1)
1
=
1
a11
⎛
⎝b1 −
n
E
j=2
a1jx(0)
j
⎞
⎠.
(If a11 is zero or very small in absolute value, we ﬁrst rearrange the equations; 
that is, we pivot.) 
Continuing in this way for the other elements of x, we have for  i = 1, . . . , n  
x Subscript i Superscript left parenthesis 1 right parenthesis Baseline equals StartFraction 1 Over a Subscript i i Baseline EndFraction left parenthesis b Subscript i Baseline minus sigma summation Underscript j equals 1 Overscript i minus 1 Endscripts a Subscript i j Baseline x Subscript j Superscript left parenthesis 1 right parenthesis Baseline minus sigma summation Underscript j equals i plus 1 Overscript n Endscripts a Subscript i j Baseline x Subscript j Superscript left parenthesis 0 right parenthesis Baseline right parenthesis commax(1)
i
= 1
aii
⎛
⎝bi −
i−1
E
j=1
aijx(1)
j
−
n
E
j=i+1
aijx(0)
j
⎞
⎠,

274
5 Solution of Linear Systems
where no sums are performed if the upper limit is smaller than the lower 
limit. After getting the approximation x(1) , we then continue this same kind 
of iteration for x(2) , x(3) , . . .. 
We continue the iterations until a convergence criterion is satisﬁed. As we 
discuss in Sect. 10.4.3, this criterion may be of the form 
upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis less than or equals epsilon commaΔ
(
x(k), x(k−1))
≤e,
where Δ
(
x(k) , x(k−1))
is a measure of the diﬀerence of x(k) and x(k−1) , such as
||x(k)−x(k−1)||. We may also base the convergence criterion on ||r(k)−r(k−1)||, 
where r(k) = b − Ax(k) . 
The Gauss-Seidel iterations can be thought of as beginning with a rear-
rangement of the original system of equations as 
StartLayout 1st Row 1st Column a 11 x 1 2nd Column Blank 3rd Column Blank 4th Column Blank 5th Column Blank 6th Column equals 7th Column b 1 8th Column minus 9th Column a 12 x 2 10th Column midline horizontal ellipsis minus 11th Column a Subscript 1 n Baseline x Subscript n 2nd Row 1st Column a 21 x 1 2nd Column plus 3rd Column a 22 x 2 4th Column Blank 5th Column Blank 6th Column equals 7th Column b 2 8th Column Blank 9th Column Blank 10th Column midline horizontal ellipsis minus 11th Column a Subscript 2 n Baseline x Subscript n 3rd Row 1st Column vertical ellipsis 2nd Column plus 3rd Column vertical ellipsis 4th Column Blank 5th Column vertical ellipsis 6th Column Blank 7th Column vertical ellipsis 4th Row 1st Column a Subscript left parenthesis n minus 1 right parenthesis 1 Baseline x 1 2nd Column plus 3rd Column a Subscript left parenthesis n minus 1 right parenthesis 2 Baseline x 2 4th Column plus midline horizontal ellipsis 5th Column Blank 6th Column equals 7th Column b Subscript n minus 1 8th Column Blank 9th Column minus 10th Column a Subscript n n Baseline x Subscript n 5th Row 1st Column a Subscript n Baseline 1 Baseline x 1 2nd Column plus 3rd Column a Subscript n Baseline 2 Baseline x 2 4th Column plus midline horizontal ellipsis plus 5th Column a Subscript n n Baseline x Subscript n 6th Column equals 7th Column b Subscript n 8th Column Blank 9th Column Blank 10th Column Blank 11th Column Blank EndLayout
a11x1
= b1 −a12x2 · · · −a1nxn
a21x1
+
a22x2
= b2
· · · −a2nxn
...
+
...
...
...
a(n−1)1x1 + a(n−1)2x2 + · · ·
= bn−1
−annxn
an1x1
+
an2x2
+ · · · + annxn = bn
(5.18) 
In this form, we identify three matrices: a diagonal matrix D, a lower trian-
gular L with 0s on the diagonal, and an upper triangular U with 0s on the 
diagonal: 
left parenthesis upper D plus upper L right parenthesis x equals b minus upper U x period(D + L)x = b −Ux.
We can write this entire sequence of Gauss-Seidel iterations in terms of these 
three ﬁxed matrices: 
x Superscript left parenthesis k plus 1 right parenthesis Baseline equals left parenthesis upper D plus upper L right parenthesis Superscript negative 1 Baseline left parenthesis minus upper U x Superscript left parenthesis k right parenthesis Baseline plus b right parenthesis periodx(k+1) = (D + L)−1(
−Ux(k) + b
)
.
(5.19) 
Convergence of the Gauss-Seidel Method 
This method will converge for any arbitrary starting value x(0) if  and only if  
rho left parenthesis left parenthesis upper D plus upper L right parenthesis Superscript negative 1 Baseline upper U right parenthesis less than 1 periodρ((D + L)−1U) < 1.
(5.20) 
We see this by considering the diﬀerence x(k) − x and writing 
x Superscript left parenthesis k plus 1 right parenthesis Baseline minus x equals left parenthesis upper D plus upper L right parenthesis Superscript negative 1 Baseline upper U left parenthesis x Superscript left parenthesis k right parenthesis Baseline minus x right parenthesis equals left parenthesis left parenthesis upper D plus upper L right parenthesis Superscript negative 1 Baseline upper U right parenthesis Superscript k Baseline left parenthesis x Superscript left parenthesis 0 right parenthesis Baseline minus x right parenthesis periodx(k+1) −x = (D + L)−1U(x(k) −x) = ((D + L)−1U)k(x(0) −x).
From (3.344) on page 196, we see that this residual goes to 0 if and only if 
ρ((D + L)−1 U) < 1. 
Moreover, as we stated informally on page 196, the rate of convergence 
increases with decreasing spectral radius. 
Notice that pivoting rearranges Eqs. (5.18) (resulting in diﬀerent D, L, and  
U) and thus changes the value of ρ((D + L)−1 U); hence, pivoting can cause 
the Gauss-Seidel method to be viable even when it is not with the original 
system (see Exercise 5.2e).

5.3 Iterative Methods for Consistent Systems
275
Successive Overrelaxation 
The Gauss-Seidel method may be unacceptably slow, so it may be modiﬁed 
so that the update is a weighted average of the regular Gauss-Seidel update 
and the previous value. This kind of modiﬁcation is called successive overre-
laxation, or  SOR. Instead of Eq. (5.19), the update is given by 
StartFraction 1 Over omega EndFraction left parenthesis upper D plus omega upper L right parenthesis x Superscript left parenthesis k plus 1 right parenthesis Baseline equals StartFraction 1 Over omega EndFraction left parenthesis left parenthesis 1 minus omega right parenthesis upper D minus omega upper U right parenthesis x Superscript left parenthesis k right parenthesis Baseline plus b comma 1
ω (D + ωL) x(k+1) = 1
ω
(
(1 −ω)D −ωU
)
x(k) + b,
(5.21) 
where the relaxation parameter ω is usually chosen to be between 0 and 
1. For ω = 1, the method is the ordinary Gauss-Seidel method; see Exer-
cises 5.2c, 5.2d, and  5.2g. 
5.3.2 Conjugate Gradient Methods for Symmetric Positive 
Deﬁnite Systems 
In the Gauss-Seidel methods, the convergence criterion is based on successive 
diﬀerences in the solutions x(k) and x(k−1) or in the residuals r(k) and r(k−1) . 
Other iterative methods focus directly on the magnitude of the residual 
r Superscript left parenthesis k right parenthesis Baseline equals b minus upper A x Superscript left parenthesis k right parenthesis Baseline periodr(k) = b −Ax(k).
(5.22) 
We seek a value x(k) such that the residual is small (in some sense). Methods 
that minimize ||r(k)||2 are called minimal residual methods. Two names asso-
ciated with minimum residual methods are MINRES and GMRES, which are 
names of speciﬁc algorithms. 
For a system with a symmetric positive deﬁnite coeﬃcient matrix A, it  
turns out that the best iterative method is based on minimizing the conjugate 
L2 norm (see Eq. (3.107)) 
parallel to r Superscript left parenthesis k right parenthesis normal upper T Baseline upper A Superscript negative 1 Baseline r Superscript left parenthesis k right parenthesis Baseline parallel to Subscript 2 Baseline period||r(k)TA−1r(k)||2.
A method based on this minimization problem is called a conjugate gradient 
method. 
The Conjugate Gradient Method 
The problem of solving the linear system Ax = b is equivalent to ﬁnding the 
minimum of the function 
f left parenthesis x right parenthesis equals one half x Superscript normal upper T Baseline upper A x minus x Superscript normal upper T Baseline b periodf(x) = 1
2xTAx −xTb.
(5.23) 
By setting the derivative of f to 0, we see that a stationary point of f occurs 
at the point x where Ax = b (see Sect. 9.6). 
If A is positive deﬁnite, the (unique) minimum of f is at x = A−1 b, 
and the value of f at the minimum is −1 
2bT Ab. The minimum point can be

276
5 Solution of Linear Systems
approached iteratively by starting at a point x(0) , moving to a point  x(1) 
that yields a smaller value of the function, and continuing to move to points 
yielding smaller values of the function. The kth point is x(k−1) +α(k−1) p(k−1) , 
where α(k−1) is a scalar and p(k−1) is a vector giving the direction of the 
movement. Hence, for the kth point, we have the linear combination 
x Superscript left parenthesis k right parenthesis Baseline equals x Superscript left parenthesis 0 right parenthesis Baseline plus alpha Superscript left parenthesis 1 right parenthesis Baseline p Superscript left parenthesis 1 right parenthesis Baseline plus midline horizontal ellipsis plus alpha Superscript left parenthesis k minus 1 right parenthesis Baseline p Superscript left parenthesis k minus 1 right parenthesis Baseline periodx(k) = x(0) + α(1)p(1) + · · · + α(k−1)p(k−1).
At the point x(k) , the function f decreases most rapidly in the direction 
of the negative gradient, −∇f(x(k) ), which is just the residual, 
minus nabla f left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis equals r Superscript left parenthesis k right parenthesis Baseline period−∇f(x(k)) = r(k).
If this residual is 0, no movement is indicated because we are at the solution. 
Moving in the direction of steepest descent may cause a very slow con-
vergence to the minimum. (The curve that leads to the minimum on the 
quadratic surface is obviously not a straight line. The direction of steepest 
descent changes as we move to a new point x(k+1) .) A good choice for the 
sequence of directions p(1) , p(2) , . . .  is such that 
left parenthesis p Superscript left parenthesis k right parenthesis Baseline right parenthesis Superscript normal upper T Baseline upper A p Superscript left parenthesis i right parenthesis Baseline equals 0 comma normal f normal o normal r i equals 1 comma ellipsis comma k minus 1 period(p(k))TAp(i) = 0,
for i = 1, . . . , k −1.
(5.24) 
Such a vector p(k) is A-conjugate to p(1) , p(2) , . . . p(k−1) (see page 115). Given 
a current point x(k) and a direction to move p(k) to the next point, we must 
also choose a distance α(k)||p(k)|| to move in that direction. We then have the 
next point, 
x Superscript left parenthesis k plus 1 right parenthesis Baseline equals x Superscript left parenthesis k right parenthesis Baseline plus alpha Superscript left parenthesis k right parenthesis Baseline p Superscript left parenthesis k right parenthesis Baseline periodx(k+1) = x(k) + α(k)p(k).
(5.25) 
(Notice that here, as often in describing algorithms in linear algebra, we use 
Greek letters, such as α, to denote scalar quantities.) 
We choose the directions as in Newton steps, so the ﬁrst direction is Ar(0) 
(see Sect. 9.6.2). The paths deﬁned by the directions p(1) , p(2) , . . .  in Eq. (5.24) 
are called the conjugate gradients. A conjugate gradient method for solving 
the linear system is shown in Algorithm 5.1. 
Algorithm 5.1 The Conjugate Gradient Method for Solving the 
Symmetric Positive Deﬁnite System Ax = b, Starting with x(0) 
0. Input stopping criteria, e and kmax. 
Set k = 0;  r(k) = b − Ax(k) ; s(k) = Ar(k) ; p(k) = s(k) ; and  γ(k) = ||s(k)||2 . 
1. If γ(k) ≤e, set  x = x(k) and terminate. 
2. Set q(k) = Ap(k) . 
3. Set α(k) = 
γ(k)
||q(k)||2 . 
4. Set x(k+1) = x(k) + α(k) p(k) . 
5. Set r(k+1) = r(k) − α(k) q(k) . 
6. Set s(k+1) = Ar(k+1) .

5.3 Iterative Methods for Consistent Systems
277
7. Set γ(k+1) = ||s(k+1)||2 . 
8. Set p(k+1) = s(k+1) + γ(k+1) 
γ(k) p(k) . 
9. If k <  kmax, 
set k = k +  1  and go to step  1;  
otherwise, 
issue message that 
“algorithm did not converge in kmax iterations.” 
There are various ways in which the computations in Algorithm 5.1 could 
be arranged. Although any vector norm could be used in Algorithm 5.1, the  
L2 norm  is  the most common one.  
This method, like other iterative methods, is more appropriate for large 
systems. (“Large” in this context means bigger than 1000 × 1000.) 
In exact arithmetic, the conjugate gradient method should converge in n 
steps for an n × n system. In practice, however, its convergence rate varies 
widely, even for systems of the same size. Its convergence rate generally de-
creases with increasing L2 condition number (which is a function of the max-
imum and minimum nonzero eigenvalues), but that is not at all the complete 
story. The rate depends in a complicated way on all of the eigenvalues. The 
more spread out the eigenvalues are, the slower the rate. For diﬀerent sys-
tems with roughly the same condition number, the convergence is faster if all 
eigenvalues are in two clusters around the maximum and minimum values. 
Krylov Methods 
Notice that the steps in the conjugate gradient algorithm involve the matrix 
A only through linear combinations of its rows or columns; that is, in any 
iteration, only a vector of the form Av or AT w is used. The conjugate gra-
dient method and related procedures, called Lanczos methods, move through 
a Krylov space in the progression to the solution. A Krylov space is the k-
dimensional vector space of order n generated by the n × n matrix A and the 
vector v by forming the basis {v, Av, A2 v, . . . , Ak−1 v}. We often denote this 
space as Kk(A, v) or just as  Kk: 
script upper K Subscript k Baseline equals script upper V left parenthesis StartSet v comma upper A v comma upper A squared v comma ellipsis comma upper A Superscript k minus 1 Baseline v EndSet right parenthesis periodKk = V({v, Av, A2v, . . . , Ak−1v}).
(5.26) 
Methods for computing eigenvalues are often based on Krylov spaces. 
GMRES Methods 
The conjugate gradient method seeks to minimize the residual vector in 
Eq. (5.22), r(k) = b − Ax(k) , and the convergence criterion is based on the 
linear combinations of the columns of the coeﬃcient matrix formed by that 
vector, ||Ar(k)||.

278
5 Solution of Linear Systems
The generalized minimal residual (GMRES) method of Saad and Schultz 
(1986) for solving Ax = b begins with an approximate solution x(0) and takes 
x(k) as x(k−1) + z(k) , where  z(k) is the solution to the minimization problem, 
min Underscript z element of script upper K Subscript k Baseline left parenthesis upper A comma r Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis Endscripts parallel to r Superscript left parenthesis k minus 1 right parenthesis Baseline minus upper A z parallel to comma
min
z∈Kk(A,r(k−1)) ||r(k−1) −Az||,
where, as before, r(k) = b−Ax(k) . This minimization problem is a constrained 
least squares problem. The speed of convergence of GMRES depends very 
strongly on the arrangements of the computations. 
Preconditioning 
As we mentioned above, the convergence rate of the conjugate gradient 
method depends on the distribution of the eigenvalues in rather complicated 
ways. The ratio of the largest to the smallest (i.e., the L2 condition number 
is important) and the convergence rate for the conjugate gradient method are 
slower for larger L2 condition numbers. The rate also is slower if the eigenval-
ues are spread out, especially if there are several eigenvalues near the largest 
or smallest. This phenomenon is characteristic of other Krylov space methods. 
One way of addressing the problem of slow convergence of iterative meth-
ods is by preconditioning, that is, by replacing the system Ax = b with another 
system, 
upper M Superscript negative 1 Baseline upper A x equals upper M Superscript negative 1 Baseline b commaM −1Ax = M −1b,
(5.27) 
where M is very similar (by some measure) to A, but the system M −1 Ax = 
M −1 b has a better condition for the problem at hand. We choose M to be 
symmetric and positive deﬁnite and such that Mx = b is easy to solve. If M is 
an approximation of A, then  M −1 A should be well-conditioned; its eigenvalues 
should all be close to each other. 
A problem with applying the conjugate gradient method to the precondi-
tioned system M −1 Ax = M −1 b is that M −1 A may not be symmetric. We can 
form an equivalent symmetric system, however, by decomposing the symmet-
ric positive deﬁnite M as M = VCV T and then 
upper M Superscript negative one half Baseline equals upper V normal d normal i normal a normal g left parenthesis left parenthesis 1 divided by StartRoot c 11 EndRoot comma ellipsis comma 1 divided by StartRoot c Subscript n n Baseline EndRoot right parenthesis right parenthesis upper V Superscript normal upper T Baseline commaM −1
2 = V diag((1/√c11, . . . , 1/√cnn))V T,
as in Eq. (3.307), after inverting the positive square roots of C. Multiplying 
both sides of M −1 Ax = M −1 b by M 
1 
2 , inserting the factor M −1 
2 M 
1 
2 , and  
arranging terms yield 
left parenthesis upper M Superscript negative one half Baseline upper A upper M Superscript negative one half Baseline right parenthesis upper M Superscript one half Baseline x equals upper M Superscript negative one half Baseline b period(M −1
2 AM −1
2 )M
1
2 x = M −1
2 b.
This can all be done, and Algorithm 5.1 can be modiﬁed without explicit for-
mation of and multiplication by M 
1 
2 . The preconditioned conjugate gradient 
method is shown in Algorithm 5.2.

5.4 Iterative Reﬁnement
279
Algorithm 5.2 The Preconditioned Conjugate Gradient Method for 
Solving the Symmetric Positive Deﬁnite System Ax = b, Starting 
with x(0) 
0. Input stopping criteria, e and kmax. 
Set k = 0;  r(k) = b−Ax(k) ; s(k) = Ar(k) ; p(k) = M −1 s(k) ; y(k) = M −1 r(k) ; 
and γ(k) = y(k)T s(k) . 
1. If γ(k) ≤e, set  x = x(k) and terminate. 
2. Set q(k) = Ap(k) . 
3. Set α(k) = 
γ(k)
||q(k)||2 . 
4. Set x(k+1) = x(k) + α(k) p(k) . 
5. Set r(k+1) = r(k) − α(k) q(k) . 
6. Set s(k+1) = Ar(k+1) . 
7. Set y(k+1) = M −1 r(k+1) . 
8. Set γ(k+1) = y(k+1)T s(k+1) . 
9. Set p(k+1) = M −1 s(k+1) + γ(k+1) 
γ(k) p(k) . 
10. If k <  kmax, 
set k = k +  1  and go to step  1;  
otherwise, 
issue message that 
“algorithm did not converge in kmax iterations.” 
The choice of an appropriate matrix M is not an easy problem, and we 
will not consider the methods here. We will also mention the preconditioned 
conjugate gradient method in Sect. 6.1.4, but there, again, we will refer the 
reader to other sources for details. 
5.3.3 Multigrid Methods 
Iterative methods have important applications in solving diﬀerential equa-
tions. The solution of diﬀerential equations by a ﬁnite diﬀerence discretiza-
tion involves the formation of a grid. The solution process may begin with a 
fairly coarse grid on which a solution is obtained. Then a ﬁner grid is formed, 
and the solution is interpolated from the coarser grid to the ﬁner grid to be 
used as a starting point for a solution over the ﬁner grid. The process is then 
continued through ﬁner and ﬁner grids. If all of the coarser grids are used 
throughout the process, the technique is a multigrid method. There are many 
variations of exactly how to do this. Multigrid methods are useful solution 
techniques for diﬀerential equations. 
5.4 Iterative Reﬁnement 
Once an approximate solution x(0) to the linear system Ax = b is available, 
iterative reﬁnement can yield a solution that is closer to the true solution. 
The residual

280
5 Solution of Linear Systems
r equals b minus upper A x Superscript left parenthesis 0 right parenthesisr = b −Ax(0)
is used for iterative reﬁnement. Clearly, if h = A+ r, then  x(0) +h is a solution 
to the original system. 
The problem considered here is not just an iterative solution to the lin-
ear system, as we discussed in Sect. 5.3. Here, we assume x(0) was computed 
accurately given the ﬁnite precision of the computer. In this case, it is likely 
that r cannot be computed accurately enough to be of any help. If, however, 
r can be computed using a higher precision, then a useful value of h can be 
computed. This process can then be iterated as shown in Algorithm 5.3. 
Algorithm 5.3 Iterative Reﬁnement of the Solution to Ax = b, 
Starting with x(0) 
0. Input stopping criteria, e and kmax. 
Set k = 0.  
1. Compute r(k) = b − Ax(k) in higher precision. 
2. Compute h(k) = A+ r(k) . 
3. Set x(k+1) = x(k) + h(k) . 
4. If ||h(k)|| ≤e||x(k+1)||, then  
set x = x(k+1) and terminate; otherwise, 
if k <  kmax, 
set k = k +  1  and go to step  1;  
otherwise, 
issue message that 
“algorithm did not converge in kmax iterations.” 
In step 2, if  A is of full rank, then A+ is A−1 . Also, as we have emphasized 
already, the fact that we write an expression such as A+ r does not mean that 
we compute A+ . The norm in step 4 is usually chosen to be the ∞ norm. 
The algorithm may not converge, so it is necessary to have an alternative exit 
criterion, such as a maximum number of iterations. 
The use of iterative reﬁnement as a general-purpose method is severely 
limited by the need for higher precision in step 1. On the other hand, if com-
putations in higher precision can be performed, they can be applied to step 2 
or just in the original computations for x(0) . In terms of both accuracy and 
computational eﬃciency, using higher precision throughout is usually better. 
5.5 Updating a Solution to a Consistent System 
In applications of linear systems, it is often the case that after the system 
Ax = b has been solved, the right-hand side is changed and the system Ax = c 
must be solved. If the linear system Ax = b has been solved by a direct 
method using one of the factorizations discussed in Chap. 4, the factors of A 
can be used to solve the new system Ax = c. If the right-hand side is a small

5.5 Updating a Solution to a Consistent System
281
perturbation of b, say  c = b+δb, an iterative method can be used to solve the 
new system quickly, starting from the solution to the original problem. 
If the coeﬃcient matrix in a linear system Ax = b is perturbed to result 
in the system (A + δA)x = b, it may be possible to use the solution x0 to the 
original system eﬃciently to arrive at the solution to the perturbed system. 
One way, of course, is to use x0 as the starting point in an iterative procedure. 
Often, in applications, the perturbations are of a special type, such as 
upper A overTilde equals upper A minus u v Superscript normal upper T Baseline comma-A = A −uvT,
where u and v are vectors. (This is a “rank-one” perturbation of A, and  
when the perturbed matrix is used as a transformation, it is called a “rank-
one” update. As we have seen, a Householder reﬂection is a special rank-one 
update.) Assuming A is an n × n matrix of full rank, it is easy to write -A−1 
in terms of A−1 : 
upper A overTilde Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline plus alpha left parenthesis upper A Superscript negative 1 Baseline u right parenthesis left parenthesis v Superscript normal upper T Baseline upper A Superscript negative 1 Baseline right parenthesis -A−1 = A−1 + α(A−1u)(vTA−1)
(5.28) 
with 
alpha equals StartFraction 1 Over 1 minus v Superscript normal upper T Baseline upper A Superscript negative 1 Baseline u EndFraction periodα =
1
1 −vTA−1u.
These are called the Sherman-Morrison formulas (from Sherman and Morrison 
1950). -A−1 exists so long as vT A−1 u /= 1. Because x0 = A−1 b, the solution to 
the perturbed system is 
x overTilde Subscript 0 Baseline equals x 0 plus StartFraction left parenthesis upper A Superscript negative 1 Baseline u right parenthesis left parenthesis v Superscript normal upper T Baseline x 0 right parenthesis Over left parenthesis 1 minus v Superscript normal upper T Baseline upper A Superscript negative 1 Baseline u right parenthesis EndFraction period˜x0 = x0 + (A−1u)(vTx0)
(1 −vTA−1u) .
If the perturbation is more than rank one (i.e., if the perturbation is 
upper A overTilde equals upper A minus upper U upper V Superscript normal upper T Baseline comma -A = A −UV T,
(5.29) 
where U and V are n × m matrices with n ≥ m), a generalization of the 
Sherman-Morrison formula, sometimes called the Woodbury formula, is 
upper A overTilde Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline plus upper A Superscript negative 1 Baseline upper U left parenthesis upper I Subscript m Baseline minus upper V Superscript normal upper T Baseline upper A Superscript negative 1 Baseline upper U right parenthesis Superscript negative 1 Baseline upper V Superscript normal upper T Baseline upper A Superscript negative 1 -A−1 = A−1 + A−1U(Im −V TA−1U)−1V TA−1
(5.30) 
(from Woodbury 1950). The solution to the perturbed system is easily seen 
to be 
x overTilde Subscript 0 Baseline equals x 0 plus upper A Superscript negative 1 Baseline upper U left parenthesis upper I Subscript m Baseline minus upper V Superscript normal upper T Baseline upper A Superscript negative 1 Baseline upper U right parenthesis Superscript negative 1 Baseline upper V Superscript normal upper T Baseline x 0 period˜x0 = x0 + A−1U(Im −V TA−1U)−1V Tx0.
As we have emphasized many times, we rarely compute the inverse of a ma-
trix, and so the Sherman-Morrison-Woodbury formulas are not used directly. 
Having already solved Ax = b, it should be easy to solve another system, 
say Ay = ui, where  ui is a column of U. If  m is relatively small, as it is in 
most applications of this kind of update, there are not many systems Ay = ui 
to solve. Solving these systems, of course, yields A−1 U, the most formidable 
component of the Sherman-Morrison-Woodbury formula. The system to solve 
is of order m also.

282
5 Solution of Linear Systems
Occasionally, the updating matrices in Eq. (5.29) may be used with a 
weighting matrix, so we have -A = A −UWV T . An extension of the Sherman-
Morrison-Woodbury formula is 
left parenthesis upper A minus upper U upper W upper V Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline equals upper A Superscript negative 1 Baseline plus upper A Superscript negative 1 Baseline upper U left parenthesis upper W Superscript negative 1 Baseline minus upper V Superscript normal upper T Baseline upper A Superscript negative 1 Baseline upper U right parenthesis Superscript negative 1 Baseline upper V Superscript normal upper T Baseline upper A Superscript negative 1 Baseline period(A −UWV T)−1 = A−1 + A−1U(W −1 −V TA−1U)−1V TA−1.
(5.31) 
This is sometimes called the Hemes formula. (The attributions of discovery 
are somewhat murky, and statements made by historians of science of the 
form “
was the ﬁrst to
” must be taken with a grain of salt; not every 
discovery has resulted in an available publication. This is particularly true in 
numerical analysis, where scientiﬁc programmers often just develop a method 
in the process of writing code and have neither the time nor the interest in 
getting a publication out of it.) 
Another situation that requires an update of a solution occurs when the 
system is augmented with additional equations and more variables: 
Start 2 By 2 Matrix 1st Row 1st Column upper A 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix StartBinomialOrMatrix x Choose x Subscript plus Baseline EndBinomialOrMatrix equals StartBinomialOrMatrix b Choose b Subscript plus Baseline EndBinomialOrMatrix period
[
A A12
A21 A22
] [
x
x+
]
=
[
b
b+
]
.
A simple way of obtaining the solution to the augmented system is to use the 
solution x0 to the original system in an iterative method. The starting point 
for a method based on Gauss-Seidel or a conjugate gradient method can be 
taken as (x0, 0) or as (x0, x  (0) 
+ ) if a better value of x (0) 
+ is known. 
In many statistical applications, the systems are overdetermined, with A 
being n×m and n > m. There are more equations, or restrictions, than there 
are variables that can take on the necessary values to satisfy the equations. In 
the next section, we consider the general problem of solving overdetermined 
systems by using least squares, and then in Sect. 5.6.5, we discuss updating a 
least squares solution to an overdetermined system. 
5.6 Overdetermined Systems: Least Squares 
In applications, linear systems are often used as models of relationships be-
tween one observable variable, a “response,” and another group of observable 
variables, “predictor variables.” The model is unlikely to ﬁt exactly any set 
of observed values of responses and predictor variables. This may be due to 
eﬀects of other predictor variables that are not included in the model, mea-
surement error, the relationship among the variables being nonlinear, or some 
inherent randomness in the system. In such applications, we generally take 
a larger number of observations than there are variables in the system; thus, 
with each set of observations on the response and associated predictors making 
up one equation, we have a system with more equations than variables. 
An overdetermined system may be written as 
upper X b almost equals y commaXb ≈y,
(5.32)

5.6 Overdetermined Systems: Least Squares
283
where X is n×m and rank(X|y) > m; that is, the system is not consistent. We 
have changed the notation slightly from the consistent systems Ax = b that 
we have been using because now we have in mind statistical applications and 
in those the notation y ≈ Xβ is more common. The problem is to determine 
a value of b that makes the approximation close in some sense. In applications 
of linear systems, we refer to this as “ﬁtting” the system, which is referred to 
as a “model.” 
The basic approach in ﬁtting the model, or “solving” the overdetermined 
system, is to ﬁnd a “close” solution or, more precisely, to determine a b∗ such 
that some norm of the lack of ﬁt is minimized. We solve an optimization 
problem of the form 
min Underscript b Endscripts parallel to y minus upper X b parallel to periodmin
b
||y −Xb||.
In this section, we will use the simple approach of diﬀerentiating a function, 
setting the result to 0, and solving the resulting equation. The details of 
diﬀerentiating a vector function are discussed in Sects. 7.1 and 7.2, methods for 
ﬁtting Xb ≈ y by minimizing ||y − Xb|| are discussed in Sect. 9.2, and general 
methods for ﬁnding the minimum of a function are discussed in Sect. 9.6. 
Overdetermined systems abound in ﬁtting equations to data. The usual 
linear regression model is an overdetermined system, and we discuss regression 
problems further in Sects. 9.1 and 9.2. We should not confuse statistical infer-
ence with ﬁtting equations to data, although the latter task is a component of 
the former activity. In this section, we consider some of the more mechanical 
and computational aspects of the problem. 
Accounting for an Intercept 
Given a set of observations, the ith row of the system Xb ≈ y represents the 
linear relationship between yi and the corresponding xs in the vector xi: 
y Subscript i Baseline almost equals b 1 x Subscript 1 i Baseline plus midline horizontal ellipsis plus b Subscript m Baseline x Subscript m i Baseline periodyi ≈b1x1i + · · · + bmxmi.
A diﬀerent formulation of the relationship between yi and the corresponding 
xs might include an intercept term: 
y Subscript i Baseline almost equals b overTilde Subscript 0 Baseline plus b overTilde Subscript 1 Baseline x Subscript 1 i Baseline plus midline horizontal ellipsis plus b overTilde Subscript m Baseline x Subscript m i Baseline periodyi ≈˜b0 + ˜b1x1i + · · · + ˜bmxmi.
There are two ways to incorporate this intercept term. One way is just to 
include a column of 1s in the X matrix. This approach makes the matrix X 
in Eq. (5.32) n × (m + 1), or else it means that we merely redeﬁne x1i to be 
the constant 1. Another way is to assume that the model is an exact ﬁt for 
some set of values of y and the xs. If we assume that the model ﬁts y = 0  
and x = 0 exactly, we have a model without an intercept (i.e., with a zero 
intercept). 
Often, a reasonable assumption is that the model may have a nonzero 
intercept, but it ﬁts the means of the set of observations; that is, the equation

284
5 Solution of Linear Systems
is exact for y = ¯y and x = ¯x, where  the  jth element of ¯x is the mean of 
the jth column vector of X. (Students with some familiarity with the subject 
may think this is a natural consequence of ﬁtting the model. It is not unless 
the model ﬁtting is by ordinary least squares.) If we require that the ﬁtted 
equation be exact for the means (or if this happens naturally, as in the case of 
ordinary least squares), we may center each column by subtracting its mean 
from each element in the same manner as we centered vectors on page 59. In  
place of y, we have the vector y − ¯y. The matrix formed by centering all of 
the columns of a given matrix is called a centered matrix, and if the original 
matrix is X, we represent the centered matrix as Xc in a notation analogous 
to what we introduced for centered vectors. If we represent the matrix whose 
ith column is the constant mean of the ith column of X as X, 
upper X Subscript normal c Baseline equals upper X minus upper X overbar periodXc = X −X.
Using the centered data provides two linear systems: a set of approximate 
equations in which the intercept is ignored and an equation that ﬁts the point 
that is assumed to be satisﬁed exactly: 
y overbar equals upper X overbar b period¯y = Xb.
In the rest of this section, we will generally ignore the question of an 
intercept. Except in a method discussed on page 298, the  X can be considered 
to include a column of 1s, to be centered, or to be adjusted by any other point. 
We will return to this idea of centering the data in Sect. 8.6.3. 
5.6.1 Least Squares Solution of an Overdetermined System 
Although there may be no b that will make the system in (5.32) an equation, 
the system can be written as the equation 
upper X b equals y minus r commaXb = y −r,
(5.33) 
where r is an n-vector of possibly arbitrary residuals or “errors.” 
A least squares solution -b to the system in (5.32) is one such that the 
Euclidean norm of the vector of residuals is minimized; that is, the solution 
to the problem 
min Underscript b Endscripts parallel to y minus upper X b parallel to Subscript 2 Baseline period min
b
||y −Xb||2.
(5.34) 
The least squares solution is also called the “ordinary least squares” (OLS) 
ﬁt. 
By rewriting the square of this norm as 
left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis comma(y −Xb)T(y −Xb),
(5.35) 
diﬀerentiating, and setting it equal to 0, we see that the minimum (of both 
the norm and its square) occurs at the -b that satisﬁes the square system

5.6 Overdetermined Systems: Least Squares
285
upper X Superscript normal upper T Baseline upper X ModifyingAbove b With caret equals upper X Superscript normal upper T Baseline y periodXTX-b = XTy.
(5.36) 
The system (5.36) based on the Gramian XT X is called the normal equa-
tions. From the results in Sect. 3.4.11, we see that for any matrix X with real 
elements, XT X is nonnegative deﬁnite. Also, for any n × m matrix X with 
real elements and with n < m, XT X is not positive deﬁnite, but if X is of 
full column rank, XT X is positive deﬁnite. 
The condition of the Gramian determines the expected accuracy of a so-
lution to the least squares problem. As we mentioned in Sect. 5.1, however, 
because the condition number of XT X is the square of the condition number 
of X, it may be better to work directly on X in (5.32) rather than to use 
the normal equations. The normal equations are useful expressions, however, 
whether or not they are used in the computations. This is another case where 
a formula does not deﬁne an algorithm, as with other cases we have encoun-
tered many times. We should note, of course, that any information about the 
stability of the problem that the Gramian may provide can be obtained from 
X directly. 
Orthogonality of Least Squares Residuals to span(X) 
The least squares ﬁt to the overdetermined system has a very useful property 
with important consequences. The least squares ﬁt partitions the space into 
two interpretable orthogonal spaces. As we see from Eq. (5.36), the residual 
vector y − X-b is orthogonal to each column in X: 
upper X Superscript normal upper T Baseline left parenthesis y minus upper X ModifyingAbove b With caret right parenthesis equals 0 periodXT(y −X-b) = 0.
(5.37) 
This fact illustrates the close relationship of least squares approximations to 
the Gram-Schmidt transformations discussed on page 48. 
A consequence of this orthogonality for models that include an intercept 
is that the sum of the residuals is 0. (The residual vector is orthogonal to the 
1 vector.) Another consequence for models that include an intercept is that 
the least squares solution provides an exact ﬁt to the mean. 
The orthogonality of the residuals of a least squares ﬁt to all columns of X 
characterizes the least squares ﬁt. To see this, let -b be such that XT (y−X-b) =  
0. Now, for any b, 
StartLayout 1st Row 1st Column parallel to y minus upper X b parallel to Subscript 2 Superscript 2 2nd Column equals 3rd Column parallel to y minus upper X b overTilde plus left parenthesis upper X b overTilde minus upper X b right parenthesis parallel to Subscript 2 Superscript 2 2nd Row 1st Column Blank 2nd Column equals 3rd Column parallel to y minus upper X b overTilde parallel to Subscript 2 Superscript 2 Baseline plus 2 left parenthesis y minus upper X b overTilde right parenthesis Superscript normal upper T Baseline left parenthesis upper X b overTilde minus upper X b right parenthesis plus parallel to upper X b overTilde minus upper X b parallel to Subscript 2 Superscript 2 3rd Row 1st Column Blank 2nd Column equals 3rd Column parallel to y minus upper X b overTilde parallel to Subscript 2 Superscript 2 Baseline plus 2 left parenthesis y minus upper X b overTilde right parenthesis Superscript normal upper T Baseline upper X left parenthesis b overTilde minus b right parenthesis plus parallel to upper X b overTilde minus upper X b parallel to Subscript 2 Superscript 2 4th Row 1st Column Blank 2nd Column equals 3rd Column parallel to y minus upper X b overTilde parallel to Subscript 2 Superscript 2 Baseline plus 2 left parenthesis upper X Superscript normal upper T Baseline left parenthesis y minus upper X b overTilde right parenthesis right parenthesis Superscript normal upper T Baseline left parenthesis b overTilde minus b right parenthesis plus parallel to upper X b overTilde minus upper X b parallel to Subscript 2 Superscript 2 5th Row 1st Column Blank 2nd Column equals 3rd Column parallel to y minus upper X b overTilde parallel to Subscript 2 Superscript 2 Baseline plus parallel to upper X b overTilde minus upper X b parallel to Subscript 2 Superscript 2 6th Row 1st Column Blank 2nd Column greater than or equals 3rd Column parallel to y minus upper X b overTilde parallel to Subscript 2 Superscript 2 Baseline semicolon EndLayout||y −Xb||2
2 = ||y −X-b + (X-b −Xb)||2
2
= ||y −X-b||2
2 + 2(y −X-b)T(X-b −Xb) + ||X-b −Xb||2
2
= ||y −X-b||2
2 + 2(y −X-b)TX(-b −b) + ||X-b −Xb||2
2
= ||y −X-b||2
2 + 2(XT(y −X-b))T(-b −b) + ||X-b −Xb||2
2
= ||y −X-b||2
2 + ||X-b −Xb||2
2
≥||y −X-b||2
2;

286
5 Solution of Linear Systems
hence, -b is a least squares ﬁt. 
These properties are so familiar to statisticians that some think that they 
are essential characteristics of any regression modeling; they are not. We will 
see in later sections that they do not hold for other approaches to ﬁtting the 
basic model y ≈ Xb. The least squares solution, however, has some desirable 
statistical properties under fairly common distributional assumptions, as we 
discuss in Chap. 9. 
Numerical Accuracy in Overdetermined Systems 
In Sect. 5.1.3, we discussed numerical accuracy in computations for solving 
a consistent (square) system of equations and showed how bounds on the 
numerical error could be expressed in terms of the condition number of the 
coeﬃcient matrix, which we had deﬁned (on page 263) as the product of norms 
of the coeﬃcient matrix and its inverse. One of the most useful versions of 
this condition number is the one using the L2 matrix norm, which is called 
the spectral condition number. This is the most commonly used condition 
number, and we generally just denote it by κ(·). The spectral condition num-
ber is the ratio of the largest eigenvalue in absolute value to the smallest in 
absolute value, and this extends easily to a deﬁnition of the spectral condition 
number that applies both to nonsquare matrices and to singular matrices: the 
condition number of a matrix is the ratio of the largest singular value to the 
smallest nonzero singular value. As we saw on page 184, the nonzero singular 
values of X are the square roots of the nonzero eigenvalues of XT X; hence, 
kappa left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis equals left parenthesis kappa left parenthesis upper X right parenthesis right parenthesis squared periodκ(XTX) = (κ(X))2.
(5.38) 
The condition number of XT X is a measure of the numerical accuracy 
we can expect in solving the normal equations (5.36). Because the condition 
number of X is smaller, we have an indication that it might be better not 
to form the normal equations unless we must. It might be better to work 
just with X. This is one of the most important principles in numerical linear 
algebra. We will work with X instead of XT X in the next sections. 
5.6.2 Least Squares with a Full Rank Coeﬃcient Matrix 
If the n × m matrix X is of full column rank, the least squares solution, 
from Eq. (5.36), is -b = (XT X)−1 XT y and is obviously unique. A good way to 
compute this is to form the QR factorization of X. 
First we write X = QR, as in Eq. (4.33) on page 238, where  R is as in 
Eq. (4.34), 
upper R equals StartBinomialOrMatrix upper R 1 Choose 0 EndBinomialOrMatrix commaR =
[ R1
0
]
,
with R1 an m×m upper triangular matrix. The squared residual norm (5.35) 
can be written as

5.6 Overdetermined Systems: Least Squares
287
StartLayout 1st Row 1st Column left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis 2nd Column equals 3rd Column left parenthesis y minus upper Q upper R b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper Q upper R b right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis upper Q Superscript normal upper T Baseline y minus upper R b right parenthesis Superscript normal upper T Baseline left parenthesis upper Q Superscript normal upper T Baseline y minus upper R b right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis c 1 minus upper R 1 b right parenthesis Superscript normal upper T Baseline left parenthesis c 1 minus upper R 1 b right parenthesis plus c 2 Superscript normal upper T Baseline c 2 comma EndLayout(y −Xb)T(y −Xb) = (y −QRb)T(y −QRb)
= (QTy −Rb)T(QTy −Rb)
= (c1 −R1b)T(c1 −R1b) + cT
2 c2,
(5.39) 
where c1 is a vector with m elements and c2 is a vector with n − m elements, 
such that 
upper Q Superscript normal upper T Baseline y equals StartBinomialOrMatrix c 1 Choose c 2 EndBinomialOrMatrix periodQTy =
(
c1
c2
)
.
(5.40) 
Because the squared norm (5.35), and, hence, the expression (5.39), is non-
negative, the minimum of the squared residual norm in Eq. (5.39) occurs when 
(c1 − R1b)T (c1 − R1b) = 0, that is, when (c1 − R1b) = 0  or  
upper R 1 b equals c 1 periodR1b = c1.
(5.41) 
We could also use diﬀerentiation to ﬁnd the minimum of Eq. (5.39), because 
in that case, the derivative of cT 
2 c2 with respect to b is 0. 
Because R1 is triangular, the system is easy to solve: -b = R−1 
1 c1. From  
Eq. (4.36), we have 
upper X Superscript plus Baseline equals Start 1 By 2 Matrix 1st Row 1st Column upper R 1 Superscript negative 1 Baseline 2nd Column 0 EndMatrix upper Q Superscript normal upper T Baseline commaX+ =
[
R−1
1
0
]
QT,
(5.42) 
and so we have 
ModifyingAbove b With caret equals upper X Superscript plus Baseline y period-b = X+y.
(5.43) 
Note that Eq. (5.43) does not hold for generalized inverses that are not Moore-
Penrose inverses. 
We also see from Eq. (5.39) that the minimum of the residual norm is cT 
2 c2. 
This is called the residual sum of squares in the least squares ﬁt. 
5.6.3 Least Squares with a Coeﬃcient Matrix Not of Full Rank 
If X is not of full rank (i.e., if X has rank r <  m), the least squares solu-
tion is not unique, and in fact, a solution is any vector ˜b = (XT X)−XT y, 
where (XT X)− is any generalized inverse. This is a solution to the normal 
equations (5.36). 
If X is not of full rank, Eq. (5.43), -b = X+ y, still provides a least squares 
solution, and it is the same as -b = (XT X)+ XT y. (The Moore-Penrose inverse 
in Eq. (5.42) would be replaced by one as shown in Eq. (4.43) on page 241, of  
course.) 
The residual corresponding to this solution is 
y minus upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y equals left parenthesis upper I minus upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline right parenthesis y periody −X(XTX)−XTy = (I −X(XTX)−XT)y.
The residual vector is invariant to the choice of generalized inverse. (We will 
see this important fact in Eq. (8.48) on page 399 and in the  numerical example  
on page 445.)

288
5 Solution of Linear Systems
An Optimal Property of the Solution Using the Moore-Penrose 
Inverse 
The solution corresponding to the Moore-Penrose inverse is unique because, as 
we have seen, that generalized inverse is unique. That solution is interesting 
for another reason, however: the b from the Moore-Penrose inverse has the 
minimum L2 norm of all solutions. 
To see that this solution has minimum norm, ﬁrst factor X, as in Eq. (4.41) 
on page 241, 
upper X equals upper Q upper R upper U Superscript normal upper T Baseline commaX = QRU T,
and form the Moore-Penrose inverse as in Eq. (4.43): 
upper X Superscript plus Baseline equals upper U Start 2 By 2 Matrix 1st Row 1st Column upper R 1 Superscript negative 1 Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix upper Q Superscript normal upper T Baseline periodX+ = U
[
R−1
1
0
0
0
]
QT.
Then 
ModifyingAbove b With caret equals upper X Superscript plus Baseline y-b = X+y
(5.44) 
is a least squares solution, just as in the full rank case. Now, let 
upper Q Superscript normal upper T Baseline y equals StartBinomialOrMatrix c 1 Choose c 2 EndBinomialOrMatrix commaQTy =
( c1
c2
)
,
as in Eq. (5.40), except ensure that c1 has exactly r elements and c2 has n −r 
elements, and let 
upper U Superscript normal upper T Baseline b equals StartBinomialOrMatrix z 1 Choose z 2 EndBinomialOrMatrix commaU Tb =
( z1
z2
)
,
where z1 has r elements. We proceed as in Eqs. (5.39). We seek to minimize
||y − Xb||2 (which is the square root of the expression in Eqs. (5.39)); and 
because multiplication by an orthogonal matrix does not change the norm, 
we have 
StartLayout 1st Row 1st Column parallel to y minus upper X b parallel to Subscript 2 2nd Column equals 3rd Column parallel to upper Q Superscript normal upper T Baseline left parenthesis y minus upper X upper U upper U Superscript normal upper T Baseline b right parenthesis parallel to Subscript 2 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartAbsoluteValue StartAbsoluteValue StartBinomialOrMatrix c 1 Choose c 2 EndBinomialOrMatrix minus Start 2 By 2 Matrix 1st Row 1st Column upper R 1 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix StartBinomialOrMatrix z 1 Choose z 2 EndBinomialOrMatrix EndAbsoluteValue EndAbsoluteValue Subscript 2 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals 3rd Column StartAbsoluteValue StartAbsoluteValue StartBinomialOrMatrix c 1 minus upper R 1 z 1 Choose c 2 EndBinomialOrMatrix EndAbsoluteValue EndAbsoluteValue Subscript 2 Baseline period EndLayout||y −Xb||2 = ||QT(y −XUU Tb)||2
=
||||
||||
(
c1
c2
)
−
[
R1 0
0 0
] (
z1
z2
)||||
||||
2
=
||||
||||
( c1 −R1z1
c2
)||||
||||
2
.
(5.45) 
The residual norm is minimized for z1 = R−1 
1 c1 and z2 arbitrary. However, if 
z2 = 0,  then ||z||2 is also minimized. Because U T b = z and U is orthogonal,
||-b||2 = ||z||2, and  so ||-b||2 is the minimum among all least squares solutions.

5.6 Overdetermined Systems: Least Squares
289
5.6.4 Weighted Least Squares 
One of the simplest variations on ﬁtting the linear model Xb ≈ y is to allow 
diﬀerent weights on the observations; that is, instead of each row of X and 
corresponding element of y contributing equally to the ﬁt, the elements of X 
and y are possibly weighted diﬀerently. The relative weights can be put into 
an n-vector w and the squared norm in Eq. (5.35) replaced by a quadratic 
form in diag(w). More generally, we form the quadratic form as 
left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline upper W left parenthesis y minus upper X b right parenthesis comma(y −Xb)TW(y −Xb),
(5.46) 
where W is a positive deﬁnite matrix. Because the weights apply to both y 
and Xb, there is no essential diﬀerence in the weighted or unweighted versions 
of the problem. 
The use of the QR factorization for the overdetermined system in which 
the weighted norm (5.46) is to be minimized is similar to the development 
above. It is exactly what we get if we replace y − Xb in Eq. (5.39) or (5.45) 
by WC(y − Xb), where WC is the Cholesky factor of W. 
5.6.5 Updating a Least Squares Solution of an Overdetermined 
System 
In Sect. 5.5 on page 280, we considered the problem of updating a given so-
lution to be a solution to a perturbed consistent system. An overdetermined 
system is often perturbed by adding either some rows or some columns to the 
coeﬃcient matrix X. This corresponds to including additional equations in 
the system, 
StartBinomialOrMatrix upper X Choose upper X Subscript plus Baseline EndBinomialOrMatrix b almost equals StartBinomialOrMatrix y Choose y Subscript plus Baseline EndBinomialOrMatrix comma
[
X
X+
]
b ≈
[
y
y+
]
,
or to adding variables, 
Start 1 By 2 Matrix 1st Row 1st Column upper X 2nd Column upper X Subscript plus Baseline EndMatrix StartBinomialOrMatrix b Choose b Subscript plus Baseline EndBinomialOrMatrix almost equals y period
[ X X+
] [ b
b+
]
≈y.
In either case, if the QR decomposition of X is available, the decomposition 
of the augmented system can be computed readily. Consider, for example, 
the addition of k equations to the original system Xb ≈ y, which has n 
approximate equations. With the QR decomposition, for the original full rank 
system, putting QT X and QT y as partitions in a matrix, we have 
Start 2 By 2 Matrix 1st Row 1st Column upper R 1 2nd Column c 1 2nd Row 1st Column 0 2nd Column c 2 EndMatrix equals upper Q Superscript normal upper T Baseline Start 1 By 2 Matrix 1st Row 1st Column upper X 2nd Column y EndMatrix period
[
R1 c1
0 c2
]
= QT [ X y ]
.
Augmenting this with the additional rows yields 
Start 3 By 2 Matrix 1st Row 1st Column upper R 2nd Column c 1 2nd Row 1st Column 0 2nd Column c 2 3rd Row 1st Column upper X Subscript plus Baseline 2nd Column y Subscript plus Baseline EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column upper Q Superscript normal upper T Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column upper I EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper X 2nd Column y 2nd Row 1st Column upper X Subscript plus Baseline 2nd Column y Subscript plus Baseline EndMatrix period
⎡
⎣
R
c1
0
c2
X+ y+
⎤
⎦=
[
QT 0
0 I
] [ X
y
X+ y+
]
.
(5.47)

290
5 Solution of Linear Systems
All that is required now is to apply orthogonal transformations, such as Givens 
rotations, to the system (5.47) to produce 
Start 2 By 2 Matrix 1st Row 1st Column upper R Subscript asterisk Baseline 2nd Column c Subscript 1 asterisk Baseline 2nd Row 1st Column 0 2nd Column c Subscript 2 asterisk Baseline EndMatrix comma
[
R∗c1∗
0 c2∗
]
,
where R∗ is an m × m upper triangular matrix and c1∗ is an m-vector as 
before but c2∗ is an (n − m + k)-vector. 
The updating is accomplished by applying m rotations to system (5.47) so  
as to zero out the (n+q)th row for q = 1, 2, . . . , k. These operations go through 
an outer loop with p = 1, 2, . . . , n  and an inner loop with  q = 1, 2, . . . , k. The  
operations rotate R through a sequence R(p,q) into R∗, and they rotate X+ 
through a sequence X (p,q) 
+
into 0. At the p, q step, the rotation matrix Qpq 
corresponding to Eq. (4.12) on page 230 has 
cosine theta equals StartFraction upper R Subscript p p Superscript left parenthesis p comma q right parenthesis Baseline Over r EndFractioncos θ = R(p,q)
pp
r
and 
sine theta equals StartFraction left parenthesis upper X Subscript plus Superscript left parenthesis p comma q right parenthesis Baseline right parenthesis Subscript q p Baseline Over r EndFraction commasin θ =
(
X(p,q)
+
)
qp
r
,
where 
r equals StartRoot left parenthesis upper R Subscript p p Superscript left parenthesis p comma q right parenthesis Baseline right parenthesis squared plus left parenthesis left parenthesis upper X Subscript plus Superscript left parenthesis p comma q right parenthesis Baseline right parenthesis Subscript q p Baseline right parenthesis squared EndRoot periodr =
/(
R(p,q)
pp
)2
+
((
X(p,q)
+
)
qp
)2
.
5.7 Other Solutions of Overdetermined Systems 
The basic form of an overdetermined linear system may be written as in 
Eq. (5.32) as  
upper X b almost equals y commaXb ≈y,
where X is n × m and rank(X|y) > m. 
As in Eq. (5.33) in Sect. 5.6.1, we can write this as an equation, 
upper X b equals y minus r commaXb = y −r,
where r is a vector of residuals. Fitting the equation y = Xb means minimizing 
r, that is, minimizing some norm of r. 
There are various norms that may provide a reasonable ﬁt. In Sect. 5.6, we  
considered use of the L2 norm, that is, an ordinary least squares (OLS) ﬁt. 
There are various other ways of approaching the problem, and we will brieﬂy 
consider a few of them in this section. 
As we have stated before, we should not confuse statistical inference with 
ﬁtting equations to data, although the latter task is a component of the former 
activity. Applications in statistical data analysis are discussed in Chap. 9.

5.7 Other Solutions of Overdetermined Systems
291
In those applications, we need to make statements (i.e., assumptions) about 
relevant probability distributions. These probability distributions, together 
with the methods used to collect the data, may indicate speciﬁc methods for 
ﬁtting the equations to the given data. In this section, we continue to address 
the more mechanical aspects of the problem of ﬁtting equations to data. 
5.7.1 Solutions That Minimize Other Norms of the Residuals 
A solution to an inconsistent, overdetermined system 
upper X b almost equals y commaXb ≈y,
where X is n × m and rank(X|y) > m, is some value b that makes y − Xb 
close to zero. We deﬁne “close to zero” in terms of a norm on y − Xb. The  
most common norm, of course, is the L2 norm as in expression (5.34), and 
the minimization of this norm is straightforward, as we have seen. In addition 
to the simple analytic properties of the L2 norm, the least squares solution 
has some desirable statistical properties under fairly common distributional 
assumptions, as we have seen. 
Minimum L1 Norm Fitting: Least Absolute Values 
A common alternative norm is the L1 norm. The minimum L1 norm solution 
is called the least absolute values ﬁt or the LAV ﬁt. It is not as aﬀected by 
outlying observations as the least squares ﬁt is. 
Consider a simple example. Assume we have observations on a response, 
y = (0, 3, 4, 0, 8), and on a single predictor variable, x = (1, 3, 4, 6, 7). We have 
¯y = 3  and  ¯x = 4.2. We write the model equation as 
y almost equals b 0 plus b 1 x periody ≈b0 + b1x.
(5.48) 
The model with the data is 
Start 5 By 1 Matrix 1st Row 0 2nd Row 3 3rd Row 4 4th Row 0 5th Row 8 EndMatrix equals Start 5 By 2 Matrix 1st Row 1st Column 1 2nd Column 1 2nd Row 1st Column 1 2nd Column 3 3rd Row 1st Column 1 2nd Column 4 4th Row 1st Column 1 2nd Column 6 5th Row 1st Column 1 2nd Column 7 EndMatrix b plus r period
⎡
⎢⎢⎢⎢⎣
0
3
4
0
8
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎣
1 1
1 3
1 4
1 6
1 7
⎤
⎥⎥⎥⎥⎦
b + r.
(5.49) 
A least squares solution yields ˆb0 = −0.3158 and ˆb1 = 0.7895. With these 
values, Eq. (5.48) goes through the mean (4.2, 3). The residual vector from 
this ﬁt is orthogonal to 1 (i.e., the sum of the residuals is 0) and to x. 
A solution that minimizes the L1 norm is ˜b0 = −1.333 and ˜b1 = 1.333. 
The LAV ﬁt may not be unique (although it is in this case). We immediately 
note that the least absolute deviations ﬁt does not go through the mean of 
the data nor is the residual vector orthogonal to the 1 vector and to x. The

292
5 Solution of Linear Systems
LAV ﬁt does go through two points in the dataset, however. (This is a special 
case of one of several interesting properties of LAV ﬁts, which we will not 
discuss here. The interested reader is referred to Kennedy and Gentle 1980, 
Chapter 11, for discussion of some of these properties, as well as assumptions 
about probability distributions that result in desirable statistical properties 
for LAV estimators.) A plot of the data, the two ﬁtted lines, and the residuals 
is  shown in Fig. 5.2. 
Figure 5.2. OLS and minimum L1 norm ﬁts 
The problem of minimizing the L1 norm can be formulated as the linear 
programming problem 
StartLayout 1st Row 1st Column min Underscript b Endscripts 2nd Column 1 Superscript normal upper T Baseline left parenthesis e Superscript plus Baseline plus e Superscript minus Baseline right parenthesis 3rd Column Blank 2nd Row 1st Column normal s period normal t period 2nd Column upper X b plus upper I e Superscript plus Baseline minus upper I e Superscript minus Baseline equals y 3rd Row 1st Column Blank 2nd Column e Superscript plus Baseline comma e Superscript minus Baseline greater than or equals 0 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column b normal u normal n normal r normal e normal s normal t normal r normal i normal c normal t normal e normal d comma EndLayoutmin
b
1T(e+ + e−)
s.t. Xb + Ie+ −Ie−= y
e+, e−≥0
(5.50) 
b unrestricted,

5.7 Other Solutions of Overdetermined Systems
293
where e+ and e− are nonnegative n-vectors. There are special algorithms that 
take advantage of the special structure of the problem to speed up the basic 
linear programming simplex algorithm. 
Minimum L∞ Norm Fitting: Minimax 
Another norm that may be useful in some applications is the L∞ norm. A 
solution minimizing that norm is called the least maximum deviation ﬁt. A 
least maximum deviation ﬁt is greatly aﬀected by outlying observations. As 
with the LAV ﬁt, the least maximum deviation ﬁt does not necessarily go 
through the mean of the data. The least maximum deviation ﬁt also may not 
be unique. 
This problem can also be formulated as a linear programming problem, and 
as with the least absolute deviations problem, there are special algorithms that 
take advantage of the special structure of the problem to speed up the basic 
linear programming simplex algorithm. Again, the interested reader is referred 
to Kennedy and Gentle (1980, Chapter 11) for a discussion of some of the 
properties of minimum L∞ norm ﬁts, as well as assumptions about probability 
distributions that result in desirable statistical properties for minimum L∞ 
norm estimators. 
Lp Norms and Iteratively Reweighted Least Squares 
More general Lp norms may also be of interest. For 1 < p <  2, if no element 
of y − Xb is zero, we can formulate the pth power of the norm as 
parallel to y minus upper X b parallel to Subscript p Superscript p Baseline equals left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline upper W left parenthesis y minus upper X b right parenthesis comma||y −Xb||p
p = (y −Xb)TW(y −Xb),
(5.51) 
where 
upper W equals normal d normal i normal a normal g left parenthesis StartAbsoluteValue y Subscript i Baseline minus x Subscript i Superscript normal upper T Baseline b EndAbsoluteValue Superscript 2 minus p Baseline right parenthesisW = diag(| yi −xT
i b|2−p)
(5.52) 
and xiT is the  ith row of X. The formulation (5.51) leads to the iteratively 
reweighted least squares (IRLS) algorithm, in which a sequence of weights 
W (k) and weighted least squares solutions b(k) are formed as in Algorithm 5.4. 
Algorithm 5.4 Iteratively Reweighted Least Squares 
0. Input a threshold for a zero residual, e1 (which may be data dependent), 
and a large value for weighting zero residuals, wbig. Input stopping criteria,
e2 and kmax. 
Set k = 0. Choose an initial value b(k) , perhaps as the OLS solution. 
1. Compute the diagonal matrix W (k) : w (k) 
i 
= |yi − xT 
i b(k)|2−p , except, if 
|yi − xT 
i b(k)| < e1, set  w (k) 
i 
= wbig. 
2. Compute b(k+1) by solving the weighted least squares problem: minimize 
(y − Xb)T W (k) (y − Xb).

294
5 Solution of Linear Systems
3. If ||b(k+1) −b(k)|| ≤e2, set  b = b(k+1) and terminate. 
4. If k <  kmax, 
set k = k +  1  and go to step  1; 
otherwise, 
issue message that 
“algorithm did not converge in kmax iterations.” 
Compute b(1) by minimizing Eq. (5.51) with  W = W (0) ; then compute W (1) 
from b(1) , and iterate in this fashion. This method is easy to implement and 
will generally work fairly well, except for the problem of zero (or small) resid-
uals. The most eﬀective way of dealing with zero residuals is to set them to 
some large value. 
Algorithm 5.4 will work for LAV ﬁtting, although the algorithms based on 
linear programming alluded to above are better for this task. As mentioned 
above, LAV ﬁts generally go through some observations; that is, they ﬁt them 
exactly, yielding zero residuals. This means that in using Algorithm 5.4, the  
manner of dealing with zero residuals may become an important aspect of the 
eﬃciency of the algorithm. 
5.7.2 Regularized Solutions 
Overdetermined systems often arise because of a belief that some response y 
is linearly related to some other set of variables. This relation is expressed in 
the system 
y almost equals upper X b periody ≈Xb.
The fact that y /= Xb for any b results because the relationship is not exact. 
There is perhaps some error in the measurements. It is also possible that there 
is some other variable not included in the columns of X. In addition, there 
may be some underlying randomness that could never be accounted for. 
In any application in which we ﬁt an overdetermined system, it is likely 
that the given values of X and y are only a sample (not necessarily a random 
sample) from some universe of interest. Whatever value of b provides the best 
ﬁt (in terms of the criterion chosen) may not provide the best ﬁt if some other 
equally valid values of X and y were used. The given dataset is ﬁt optimally, 
but the underlying phenomenon of interest may not be modeled very well. 
The given dataset may suggest relationships among the variables that are not 
present in the larger universe of interest. Some element of the “true” b may 
be zero, but in the best ﬁt for a given dataset, the value of that element may 
be signiﬁcantly diﬀerent from zero. Deciding on the evidence provided by a 
given dataset that there is a relationship among certain variables when indeed 
there is no relationship in the broader universe is an example of overﬁtting. 
There are various approaches we may take to avoid overﬁtting, but there 
is no panacea. The problem is inherent in the process. 
One approach to overﬁtting is regularization. In this technique, we restrain 
the values of b in some way. Minimizing ||y − Xb|| may yield a b with large

5.7 Other Solutions of Overdetermined Systems
295
elements or values that are likely to vary widely from one dataset to another. 
One way of “regularizing” the solution is to minimize also some norm of b. 
The general formulation of the problem then is 
min Underscript b Endscripts left parenthesis parallel to y minus upper X b parallel to Subscript r Baseline plus lamda parallel to b parallel to Subscript b Baseline right parenthesis comma min
b (||y −Xb||r + λ||b||b),
(5.53) 
where λ is some appropriately chosen nonnegative number. The norm on the 
residuals, || · ||r, and that on the solution vector b, || · ||b, are often chosen 
to be the same, and, of course, most often, they are chosen as the L2 norm. 
If both norms are the L2 norm, the ﬁtting is called Tikhonov regularization. 
In statistical applications, this leads to “ridge regression.” If || · ||r is the L2 
norm and || · ||b is the L1 norm, the statistical method is called the “lasso.” 
We discuss these formulations brieﬂy in Sect. 9.4.4. 
As an example, let us consider the data in Eq. (5.49) for the equation 
y equals b 0 plus b 1 x periody = b0 + b1x.
We found the least squares solution to be ˆb0 = −0.3158 and ˆb1 = 0.7895, 
which ﬁts the means and has a residual vector that is orthogonal to 1 and to 
x. Now  let us regularize the  least squares ﬁt with an L2 norm on b and with 
λ =  5. (The choice of  λ depends on the scaling of the data and a number of 
other things we will not consider here. Typically, in an application, various 
values of λ are considered.) Again, we face the question of treating b0 and b1 
diﬀerently. The regularization, which is a shrinkage, can be applied to both 
or just to b1. Furthermore, we have the question of whether we want to force 
the equation to ﬁt some data point exactly. In statistical applications, it is 
common not to apply the shrinkage to the intercept term and to force the 
ﬁtted equation to ﬁt the means exactly. Doing that, we get ˆb1λ = 0.6857, 
which is shrunken from the value of ˆb1, and  ˆb0λ = 0.1200, which is chosen 
so as to ﬁt the mean. A plot of the data and the two ﬁtted lines is shown in 
Fig. 5.3. 
5.7.3 Other Restrictions on the Solutions 
Given the basic problem in an overdetermined system, that is, to determine 
an optimal b so that 
upper X b almost equals y commaXb ≈y,
for given X and y, we realize that there may be various properties that make 
one value of b better than another, even when the basic criterion is minimiza-
tion of some norm. We have mentioned two types of regularization under a 
general least squares criterion. 
We might consider other restrictions using a basic least squares criterion. 
One restriction of interest may be to determine a value of b that minimizes 
the L2 norm of the residuals subject to b ≥ 0. This is called “nonnegative least 
squares.” An eﬃcient iterative algorithm to determine the restricted value of

296
5 Solution of Linear Systems
Figure 5.3. OLS and L2 norm regularized minimum L2 norm ﬁts 
b was given by Lawson and Hanson ( 1974, 1995) and is implemented in the R 
function nnls. 
We will consider other inequality-constrained least squares problems in 
Chap. 9, beginning on page 507. 
5.7.4 Minimizing Orthogonal Distances 
In writing the equation Xb = y + r in place of the overdetermined linear 
system Xb ≈ y, we are allowing adjustments to y so as to get an equation. 
Another way of making an equation out of the overdetermined linear system 
Xb ≈ y is to write it as 
left parenthesis upper X plus upper E right parenthesis b equals y plus r comma(X + E)b = y + r,
(5.54) 
that is, to allow adjustments to both X and y. Both  X and E are in IRn×m 
(and we assume n > m). 
In ﬁtting the linear model only with adjustments to y, we determine b so 
as to minimize some norm of r. Likewise, with adjustments to both X and 
y, we seek b so as to minimize some norm of the matrix E and the vector r. 
There are obviously several ways to approach this. We could take norms of 
E and r separately and consider some weighted combination of the norms.

5.7 Other Solutions of Overdetermined Systems
297
Another way is to adjoin r to E and minimize some norm of the n × (m + 1)  
matrix [E|r]. 
A common approach is to minimize ||[E|r]||F. This, of course, is the sum 
of squares of all elements in [E|r]. The method is therefore sometimes called 
“total least squares.” 
If it exists, the minimum of ||[E|r]||F is achieved at 
b equals minus v Subscript 2 asterisk Baseline divided by v 22 commab = −v2∗/v22,
(5.55) 
where 
left bracket upper X vertical bar y right bracket equals upper U upper D upper V Superscript normal upper T[X|y] = UDV T
(5.56) 
is the singular value decomposition (see Eq. (3.299) on page 183), and V is 
partitioned as 
upper V equals Start 2 By 2 Matrix 1st Row 1st Column upper V 11 2nd Column v Subscript asterisk 2 Baseline 2nd Row 1st Column v Subscript 2 asterisk Baseline 2nd Column v 22 EndMatrix periodV =
[ V11 v∗2
v2∗v22
]
.
If E has some special structure, the problem of minimizing the orthogonal 
residuals may not have a solution. Golub and Van Loan (1980) show that a 
suﬃcient condition for a solution to exist is that dm > dm+1. (Recall that the 
ds in the SVD are nonnegative and they are indexed so as to be nonincreasing. 
If dm = dm+1, a solution may or may not exist.) 
Again, as an example, let us consider the data in Eq. (5.49) for the equation 
y equals b 0 plus b 1 x periody = b0 + b1x.
We found the least squares solution to be ˆb0 = −0.3158 and ˆb1 = 0.7895, 
which ﬁts the mean and has a residual vector that is orthogonal to 1 and to 
x. Now we determine a ﬁt so that the L2 norm of the orthogonal residuals is 
minimized. Again, we will force the equation to ﬁt the mean exactly. We get 
ˆb0orth = −4.347 and ˆb0orth = 1.749. A plot of the data, the two ﬁtted lines, 
and the  residuals  is  shown in Fig. 5.4. 
The orthogonal residuals can be weighted in the usual way by premulti-
plication by a Cholesky factor of a weight matrix, as discussed on page 289. 
If some norm other than the L2 norm is to be minimized, an iterative 
approach must be used. Ammann and Van Ness (1988) describe an iterative 
method that is applicable to any norm, so long as a method is available to 
compute a value of b that minimizes the norm of the usual vertical distances 
in a model such as Eq. (9.6). The method is simple. We ﬁrst ﬁt y = Xb, 
minimizing the vertical distances in the usual way; we then rotate y into ˜y 
and X into -
X, so that the ﬁtted plane is horizontal. Next, we ﬁt ˜y = -
Xb and 
repeat. After continuing this way until the ﬁts in the rotated spaces do not 
change from step to step, we adjust the ﬁtted b back to the original unrotated 
space. Because of these rotations, if we assume that the model ﬁts some point 
exactly, we must adjust y and X accordingly (see the discussion on page 283). 
In the following, we assume that the model ﬁts the means exactly, so we center 
the data. We let m be the number of columns in the centered data matrix.

298
5 Solution of Linear Systems
6 
Figure 5.4. OLS and minimum orthogonal L2 norm ﬁts 
(The centered matrix does not contain a column of 1s. If the formulation of 
the model y = Xb includes an intercept term, then X is n × (m + 1)  with  a  
column of 1s.) 
Algorithm 5.5 Iterative Orthogonal Residual Fitting 
Through the Means 
0. Input stopping criteria, e and kmax. 
Set k = 1,  y (0) 
c 
= yc, X (0) 
c 
= Xc, and  D(0) = Im+1. 
1. Determine a value b (k) 
c 
that minimizes the norm of
(
y (k−1) 
c 
− X (k−1) 
c
b (k) 
c
)
. 
2. If b (k) 
c 
≤e, go to step  7. 
3. Determine a rotation matrix Q(k) that makes the kth ﬁt horizontal. 
4. Transform the matrix
[
y (k−1) 
c 
|X (k−1) 
c
] [
y (k) 
c |X (k) 
c
]
by a rotation matrix: 
left bracket y Subscript normal c Superscript left parenthesis k right parenthesis Baseline vertical bar upper X Subscript normal c Superscript left parenthesis k right parenthesis Baseline right bracket equals left bracket y Subscript normal c Superscript left parenthesis k minus 1 right parenthesis Baseline vertical bar upper X Subscript normal c Superscript left parenthesis k minus 1 right parenthesis Baseline right bracket upper Q Superscript left parenthesis k right parenthesis Baseline period
[
y(k)
c |X(k)
c
]
=
[
y(k−1)
c
|X(k−1)
c
]
Q(k).
5. Transform D(k−1) by the same rotation: D(k) = D(k−1) Q(k) .

5.7 Other Solutions of Overdetermined Systems
299
6. If k <  kmax, 
set k = k +  1  and go to step  1; 
otherwise, 
issue message that 
“algorithm did not converge in kmax iterations.” 
7. For j = 2, . . . , m, choose  bj = dj,m+1/dm+1,m+1. (So long as the rotations 
have not produced a vertical plane in the unrotated space, dm+1,m+1 will 
not be zero.) 
8. Compute b1 = ¯y −Ek 
j=2 bj ∗¯xj (where ¯xj is the mean of the jth column 
of the original uncentered X). 
An appropriate rotation matrix for Algorithm 5.5 is Q in the QR decomposi-
tion of 
Start 3 By 2 Matrix 1st Row 1st Column upper I Subscript m Baseline 2nd Column 0 2nd Row 1st Column Blank 3rd Row 1st Column left parenthesis b Superscript left parenthesis k right parenthesis Baseline right parenthesis Superscript normal upper T Baseline 2nd Column 1 EndMatrix period
⎡
⎣
Im
0
(b(k))T 1
⎤
⎦.
Note that forcing the ﬁt to go through the means, as we do in Algo-
rithm 5.5, is not usually done for norms other than the L2 norm (see Fig. 5.2). 
Appendix: R Functions for Solving Linear Systems 
In the appendix to Chap. 3 beginning on page 201, we discussed general capa-
bilities in R for working with matrices, and in Table 4.2 on page 251, we listed 
some R functions for operations on matrices. R also has several functions for 
solving linear systems, as shown in Table 5.1. These functions use the factor-
ization functions from Table 4.2. The most basic function for working with 
linear systems is solve. There  is  a  solve function in the base package, but 
there are also versions of the solve function in the Matrix and matlib pack-
ages that have more options. The solve function can also be used to compute 
the inverse or pseudoinverse of a matrix, although, as we have mentioned, in 
applications we rarely compute a matrix inverse. 
The functions for ﬁtting overdetermined systems, lsfit, l1fit, and  nnls, 
all require the input to be of the form (A,b,. . . ). The designs of these functions 
are not particularly oriented toward statistical applications. In Chap. 9, we will 
consider another R function, lm, that ﬁts a linear model using a least squares 
criterion. The function lm allows the linear model to be speciﬁed in a more 
general way, and the function computes additional statistics that can be used 
in making inferences about the model. 
The solve and lsfit functions allow multiple left-hand sides. Only 
lsfit handles missing values. (It performs casewise deletion.) The solve 
function produces output of class matrix, and the other functions produce 
output of class list. The lists contain the components one would expect 
when ﬁtting an overdetermined system. The components have names such

300
5 Solution of Linear Systems
Table 5.1. Some R functions for solving linear systems 
solve
Solve system of linear equations with square, nonsingular 
coeﬃcient matrix, 
or compute matrix inverse 
(More options in Matrix package and matlib package) 
lsfit
Ordinary or weighted least squares ﬁt 
l1fit {L1pack} Least absolute values ﬁt 
nnls {nnls}
Nonnegative least squares ﬁt 
as "coefficients", "residuals", "intercept", and  "qr". The components 
can be accessed in the usual way with the $ list extractor; see page 251. 
Exercises 
5.1. Let A be nonsingular, and let κ(A) = ||A|| ||A−1||. 
a) Prove Eq. (5.10): 
kappa 2 left parenthesis upper A right parenthesis equals StartStartFraction max Underscript x not equals 0 Endscripts StartFraction parallel to upper A x parallel to Over parallel to x parallel to EndFraction OverOver min Underscript x not equals 0 Endscripts StartFraction parallel to upper A x parallel to Over parallel to x parallel to EndFraction EndEndFraction periodκ2(A) =
maxx/=0
||Ax||
||x||
minx/=0
||Ax||
||x||
.
b) Using the relationship above, explain heuristically why κ(A) is called 
the “condition number” of A. 
5.2. Consider the system of linear equations 
StartLayout 1st Row 1st Column x 1 2nd Column plus 3rd Column 4 x 2 4th Column plus 5th Column x 3 6th Column equals 7th Column 12 comma 2nd Row 1st Column 2 x 1 2nd Column plus 3rd Column 5 x 2 4th Column plus 5th Column 3 x 3 6th Column equals 7th Column 19 comma 3rd Row 1st Column x 1 2nd Column plus 3rd Column 2 x 2 4th Column plus 5th Column 2 x 3 6th Column equals 7th Column 9 period EndLayout
x1 + 4x2 + x3 = 12,
2x1 + 5x2 + 3x3 = 19,
x1 + 2x2 + 2x3 =
9.
(5.57) 
a) Solve the system using Gaussian elimination with partial pivoting. 
b) Solve the system using Gaussian elimination with complete pivoting. 
c) Determine the D, L, and  U matrices of the Gauss-Seidel method 
(Eq. (5.19), page 274), and determine the spectral radius of 
left parenthesis upper D plus upper L right parenthesis Superscript negative 1 Baseline upper U period(D + L)−1U.
d) Do three steps of the Gauss-Seidel method using the original formu-
lation (5.57), and starting with x(0) = (1, 1, 1), and evaluate the L2 
norm of the diﬀerence of two successive approximate solutions. 
e) Now do one partial pivot on the system (5.57) to form the coeﬃcient 
matrix 
upper A overTilde equals Start 3 By 3 Matrix 1st Row 1st Column 2 2nd Column 5 3rd Column 3 2nd Row 1st Column 1 2nd Column 4 3rd Column 1 3rd Row 1st Column 1 2nd Column 2 3rd Column 2 EndMatrix period-A =
⎡
⎣
2 5 3
1 4 1
1 2 2
⎤
⎦.
Write the corresponding -D, -L, and -U, and determine ρ(( -D+ -L)−1 -U).

Exercises
301
f) Do three steps of the Gauss-Seidel method using the one-pivot formu-
lation of Exercise 5.2e, and starting with x(0) = (1, 1, 1), and evaluate 
the L2 norm of the diﬀerence of two successive approximate solutions. 
g) Do three steps of the Gauss-Seidel method with successive overre-
laxation using the one-pivot formulation of Exercise 5.2e and with 
ω = 0.1, starting with x(0) = (1, 1, 1), and evaluate the L2 norm of 
the diﬀerence of two successive approximate solutions. 
h) Do three steps of the conjugate gradient method using the original 
formulation (5.57), starting with x(0) = (1, 1, 1), and evaluate the L2 
norm of the diﬀerence of two successive approximate solutions. 
5.3. On page 287, we derived the least squares solution, -b = X+ y, to the  
overdetermined system Xb = y by use of the QR decomposition of X. 
This equation for the least squares solution can also be shown to be 
correct in other ways. 
a) For simplicity in this part, let us assume that X is of full rank; 
that is, we will write -b as the solution to the normal equations,
-b = (XT X)−1 XT y. (This assumption is not necessary, but the proof 
without it is much messier.) 
Show that -b = X+ y by  showing that (XT X)−1 XT = X+ . 
b) On page 285, we showed that orthogonality of the residuals charac-
terizes a least squares solution. Show that -b = X+ y is a least squares 
solution by showing that in this case XT (y − X-b) = 0.  
5.4. Verify Eq. (5.51). 
R Exercises 
These exercises are intended to illustrate R functions and commands, rather 
than to follow eﬃcient or proper computational methods. 
5.5. Write an R function to implement the iteratively reweighted least squares 
Algorithm 5.4 for an L1 ﬁt. Use the arguments of the form (A,B,. . . ),  as  
in the R functions lsfit, l1fit, and  nnls. For residuals smaller than
e1, set the residual to 0. (This is counterintuitive.) 
Make up a test problem, and compare the solution using your R function 
with the solution from l1fit. 
5.6. Consider the small balanced two-binary-factor classiﬁcation model with-
out interaction with eight observations. The design matrix X is 
StartLayout 1st Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 2nd Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 3rd Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 0 5th Column 1 4th Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 0 5th Column 1 5th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 1 5th Column 0 6th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 1 5th Column 0 7th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 8th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 EndLayout
1 1 0 1 0
1 1 0 1 0
1 1 0 0 1
1 1 0 0 1
1 0 1 1 0
1 0 1 1 0
1 0 1 0 1
1 0 1 0 1

302
5 Solution of Linear Systems
Set the seed to 12345 (set.seed(12345)), and use rnorm to generate 
eight random responses as Xα + e, where  α = (2, 3, 1, 1, 4) and where ei 
are iid as N(0, 1) (see page 13). 
Now, compute -αMP using the Moore-Penrose inverse of X (use ginv), 
and compute -αg12 using a g12 inverse that is not Moore-Penrose inverse 
(use Ginv in the matlib package). 
Compute and compare the Euclidean lengths of -αMP and -αg12 . 
5.7. Write an R function lsfitortho to implement Algorithm 5.5 for the 
least squares of orthogonal residuals. Use argument of the form as in the 
R function l1fit, which is similar to those in lsfit and nnls, but does 
not allow multiple left-hand sides and does not allow missing values. 
Use the function deﬁnition with an initialized argument 
lsfitortho <- function(x, y, intercept = TRUE){

6 
Evaluation of Eigenvalues and Eigenvectors 
Before we discuss methods for computing eigenvalues, we recall a remark made 
in Chap. 4. A given n normal t normal hnth-degree polynomial p left parenthesis c right parenthesisp(c) is the characteristic polynomial 
of some matrix. The companion matrix of Eq. (3.247) is one such matrix. Thus, 
given a general polynomial p, we can form a matrix A whose eigenvalues 
are the roots of the polynomial; and likewise, given a square matrix, we can 
write a polynomial in its eigenvalues. It is a well-known fact in the theory 
of equations that there is no general formula for the roots of a polynomial 
of degree greater than 4. This means that we cannot expect to have a direct 
method for calculating eigenvalues of any given matrix. 
The eigenvalues of some matrices, of course, can be evaluated directly. 
The eigenvalues of a diagonal matrix, for example, are merely the diagonal 
elements. In that case, the characteristic polynomial is of the factored form 
product left parenthesis a Subscript i i Baseline minus c right parenthesis| |(aii −c), whose roots are immediately obtainable. For general eigenvalue 
computations, however, we must use an iterative method. 
In statistical applications, the matrices whose eigenvalues are of interest 
are often symmetric. Symmetric matrices are diagonalizable and have only 
real eigenvalues. (As usual, we will assume the matrices themselves are real.) 
The problem of determining the eigenvalues of a symmetric matrix therefore 
is simpler than the corresponding problem for a general matrix. In many 
statistical applications, the symmetric matrices of interest are nonnegative 
deﬁnite, and this can allow use of simpler methods for computing eigenvalues 
and eigenvectors. In addition, nonsymmetric matrices of interest in statistical 
applications are often irreducible nonnegative matrices, and computations for 
eigenvalues and eigenvectors for matrices of this type are also often simpler. 
(We will discuss such matrices in Sect. 8.7.2.) 
In this chapter, we describe various methods for computing eigenvalues. A 
given method may have some desirable property for particular applications, 
and in some cases, the methods may be used in combination. Some of the 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 6 
303

304
6 Evaluation of Eigenvalues
methods rely on sequences that converge to a particular eigenvalue or eigen-
vector. The power method, discussed in Sect. 6.2, is of this type; one eigenpair 
at a time is computed. Other methods are based on sequences of orthogonally 
similar matrices that converge to a diagonal matrix. An example of such a 
method is called the LR method. This method, which we will not consider in 
detail, is based on a factorization of A into left and right factors, upper F Subscript normal upper LFL and upper F Subscript normal upper RFR, 
and the fact that if c is an eigenvalue of upper F Subscript normal upper L Baseline upper F Subscript normal upper RFLFR, then it is also an eigenvalue 
of upper F Subscript normal upper R Baseline upper F Subscript normal upper LFRFL (property 8, page 158). If upper A equals upper L Superscript left parenthesis 0 right parenthesis Baseline upper U Superscript left parenthesis 0 right parenthesisA = L(0)U (0) is an LU decomposition of A 
with 1s on the diagonal of either upper L Superscript left parenthesis 0 right parenthesisL(0) or upper U Superscript left parenthesis 0 right parenthesisU (0), iterations of LU decompositions 
of the similar matrices 
upper L Superscript left parenthesis k plus 1 right parenthesis Baseline upper U Superscript left parenthesis k plus 1 right parenthesis Baseline equals upper U Superscript left parenthesis k right parenthesis Baseline upper L Superscript left parenthesis k right parenthesis Baseline commaL(k+1)U (k+1) = U (k)L(k),
under some conditions, will converge to a similar diagonal matrix. The suﬃ-
cient conditions for convergence include nonnegative deﬁniteness. 
6.1 General Computational Methods 
For whatever approach is taken for ﬁnding eigenpairs, there are some general 
methods that may speed up the process or that may help in achieving higher 
numerical accuracy. Before describing some of the techniques, we consider a 
bound on the sensitivity of eigenvalues to perturbations of the matrix. 
6.1.1 Numerical Condition of an Eigenvalue Problem 
The upper bounds on the largest eigenvalue, given in inequalities (3.258) 
and (3.259) on page 165, provide a simple indication of the region in the 
complex plane in which the eigenvalues lie. 
The Gershgorin disks (inequalities (3.262) and  (3.263), page 167) provide 
additional information about the regions of the complex plane in which the 
eigenvalues lie. The Gershgorin disks can be extended to deﬁne separate re-
gions that contain eigenvalues, but we will not consider those reﬁnements here. 
The spectral radius and/or Gershgorin disks can be used to obtain approxi-
mate values to use in some iterative approximation methods; see Eq. (6.13), 
for example. 
In any computational problem, it is of interest to know what is the eﬀect 
on the solution when there are small changes in the problem itself. This leads 
to the concept of a condition number, as we discussed in Sect. 5.1.1 beginning 
on page 261. The objective is to quantify or at least determine bounds on the 
rate of change in the “output” relative to changes in the “input.” 
In the eigenvalue problem, we begin with a square matrix A. We as-
sume that A is diagonalizable. (All symmetric matrices are diagonalizable, 
and Eq. (3.272) on page 172 gives necessary and suﬃcient conditions which 
many other matrices encountered in statistical applications also satisfy.) We 
form upper V Superscript negative 1 Baseline upper A upper V equals upper C equals normal d normal i normal a normal g left parenthesis left parenthesis c 1 comma ellipsis comma c Subscript n Baseline right parenthesis right parenthesisV −1AV = C = diag((c1, . . . , cn)), where  the  c Subscript ici are the eigenvalues of A.

6.1 General Computational Methods
305
The approach, as in Sect. 5.1.1, is to perturb the problem slightly by adding 
a small amount delta upper AδA to A. Let  upper A overTilde equals upper A plus delta upper A -A = A+δA. (Notice that delta upper AδA does not necessarily 
represent a scalar multiple of the matrix.) 
If A is well-conditioned for the eigenvalue problem, then if parallel to delta upper A parallel to||δA|| is small 
relative to parallel to upper A parallel to||A||, the diﬀerences in the eigenvalues of A and of upper A overTilde-A are likewise 
small. Let d be any eigenvalue of upper A overTilde-A that is not an eigenvalue of A. (If  all  
eigenvalues of upper A overTilde-A are eigenvalues of A, then the perturbation has had no eﬀect, 
and the question we are addressing is not of interest.) Our interest will be in 
min Underscript c element of sigma left parenthesis upper A right parenthesis Endscripts StartAbsoluteValue c minus d EndAbsoluteValue period min
c∈σ(A) |c −d|.
If d is an eigenvalue of upper A overTilde-A, then  upper A plus delta upper A minus d upper IA + δA −dI is singular, and so upper V Superscript negative 1 Baseline left parenthesis upper A plus delta upper A minus d upper I right parenthesis upper VV −1(A +
δA −dI)V is also singular. Simplifying this latter expression, we have that 
upper C minus d upper I plus upper V Superscript negative 1 Baseline delta upper A upper VC −dI + V −1δAV is singular. Since d is not an eigenvalue of A, however, 
upper C minus d upper IC −dI must be nonsingular, and so left parenthesis upper C minus d upper I right parenthesis Superscript negative 1(C −dI)−1 exists. Multiplying the two 
expressions, we have that upper I plus left parenthesis upper C minus d upper I right parenthesis Superscript negative 1 Baseline upper V Superscript negative 1 Baseline delta upper A upper VI + (C −dI)−1V −1δAV is also singular; hence, negative 1−1
is an eigenvalue of left parenthesis upper C minus d upper I right parenthesis Superscript negative 1 Baseline upper V Superscript negative 1 Baseline delta upper A upper V(C −dI)−1V −1δAV , and so by property 16 on page 163, 
we have 
1 less than or equals parallel to left parenthesis upper C minus d upper I right parenthesis Superscript negative 1 Baseline upper V Superscript negative 1 Baseline delta upper A upper V parallel to comma1 ≤||(C −dI)−1V −1δAV ||,
for any consistent norm. (Recall that all matrix norms are consistent in my 
deﬁnition.) Furthermore, again using the consistency property multiple times, 
parallel to left parenthesis upper C minus d upper I right parenthesis Superscript negative 1 Baseline upper V Superscript negative 1 Baseline delta upper A upper V parallel to less than or equals parallel to left parenthesis upper C minus d upper I right parenthesis Superscript negative 1 Baseline parallel to parallel to upper V Superscript negative 1 Baseline parallel to parallel to delta upper A parallel to parallel to upper V parallel to period||(C −dI)−1V −1δAV || ≤||(C −dI)−1|| ||V −1|| ||δA|| ||V ||.
In Eq. (5.7) on page 263, we deﬁned “the” condition number for a nonsingular 
matrix V as kappa left parenthesis upper V right parenthesis equals parallel to upper V parallel to parallel to upper V Superscript negative 1 Baseline parallel toκ(V ) = ||V || ||V −1||. Now, since  upper C minus d upper IC −dI is a diagonal matrix, we 
can rewrite the two inequalities above as 
min Underscript c element of sigma left parenthesis upper A right parenthesis Endscripts StartAbsoluteValue c minus d EndAbsoluteValue less than or equals kappa left parenthesis upper V right parenthesis parallel to delta upper A parallel to semicolon min
c∈σ(A) |c −d| ≤κ(V )||δA||;
(6.1) 
that is, the eigenvalues of the perturbed matrix are within given bounds from 
the eigenvalues of the original matrix. 
This fact is called the Bauer-Fike theorem, and it has several variations 
and ramiﬁcations. It is closely related to Gershgorin disks. Our interest here is 
just to provide a perturbation bound that conveniently relates to the condition 
number of the diagonalizing matrix. 
If A is symmetric, it is orthogonally diagonalizable, and the V above is an 
orthogonal matrix. Hence, if A is a symmetric matrix, upper A overTilde equals upper A plus delta upper A -A = A + δA, and  d is 
an eigenvalue of upper A overTilde equals upper A plus delta upper A -A = A + δA, then  
min Underscript c element of sigma left parenthesis upper A right parenthesis Endscripts StartAbsoluteValue c minus d EndAbsoluteValue less than or equals parallel to delta upper A parallel to period min
c∈σ(A) |c −d| ≤||δA||.

306
6 Evaluation of Eigenvalues
6.1.2 Eigenvalues from Eigenvectors and Vice Versa 
Some methods for eigenanalysis yield the eigenvalues, and other methods yield 
the eigenvectors. Given one member of an eigenpair, we usually want to ﬁnd 
the other member. 
If we are given an eigenvector v of the matrix A, there must be some 
element v Subscript jvj that is not zero. For any nonzero element of the eigenvector, the 
eigenvalue corresponding to v is 
left parenthesis upper A v right parenthesis Subscript j Baseline divided by v Subscript j Baseline period(Av)j/vj.
(6.2) 
Likewise, if the eigenvalue c is known, a corresponding eigenvector is any 
solution to the singular system 
left parenthesis upper A minus c upper I right parenthesis v equals 0 period(A −cI)v = 0.
(6.3) 
(It is relevant to note that the system is singular because many standard 
software packages will refuse to solve singular systems whether or not they 
are consistent!) 
An eigenvector associated with the eigenvalue c can be found using 
Eq. (6.3) if we know the position of any nonzero element in the vector. Sup-
pose, for example, it is known that v 1 not equals 0v1 /= 0. We can  set  v 1 equals 1v1 = 1 and form 
another system to solve for the remaining elements of v by writing 
Start 2 By 2 Matrix 1st Row 1st Column a 11 minus 1 2nd Column a 1 Superscript normal upper T Baseline 2nd Row 1st Column a 2 2nd Column upper A 22 minus c upper I Subscript n minus 1 Baseline EndMatrix StartBinomialOrMatrix 1 Choose v 2 EndBinomialOrMatrix equals StartBinomialOrMatrix 0 Choose 0 EndBinomialOrMatrix comma
[
a11 −1
aT
1
a2
A22 −cIn−1
] [ 1
v2
]
=
[ 0
0
]
,
(6.4) 
where v 2v2 is an left parenthesis n minus 1 right parenthesis(n −1)-vector and a 1 Superscript normal upper TaT
1 and a 2a2 are the remaining elements in 
the ﬁrst row and ﬁrst column, respectively, of A. Rearranging this, we get the 
left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1) system 
left parenthesis upper A 22 minus c upper I Subscript n minus 1 Baseline right parenthesis v 2 equals minus a 2 period(A22 −cIn−1)v2 = −a2.
(6.5) 
The locations of any zero elements in the eigenvector are critical for using 
this method. To form a system as in Eq. (6.4), the position of some nonzero 
element must be known. Another problem in using this method arises when 
the geometric multiplicity of the eigenvalue is greater than 1. In that case, 
the system in Eq. (6.5) is also singular, and the process must be repeated to 
form an left parenthesis n minus 2 right parenthesis times left parenthesis n minus 2 right parenthesis(n −2) × (n −2) system. If the multiplicity of the eigenvalue is k, 
the ﬁrst full rank system encountered while continuing in this way is the one 
that is left parenthesis n minus k right parenthesis times left parenthesis n minus k right parenthesis(n −k) × (n −k). 
6.1.3 Deﬂation 
Whenever an eigenvalue together with its associated left and right eigenvectors 
for a real matrix A is available, another matrix can be formed for which all 
the other nonzero eigenvalues and corresponding eigenvectors are the same

6.1 General Computational Methods
307
as for A. (Of course, the left and right eigenvalues for many matrices are the 
same.) 
Suppose c Subscript ici is an eigenvalue of A with associated right and left eigenvectors 
v Subscript ivi and w Subscript iwi, respectively. Now, suppose that c Subscript jcj is a nonzero eigenvalue of A 
such that c Subscript j Baseline not equals c Subscript icj /= ci. Let  v Subscript jvj and w Subscript jwj be, respectively, right and left eigenvectors 
associated with c Subscript jcj. Now,  
left angle bracket upper A v Subscript i Baseline comma w Subscript j Baseline right angle bracket equals left angle bracket c Subscript i Baseline v Subscript i Baseline comma w Subscript j Baseline right angle bracket equals c Subscript i Baseline left angle bracket v Subscript i Baseline comma w Subscript j Baseline right angle bracket comma<Avi, wj> = <civi, wj> = ci<vi, wj>,
but also 
left angle bracket upper A v Subscript i Baseline comma w Subscript j Baseline right angle bracket equals left angle bracket v Subscript i Baseline comma upper A Superscript normal upper T Baseline w Subscript j Baseline right angle bracket equals left angle bracket v Subscript i Baseline comma c Subscript j Baseline w Subscript j Baseline right angle bracket equals c Subscript j Baseline left angle bracket v Subscript i Baseline comma w Subscript j Baseline right angle bracket period<Avi, wj> = <vi, ATwj> = <vi, cjwj> = cj<vi, wj>.
But if 
c Subscript i Baseline left angle bracket v Subscript i Baseline comma w Subscript j Baseline right angle bracket equals c Subscript j Baseline left angle bracket v Subscript i Baseline comma w Subscript j Baseline right angle bracketci<vi, wj> = cj<vi, wj>
and c Subscript j Baseline not equals c Subscript icj /= ci, then  left angle bracket v Subscript i Baseline comma w Subscript j Baseline right angle bracket equals 0<vi, wj> = 0. Consider the matrix 
upper B equals upper A minus c Subscript i Baseline v Subscript i Baseline w Subscript i Superscript normal upper H Baseline periodB = A −civiwH
i .
(6.6) 
We see that 
StartLayout 1st Row 1st Column upper B w Subscript j 2nd Column equals 3rd Column upper A w Subscript j minus c Subscript i Baseline v Subscript i Baseline w Subscript i Superscript normal upper H Baseline w Subscript j 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper A w Subscript j 3rd Row 1st Column Blank 2nd Column equals 3rd Column c Subscript j Baseline w Subscript j Baseline comma EndLayoutBwj = Awj −civiwH
i wj
= Awj
= cjwj,
so c Subscript jcj and w Subscript jwj are, respectively, an eigenvalue and an eigenvector of B. 
The matrix B has some of the ﬂavor of the sum of some terms in a spectral 
decomposition of A. (Recall that the spectral decomposition is guaranteed to 
exist only for matrices with certain properties. In Chap. 3, we stated the exis-
tence for diagonalizable matrices but derived it only for symmetric matrices.) 
The ideas above lead to a useful method for ﬁnding eigenpairs of a diag-
onalizable matrix. (The method also works if we begin with a simple eigen-
value.) We will show the details only for a real symmetric matrix. 
Deﬂation of Symmetric Matrices 
Let A be an n times nn × n symmetric matrix. A therefore is diagonalizable, its eigen-
values and eigenvectors are real, and the left and right eigenvalues are the 
same. 
Let left parenthesis c comma v right parenthesis(c, v), with  v Superscript normal upper T Baseline v equals 1vTv = 1, be an eigenpair of A. Now  let  X be an n times n minus 1n × n −1
matrix whose columns form an orthogonal basis for script upper V left parenthesis upper A minus v v Superscript normal upper T Baseline right parenthesisV(A−vvT). One  easy  way  
of doing this is to choose n minus 1n −1 of the n unit vectors of order n such that none 
are equal to v and then, beginning with v, use Gram-Schmidt transformations 
to orthogonalize the vectors, using Algorithm 2.1 on page 50. (Assuming v is 
not a unit vector, we merely choose e 1 comma ellipsis comma e Subscript n minus 1 Baselinee1, . . . , en−1 together with v as the starting 
set of linearly independent vectors.) Now let upper P equals left bracket v vertical bar upper X right bracketP = [v|X]. We have

308
6 Evaluation of Eigenvalues
upper P Superscript negative 1 Baseline equals StartBinomialOrMatrix v Superscript normal upper T Baseline Choose upper X Superscript normal upper T Baseline left parenthesis upper I minus v v Superscript normal upper T Baseline right parenthesis EndBinomialOrMatrix commaP −1 =
[
vT
XT(I −vvT)
]
,
as we see by direct multiplication, and 
upper P Superscript negative 1 Baseline upper A upper P equals Start 2 By 2 Matrix 1st Row 1st Column c 2nd Column 0 2nd Row 1st Column 0 2nd Column upper B EndMatrix commaP −1AP =
[ c 0
0 B
]
,
(6.7) 
where B is the left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1) matrix upper X Superscript normal upper T Baseline upper A upper XXTAX. 
Clearly, B is symmetric and the eigenvalues of B are the same as the other 
n minus 1n −1 eigenvalues of A. The important point is that B is left parenthesis n minus 1 right parenthesis times left parenthesis n minus 1 right parenthesis(n −1) × (n −1). 
6.1.4 Preconditioning 
The convergence of iterative methods applied to a linear system upper A x equals bAx = b
can often be speeded up by replacing the system by an equivalent system 
upper M Superscript negative 1 Baseline upper A x equals upper M Superscript negative 1 Baseline bM −1Ax = M −1b. The iterations then depend on the properties, such as the 
relative magnitudes of the eigenvalues, of upper M Superscript negative 1 Baseline upper AM −1A rather than A. The replace-
ment of the system upper A x equals bAx = b by upper M Superscript negative 1 Baseline upper A x equals upper M Superscript negative 1 Baseline bM −1Ax = M −1b is called preconditioning. 
(It is also sometimes called left preconditioning, and the use of the system 
upper A upper M Superscript negative 1 Baseline y equals bAM −1y = b with y equals upper M xy = Mx is called right preconditioning. Either or both kinds 
of preconditioning may be used in a given iterative algorithm.) The matrix 
M is called a preconditioner. 
Determining an eﬀective preconditioner matrix upper M Superscript negative 1M −1 for eigenvalue compu-
tations is not straightforward. In general, the objective would be to determine 
upper M Superscript negative 1 Baseline upper AM −1A so that it is “close” to I, because then the eigenvalues might be easier 
to obtain by whatever method we may use. The salient properties of I are 
that it is normal (see Sect. 8.2.3 beginning on page 383) and its eigenvalues 
are clustered. 
There are various kinds of preconditioning. We have considered precon-
ditioning in the context of an iterative algorithm for solving linear systems 
on page 278. Some preconditioning methods work better as an adjunct to 
one algorithm, and others work better in conjunction with some other algo-
rithm. Obviously, the eﬃcacy depends on the nature of the data input to the 
problem. In the case of a sparse matrix A, for example, an incomplete factor-
ization upper A almost equals upper L overTilde upper U overTildeA ≈-L-U where both upper L overTilde-L and upper U overTilde-U are sparse, upper M equals upper L overTilde upper U overTildeM = -L-U may be a good  
preconditioner. We will not consider any of the details further here. 
6.1.5 Shifting 
If c is an eigenvalue of A, then  c minus dc −d is an eigenvalue of upper A minus d upper IA −dI, and  the  
associated eigenvectors are the same. (This is property 7 on page 158.) Hence, 
instead of seeking an eigenvalue of A, we might compute (or approximate) an 
eigenvalue of upper A minus d upper IA −dI. (We recall also, from Eq. (5.11) on page 266, that,  for  
appropriate signs of d and the eigenvalues, the condition number of upper A minus d upper IA −dI is 
better than the condition number of A.)

6.2 Power Method
309
Use of upper A minus d upper IA−dI amounts to a “shift” in the eigenvalue. This can often improve 
the convergence rate in an algorithm to compute an eigenvalue. (Remember 
that all general algorithms to compute eigenvalues are iterative.) 
The best value  of  d in the shift depends on both the algorithm and the 
characteristics of the matrix. Various shifts have been suggested. One common 
value of the shift is based on the Rayleigh quotient shift; another common 
value is called the “Wilkinson shift,” after James Wilkinson. We will not 
discuss any of the particular shift values here. 
6.2 Power Method 
The power method is a straightforward method that can be used for a real 
diagonalizable matrix with a simple dominant eigenvalue. A symmetric matrix 
is diagonalizable, of course, but it may not have a simple dominant eigenvalue. 
The power method ﬁnds the dominant eigenvalue. In some applications, 
only the dominant eigenvalue is of interest. If other eigenvalues are needed, 
however, we can ﬁnd them one at a time by deﬂation. 
Let A be a real n times nn×n diagonalizable matrix with a simple dominant eigen-
value. Index the eigenvalues c Subscript ici so that StartAbsoluteValue c 1 EndAbsoluteValue greater than StartAbsoluteValue c 2 EndAbsoluteValue greater than or equals midline horizontal ellipsis StartAbsoluteValue c Subscript n Baseline EndAbsoluteValue|c1| > |c2| ≥· · · |cn|, with corresponding 
normalized eigenvectors v Subscript ivi. Note that the requirement for the dominant eigen-
value that c 1 greater than c 2c1 > c2 implies that c 1c1 and the dominant eigenvector v 1v1 are unique 
and that c 1c1 is real (because otherwise c overbar Subscript 1¯c1 would also be an eigenvalue, and 
that would violate the requirement). 
Now let x be an n-vector that is not orthogonal to v 1v1. Because A is assumed 
to be diagonalizable, the eigenvectors are linearly independent, and so x can 
be represented as a linear combination of the eigenvectors, 
x equals b 1 v 1 plus midline horizontal ellipsis plus b Subscript n Baseline v Subscript n Baseline periodx = b1v1 + · · · + bnvn.
(6.8) 
Because x is not orthogonal to v 1v1, b 1 not equals 0b1 /= 0. The power method is based on a 
sequence 
x comma upper A x comma upper A squared x comma ellipsis periodx, Ax, A2x, . . . .
(This sequence is a ﬁnite Krylov space generating set; see Eq. (5.26).) From 
the relationships above and the deﬁnition of eigenvalues and eigenvectors, we 
have 
StartLayout 1st Row 1st Column upper A x 2nd Column equals 3rd Column b 1 upper A v 1 plus midline horizontal ellipsis plus b Subscript n Baseline upper A v Subscript n 2nd Row 1st Column Blank 2nd Column equals 3rd Column b 1 c 1 v 1 plus midline horizontal ellipsis plus b Subscript n Baseline c Subscript n Baseline v Subscript n 3rd Row 1st Column upper A squared x 2nd Column equals 3rd Column b 1 c 1 squared v 1 plus midline horizontal ellipsis plus b Subscript n Baseline c Subscript n Superscript 2 Baseline v Subscript n 4th Row 1st Column midline horizontal ellipsis 2nd Column equals 3rd Column midline horizontal ellipsis 5th Row 1st Column upper A Superscript j Baseline x 2nd Column equals 3rd Column b 1 c 1 Superscript j Baseline v 1 plus midline horizontal ellipsis plus b Subscript n Baseline c Subscript n Superscript j Baseline v Subscript n 6th Row 1st Column Blank 2nd Column equals 3rd Column c 1 Superscript j Baseline left parenthesis b 1 v 1 plus midline horizontal ellipsis plus b Subscript n Baseline left parenthesis StartFraction c Subscript n Baseline Over c 1 EndFraction right parenthesis Superscript j Baseline v Subscript n Baseline right parenthesis period EndLayoutAx = b1Av1 + · · · + bnAvn
= b1c1v1 + · · · + bncnvn
A2x = b1c2
1v1 + · · · + bnc2
nvn
· · · = · · ·
Ajx = b1cj
1v1 + · · · + bncj
nvn
= cj
1
(
b1v1 + · · · + bn
(cn
c1
)j
vn
)
.
(6.9)

310
6 Evaluation of Eigenvalues
To simplify the notation, let 
u Superscript left parenthesis j right parenthesis Baseline equals upper A Superscript j Baseline x divided by c 1 Superscript ju(j) = Ajx/cj
1
(6.10) 
(or, equivalently, u Superscript left parenthesis j right parenthesis Baseline equals upper A u Superscript left parenthesis j minus 1 right parenthesis Baseline divided by c 1u(j) = Au(j−1)/c1). From Eqs. (6.9) and  the fact that  StartAbsoluteValue c 1 EndAbsoluteValue greater than StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue|c1| >
|ci| for i greater than 1i > 1, we see that u Superscript left parenthesis j right parenthesis Baseline right arrow b 1 v 1u(j) →b1v1, which is the nonnormalized dominant 
eigenvector. 
We have the bound 
StartLayout 1st Row 1st Column double vertical bar u Superscript left parenthesis j right parenthesis Baseline minus b 1 v 1 double vertical bar 2nd Column equals 3rd Column double vertical bar b 2 left parenthesis StartFraction c 2 Over c 1 EndFraction right parenthesis Superscript j Baseline v 2 plus midline horizontal ellipsis 2nd Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis plus b Subscript n Baseline left parenthesis StartFraction c Subscript n Baseline Over c 1 EndFraction right parenthesis Superscript j Baseline v Subscript n Baseline double vertical bar 3rd Row 1st Column Blank 4th Row 1st Column Blank 2nd Column less than or equals 3rd Column StartAbsoluteValue b 2 EndAbsoluteValue StartAbsoluteValue StartFraction c 2 Over c 1 EndFraction EndAbsoluteValue Superscript j Baseline parallel to v 2 parallel to plus midline horizontal ellipsis 5th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis plus StartAbsoluteValue b Subscript n Baseline EndAbsoluteValue StartAbsoluteValue StartFraction c Subscript n Baseline Over c 1 EndFraction EndAbsoluteValue Superscript j Baseline parallel to v Subscript n Baseline parallel to 6th Row 1st Column Blank 7th Row 1st Column Blank 2nd Column less than or equals 3rd Column left parenthesis StartAbsoluteValue b 2 EndAbsoluteValue plus midline horizontal ellipsis plus StartAbsoluteValue b Subscript n Baseline EndAbsoluteValue right parenthesis StartAbsoluteValue StartFraction c 2 Over c 1 EndFraction EndAbsoluteValue Superscript j Baseline period EndLayout
||||u(j) −b1v1
|||| =
||||||||||b2
(c2
c1
)j
v2 + · · ·
· · · + bn
(cn
c1
)j
vn
||||||||||
≤|b2|
||||
c2
c1
||||
j
||v2|| + · · ·
· · · + |bn|
||||
cn
c1
||||
j
||vn||
≤(|b2| + · · · + |bn|)
||||
c2
c1
||||
j
.
(6.11) 
The last expression results from the fact that StartAbsoluteValue c 2 EndAbsoluteValue greater than or equals StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue|c2| ≥|ci| for i greater than 2i > 2 and that the 
v Subscript ivi are unit vectors. 
From Eq. (6.11), we see that the norm of the diﬀerence of u Superscript left parenthesis j right parenthesisu(j) and b 1 v 1b1v1
decreases by a factor of approximately StartAbsoluteValue c 2 divided by c 1 EndAbsoluteValue|c2/c1| with each iteration; hence, this 
ratio is an important indicator of the rate of convergence of u Superscript left parenthesis j right parenthesisu(j) to the domi-
nant eigenvector. 
If StartAbsoluteValue c 1 EndAbsoluteValue greater than StartAbsoluteValue c 2 EndAbsoluteValue greater than StartAbsoluteValue c 3 EndAbsoluteValue|c1| > |c2| > |c3|, b 2 not equals 0b2 /= 0, and  b 1 not equals 0b1 /= 0, the power method converges 
linearly (see page 576); that is, 
0 less than limit Underscript j right arrow normal infinity Endscripts StartFraction parallel to u Superscript left parenthesis j plus 1 right parenthesis Baseline minus b 1 v 1 parallel to Over parallel to u Superscript left parenthesis j right parenthesis Baseline minus b 1 v 1 parallel to EndFraction less than 10 < lim
j→∞
||u(j+1) −b1v1||
||u(j) −b1v1||
< 1
(6.12) 
(see Exercise 6.1c, page 320). Shifting the matrix to form upper A minus d upper IA −dI results in 
a matrix with eigenvalues with diﬀerent relative sizes and may be useful in 
speeding up the convergence. 
If an approximate value of the eigenvector v 1v1 is available and x is taken to 
be that approximate value, the convergence will be faster. If an approximate 
value of the dominant eigenvalue, ModifyingAbove c With caret Subscript 1 Baseline comma-c1, is available, starting with any y Superscript left parenthesis 0 right parenthesisy(0), a few  
iterations on 
left parenthesis upper A minus ModifyingAbove c With caret Subscript 1 Baseline upper I right parenthesis y Superscript left parenthesis k right parenthesis Baseline equals y Superscript left parenthesis k minus 1 right parenthesis(A −-c1I)y(k) = y(k−1)
(6.13) 
may yield a better starting value for x. Once the eigenvector associated with 
the dominant eigenvalue is determined, the eigenvalue c 1c1 can easily be deter-
mined, as described above.

6.3 Jacobi Method
311
Inverse Power Method 
If A is nonsingular, we can also use the power method on upper A Superscript negative 1A−1 to determine 
the smallest eigenvalue of A. This is called the “inverse power method.” 
The rate of convergence may be very diﬀerent from that of the power 
method applied to A. Shifting is also generally important in the inverse power 
method. Of course, this method only determines the eigenvalue with the small-
est absolute value. If other eigenvalues are needed, we can ﬁnd them one at a 
time by deﬂation. 
6.3 Jacobi Method 
The Jacobi method for determining the eigenvalues of a simple symmetric ma-
trix A uses a sequence of orthogonal similarity transformations that eventually 
results in the transformation 
upper A equals upper P upper C upper P Superscript negative 1A = PCP −1
(see Eq. (3.271) on page 172) or  
upper C equals upper P Superscript negative 1 Baseline upper A upper P commaC = P −1AP,
where C is diagonal. Recall that similar matrices have the same eigenvalues. 
The matrices for the similarity transforms are the Givens rotation or Jacobi 
rotation matrices discussed on page 228. The general form of one of these 
orthogonal matrices, upper G Subscript p q Baseline left parenthesis theta right parenthesisGpq(θ), given in Eq. (4.12) on page 230, is the identity 
matrix with cosine thetacos θ in the left parenthesis p comma p right parenthesis normal t normal h(p, p)th and left parenthesis q comma q right parenthesis normal t normal h(q, q)th positions, sine thetasin θ in the left parenthesis p comma q right parenthesis normal t normal h(p, q)th
position, and minus sine theta−sin θ in the left parenthesis q comma p right parenthesis normal t normal h(q, p)th position: 
upper G Subscript p q Baseline left parenthesis theta right parenthesis equals StartEnclose right bottom StartLayout 1st Row 1st Column Blank 2nd Column p 3rd Column Blank 4th Column q 5th Column Blank 2nd Row 1st Column Blank 2nd Column Blank 3rd Column upper I 4th Column 0 5th Column 0 6th Column 0 7th Column 0 8th Column Blank 3rd Row 1st Column p 2nd Column Blank 3rd Column 0 4th Column cosine theta 5th Column 0 6th Column sine theta 7th Column 0 8th Column Blank 4th Row 1st Column Blank 2nd Column Blank 3rd Column 0 4th Column 0 5th Column upper I 6th Column 0 7th Column 0 8th Column Blank 5th Row 1st Column q 2nd Column Blank 3rd Column 0 4th Column minus sine theta 5th Column 0 6th Column cosine theta 7th Column 0 8th Column Blank 6th Row 1st Column Blank 2nd Column Blank 3rd Column 0 4th Column 0 5th Column 0 6th Column 0 7th Column upper I 8th Column Blank EndLayout EndEnclose periodGpq(θ) =
p
q
I
0
0
0
0
p 0 cos θ 0 sin θ 0
0
0
I
0
0
q 0 −sin θ 0 cos θ 0
0
0
0
0
I
.
The Jacobi iteration is 
upper A Superscript left parenthesis k right parenthesis Baseline equals upper G Subscript p Sub Subscript k Subscript q Sub Subscript k Subscript Superscript normal upper T Baseline left parenthesis theta Subscript k Baseline right parenthesis upper A Superscript left parenthesis k minus 1 right parenthesis Baseline upper G Subscript p Sub Subscript k Subscript q Sub Subscript k Subscript Baseline left parenthesis theta Subscript k Baseline right parenthesis commaA(k) = GT
pkqk(θk)A(k−1)Gpkqk(θk),
where p Subscript kpk, q Subscript kqk, and  theta Subscript kθk are chosen so that the upper A Superscript left parenthesis k right parenthesisA(k) is “more diagonal” than 
upper A Superscript left parenthesis k minus 1 right parenthesisA(k−1). Speciﬁcally, the iterations will be chosen so as to reduce the sum of 
the squares of the oﬀ-diagonal elements, which for any square matrix A is 
parallel to upper A parallel to Subscript normal upper F Superscript 2 Baseline minus sigma summation Underscript i Endscripts a Subscript i i Superscript 2 Baseline period||A||2
F −
E
i
a2
ii.

312
6 Evaluation of Eigenvalues
The orthogonal similarity transformations preserve the Frobenius norm 
double vertical bar upper A Superscript left parenthesis k right parenthesis Baseline double vertical bar Subscript normal upper F Baseline equals double vertical bar upper A Superscript left parenthesis k minus 1 right parenthesis Baseline double vertical bar Subscript normal upper F Baseline period
||||||A(k)||||||
F =
||||||A(k−1)||||||
F.
Because the rotation matrices change only the elements in the left parenthesis p comma p right parenthesis normal t normal h(p, p)th, left parenthesis q comma q right parenthesis normal t normal h(q, q)th, 
and left parenthesis p comma q right parenthesis normal t normal h(p, q)th positions (and also the left parenthesis q comma p right parenthesis normal t normal h(q, p)th position since both matrices are 
symmetric), we have 
left parenthesis a Subscript p p Superscript left parenthesis k right parenthesis Baseline right parenthesis squared plus left parenthesis a Subscript q q Superscript left parenthesis k right parenthesis Baseline right parenthesis squared plus 2 left parenthesis a Subscript p q Superscript left parenthesis k right parenthesis Baseline right parenthesis squared equals left parenthesis a Subscript p p Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis squared plus left parenthesis a Subscript q q Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis squared plus 2 left parenthesis a Subscript p q Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis squared period
(
a(k)
pp
)2
+
(
a(k)
qq
)2
+ 2
(
a(k)
pq
)2
=
(
a(k−1)
pp
)2
+
(
a(k−1)
qq
)2
+ 2
(
a(k−1)
pq
)2
.
The oﬀ-diagonal sum of squares at the k normal t normal hkth stage in terms of that at the 
left parenthesis k minus 1 right parenthesis normal t normal h(k −1)th stage is 
StartLayout 1st Row 1st Column double vertical bar upper A Superscript left parenthesis k right parenthesis Baseline double vertical bar Subscript normal upper F Superscript 2 Baseline minus sigma summation Underscript i Endscripts left parenthesis a Subscript i i Superscript left parenthesis k right parenthesis Baseline right parenthesis squared 2nd Column equals 3rd Column double vertical bar upper A Superscript left parenthesis k right parenthesis Baseline double vertical bar Subscript normal upper F Superscript 2 Baseline minus sigma summation Underscript i not equals p comma q Endscripts left parenthesis a Subscript i i Superscript left parenthesis k right parenthesis Baseline right parenthesis squared minus left parenthesis left parenthesis a Subscript p p Superscript left parenthesis k right parenthesis Baseline right parenthesis squared plus left parenthesis a Subscript q q Superscript left parenthesis k right parenthesis Baseline right parenthesis squared right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column double vertical bar upper A Superscript left parenthesis k minus 1 right parenthesis Baseline double vertical bar Subscript normal upper F Superscript 2 Baseline minus sigma summation Underscript i Endscripts left parenthesis a Subscript i i Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis squared minus 2 left parenthesis a Subscript p q Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis squared plus 2 left parenthesis a Subscript p q Superscript left parenthesis k right parenthesis Baseline right parenthesis squared period EndLayout
||||||A(k)||||||
2
F −
E
i
(
a(k)
ii
)2
=
||||||A(k)||||||
2
F −
E
i/=p,q
(
a(k)
ii
)2
−
((
a(k)
pp
)2
+
(
a(k)
qq
)2)
=
||||||A(k−1)||||||
2
F −
E
i
(
a(k−1)
ii
)2
−2
(
a(k−1)
pq
)2
+ 2
(
a(k)
pq
)2
.
(6.14) 
Hence, for a given index pair, left parenthesis p comma q right parenthesis(p, q), at the k normal t normal hkth iteration, the sum of the squares 
of the oﬀ-diagonal elements is minimized by choosing the rotation matrix so 
that 
a Subscript p q Superscript left parenthesis k right parenthesis Baseline equals 0 perioda(k)
pq = 0.
(6.15) 
As we saw on page 230, it is easy to determine the angle thetaθ so as to intro-
duce a zero in a single Givens rotation. Here, we are using the rotations in a 
similarity transformation, so it is a little more complicated. 
The requirement that a Subscript p q Superscript left parenthesis k right parenthesis Baseline equals 0a(k)
pq = 0 implies 
a Subscript p q Superscript left parenthesis k minus 1 right parenthesis Baseline left parenthesis cosine squared theta minus sine squared theta right parenthesis plus left parenthesis a Subscript p p Superscript left parenthesis k minus 1 right parenthesis Baseline minus a Subscript q q Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis cosine theta sine theta equals 0 perioda(k−1)
pq
(
cos2 θ −sin2 θ
)
+
(
a(k−1)
pp
−a(k−1)
qq
)
cos θ sin θ = 0.
(6.16) 
Using the trigonometric identities 
StartLayout 1st Row 1st Column cosine left parenthesis 2 theta right parenthesis 2nd Column equals 3rd Column cosine squared theta minus sine squared theta 2nd Row 1st Column sine left parenthesis 2 theta right parenthesis 2nd Column equals 3rd Column 2 cosine theta sine theta comma EndLayoutcos(2θ) = cos2 θ −sin2 θ
sin(2θ) = 2 cos θ sin θ,
in Eq. (6.16), we have 
tangent left parenthesis 2 theta right parenthesis equals StartFraction 2 a Subscript p q Superscript left parenthesis k minus 1 right parenthesis Baseline Over a Subscript p p Superscript left parenthesis k minus 1 right parenthesis Baseline minus a Subscript q q Superscript left parenthesis k minus 1 right parenthesis Baseline EndFraction commatan(2θ) =
2a(k−1)
pq
a(k−1)
pp
−a(k−1)
qq
,
which yields a unique angle in left bracket negative pi divided by 4 comma pi divided by 4 right bracket[−π/4, π/4]. Of course, the quantities we need 
are cosine thetacos θ and sine thetasin θ, not the angle itself. First, using the identity 
tangent theta equals StartFraction tangent left parenthesis 2 theta right parenthesis Over 1 plus StartRoot 1 plus tangent squared left parenthesis 2 theta right parenthesis EndRoot EndFraction comma tan θ =
tan(2θ)
1 +
/
1 + tan2(2θ)
,

6.3 Jacobi Method
313
we get tangent thetatan θ from tangent left parenthesis 2 theta right parenthesistan(2θ); and then from tangent thetatan θ, we can compute the quantities 
required for the rotation matrix upper G Subscript p q Baseline left parenthesis theta right parenthesisGpq(θ): 
StartLayout 1st Row 1st Column cosine theta 2nd Column equals 3rd Column StartFraction 1 Over StartRoot 1 plus tangent squared theta EndRoot EndFraction comma 2nd Row 1st Column sine theta 2nd Column equals 3rd Column cosine theta tangent theta period EndLayoutcos θ =
1
√
1 + tan2 θ
,
sin θ = cos θ tan θ.
Convergence occurs when the oﬀ-diagonal elements are suﬃciently small. 
The quantity (6.14) using the Frobenius norm is the usual value to compare 
with a convergence criterion, epsilone. 
From Eq. (6.15), we see that the best index pair, left parenthesis p comma q right parenthesis(p, q), is such that 
StartAbsoluteValue a Subscript p q Superscript left parenthesis k minus 1 right parenthesis Baseline EndAbsoluteValue equals max Underscript i less than j Endscripts StartAbsoluteValue a Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline EndAbsoluteValue period
|||a(k−1)
pq
||| = max
i<j
|||a(k−1)
ij
|||.
If this choice is made, the Jacobi method can be shown to converge (see 
Watkins 2002). The method with this choice is called the classical Jacobi 
method. 
For an n times nn × n matrix, the number of operations to identify the maximum 
oﬀ-diagonal is normal upper O left parenthesis n squared right parenthesisO(n2). The computations for the similarity transform itself are 
only normal upper O left parenthesis n right parenthesisO(n) because of the sparsity of the rotators. Of course, the computations 
for the similarity transformations are more involved than those to identify the 
maximum oﬀ-diagonal, so, for small n, the classical Jacobi method should 
be used. If n is large, however, it may be better not to spend time look-
ing for the maximum oﬀ-diagonal. Various cyclic Jacobi methods have been 
proposed in which the pairs left parenthesis p comma q right parenthesis(p, q) are chosen systematically without regard 
to the magnitude of the oﬀ-diagonal being zeroed. Depending on the nature 
of the cyclic Jacobi method, it may or may not be guaranteed to converge. 
For certain schemes, quadratic convergence has been proven; for at least one 
other scheme, an example showing failure of convergence has been given. See 
Watkins (2002) for a discussion of the convergence issues. 
The Jacobi method is one of the oldest algorithms for computing eigenval-
ues and has recently become important again because it lends itself to easy 
implementation on parallel processors (see Zhou and Brent 2003). 
Notice that at the k normal t normal hkth iteration, only two rows and two columns of upper A Superscript left parenthesis k right parenthesisA(k) are 
modiﬁed. This is what allows the Jacobi method to be performed in parallel. 
We can form left floor n divided by 2 right floor[n/2] pairs and do left floor n divided by 2 right floor[n/2] rotations simultaneously. Thus, each 
parallel iteration consists of a choice of a set of index pairs and then a batch 
of rotations. Although, as we have indicated, the convergence may depend on 
which rows are chosen for the rotations, if we are to achieve much eﬃciency by 
performing the operations in parallel, we cannot spend much time in deciding 
how to form the pairs for the rotations. Various schemes have been suggested 
for forming the pairs for a parallel iteration. A simple scheme, called “mobile 
Jacobi” (see Watkins 2002), is: 
1. Perform left floor n divided by 2 right floor[n/2] rotations using the pairs 
left parenthesis 1 comma 2 right parenthesis comma left parenthesis 3 comma 4 right parenthesis comma left parenthesis 5 comma 6 right parenthesis comma ellipsis period(1, 2), (3, 4), (5, 6), . . . .

314
6 Evaluation of Eigenvalues
2. Interchange all rows and columns that were rotated. 
3. Perform left floor left parenthesis n minus 1 right parenthesis divided by 2 right floor[(n −1)/2] rotations using the pairs 
left parenthesis 2 comma 3 right parenthesis comma left parenthesis 4 comma 5 right parenthesis comma left parenthesis 6 comma 7 right parenthesis comma ellipsis period(2, 3), (4, 5), (6, 7), . . . .
4. Interchange all rows and columns that were rotated. 
5. If convergence has not been achieved, go to 1. 
The notation above that speciﬁes the pairs refers to the rows and columns 
at the current state, that is, after the interchanges up to that point. The 
interchange operation is a similarity transformation using an elementary per-
mutation matrix (see page 103), and hence, the eigenvalues are left unchanged 
by this operation. The method described above is a good one, but there are 
other ways of forming pairs. Some of the issues to consider are discussed by 
Luk and Park (1989), who analyzed and compared some proposed schemes. 
6.4 QR Method 
The most common algorithm for extracting eigenvalues is the QR method. 
While the power method and the Jacobi method require diagonalizable matri-
ces, which restricts their practical use to symmetric matrices, the QR method 
can be used for nonsymmetric matrices. It is simpler for symmetric matrices, 
of course, because the eigenvalues are real. Also, for symmetric matrices, the 
computer storage is less, the computations are fewer, and some transforma-
tions are particularly simple. In the following description, we will assume that 
the matrix is symmetric. 
The basic idea behind the use of the QR method is that for a symmetric 
matrix A, the simple iterations beginning with upper A Superscript left parenthesis 0 right parenthesis Baseline equals upper AA(0) = A, for  k equals 1 comma 2 comma ellipsisk = 1, 2, . . ., 
StartLayout 1st Row 1st Column upper Q Superscript left parenthesis k right parenthesis Baseline upper R Superscript left parenthesis k right parenthesis 2nd Column equals 3rd Column upper A Superscript left parenthesis k minus 1 right parenthesis 2nd Row 1st Column upper A Superscript left parenthesis k right parenthesis 2nd Column equals 3rd Column upper R Superscript left parenthesis k right parenthesis Baseline upper Q Superscript left parenthesis k right parenthesis EndLayoutQ(k)R(k) = A(k−1)
A(k) = R(k)Q(k)
lead to an orthogonal triangularization of A. 
These iterations by themselves would be slow and would only work for 
certain matrices, so the QR method requires that the matrix ﬁrst be trans-
formed into upper Hessenberg form (see page 77). A matrix can be reduced to 
Hessenberg form in a ﬁnite number of similarity transformations using either 
Householder reﬂections or Givens rotations. 
The Hessenberg form for a symmetric matrix is tridiagonal. The Hessen-
berg form allows a large savings in the subsequent computations, even for 
nonsymmetric matrices. 
Even in the Hessenberg form, the matrices upper A Superscript left parenthesis k right parenthesisA(k) are shifted by c Superscript left parenthesis k right parenthesis Baseline upper Ic(k)I, where  
c Superscript left parenthesis k right parenthesis Baseline upper Ic(k)I is an approximation of an eigenvalue, which can be obtained in various 
ways.

6.4 QR Method
315
After the matrix has been transformed into a similar Hessenberg matrix, 
a sequence of similar Hessenberg matrices that converge to triangular matrix 
is formed. The QR method for determining the eigenvalues is iterative and 
produces a sequence of Hessenberg matrices that converge to a triangular ma-
trix. An upper Hessenberg matrix is formed, and its eigenvalues are extracted 
by a process called “chasing,” which consists of steps that alternate between 
creating nonzero entries in positions left parenthesis i plus 2 comma i right parenthesis(i + 2, i), left parenthesis i plus 3 comma i right parenthesis(i + 3, i), and  left parenthesis i plus 3 comma i plus 1 right parenthesis(i + 3, i + 1) and 
restoring these entries to zero, as the nonzero entries are moved farther down 
the matrix. For example, 
Start 7 By 7 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 2nd Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 4th Row 1st Column monospace 0 2nd Column monospace upper Y 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 5th Row 1st Column monospace 0 2nd Column monospace upper Y 3rd Column monospace upper Y 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 6th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 7th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace upper X 7th Column monospace upper X EndMatrix right arrow Start 7 By 7 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 2nd Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 3rd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 5th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper Y 4th Column monospace upper X 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 6th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper Y 4th Column monospace upper Y 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace upper X 7th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace upper X 7th Column monospace upper X EndMatrix period
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X X X X X X
X X X X X X X
0 X X X X X X
0 Y X X X X X
0 Y Y X X X X
0 0 0 0 X X X
0 0 0 0 0 X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
→
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X X X X X X
X X X X X X X
0 X X X X X X
0 0 X X X X X
0 0 Y X X X X
0 0 Y Y X X X
0 0 0 0 0 X X
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
In the j normal t normal hjth step of the QR method, a bulge is created and is chased down the 
matrix by similarity transformations, usually Givens transformations, 
upper G Subscript k Superscript negative 1 Baseline upper A Superscript left parenthesis j minus 1 comma k right parenthesis Baseline upper G Subscript k Baseline periodG−1
k A(j−1,k)Gk.
The transformations are based on the eigenvalues of 2 times 22 × 2 matrices in the 
lower right-hand part of the matrix. 
There are some variations on the way the chasing occurs. Haag and 
Watkins (1993) describe an eﬃcient modiﬁed QR algorithm that uses both 
Givens transformations and Gaussian elimination transformations, with or 
without pivoting. For the n times nn × n Hessenberg matrix upper A Superscript left parenthesis 0 comma 0 right parenthesisA(0,0), the ﬁrst step of the 
Haag-Watkins procedure begins with a 3 times 33 × 3 Householder reﬂection matrix, 
upper G overTilde Subscript 0-G0, whose ﬁrst column is 
left parenthesis upper A Superscript left parenthesis 0 comma 0 right parenthesis Baseline minus sigma 1 upper I right parenthesis left parenthesis upper A Superscript left parenthesis 0 comma 0 right parenthesis Baseline minus sigma 2 upper I right parenthesis e 1 comma(A(0,0) −σ1I)(A(0,0) −σ2I)e1,
where sigma 1σ1 and sigma 2σ2 are the eigenvalues of the 2 times2×2 matrix  
Start 2 By 2 Matrix 1st Row 1st Column a Subscript n minus 1 comma n minus 1 2nd Column a Subscript n minus 1 comma n 2nd Row 1st Column a Subscript n minus 1 comma n 2nd Column a Subscript n comma n EndMatrix
[ an−1,n−1 an−1,n
an−1,n
an,n
]
and e 1e1 is the ﬁrst unit vector of length n. The  n times nn × n matrix upper G 0G0 is normal d normal i normal a normal g left parenthesis upper G overTilde Subscript 0 Baseline comma upper I right parenthesisdiag( -G0, I). 
The initial transformation upper G 0 Superscript negative 1 Baseline upper A Superscript left parenthesis 0 comma 0 right parenthesis Baseline upper G 0G−1
0 A(0,0)G0 creates a bulge with nonzero elements 
a 31 Superscript left parenthesis 0 comma 1 right parenthesisa(0,1)
31
, a 41 Superscript left parenthesis 0 comma 1 right parenthesisa(0,1)
41
, and  a 42 Superscript left parenthesis 0 comma 1 right parenthesisa(0,1)
42
. 
After the initial transformation, the Haag-Watkins procedure makes n minus 3n −3
transformations 
upper A Superscript left parenthesis 0 comma k plus 1 right parenthesis Baseline equals upper G Subscript k Superscript negative 1 Baseline upper A Superscript left parenthesis 0 comma k right parenthesis Baseline upper G Subscript k Baseline commaA(0,k+1) = G−1
k A(0,k)Gk,
for k equals 1 comma 2 comma ellipsis comma n minus 3k = 1, 2, . . . , n−3, that chase the bulge diagonally down the matrix, so that 
upper A Superscript left parenthesis 0 comma k plus 1 right parenthesisA(0,k+1) diﬀers from Hessenberg form only by the nonzero elements a Subscript k plus 3 comma k plus 1 Superscript left parenthesis 0 comma k plus 1 right parenthesisa(0,k+1)
k+3,k+1,

316
6 Evaluation of Eigenvalues
a Subscript k plus 4 comma k plus 1 Superscript left parenthesis 0 comma k plus 1 right parenthesisa(0,k+1)
k+4,k+1, and  a Subscript k plus 4 comma k plus 2 Superscript left parenthesis 0 comma k plus 1 right parenthesisa(0,k+1)
k+4,k+2. To accomplish this, the matrix upper G Subscript kGk diﬀers from the 
identity only in rows and columns k plus 1k + 1, k plus 2k + 2, and  k plus 3k + 3. The transformation 
upper G Subscript k Superscript negative 1 Baseline upper A Superscript left parenthesis 0 comma k right parenthesisG−1
k A(0,k)
annihilates the entries a Subscript k plus 2 comma k Superscript left parenthesis 0 comma k right parenthesisa(0,k)
k+2,k and a Subscript k plus 3 comma k Superscript left parenthesis 0 comma k right parenthesisa(0,k)
k+3,k, and the transformation 
left parenthesis upper G Subscript k Superscript negative 1 Baseline upper A Superscript left parenthesis 0 comma k right parenthesis Baseline right parenthesis upper G Subscript k(G−1
k A(0,k))Gk
produces upper A Superscript left parenthesis 0 comma k plus 1 right parenthesisA(0,k+1) with two new nonzero elements, a Subscript k plus 4 comma k plus 1 Superscript left parenthesis 0 comma k plus 1 right parenthesisa(0,k+1)
k+4,k+1 and a Subscript k plus 4 comma k plus 2 Superscript left parenthesis 0 comma k plus 1 right parenthesisa(0,k+1)
k+4,k+2. The  
ﬁnal transformation in the ﬁrst step, for k equals n minus 2k = n −2, annihilates a Subscript n comma n minus 2 Superscript left parenthesis 0 comma k right parenthesisa(0,k)
n,n−2. The  
transformation matrix upper G Subscript n minus 2Gn−2 diﬀers from the identity only in rows and columns 
n minus 1n −1 and n. These steps are iterated until the matrix becomes triangular. 
As the subdiagonal elements converge to zero, the shifts for use in the ﬁrst 
transformation of a step (corresponding to sigma 1σ1 and sigma 2σ2) are determined by 2 times 22×2
submatrices higher on the diagonal. Special consideration must be given to 
situations in which these submatrices contain zero elements. 
This description has just indicated the general ﬂavor of the QR method. 
There are diﬀerent variations on the overall procedure and then many com-
putational details that must be observed. In the Haag-Watkins procedure, for 
example, the upper G Subscript kGks are not unique, and their form can aﬀect the eﬃciency and 
the stability of the algorithm. Haag and Watkins (1993) describe criteria for 
the selection of the upper G Subscript kGks. They also discuss some of the details of programming 
the algorithm. 
6.5 Krylov Methods 
In the power method, we encountered the sequence 
x comma upper A x comma upper A squared x comma ellipsis periodx, Ax, A2x, . . . .
This sequence is a ﬁnite Krylov space generating set. As we mentioned on 
page 277, several methods for computing eigenvalues are often based on a 
Krylov space, 
script upper K Subscript k Baseline equals script upper V left parenthesis StartSet v comma upper A v comma upper A squared v comma ellipsis comma upper A Superscript k minus 1 Baseline v EndSet right parenthesis periodKk = V({v, Av, A2v, . . . , Ak−1v}).
(Aleksey Krylov used these vectors to construct the characteristic polyno-
mial.) 
The two most important Krylov methods are the Lanczos tridiagonal-
ization algorithm and the Arnoldi orthogonalization algorithm. We will not 
discuss these methods here but rather refer the interested reader to Golub 
and Van Loan (1996).

6.6 Generalized Eigenvalues
317
6.6 Generalized Eigenvalues 
In Sect. 3.9.10, we deﬁned the generalized eigenvalues and eigenvectors by 
replacing the identity in the deﬁnition of ordinary eigenvalues and eigenvectors 
by a general (square) matrix B: 
StartAbsoluteValue upper A minus c upper B EndAbsoluteValue equals 0 period|A −cB| = 0.
(6.17) 
If there exists a ﬁnite c such that this determinant is zero, then there is some 
nonzero, ﬁnite vector v such that 
upper A v equals c upper B v periodAv = cBv.
(6.18) 
As we have seen in the case of ordinary eigenvalues, symmetry of the 
matrix, because of diagonalizability, allows for simpler methods to evaluate 
the eigenvalues. In the case of generalized eigenvalues, symmetry together 
with positive deﬁniteness allows us to reformulate the problem to be much 
simpler. If A and B are symmetric and B is positive deﬁnite, we refer to the 
pair left parenthesis upper A comma upper B right parenthesis(A, B) as symmetric. 
If A and B are a symmetric pair, B has a Cholesky decomposition, upper B equals upper T Superscript normal upper T Baseline upper TB =
T TT, where  T is an upper triangular matrix with positive diagonal elements. 
We can therefore rewrite Eq. (6.18) as  
upper T Superscript negative normal upper T Baseline upper A upper T Superscript negative 1 Baseline u equals c u commaT −TAT −1u = cu,
(6.19) 
where u equals upper T vu = Tv. Note that because A is symmetric, upper T Superscript negative normal upper T Baseline upper A upper T Superscript negative 1T −TAT −1 is symmetric, 
and since c is an eigenvalue of this matrix, it is real. Its associated eigenvector 
(with respect to upper T Superscript negative normal upper T Baseline upper A upper T Superscript negative 1T −TAT −1) is likewise real, and therefore, so is the general-
ized eigenvector v. Because upper T Superscript negative normal upper T Baseline upper A upper T Superscript negative 1T −TAT −1 is symmetric, the ordinary eigenvectors 
can be chosen to be orthogonal. (Recall from page 176 that eigenvectors cor-
responding to distinct eigenvalues are orthogonal and those corresponding to 
a multiple eigenvalue can be chosen to be orthogonal.) This implies that the 
generalized eigenvectors of the symmetric pair left parenthesis upper A comma upper B right parenthesis(A, B) can be chosen to be  
B-conjugate. 
Because of the equivalence of a generalized eigenproblem for a symmetric 
pair to an ordinary eigenproblem for a symmetric matrix, any of the methods 
discussed in this chapter can be used to evaluate the generalized eigenpairs 
of a symmetric pair. The matrices in statistical applications for which the 
generalized eigenvalues are required are often symmetric pairs. For example, 
Roy’s maximum root statistic, which is used in multivariate analysis, is a 
generalized eigenvalue of two Wishart matrices. 
The generalized eigenvalues of a pair that is not symmetric are more dif-
ﬁcult to evaluate. The approach of forming upper Hessenberg matrices, as 
in the QR method, is also used for generalized eigenvalues. We will not dis-
cuss this method here but instead refer the reader to Watkins (2002) for  a  
description of the method, which is called the QZ algorithm.

318
6 Evaluation of Eigenvalues
6.7 Singular Value Decomposition 
The standard algorithm for computing the singular value decomposition 
upper A equals upper U upper D upper V Superscript normal upper TA = UDV T
is due to Golub and Reinsch (1970) and is built on ideas of Golub and Kahan 
(1965). The ﬁrst step in the Golub-Reinsch algorithm for the singular value 
decomposition of the n times mn×m matrix A is to reduce A to upper bidiagonal form: 
upper A Superscript left parenthesis 0 right parenthesis Baseline equals Start 9 By 6 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 4th Row 1st Column Blank 2nd Column Blank 3rd Column down right diagonal ellipsis 4th Column down right diagonal ellipsis 5th Column Blank 5th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace upper X 6th Column monospace upper X 6th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace upper X 7th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 8th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column midline horizontal ellipsis 5th Column vertical ellipsis 6th Column vertical ellipsis 9th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column midline horizontal ellipsis 5th Column monospace 0 6th Column monospace 0 EndMatrix periodA(0) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X 0 · · · 0 0
0 X X · · · 0 0
0 0 X · · · 0 0
...
...
0 0 0 · · · X X
0 0 0 · · · 0 X
0 0 0 · · · 0 0
...
...
... · · ·
...
...
0 0 0 · · · 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
We assume n greater than or equals mn ≥m. (If this is not the case, we merely use upper A Superscript normal upper TAT.) This algorithm is 
basically a factored form of the QR algorithm for the eigenvalues of upper A Superscript left parenthesis 0 right parenthesis normal upper T Baseline upper A Superscript left parenthesis 0 right parenthesisA(0)TA(0), 
which would be symmetric and tridiagonal. 
The Golub-Reinsch method produces a sequence of upper bidiagonal ma-
trices, upper A Superscript left parenthesis 0 right parenthesis Baseline comma upper A Superscript left parenthesis 1 right parenthesis Baseline comma upper A Superscript left parenthesis 2 right parenthesis Baseline comma ellipsisA(0), A(1), A(2), . . . , which converges to the diagonal matrix D. (Each  
of these has a zero submatrix below the square submatrix.) Similar to the QR 
method for eigenvalues, the transformation from upper A Superscript left parenthesis j right parenthesisA(j) to upper A Superscript left parenthesis j plus 1 right parenthesisA(j+1) is eﬀected by 
a sequence of orthogonal transformations, 
StartLayout 1st Row 1st Column upper A Superscript left parenthesis j plus 1 right parenthesis 2nd Column equals 3rd Column upper R Subscript m minus 2 Superscript normal upper T Baseline upper R Subscript m minus 3 Superscript normal upper T Baseline midline horizontal ellipsis upper R 0 Superscript normal upper T Baseline upper A Superscript left parenthesis j right parenthesis Baseline upper T 0 upper T 1 midline horizontal ellipsis upper T Subscript m minus 2 Baseline 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper R Superscript normal upper T Baseline upper A Superscript left parenthesis j right parenthesis Baseline upper T comma EndLayoutA(j+1) = RT
m−2RT
m−3 · · · RT
0 A(j)T0T1 · · · Tm−2
= RTA(j)T,
which ﬁrst introduces a nonzero entry below the diagonal (upper T 0T0 does this) and 
then chases it down the diagonal. After upper T 0T0 introduces a nonzero entry in the 
left parenthesis 2 comma 1 right parenthesis(2, 1) position, upper R 0 Superscript normal upper TRT
0 annihilates it and produces a nonzero entry in the left parenthesis 1 comma 3 right parenthesis(1, 3)
position; upper T 1T1 annihilates the left parenthesis 1 comma 3 right parenthesis(1, 3) entry and produces a nonzero entry in the 
left parenthesis 3 comma 2 right parenthesis(3, 2) position, which upper R 1 Superscript normal upper TRT
1 annihilates; and so on. Each of the upper R Subscript kRks and  upper T Subscript kTks are  
Givens transformations, and, except for upper T 0T0, it should be clear how to form 
them. 
If none of the elements along the main diagonal or the diagonal above the 
main diagonal is zero, then upper T 0T0 is chosen as the Givens transformation such 
that upper T 0 Superscript normal upper TT T
0 will annihilate the second element in the vector 
left parenthesis a 11 squared minus sigma 1 comma a 11 a 12 comma 0 comma midline horizontal ellipsis comma 0 right parenthesis comma(a2
11 −σ1, a11a12, 0, · · · , 0),

6.7 Singular Value Decomposition
319
where sigma 1σ1 is the eigenvalue of the lower right-hand 2 times 22×2 submatrix of upper A Superscript left parenthesis 0 right parenthesis normal upper T Baseline upper A Superscript left parenthesis 0 right parenthesisA(0)TA(0)
that is closest in value to the left parenthesis m comma m right parenthesis(m, m) element of upper A Superscript left parenthesis 0 right parenthesis normal upper T Baseline upper A Superscript left parenthesis 0 right parenthesisA(0)TA(0). This is easy to  
compute (see Exercise 6.6). 
If an element along the main diagonal or the diagonal above the main 
diagonal is zero, we must proceed slightly diﬀerently. (Remember that for 
purposes of computations, “zero” generally means “near zero,” that is, to 
within some set tolerance.) 
If an element above the main diagonal is zero, the bidiagonal matrix is 
separated at that value into a block diagonal matrix, and each block (which 
is bidiagonal) is treated separately. 
If an element on the main diagonal, say a Subscript k kakk, is zero, then a singular value 
is zero. In this case, we apply a set of Givens transformations from the left. 
We ﬁrst use upper G 1G1, which diﬀers from the identity only in rows and columns k 
and k plus 1k + 1, to annihilate the left parenthesis k comma k plus 1 right parenthesis(k, k + 1) entry and introduce a nonzero in the 
left parenthesis k comma k plus 2 right parenthesis(k, k + 2) position. We then use upper G 2G2, which diﬀers from the identity only in 
rows and columns k and k plus 2k +2, to annihilate the left parenthesis k comma k plus 2 right parenthesis(k, k +2) entry and introduce 
a nonzero in the  left parenthesis k comma k plus 3 right parenthesis(k, k +3) position. Continuing this process, we form a matrix 
of the form 
Start 10 By 8 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace 0 2nd Row 1st Column monospace 0 2nd Column monospace upper X 3rd Column monospace upper X 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace 0 3rd Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace upper X 4th Column monospace upper Y 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace 0 4th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace 0 5th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace upper X 6th Column monospace upper X 7th Column monospace 0 8th Column monospace 0 6th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace upper X 7th Column monospace upper X 8th Column monospace 0 7th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace upper X 8th Column monospace upper X 8th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace upper X 9th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column midline horizontal ellipsis 5th Column vertical ellipsis 6th Column vertical ellipsis 7th Column vertical ellipsis 8th Column vertical ellipsis 10th Row 1st Column monospace 0 2nd Column monospace 0 3rd Column monospace 0 4th Column monospace 0 5th Column monospace 0 6th Column monospace 0 7th Column monospace 0 8th Column monospace 0 EndMatrix period
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
X X 0 0 0 0 0 0
0 X X 0 0 0 0 0
0 0 X Y 0 0 0 0
0 0 0 0 0 0 0 0
0 0 0 0 X X 0 0
0 0 0 0 0 X X 0
0 0 0 0 0 0 X X
0 0 0 0 0 0 0 X
...
...
... · · ·
...
...
...
...
0 0 0 0 0 0 0 0
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
The Y in this matrix (in position left parenthesis k minus 1 comma k right parenthesis(k−1, k)) is then chased up the upper block 
consisting of the ﬁrst k rows and columns of the original matrix by using 
Givens transformations applied from the right. This then yields two block 
bidiagonal matrices (and a 1 times 11 × 1
0 matrix). We operate on the individual 
blocks as before. 
After the steps have converged to yield a diagonal matrix, upper D overTilde-D, all  of  the  
Givens matrices applied from the left are accumulated into a single matrix, and 
all from the right are accumulated into a single matrix to yield a decomposition 
upper A equals upper U overTilde upper D overTilde upper V overTilde Superscript normal upper T Baseline periodA = -U -D -V T.
There is one  last  thing to do.  The elements of  upper D overTilde-D may not be nonnegative. 
This is easily remedied by postmultiplying by a diagonal matrix G that is the 
same as the identity except for having a negative 1−1 in any position corresponding 
to a negative value in upper D overTilde-D. In addition, we generally form the singular value 
decomposition in such a way that the elements in D are nonincreasing. The

320
6 Evaluation of Eigenvalues
entries in upper D overTilde-D can be rearranged by a permutation matrix upper E Subscript left parenthesis pi right parenthesisE(π) so they are in 
nonincreasing order. So we have 
upper D equals upper E Subscript left parenthesis pi right parenthesis Superscript normal upper T Baseline upper D overTilde upper G upper E Subscript left parenthesis pi right parenthesis Baseline commaD = ET
(π) -DGE(π),
and the ﬁnal decomposition is 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column upper U overTilde upper E Subscript left parenthesis pi right parenthesis Baseline upper G upper D upper E Subscript left parenthesis pi right parenthesis Superscript normal upper T Baseline upper V overTilde Superscript normal upper T 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper U upper D upper V Superscript normal upper T Baseline period EndLayoutA = -UE(π)GDET
(π) -V T
= UDV T.
If n greater than or equals five thirds mn ≥5
3m, a modiﬁcation of this algorithm by Chan (1982a,b) is more  
eﬃcient than the standard Golub-Reinsch method. 
Exercises 
6.1. Simple matrices and the power method. 
a) Let A be an n × n matrix whose elements are generated indepen-
dently (but not necessarily identically) from real-valued continuous 
distributions. What is the probability that A is simple? 
b) Under the same conditions as in Exercise 6.1a, and  with  n ≥ 3, what 
is the probability that |cn−2| < |cn−1| < |cn|, where  cn−2, cn−1, and  
cn are the three eigenvalues with the largest absolute values? 
c) Prove that the power method converges linearly if |cn−2| < |cn−1| < 
|cn|, bn−1 /= 0,  and  bn /= 0.  (The  bs are the coeﬃcients in the expan-
sion of x(0) .) 
Hint: Substitute the expansion in Eq. (6.11) on page 310 into the 
expression for the convergence ratio in Eq. (6.12). 
d) Suppose A is simple and the elements of x(0) are generated indepen-
dently (but not necessarily identically) from continuous distributions. 
What is the probability that the power method will converge linearly? 
6.2. Consider the matrix 
Start 4 By 4 Matrix 1st Row 1st Column 4 2nd Column 1 3rd Column 2 4th Column 3 2nd Row 1st Column 1 2nd Column 5 3rd Column 3 4th Column 2 3rd Row 1st Column 2 2nd Column 3 3rd Column 6 4th Column 1 4th Row 1st Column 3 2nd Column 2 3rd Column 1 4th Column 7 EndMatrix period
⎡
⎢⎢⎣
4 1 2 3
1 5 3 2
2 3 6 1
3 2 1 7
⎤
⎥⎥⎦.
a) Use the power method to determine the largest eigenvalue and an 
associated eigenvector of this matrix. 
b) Find a 3 × 3 matrix, as in Eq. (6.7), that has the same eigenvalues as 
the remaining eigenvalues of the matrix above. 
c) Using Givens transformations, reduce the matrix to upper Hessenberg 
form.

Exercises
321
6.3. In the matrix 
Start 4 By 4 Matrix 1st Row 1st Column 2 2nd Column 1 3rd Column 0 4th Column 0 2nd Row 1st Column 1 2nd Column 5 3rd Column 2 4th Column 0 3rd Row 1st Column 3 2nd Column 2 3rd Column 6 4th Column 1 4th Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 8 EndMatrix comma
⎡
⎢⎢⎣
2 1 0 0
1 5 2 0
3 2 6 1
0 0 1 8
⎤
⎥⎥⎦,
determine the Givens transformations to chase the 3 in the (3, 1) position 
out of the matrix. 
6.4. In the matrix 
Start 5 By 4 Matrix 1st Row 1st Column 2 2nd Column 1 3rd Column 0 4th Column 0 2nd Row 1st Column 3 2nd Column 5 3rd Column 2 4th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 6 4th Column 1 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 8 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 EndMatrix comma
⎡
⎢⎢⎢⎢⎣
2 1 0 0
3 5 2 0
0 0 6 1
0 0 0 8
0 0 0 0
⎤
⎥⎥⎥⎥⎦
,
determine the Givens transformations to chase the 3 in the (2, 1) position 
out of the matrix. 
6.5. In the QR methods for eigenvectors and singular values, why can we 
not just use additional orthogonal transformations to triangularize the 
given matrix (instead of just forming a similar Hessenberg matrix, as in 
Sect. 6.4) or to diagonalize the given matrix (instead of just forming the 
bidiagonal matrix, as in Sect. 6.7)? 
6.6. Determine the eigenvalue σ1 (on page 319) used in forming the matrix 
T0 for initiating the chase in the algorithm for the singular value decom-
position. Express it in terms of am,m, am−1,m−1, am−1,m, and  am−1,m−2.

7 
Real Analysis and Probability Distributions 
of Vectors and Matrices 
The earlier chapters in Part I have been in the area of mathematics generally 
called “algebra.” It includes the study of the familiar structures such as groups, 
ﬁelds, vector spaces or linear algebra, and so on. Another important area 
of mathematics is called “analysis.” It includes the ordinary calculus, with 
diﬀerentiation and integration usually based in measure theory. If the objects 
are real numbers or are composed of real numbers, the area is called “real 
analysis.” 
In this chapter, we ﬁrst consider the basic calculus operations in the con-
text of vectors and matrices. We then consider an important area of applica-
tion of real analysis in statistics, probability theory. In Sect. 9.6 of Chap. 9, 
we discuss an area of application in the intersection of real analysis and linear 
algebra, namely, optimization. 
Functions of Vectors and Matrices 
There are various types of functions of vectors and matrices. Some functions 
of matrices, such as the trace, the determinant, and the diﬀerent norms, are 
all functions from IRn×n into IR or into the nonnegative reals, ¯IR+. Other 
functions of vectors and matrices are just those deﬁned by elementwise opera-
tions, such as sin(A) = (sin(aij)) and exp(A) = (exp(aij)). That is, a standard 
function that maps IR to IR, when evaluated on IRn×m , maps to IRn×m in a 
very direct way. The derivative of a function deﬁned in this way is simply the 
derivative of the individual elements if they all exist. Most of the mathemat-
ical software, such as R, MATLAB, and modern Fortran, interpret built-in 
functions this way when a matrix is given as the argument. 
For a diagonalizable matrix, another way of deﬁning a function of a matrix 
that corresponds to some function of a scalar, f(x), is to use the diagonal 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 7 
323

324
7 Real Analysis and Probability Distributions of Vectors and Matrices
factorization as in Eq. (3.274) on page 175: 
f left parenthesis upper A right parenthesis equals upper V normal d normal i normal a normal g left parenthesis left parenthesis f left parenthesis c 1 right parenthesis comma ellipsis comma f left parenthesis c Subscript n Baseline right parenthesis right parenthesis right parenthesis upper V Superscript negative 1 Baseline commaf(A) = V diag((f(c1), . . . , f(cn)))V −1,
if f(·) is deﬁned for each eigenvalue ci. 
If a function of a scalar f(x) has a convergent series expansion, another way 
of deﬁning a corresponding function of a matrix is to use the series expansion. 
For example, using the power series expansion of ex = E∞ 
k=0 
xk 
k! , we deﬁned 
the matrix exponential for the square matrix A in Eq. (3.275) as the  matrix  
normal e Superscript upper A Baseline equals sigma summation Underscript k equals 0 Overscript normal infinity Endscripts StartFraction upper A Superscript k Baseline Over k factorial EndFraction periodeA =
∞
E
k=0
Ak
k! .
(Both R and MATLAB have a function expm for the matrix exponential.) 
Of course, eA may also be interpreted as the matrix with elements (eaij ), as 
mentioned above. The derivative of a function deﬁned as a convergent series 
is simply the series of the derivatives of the individual terms, if they all exist 
and if they converge. (Otherwise, it may not be deﬁned.) 
The form of the vector or matrix function obviously determines how we 
must interpret a derivative or an integral involving the function. If diﬀerenti-
ation or integration can be done term by term, the nature of the terms must 
be taken into account. 
Vector/Matrix Derivatives and Integrals 
The operations of diﬀerentiation and integration of vectors and matrices are 
logical extensions of the corresponding operations on scalars. There are three 
objects involved in these operations: 
• The variable of the operation 
• The operand (the function being diﬀerentiated or integrated) 
• The result of the  operation  
In the simplest case, all three of these objects are of the same type, and 
they are scalars. If either the variable or the operand is a vector or a matrix, 
however, the structure of the result may be more complicated. This statement 
will become clearer as we proceed to consider speciﬁc cases. 
In this chapter, we state or show the form that the derivative takes in 
terms of simpler derivatives. We state high-level rules for the nature of the 
diﬀerentiation in terms of simple partial diﬀerentiation of a scalar with respect 
to a scalar. We do not consider whether or not the derivatives exist. In gen-
eral, if the simpler derivatives we write that comprise the more complicated 
object exist, then the derivative of that more complicated object exists. Once 
a shape of the derivative is determined, deﬁnitions or derivations in e-δ terms 
could be given, but we will refrain from that kind of formal exercise. The pur-
pose of this chapter is not to develop a calculus for vectors and matrices but

7.1 Basics of Diﬀerentiation
325
rather to consider some cases that ﬁnd wide applications in statistics. For a 
more careful treatment of diﬀerentiation of vectors and matrices, the reader is 
referred to Magnus and Neudecker (1999) or to Kollo and von Rosen (2005). 
Anderson (2003) and Nachbin (1965) also cover various aspects of integration 
with respect to vector or matrix diﬀerentials. 
Diﬀerentiation is a common operation in solving optimization problems 
such as least squares or maximum likelihood, and in Sect. 9.6, we illustrate 
some of the uses of vector/matrix diﬀerentiation. 
Because integration is a key operation in work with probability distribu-
tions (it is the deﬁnition of an expected value), in Sect. 7.3, we brieﬂy discuss 
multivariate probability distributions and some expectations of multivariate 
random variables. 
7.1 Basics of Diﬀerentiation 
It is useful to recall the heuristic interpretation of a derivative. A derivative 
of a function is the inﬁnitesimal rate of change of the function with respect 
to the variable with which the diﬀerentiation is taken. If both the function 
and the variable are scalars, this interpretation is unambiguous. If, however, 
the operand of the diﬀerentiation, Φ, is a more complicated function, say a 
vector or a matrix, and/or the variable of the diﬀerentiation, Ξ, is a more  
complicated object, the changes are more diﬃcult to measure. Change in the 
value both of the function, 
delta normal upper Phi equals normal upper Phi Subscript normal n normal e normal w Baseline minus normal upper Phi Subscript normal o normal l normal d Baseline commaδΦ = Φnew −Φold,
and of the variable, 
delta normal upper Xi equals normal upper Xi Subscript normal n normal e normal w Baseline minus normal upper Xi Subscript normal o normal l normal d Baseline commaδΞ = Ξnew −Ξold,
could be measured in various ways, for example, by using various norms, as 
discussed in Sects. 2.1.5 and 3.11. (Note that the subtraction is not necessarily 
ordinary scalar subtraction.) 
Furthermore, we cannot just divide the function values by δΞ. We do not  
have a deﬁnition for division by that kind of object. We need a mapping, 
possibly a norm, that assigns a positive real number to δΞ. We can deﬁne 
the change in the function value as just the simple diﬀerence of the function 
evaluated at the two points. This yields 
limit Underscript parallel to delta normal upper Xi parallel to right arrow 0 Endscripts StartFraction normal upper Phi left parenthesis normal upper Xi plus delta normal upper Xi right parenthesis minus normal upper Phi left parenthesis normal upper Xi right parenthesis Over parallel to delta normal upper Xi parallel to EndFraction period
lim
||δΞ||→0
Φ(Ξ + δΞ) −Φ(Ξ)
||δΞ||
.
(7.1) 
So long as we remember the complexity of δΞ, however, we can adopt a 
simpler approach. Since for both vectors and matrices, we have deﬁnitions of 
multiplication by a scalar and of addition, we can simplify the limit in the 
usual deﬁnition of a derivative, δΞ → 0. Instead of using δΞ as the element

326
7 Real Analysis and Probability Distributions of Vectors and Matrices
of change, we will use tΥ, where  t is a scalar and Υ is an element to be added 
to Ξ. The limit then will be taken in terms of t → 0. This leads to 
limit Underscript t right arrow 0 Endscripts StartFraction normal upper Phi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Phi left parenthesis normal upper Xi right parenthesis Over t EndFraction lim
t→0
Φ(Ξ + tΥ) −Φ(Ξ)
t
(7.2) 
as a formula for the derivative of Φ with respect to Ξ. 
The expression (7.2) may be a useful formula for evaluating a derivative, 
but we must remember that it is not the derivative. The type of object of 
this formula is the same as the type of object of the function, Φ; it does not  
accommodate the type of object of the argument, Ξ, unless Ξ is a scalar. As 
we will see below, for example, if Ξ is a vector and Φ is a scalar, the derivative 
must be a vector, yet in that case, the expression (7.2) is a scalar. 
The expression (7.1) is rarely directly useful in evaluating a derivative, but 
it serves to remind us of both the generality and the complexity of the concept. 
Both Φ and its arguments could be functions, for example. (In functional 
analysis, various kinds of functional derivatives are deﬁned, such as a Gˆateaux 
derivative. These derivatives ﬁnd applications in developing robust statistical 
methods.) In this chapter, we are interested in the combinations of three 
possibilities for Φ, namely, scalar, vector, and matrix, and the same three 
possibilities for Ξ and Υ. 
7.1.1 Continuity 
For the derivative of a function to exist at a point, the function must be 
continuous at that point. A function of a vector or a matrix is continuous 
if it is continuous for each element of the vector or matrix. Just as scalar 
sums and products are continuous, vector/matrix sums and all of the types 
of vector/matrix products we have discussed are continuous. A continuous 
function of a continuous function is continuous. 
Many of the vector/matrix functions we have discussed are clearly con-
tinuous. For example, the Lp vector norms in Eq. (2.32) are continuous. The 
determinant of a matrix is continuous, as we see from the deﬁnition of the de-
terminant and the fact that sums and scalar products are continuous. The fact 
that the determinant is a continuous function immediately yields the result 
that cofactors and hence the adjugate are continuous. From the relationship 
between an inverse and the adjugate (Eq. (3.186)), we see that the inverse is 
a continuous function. 
One important function that is not continuous is the rank of a matrix. 
7.1.2 Notation and Properties 
We write the diﬀerential operator with respect to the dummy variable x as 
∂/∂x or ∂/∂xT . We usually denote diﬀerentiation using the symbol for “par-
tial” diﬀerentiation, ∂, whether the operator is written as ∂xi for diﬀerenti-
ation with respect to a speciﬁc scalar variable or ∂x for diﬀerentiation with

7.1 Basics of Diﬀerentiation
327
respect to the array x that contains all of the individual elements. Sometimes, 
however, if the diﬀerentiation is being taken with respect to the whole array 
(the vector or the matrix), we use the notation d/dx. 
The operand of the diﬀerential operator ∂/∂x is a function of x. (If  it  
is not a function of x – i.e., if it is a constant function with respect to x 
– then the operator evaluates to 0.) The result of the operation, written as 
∂f/∂x, is also a function of x, with the same domain as f, and we sometimes 
write ∂f(x)/∂x to emphasize this fact. The value of this function at the ﬁxed 
point x0 is written as ∂f(x0)/∂x. (The derivative of the constant f(x0) is  
identically 0, but it is not necessary to write ∂f(x)/∂x|x0 because ∂f(x0)/∂x 
is interpreted as the value of the function ∂f(x)/∂x at the ﬁxed point x0.) 
If ∂/∂x operates on f, and  f : S → T, then  ∂/∂x : S → U. The nature 
of S, or more directly the nature of x, whether it is a scalar, a vector, or 
a matrix, and the nature of T determine the structure of the result U. For  
example, if x is an n-vector and f(x) =  xT x, then  
f colon normal fraktur upper R Superscript n Baseline right arrow normal fraktur upper Rf : IRn →IR
and 
partial differential f divided by partial differential x colon normal fraktur upper R Superscript n Baseline right arrow normal fraktur upper R Superscript n Baseline comma∂f/∂x : IRn →IRn,
as we will see. The outer product, h(x) =  xxT , is a mapping to a higher-rank 
array, but the derivative of the outer product is a mapping to an array of the 
same rank; that is, 
h colon normal fraktur upper R Superscript n Baseline right arrow normal fraktur upper R Superscript n times nh : IRn →IRn×n
and 
partial differential h divided by partial differential x colon normal fraktur upper R Superscript n Baseline right arrow normal fraktur upper R Superscript n Baseline period∂h/∂x : IRn →IRn.
(Note that “rank” here means the number of dimensions; see page 3.) 
As another example, consider g(·) = det(·), so 
g colon normal fraktur upper R Superscript n times n Baseline right arrow normal fraktur upper R periodg : IRn×n →IR.
In this case, 
partial differential g divided by partial differential upper X colon normal fraktur upper R Superscript n times n Baseline right arrow normal fraktur upper R Superscript n times n Baseline semicolon∂g/∂X : IRn×n →IRn×n;
that is, the derivative of the determinant of a square matrix is a square matrix, 
as we will see later. 
Higher-order diﬀerentiation is a composition of the ∂/∂x operator with 
itself or of the ∂/∂x operator with the ∂/∂xT operator. For example, consider 
the familiar function in linear least squares 
f left parenthesis b right parenthesis equals left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis periodf(b) = (y −Xb)T(y −Xb).
This is a mapping from IRm to IR. The ﬁrst derivative with respect to the m-
vector b is a mapping from IRm to IRm , namely,  2XT Xb −2XT y. The second 
derivative with respect to bT is a mapping from IRm to IRm×m , namely,  2XT X.

328
7 Real Analysis and Probability Distributions of Vectors and Matrices
(Many readers will already be familiar with these facts. We will discuss the 
general case of diﬀerentiation with respect to a vector in Sect. 7.2.2.) 
We see from expression (7.1) that diﬀerentiation is a linear operator; that 
is, if D(Φ) represents the operation deﬁned in expression (7.1), Ψ is another 
function in the class of functions over which D is deﬁned, and a is a scalar 
that does not depend on the variable Ξ, then  D(aΦ + Ψ) =  aD(Φ) +  D(Ψ). 
This yields the familiar rules of diﬀerential calculus for derivatives of sums or 
constant scalar products. Other usual rules of diﬀerential calculus apply, such 
as for diﬀerentiation of products and composition (the chain rule). We can 
use expression (7.2) to work these out. For example, for the derivative of the 
product ΦΨ, after some rewriting of terms, we have the numerator 
StartLayout 1st Row 1st Column Blank 2nd Column Blank 3rd Column normal upper Phi left parenthesis normal upper Xi right parenthesis left parenthesis normal upper Psi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Psi left parenthesis normal upper Xi right parenthesis right parenthesis 2nd Row 1st Column Blank 2nd Column Blank 3rd Column plus normal upper Psi left parenthesis normal upper Xi right parenthesis left parenthesis normal upper Phi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Phi left parenthesis normal upper Xi right parenthesis right parenthesis 3rd Row 1st Column Blank 2nd Column Blank 3rd Column plus left parenthesis normal upper Phi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Phi left parenthesis normal upper Xi right parenthesis right parenthesis left parenthesis normal upper Psi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Psi left parenthesis normal upper Xi right parenthesis right parenthesis period EndLayout Φ(Ξ)
(
Ψ(Ξ + tΥ) −Ψ(Ξ)
)
+ Ψ(Ξ)
(
Φ(Ξ + tΥ) −Φ(Ξ)
)
+
(
Φ(Ξ + tΥ) −Φ(Ξ)
)(
Ψ(Ξ + tΥ) −Ψ(Ξ)
)
.
Now, dividing by t and taking the limit, assuming that as 
t right arrow 0 commat →0,
left parenthesis normal upper Phi left parenthesis normal upper Xi plus t normal upper Upsilon right parenthesis minus normal upper Phi left parenthesis normal upper Xi right parenthesis right parenthesis right arrow 0 comma(Φ(Ξ + tΥ) −Φ(Ξ)) →0,
we have 
script upper D left parenthesis normal upper Phi normal upper Psi right parenthesis equals script upper D left parenthesis normal upper Phi right parenthesis normal upper Psi plus normal upper Phi script upper D left parenthesis normal upper Psi right parenthesis commaD(ΦΨ) = D(Φ)Ψ + ΦD(Ψ),
(7.3) 
where again D represents the diﬀerentiation operation. 
7.1.3 Diﬀerentials 
For a diﬀerentiable scalar function of a scalar variable, f(x), the diﬀerential 
of f at c with increment u is udf/dx|c. This is the linear term in a truncated 
Taylor series expansion: 
f left parenthesis c plus u right parenthesis equals f left parenthesis c right parenthesis plus u StartFraction normal d Over normal d x EndFraction f left parenthesis c right parenthesis plus r left parenthesis c comma u right parenthesis periodf(c + u) = f(c) + u d
dxf(c) + r(c, u).
(7.4) 
Technically, the diﬀerential is a function of both x and u, but the notation 
df is used in a generic sense to mean the diﬀerential of f. For vector/matrix 
functions of vector/matrix variables, the diﬀerential is deﬁned in a similar 
way. The structure of the diﬀerential is the same as that of the function; that 
is, for example, the diﬀerential of a matrix-valued function is a matrix. 
7.1.4 Use of Diﬀerentiation in Optimization 
The derivatives of a scalar-valued function measure the rate of change, so if the 
derivative is 0 at some point, it is possible that the function is at a minimum 
or at that point. One of the main uses of diﬀerentiation of functions is to ﬁnd 
a minimum (as in least squares) or a maximum (as in maximum likelihood). 
We discuss optimization in general in Sect. 9.6, where diﬀerentiation plays an 
important role.

7.2 Types of Diﬀerentiation
329
7.2 Types of Diﬀerentiation 
In the following sections, we consider diﬀerentiation with respect to diﬀerent 
types of objects ﬁrst, and then we consider diﬀerentiation of diﬀerent types 
of objects. 
7.2.1 Diﬀerentiation with Respect to a Scalar 
Diﬀerentiation of a structure (vector or matrix, e.g.) with respect to a scalar 
is quite simple; it just yields the ordinary derivative of each element of the 
structure in the same structure. Thus, the derivative of a vector or a matrix 
with respect to a scalar variable is a vector or a matrix, respectively, of the 
derivatives of the individual elements. 
Diﬀerentiation with respect to a vector or matrix, which we will consider 
below, is often best approached by considering diﬀerentiation with respect to 
the individual elements of the vector or matrix, that is, with respect to scalars. 
Derivatives of Vectors with Respect to Scalars 
The derivative of the vector y(x) = (y1, . . . , yn) with respect to the scalar x 
is the vector 
partial differential y divided by partial differential x equals left parenthesis partial differential y 1 divided by partial differential x comma ellipsis comma partial differential y Subscript n Baseline divided by partial differential x right parenthesis period∂y/∂x = (∂y1/∂x, . . . , ∂yn/∂x).
(7.5) 
The second or higher derivative of a vector with respect to a scalar is 
likewise a vector of the derivatives of the individual elements; that is, it is an 
array of higher rank. 
Derivatives of Matrices with Respect to Scalars 
The derivative of the matrix Y (x) = (yij) with respect to the scalar x is the 
matrix 
partial differential upper Y left parenthesis x right parenthesis divided by partial differential x equals left parenthesis partial differential y Subscript i j Baseline divided by partial differential x right parenthesis period∂Y (x)/∂x = (∂yij/∂x).
(7.6) 
The second or higher derivative of a matrix with respect to a scalar is 
likewise a matrix of the derivatives of the individual elements. 
Derivatives of Functions with Respect to Scalars 
Diﬀerentiation of a function of a vector or matrix that is linear in the elements 
of the vector or matrix involves just the diﬀerentiation of the elements, fol-
lowed by application of the function. For example, the derivative of a trace of 
a matrix is just the trace of the derivative of the matrix. On the other hand, 
the derivative of the determinant of a matrix is not the determinant of the 
derivative of the matrix (see below).

330
7 Real Analysis and Probability Distributions of Vectors and Matrices
Higher-Order Derivatives with Respect to Scalars 
Because diﬀerentiation with respect to a scalar does not change the rank 
of the object (“rank” here means rank of an array or “shape”), higher-order 
derivatives ∂k /∂xk with respect to scalars are merely objects of the same rank 
whose elements are the higher-order derivatives of the individual elements. 
7.2.2 Diﬀerentiation with Respect to a Vector 
Diﬀerentiation of a given object with respect to an n-vector yields a vector 
for each element of the given object. The basic expression for the derivative, 
from formula (7.2), is 
limit Underscript t right arrow 0 Endscripts StartFraction normal upper Phi left parenthesis x plus t y right parenthesis minus normal upper Phi left parenthesis x right parenthesis Over t EndFraction lim
t→0
Φ(x + ty) −Φ(x)
t
(7.7) 
for an arbitrary conformable vector y. The arbitrary y indicates that the 
derivative is omnidirectional; it is the rate of change of a function of the 
vector in any direction. 
Derivatives of Scalars with Respect to Vectors; The Gradient 
The derivative of a scalar-valued function with respect to a vector is a vector 
of the partial derivatives of the function with respect to the elements of the 
vector. If f(x) is a scalar function of the vector x = (x1, . . . , xn), 
StartFraction partial differential f Over partial differential x EndFraction equals left parenthesis StartFraction partial differential f Over partial differential x 1 EndFraction comma ellipsis comma StartFraction partial differential f Over partial differential x Subscript n Baseline EndFraction right parenthesis comma∂f
∂x =
( ∂f
∂x1
, . . . , ∂f
∂xn
)
,
(7.8) 
if those derivatives exist. This vector is called the gradient of the scalar-valued 
function and is sometimes denoted by gf(x) or  ∇f(x) or sometimes just gf 
or ∇f: 
normal g Subscript f Baseline equals nabla f equals StartFraction partial differential f Over partial differential x EndFraction periodgf = ∇f = ∂f
∂x.
(7.9) 
The notation gf or ∇f implies diﬀerentiation with respect to “all” arguments 
of f; hence, if f is a scalar-valued function of a vector argument, they represent 
a vector. The symbol ∇ with this interpretation is called “nabla.” 
This derivative is useful in ﬁnding the maximum or minimum of a func-
tion. Such applications arise throughout statistical and numerical analysis. In 
Sect. 5.3.2, we will discuss a method of solving linear systems of equations by 
formulating the problem as a minimization problem. 
Inner products, bilinear forms, norms, and variances are interesting scalar-
valued functions of vectors. In these cases, the function Φ in Eq. (7.7) is scalar-
valued, and the numerator is merely Φ(x + ty) −Φ(x). Consider, for example, 
the quadratic form xT Ax. Using  Eq. (7.7) to evaluate  ∂xT Ax/∂x, we have

7.2 Types of Diﬀerentiation
331
StartLayout 1st Row 1st Column limit Underscript t right arrow 0 Endscripts StartFraction left parenthesis x plus t y right parenthesis Superscript normal upper T Baseline upper A left parenthesis x plus t y right parenthesis minus x Superscript normal upper T Baseline upper A x Over t EndFraction 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals limit Underscript t right arrow 0 Endscripts StartFraction x Superscript normal upper T Baseline upper A x plus t y Superscript normal upper T Baseline upper A x plus t y Superscript normal upper T Baseline upper A Superscript normal upper T Baseline x plus t squared y Superscript normal upper T Baseline upper A y minus x Superscript normal upper T Baseline upper A x Over t EndFraction 4th Row 1st Column Blank 5th Row 1st Column Blank 2nd Column equals y Superscript normal upper T Baseline left parenthesis upper A plus upper A Superscript normal upper T Baseline right parenthesis x comma EndLayout
lim
t→0
(x + ty)TA(x + ty) −xTAx
t
= lim
t→0
xTAx + tyTAx + tyTATx + t2yTAy −xTAx
t
= yT(A + AT)x,
(7.10) 
for an arbitrary y (i.e., “in any direction”), and so ∂xT Ax/∂x = (A + AT )x. 
This immediately yields the derivative of the square of the Euclidean norm 
of a vector, ||x||2 
2, and the derivative of the Euclidean norm itself by using 
the chain rule. Other Lp vector norms may not be diﬀerentiable everywhere 
because of the presence of the absolute value in their deﬁnitions. The fact that 
the Euclidean norm is diﬀerentiable everywhere is one of its most important 
properties. 
The derivative of the quadratic form also immediately yields the derivative 
of the variance. The derivative of the correlation, however, is slightly more 
diﬃcult because it is a ratio (see Exercise 7.2). 
The operator ∂/∂xT applied to the scalar function f results in gT 
f . 
The second derivative of a scalar-valued function with respect to a vector 
is a derivative of the ﬁrst derivative, which is a vector. We will now consider 
derivatives of vectors with respect to vectors. 
Derivatives of Vectors with Respect to Vectors; The Jacobian 
The derivative of an m-vector-valued function of an n-vector argument con-
sists of nm scalar derivatives. These derivatives could be put into various 
structures. Two obvious structures are an n×m matrix and an m×n matrix. 
For a function f : S ⊆ IRn → IRm , we deﬁne ∂f T /∂x to be the n × m ma-
trix, which is the natural extension of ∂/∂x applied to a scalar function, and 
∂f/∂xT to be its transpose, the m×n matrix. Although the notation ∂f T /∂x 
is more precise because it indicates that the elements of f correspond to the 
columns of the result, we often drop the transpose in the notation. We have 
StartLayout 1st Row 1st Column StartFraction partial differential f Over partial differential x EndFraction 2nd Column equals 3rd Column StartFraction partial differential f Superscript normal upper T Baseline Over partial differential x EndFraction normal b normal y normal c normal o normal n normal v normal e normal n normal t normal i normal o normal n 2nd Row 1st Column Blank 2nd Column equals 3rd Column left bracket StartFraction partial differential f 1 Over partial differential x EndFraction ellipsis StartFraction partial differential f Subscript m Baseline Over partial differential x EndFraction right bracket 3rd Row 1st Column Blank 4th Row 1st Column Blank 2nd Column equals 3rd Column Start 5 By 4 Matrix 1st Row 1st Column StartFraction partial differential f 1 Over partial differential x 1 EndFraction 2nd Column StartFraction partial differential f 2 Over partial differential x 1 EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential f Subscript m Baseline Over partial differential x 1 EndFraction 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 3rd Row 1st Column StartFraction partial differential f 1 Over partial differential x 2 EndFraction 2nd Column StartFraction partial differential f 2 Over partial differential x 2 EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential f Subscript m Baseline Over partial differential x 2 EndFraction 4th Row 1st Column Blank 2nd Column midline horizontal ellipsis 3rd Column Blank 5th Row 1st Column StartFraction partial differential f 1 Over partial differential x Subscript n Baseline EndFraction 2nd Column StartFraction partial differential f 2 Over partial differential x Subscript n Baseline EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential f Subscript m Baseline Over partial differential x Subscript n Baseline EndFraction EndMatrix EndLayout∂f
∂x = ∂f T
∂x
by convention
=
[∂f1
∂x . . . ∂fm
∂x
]
=
⎡
⎢⎢⎢⎢⎢⎣
∂f1
∂x1
∂f2
∂x1 · · · ∂fm
∂x1
∂f1
∂x2
∂f2
∂x2 · · · ∂fm
∂x2
· · ·
∂f1
∂xn
∂f2
∂xn · · · ∂fm
∂xn
⎤
⎥⎥⎥⎥⎥⎦
(7.11) 
if those derivatives exist. This derivative is called the matrix gradient and is 
denoted by Gf or ∇f for the vector-valued function f. (Note that the nabla

332
7 Real Analysis and Probability Distributions of Vectors and Matrices
symbol can denote either a vector or a matrix, depending on whether the 
function being diﬀerentiated is scalar-valued or vector-valued.) 
The m × n matrix ∂f/∂xT = (∇f)T is called the Jacobian of f and is 
denoted by Jf: 
normal upper J Subscript f Baseline equals normal upper G Subscript f Superscript normal upper T Baseline equals left parenthesis nabla f right parenthesis Superscript normal upper T Baseline periodJf = GT
f = (∇f)T.
(7.12) 
The absolute value of the determinant of the Jacobian appears in integrals 
involving a change of variables. (Occasionally, the term “Jacobian” is used 
to refer to the absolute value of the determinant rather than to the matrix 
itself.) We will discuss the role of the Jacobian in integration in Sect. 7.3.2 
beginning on page 338. 
To emphasize that the quantities are functions of x, we sometimes write 
∂f(x)/∂x, Jf(x), Gf(x), or ∇f(x). 
Derivatives of Vectors with Respect to Vectors in IR3 ; The  
Divergence and the Curl 
In some applications in which the order of f and x are the same, the trace of 
the matrix gradient is of interest. This is particularly the case in IR3 , where,  
for example, the three elements of f may represent the expansion of a gas in 
the three directions of an orthogonal Cartesian coordinate system. The change 
in the density of the gas at the point x is the sum of the changes of the density 
in each of the orthogonal directions: 
StartFraction partial differential f 1 Over partial differential x 1 EndFraction left parenthesis x right parenthesis plus StartFraction partial differential f 2 Over partial differential x 2 EndFraction left parenthesis x right parenthesis plus StartFraction partial differential f 3 Over partial differential x 3 EndFraction left parenthesis x right parenthesis equals normal t normal r left parenthesis nabla f left parenthesis x right parenthesis right parenthesis period∂f1
∂x1
(x) + ∂f2
∂x2
(x) + ∂f3
∂x3
(x) = tr(∇f(x)).
This type of expression arises so often in physical applications that it is 
given a name, “divergence,” and a special symbol for the operation on the 
function f, div. Another expression for the divergence that is common in 
physics arises from the interpretation of ∇ as an ordered list of operators, 
(∂/∂x1, ∂/∂x2, . . .). The nabla symbol with this interpretation is called “del.” 
The application of ∇ in this form to the function f is analogous to the dot 
product of ∇ and f; hence, it is often written as ∇·  f; that is, in diﬀerent 
notations, we have 
nabla dot f equals normal d normal i normal v left parenthesis f right parenthesis equals normal t normal r left parenthesis nabla f right parenthesis period∇· f = div(f) = tr(∇f).
In physical applications, when the vector function f above is actually the 
gradient of a scalar function, the divergence of f determines some interesting 
characteristics of the underlying scalar function. For example, if u(x) is the  
temperature at x, then the gradient ∇u is the change in temperature mea-
sured in each of the components of x. The divergence of this gradient can be 
interpreted as the total change in the heat. In a closed system, this should 
be zero, and if the total heat is changing, this should be proportional to the 
change in heat. (This fact is called the “heat equation.”) There are similar 
situations in which the divergence of the gradient of a scalar function is of 
interest.

7.2 Types of Diﬀerentiation
333
The operator representing the operation just described is called the 
Laplace operator or Laplacian operator. In the notation above, it can be rep-
resented as ∇· ∇u, and so the operator itself is sometimes represented as ∇2 , 
which is called “del-squared”; however, it is sometimes also represented as Δ. 
We also use the symbol ∇2 to denote the Hessian of a scalar function. In that 
context, it is called “nabla-squared.” The Laplace operator as deﬁned here is 
the trace of the Hessian matrix as deﬁned in Eq. (7.16). 
There are many important physical applications and special operations for 
vectors in IR3 , as we have discussed in Sect. 2.2.9, beginning on page 56. In  
particular, we deﬁned the cross product of the vectors x and y, as  
x times y equals left parenthesis x 2 y 3 minus x 3 y 2 comma x 3 y 1 minus x 1 y 3 comma x 1 y 2 minus x 2 y 1 right parenthesis periodx × y = (x2y3 −x3y2, x3y1 −x1y3, x1y2 −x2y1).
Now, suppose that y is a 3-vector function of x, say,  f(x), and as we did with 
the del operator ∇ = (∂/∂x1, ∂/∂x2, ∂/∂x3) above, let us consider a formal 
substitution: 
nabla times f equals left parenthesis partial differential f 3 divided by partial differential x 2 minus partial differential f 2 divided by partial differential x 3 comma partial differential f 1 divided by partial differential x 3 minus partial differential f 3 divided by partial differential x 1 comma partial differential f 2 divided by partial differential x 1 minus partial differential f 1 divided by partial differential x 2 right parenthesis period∇× f = (∂f3/∂x2 −∂f2/∂x3,
∂f1/∂x3 −∂f3/∂x1,
∂f2/∂x1 −∂f1/∂x2).
We call this the curl of f with respect to x and write 
normal c normal u normal r normal l left parenthesis f right parenthesis equals nabla times f periodcurl(f) = ∇× f.
The uses of the curl in both physical and geometrical applications are far-
reaching, but we will not go into them here. An interesting mathematical fact 
is that the curl of the gradient of a twice-diﬀerential scalar function is 0: 
normal c normal u normal r normal l left parenthesis normal g Subscript f Baseline right parenthesis equals 0 commacurl(gf) = 0,
(7.13) 
where f is a twice-diﬀerential scalar function. You are asked to show this in 
Exercise 7.3. 
In IR3 , for a vector function f that is twice diﬀerential, there is an inter-
esting relationship between the divergence and the curl: 
normal d normal i normal v left parenthesis normal c normal u normal r normal l left parenthesis f right parenthesis right parenthesis equals 0div(curl(f)) = 0
(7.14) 
which you are asked to show in Exercise 7.4. 
Derivatives of Matrices with Respect to Vectors 
The derivative of a matrix with respect to a vector is a three-dimensional 
object that results from applying Eq. (7.8) to each of the elements of the 
matrix. For this reason, it is simpler to consider only the partial derivatives 
of the matrix Y with respect to the individual elements of the vector x, that  
is, ∂Y/∂xi. The expressions involving the partial derivatives can be thought 
of as deﬁning one two-dimensional layer of a three-dimensional object.

334
7 Real Analysis and Probability Distributions of Vectors and Matrices
Using the rules for diﬀerentiation of powers that result directly from the 
deﬁnitions, we can write the partial derivatives of the inverse of the matrix Y 
as 
StartFraction partial differential Over partial differential x EndFraction upper Y Superscript negative 1 Baseline equals minus upper Y Superscript negative 1 Baseline left parenthesis StartFraction partial differential Over partial differential x EndFraction upper Y right parenthesis upper Y Superscript negative 1 ∂
∂xY −1 = −Y −1
( ∂
∂xY
)
Y −1
(7.15) 
(see Exercise 7.5). 
Beyond the basics of diﬀerentiation of constant multiples or powers of a 
variable, the two most important properties of derivatives of expressions are 
the linearity of the operation and the chaining of the operation. These yield 
rules that correspond to the familiar rules of the diﬀerential calculus. A simple 
result of the linearity of the operation is the rule for diﬀerentiation of the trace: 
StartFraction partial differential Over partial differential x EndFraction normal t normal r left parenthesis upper Y right parenthesis equals normal t normal r left parenthesis StartFraction partial differential Over partial differential x EndFraction upper Y right parenthesis period ∂
∂xtr(Y ) = tr
( ∂
∂xY
)
.
Higher-Order Derivatives with Respect to Vectors; The Hessian 
Higher-order derivatives are derivatives of lower-order derivatives. As we have 
seen, a derivative of a given function with respect to a vector is a more compli-
cated object than the original function. The simplest higher-order derivative 
with respect to a vector is the second-order derivative of a scalar-valued func-
tion. Higher-order derivatives may become uselessly complicated. 
In accordance with the meaning of derivatives of vectors with respect to 
vectors, the second derivative of a scalar-valued function with respect to a 
vector is a matrix of the partial derivatives of the function with respect to the 
elements of the vector. This matrix is called the Hessian and is denoted by 
Hf or sometimes by ∇∇f or ∇2 f (“nabla-squared,” not to confused with the 
Laplace operator, “del-squared”): 
normal upper H Subscript f Baseline equals StartFraction partial differential squared f Over partial differential x partial differential x Superscript normal upper T Baseline EndFraction equals Start 5 By 4 Matrix 1st Row 1st Column StartFraction partial differential squared f Over partial differential x 1 squared EndFraction 2nd Column StartFraction partial differential squared f Over partial differential x 1 partial differential x 2 EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential squared f Over partial differential x 1 partial differential x Subscript m Baseline EndFraction 2nd Row 1st Column Blank 2nd Column Blank 3rd Column Blank 4th Column Blank 3rd Row 1st Column StartFraction partial differential squared f Over partial differential x 2 partial differential x 1 EndFraction 2nd Column StartFraction partial differential squared f Over partial differential x 2 squared EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential squared f Over partial differential x 2 partial differential x Subscript m Baseline EndFraction 4th Row 1st Column Blank 2nd Column midline horizontal ellipsis 3rd Column Blank 5th Row 1st Column StartFraction partial differential squared f Over partial differential x Subscript m Baseline partial differential x 1 EndFraction 2nd Column StartFraction partial differential squared f Over partial differential x Subscript m Baseline partial differential x 2 EndFraction 3rd Column midline horizontal ellipsis 4th Column StartFraction partial differential squared f Over partial differential x Subscript m Superscript 2 Baseline EndFraction EndMatrix periodHf =
∂2f
∂x∂xT =
⎡
⎢⎢⎢⎢⎢⎢⎣
∂2f
∂x2
1
∂2f
∂x1∂x2 · · ·
∂2f
∂x1∂xm
∂2f
∂x2∂x1
∂2f
∂x2
2
· · ·
∂2f
∂x2∂xm
· · ·
∂2f
∂xm∂x1
∂2f
∂xm∂x2 · · ·
∂2f
∂x2
m
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(7.16) 
To emphasize that the Hessian is a function of x, we sometimes write 
Hf(x) or  ∇∇f(x) or  ∇2 f(x). 
Summary of Derivatives with Respect to Vectors 
As we have seen, the derivatives of functions are complicated by the problem 
of measuring the change in the function, but often the derivatives of functions 
with respect to a vector can be determined by using familiar scalar diﬀeren-
tiation. In general, we see that:

7.2 Types of Diﬀerentiation
335
• The derivative of a scalar (a quadratic form) with respect to a vector is a 
vector 
• The derivative of a vector with respect to a vector is a matrix 
Table 7.1 lists formulas for the vector derivatives of some common expres-
sions. The derivative ∂f/∂xT is the transpose of ∂f/∂x. 
Table 7.1. Formulas for some vector derivatives 
f(x)
∂f/∂x 
ax
aI 
bT x
b
 
xT b
b
 
xT x
2x 
xxT 
2xT 
bT Ax
AT b 
xT Ab
Ab 
xT Ax
(A + AT )x 
2Ax, if  A is symmetric 
exp(−1 
2 xT Ax) 
−exp(−1 
2 xT Ax)Ax, if  A is symmetric
||x||2 
2
2x 
In this table, x is an n-vector, a is a constant scalar, b is a 
constant conformable vector, and A is a constant conformable 
matrix 
As noted above, some authors express derivatives in diﬀerent structures. 
The values of the individual elements are the same, but their organization in 
a vector or matrix may be diﬀerent. 
7.2.3 Diﬀerentiation with Respect to a Matrix 
The derivative of a function with respect to a matrix is a matrix with the same 
shape consisting of the partial derivatives of the function with respect to the 
elements of the matrix. This rule deﬁnes what we mean by diﬀerentiation with 
respect to a matrix. 
For scalar-valued functions, this rule is fairly simple: 
StartFraction partial differential f left parenthesis upper X right parenthesis Over partial differential upper X EndFraction equals left parenthesis StartFraction partial differential f left parenthesis upper X right parenthesis Over partial differential x Subscript i j Baseline EndFraction right parenthesis period∂f(X)
∂X
=
(∂f(X)
∂xij
)
.
(7.17) 
For example, consider the trace. If X is a square matrix and we apply this 
rule to evaluate ∂ tr(X)/∂X, we get the identity matrix, where the nonzero 
elements arise only when j = i in ∂(E xii)/∂xij. If  AX is a square matrix, 
we have for the (i, j) term in ∂ tr(AX)/∂X, ∂E
i
E
k aikxki/∂xij = aji, and

336
7 Real Analysis and Probability Distributions of Vectors and Matrices
so ∂ tr(AX)/∂X = AT , and likewise, inspecting ∂E
i
E
k xikxki/∂xij, we get  
∂ tr(XT X)/∂X = 2XT . Likewise for the scalar-valued aT Xb, where  a and 
b are conformable constant vectors, for ∂E
m(E
k akxkm)bm/∂xij = aibj, so  
∂aT Xb/∂X = abT . 
Now consider ∂det(X)/∂X. Using an expansion in cofactors (Eq. (3.37) 
or (3.38)), the only term in det(X) that involves xij is xij(−1)i+j det(X−(i)(j)), 
and the cofactor (x(ij)) = (−1)i+j det(X−(i)(j)) does not involve xij. Hence, 
∂det(X)/∂xij = (x(ij)), and so ∂det(X)/∂X = (adj(X))T from Eq. (3.42). 
Using Eq. (3.186), we can write this as ∂det(X)/∂X = det(X)X−T . 
By the deﬁnition of diﬀerentiation with respect to a matrix X, we see that 
the derivative ∂f/∂XT is the transpose of ∂f/∂X. 
The chain rule can be used to evaluate ∂ log(det(X))/∂X. 
Applying the rule stated at the beginning of this section, we see that the 
derivative of a matrix Y with respect to the matrix X is 
StartFraction normal d upper Y Over normal d upper X EndFraction equals StartFraction normal d left parenthesis normal v normal e normal c left parenthesis upper Y right parenthesis right parenthesis Over normal d left parenthesis normal v normal e normal c left parenthesis upper X right parenthesis right parenthesis EndFraction comma dY
dX = d(vec(Y ))
d(vec(X)),
(7.18) 
or 
StartFraction normal d upper Y Over normal d upper X EndFraction equals upper Y circled times StartFraction normal d Over normal d upper X EndFraction comma dY
dX = Y ⊗d
dX ,
(7.19) 
especially in statistical applications. (Recall the comment above about the use 
by diﬀerent authors of diﬀerent structures for derivatives.) 
Tables 7.2 and 7.3 list some formulas for the matrix derivatives of some 
common expressions. The derivatives shown in those tables can be obtained 
by using Eq. (7.17) or (7.18), possibly also using the chain rule. 
Table 7.2. Formulas for some matrix derivatives 
General X 
f(X)
∂f/∂X 
aT Xb
abT 
tr(AX)
AT 
tr(XT X)
2XT 
BX
In ⊗ B 
XC
CT ⊗ Im 
BXC
CT ⊗ B 
In this table, X is an n × m matrix, a is a 
constant n-vector, b is a constant m-vector, 
A is a constant m×n matrix, B is a constant 
p×n matrix, and C is a constant m×q matrix 

7.3 Integration
337
Table 7.3. Formulas for some matrix derivatives 
Square and possibly invertible X 
f(X)
∂f/∂X 
tr(X)
In 
tr(Xk )
kXk−1 
tr(BX−1 C)
−(X−1 CBX−1 )T 
det(X)
det(X)X−T 
log(det(X))
X−T 
(det(X))k 
k(det(X))k X−T 
BX−1 C
−(X−1 C)T ⊗ BX−1 
In this table, X is an n × n matrix, B is a 
constant p × n matrix, and C is a constant 
n × q matrix 
7.3 Integration 
Just as we can take derivatives with respect to vectors or matrices, we can also 
take antiderivatives and deﬁnite integrals with respect to vectors or matrices. 
Our interest is in the integration of functions weighted by a multivariate prob-
ability density function, and for our purposes, we will be interested only in 
deﬁnite integrals. 
Again, there are three components: 
• The diﬀerential (the variable of the operation) and its domain (the range 
of the integration) 
• The integrand (the function) 
• The result of the operation (the integral) 
In the simplest case, all three of these objects are of the same type; they are 
scalars. In the happy cases that we consider, each deﬁnite integral within a 
nested sequence of diﬀerentials exists, so convergence and order of integration 
are not issues. (The implication of these remarks is that while there is a much 
bigger ﬁeld of mathematics here, we are concerned about the relatively simple 
cases that suﬃce for our purposes.) 
In this section, we consider some general issues relating to integration 
with respect to a vector. In Sect. 7.4.1, we consider applications to probabil-
ity, speciﬁcally, to the computation of moments of a vector-valued random 
variable. The moments of a scalar-valued random variable X are deﬁned as 
E(Xk ) (where the expectation operator, E(·), is an integral, as deﬁned in 
Eq. (7.27)). Obviously, the moments of vector-valued random variables must 
be deﬁned slightly diﬀerently. 
In some cases of interest involving vector-valued random variables, the 
diﬀerential is the vector representing the values of the random variable, and

338
7 Real Analysis and Probability Distributions of Vectors and Matrices
the integrand has a scalar function (the probability density) as a factor. In one 
type of such an integral, the integrand is only the probability density function, 
and the integral evaluates to a probability, which of course is a scalar. In 
another type of such an integral, the integrand is a vector representing the 
values of the random variable times the probability density function. The 
integral in this case evaluates to a vector, namely, the expectation of the 
random variable over the domain of the integration. Finally, in an example of 
a third type of such an integral, the integrand is an outer product with itself 
of a vector representing the values of the random variable minus its mean 
times the probability density function. The integral in this case evaluates to 
a variance-covariance matrix. In each of these cases, the integral is the same 
type of object as the integrand. 
7.3.1 Multidimensional Integrals and Integrals Involving Vectors 
and Matrices 
An integral of the form
f
f(v) dv, where  v is a vector, can usually be evaluated 
as a multiple integral with respect to each diﬀerential dvi. Likewise, an integral 
of the form
f
f(M) dM, where  M is a matrix can usually be evaluated by 
“unstacking” the columns of dM, evaluating the integral as a multiple integral 
with respect to each diﬀerential dmij, and then possibly “restacking” the 
result. 
Probabilities and expectations in multivariate probability distributions are 
deﬁned in terms of multivariate integrals. As with many well-known univariate 
integrals, such as Γ(·), that relate to univariate probability distributions, there 
are standard multivariate integrals, such as the multivariate gamma, Γd(·), 
that relate to multivariate probability distributions. Using standard integrals 
often facilitates the computations. 
7.3.2 Change of Variables; The Jacobian 
When evaluating an integral of the form
f
f(x) dx, where  x is a vector, for 
various reasons, we may form a one-to-one diﬀerentiable transformation of 
the variables of integration, that is, of x. We write  x as a function of the new 
variables; that is, x = g(y), and so y = g−1 (x). A simple fact from elementary 
multivariate calculus is 
integral Underscript upper R left parenthesis x right parenthesis Endscripts f left parenthesis x right parenthesis normal d x equals integral Underscript upper R left parenthesis y right parenthesis Endscripts f left parenthesis g left parenthesis y right parenthesis right parenthesis StartAbsoluteValue normal d normal e normal t left parenthesis normal upper J Subscript g Baseline left parenthesis y right parenthesis right parenthesis EndAbsoluteValue normal d y comma
f
R(x)
f(x) dx =
f
R(y)
f(g(y)) |det(Jg(y))|dy,
(7.20) 
where R(y) is the image of R(x) under g−1 and Jg(y) is the Jacobian of g (see 
Eq. (7.12)). (This is essentially a chain rule result for dx = d(g(y)) = Jgdy 
under the interpretation of dx and dy as positive diﬀerential elements and the 
interpretation of |det(Jg)| as a volume element, as discussed on page 94.) 
In the simple case of a full rank linear transformation of a vector, the 
Jacobian is constant, and so for y = Ax with A a ﬁxed matrix, we have

7.3 Integration
339
integral f left parenthesis x right parenthesis normal d x equals StartAbsoluteValue normal d normal e normal t left parenthesis upper A right parenthesis EndAbsoluteValue Superscript negative 1 Baseline integral f left parenthesis upper A Superscript negative 1 Baseline y right parenthesis normal d y period
f
f(x) dx = |det(A)|−1
f
f(A−1y) dy.
(7.21) 
(Note that we write det(A) instead of |A| for the determinant if we are to 
take the absolute value of it because otherwise we would have ||A||, which  is  
a symbol for a norm. However, |det(A)| is not a norm; it lacks each of the 
properties listed on page 35.) 
In the case of a full rank linear transformation of a matrix variable of 
integration, the Jacobian is somewhat more complicated, but the Jacobian is 
constant for a ﬁxed transformation matrix. For a transformation Y = AX, 
we determine the Jacobian as above by considering the columns of X one by 
one. Hence, if X is an n × m matrix and A is a constant nonsingular matrix, 
we have 
integral f left parenthesis upper X right parenthesis normal d upper X equals StartAbsoluteValue normal d normal e normal t left parenthesis upper A right parenthesis EndAbsoluteValue Superscript negative m Baseline integral f left parenthesis upper A Superscript negative 1 Baseline upper Y right parenthesis normal d upper Y period
f
f(X) dX = |det(A)|−m
f
f(A−1Y ) dY.
(7.22) 
For a transformation of the form Z = XB, we determine the Jacobian by 
considering the rows of X one by one. 
7.3.3 Integration Combined with Other Operations 
Integration and another ﬁnite linear operator can generally be performed in 
any order. For example, because the trace is a ﬁnite linear operator, integra-
tion and the trace can be performed in either order: 
integral normal t normal r left parenthesis upper A left parenthesis x right parenthesis right parenthesis normal d x equals normal t normal r left parenthesis integral upper A left parenthesis x right parenthesis normal d x right parenthesis period
f
tr(A(x))dx = tr
(f
A(x)dx
)
.
(7.23) 
For a scalar function of two vectors x and y, it is often of interest to perform 
diﬀerentiation with respect to one vector and integration with respect to the 
other vector. In such cases, it is of interest to know when these operations 
can be interchanged. The answer is given in the following theorem, which is 
a consequence of the Lebesgue dominated convergence theorem. Its proof can 
be found in any standard text on real analysis. 
Let X be an open set, and let f(x, y) and  ∂f/∂x be scalar-valued 
functions that are continuous on X × Y  for some set Y. Now suppose 
there are scalar functions g0(y) and  g1(y) such that 
StartLayout 1st Row StartAbsoluteValue f left parenthesis x comma y right parenthesis EndAbsoluteValue less than or equals g 0 left parenthesis y right parenthesis 2nd Row Blank 3rd Row parallel to StartFraction partial differential Over partial differential x EndFraction f left parenthesis x comma y right parenthesis parallel to less than or equals g 1 left parenthesis y right parenthesis EndLayout right brace f normal o normal r normal a normal l normal l left parenthesis x comma y right parenthesis element of script upper X times script upper Y comma
|f(x, y)| ≤g0(y)
|| ∂
∂xf(x, y)|| ≤g1(y)
⎫
⎬
⎭
for all (x, y) ∈X × Y,
integral Underscript script upper Y Endscripts g 0 left parenthesis y right parenthesis normal d y less than normal infinity comma
f
Y
g0(y) dy < ∞,
and 
integral Underscript script upper Y Endscripts g 1 left parenthesis y right parenthesis normal d y less than normal infinity period
f
Y
g1(y) dy < ∞.

340
7 Real Analysis and Probability Distributions of Vectors and Matrices
Then 
StartFraction partial differential Over partial differential x EndFraction integral Underscript script upper Y Endscripts f left parenthesis x comma y right parenthesis normal d y equals integral Underscript script upper Y Endscripts StartFraction partial differential Over partial differential x EndFraction f left parenthesis x comma y right parenthesis normal d y period ∂
∂x
f
Y
f(x, y) dy =
f
Y
∂
∂xf(x, y) dy.
(7.24) 
An important application of this interchange is in developing the information 
inequality for “regular” families of probability distributions. 
7.4 Multivariate Probability Theory 
To many statisticians, “real analysis” means “probability theory.” 
Most methods of statistical inference are based on assumptions about some 
underlying probability distribution of a random variable. In some cases, these 
assumptions completely specify the form of the distribution, and in other 
cases, especially in nonparametric methods, the assumptions are more general. 
Many statistical methods in estimation and hypothesis testing rely on the 
properties of various transformations of a random variable. 
In this section, we do not attempt to develop a theory of probability dis-
tribution; rather, we assume some basic facts and then derive some important 
properties that depend on the matrix theory of the previous chapters. 
7.4.1 Random Variables and Probability Distributions 
Note on notation: I generally use uppercase letters to represent matrices 
and lowercase letters to represent vectors and scalars. I also generally use 
uppercase letters to represent random variables and corresponding lowercase 
letters to represent their realizations. I will usually follow this convention in 
this section, but where the two conventions come in conﬂict, I will give prece-
dence to the uppercase representation; that is, an uppercase letter may be a 
matrix, random or ﬁxed, or it may be a random variable with any structure. I 
also generally use Greek letters to represent parameters of probability distribu-
tions, and for parameters, I also use my convention of uppercase for matrices 
and lowercase for vectors or scalars. 
Random variables are functions from a “sample space” into the reals. A 
d-vector random variable is a function from some sample space into IRd , and  
an n × d matrix random variable is a function from a sample space into 
IRn×d . (Technically, in each case, the function is required to be measurable 
with respect to a measure deﬁned in the context of the sample space and an 
appropriate collection of subsets of the sample space. A careful development 
of the theory is available in Gentle (2023) at  
https://mason.gmu.edu/~jgentle/books/MathStat.pdf.) 
Associated with each vector or matrix random variable is a distribution 
function, or cumulative distribution function (CDF), which represents the 
probability that the random variable takes values less than or equal to a

7.4 Multivariate Probability Theory
341
speciﬁed value. Let X be a random variable. The CDF of X, call it  FX(·), is 
deﬁned as 
upper F Subscript upper X Baseline left parenthesis x right parenthesis equals normal upper P normal r left parenthesis upper X less than or equals x right parenthesis commaFX(x) = Pr(X ≤x),
(7.25) 
where Pr(·) denotes “probability of · .” 
The Distribution Function and Probability Density Function 
The derivative of the distribution function with respect to an appropriate 
measure is nonnegative and integrates to 1 over the full space formed by 
IR (i.e., IR, IRd , or IRn×d ). This derivative is called the probability density 
function, or  PDF. (There are some technical issues that arise in some cases, 
but I will ignore those issues here.) 
If X is a random variable, and fX(x) is the  PDF of  X, we have  
integral Underscript upper D left parenthesis upper X right parenthesis Endscripts f Subscript upper X Baseline left parenthesis x right parenthesis normal d x equals 1 comma
f
D(X)
fX(x) dx = 1,
(7.26) 
where D(X) is the set of the image of X in which fX(x) > 0 and is called the 
support of the distribution. 
Expected Values; The Expectation Operator 
The expected value of a function g of the random variable X is 
normal upper E left parenthesis f left parenthesis upper X right parenthesis right parenthesis equals integral Underscript upper D left parenthesis upper X right parenthesis Endscripts g left parenthesis x right parenthesis f Subscript upper X Baseline left parenthesis x right parenthesis normal d x commaE(f(X)) =
f
D(X)
g(x)fX(x) dx,
(7.27) 
if the integral exists. 
We see immediately that the expected value operator is linear; that is, if a 
is independent of X and consists of real elements of the appropriate structure 
that ag(X) makes sense (and exists), and if h(X) has the structure such that 
ag(X) +  h(X) makes sense (and exists), then 
normal upper E left parenthesis a g left parenthesis upper X right parenthesis plus h left parenthesis upper X right parenthesis right parenthesis equals a normal upper E left parenthesis g left parenthesis upper X right parenthesis right parenthesis plus normal upper E left parenthesis h left parenthesis upper X right parenthesis right parenthesis periodE (ag(X) + h(X)) = aE(g(X)) + E(h(X)).
(7.28) 
Some special forms of g in Eq. (7.27) yield the (raw) moments of the ran-
dom variable. In the case of a scalar random variable, these are just the power 
functions, and E(Xk ) is the  kth moment of X. 
Expected Values; Generating Functions 
Other special expectations that are useful in working with probability distri-
butions are the moment-generating function (MGF), 
psi Subscript upper X Baseline left parenthesis t right parenthesis equals normal upper E left parenthesis normal e Superscript t Super Superscript normal upper T Superscript upper X Baseline right parenthesis commaψX(t) = E
(
etTX)
,
(7.29)

342
7 Real Analysis and Probability Distributions of Vectors and Matrices
and the characteristic function, 
phi Subscript upper X Baseline left parenthesis t right parenthesis equals normal upper E left parenthesis normal e Superscript normal i t Super Superscript normal upper T Superscript upper X Baseline right parenthesis commaϕX(t) = E
(
eitTX)
,
(7.30) 
where t is a real object of appropriate dimension. In the case of the moment-
generating function, the domain of t must include an interval about 0, but 
may be limited in extent. 
These transforms are generating functions in the sense that they can be 
used to determine various properties of a probability distribution, such as its 
moments. Assuming certain regularity conditions on the CDF of the random 
variable X, it is easy to see that the raw moments of X are the derivatives of 
the MGF evaluated at 0; that is, 
StartFraction normal d Superscript k Baseline psi Subscript upper X Baseline left parenthesis t right parenthesis Over normal d t Superscript k Baseline EndFraction vertical bar Subscript t equals 0 Baseline equals normal upper E left parenthesis upper X Superscript k Baseline right parenthesis comma dkψX(t)
dtk
||||
t=0
= E(Xk),
(7.31) 
for k = 1, 2, . . .. 
An important use of these transforms is in identifying a probability distri-
bution because they have a one-to-one relation to distributions. The general 
forms of moment-generating or characteristics of the common distributions 
such as the normal or chi-squared are immediately recognizable. 
The moment-generating or characteristic function of a sum of iid random 
variables has a very simple and useful relationship to the moment-generating 
or characteristic function of a single random variable. Let X1, . . . , Xn be iid 
random variables with an MGF ψX(t). Now let Y = E
i Xi. It is easy to see  
from the deﬁnition of the MGF that 
psi Subscript upper Y Baseline left parenthesis t right parenthesis equals left parenthesis psi Subscript upper X Baseline left parenthesis t right parenthesis right parenthesis Superscript n Baseline periodψY (t) = (ψX(t))n.
(7.32) 
On page 351, we use this fact in relating chi-squared distributions with dif-
ferent degrees of freedom. 
Vector Random Variables 
Some authors use the term “random variable” to refer only to functions into 
IR. For a random variable with range in IRd with d >  1, those authors use 
the term “random vector.” I use the term random variable for variables in IRd 
with d ≥ 1. Deﬁnitions and other expressions involving vectors apply also to 
scalars if we sometimes blur the formal distinctions in scalars, vectors, and 
matrices, as we have mentioned earlier. For example, a second moment of a 
scalar x is formed from x2 , while a second moment of a vector x is formed 
from xxT . The second moment of a vector with one element is essentially the 
same as that of a scalar if we interpret a 1 × 1 matrix as a scalar. 
If the random variable X, and, consequently, x in Eq. (7.27), is a vector, 
then if we interpret
f
D(X) dx as a nest of univariate integrals, the result of the 
integration of the vector g(x)fX(x) is clearly  the same type of mathematical

7.4 Multivariate Probability Theory
343
object as g(x). For example, if g(x) =  x, the expectation is the mean, which 
is a vector. 
The ﬁrst consideration, for a vector-valued random variable X, is how  
should the second moment be represented. (This is not a question of what are 
the individual elements that must comprise this moment; those are obvious. 
It is merely a question of representation. Should we use the inner or outer 
product, e.g.?) Kollo and von Rosen (2005), page 173, suggest three obvious 
ways. The most commonly used method for the second moment of X is the 
expectation of the outer product, 
normal upper E left parenthesis upper X upper X Superscript normal upper T Baseline right parenthesis commaE(XXT),
(7.33) 
which in general  is  a matrix.  
From (7.33), we get the variance-covariance matrix for the vector random 
variable X, 
normal upper V left parenthesis upper X right parenthesis equals normal upper E left parenthesis left parenthesis upper X minus normal upper E left parenthesis upper X right parenthesis right parenthesis left parenthesis upper X minus normal upper E left parenthesis upper X right parenthesis right parenthesis Superscript normal upper T Baseline right parenthesis periodV(X) = E
(
(X −E(X))(X −E(X))T)
.
(7.34) 
The linearity of the expectation operator allows us to see immediately the 
expectation and variance of a linear transformation of the random variable 
X. Let  a be a ﬁxed vector. The expectation and variance of the inner product 
aT X are 
normal upper E left parenthesis a Superscript normal upper T Baseline upper X right parenthesis equals a Superscript normal upper T Baseline normal upper E left parenthesis upper X right parenthesis normal a normal n normal d normal upper V left parenthesis a Superscript normal upper T Baseline upper X right parenthesis equals a Superscript normal upper T Baseline a normal upper V left parenthesis upper X right parenthesis periodE(aTX) = aTE(X)
and
V(aTX) = aTaV(X).
(7.35) 
If A is a ﬁxed matrix not dependent on X, with a number of columns 
equal to the order of X, then we have  
normal upper E left parenthesis upper A upper X right parenthesis equals upper A normal upper E left parenthesis upper X right parenthesis normal a normal n normal d normal upper V left parenthesis upper A upper X right parenthesis equals upper A normal upper V left parenthesis upper X right parenthesis upper A Superscript normal upper T Baseline periodE(AX) = AE(X)
and
V(AX) = AV(X)AT.
(7.36) 
The form of V(AX) sometimes suggests estimators of the same general form, 
which leads to their description as “sandwich” estimators. 
The structure of V(X) may allow some simpliﬁcation of V(AX). A com-
mon assumption for a vector random variable is that each element has the 
same variance and the elements are all uncorrelated, that is, V(AX) =  σ2 I, 
where σ2 is a ﬁxed positive constant. 
In addition to just the ﬁrst and second moments, we are often interested 
in the probability distribution of a transformed random variable. We discuss 
methods for determining the distributions in Sect. 7.4.2. Our interest will be 
in linear transformations and quadratic forms. 
Matrix Random Variables 
The relationships of the elements of a matrix-valued random variable to each 
other may be useful in modeling interrelated populations. In some cases, a ma-
trix is the obvious structure for a random variable, such as a Wishart matrix 
for modeling the distribution of a sample variance-covariance matrix. Kollo

344
7 Real Analysis and Probability Distributions of Vectors and Matrices
and von Rosen (2005) take a matrix as the basic structure for a multivariate 
random variable, in particular, for a random variable with a multivariate nor-
mal distribution. (This is not the usual approach; we will follow it beginning 
on page 349, after ﬁrst developing the multivariate normal distribution of a 
vector beginning on page 346.) 
A matrix random variable is often useful in certain applications, such as 
when p n-variate random variables may be related. The columns of an n × p 
matrix-valued random variable X may correspond to these n-variate random 
variables. In applications, it may be reasonable to allow the columns to have 
diﬀerent means, but to assume that they have a common variance-covariance 
matrix Σ. The mean of X is the matrix M, whose columns, μ1, . . . , μp, are  
the means of the columns of X. 
The probability distributions governing random matrices determine vari-
ous properties of the matrices, such as whether or not the matrices are positive 
deﬁnite or the probability that the matrices are positive deﬁnite, for example. 
One fact that is fairly obvious is that if the individual elements of a matrix 
have continuous probability distributions, both marginal and all nondegener-
ate conditional distributions, then the probability that the matrix is full rank 
is 1. (You are asked to write a proof of this in Exercise 7.7.) 
Special Random Variables 
There are a few probability distributions, both univariate and multivariate, 
that occur frequently in statistical applications. One special distribution is 
the univariate uniform, in which the probability density is either 0 or it is a 
constant over some interval. We denote a univariate uniform random variable 
over the interval (a, b) as U(a, b). 
The most common interval we work with is (0, 1), and a U(0, 1) random 
variable has CDF FU(·) deﬁned by 
upper F Subscript upper U Baseline left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column 0 comma 2nd Column u less than or equals 0 semicolon 2nd Row 1st Column u comma 2nd Column 0 less than u less than or equals 1 semicolon 3rd Row 1st Column 1 comma 2nd Column u greater than 1 period EndLayoutFU(u) =
⎧
⎨
⎩
0, u ≤0;
u, 0 < u ≤1;
1, u > 1.
Any random variable with this CDF is stochastically equivalent to a U(0, 1) 
random variable. This univariate distribution has an immediate extension to 
a multivariate distribution over a unit hypercube. 
Other special distributions are the normal (or “Gaussian”) distribution 
and distributions derived from it. We consider the normal and some derived 
distributions in Sect. 7.4.3. 
7.4.2 Distributions of Transformations of Random Variables 
Given a random variable X, we are often interested in the probability distri-
bution a random variable deﬁned as some function of X, say  Y = g(X). In

7.4 Multivariate Probability Theory
345
order for Y to have a well-deﬁned probability distribution, g must be one-to-
one, or there must be a ﬁnite number of subsets of the range of X over which 
the function g is a one-to-one function. 
For a discrete random variable, we determine the probability function of 
Y by simple substitution into the probability function of X. For an abso-
lutely continuous random variable X, the basic idea is to determine how the 
probability density of X is spread over the range of Y . 
We can relate the probability density of Y to the probability density of 
X using g−1 , possibly over separate regions. Sometimes, this can be done 
by introducing another random variable. For example, if g(x) =  x2 and the 
support of X is IR, then g−1 (Y ) = (−1)α√
Y , where  α = 1 with probability 
Pr(X <  0) and α = 0  otherwise.  
There are several ways in which we can determine the probability distri-
bution of Y from the probability distribution of the absolutely continuous 
random variable X. We discuss three general ways below. 
Change-of-Variables Method 
The probability of Y within any region within its range can be evaluated using 
the probability density of X over a region of the range of X that g transforms 
into the given range of Y . The probability of Y within that region is an 
integral. The probability of Y in the region is the corresponding probability 
of X scaled by the Jacobian of g−1 (·), as in Eq. (7.12). Hence, we obtain 
the PDF of Y by just making the appropriate substitutions. The use of the 
Jacobian for this kind of scaling of within integrals is discussed in Sect. 7.3.2 
beginning on page 338. If the order of the vector X is greater than the order of 
Y , additional elements must be appended to Y so as that the transformation 
g is one-to-one. These additional elements are then integrated out after the 
PDF of the full vector has been obtained. (We illustrate this process beginning 
on page 338.) This technique is called change-of-variables. 
We will illustrate change-of-variables method below and again on pages 348 
and 351. 
Inverse CDF Method 
Another simple method is called the inverse CDF method. In this method, 
we make use of the simple fact that if X is an absolutely continuous random 
variable with CDF FX(·), then 
upper U equals upper F Subscript upper X Baseline left parenthesis upper X right parenthesisU = FX(X)
(7.37) 
is a univariate random variable with the uniform distribution over (0, 1). The 
inverse CDF method is a very useful method in random number generation, 
and we mention it in that context on page 355.

346
7 Real Analysis and Probability Distributions of Vectors and Matrices
We can show that U in Eq. (7.37) is U(0, 1) by deriving the PDF of U, 
fU. We will use the PDF of X, fX, and the change-of-variables method. First 
consider the set D(X) over which the probability density of X, fX, is positive 
and the complementary set D(X) in which it is 0. Wherever fX is 0, fU is 
likewise 0. Wherever fX is positive, 0 < FX < 1, and, since X is absolutely 
continuous, F −1 
X exists in those regions. The absolute value of the Jacobian 
of the inverse transformation is f −1 
X ; hence, we have 
f Subscript upper U Baseline left parenthesis u right parenthesis equals StartLayout Enlarged left brace 1st Row 1st Column f Subscript upper X Superscript negative 1 Baseline left parenthesis upper F Subscript upper X Superscript negative 1 Baseline left parenthesis u right parenthesis right parenthesis f Subscript upper X Baseline left parenthesis upper F Subscript upper X Superscript negative 1 Baseline left parenthesis u right parenthesis right parenthesis equals 1 comma 2nd Column 0 less than u less than 1 semicolon 2nd Row 1st Column 0 comma 2nd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e comma EndLayoutfU(u) =
{
f −1
X (F −1
X (u))fX(F −1
X (u)) = 1, 0 < u < 1;
0,
otherwise,
which is the PDF of a U(0, 1) random variable. 
Moment-Generating Function Method 
A third method makes use of knowledge of the characteristic function or the 
moment-generating function. The technique is moment-generating function 
method. The method requires inverting the MGF transform or else just rec-
ognizing the MGF of a known distribution. We will illustrate it on page 351. 
Further discussion of distributions of transformed random variables can 
be found in Gentle (2023) at  
https://mason.gmu.edu/~jgentle/books/MathStat.pdf. 
7.4.3 The Multivariate Normal Distribution 
The most important random variables are those whose distributions are in 
the normal family. In a family of normal distributions, the variances and 
covariances together with the means completely characterize the distribution. 
In this section, we will discuss what is called the multivariate normal fam-
ily, in which the random variable is a vector. The deﬁnitions and general 
expressions apply also to scalar random variables if we blur the formal dis-
tinctions in scalars, vectors, and matrices, as we have mentioned earlier. 
The PDF of the multivariate normal with mean μ and variance-covariance 
matrix Σ is 
f Subscript upper X Baseline left parenthesis x right parenthesis equals left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline normal e Superscript minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 Baseline commafX(x) = (2π)−d/2|Σ|−1/2e−(x−μ)TΣ−1(x−μ)/2,
(7.38) 
where |Σ| = det(Σ). (I prefer the notation “det(·)” for a determinant, but in 
this section, I will mix the two notations.) In this notation, we are obviously 
assuming that the variance-covariance matrix Σ is nonsingular, and we will 
assume this anytime we refer to the “multivariate normal distribution.” There 
is a related normal distribution, called a “singular normal distribution” or a 
“curved normal distribution,” in which this is not the case (and |Σ|−1/2 does 
not exist), but we will not discuss that family of distributions. 
For a vector of order d, we often denote the family of multivariate normal 
distributions by Nd(μ, Σ). We write

7.4 Multivariate Probability Theory
347
upper X tilde normal upper N Subscript d Baseline left parenthesis mu comma normal upper Sigma right parenthesis periodX ∼Nd(μ, Σ).
(7.39) 
Each member of this family of distributions corresponds to a speciﬁc μ and 
Σ. 
As with any PDF, the function pX in (7.38) integrates to 1. The funda-
mental integral associated with the d-variate normal distribution, sometimes 
called Aitken’s integral, is  
integral Underscript normal fraktur upper R Superscript d Baseline Endscripts normal e Superscript minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 Baseline normal d x period
f
IRd e−(x−μ)TΣ−1(x−μ)/2 dx.
(7.40) 
The rank of the integral is the same as the rank of the integrand. (“Rank” is 
used here in the sense of “number of dimensions.”) In this case, the integrand 
and the integral are scalars. 
We can evaluate the integral (7.40) by evaluating the individual single 
integrals after making the change of variables yi = xi −μi. It can also be seen 
by ﬁrst noting that because Σ−1 is positive deﬁnite, as in Eq. (3.306), it can 
be written as P T Σ−1 P = I for some nonsingular matrix P. Now, after the 
translation y = x−μ, which leaves the integral unchanged, we make the linear 
change of variables z = P −1 y, with the associated Jacobian |det(P)|, as in  
Eq. (7.20). From P T Σ−1 P = I, we have  |det(P)| = (det(Σ))1/2 because the 
determinant is positive. (Note that (det(Σ))1/2 is the reciprocal of |Σ|−1/2 
above.) Aitken’s integral therefore is 
StartLayout 1st Row 1st Column integral Underscript normal fraktur upper R Superscript d Endscripts normal e Superscript minus y Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript y divided by 2 Baseline normal d y 2nd Column equals 3rd Column integral Underscript normal fraktur upper R Superscript d Endscripts normal e Superscript minus left parenthesis upper P z right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript upper P z divided by 2 Baseline left parenthesis normal d normal e normal t left parenthesis normal upper Sigma right parenthesis right parenthesis Superscript 1 divided by 2 normal d z 2nd Row 1st Column Blank 2nd Column equals 3rd Column integral Underscript normal fraktur upper R Superscript d Endscripts normal e Superscript minus z Super Superscript normal upper T Superscript z divided by 2 Baseline normal d z left parenthesis normal d normal e normal t left parenthesis normal upper Sigma right parenthesis right parenthesis Superscript 1 divided by 2 3rd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript d divided by 2 Baseline left parenthesis normal d normal e normal t left parenthesis normal upper Sigma right parenthesis right parenthesis Superscript 1 divided by 2 Baseline period EndLayout
f
IRd e−yTΣ−1y/2 dy =
f
IRd e−(P z)TΣ−1P z/2 (det(Σ))1/2dz
=
f
IRd e−zTz/2 dz (det(Σ))1/2
= (2π)d/2(det(Σ))1/2.
(7.41) 
We stated that this normal distribution has mean μ and variance-covariance 
matrix Σ. We can see this directly, using Aitken’s integral, 
StartLayout 1st Row 1st Column normal upper E left parenthesis upper X right parenthesis 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline integral Underscript normal fraktur upper R Superscript d Endscripts x normal e Superscript minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 normal d x 2nd Row 1st Column Blank 2nd Column equals 3rd Column mu semicolon EndLayoutE(X) = (2π)−d/2|Σ|−1/2
f
IRd xe−(x−μ)TΣ−1(x−μ)/2 dx
= μ;
and for the variance, we have 
StartLayout 1st Row 1st Column normal upper V left parenthesis upper X right parenthesis 2nd Column equals 3rd Column normal upper E left parenthesis left parenthesis upper X minus normal upper E left parenthesis upper X right parenthesis right parenthesis left parenthesis upper X minus normal upper E left parenthesis upper X right parenthesis right parenthesis Superscript normal upper T Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline integral Underscript normal fraktur upper R Superscript d Endscripts left parenthesis left parenthesis x minus mu right parenthesis left parenthesis x minus mu right parenthesis Superscript normal upper T Baseline right parenthesis normal e Superscript minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 normal d x 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal upper Sigma period EndLayoutV(X) = E
(
(X −E(X))(X −E(X))T)
= (2π)−d/2|Σ|−1/2
f
IRd
(
(x −μ)(x −μ)T)
e−(x−μ)TΣ−1(x−μ)/2 dx
= Σ.
Likewise, we can derive the multivariate normal moment-generating func-
tion:

348
7 Real Analysis and Probability Distributions of Vectors and Matrices
StartLayout 1st Row 1st Column normal upper E left parenthesis normal e Superscript t Super Superscript normal upper T Superscript upper X Baseline right parenthesis 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline integral Underscript normal fraktur upper R Superscript d Endscripts normal e Superscript t Super Superscript normal upper T Superscript x minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 Baseline normal d x 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal e Superscript t Super Superscript normal upper T Superscript mu plus t Super Superscript normal upper T Superscript normal upper Sigma t divided by 2 Baseline period EndLayoutE(etTX) = (2π)−d/2|Σ|−1/2
f
IRd etTx−(x−μ)TΣ−1(x−μ)/2 dx
= etTμ+tTΣt/2.
(7.42) 
Linear Transformations of a Multivariate Normal Random 
Variable 
Let X be a random  d-vector distributed as Nd(μ, Σ), and consider Y = AX+b, 
where A is a k × d constant matrix with full row rank k and b is a d-vector. 
The number of elements in Y may be fewer than the number in X. 
We want to derive the PDF of Y from the PDF of X, 
f Subscript upper X Baseline left parenthesis x right parenthesis equals left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline normal e Superscript minus left parenthesis x minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis x minus mu right parenthesis divided by 2 Baseline periodfX(x) = (2π)−d/2|Σ|−1/2e−(x−μ)TΣ−1(x−μ)/2.
We will use the change-of-variables method to determine the PDF of Y (see 
page 345). The change-of-variables technique makes use of the inverse trans-
formation, so ﬁrst, if the transformation matrix A is not of full rank, that is, 
there are fewer elements in Y than in X, we must add independent trans-
formation of X. The additional variables that result can then be integrated 
out to obtain the marginal distribution of the variables of interest, as shown 
below. 
The inverse transformation is X = A−1 (Y − b), and the absolute value 
of the determinant of the Jacobian is |A|−1 . We use the properties of deter-
minants given in Eq. (3.30), page 87, and  in  Eqs. (3.95) and  (3.97), page 110; 
and in the exponent, we premultiply by I = A−T AT and postmultiply by 
I = AA−1 . After rearranging terms we have, 
StartLayout 1st Row 1st Column f Subscript upper Y Baseline left parenthesis y right parenthesis 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue upper A EndAbsoluteValue Superscript negative 1 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript negative 1 divided by 2 Baseline normal e Superscript minus left parenthesis upper A Super Superscript negative 1 Superscript left parenthesis y minus b right parenthesis minus mu right parenthesis Super Superscript normal upper T Superscript normal upper Sigma Super Superscript negative 1 Superscript left parenthesis upper A Super Superscript negative 1 Superscript left parenthesis y minus b right parenthesis minus mu right parenthesis divided by 2 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis 2 pi right parenthesis Superscript negative d divided by 2 Baseline StartAbsoluteValue upper A normal upper Sigma upper A Superscript normal upper T Baseline EndAbsoluteValue Superscript negative 1 divided by 2 Baseline normal e Superscript minus left parenthesis y minus upper A mu minus b right parenthesis Super Superscript normal upper T Superscript left parenthesis upper A normal upper Sigma upper A Super Superscript normal upper T Superscript right parenthesis Super Superscript negative 1 Superscript left parenthesis y minus upper A mu minus b right parenthesis divided by 2 Baseline comma EndLayoutfY (y) = (2π)−d/2|A|−1|Σ|−1/2e−(A−1(y−b)−μ)TΣ−1(A−1(y−b)−μ)/2
= (2π)−d/2|AΣAT|−1/2e−(y−Aμ−b)T(AΣAT)
−1(y−Aμ−b)/2,
(7.43) 
which we recognize as the PDF of a multivariate normal. 
Thus, if 
upper X tilde normal upper N Subscript d Baseline left parenthesis mu comma normal upper Sigma right parenthesis comma normal a normal n normal d upper Y equals upper A upper X plus b comma normal a normal n normal d upper A semicolon normal n normal o normal n normal s normal i normal n normal g normal u normal l normal a normal r commaX ∼Nd(μ, Σ),
and Y = AX + b,
and A; nonsingular,
then 
upper Y tilde normal upper N Subscript d Baseline left parenthesis upper A mu plus b comma upper A normal upper Sigma upper A Superscript normal upper T Baseline right parenthesis periodY ∼Nd(Aμ + b, AΣAT).
(7.44) 
A very useful transformation of a random variable X that is distributed 
as Nd(μ, Σ) is  Y = Σ−1 
2 (X −μ). This results in Y ∼Nd(0, I). 
Notice also that for the important case of a multivariate normal distribu-
tion Nd(0, σ2 I) formed as a vector of iid N(0, σ2 ) variates, for Y = AX, we  
have the multivariate normal Nd(0, AAT σ2 ). 
The linear invariance of the normal distribution is one of its most useful 
properties and indeed is a characterization of the normal distribution.

7.4 Multivariate Probability Theory
349
The Matrix Normal Distribution 
The matrix normal distribution can be deﬁned from a set of s random r-vectors 
Y1, . . . , Ys that are independently and identically distributed as Nr(0, Ir). For 
a ﬁxed n × r matrix A and ﬁxed n-vector μi, the vector -Yi = μi + AYi is 
distributed as Nn(μi, AAT ). Now consider the s-vector Yj∗ consisting of the 
jth element of each Yi. It is clear that Yj∗ ∼ Nr(0, Is), and hence for a given 
d×s matrix B, BYj∗ ∼ Nd(0, BBT ). Now, forming a matrix Y with the Yis as  
the columns and a matrix M with the μis as the columns, and putting these 
two transformations together, we have a matrix normal random variable: 
upper X equals upper M plus upper A upper Y upper B Superscript normal upper T Baseline periodX = M + AY BT.
(7.45) 
Let Σ = AAT and Ψ = BBT denote the variance-covariance matrices in the 
multivariate distributions above. These are parameters in the distribution of 
X. We denote the distribution of the random matrix X with three parameters 
as 
upper X tilde normal upper N Subscript n comma d Baseline left parenthesis upper M comma normal upper Sigma comma normal upper Psi right parenthesis periodX ∼Nn,d(M, Σ, Ψ).
(7.46) 
We may also wish to require that Σ and Ψ be positive deﬁnite. This require-
ment would be satisﬁed if A and B are each of full row rank. 
There are of course various ways that relationships among the individual 
multivariate random variables (i.e., the columns of X) could be modeled. A 
useful model is a multivariate normal distribution for vec(X) as in expres-
sion (7.39), instead of the matrix normal distribution for X as above: 
normal v normal e normal c left parenthesis upper X right parenthesis tilde normal upper N Subscript n d Baseline left parenthesis normal v normal e normal c left parenthesis upper M right parenthesis comma normal upper Psi circled times normal upper Sigma right parenthesis periodvec(X) ∼Nnd (vec(M), Ψ ⊗Σ) .
(7.47) 
This distribution is easy to work with because it is just an ordinary multivari-
ate (vector) normal distribution. The variance-covariance matrix has a special 
structure, called a Kronecker structure, that incorporates the homoscedastic-
ity of the columns as well as their interdependence. 
Another distribution for random matrices is one in which the individual el-
ements have identical and independent normal distributions. This distribution 
of matrices was named the BMvN distribution by Birkhoﬀ and Gulati (1979) 
(from the last names of three mathematicians who used such random matrices 
in numerical studies). Birkhoﬀ and Gulati showed that if the elements of the 
n × n matrix X are iid N(0, σ2 ), and if Q is an orthogonal matrix and R is 
an upper triangular matrix with positive elements on the diagonal such that 
QR = X, then  Q has the Haar distribution. (The factorization X = QR is 
called the QR decomposition and is discussed on page 241. If  X is a random 
matrix as described, this factorization exists with probability 1.) The Haar(n) 
distribution is uniform over the space of n × n orthogonal matrices. 
The measure 
mu left parenthesis upper D right parenthesis equals integral Underscript upper D Endscripts upper H Superscript normal upper T Baseline normal d upper H commaμ(D) =
f
D
HT dH,
(7.48)

350
7 Real Analysis and Probability Distributions of Vectors and Matrices
where D is a subset of the orthogonal group O(n) (see page 156), is called the 
Haar measure. This measure is used to deﬁne a kind of “uniform” probability 
distribution for orthogonal factors of random matrices. For any Q ∈O(n), 
let QD represent the subset of O(n) consisting of the matrices ˜H = QH for 
H ∈ D and DQ represent the subset of matrices formed as HQ. From the  
integral, we see that 
mu left parenthesis upper Q upper D right parenthesis equals mu left parenthesis upper D upper Q right parenthesis equals mu left parenthesis upper D right parenthesis commaμ(QD) = μ(DQ) = μ(D),
so the Haar measure is invariant to multiplication within the group. The mea-
sure is therefore also called the Haar invariant measure over the orthogonal 
group. 
7.4.4 Distributions Derived from the Multivariate Normal 
The normal distribution is widely used in statistical applications. While the 
assumption of a normal distribution may not be “true,” it is often the case that 
it provides a relatively good ﬁt to observed data. Its usefulness in modeling 
derives also from the facts that several statistics in data analysis have simple 
distributions if the underlying distribution is normal and that the asymptotic 
distributions of many statistics can be determined using the central limit 
theorem. 
Given a basic assumption that a model includes one or more normally 
distributed random variables, we are interested in the distributions of various 
transformations or combinations of the normal random variables. 
There are three general types of transformed variables: 
• Second-degree normal variables −→ chi-squared or Wishart variable 
• Combination of normal and chi-squared variables −→ t variable 
• Chi-squared variables −→ F variable 
We will only discuss the chi-squared and Wishart distributions below be-
cause the t and F distributions and other distributions used in linear statistical 
inference are derived from those two fundamental distributions. 
7.4.5 Chi-Squared Distributions 
Chi-squared distributions are derived from sums of squares of normal distri-
butions. 
Let us ﬁrst consider the distribution of the square of a univariate normal 
random variable with mean 0. Next, we will consider the distribution of the 
sum of squares of iid normals with mean 0. 
The Family of Distributions Nn(0, σ2 In) 
Let 
upper Z tilde normal upper N left parenthesis 0 comma sigma squared right parenthesis commaZ ∼N(0, σ2),

7.4 Multivariate Probability Theory
351
and let 
upper X equals g left parenthesis upper Z right parenthesis equals StartFraction upper Z squared Over sigma squared EndFraction periodX = g(Z) = Z2
σ2 .
We will use the change-of-variables technique to determine the PDF of X. 
The PDF of X is 
f Subscript upper Z Baseline left parenthesis z right parenthesis equals StartFraction 1 Over StartRoot 2 pi EndRoot EndFraction normal e Superscript minus z squared divided by 2 sigma squared Baseline periodfZ(z) =
1
√
2π e−z2/2σ2.
The function g is one-to-one over (−∞, 0) and, separately, one-to-one over 
[0, ∞), and the absolute value of the Jacobian of the inverse over both regions 
is σx−1/2 . Hence, the PDF of X is 
f Subscript upper X Baseline left parenthesis x right parenthesis equals StartFraction 1 Over StartRoot 2 pi EndRoot EndFraction x Superscript negative 1 divided by 2 Baseline normal e Superscript negative x divided by 2 Baseline comma f normal o normal r x greater than or equals 0 periodfX(x) =
1
√
2π x−1/2e−x/2,
for x ≥0.
(7.49) 
(See Sect. 7.4.2, beginning on page 344.) This is the PDF of a chi-squared 
random variable with one degree of freedom. 
Now let us consider the sum of squares. Assume Z1, . . . , Zn 
iid
∼ N(0, σ2 ), 
let 
upper Y Subscript i Baseline equals StartFraction upper Z Subscript i Superscript 2 Baseline Over sigma squared EndFraction commaYi = Z2
i
σ2 ,
and let 
upper Y equals sigma summation upper Y Subscript i Baseline periodY =
E
Yi.
We will use the MGF of Yi to determine the MGF of Y using the relation in 
Eq. (7.32). From the deﬁnition, we see that the MGF of Yi (which is the same 
as X above) is 
StartLayout 1st Row 1st Column normal upper E left parenthesis normal e Superscript t upper Y Super Subscript i Superscript Baseline right parenthesis 2nd Column equals 3rd Column integral Subscript 0 Superscript normal infinity Baseline StartFraction 1 Over StartRoot 2 pi EndRoot EndFraction y Superscript negative 1 divided by 2 Baseline normal e Superscript minus y left parenthesis 1 minus 2 t right parenthesis divided by 2 normal d y 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis 1 minus 2 t right parenthesis Superscript negative 1 divided by 2 Baseline normal f normal o normal r t less than one half period EndLayoutE
(
etYi)
=
f ∞
0
1
√
2π y−1/2e−y(1−2t)/2dy
= (1 −2t)−1/2
for
t < 1
2.
(7.50) 
Now, because the Xi are iid, the Yi are also iid, and so from Eq. (7.32), the 
MGF of the sum of the squares, Y , is  
psi left parenthesis t right parenthesis equals left parenthesis 1 minus 2 t right parenthesis Superscript negative n divided by 2 Baseline comma normal f normal o normal r semicolon t less than 1 divided by 2 periodψ(t) = (1 −2t)−n/2,
for; t < 1/2.
This is known to be the MGF of a chi-squared random variable with n degrees 
of freedom. (This MGF can easily be evaluated from the PDF of the chi-
squared distribution.) 
The development above can be expressed in terms of a multivariate normal 
distribution. Forming a vector Z from the Zi above, we have 
upper Z tilde normal upper N Subscript n Baseline left parenthesis 0 comma sigma squared upper I Subscript n Baseline right parenthesis commaZ ∼Nn(0, σ2In),
and Y is the quadratic form with the identity matrix, ZT Z/σ2 . 
The distributions of quadratic forms involving more general multivariate 
normal distributions and with more general matrices within the quadratic 
forms can be worked out in similar fashions.

352
7 Real Analysis and Probability Distributions of Vectors and Matrices
The Family of Distributions Nd(μ, Σ) 
If X is a random variable with distribution Nd(μ, Σ), A is a q × d matrix 
with rank q (which implies q ≤ d), and Y = AX, then the straightforward 
change-of-variables technique yields the distribution of Y as Nd(Aμ, AΣAT ). 
Useful transformations of the random variable X with distribution Nd(μ, Σ) 
are Y1 = Σ−1/2 X and Y2 = Σ−1 
C X, where  ΣC is a Cholesky factor of Σ. In  
either case, the variance-covariance matrix of the transformed variate Y1 or 
Y2 is Id. 
Quadratic forms involving a Y that is distributed as Nd(μ, Id) have useful  
properties. For statistical inference, it is important to know the distribution 
of these quadratic forms. The simplest quadratic form involves the identity 
matrix: Sd = Y T Y . 
We can derive the PDF of Sd by beginning with d = 1 and using induction. 
If d = 1,  for  t >  0, we have 
normal upper P normal r left parenthesis upper S 1 less than or equals t right parenthesis equals normal upper P normal r left parenthesis upper Y less than or equals StartRoot t EndRoot right parenthesis minus normal upper P normal r left parenthesis upper Y less than or equals minus StartRoot t EndRoot right parenthesis commaPr(S1 ≤t) = Pr(Y ≤
√
t) −Pr(Y ≤−
√
t),
where Y ∼ N1(μ, 1), and so the PDF of S1 is 
StartLayout 1st Row 1st Column p Subscript upper S 1 Baseline left parenthesis t right parenthesis 2nd Column equals 3rd Column StartFraction 1 Over 2 StartRoot 2 pi t EndRoot EndFraction left parenthesis normal e Superscript minus left parenthesis StartRoot t EndRoot minus mu right parenthesis squared divided by 2 Baseline plus normal e Superscript minus left parenthesis minus StartRoot t EndRoot minus mu right parenthesis squared divided by 2 Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction normal e Superscript minus mu squared divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline Over 2 StartRoot 2 pi t EndRoot EndFraction left parenthesis normal e Superscript mu StartRoot t EndRoot Baseline plus normal e Superscript minus mu StartRoot t EndRoot Baseline right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction normal e Superscript minus mu squared divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline Over 2 StartRoot 2 pi t EndRoot EndFraction left parenthesis sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis mu StartRoot t EndRoot right parenthesis Superscript j Baseline Over j factorial EndFraction plus sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis minus mu StartRoot t EndRoot right parenthesis Superscript j Baseline Over j factorial EndFraction right parenthesis 4th Row 1st Column Blank 2nd Column equals 3rd Column StartFraction normal e Superscript minus mu squared divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline Over StartRoot 2 t EndRoot EndFraction sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis mu squared t right parenthesis Superscript j Baseline Over StartRoot pi EndRoot left parenthesis 2 j right parenthesis factorial EndFraction 5th Row 1st Column Blank 2nd Column equals 3rd Column StartFraction normal e Superscript minus mu squared divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline Over StartRoot 2 t EndRoot EndFraction sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis mu squared t right parenthesis Superscript j Baseline Over j factorial upper Gamma left parenthesis j plus 1 divided by 2 right parenthesis 2 Superscript 2 j Baseline EndFraction comma EndLayoutpS1(t) =
1
2
√
2πt
(
e−(
√
t−μ)2/2 + e−(−
√
t−μ)2/2)
= e−μ2/2e−t/2
2
√
2πt
(
eμ
√
t + e−μ
√
t)
= e−μ2/2e−t/2
2
√
2πt
⎛
⎝
∞
E
j=0
(μ
√
t)j
j!
+
∞
E
j=0
(−μ
√
t)j
j!
⎞
⎠
= e−μ2/2e−t/2
√
2t
∞
E
j=0
(μ2t)j
√π(2j)!
= e−μ2/2e−t/2
√
2t
∞
E
j=0
(μ2t)j
j!Γ(j + 1/2)22j ,
in  which we use  the fact that  
upper Gamma left parenthesis j plus 1 divided by 2 right parenthesis equals StartFraction StartRoot pi EndRoot left parenthesis 2 j right parenthesis factorial Over j factorial 2 Superscript 2 j Baseline EndFractionΓ(j + 1/2) =
√π(2j)!
j!22j
(see page 659). This can now be written as 
p Subscript upper S 1 Baseline left parenthesis t right parenthesis equals normal e Superscript minus mu squared divided by 2 Baseline sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis mu squared right parenthesis Superscript j Baseline Over j factorial 2 Superscript j Baseline EndFraction StartFraction 1 Over upper Gamma left parenthesis j plus 1 divided by 2 right parenthesis 2 Superscript j plus 1 divided by 2 Baseline EndFraction t Superscript j minus 1 divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline commapS1(t) = e−μ2/2
∞
E
j=0
(μ2)j
j!2j
1
Γ(j + 1/2)2j+1/2 tj−1/2e−t/2,
(7.51) 
in which we recognize the PDF of the central chi-squared distribution with 
2j + 1 degrees of freedom,

7.4 Multivariate Probability Theory
353
p Subscript chi Sub Subscript 2 j plus 1 Sub Superscript 2 Subscript Baseline left parenthesis t right parenthesis equals StartFraction 1 Over upper Gamma left parenthesis j plus 1 divided by 2 right parenthesis 2 Superscript j plus 1 divided by 2 Baseline EndFraction t Superscript j minus 1 divided by 2 Baseline normal e Superscript negative t divided by 2 Baseline periodpχ2
2j+1(t) =
1
Γ(j + 1/2)2j+1/2 tj−1/2e−t/2.
(7.52) 
A similar manipulation for d = 2 (i.e., for Y ∼N2(μ, 1), and maybe d = 3,  
or as far as you need to go) leads us to a general form for the PDF of the 
χ2 
d(δ) random variable Sd: 
p Subscript upper S Sub Subscript d Subscript Baseline left parenthesis t right parenthesis equals normal e Superscript minus mu squared divided by 2 Baseline sigma summation Underscript j equals 0 Overscript normal infinity Endscripts StartFraction left parenthesis mu squared divided by 2 right parenthesis Superscript j Baseline Over j factorial EndFraction p Subscript chi Sub Subscript 2 j plus 1 Sub Superscript 2 Subscript Baseline left parenthesis t right parenthesis periodpSd(t) = e−μ2/2
∞
E
j=0
(μ2/2)j
j!
pχ2
2j+1(t).
(7.53) 
We can show that Eq. (7.53) holds  for any  d by induction. The distribution of 
Sd is called the noncentral chi-squared distribution with d degrees of freedom 
and noncentrality parameter δ = μT μ. We denote this distribution as χ2 
d(δ). 
The induction method above involves a special case of a more general fact: 
if Xi for i = 1, . . . , k  are independently distributed as χ2 
ni (δi), then E
i Xi is 
distributed as χ2 
n(δ), where n = E
i ni and δ = E
i δi. (Compare this with the 
result for Wishart distributions in Exercise 7.9b on page 361.) 
In applications of linear models, a quadratic form involving Y is often 
partitioned into a sum of quadratic forms. Assume that Y is distributed as 
Nd(μ, Id), and for i = 1, . . . k, let  Ai be a d × d symmetric matrix with rank 
ri such that E
i Ai = Id. This yields a partition of the total sum of squares 
Y T Y into k components: 
upper Y Superscript normal upper T Baseline upper Y equals upper Y Superscript normal upper T Baseline upper A 1 upper Y plus midline horizontal ellipsis plus upper Y Superscript normal upper T Baseline upper A Subscript k Baseline upper Y periodY TY = Y TA1Y + · · · + Y TAkY.
(7.54) 
One of the most important results in the analysis of linear models states 
that the Y T AiY have independent noncentral chi-squared distributions χ2 
ri (δi) 
with δi = μT Aiμ if and only if E
i ri = d. This follows from Cochran’s theo-
rem; see page 393. 
The chi-squared distributions of various quadratic forms provide the basis 
for statistical inference in linear models, as we will discuss in Sect. 9.2. The  
basic chi-squared distribution can be used to identify statistics with t and F 
distributions, which can then be used to test statistical hypotheses or to form 
conﬁdence regions. 
7.4.6 Wishart Distributions 
A Wishart distribution is a useful distribution of random matrices of sums 
of squares and cross products. It can be thought of as a multivariate gen-
eralization of a chi-squared distribution. While the chi-squared families of 
univariate distributions arise naturally as sums of squares of univariate nor-
mal random variables, Wishart random matrices are often formed as outer 
products of multivariate normal random variables. It is the distribution of 
random variance-covariance matrices formed from n independent d-variate 
normal random variables distributed as Nd(0, Σ).

354
7 Real Analysis and Probability Distributions of Vectors and Matrices
Let 
upper X 1 comma ellipsis comma upper X Subscript n Baseline tilde Overscript i i d Endscripts normal upper N Subscript d Baseline left parenthesis mu comma normal upper Sigma right parenthesis commaX1, . . . , Xn
iid
∼Nd(μ, Σ),
and let X = [X1 . . .  Xn]. The sums of squares and cross products matrix 
W = XXT has a Wishart distribution. 
Because of the squared terms and the cross products, just as with the chi-
squared variates, the distribution is simpler if μ = 0. In that case, as with the 
chi-squared, we have a “central” Wishart distribution. 
A common matrix integral that arises in the Wishart and related distribu-
tions is the complete d-variate gamma function, denoted by Γd(x) and deﬁned 
as 
upper Gamma Subscript d Baseline left parenthesis x right parenthesis equals integral Underscript upper D Endscripts normal e Superscript minus normal t normal r left parenthesis upper A right parenthesis Baseline StartAbsoluteValue upper A EndAbsoluteValue Superscript x minus left parenthesis d plus 1 right parenthesis divided by 2 Baseline normal d upper A commaΓd(x) =
f
D
e−tr(A)|A|x−(d+1)/2 dA,
(7.55) 
where D is the set of all d × d positive deﬁnite matrices, A ∈ D, and  
x >  (d −1)/2. A multivariate gamma distribution can be deﬁned in terms 
of the integrand. (There are diﬀerent deﬁnitions of a multivariate gamma dis-
tribution.) The multivariate gamma function also appears in the probability 
density function for a Wishart random variable. 
For the chi-squared distribution, we assumed that the normal variates 
were scaled by the standard deviation; hence, the chi-squared distribution 
does not involve a parameter corresponding to the variances of the normals. 
The variates forming the Wishart, however, are not adjusted for Σ; hence, this 
quantity is a parameter in the Wishart distribution. The number of normals 
in the outer product is also a parameter, similar to the degrees of freedom of 
the chi-squared. 
For W formed as above, we denote the Wishart as Wd(Σ, n). By making 
a change of variables W = 2Σ 
1 
2 XΣ 
1 
2 , we see that the PDF of the Wishart is 
proportional to 
normal e Superscript minus normal t normal r left parenthesis normal upper Sigma Super Superscript negative 1 Superscript upper W divided by 2 right parenthesis Baseline StartAbsoluteValue upper W EndAbsoluteValue Superscript left parenthesis n minus d minus 1 right parenthesis divided by 2 Baseline periode−tr(Σ−1W/2)|W|(n−d−1)/2.
In Exercise 7.9a, you are asked to show this and to determine the constant of 
proportionality by integrating the PDF. 
In the Wishart distribution, it is assumed that n > d. There are related 
distributions in which d ≥ n, which we call “pseudo-Wishart” distributions. 
In the general case, with μ /= 0, we denote the distribution as 
normal upper W tilde normal upper W Subscript d Baseline left parenthesis normal upper Sigma comma n comma normal upper Delta right parenthesis commaW ∼Wd(Σ, n, Δ),
where Δ = μμT . This is called the noncentral Wishart distribution. 
The importance of the Wishart distribution is that the Wishart matrix 
can be used to make inferences on the variance-covariance matrix Σ of an 
underlying multivariate normal distribution. 
There are several situations in which the Wishart can be used in statistical 
inference. We state some of the relevant properties here. You are asked to 
provide proofs in Exercise 7.9. We will use some of these facts in Sect. 9.2, on  
statistical inference in linear models.

7.5 Multivariate Random Number Generation
355
If W1 ∼ Wd(Σ, n, Δ1) and  W2 ∼ Wd(Σ, m, Δ2) is independent of W1, 
then 
normal upper W 1 plus normal upper W 2 tilde normal upper W Subscript d Baseline left parenthesis normal upper Sigma comma n plus m comma normal upper Delta 1 plus normal upper Delta 2 right parenthesis periodW1 + W2 ∼Wd(Σ, n + m, Δ1 + Δ2).
If W ∼Wd(Σ, n, Δ) and  A is a ﬁxed q × d matrix, then 
upper A normal upper W upper A Superscript normal upper T Baseline tilde normal upper W Subscript q Baseline left parenthesis upper A normal upper Sigma upper A Superscript normal upper T Baseline comma n comma upper A normal upper Delta upper A Superscript normal upper T Baseline right parenthesis periodAWAT ∼Wq(AΣAT, n, AΔAT).
Finally, if W ∼ Wd(σ2 I, n, Δ), then 
StartFraction 1 Over sigma squared EndFraction upper W Subscript i i Baseline tilde chi Subscript n Superscript 2 Baseline left parenthesis normal upper Delta Subscript i i Baseline right parenthesis period 1
σ2 Wii ∼χ2
n(Δii).
7.5 Multivariate Random Number Generation 
The need to simulate realizations of random variables arises often in statistical 
applications, both in the development of statistical theory and in applied 
data analysis. In this section, we will illustrate and only brieﬂy describe some 
methods of multivariate random number generation. These make use of some 
of the properties we have discussed previously. 
Most methods for random number generation assume an underlying source 
of realizations of a U(0, 1) random variable. For a random variable from a 
diﬀerent distribution, there are several ways that some U(0, 1) variables may 
be transformed into a variable with the given distribution. If U is a U(0, 1) 
random variable, and F is the cumulative distribution function of a continuous 
random variable, then the random variable 
upper X equals upper F Superscript negative 1 Baseline left parenthesis upper U right parenthesisX = F −1(U)
(7.56) 
has the cumulative distribution function F. (If  the support of  X is ﬁnite, 
F −1 (0) and F −1 (1) are interpreted as the limits of the support.) This same 
idea, the basis of the so-called inverse CDF method, can also be applied to 
discrete random variables. As mentioned above, however, there are other ways 
to transform the U(0, 1) random variables. 
7.5.1 The Multivariate Normal Distribution 
If Z has a multivariate normal distribution with the identity as variance-
covariance matrix, then for a given positive deﬁnite matrix Σ, both  
upper Y 1 equals normal upper Sigma Superscript one half Baseline upper ZY1 = Σ
1
2 Z
(7.57) 
and 
upper Y 2 equals normal upper Sigma Subscript upper C Baseline upper Z commaY2 = ΣCZ,
(7.58) 
where ΣC is a Cholesky factor of Σ, have a multivariate normal distribution 
with variance-covariance matrix Σ (see page 350). The mean of Y1 is Σ 
1 
2 μ,

356
7 Real Analysis and Probability Distributions of Vectors and Matrices
where μ is the mean of Z, and the mean of Y1 is ΣCμ. If  Z has 0 mean, then 
the distributions are identical, that is, Y1 
d 
= Y2. 
This leads to a very simple method for generating a multivariate normal 
random d-vector: generate into a d-vector z d  independent N1(0, 1). Then 
form a vector from the desired distribution by the transformation in Eq. (7.57) 
or (7.58) together with the addition of a mean vector if necessary. 
7.5.2 Random Correlation Matrices 
Occasionally, we wish to generate random numbers but do not wish to specify 
the distribution fully. We may want a “random” matrix, but we do not know an 
exact distribution that we wish to simulate. (There are only a few “standard” 
distributions of matrices. The Wishart distribution and the Haar distribution 
(page 349) are the only two common ones. We can also, of course, specify the 
distributions of the individual elements.) 
We may want to simulate random correlation matrices. Although we do 
not have a speciﬁc distribution, we may want to specify some characteristics, 
such as the eigenvalues. (All of the eigenvalues of a correlation matrix, not 
just the largest and smallest, determine the condition of data matrices that 
are realizations of random variables with the given correlation matrix.) 
Any nonnegative deﬁnite (symmetric) matrix with 1s on the diagonal is a 
correlation matrix. A correlation matrix is diagonalizable, so if the eigenvalues 
are c1, . . . , cd, we can represent the matrix as 
upper V normal d normal i normal a normal g left parenthesis left parenthesis c 1 comma ellipsis comma c Subscript d Baseline right parenthesis right parenthesis upper V Superscript normal upper TV diag((c1, . . . , cd))V T
for an orthogonal matrix V . (For a  d×d correlation matrix, we have E ci = d; 
see page 405.) Generating a random correlation matrix with given eigenvalues 
becomes a problem of generating the random orthogonal eigenvectors and then 
forming the matrix V from them. (Recall from page 176 that the eigenvectors 
of a symmetric matrix can be chosen to be orthogonal.) In the following, we 
let C = diag((c1, . . . , cd)) and begin with E = I (the d×d identity) and k = 1.  
The method makes use of deﬂation in step 6 (see page 306). The underlying 
randomness is that of a normal distribution. 
Algorithm 7.1 Random Correlation Matrices with Given Eigenval-
ues 
1. Generate a d-vector w of iid standard normal deviates, form x = Ew, and  
compute a = xT (I −C)x. 
2. Generate a d-vector z of iid standard normal deviates, form y = Ez, and  
compute b = xT (I − C)y, c = yT (I − C)y, and  e2 = b2 − ac. 
3. If e2 < 0, then go to step 2. 
4. Choose a random sign, s = −1 or  s = 1.  Set  r = b + se 
a 
x − y.

7.5 Multivariate Random Number Generation
357
5. Choose another random sign, s = −1 or  s = 1,  and  set  vk = 
sr 
(rTr) 
1 
2 . 
6. Set E = E − vkvT 
k , and  set  k = k + 1.  
7. If k <  d, then go to step  1. 
8. Generate a d-vector w of iid standard normal deviates, form x = Ew, and  
set vd = 
x 
(xTx) 
1 
2 . 
9. Construct the matrix V using the vectors vk as its rows. Deliver V CV  T 
as the random correlation matrix. 
Appendix: R for Working with Probability Distributions 
and for Simulating Random Data 
For a given probability distribution, there are three quantities that we may 
wish to compute: 
• d, the probability function or the PDF, the probability or the probability 
density at a given point 
• p, the CDF, the cumulative probability at a given point 
• q, the inverse of a univariate CDF, the quantile corresponding to a given 
probability 
For many of the common distributions, R provides a function to compute 
each of these quantities. The name of the R function is a character string 
suggesting the distribution, such as norm, beta, pois, and so on, preﬁxed by 
the letter d, p, or  q, as indicated above. Any parameters for the distribution 
family are arguments for the function, and if there are standard values for 
the parameters, the arguments are initialized to those values by default. For 
example, 
dnorm(1, mean=3, sd=2) 
pnorm(1, sd=2) 
qnorm(0.3) 
represent, respectively, the value of the PDF of a N(3, 4) distribution at the 
point 1, the value of the CDF of a N(0, 4) distribution at the point 1, and the 
quantile of a N(0, 1) distribution corresponding to a probability of 0.3. (This is 
because N(0, 1) is the standard normal distribution. Note that the argument 
to the norm functions is the standard deviation, while in the common notation 
N(μ, σ2 ), the second quantity is the variance.) 
There are not many multivariate distributions that are commonly used, 
except for the multivariate normal. The problem is how to specify the mul-
tivariate relationships. For the normal family of distributions, the covariance 
is a complete speciﬁcation. Also, it has an intuitive appeal. For other fam-
ilies of distributions, however, this is not the case. Of course, we often use

358
7 Real Analysis and Probability Distributions of Vectors and Matrices
the covariance to measure relationships in any distribution family, and we 
have a heuristic familiarity with that measure. Another way of measuring and 
specifying the multivariate relationships is by use of copulas. Multivariate 
analogues of a quantile function are not very useful; a quantile is a multi-
dimensional surface with variable shapes. Even a multivariate CDF is rarely 
used; rather, proﬁle univariate CDFs are usually more meaningful. We will 
not pursue these issues further here. 
The dmvnorm function and the pmvnorm function in the mvtnorm package 
computes the density and CDF of a multivariate normal distribution, but 
there is no corresponding q function. 
Generating Random Data 
Many statistical procedures for data analysis depend on an assumption that 
the data are random. Random data that appear to come from a speciﬁed 
probability distribution are also useful for studying eﬀectiveness of statistical 
methods. Such data can be generated on the computer, and R provides several 
functions for generating them. 
There are several algorithms for generating a sequence of numbers that 
appear to be uniformly randomly distributed between 0 and 1 and to be inde-
pendent of each other. R provides several algorithms, and the knowledgeable 
user can choose one by use of the RNGkind function; otherwise, a default al-
gorithm is used. All of these algorithms require a starting point, which may 
be a single number or a vector of several numbers. In R, this starting point or 
current state is maintained in a integer 626-vector .Random.seed. The  cur-
rent state is updated every time a random number is generated. The user can 
set the current state by use of the set.seed function. In an R session, if the 
user has not set a seed, the ﬁrst time a random number is to be generated, R 
will construct a random state using the system clock. It is recommended that 
the user invoke the set.seed function prior to generating random numbers 
so that the same numbers can be used when the program is executed again. 
The state of the underlying generator at any time can be saved as the 
current value of .Random.seed. If later, the sequence of random numbers is 
to be continued from the where the sequence ended, .Random.seed can be 
restored to its previous value. 
The R function that generates the basic U(0, 1) random variate is runif, 
and its only required argument is the number of random numbers to be gen-
erated. 
Random data from other distributions are generated by converting uniform 
numbers from runif, using any of the methods discussed in Sect. 7.4.2, as well  
as other methods. 
R functions for generating random data have the same root name as the 
d, p, and  q functions, and their preﬁx name is r. The ﬁrst argument is the 
number of simulated observations requested, and the other arguments are the 
same as for the other R functions for that probability distribution; thus,

7.5 Multivariate Random Number Generation
359
rnorm(1000, sd=2) 
requests 1,000 observations from a N(0, 4) distribution. 
The vector output of any of the “r” functions, rnorm, rgamma, and  so  on,  
can be considered as an iid sequence; hence, the vector can be considered to be 
a single observation from a multivariate distribution in which the individual 
elements are iid If x <- rnorm(d), then  x is both a sample of d iid N(0, 1) 
variates and equivalently a single observation from Nd(0, Id). 
There are several ways that a multivariate random variable with iid ele-
ments can be transformed into a multivariate random variable with elements 
with various relationships to each other. The two most common ways are by 
multiplication by a matrix and by working with the copulas of the distribu-
tion. The multivariate normal distribution is the only one commonly used and 
with a well-worked-out theory. It is the only multivariate distribution we will 
consider here. 
An appeal of the multivariate normal is that the relationships among the 
individual elements that fully characterize the distribution are the ones that 
we are most comfortable with, covariances or correlations. The transforma-
tions discussed on page 348 show us how to go from a Nd(0, Id) variable to a  
Nd(μ, Σ) variable. 
The R functions mvrnorm in the MASS package and the rmvnorm in the 
mvtnorm package generate random multivariate normal random variates. 
(Note the variance-covariance matrices in both of these functions are not 
made up of standard deviations, which are used in rnorm.) 
Monte Carlo Methods 
One of the most important applications of random number generation is to 
estimate a quantity that can be represented as an expected value. Many inte-
grals can be interpreted as an expected value of some function evaluated over 
a random variable with some reasonable distribution. Given the integral 
upper I equals integral Underscript script upper D Endscripts f left parenthesis x right parenthesis normal d x commaI =
f
D
f(x)dx,
it  may be possible to decompose the  integrand  into a PDF  over  D and some 
other function, as 
upper I equals integral Underscript script upper D Endscripts g left parenthesis x right parenthesis p Subscript upper X Baseline left parenthesis x right parenthesis normal d x commaI =
f
D
g(x)pX(x)dx,
(7.59) 
where pX(x) is a “nice” PDF with support D. In that case, the integral (7.59) 
is E(g(X)), where X is a random variable with PDF pX(x). 
Now, if we can sample random numbers x1, . . . , xm from the distribution 
with PDF pX(x), we have an unbiased estimator, 
ModifyingAbove upper I With caret equals ModifyingAbove normal upper E left parenthesis g left parenthesis upper X right parenthesis right parenthesis With caret equals StartFraction 1 Over m EndFraction sigma summation g left parenthesis x Subscript i Baseline right parenthesis period-I =
-
E(g(X)) = 1
m
E
g(xi).
(7.60)

360
7 Real Analysis and Probability Distributions of Vectors and Matrices
Exercises 
7.1. Use Eq. (7.6), which deﬁnes the derivative of a matrix with respect to a 
scalar, to show the product rule Eq. (7.3) directly: 
StartFraction partial differential upper Y upper W Over partial differential x EndFraction equals StartFraction partial differential upper Y Over partial differential x EndFraction upper W plus upper Y StartFraction partial differential upper W Over partial differential x EndFraction period∂Y W
∂x
= ∂Y
∂x W + Y ∂W
∂x .
7.2. For the n-vector x, compute the gradient gV(x), where V(x) is the  vari-
ance of x, as given in Eq. (2.74). 
Hint: Use the chain rule. 
7.3. Prove ∇×  (∇f) = 0.  
(This is Eq. (7.13) expressed in diﬀerent notation.) 
7.4. Prove ∇·  (∇×  f) = 0.  
(This is Eq. (7.14) expressed in diﬀerent notation.) 
7.5. For the square, nonsingular matrix Y , derive Eq. (7.15): 
StartFraction partial differential upper Y Superscript negative 1 Baseline Over partial differential x EndFraction equals minus upper Y Superscript negative 1 Baseline StartFraction partial differential upper Y Over partial differential x EndFraction upper Y Superscript negative 1 Baseline period∂Y −1
∂x
= −Y −1 ∂Y
∂x Y −1.
Hint: Diﬀerentiate Y Y  −1 = I. 
7.6. Let 
upper D equals left brace Start 2 By 2 Matrix 1st Row 1st Column c 2nd Column negative s 2nd Row 1st Column s 2nd Column c EndMatrix colon negative 1 less than or equals c less than or equals 1 comma c squared plus s squared equals 1 right brace periodD =
{[
c −s
s
c
]
: −1 ≤c ≤1, c2 + s2 = 1
}
.
Evaluate the Haar measure μ(D). (This is the class of 2 × 2 rotation 
matrices; see Eq. (4.3), page 222.) 
7.7. Let A be an n × m random matrix whose elements all have continuous 
marginal distributions. (By “continuous distribution,” we mean that any 
set of zero Lebesgue measure has zero probability.) In addition, assume 
that all nondegenerate conditional distributions of all elements are con-
tinuous (i.e., assume that the joint distribution is nonsingular). 
Show that Pr(A is full rank) = 1. 
Hint: Let r = min(n, m). If r = 1, it is proven. Otherwise, without loss 
of generality, assume r = m, and  show  that  for any  two  ai∗ and aj∗ and 
any ﬁxed ci, cj /= 0,  Pr(ciai∗ + cjaj∗ = 0) = 0. Then extend this to all 
sets of columns. 
7.8. The moment-generating function. In these exercises, you only need to 
make simple manipulations of the deﬁnition. 
a) Show that if ψX(t) is the MGF of the random variable X, then the 
raw moments of X are given by Eq. (7.31): 
StartFraction normal d Superscript k Baseline psi Subscript upper X Baseline left parenthesis t right parenthesis Over normal d t Superscript k Baseline EndFraction vertical bar Subscript t equals 0 Baseline equals normal upper E left parenthesis upper X Superscript k Baseline right parenthesis comma dkψX(t)
dtk
||||
t=0
= E(Xk),
for k = 1, 2, . . ..

Exercises
361
b) Let X1, . . . , Xn be iid random variables with an MGF ψX(t), and let 
Y = E
i Xi. Prove  Eq. (7.32): 
psi Subscript upper Y Baseline left parenthesis t right parenthesis equals left parenthesis psi Subscript upper X Baseline left parenthesis t right parenthesis right parenthesis Superscript n Baseline periodψY (t) = (ψX(t))n.
c) Expand the integrand to show that the MGF of the normal distribu-
tion is as shown in Eq. (7.42). 
d) Use the deﬁnition of the MGF and the PDF of the chi-squared dis-
tribution with 1 degree of freedom as given in Eq. (7.49) to show that 
the MGF of the chi-squared distribution with 1 degree of freedom is 
as shown in Eq. (7.50). 
e) By diﬀerentiation of the MGF of the normal distribution, show that 
the mean is μ and the variance-covariance is Σ. 
7.9. The Wishart distribution. 
a) Show that the probability density function for the (central) Wishart 
distribution is proportional to 
normal e Superscript minus normal t normal r left parenthesis normal upper Sigma Super Superscript negative 1 Superscript upper W divided by 2 right parenthesis Baseline StartAbsoluteValue upper W EndAbsoluteValue Superscript left parenthesis n minus d minus 1 right parenthesis divided by 2 Baseline commae−tr(Σ−1W/2)|W|(n−d−1)/2,
and determine the constant of proportionality. 
b) Let W1 ∼ Wd(Σ, n, Δ1), and let W2 ∼ Wd(Σ, m, Δ2) and be inde-
pendent of W1. Show that 
normal upper W 1 plus normal upper W 2 tilde normal upper W Subscript d Baseline left parenthesis normal upper Sigma comma n plus m comma normal upper Delta 1 plus normal upper Delta 2 right parenthesis periodW1 + W2 ∼Wd(Σ, n + m, Δ1 + Δ2).
Hint: Using the deﬁnition of the Wishart distribution, you can assume 
that there are n+m independent d-variate normal random variables. 
c) Let W ∼ Wd(Σ, n, Δ), and let A be a ﬁxed q × d matrix. Show that 
upper A normal upper W upper A Superscript normal upper T Baseline tilde normal upper W Subscript q Baseline left parenthesis upper A normal upper Sigma upper A Superscript normal upper T Baseline comma n comma upper A normal upper Delta upper A Superscript normal upper T Baseline right parenthesis periodAWAT ∼Wq(AΣAT, n, AΔAT).
d) Let W ∼ Wd(σ2 I, n, Δ). Show that 
StartFraction 1 Over sigma squared EndFraction upper W Subscript i i Baseline tilde chi Subscript n Superscript 2 Baseline left parenthesis normal upper Delta Subscript i i Baseline right parenthesis period 1
σ2 Wii ∼χ2
n(Δii).
(The PDF of the noncentral chi-squared distribution is given in 
Eq. (7.52) on page 353.)

362
7 Real Analysis and Probability Distributions of Vectors and Matrices
R Exercises 
7.10. Use one of the methods of Sect. 7.4.2 to generate random variates from 
a distribution with PDF f(x) = 2x for 0 ≤ x ≤ 1 and  0 elsewhere.  
7.11. Generating random multivariate normal variates. 
The rmvnorm function in the mvtnorm package has an argument 
method=c("eigen", "svd", "chol")} 
to determine how d iid N(0, 1) variates are converted to a single variate 
from a Nd(0, Σ). 
a) Write three R functions rmvnorm1, rmvnorm2, and  rmvnorm3 to gener-
ate n random multivariate normal variates with 0 mean and a spec-
iﬁed variance-covariance matrix, Σ. Each function should have an 
interface similar to the basic part of rmvnorm: 
rmvnormx(n, mean = rep(0, nrow(sigma)), 
sigma = diag(length(mean))) { 
(Note that diag(k), for a positive integer k, is a  k × k identity ma-
trix.) These functions are to invoke rnorm to get nd iid N(0, 1) ran-
dom variates and then use, respectively, an eigenvalue or spectral 
decomposition (page 177 and page 251), a singular value decomposi-
tion (page 183), and a Cholesky decomposition (page 245) to deter-
mine a transformation matrix to form a random multivariate normal 
variate from Nd(0, Id) to a random multivariate normal variate from 
Nd(0, Σ). 
The functions return an n by length(mean) matrix with one obser-
vation in each row. 
b) Let 
mu equals left parenthesis 3 comma 0 comma negative 3 right parenthesisμ = (3, 0, −3)
and 
normal upper Sigma equals Start 3 By 3 Matrix 1st Row 1st Column 5 2nd Column 2 3rd Column 1 2nd Row 1st Column 2 2nd Column 4 3rd Column 1 3rd Row 1st Column 1 2nd Column 1 3rd Column 3 EndMatrix periodΣ =
⎡
⎣
5 2 1
2 4 1
1 1 3
⎤
⎦.
Use each of your functions from Exercise 7.11a to generate 1,000 
random multivariate normal deviates from N3(μ, Σ) and compute the 
variance-covariance for each sample of 1,000. 
Compare your sample means and sample variance-covariance matri-
ces with μ and Σ. 
7.12. Random orthogonal matrices. 
a) Write an R function to generate n d × d random orthogonal matrices 
with the Haar uniform distribution.

Exercises
363
rhaar(n, d) { 
Use the following method due to Heiberger (1978), which was modi-
ﬁed by Stewart (1980). (See also Tanner and Thisted 1982.) 
1. Generate d−1 independent i-vectors, x2, x3, . . . , xd, from  Ni(0, Ii) 
(xi is of length i). Use rnorm. 
2. Let ri = ||xi||2, and  let -Hi be the i × i reﬂection matrix that 
transforms xi into the i-vector (ri, 0, 0, . . . , 0). 
3. Let Hi be the d × d matrix 
Start 2 By 2 Matrix 1st Row 1st Column upper I Subscript d minus i Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column upper H overTilde Subscript i Baseline EndMatrix comma
[ Id−i 0
0
-Hi
]
,
and form the diagonal matrix, 
upper J equals normal d normal i normal a normal g left parenthesis left parenthesis negative 1 right parenthesis Superscript b 1 Baseline comma left parenthesis negative 1 right parenthesis Superscript b 2 Baseline comma ellipsis comma left parenthesis negative 1 right parenthesis Superscript b Super Subscript d Superscript Baseline right parenthesis commaJ = diag
(
(−1)b1, (−1)b2, . . . , (−1)bd)
,
where the bi are independent realizations of a Bernoulli random 
variable. Use rbinom. 
4. Deliver the orthogonal matrix Q = JH1H2 · · · Hd. 
The matrix Q generated in this way is orthogonal and has a Haar 
distribution. 
b) Can you think of any way to test the goodness of ﬁt of samples from 
this algorithm? Generate a sample of 1,000 2 × 2 random orthogonal 
matrices, and assess how well the sample follows a Haar uniform 
distribution. 
7.13. Random correlation matrices. 
A correlation matrix can be deﬁned in terms of a Gramian matrix 
formed by a centered and scaled matrix (see page 405). Sometimes in 
the development of statistical theory, we are interested in the properties 
of correlation matrices with given eigenvalues or with given ratios of 
the largest eigenvalue to other eigenvalues. 
a) Write an R function to generate n d × d random correlation matrices 
R with speciﬁed eigenvalues, c1, . . . , cd. 
rcorr(n, eigen) { 
The only requirements on R are that its diagonals be 1, that it be 
symmetric, and that its eigenvalues all be positive and sum to d. 
Use the following method due to Davies and Higham (2000) that  
uses random orthogonal matrices with the Haar uniform distribution 
generated using the method described in Exercise 7.12a. 
0. Generate a random orthogonal matrix Q; set  k = 0,  and  form  
upper R Superscript left parenthesis 0 right parenthesis Baseline equals upper Q normal d normal i normal a normal g left parenthesis left parenthesis c 1 comma ellipsis comma c Subscript d Baseline right parenthesis right parenthesis upper Q Superscript normal upper T Baseline periodR(0) = Qdiag((c1, . . . , cd))QT.

364
7 Real Analysis and Probability Distributions of Vectors and Matrices
1. If r (k) 
ii = 1 for all i in {1, . . . , d}, go to step 3. 
2. Otherwise, choose p and q with p < j, such that r (k) 
pp < 1 < r  
(k) 
qq 
or r (k) 
pp > 1 > r  (k) 
qq , and  form  G(k) as in Eq. (4.13), where c and s 
are as in Eqs. (4.17) and  (4.18), with a = 1.  
Form R(k+1) = (G(k) )T R(k) G(k) . 
Set k = k + 1, and go to step 1. 
3. Deliver R = R(k) . 
b) Using the R function from Exercise 7.13a, generate a 5×5 correlation 
matrix with eigenvalues (1, 2, 3, 4). Next, compute the eigenvalues of 
your generated matrix, and compare them with the desired eigenval-
ues. 
7.14. Use R to obtain a Monte Carlo estimate of the integral 
integral Underscript normal fraktur upper R squared Endscripts parallel to x parallel to exp left parenthesis minus x Superscript normal upper T Baseline upper S x right parenthesis normal d x comma
f
IR2 ||x|| exp(−xTSx)dx,
where 
upper S equals Start 2 By 2 Matrix 1st Row 1st Column 2 2nd Column 1 2nd Row 1st Column 1 2nd Column 4 EndMatrix periodS =
[ 2 1
1 4
]
.

Part II 
Applications in Statistics and Data Science

8 
Special Matrices and Operations Useful 
in Modeling and Data Science 
In previous chapters, we encountered a number of special matrices, such as 
symmetric matrices, banded matrices, elementary operator matrices, and so 
on. In this chapter, we will discuss some of these matrices in more detail and 
also introduce some other special matrices and data structures that are useful 
in statistics. 
In statistical applications in which data analysis is the objective, the ini-
tial step is the representation of observational data in some convenient form, 
which often is a matrix. The matrices for operating on observational data or 
summarizing the data often have special structures and properties. We discuss 
the representation of observations using matrices in Sect. 8.1. 
Matrices holding data are often manipulated into symmetric Gramian ma-
trices, and in Sect. 8.2, we review and discuss some of the properties of sym-
metric matrices. 
One of the most important properties of Gramian matrices that occur 
in statistical data analysis is nonnegative or positive deﬁniteness; this is the 
subject of Sects. 8.3 and 8.4. 
Fitted values of a response variable that are associated with given values 
of covariates in linear models are often projections of the observations onto 
a subspace determined by the covariates. Projection matrices and Gramian 
matrices useful in linear models are considered in Sects. 8.5 and 8.6. 
Another important property of many matrices occurring in statistical 
modeling is irreducible nonnegativeness or positiveness; this is the subject 
of Sect. 8.7. 
Many of the deﬁning properties of the special matrices discussed in this 
chapter are invariant under scalar multiplication; hence, the special matrices 
are members of cones. Interestingly even further, convex combinations of some 
types of special matrices yield special matrices of the same type; hence, those 
special matrices are members of convex cones (see Sect. 2.2.8 beginning on 
page 53). 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 8 
367

368
8 Matrices with Special Properties
8.1 Data Matrices and Association Matrices 
There are several ways that data can be organized for representation in the 
computer. We distinguish logical structures from computer-storage structures. 
Data structure in computers is an important concern and can greatly aﬀect 
the eﬃciency of computer processing. We discuss some simple aspects of the 
organization for computer storage in Sect. 11.1, beginning on page 591. In the  
present section, we consider some general issues of logical organization and 
structure. 
There are two important aspects of data in applications that we will not 
address here. One is metadata, that is, data about the data. Metadata includes 
names or labels associated with data, information about how and when the 
data were collected, information about how the data are stored in the com-
puter, and so on. Another important concern in applications is missing data. 
In real-world applications it is common to have incomplete data. If the data 
are stored in some structure that naturally contains a cell or a region for the 
missing data, the computer representation of the dataset must contain some 
indication that the cell is empty. For numeric data, the convenient way of 
doing this is by using “not-available,” NA (see pages 14 and 510), or “not-
a-number,” NaN (see pages 14 and 541). The eﬀect on a statistical analysis 
when some data are missing varies with the type of analysis. We consider some 
eﬀects of missing data on the estimation of variance-covariance matrices, for 
example, in Sect. 9.4.6, beginning on page 483. 
8.1.1 Flat Files 
If several features or attributes are observed on each of several entities, a 
convenient way of organizing the data is as a two-dimensional array with 
each column corresponding to a speciﬁc feature and each row corresponding 
to a speciﬁc observational entity. In the statistical applications, data for the 
features are stored in “variables,” the entities are called “observational units,” 
and a row of the array is called an “observation” (see Fig. 8.1). 
Figure 8.1. Data appropriate for representation in a ﬂat ﬁle 
The data may be various types of objects, such as names, real numbers, 
numbers with associated measurement units, sets, vectors, and so on. If the 
data are represented as real numbers, the data array is a matrix. (Note again

8.1 Data Matrices and Association Matrices
369
our use of the word “matrix”; not just any rectangular array is a matrix in the 
sense used in this book.) Other types of data can often be made equivalent to 
a matrix in an intuitive manner. 
The ﬂat ﬁle arrangement emphasizes the relationships of the data both 
within an observational unit or row and within a variable or column. Simple 
operations on the data matrix may reveal relationships among observational 
units or among variables. 
Flat ﬁles are the appropriate data structure for the analysis of linear mod-
els, but statistics is not just about analysis of linear models anymore. (It never 
was.) 
8.1.2 Graphs and Other Data Structures 
If the numbers of measurements on the observational units varies or if the in-
terest is primarily in simple relationships among observational units or among 
variables, the ﬂat ﬁle structure may not be very useful. Sometimes a graph 
structure can be used advantageously. 
A graph is a nonempty set V of points, called vertices, together with a 
collection E of unordered pairs of elements of V , called edges. (Other deﬁni-
tions of “graph” allow the null set to be a graph.) If we let G be a graph, we 
represent it as (V, E). We often represent the set of vertices as V (G) and  the  
collection of edges as E(G). An edge is said to be incident on each vertex in 
the edge. The number of vertices (i.e., the cardinality of V ) is the  order of the 
graph, and the number of edges, the cardinality of E, is the  size of the graph. 
An edge in which the two vertices are the same is called a loop. If two  or  
more elements of E(G) contain the same two vertices, those edges are called 
multiple edges, and the graph itself is called a multigraph. A graph with no 
loops and with no multiple edges is called a simple graph. In some literature, 
“graph” means “simple graph,” as I have deﬁned it; and a graph as I have 
deﬁned it that is not simple is called a “pseudograph.” 
A path or walk is a sequence of edges, e1, . . . , en, such that for i ≥ 2 one  
vertex in ei is a vertex in edge ei−1. Alternatively, a path or walk is deﬁned 
as a sequence of vertices with common edges. 
A graph such that there is a path that includes any pair of vertices is said 
to be connected. 
A graph with more than one vertex such that all possible pairs of vertices 
occur as edges is a complete graph. 
A closed path or closed walk is a path such that a vertex in the ﬁrst edge 
(or the ﬁrst vertex in the alternate deﬁnition) is in the last edge (or the last 
vertex). 
A cycle is a closed path in which all vertices occur exactly twice (or in 
the alternate deﬁnition, in which all vertices except the ﬁrst and the last are 
distinct). A graph with no cycles is said to be acyclic. An acyclic graph is also 
called a tree. Trees are used extensively in statistics to represent clusters.

370
8 Matrices with Special Properties
The number of edges that contain a given vertex (i.e., the number of edges 
incident on the vertex v) denoted by d(v) is the  degree of the vertex. 
A vertex with degree 0 is said to be isolated. 
We see immediately that the sum of the degrees of all vertices equals twice 
the number of edges, that is, 
sigma summation d left parenthesis v Subscript i Baseline right parenthesis equals 2 number sign left parenthesis upper E right parenthesis period
E
d(vi) = 2#(E).
The sum of the degrees hence must be an even number. 
A regular graph is one for which d(vi) is constant for all vertices vi; more  
speciﬁcally, a graph is k-regular if d(vi) =  k for all vertices vi. 
The natural data structure for a graph is a pair of lists, but a graph is often 
represented graphically (no pun!) as in Fig. 8.2, which shows a graph with 
ﬁve vertices and seven edges. While a matrix is usually not an appropriate 
structure for representing raw data from a graph, there are various types of 
matrices that are useful for studying the data represented by the graph, which 
we will discuss in Sect. 8.8.9. 
Figure 8.2. A simple graph 
If G is the graph represented in Fig. 8.2, the vertices are 
upper V left parenthesis script upper G right parenthesis equals StartSet a comma b comma c comma d comma e EndSetV (G) = {a, b, c, d, e}
and the edges are 
upper E left parenthesis script upper G right parenthesis equals StartSet left parenthesis a comma b right parenthesis comma left parenthesis a comma c right parenthesis comma left parenthesis a comma d right parenthesis comma left parenthesis a comma e right parenthesis comma left parenthesis b comma e right parenthesis comma left parenthesis c comma d right parenthesis comma left parenthesis d comma e right parenthesis EndSet periodE(G) = {(a, b), (a, c), (a, d), (a, e), (b, e), (c, d), (d, e)}.
The presence of an edge between two vertices can indicate the existence 
of a relationship between the objects represented by the vertices. The graph 
represented in Fig. 8.2 may represent ﬁve observational units for which our 
primary interest is in their relationships with one another. For example, the 
observations may be authors of scientiﬁc papers, and an edge between two 
authors may represent the fact that the two have been coauthors on some 
paper.

8.1 Data Matrices and Association Matrices
371
Figure 8.3. An alternate representation 
The same information represented in the 5-order graph of Fig. 8.2 may be 
represented in a 5 × 5 rectangular array, as in Fig. 8.3. 
In the graph represented in Fig. 8.2, there are no isolated vertices and the 
graph is connected. (Note that a graph with no isolated vertices is not neces-
sarily connected.) The graph represented in Fig. 8.2 is not complete because, 
for example, there is no edge that contains vertices c and e. The graph is cyclic 
because of the closed path (deﬁned by vertices) (c, d, e, b, a, c). Note that the 
closed path (c, d, a, e, b, a, c) is  not  a  cycle.  
This use of a graph immediately suggests various extensions of a basic 
graph. For example, E may be a multiset, with multiple instances of edges 
containing the same two vertices, perhaps, in the example above, representing 
multiple papers in which the two authors are coauthors. As we stated above, 
a graph in which E is a multiset is called a multigraph. Instead of just the 
presence or absence of edges between vertices, a weighted graph may be more 
useful, that is, one in which a real number is associated with a pair of vertices 
to represent the strength of the relationship, not just presence or absence, 
between the two vertices. A degenerate weighted graph (i.e., an unweighted 
graph as discussed above) has weights of 0 or 1 between all vertices. A multi-
graph is a weighted graph in which the weights are restricted to nonnegative 
integers. Although the data in a weighted graph carry much more information 
than a graph with only its edges, or even a multigraph that allows strength to 
be represented by multiple edges, the simplicity of a graph sometimes recom-
mends its use even when there are varying degrees of strength of relationships. 
A standard approach in applications is to set a threshold for the strength of 
relationship and to deﬁne an edge only when the threshold is exceeded. 
Adjacency Matrix: Connectivity Matrix 
The connections between vertices in the graphs shown in Fig. 8.2 or in Fig. 8.4 
can be represented in an association matrix called an adjacency matrix, a  con-
nectivity matrix, or an  incidence matrix to represent edges between vertices, as 
shown in Eq. (8.1). (The terms “adjacency,” “connectivity,” and “incidence” 
are synonymous. “Adjacency” is perhaps the most commonly used term, but 
I will naturally use both that term and “connectivity” because of the conno-
tative value of the latter term.) The graph, G, represented in Fig. 8.2 has the 
symmetric adjacency matrix

372
8 Matrices with Special Properties
upper A left parenthesis script upper G right parenthesis equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 1 5th Column 1 2nd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 3rd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 4th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 5th Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 EndMatrix periodA(G) =
⎡
⎢⎢⎢⎢⎣
0 1 1 1 1
1 0 0 0 1
1 0 0 1 0
1 0 1 0 1
1 1 0 1 0
⎤
⎥⎥⎥⎥⎦
.
(8.1) 
As above, we often use this kind of notation; a symbol, such as G, represents a  
particular graph, and other objects that relate to the graph make use of that 
symbol. 
There is no diﬀerence in the connectivity matrix and a table such as in 
Fig. 8.3 except for the metadata. 
A graph as we have described it is “nondirected”; that is, an edge has 
no direction. The edge (a, b) is the  same  as  (b, a). An adjacency matrix for a 
nondirected graph is symmetric. 
An interesting property of an adjacency matrix, which we will discuss 
further on page 430, is that if  A is the adjacency matrix of a graph, then Ak 
ij 
is the number of paths of length k between nodes i and j in that graph. (See 
Exercise 8.20.) 
The adjacency matrix for graph with no loops is hollow; that is, all diagonal 
elements are 0s. Another common way of representing a hollow adjacency 
matrix is to use −1s in place of the oﬀ-diagonal zeros; that is, the absence 
of a connection between two diﬀerent vertices is denoted by −1 instead of 
by 0. Such a matrix is called a Seidel adjacency matrix. (This matrix has no 
relationship to the Gauss-Seidel method discussed in Chap. 5.) 
The relationship can obviously be deﬁned in the other direction; that is, 
given an n × n symmetric matrix A, we deﬁne the graph of the matrix as the 
graph with n vertices and edges between vertices i and j if aij /= 0. We often 
denote the graph of the matrix A by G(A). 
Generally we restrict the elements of the connectivity matrix to be 1 or 0 to 
indicate only presence or absence of a connection, but not to indicate strength 
of the connection. In this case, a connectivity matrix is a nonnegative matrix; 
that is, all of its elements are nonnegative. We indicate that a matrix A is 
nonnegative by 
upper A greater than or equals 0 periodA ≥0.
We discuss the notation and properties of nonnegative (and positive) matrices 
in Sect. 8.7. 
Digraphs 
Another extension of a basic graph is one in which the relationship may not 
be the same in both directions. This yields a digraph, or “directed graph,” 
in which the edges are ordered pairs called directed edges. The vertices in 
a digraph have two kinds of degree, an indegree and an outdegree, with the  
obvious meanings.

8.1 Data Matrices and Association Matrices
373
The simplest applications of digraphs are for representing networks. Con-
sider, for example, the digraph represented by the network in Fig. 8.4. This is  
a network with ﬁve vertices, perhaps representing cities, and directed edges 
between some of the vertices. The edges could represent airline connections 
between the cities; for example, there are ﬂights from x to u and from u to x, 
and from y to z, but not from z to y. 
Figure 8.4. A simple digraph 
Figure 8.4 represents a digraph with order 5 (there are 5 vertices) and size 
11 (11 directed edges). A sequence of edges, e1, . . . , en, constituting a path in 
a digraph must be such that for i ≥ 2 the ﬁrst vertex in ei is the second vertex 
in edge ei−1. For example, the sequence x, y, z, w, u, x in the graph of Fig. 8.4 
is a path (in fact, a cycle) but the sequence x, u, w, z, y, x is not a path. 
The connectivity matrix for the digraph in Fig. 8.4 with nodes ordered as 
u, w, x, y, z is 
upper C equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 1 5th Column 1 2nd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 3rd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 4th Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 5th Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 EndMatrix periodC =
⎡
⎢⎢⎢⎢⎣
0 1 1 1 1
1 0 0 0 0
1 0 0 1 0
1 0 0 0 1
1 1 0 0 0
⎤
⎥⎥⎥⎥⎦
.
(8.2) 
A connectivity matrix for a (nondirected) graph is symmetric, but for a di-
graph it is not necessarily symmetric. Given an n × n matrix A, we deﬁne the 
digraph of the matrix as the digraph with n vertices and edges from vertex i 
to j if aij /= 0. We use the same notation for a digraph as we used above for 
a graph, G(A). 
In statistical applications, graphs are used for representing symmetric as-
sociations. Digraphs are used for representing asymmetric associations or one-
way processes such as a stochastic process. 
In a simple digraph, the edges only indicate the presence or absence of a 
relationship, but just as in the case of a simple graph, we can deﬁne a weighted 
digraph by associating nonnegative numbers with each directed edge. 
Graphical modeling is useful for analyzing relationships between elements 
of a collection of sets. For example, in an analysis of internet traﬃc, proﬁles 
of users may be constructed based on the set of websites each user visits in 
relation to the sets visited by other users. For this kind of application, an 
intersection graph may be useful. An intersection graph, for a given collection

374
8 Matrices with Special Properties
of sets S, is a graph whose vertices correspond to the sets in S and whose 
edges between any two sets have a common element. 
The word “graph” is often used without qualiﬁcation to mean any of these 
types. 
Connectivity of Digraphs 
There are two kinds of connected digraphs. A digraph such that there is 
a (directed) path that includes any pair of vertices is said to be strongly 
connected. A digraph such that there is a path without regard to the direction 
of any edge that includes any pair of vertices is said to be weakly connected. 
The digraph  shown in Fig. 8.4 is strongly connected. The digraph shown in 
Fig. 8.5 is weakly connected but not strongly connected. 
A digraph that is not weakly connected must have two sets of nodes with 
no edges between any nodes in one set and any nodes in the other set. 
Figure 8.5. A digraph that is not strongly connected 
The connectivity matrix of the digraph in Fig. 8.5 is 
upper C equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 0 5th Column 1 2nd Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 4th Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 5th Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 EndMatrix periodC =
⎡
⎢⎢⎢⎢⎣
0 1 1 0 1
0 0 0 0 0
0 0 0 1 0
1 0 0 0 1
0 1 0 0 0
⎤
⎥⎥⎥⎥⎦
.
(8.3) 
The matrix of a digraph that is not strongly connected can always be reduced 
to a special block upper triangular form by row and column permutations; that 
is, if the  digraph  G is not strongly connected, then there exists a permutation 
matrix E(π) such that 
upper E Subscript left parenthesis pi right parenthesis Baseline upper A left parenthesis script upper G right parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline equals Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column 0 2nd Column upper B 22 EndMatrix commaE(π)A(G)E(π) =
⎡
B11 B12
0
B22
⎤
,
(8.4) 
where B11 and B22 are square. Such a transformation is called a symmetric 
permutation. 
Later we will formally prove this relationship between strong connectivity 
and this reduced form of the matrix, but ﬁrst we consider the matrix in

8.1 Data Matrices and Association Matrices
375
Eq. (8.3). If we interchange the second and fourth columns and rows, we get 
the reduced form 
upper E 24 upper C upper E 24 equals Start 5 By 5 Matrix 1st Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 1 5th Column 1 2nd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column 1 3rd Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 5th Column 0 EndMatrix periodE24CE24 =
⎡
⎢⎢⎢⎢⎣
0 0 1 1 1
1 0 0 0 1
0 1 0 0 0
0 0 0 0 0
0 0 0 1 0
⎤
⎥⎥⎥⎥⎦
.
Irreducible Matrices 
Any nonnegative square matrix that can be permuted into the form in 
Eq. (8.4) with square diagonal submatrices is said to be reducible; a matrix  
that cannot be put into that form is irreducible. We also use the terms re-
ducible and irreducible to refer to the graph itself. 
Irreducible matrices have many interesting properties, some of which we 
will discuss in Sect. 8.7.2, beginning on page 412. The implication (8.77) in  
that section provides a simple characterization of irreducibility. 
Strong Connectivity of Digraphs and Irreducibility of Matrices 
A nonnegative matrix is irreducible if and only if its digraph is strongly con-
nected. Stated another way, a digraph is not strongly connected if and only if 
its matrix is reducible. 
To see this, ﬁrst consider a reducible matrix. In its reduced form of 
Eq. (8.4), none of the nodes corresponding to the last rows have directed edges 
leading to any of the nodes corresponding to the ﬁrst rows; hence, the digraph 
is not strongly connected. 
Now, assume that a given digraph G is not strongly connected. In that 
case, there is some node, say the ith node, from which there is no directed 
path to some other node. Assume that there are m −1 nodes that can be 
reached from node i. If  m = 1, then we have a trivial partitioning of the 
n × n connectivity in which B11 of Eq. (8.4) is (n − 1) × (n − 1) and B22 is 
a 1  × 1 zero matrix (i.e., 01). If m ≥ 1, perform symmetric permutations so 
that the rows corresponding to node i and all other m − 1 nodes  are the  last  
m rows of the permuted connectivity matrix. In this case, the ﬁrst n − m 
elements in each of those rows must be 0. To see that this must be the case, 
let k >  n  − m and j ≤ n − m and assume that the element in the (k, j)th 
position is nonzero. In that case, there is a path from node i to node k to node 
j, which is in the set of nodes not reachable from node i; hence, the (k, j)th 
element (in the permuted matrix) must be 0. The submatrix corresponding 
to B11 is n − m × n − m, and that corresponding to B22 is m × m. These  
properties also hold for connectivity matrices with simple loops (with 1s on 
the diagonal) and for an augmented connectivity matrix (see page 430). 
Reducibility plays an important role in the analysis of Markov chains (see 
Sect. 9.5.1).

376
8 Matrices with Special Properties
8.1.3 Term-by-Document Matrices 
An interesting area of statistical application is in clustering and classifying 
documents. In the simpler cases, the documents consist of text only, and 
much recent research has been devoted to “text data-mining.” (The problem 
is not a new one; Mosteller and Wallace 1963, studied a related problem with 
the Federalist Papers.) 
We have a set of text documents (often called a “corpus”). A basic set 
of data to use in studying the text documents is the term-document matrix, 
which is a matrix whose columns correspond to documents and whose rows 
correspond to the various terms used in the documents. The terms are usu-
ally just words. The entries in the term-document matrix are measures of 
the importance of the term in the document. Importance may be measured 
simply by the number of times the word occurs in the document, possibly 
weighted by some measure of the total number of words in the document. In 
other measures of the importance, the relative frequency of the word in the 
given document may be adjusted for the relative frequency of the word in the 
corpus. (Such a measure is called term frequency-inverse document frequency 
or tf-idf.) Certain common words, such as “the,” called “stop-words,” may be 
excluded from the data. Also, words may be “stemmed”; that is, they may 
be associated with a root that ignores modifying letters such as an “s” in a 
plural or an “ed” in a past-tense verb. Other variations include accounting for 
sequences of terms (called “collocation”). 
There are several interesting problems that arise in text analysis. Term-
document matrices can be quite large. Terms are often misspelled. The docu-
ments may be in diﬀerent languages. Some terms may have multiple meanings. 
The documents themselves may be stored on diﬀerent computer systems. 
Two types of manipulation of the term-document matrix are commonly 
employed in the analysis. One is singular value decomposition (SVD), and the 
other is nonnegative matrix factorization (NMF). 
SVD is used in what is called “latent semantic analysis” (“LSA”) or “la-
tent semantic indexing” (“LSI”). A variation of latent semantic analysis is 
“probabilistic” latent semantic analysis, in which a probability distribution is 
assumed for the words. In a speciﬁc instance of probabilistic latent semantic 
analysis, the probability distribution is a Dirichlet distribution. (Most users 
of this latter method are not statisticians; they call the method “LDA,” for 
“latent Dirichlet allocation.”) 
NMF is often used in determining which documents are similar to each 
other or, alternatively, which terms are clustered in documents. If A is the 
term-document matrix, and A = WH is a nonnegative factorization, then 
the element wij can be interpreted as the degree to which term i belongs 
to cluster j, while element hij can be interpreted as the degree to which 
document j belongs to cluster i. A method of clustering the documents is to 
assign document j (corresponding to the jth column of A) to the  kth cluster 
if hkj is the maximum element in the column h∗j.

8.1 Data Matrices and Association Matrices
377
There is a wealth of literature on text data-mining, but we will not discuss 
the analysis methods further. 
8.1.4 Sparse Matrices 
Connectivity and term-document matrices tend to be sparse. In applications 
in data science, there may be very large numbers of vertices or terms and docu-
ments, so it is important to store the data in an eﬃcient manner. On page 618, 
we discuss general methods of storing sparse matrices; and on page 644, we  
discuss R support for sparse matrices. 
Since connectivity matrices contain only 1s and 0s, the storage can be 
reduced even further because no values need be stored, only the indexes. 
8.1.5 Probability Distribution Models 
Probability models in statistical data analysis are often multivariate distribu-
tions, and hence, matrices arise in the model. In the analysis itself, matrices 
that represent associations are computed from the observational data. In this 
section we mention some matrices in the models, and in the next section we 
refer to some association matrices that are computed in the analysis. 
Data in rows of ﬂat ﬁles are often assumed to be realizations of vector 
random variables, some elements of which may have a degenerate distribution 
(i.e., the elements in some columns of the data matrix may be considered 
to be ﬁxed rather than random). The data in one row are often considered 
independent of the data in another row. Statistical data analysis is generally 
concerned with studying various models of relationships among the elements 
of the vector random variables. For example, the familiar linear regression 
model relates one variable (one column in a matrix) to a linear combination 
of other variables plus a translation and random noise. 
In graphical models, a random graph of ﬁxed order is a discrete probability 
space over all possible graphs of that order. For a graph of order n, there  are  
2(
n 
2) possible graphs. Asymptotic properties of the probability distribution 
relate to the increase of the order without limit. Occasionally it is useful to 
consider the order of the graph to be random also. If the order is unrestricted, 
the sample space for a random graph of random order is inﬁnite but countable. 
The number of digraphs of order n is 4(
n 
2). 
Random graphs have many uses in the analysis of large systems of inter-
acting objects; for example, a random intersection graph may be used to make 
inferences about the clustering of internet users based on the web sites they 
visit. 
8.1.6 Derived Association Matrices 
In data analysis, the interesting questions usually involve the relationships 
among the variables or among the observational units. Matrices formed from

378
8 Matrices with Special Properties
the original data matrix for the purpose of measuring these relationships 
are called association matrices. There are basically two types: similarity and 
dissimilarity matrices. The variance-covariance matrix, which we discuss in 
Sect. 8.6.3, is an example of an association matrix that measures similarity. 
We discuss dissimilarity matrices in Sect. 8.6.6 and in Sect. 8.8.9 discuss a type 
of similarity matrix for data represented in graphs. 
In addition to the distinction between similarity and dissimilarity associ-
ation matrices, we may identify two types of association matrices based on 
whether the relationships of interest are among the rows (observations) or 
among the columns (variables or features). In applications, dissimilarity rela-
tionships among rows tend to be of more interest, and similarity relationships 
among columns are usually of more interest. (The applied statistician may 
think of clustering, multidimensional scaling, or Q factor analysis for the for-
mer and correlation analysis, principal components analysis, or factor analysis 
for the latter.) 
8.2 Symmetric Matrices and Other Unitarily 
Diagonalizable Matrices 
Most association matrices encountered in applications are real and symmetric. 
Because real symmetric matrices occur so frequently in statistical applications 
and because such matrices have so many interesting properties, it is useful to 
review some of those properties that we have already encountered and to state 
some additional properties. 
First, perhaps, we should iterate a trivial but important fact: the product 
of symmetric matrices is not, in general, symmetric. A power of a symmetric 
matrix, however, is symmetric. 
We should also emphasize that some of the special matrices we have dis-
cussed are assumed to be symmetric because, if they were not, we could deﬁne 
equivalent symmetric matrices. This includes positive deﬁnite matrices and 
more generally the matrices in quadratic forms. 
8.2.1 Some Important Properties of Symmetric Matrices 
For convenience, here we list some of the important properties of symmetric 
matrices, many of which concern their eigenvalues. 
In the following, let A be a real symmetric matrix with eigenvalues ci and 
corresponding eigenvectors vi. 
• If k is any positive integer, Ak is symmetric. 
• AB is not necessarily symmetric even if B is a symmetric matrix. 
• A ⊗B is symmetric if B is symmetric. 
• If A is nonsingular, then A−1 is also symmetric because (A−1 )T = 
(AT )−1 = A−1 .

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
379
• If A is nonsingular (so that Ak is deﬁned for nonpositive integers), Ak is 
symmetric and nonsingular for any integer k. 
• All eigenvalues of A are real (see page 162). 
• A is diagonalizable (or simple), and in fact A is orthogonally diago-
nalizable; that is, it has an orthogonally similar canonical factorization, 
A = VCV T (see page 177). 
• A has the spectral decomposition A = E
i civivT 
i , where  the  ci are the 
eigenvalues and vi are the corresponding eigenvectors (see page 178). 
• A power  of  A has the spectral decomposition Ak = E
i ck 
i vivT 
i . 
• Any quadratic form xT Ax can be expressed as E
i b2 
i ci, where  the  bi are 
elements in the vector V −1 x. 
• We have 
max Underscript x not equals 0 Endscripts StartFraction x Superscript normal upper T Baseline upper A x Over x Superscript normal upper T Baseline x EndFraction equals max left brace c Subscript i Baseline right bracemax
x/=0
xTAx
xTx = max{ci}
(see page 179). If A is nonnegative deﬁnite, this is the spectral radius ρ(A). 
• For the L2 norm of the symmetric matrix A, we have  
parallel to upper A parallel to Subscript 2 Baseline equals rho left parenthesis upper A right parenthesis period||A||2 = ρ(A).
• For the Frobenius norm of the symmetric matrix A, we have  
parallel to upper A parallel to Subscript upper F Baseline equals StartRoot sigma summation c Subscript i Superscript 2 Baseline EndRoot period||A||F =
/E
c2
i .
This follows immediately from the fact that A is diagonalizable, as do the 
following properties: 
• 
normal t normal r left parenthesis upper A right parenthesis equals sigma summation c Subscript itr(A) =
E
ci
• 
StartAbsoluteValue upper A EndAbsoluteValue equals product c Subscript i|A| =
| |
ci
(see Eqs. (3.249) and  (3.250) on page 163). 
The symmetric matrices that arise in statistical data analysis are often 
Gramian matrices of the form XT X, where  X is a matrix of observations. 
These symmetric matrices have additional properties of interest, as we have 
discussed and will again consider in Sect. 8.6.1. 
8.2.2 Approximation of Symmetric Matrices and an Important 
Inequality 
In Sect. 3.12, we considered the problem of approximating a given matrix by 
another matrix of lower rank. There are other situations in statistics in which 
we need to approximate one matrix by another one. In data analysis, this may 
be because our given matrix arises from poor observations and we know the 
“true” matrix has some special properties not possessed by the given matrix

380
8 Matrices with Special Properties
computed from the data. A familiar example is a sample variance-covariance 
matrix computed from incomplete data (see Sect. 9.4.6). Other examples in 
statistical applications occur in the simulation of random matrices (see Gentle 
2003, Section 5.3.3). In most cases of interest, the matrix to be approximated 
is a symmetric matrix. 
Consider the diﬀerence of two symmetric n × n matrices, A and -A; that  
is, 
upper E equals upper A minus upper A overTilde periodE = A −-A.
(8.5) 
The matrix of the diﬀerences, E, is also symmetric. We measure the “close-
ness” of A and -A by some norm of E. 
The Hoﬀman-Wielandt theorem gives a lower bound on the Frobenius 
norm of E in terms of the diﬀerences of the eigenvalues of A and -A: if the  
eigenvalues of A are c1, . . . cn and the eigenvalues of -A are ˜c1, . . .  ˜cn, each set  
being arranged in nonincreasing order, we have 
sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis c Subscript i Baseline minus c overTilde Subscript i Baseline right parenthesis squared less than or equals parallel to upper E parallel to Subscript normal upper F Superscript 2 Baseline period
n
E
i=1
(ci −˜ci)2 ≤||E||2
F.
(8.6) 
This fact was proved by Hoﬀman and Wielandt (1953) using techniques from 
linear programming. Wilkinson (1965) gives a simpler proof (which he at-
tributes to Wallace Givens) along the following lines. 
Because A, -A, and  E are symmetric, they are all orthogonally diagonaliz-
able. Let the diagonal factorizations of A and E, respectively, be VCV T and 
Udiag((e1, . . . , en))U T , where  e1, . . . en are the eigenvalues of E in nonincreas-
ing order. Hence, we have 
StartLayout 1st Row 1st Column upper U normal d normal i normal a normal g left parenthesis left parenthesis e 1 comma ellipsis comma e Subscript n Baseline right parenthesis right parenthesis upper U Superscript normal upper T 2nd Column equals 3rd Column upper U left parenthesis upper A minus upper A overTilde right parenthesis upper U Superscript normal upper T 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper U left parenthesis upper V upper C upper V Superscript normal upper T Baseline minus upper A overTilde right parenthesis upper U Superscript normal upper T 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper U upper V left parenthesis upper C minus upper V Superscript normal upper T Baseline upper A overTilde upper V right parenthesis upper V Superscript normal upper T Baseline upper U Superscript normal upper T Baseline period EndLayoutUdiag((e1, . . . , en))U T = U(A −-A)U T
= U(VCV T −-A)U T
= UV(C −V T -AV )V TU T.
Taking norms of both sides, we have 
sigma summation Underscript i equals 1 Overscript n Endscripts e Subscript i Superscript 2 Baseline equals parallel to upper C minus upper V Superscript normal upper T Baseline upper A overTilde upper V parallel to squared period
n
E
i=1
e2
i = ||C −V T -AV ||2.
(8.7) 
(All norms in the remainder of this section will be the Frobenius norm.) Now, 
let 
f left parenthesis upper Q right parenthesis equals parallel to upper C minus upper Q Superscript normal upper T Baseline upper A overTilde upper Q parallel to squaredf(Q) = ||C −QT -AQ||2
(8.8) 
be a function of any n × n orthogonal matrix, Q. (Eq. (8.7) yields f(V ) =
E e2 
i .) To arrive at inequality (8.6), we show that this function is bounded 
below by the sum of the diﬀerences in the squares of the elements of C (which 
are the eigenvalues of A) and the eigenvalues of QT -AQ (which are the eigen-
values of the matrix approximating A).

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
381
Because the elements of Q are bounded, f(·) is bounded, and because the 
set of orthogonal matrices is compact (see page 156) and  f(·) is continuous, 
f(·) must attain its lower bound, say l. To simplify the notation, let 
upper X equals upper Q Superscript normal upper T Baseline upper A overTilde upper Q periodX = QT -AQ.
Now suppose that there are r distinct eigenvalues of A (i.e., the diagonal 
elements in C): 
d 1 greater than midline horizontal ellipsis greater than d Subscript r Baseline periodd1 > · · · > dr.
We can write C as diag(diImi ), where mi is the multiplicity of di. We  
now partition QT -AQ to correspond to the partitioning of C represented by 
diag(diImi ): 
upper X equals Start 3 By 3 Matrix 1st Row 1st Column upper X 11 2nd Column midline horizontal ellipsis 3rd Column upper X Subscript 1 r Baseline 2nd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 3rd Row 1st Column upper X Subscript r Baseline 1 Baseline 2nd Column midline horizontal ellipsis 3rd Column upper X Subscript r r Baseline EndMatrix periodX =
⎡
⎢⎣
X11 · · · X1r
...
...
...
Xr1 · · · Xrr
⎤
⎥⎦.
(8.9) 
In this partitioning, the diagonal blocks, Xii, are  mi×mi symmetric matrices. 
The submatrix Xij is an mi × mj matrix. 
We now proceed in two steps to show that in order for f(Q) to attain its 
lower bound l, X must be diagonal. First, we will show that when f(Q) =  l, 
the submatrix Xij in Eq. (8.9) must be null if  i /= j. To this end, let Q∇ be 
such that f(Q∇) =  l, and assume the contrary regarding the corresponding 
X∇ = QT 
∇-AQ∇; that is, assume that in some submatrix Xij∇ where i /= j, there  
is  a nonzero element, say  x∇. We arrive at a contradiction by showing that 
in this case there is another X0 of the form QT 
0 -AQ0, where  Q0 is orthogonal 
and such that f(Q0) < f(Q∇). 
To establish some useful notation, let p and q be the row and column, 
respectively, of X∇ where this nonzero element x∇ occurs; that is, xpq = x∇/= 
0 and  p /= q because xpq is in Xij∇. (Note the distinction between uppercase 
letters, which represent submatrices, and lowercase letters, which represent 
elements of matrices.) Also, because X∇ is symmetric, xqp = x∇. Now  let  
a∇ = xpp and b∇ = xqq. We form  Q0 as Q∇R, where  R is an orthogonal 
rotation matrix of the form Gpq in Eq. (4.12) on page 230. We have, therefore,
||QT 
0 -AQ0||2 = ||RT QT 
∇-AQ∇R||2 = ||QT 
∇-AQ∇||2 . Let  a0, b0, and  x0 represent 
the elements of QT 
0 -AQ0 that correspond to a∇, b∇, and  x∇ in QT 
∇-AQ∇. 
From the deﬁnition of the Frobenius norm, we have 
f left parenthesis upper Q 0 right parenthesis minus f left parenthesis upper Q Subscript Sub Subscript nabla Subscript Baseline right parenthesis equals 2 left parenthesis a Subscript Sub Subscript nabla Subscript Baseline minus a 0 right parenthesis d Subscript i Baseline plus 2 left parenthesis b Subscript Sub Subscript nabla Subscript Baseline minus b 0 right parenthesis d Subscript jf(Q0) −f(Q∇) = 2(a∇−a0)di + 2(b∇−b0)dj
because all other terms cancel. If the angle of rotation is θ, then  
StartLayout 1st Row 1st Column a 0 2nd Column equals 3rd Column a Subscript Sub Subscript nabla Subscript Baseline cosine squared theta minus 2 x Subscript Sub Subscript nabla Subscript Baseline cosine theta sine theta plus b Subscript Sub Subscript nabla Subscript Baseline sine squared theta comma 2nd Row 1st Column b 0 2nd Column equals 3rd Column a Subscript Sub Subscript nabla Subscript Baseline sine squared theta minus 2 x Subscript Sub Subscript nabla Subscript Baseline cosine theta sine theta plus b Subscript Sub Subscript nabla Subscript Baseline cosine squared theta comma EndLayouta0 = a∇cos2 θ −2x∇cos θ sin θ + b∇sin2 θ,
b0 = a∇sin2 θ −2x∇cos θ sin θ + b∇cos2 θ,
and so for a function h of θ we can write

382
8 Matrices with Special Properties
StartLayout 1st Row 1st Column h left parenthesis theta right parenthesis 2nd Column equals 3rd Column f left parenthesis upper Q 0 right parenthesis minus f left parenthesis upper Q Subscript Sub Subscript nabla Subscript Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column 2 d Subscript i Baseline left parenthesis left parenthesis a Subscript Sub Subscript nabla Subscript Baseline minus b Subscript Sub Subscript nabla Subscript Baseline right parenthesis sine squared theta plus x Subscript Sub Subscript nabla Subscript Baseline sine 2 theta right parenthesis plus 2 d Subscript j Baseline left parenthesis left parenthesis b Subscript Sub Subscript nabla Subscript Baseline minus b 0 right parenthesis sine squared theta minus x Subscript Sub Subscript nabla Subscript Baseline sine 2 theta right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column 2 d Subscript i Baseline left parenthesis left parenthesis a Subscript Sub Subscript nabla Subscript Baseline minus b Subscript Sub Subscript nabla Subscript Baseline right parenthesis plus 2 d Subscript j Baseline left parenthesis b Subscript Sub Subscript nabla Subscript Baseline minus b 0 right parenthesis right parenthesis sine squared theta plus 2 x Subscript Sub Subscript nabla Subscript Baseline left parenthesis d Subscript i Baseline minus d Subscript j Baseline right parenthesis sine 2 theta comma EndLayouth(θ) = f(Q0) −f(Q∇)
= 2di((a∇−b∇) sin2 θ + x∇sin 2θ) + 2dj((b∇−b0) sin2 θ −x∇sin 2θ)
= 2di((a∇−b∇) + 2dj(b∇−b0)) sin2 θ + 2x∇(di −dj) sin 2θ,
and so 
StartFraction normal d Over normal d theta EndFraction h left parenthesis theta right parenthesis equals 2 d Subscript i Baseline left parenthesis left parenthesis a Subscript Sub Subscript nabla Subscript Baseline minus b Subscript Sub Subscript nabla Subscript Baseline right parenthesis plus 2 d Subscript j Baseline left parenthesis b Subscript Sub Subscript nabla Subscript Baseline minus b 0 right parenthesis right parenthesis sine 2 theta plus 4 x Subscript Sub Subscript nabla Subscript Baseline left parenthesis d Subscript i Baseline minus d Subscript j Baseline right parenthesis cosine 2 theta period d
dθh(θ) = 2di((a∇−b∇) + 2dj(b∇−b0)) sin 2θ + 4x∇(di −dj) cos 2θ.
The coeﬃcient of cos 2θ, 4x∇(di−dj), is nonzero because di and dj are distinct, 
and x∇ is nonzero by the second assumption to be contradicted, and so the 
derivative at θ = 0 is nonzero. Hence, by the proper choice of a direction of 
rotation (which eﬀectively interchanges the roles of di and dj), we can make 
f(Q0)−f(Q∇) positive or negative, showing that f(Q∇) cannot be a minimum  
if some Xij in Eq. (8.9) with  i /= j is nonnull; that is, if Q∇ is a matrix such 
that f(Q∇) is the minimum of f(Q), then in the partition of QT 
∇-AQ∇ only the 
diagonal submatrices Xii∇ can be nonnull: 
upper Q Subscript Sub Subscript nabla Subscript Superscript normal upper T Baseline upper A overTilde upper Q Subscript Sub Subscript nabla Subscript Baseline equals normal d normal i normal a normal g left parenthesis upper X Subscript 11 Sub Subscript Sub Sub Subscript nabla Sub Subscript Subscript Baseline comma ellipsis comma upper X Subscript r r Sub Subscript Sub Sub Subscript nabla Sub Subscript Subscript Baseline right parenthesis periodQT
∇-AQ∇= diag(X11∇, . . . , Xrr∇).
The next step is to show that each Xii∇ must be diagonal. Because it is 
symmetric, we can diagonalize it with an orthogonal matrix Pi as 
upper P Subscript i Superscript normal upper T Baseline upper X Subscript i i Sub Subscript Sub Sub Subscript nabla Sub Subscript Subscript Baseline upper P Subscript i Baseline equals upper G Subscript i Baseline periodP T
i Xii∇Pi = Gi.
Now let P be the direct sum of the Pi and form 
StartLayout 1st Row 1st Column upper P Superscript normal upper T Baseline upper C upper P minus upper P Superscript normal upper T Baseline upper Q Subscript Sub Subscript nabla Superscript normal upper T Baseline upper A overTilde upper Q Subscript Sub Subscript nabla Baseline upper P 2nd Column equals 3rd Column normal d normal i normal a normal g left parenthesis d 1 upper I comma ellipsis comma d Subscript r Baseline upper I right parenthesis minus normal d normal i normal a normal g left parenthesis upper G 1 comma ellipsis comma upper G Subscript r Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper C minus upper P Superscript normal upper T Baseline upper Q Subscript Sub Subscript nabla Subscript Superscript normal upper T Baseline upper A overTilde upper Q Subscript Sub Subscript nabla Subscript Baseline upper P period EndLayoutP TCP −P TQT
∇-AQ∇P = diag(d1I, . . . , drI) −diag(G1, . . . , Gr)
= C −P TQT
∇-AQ∇P.
Hence, 
f left parenthesis upper Q Subscript Sub Subscript nabla Subscript Baseline upper P right parenthesis equals f left parenthesis upper Q Subscript Sub Subscript nabla Subscript Baseline right parenthesis commaf(Q∇P) = f(Q∇),
and so the minimum occurs for a matrix Q∇P that reduces -A to a diagonal 
form. The elements of the Gi must be the ˜ci in some order, so the minimum 
of f(Q), which we have denoted by f(Q∇), is E(ci − ˜cpi )2 , where  the  pi are 
a permutation of 1, . . . , n. As the ﬁnal step, we show pi = i. We begin with 
p1. Suppose p1 /= 1 but ps = 1;  that  is,  ˜c1 ≥ ˜cp1 . Interchange p1 and ps in the 
permutation. The change in the sum E(ci − ˜cpi )2 is 
StartLayout 1st Row 1st Column left parenthesis c 1 minus c overTilde Subscript 1 Baseline right parenthesis squared plus left parenthesis c Subscript s Baseline minus c overTilde Subscript p Sub Subscript s Subscript Baseline right parenthesis squared minus left parenthesis c 1 minus c overTilde Subscript p Sub Subscript s Subscript Baseline right parenthesis squared minus left parenthesis c Subscript s Baseline minus c overTilde Subscript 1 Baseline right parenthesis squared 2nd Column equals 3rd Column minus 2 left parenthesis c Subscript s Baseline minus c 1 right parenthesis left parenthesis c overTilde Subscript p 1 Baseline minus c overTilde Subscript 1 Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column less than or equals 3rd Column 0 semicolon EndLayout(c1 −˜c1)2 + (cs −˜cps)2 −(c1 −˜cps)2 −(cs −˜c1)2 = −2(cs −c1)(˜cp1 −˜c1)
≤0;
that is, the interchange reduces the value of the sum. Similarly, we proceed 
through the pi to pn, getting pi = i. 
We have shown, therefore, that the minimum of f(Q) is En 
i=1(ci − ˜ci)2 , 
where both sets of eigenvalues are ordered in nonincreasing value. From 
Eq. (8.7), which is f(V ), we have the inequality (8.6).

8.2 Symmetric Matrices and Other Unitarily Diagonalizable Matrices
383
While an upper bound may be of more interest in the approximation prob-
lem, the lower bound in the Hoﬀman-Wielandt theorem gives us a measure of 
the goodness of the approximation of one matrix by another matrix. There are 
various extensions and other applications of the Hoﬀman-Wielandt theorem. 
8.2.3 Normal Matrices 
A real square matrix A is said to be normal if AT A = AAT . (In  general,  
a square matrix is normal if AH A = AAH .) The Gramian matrix formed 
from a normal matrix is the same as the Gramian formed from the transpose 
(or conjugate transpose) of the matrix. Normal matrices include symmetric 
(and Hermitian), skew symmetric (and Hermitian), square orthogonal (and 
unitary) matrices, and circulant matrices. The identity is also obviously a 
normal matrix. 
There are a number of interesting properties possessed by a normal ma-
trix, but the most important property is that it can be diagonalized by a 
unitary matrix. Recall from page 177 that a matrix can be orthogonally diag-
onalized if and only if the matrix is symmetric. Not all normal matrices can 
be orthogonally diagonalized, but all can be diagonalized by a unitary matrix 
(“unitarily diagonalized”). In fact, a matrix can be unitarily diagonalized if 
and only if the matrix is normal. (This is the reason the word “normal” is 
used to describe these matrices; and a matrix that is unitarily diagonalizable 
is an alternate and more meaningful way of deﬁning a normal matrix.) 
It is easy to see that a matrix A is unitarily diagonalizable if and only if 
AH A = AAH (and for real A, AT A = AAT implies AH A = AAH ). 
First, suppose A is unitarily diagonalizable. Let A = P D  P H , where  P is 
unitary and D is diagonal. In that case, the elements of D are the eigenvalues 
of A, as we see by considering each column in AP. Now consider AAH : 
StartLayout 1st Row upper A upper A Superscript normal upper H Baseline equals upper P upper D upper P Superscript normal upper H Baseline upper P upper D Superscript normal upper H Baseline upper P Superscript normal upper H Baseline equals upper P upper D upper D Superscript normal upper H Baseline upper P Superscript normal upper H Baseline equals 2nd Row upper P upper D Superscript normal upper H Baseline upper D upper P Superscript normal upper H Baseline equals upper P upper D Superscript normal upper H Baseline upper P Superscript normal upper H Baseline upper P upper D upper P Superscript normal upper H Baseline equals upper A Superscript normal upper H Baseline upper A period EndLayout
AAH = P D P HP DH P H = P D DH P H =
P DHD P H = P DH P HP D P H = AHA.
Next, suppose AH A = AAH . To see that A is unitarily diagonalizable, form 
the Schur factorization, A = UTU H (see Sect. 3.9.6 on page 170). We have 
upper A Superscript normal upper H Baseline upper A equals upper U upper T Superscript normal upper H Baseline upper U Superscript normal upper H Baseline upper U upper T upper U Superscript normal upper H Baseline equals upper U upper T Superscript normal upper H Baseline upper T upper U Superscript normal upper HAHA = UT HU HUTU H = UT HTU H
and 
upper A upper A Superscript normal upper H Baseline equals upper U upper T upper U Superscript normal upper H Baseline upper U upper T Superscript normal upper H Baseline upper U Superscript normal upper H Baseline equals upper U upper T upper T Superscript normal upper H Baseline upper U Superscript normal upper H Baseline periodAAH = UTU HUT HU H = UTT HU H.
Now under the assumption that AH A = AAH , 
upper U upper T Superscript normal upper H Baseline upper T upper U Superscript normal upper H Baseline equals upper U upper T upper T Superscript normal upper H Baseline upper U Superscript normal upper H Baseline commaUT HTU H = UTT HU H,
which implies T H T = TT H , in which  case  
StartAbsoluteValue t Subscript i i Baseline EndAbsoluteValue squared equals sigma summation Underscript j equals 1 Overscript n Endscripts StartAbsoluteValue t Subscript i j Baseline EndAbsoluteValue squared comma|tii|2 =
n
E
j=1
|tij|2,

384
8 Matrices with Special Properties
that is, tij = 0 unless j = i. We conclude that T is diagonal, and hence, 
the Schur factorization A = UTU H is a unitary diagonalization of A, so  A is 
unitarily diagonalizable. 
Spectral methods, based on the unitary diagonalization, are useful in many 
areas of applied mathematics. The spectra of nonnormal matrices, however, 
are quite diﬀerent. 
8.3 Nonnegative Deﬁnite Matrices: Cholesky 
Factorization 
We deﬁned nonnegative deﬁnite and positive deﬁnite matrices on page 113 
and discussed some of their properties, particularly in Sect. 3.10. We have  
seen that these matrices have useful factorizations, in particular, the square 
root and the Cholesky factorization. In this section, we recall those deﬁnitions, 
properties, and factorizations. 
A symmetric matrix A such that any quadratic form involving the matrix 
is nonnegative is called a nonnegative deﬁnite matrix. That is, a symmetric 
matrix A is a nonnegative deﬁnite matrix if, for any (conformable) vector x, 
x Superscript normal upper T Baseline upper A x greater than or equals 0 periodxTAx ≥0.
(8.10) 
(We remind the reader that there is a related term, positive semideﬁnite ma-
trix, that is not used consistently in the literature. We will generally avoid the 
term “semideﬁnite.”) 
We denote the fact that A is nonnegative deﬁnite by 
upper A succeeds above single line equals sign 0 periodA > 0.
(8.11) 
(Some people use the notation A ≥ 0 to denote a nonnegative deﬁnite matrix, 
but we have decided to use this notation to indicate that each element of A 
is nonnegative; see page 83.) 
There are several properties that follow immediately from the deﬁnition. 
• The sum of two (conformable) nonnegative matrices is nonnegative deﬁ-
nite. 
• All diagonal elements of a nonnegative deﬁnite matrix are nonnegative. 
Hence, if A is nonnegative deﬁnite, tr(A) ≥ 0. 
• Any square submatrix whose principal diagonal is a subset of the principal 
diagonal of a nonnegative deﬁnite matrix is nonnegative deﬁnite. In par-
ticular, any square principal submatrix of a nonnegative deﬁnite matrix is 
nonnegative deﬁnite. 
It is easy to show that the latter two facts follow from the deﬁnition by 
considering a vector x with zeros in all positions except those corresponding 
to the submatrix in question. For example, to see that all diagonal elements

8.3 Nonnegative Deﬁnite Matrices: Cholesky Factorization
385
of a nonnegative deﬁnite matrix are nonnegative, assume the (i, i) element is 
negative, and then consider the vector x to consist of all zeros except for a 1 
in the ith position. It is easy to see that the quadratic form is negative, so the 
assumption that the (i, i) element is negative leads to a contradiction. 
• A diagonal matrix is nonnegative deﬁnite if and only if all of the diagonal 
elements are nonnegative. 
This must be true because a quadratic form in a diagonal matrix is the sum 
of the diagonal elements times the squares of the elements of the vector. 
We can also form other submatrices that are nonnegative deﬁnite: 
• If A is nonnegative deﬁnite, then A−(i1,...,ik)(i1,...,ik) is nonnegative deﬁnite. 
(See page 663 for notation.) 
Again, we can see this by selecting an x in the deﬁning inequality (8.10) 
consisting of 1s in the positions corresponding to the rows and columns of A 
that are retained and 0s elsewhere. 
By considering xT CT ACx and y = Cx, we see that 
• If A is nonnegative deﬁnite and C is conformable for the multiplication, 
then CT AC is nonnegative deﬁnite. 
From Eq. (3.276) and the fact that the determinant of a product is the 
product of the determinants, we have that 
• The determinant of a nonnegative deﬁnite matrix is nonnegative. 
Finally, for the nonnegative deﬁnite matrix A, we have  
a Subscript i j Superscript 2 Baseline less than or equals a Subscript i i Baseline a Subscript j j Baseline commaa2
ij ≤aiiajj,
(8.12) 
as we see from the deﬁnition xT Ax ≥ 0 and choosing the vector x to have a 
variable y in position i, a 1 in position j, and 0s in all other positions. For 
a symmetric matrix A, this yields the quadratic aiiy2 + 2aijy + ajj. If this  
quadratic is to be nonnegative for all y, then the discriminant 4a2 
ij − 4aiiajj 
must be nonpositive; that is, inequality (8.12) must be true.  
8.3.1 Eigenvalues of Nonnegative Deﬁnite Matrices 
We have seen on page 186 that a real symmetric matrix is nonnegative (pos-
itive) deﬁnite if and only if all of its eigenvalues are nonnegative (positive). 
This fact allows a generalization of the statement above: a triangular ma-
trix is nonnegative (positive) deﬁnite if and only if all of the diagonal elements 
are nonnegative (positive).

386
8 Matrices with Special Properties
8.3.2 The Square Root and the Cholesky Factorization 
Two important factorizations of nonnegative deﬁnite matrices are the square 
root, 
upper A equals left parenthesis upper A Superscript one half Baseline right parenthesis squared commaA = (A
1
2 )2,
(8.13) 
discussed in Sect. 4.7.1, and the Cholesky factorization, 
upper A equals upper T Superscript normal upper T Baseline upper T commaA = T TT,
(8.14) 
discussed in Sect. 4.7.2. If  T is as in Eq. (8.14), the symmetric matrix T + T T 
is also nonnegative deﬁnite or positive deﬁnite if A is. 
The square root matrix is used often in theoretical developments, such 
as Exercise 9.12b, but the Cholesky factor is more useful in practice. The 
Cholesky factorization also has a prominent role in multivariate analysis, 
where it appears in the Bartlett decomposition. If  W is a Wishart matrix 
with variance-covariance matrix Σ (see Exercise 7.9 on page 361), then the 
Bartlett decomposition of W is 
upper W equals left parenthesis upper T upper U right parenthesis Superscript normal upper T Baseline upper T upper U commaW = (TU)TTU,
where U is the Cholesky factor of Σ and T is an upper triangular matrix with 
positive diagonal elements. The diagonal elements of T have independent chi-
squared distributions and the oﬀ-diagonal elements of T have independent 
standard normal distributions. 
8.3.3 The Convex Cone of Nonnegative Deﬁnite Matrices 
The set of all n × n nonnegative deﬁnite matrices is a cone because if X is 
a nonnegative deﬁnite matrix and a >  0, then aX is a nonnegative deﬁnite 
matrix (see page 53). Furthermore, it is a convex cone in IRn×n , because if X1 
and X2 are n × n nonnegative deﬁnite matrices and a, b ≥ 0, then aX1 + bX2 
is nonnegative deﬁnite so long as either a >  0 or  b >  0. 
This set is not closed under Cayley multiplication (i.e., in particular, it 
does not form a group with respect to that operation). The product of two 
nonnegative deﬁnite matrices might not even be symmetric. 
The convex cone of nonnegative deﬁnite matrices is an important object in 
a common optimization problem called convex cone programming (“program-
ming” here means “optimization”). A special case of convex cone program-
ming is called “semideﬁnite programming,” or “SDP” (where “semideﬁnite” 
comes from the alternative terminology for nonnegative deﬁnite). The canon-
ical SDP problem for given n × n symmetric matrices C and Ai, and  real  
numbers bi, for  i = 1, . . . , m, is  
StartLayout 1st Row 1st Column normal m normal i normal n normal i normal m normal i normal z normal e 2nd Column left angle bracket upper C comma upper X right angle bracket 2nd Row 1st Column normal s normal u normal b normal j normal e normal c normal t normal t normal o 2nd Column left angle bracket upper A Subscript i Baseline comma upper X right angle bracket equals b Subscript i Baseline 3rd Column i equals 1 comma ellipsis comma m 3rd Row 1st Column Blank 2nd Column upper X succeeds above single line equals sign 0 period EndLayout
minimize <C, X>
subject to <Ai, X> = bi
i = 1, . . . , m
X > 0.

8.4 Positive Deﬁnite Matrices
387
The notation <C, X> here means the matrix inner product (see page 119). 
SDP includes linear programming as a simple special case, in which C and X 
are vectors. 
8.4 Positive Deﬁnite Matrices 
An important class of nonnegative deﬁnite matrices are those that satisfy 
strict inequalities in the deﬁnition involving xT Ax. These matrices are called 
positive deﬁnite matrices and they have all of the properties discussed above 
for nonnegative deﬁnite matrices as well as some additional useful properties. 
A symmetric matrix A is called a positive deﬁnite matrix if, for any (con-
formable) vector x /= 0, the quadratic form is positive; that is, 
x Superscript normal upper T Baseline upper A x greater than 0 periodxTAx > 0.
(8.15) 
We denote the fact that A is positive deﬁnite by 
upper A succeeds 0 periodA > 0.
(8.16) 
(Some people use the notation A >  0 to denote a positive deﬁnite matrix, 
but we have decided to use this notation to indicate that each element of A 
is positive.) 
The properties of nonnegative deﬁnite matrices noted above hold also for 
positive deﬁnite matrices, generally with strict inequalities. It is obvious that 
all diagonal elements of a positive deﬁnite matrix are positive. Hence, if A is 
positive deﬁnite, tr(A) > 0. Furthermore, as above and for the same reasons, if 
A is positive deﬁnite, then A−(i1,...,ik)(i1,...,ik) is positive deﬁnite. In particular, 
• Any square principal submatrix of a positive deﬁnite matrix is positive 
deﬁnite. 
Because a quadratic form in a diagonal matrix is the sum of the diagonal 
elements times the squares of the elements of the vector, a diagonal matrix is 
positive deﬁnite if and only if all of the diagonal elements are positive. 
From Eq. (3.276) on page 177 and the fact that the determinant of a prod-
uct is the product of the determinants, we have: 
• The determinant of a positive deﬁnite matrix is positive. 
• If A is positive deﬁnite, 
a Subscript i j Superscript 2 Baseline less than a Subscript i i Baseline a Subscript j j Baseline commaa2
ij < aiiajj,
(8.17) 
which we see using the same argument as for inequality (8.12). 
We have a slightly stronger statement regarding sums involving positive 
deﬁnite matrices than what we could conclude about nonnegative deﬁnite 
matrices:

388
8 Matrices with Special Properties
• The sum of a positive deﬁnite matrix and a (conformable) nonnegative 
deﬁnite matrix is positive deﬁnite. 
That is, 
x Superscript normal upper T Baseline upper A x greater than 0 for all x not equals 0 normal a normal n normal d y Superscript normal upper T Baseline upper B y greater than or equals 0 for all y long right double arrow z Superscript normal upper T Baseline left parenthesis upper A plus upper B right parenthesis z greater than 0 for all z not equals 0 periodxTAx > 0 ∀x /= 0
and
yTBy ≥0 ∀y =⇒zT(A + B)z > 0 ∀z /= 0. (8.18) 
• A positive deﬁnite matrix is necessarily nonsingular. (We see this from 
the fact that no nonzero combination of the columns, or rows, can be 0.) 
Furthermore, if A is positive deﬁnite, then A−1 is positive deﬁnite. (We 
showed this in Sect. 3.10, but we can see it in another way: because for 
any y /= 0  and  x = A−1 y, we have  yT A−1 y = xT y = xT Ax > 0.) 
• A (strictly) diagonally dominant symmetric matrix with positive diagonals 
is positive deﬁnite. The proof of this is Exercise 8.4. 
• A positive deﬁnite matrix is orthogonally diagonalizable. 
• A positive deﬁnite matrix has a square root. 
• A positive deﬁnite matrix has a Cholesky factorization. 
We cannot conclude that the product of two positive deﬁnite matrices is 
positive deﬁnite, but we do have the useful fact: 
• If A is positive deﬁnite and C is of full rank and conformable for the 
multiplication AC, then  CT AC is positive deﬁnite (see page 133). 
We have seen from the deﬁnition of positive deﬁniteness and the distribu-
tion of multiplication over addition that the sum of a positive deﬁnite matrix 
and a nonnegative deﬁnite matrix is positive deﬁnite. We can deﬁne an or-
dinal relationship between positive deﬁnite and nonnegative deﬁnite matrices 
of the same size. If A is positive deﬁnite and B is nonnegative deﬁnite of the 
same size, we say A is strictly greater than B and write 
upper A succeeds upper BA > B
(8.19) 
if A − B is positive deﬁnite; that is, if A − B > 0. 
We can form a partial ordering of nonnegative deﬁnite matrices of the same 
order based on this additive property. We say A is greater than B and write 
upper A succeeds above single line equals sign upper BA > B
(8.20) 
if A−B is either the zero matrix or is nonnegative deﬁnite; that is, if A−B > 0 
(see Exercise 8.2a). The “strictly greater than” relation implies the “greater 
than” relation. These relations are partial in the sense that they do not apply 
to all pairs of nonnegative matrices; that is, there are pairs of matrices A and 
B for which neither A > B nor B > A. 
If A > B, we also write  B ≺A; and  if  A > B, we may  write  B < A.

8.4 Positive Deﬁnite Matrices
389
8.4.1 Leading Principal Submatrices of Positive Deﬁnite Matrices 
A suﬃcient condition for a symmetric matrix to be positive deﬁnite is that 
the determinant of each of the leading principal submatrices be positive. To 
see this, ﬁrst let the n × n symmetric matrix A be partitioned as 
StartLayout 1st Row 1st Column upper A 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column upper A Subscript n minus 1 Baseline 2nd Column a 2nd Row 1st Column a Superscript normal upper T Baseline 2nd Column a Subscript n n Baseline EndMatrix comma EndLayoutA =
⎡An−1
a
aT
ann
⎤
,
and assume that An−1 is positive deﬁnite and that |A| > 0.  (This is not  the  
same notation that we have used for these submatrices, but the notation is 
convenient in this context.) From Eq. (3.206) on page 144, 
StartAbsoluteValue upper A EndAbsoluteValue equals StartAbsoluteValue upper A Subscript n minus 1 Baseline EndAbsoluteValue left parenthesis a Subscript n n Baseline minus a Superscript normal upper T Baseline upper A Subscript n minus 1 Superscript negative 1 Baseline a right parenthesis period|A| = |An−1|(ann −aTA−1
n−1a).
Because An−1 is positive deﬁnite, |An−1| > 0, and so (ann − aT A−1 
n−1a) > 0; 
hence, the 1×1 matrix (ann −aT A−1 
n−1a) is positive deﬁnite. That any matrix 
whose leading principal submatrices have positive determinants is positive 
deﬁnite follows from this by induction, beginning with a 2 × 2 matrix.  
8.4.2 The Convex Cone of Positive Deﬁnite Matrices 
The class of all n × n positive deﬁnite matrices is a cone because if X is a 
positive deﬁnite matrix and a >  0, then aX is a positive deﬁnite matrix (see 
page 53). Furthermore, this class is a convex cone in IRn×n because if X1 and 
X2 are n×n positive deﬁnite matrices and a, b ≥ 0, then aX1+bX2 is positive 
deﬁnite so long as either a /= 0  or  b /= 0.  
As with the cone of nonnegative deﬁnite matrices, this class is not closed 
under Cayley multiplication. 
8.4.3 Inequalities Involving Positive Deﬁnite Matrices 
Quadratic forms of positive deﬁnite matrices and nonnegative matrices occur 
often in data analysis. There are several useful inequalities involving such 
quadratic forms. 
On page 179, we showed that if x /= 0, for any symmetric matrix A with 
eigenvalues ci, 
StartFraction x Superscript normal upper T Baseline upper A x Over x Superscript normal upper T Baseline x EndFraction less than or equals max left brace c Subscript i Baseline right brace periodxTAx
xTx ≤max{ci}.
(8.21) 
If A is nonnegative deﬁnite, by our convention of labeling the eigenvalues, we 
have max{ci} = c1. If the  rank  of  A is r, the minimum nonzero eigenvalue 
is denoted cr. Letting the eigenvectors associated with c1, . . . , cr be v1, . . . , vr 
(and recalling that these choices may be arbitrary in the case where some 
eigenvalues are not simple), by an argument similar to that used on page 179, 
we have that if A is nonnegative deﬁnite of rank r,

390
8 Matrices with Special Properties
StartFraction v Subscript i Superscript normal upper T Baseline upper A v Subscript i Baseline Over v Subscript i Superscript normal upper T Baseline v Subscript i Baseline EndFraction greater than or equals c Subscript r Baseline commavT
i Avi
vT
i vi
≥cr,
(8.22) 
for 1 ≤ i ≤ r. 
If A is positive deﬁnite and x and y are conformable nonzero vectors, we 
see that 
x Superscript normal upper T Baseline upper A Superscript negative 1 Baseline x greater than or equals StartFraction left parenthesis y Superscript normal upper T Baseline x right parenthesis squared Over y Superscript normal upper T Baseline upper A y EndFractionxTA−1x ≥(yTx)2
yTAy
(8.23) 
by using the same argument as used in establishing the Cauchy-Schwarz in-
equality (2.23). We ﬁrst obtain the Cholesky factor T of A (which  is, of course,  
of full rank) and then observe that for every real number t 
left parenthesis t upper T y plus upper T Superscript negative normal upper T Baseline x right parenthesis Superscript normal upper T Baseline left parenthesis t upper T y plus upper T Superscript negative normal upper T Baseline x right parenthesis greater than or equals 0 comma
(
tTy + T −Tx
)T (
tTy + T −Tx
)
≥0,
and hence the discriminant of the quadratic equation in t must be nonnegative: 
4 left parenthesis left parenthesis upper T y right parenthesis Superscript normal upper T Baseline upper T Superscript negative normal upper T Baseline x right parenthesis squared minus 4 left parenthesis upper T Superscript negative normal upper T Baseline x right parenthesis Superscript normal upper T Baseline left parenthesis upper T Superscript negative normal upper T Baseline minus x right parenthesis left parenthesis upper T y right parenthesis Superscript normal upper T Baseline upper T y less than or equals 0 period4
(
(Ty)TT −Tx
)2 −4
(
T −Tx
)T (
T −T −x
)
(Ty)TTy ≤0.
The inequality (8.23) is used in constructing Scheﬀ´e simultaneous conﬁdence 
intervals in linear models. 
The Kantorovich inequality states, for positive numbers c1 ≥c2 ≥· · · ≥ cn 
and nonnegative numbers y1, . . . , yn such that E yi = 1,  that  
left parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts y Subscript i Baseline c Subscript i Baseline right parenthesis left parenthesis sigma summation Underscript i equals 1 Overscript n Endscripts y Subscript i Baseline c Subscript i Superscript negative 1 Baseline right parenthesis less than or equals StartFraction left parenthesis c 1 plus c 2 right parenthesis squared Over 4 c 1 c 2 EndFraction period
( n
E
i=1
yici
) ( n
E
i=1
yic−1
i
)
≤(c1 + c2)2
4c1c2
.
The Kantorovich inequality for positive numbers has an immediate exten-
sion to an inequality that involves positive deﬁnite matrices. Let A be an n×n 
positive deﬁnite matrix with eigenvalues c1 ≥ c2 ≥ · · ·  ≥ cn > 0. We substi-
tute x2 for y, thus removing the nonnegativity restriction, and incorporate 
the restriction on the sum directly into the inequality. Then, using the similar 
canonical factorization of A and A−1 , we have  
StartFraction left parenthesis x Superscript normal upper T Baseline upper A x right parenthesis left parenthesis x Superscript normal upper T Baseline upper A Superscript negative 1 Baseline x right parenthesis Over left parenthesis x Superscript normal upper T Baseline x right parenthesis squared EndFraction less than or equals StartFraction left parenthesis c 1 plus c Subscript n Baseline right parenthesis squared Over 4 c 1 c Subscript n Baseline EndFraction period
(
xTAx
) (
xTA−1x
)
(xTx)2
≤(c1 + cn)2
4c1cn
.
(8.24) 
This Kantorovich matrix inequality likewise has applications in optimization, 
in particular, for assessing the convergence of iterative algorithms. 
The left-hand side of the Kantorovich matrix inequality also has a lower 
bound, 
StartFraction left parenthesis x Superscript normal upper T Baseline upper A x right parenthesis left parenthesis x Superscript normal upper T Baseline upper A Superscript negative 1 Baseline x right parenthesis Over left parenthesis x Superscript normal upper T Baseline x right parenthesis squared EndFraction greater than or equals 1 comma
(
xTAx
) (
xTA−1x
)
(xTx)2
≥1,
(8.25) 
which can be seen in a variety of ways, perhaps most easily by using the 
inequality (8.23). (You were asked to prove this directly in Exercise 3.31.) 
All of the inequalities (8.21) through (8.25) are sharp. We know that (8.21) 
and (8.22) are sharp by using the appropriate eigenvectors. We can see the 
others are sharp by using A = I.

8.5 Idempotent and Projection Matrices
391
8.5 Idempotent and Projection Matrices 
An important class of matrices are those that, like the identity, have the 
property that raising them to a power leaves them unchanged. A matrix A 
such that 
upper A upper A equals upper AAA = A
(8.26) 
is called an idempotent matrix. An idempotent matrix is square, and it is either 
singular or the identity matrix. (It must be square in order to be conformable 
for the indicated multiplication. If it is not singular, we have A = (A−1 A)A = 
A−1 (AA) =  A−1 A = I; hence, an idempotent matrix is either singular or the 
identity matrix.) 
An idempotent matrix that is symmetric is called a projection matrix. 
8.5.1 Idempotent Matrices 
Many matrices encountered in the statistical analysis of linear models are 
idempotent. One such matrix is X−X (see page 149 and Sects. 9.1 and 9.2). 
This matrix exists for any n × m matrix X, and it is square. (It is m × m.) 
Because the eigenvalues of A2 are the squares of the eigenvalues of A, all  
eigenvalues of an idempotent matrix must be either 0 or 1. 
Any vector in the column space of an idempotent matrix A is an eigenvec-
tor of A. (This follows immediately from AA = A.) More generally, if x and 
y are vectors in span(A) and  a is a scalar, then 
upper A left parenthesis a x plus y right parenthesis equals a x plus y periodA(ax + y) = ax + y.
(8.27) 
(To see this, we merely represent x and y as linear combinations of columns 
(or rows) of A and substitute in the equation.) 
The number of eigenvalues that are 1 is the rank of an idempotent matrix. 
(Exercise 8.5 asks why this is the case.) We therefore have, for an idempotent 
matrix A, 
normal t normal r left parenthesis upper A right parenthesis equals normal r normal a normal n normal k left parenthesis upper A right parenthesis periodtr(A) = rank(A).
(8.28) 
Because the eigenvalues of an idempotent matrix are either 0 or 1, a symmetric 
idempotent matrix is nonnegative deﬁnite. 
If A is idempotent and n × n, then  
normal r normal a normal n normal k left parenthesis upper I minus upper A right parenthesis equals n minus normal r normal a normal n normal k left parenthesis upper A right parenthesis periodrank(I −A) = n −rank(A).
(8.29) 
We showed this in Eq. (3.229) on page 150. (Although there we were consid-
ering the special matrix A−A, the only properties used were the idempotency 
of A−A and the fact that rank(A−A) = rank(A).) 
Equation (8.29) together with the diagonalizability theorem (Eq. (3.272)) 
implies that an idempotent matrix is diagonalizable. 
If A is idempotent and V is an orthogonal matrix of the same size, then 
V T AV is idempotent (whether or not V is a matrix that diagonalizes A) 
because

392
8 Matrices with Special Properties
left parenthesis upper V Superscript normal upper T Baseline upper A upper V right parenthesis left parenthesis upper V Superscript normal upper T Baseline upper A upper V right parenthesis equals upper V Superscript normal upper T Baseline upper A upper A upper V equals upper V Superscript normal upper T Baseline upper A upper V period(V TAV )(V TAV ) = V TAAV = V TAV.
(8.30) 
If A is idempotent, then (I −A) is also idempotent, as we see by multipli-
cation. This fact and Eq. (8.29) have generalizations for sums of idempotent 
matrices that are parts of Cochran’s theorem, which we consider below. 
Although if A is idempotent so (I − A) is also idempotent and hence is 
not of full rank (unless A = 0), for any scalar a /= −1, (I + aA) is of full rank, 
and 
left parenthesis upper I plus a upper A right parenthesis Superscript negative 1 Baseline equals upper I minus StartFraction a Over a plus 1 EndFraction upper A comma(I + aA)−1 = I −
a
a + 1A,
(8.31) 
as we see by multiplication. 
On page 168, we saw that similar matrices are equivalent (have the same 
rank). For idempotent matrices, we have the converse: idempotent matrices 
of the same rank (and size) are similar (see Exercise 8.6). 
If A1 and A2 are matrices conformable for addition, then A1 +A2 is idem-
potent if and only if A1A2 = A2A1 = 0. It is easy to see that this condition 
is suﬃcient by multiplication: 
left parenthesis upper A 1 plus upper A 2 right parenthesis left parenthesis upper A 1 plus upper A 2 right parenthesis equals upper A 1 upper A 1 plus upper A 1 upper A 2 plus upper A 2 upper A 1 plus upper A 2 upper A 2 equals upper A 1 plus upper A 2 period(A1 + A2)(A1 + A2) = A1A1 + A1A2 + A2A1 + A2A2 = A1 + A2.
To see that it is necessary, we ﬁrst observe from the expansion above that 
A1 + A2 is idempotent only if A1A2 + A2A1 = 0. Multiplying this necessary 
condition on the left by A1 yields 
upper A 1 upper A 1 upper A 2 plus upper A 1 upper A 2 upper A 1 equals upper A 1 upper A 2 plus upper A 1 upper A 2 upper A 1 equals 0 commaA1A1A2 + A1A2A1 = A1A2 + A1A2A1 = 0,
and multiplying on the right by A1 yields 
upper A 1 upper A 2 upper A 1 plus upper A 2 upper A 1 upper A 1 equals upper A 1 upper A 2 upper A 1 plus upper A 2 upper A 1 equals 0 periodA1A2A1 + A2A1A1 = A1A2A1 + A2A1 = 0.
Subtracting these two equations yields 
upper A 1 upper A 2 equals upper A 2 upper A 1 commaA1A2 = A2A1,
and since A1A2 + A2A1 = 0,  we  must  have  A1A2 = A2A1 = 0.  
Symmetric Idempotent Matrices 
Many of the idempotent matrices in statistical applications are symmetric, 
and such matrices have some useful properties. 
Because the eigenvalues of an idempotent matrix are either 0 or 1, the 
spectral decomposition of a symmetric idempotent matrix A can be written 
as 
upper V Superscript normal upper T Baseline upper A upper V equals normal d normal i normal a normal g left parenthesis upper I Subscript r Baseline comma 0 right parenthesis commaV TAV = diag(Ir, 0),
(8.32) 
where V is a square orthogonal matrix and r = rank(A). (This is from 
Eq. (3.277) on page 177.)

8.5 Idempotent and Projection Matrices
393
For symmetric matrices, there is a converse to the fact that all eigenvalues 
of an idempotent matrix are either 0 or 1. If A is a symmetric matrix, all of 
whose eigenvalues are either 0 or 1, then A is idempotent. We see this from the 
spectral decomposition of A, A = V diag(Ir, 0)V T , and, with C = diag(Ir, 0), 
by observing 
upper A upper A equals upper V upper C upper V Superscript normal upper T Baseline upper V upper C upper V Superscript normal upper T Baseline equals upper V upper C upper C upper V Superscript normal upper T Baseline equals upper V upper C upper V Superscript normal upper T Baseline equals upper A commaAA = V CV TV CV T = V CCV T = V CV T = A,
because the diagonal matrix of eigenvalues C contains only 0s and 1s. 
If A is symmetric and p is any positive integer, 
upper A Superscript p plus 1 Baseline equals upper A Superscript p Baseline long right double arrow upper A is idempotent periodAp+1 = Ap =⇒A is idempotent.
(8.33) 
This follows by considering the eigenvalues of A, c1, . . . , cn. The eigenvalues 
of Ap+1 are cp+1 
1 
, . . . , cp+1 
n 
and the eigenvalues of Ap are cp 
1, . . . , cp 
n, but since 
Ap+1 = Ap , it must be the  case  that  cp+1 
i 
= cp 
i for each i = 1, . . . , n. The  only  
way this is possible is for each eigenvalue to be 0 or 1, and in this case the 
symmetric matrix must be idempotent. 
There are bounds on the elements of a symmetric idempotent matrix. 
Because A is symmetric and AT A = A, 
a Subscript i i Baseline equals sigma summation Underscript j equals 1 Overscript n Endscripts a Subscript i j Superscript 2 Baseline semicolonaii =
n
E
j=1
a2
ij;
(8.34) 
hence, 0 ≤ aii. Rearranging Eq. (8.34), we have 
a Subscript i i Baseline equals a Subscript i i Superscript 2 Baseline plus sigma summation Underscript j not equals i Endscripts a Subscript i j Superscript 2 Baseline commaaii = a2
ii +
E
j/=i
a2
ij,
(8.35) 
so a2 
ii ≤ aii or 0 ≤aii(1−aii); that is, aii ≤ 1. Now, if aii = 0  or  aii = 1,  then  
Eq. (8.35) implies 
sigma summation Underscript j not equals i Endscripts a Subscript i j Superscript 2 Baseline equals 0 comma
E
j/=i
a2
ij = 0,
and the only way this can happen is if aij = 0 for all j /= i. So, in summary, 
if A is an n × n symmetric idempotent matrix, then 
0 less than or equals a Subscript i i Baseline less than or equals 1 for i equals 1 comma ellipsis comma m comma0 ≤aii ≤1 for i = 1, . . . , m,
(8.36) 
and 
if a Subscript i i Baseline equals 0 or a Subscript i i Baseline equals 1 comma then a Subscript i j Baseline equals a Subscript j i Baseline equals 0 for all j not equals i periodif aii = 0 or aii = 1, then aij = aji = 0 for all j /= i.
(8.37) 
Cochran’s Theorem 
There are various facts that are sometimes called Cochran’s theorem. The  
simplest one concerns k symmetric idempotent n × n matrices, A1, . . . , Ak, 
such that

394
8 Matrices with Special Properties
upper I Subscript n Baseline equals upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline periodIn = A1 + · · · + Ak.
(8.38) 
Under these conditions, we have 
upper A Subscript i Baseline upper A Subscript j Baseline equals 0 for all i not equals j periodAiAj = 0 for all i /= j.
(8.39) 
We see this by the following argument. For an arbitrary j, as in Eq. (8.32), 
for some matrix V , we have  
upper V Superscript normal upper T Baseline upper A Subscript j Baseline upper V equals normal d normal i normal a normal g left parenthesis upper I Subscript r Baseline comma 0 right parenthesis commaV TAjV = diag(Ir, 0),
where r = rank(Aj). Now 
StartLayout 1st Row 1st Column upper I Subscript n 2nd Column equals 3rd Column upper V Superscript normal upper T Baseline upper I Subscript n Baseline upper V 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript k Endscripts upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper V 3rd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal i normal a normal g left parenthesis upper I Subscript r Baseline comma 0 right parenthesis plus sigma summation Underscript i not equals j Endscripts upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper V comma EndLayoutIn = V TInV
=
k
E
i=1
V TAiV
= diag(Ir, 0) +
E
i/=j
V TAiV,
which implies 
sigma summation Underscript i not equals j Endscripts upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper V equals normal d normal i normal a normal g left parenthesis 0 comma upper I Subscript n minus r Baseline right parenthesis period
E
i/=j
V TAiV = diag(0, In−r).
(8.40) 
Now, from Eq. (8.30), for each i, V T AiV is idempotent, and so from Eq. (8.36) 
the diagonal elements are all nonnegative, and hence, Eq. (8.40) implies that 
for each i /= j, the ﬁrst r diagonal elements are 0. Furthermore, since these 
diagonal elements are 0, Eq. (8.37) implies that all elements in the ﬁrst r rows 
and columns are 0. We have, therefore, for each i /= j, 
upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper V equals normal d normal i normal a normal g left parenthesis 0 comma upper B Subscript i Baseline right parenthesisV TAiV = diag(0, Bi)
for some (n − r) × (n − r) symmetric idempotent matrix Bi. Now, for any 
i /= j, consider  AiAj and form V T AiAjV . We have  
StartLayout 1st Row 1st Column upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper A Subscript j Baseline upper V 2nd Column equals 3rd Column left parenthesis upper V Superscript normal upper T Baseline upper A Subscript i Baseline upper V right parenthesis left parenthesis upper V Superscript normal upper T Baseline upper A Subscript j Baseline upper V right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column normal d normal i normal a normal g left parenthesis 0 comma upper B Subscript i Baseline right parenthesis normal d normal i normal a normal g left parenthesis upper I Subscript r Baseline comma 0 right parenthesis 3rd Row 1st Column Blank 2nd Column equals 3rd Column 0 period EndLayoutV TAiAjV = (V TAiV )(V TAjV )
= diag(0, Bi)diag(Ir, 0)
= 0.
Because V is nonsingular, this implies the desired conclusion; that is, that 
AiAj = 0  for  any  i /= j. 
We can now extend this result to an idempotent matrix in place of I; that  
is, for an idempotent matrix A with A = A1 + · · ·  + Ak. Rather than stating 
it simply as in Eq. (8.39), however, we will state the implications diﬀerently. 
Let A1, . . . , Ak be n × n symmetric matrices and let 
upper A equals upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline periodA = A1 + · · · + Ak.
(8.41) 
Then any two of the following conditions imply the third one:

8.5 Idempotent and Projection Matrices
395
(a). A is idempotent. 
(b). Ai is idempotent for i = 1, . . . , k. 
(c). AiAj = 0 for all i /= j. 
This is also called Cochran’s theorem. (The theorem also applies to non-
symmetric matrices if condition (c) is augmented with the requirement that 
rank(A2 
i ) = rank(Ai) for all i. We will restrict our attention to symmetric 
matrices, however, because in most applications of these results, the matrices 
are symmetric.) 
First, if we assume properties (a) and (b), we can show that property (c) 
follows using an argument similar to that used to establish Eq. (8.39) for  the  
special case A = I. The formal steps are left as an exercise. 
Now, let us assume properties (b) and (c) and show that property (a) 
holds. With properties (b) and (c), we have  
StartLayout 1st Row 1st Column upper A upper A 2nd Column equals 3rd Column left parenthesis upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline right parenthesis left parenthesis upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript k Endscripts upper A Subscript i Baseline upper A Subscript i plus sigma summation Underscript i not equals j Endscripts sigma summation Underscript j equals 1 Overscript k Endscripts upper A Subscript i Baseline upper A Subscript j 3rd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript k Endscripts upper A Subscript i 4th Row 1st Column Blank 2nd Column equals 3rd Column upper A period EndLayoutAA = (A1 + · · · + Ak) (A1 + · · · + Ak)
=
k
E
i=1
AiAi +
E
i/=j
k
E
j=1
AiAj
=
k
E
i=1
Ai
= A.
Hence, we have property (a); that is,  A is idempotent. 
Finally, let us assume properties (a) and (c). Property (b) follows imme-
diately from 
upper A Subscript i Superscript 2 Baseline equals upper A Subscript i Baseline upper A Subscript i Baseline equals upper A Subscript i Baseline upper A equals upper A Subscript i Baseline upper A upper A equals upper A Subscript i Superscript 2 Baseline upper A equals upper A Subscript i Superscript 3A2
i = AiAi = AiA = AiAA = A2
i A = A3
i
and the implication (8.33). 
Any two of the properties (a) through (c) also imply a fourth property for 
A = A1 + · · · + Ak when the Ai are symmetric: 
(d). rank(A) = rank(A1) + · · ·  + rank(Ak). 
We ﬁrst note that any two of properties (a) through (c) imply the third one, 
so we will just use properties (a) and (b). Property (a) gives 
normal r normal a normal n normal k left parenthesis upper A right parenthesis equals normal t normal r left parenthesis upper A right parenthesis equals normal t normal r left parenthesis upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline right parenthesis equals normal t normal r left parenthesis upper A 1 right parenthesis plus midline horizontal ellipsis plus normal t normal r left parenthesis upper A Subscript k Baseline right parenthesis commarank(A) = tr(A) = tr(A1 + · · · + Ak) = tr(A1) + · · · + tr(Ak),
and property (b) states that the latter expression is rank(A1)+· · ·+rank(Ak), 
thus yielding property (d). 
There is also a partial converse: properties (a) and (d) imply the other 
properties. 
One of the most important special cases of Cochran’s theorem is when 
A = I in the sum (8.41): 
upper I Subscript n Baseline equals upper A 1 plus midline horizontal ellipsis plus upper A Subscript k Baseline periodIn = A1 + · · · + Ak.

396
8 Matrices with Special Properties
The identity matrix is idempotent, so if rank(A1) +  · · ·  + rank(Ak) =  n, all  
the properties above hold. 
The most important statistical application of Cochran’s theorem is for the 
distribution of quadratic forms of normally distributed random vectors. 
8.5.2 Projection Matrices: Symmetric Idempotent Matrices 
For a given vector space V, a symmetric idempotent matrix A whose columns 
span V is said to be a projection matrix onto V; in other words, a matrix A is a 
projection matrix onto span(A) if and only if A is symmetric and idempotent. 
(Some authors do not require a projection matrix to be symmetric. In that 
case, the terms “idempotent” and “projection” are synonymous.) Because a 
projection matrix is an idempotent matrix, the trace of a projection matrix 
and the rank of a projection matrix are the same. 
It is easy to see that, for any vector x, if  A is a projection matrix onto 
V, the vector Ax is in V, and the vector x − Ax is in V⊥ (the vectors Ax 
and x−Ax are orthogonal). For this reason, a projection matrix is sometimes 
called an “orthogonal projection matrix.” Note that an orthogonal projection 
matrix is not an orthogonal matrix, however, unless it is the identity matrix. 
Stating this in alternative notation, if A is a projection matrix and A ∈ IRn×n , 
then A maps IRn onto V(A) and  I − A is also a projection matrix (called the 
complementary projection matrix of A), and it maps IRn onto the orthogonal 
complement, N(A). These spaces are such that V(A) ⊕N(A) = IRn . 
In this text, we use the term “projection” to mean “orthogonal projection,” 
but we should note that in some literature “projection” can include “oblique 
projection.” In the less restrictive deﬁnition, for vector spaces V, X, and  Y, if  
V = X ⊕Y  and v = x + y with x ∈X  and y ∈Y, then the vector x is called 
the projection of v onto X along Y. In this text, to use the unqualiﬁed term 
“projection,” we require that X and Y be orthogonal; if they are not, then 
we call x the oblique projection of v onto X along Y. The choice of the more 
restrictive deﬁnition is because of the overwhelming importance of orthogonal 
projections in statistical applications. The restriction is also consistent with 
the deﬁnition in Eq. (2.50) of the projection of a vector onto another vector 
(as opposed to the projection onto a vector space). 
Because a projection matrix is idempotent, the matrix projects any of 
its columns onto itself, and of course it projects the full matrix onto itself: 
AA = A (see Eq. (8.27)). 
If x is a general vector in IRn , that is,  if  x has order n and belongs to an 
n-dimensional space, and A is a projection matrix of rank r ≤ n, then  Ax has 
order n and belongs to span(A), which is an r-dimensional space. Thus, we 
say projections are dimension reductions. 
Useful projection matrices often encountered in statistical linear models 
are X+ X and XX+ . (Recall that for any generalized inverse X−X is an 
idempotent matrix.) The matrix X+ exists for any n × m matrix X, and  
X+ X is square (m × m) and symmetric.

8.6 Special Matrices Occurring in Data Analysis
397
Projections onto Linear Combinations of Vectors 
On page 46, we gave the projection of a vector y onto a vector x as 
StartFraction x Superscript normal upper T Baseline y Over x Superscript normal upper T Baseline x EndFraction x periodxTy
xTxx.
The projection matrix to accomplish this is the “outer/inner products ma-
trix,” 
StartFraction 1 Over x Superscript normal upper T Baseline x EndFraction x x Superscript normal upper T Baseline period 1
xTxxxT.
(8.42) 
The outer/inner products matrix has rank 1. It is useful in a variety of matrix 
transformations. If x is normalized, the projection matrix for projecting a 
vector on x is just xxT . The projection matrix for projecting a vector onto a 
unit vector ei is eieT 
i , and  eieT 
i y = (0, . . . , yi, . . . , 0). 
This idea can be used to project y onto the plane formed by two vectors, 
x1 and x2, by forming a projection matrix in a similar manner and replacing 
x in Eq. (8.42) with the matrix X = [x1|x2]. On page 450, we will view linear 
regression ﬁtting as a projection onto the space spanned by the independent 
variables. 
The angle between vectors we deﬁned on page 47 can be generalized to 
the angle between a vector and a plane or any linear subspace by deﬁning 
it as the angle between the vector and the projection of the vector onto the 
subspace. By applying the deﬁnition (2.53) to the projection, we see that the 
angle θ between the vector y and the subspace spanned by the columns of a 
projection matrix A is determined by the cosine 
cosine left parenthesis theta right parenthesis equals StartFraction y Superscript normal upper T Baseline upper A y Over y Superscript normal upper T Baseline y EndFraction period cos(θ) = yTAy
yTy .
(8.43) 
8.6 Special Matrices Occurring in Data Analysis 
Some of the most useful applications of matrices are in the representation of 
observational data, as in Fig. 8.1 on page 368. If the data are represented as 
real numbers, the array is a matrix, say X. The  rows  of  the  n×m data matrix 
X are “observations” and correspond to a vector of measurements on a single 
observational unit, and the columns of X correspond to n measurements of a 
single variable or feature. In data analysis we may form various association 
matrices that measure relationships among the variables or the observations 
that correspond to the columns or the rows of X. Many summary statistics 
arise from a matrix of the form XT X. (If  the data in  X are incomplete – 
that is, if some elements are missing – problems may arise in the analysis. 
We discuss some of these issues in Sect. 9.4.6.)

398
8 Matrices with Special Properties
8.6.1 Gramian Matrices 
A (real) matrix A such that for some (real) matrix B, A = BT B, is called a 
Gramian matrix. Any nonnegative deﬁnite matrix is Gramian (from Eq. (8.14) 
and Sect. 4.7.2 on page 246). (This is not a deﬁnition of “Gramian” or “Gram” 
matrix; these terms have more general meanings, but they do include any 
matrix expressible as BT B.) 
Sums of Squares and Cross-Products 
Although the properties of Gramian matrices are of interest, our starting point 
is usually the data matrix X, which we may analyze by forming a Gramian 
matrix XT X or XXT (or a related matrix). These Gramian matrices are also 
called sums of squares and cross-products matrices. (The term “cross product” 
does not refer to the cross product of vectors deﬁned on page 57, but rather to 
the presence of sums over i of the products xijxik along with sums of squares 
x2 
ij.) These matrices and other similar ones are useful association matrices in 
statistical applications. 
Some Immediate Properties of Gramian Matrices 
Some interesting properties of a Gramian matrix XT X that we have discussed 
are: 
• XT X is symmetric. 
• rank(XT X) = rank(X). 
• XT X is of full rank if and only if X is of full column rank. 
• XT X is nonnegative deﬁnite. 
• XT X is positive deﬁnite if and only if X is of full column rank. 
• XT X = 0  ⇐⇒ 
X = 0.  
• BXT X = CXT X 
⇐⇒ 
BXT = CXT . 
• XT XB = XT XC 
⇐⇒ 
XB = XC. 
• If d is a singular value of X, then  c = d2 is an eigenvalue of XT X; or,  
expressed another way, 
if c is a nonzero eigenvalue of XT X, then there is a singular value d of X 
such that d2 = c. 
These properties were shown in Sect. 3.4.11, beginning on page 137, except 
for the last one, which was shown on page 184. 
Each element of a Gramian matrix is the dot product of columns of the 
constituent matrix. If x∗i and x∗j are the ith and jth columns of the matrix 
X, then  
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Subscript i j Baseline equals x Subscript asterisk i Superscript normal upper T Baseline x Subscript asterisk j Baseline period(XTX)ij = xT
∗ix∗j.
(8.44) 
A Gramian matrix is also the sum of the outer products of the rows of the 
constituent matrix. If xi∗ is the ith row of the n × m matrix X, then

8.6 Special Matrices Occurring in Data Analysis
399
upper X Superscript normal upper T Baseline upper X equals sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i asterisk Baseline x Subscript i asterisk Superscript normal upper T Baseline periodXTX =
n
E
i=1
xi∗xT
i∗.
(8.45) 
This is generally the way a Gramian matrix is computed. 
By Eq. (8.14), we see that any Gramian matrix formed from a general 
matrix X is the same as a Gramian matrix formed from a square upper 
triangular matrix T: 
upper X Superscript normal upper T Baseline upper X equals upper T Superscript normal upper T Baseline upper T periodXTX = T TT.
Generalized Inverses of Gramian Matrices 
The generalized inverses of XT X have useful properties. First, we see from 
the deﬁnition, for any generalized inverse (XT X)−, that ((XT X)−)T is also a 
generalized inverse of XT X. (Note that (XT X)− is not necessarily symmet-
ric.) Also, we have, from Eq. (3.177), 
upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X equals upper X periodX(XTX)−XTX = X.
(8.46) 
This means that (XT X)−XT is a generalized inverse of X. 
The Moore-Penrose inverse of X has an interesting relationship with a 
generalized inverse of XT X: 
upper X upper X Superscript plus Baseline equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline periodXX+ = X(XTX)−XT.
(8.47) 
This can be established directly from the deﬁnition of the Moore-Penrose 
inverse. 
An important property of X(XT X)−XT is its invariance to the choice of 
the generalized inverse of XT X. As we saw  in  Eq. (3.232) on page 151, if  G is 
any generalized inverse of XT X, then we have  
upper X upper G upper X Superscript normal upper T Baseline equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline semicolonXGXT = X(XTX)−XT;
(8.48) 
that is, X(XT X)−XT is invariant to the choice of generalized inverse. 
Eigenvalues of Gramian Matrices 
The nonzero eigenvalues of XT X are the same as the nonzero eigenvalues of 
XXT (property 15 on page 162). 
If the singular value decomposition of X is UDV T (page 183), then the 
similar canonical factorization of XT X (Eq. (3.276)) is V DT DV T . Hence, we 
see that the nonzero singular values of X are the square roots of the nonzero 
eigenvalues of the symmetric matrix XT X. By using  DDT similarly, we see 
that they are also the square roots of the nonzero eigenvalues of XXT . 
8.6.2 Projection and Smoothing Matrices 
It is often of interest to approximate an arbitrary n-vector in a given m-
dimensional vector space, where m < n. An  n × n projection matrix of rank 
m clearly does this.

400
8 Matrices with Special Properties
A Projection Matrix Formed from a Gramian Matrix 
An important matrix that arises in analysis of a linear model of the form 
y equals upper X beta plus epsilony = Xβ + e
(8.49) 
is 
upper H equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline commaH = X(XTX)−XT,
(8.50) 
where (XT X)− is any generalized inverse. From Eq. (8.48), H is invariant to 
the choice of generalized inverse. By Eq. (8.47), this matrix can be obtained 
from the pseudoinverse and so 
upper H equals upper X upper X Superscript plus Baseline periodH = XX+.
(8.51) 
In the full-rank case, this is uniquely 
upper H equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline periodH = X(XTX)−1XT.
(8.52) 
Whether or not X is of full rank, H is a projection matrix onto span(X). 
It is called the “hat matrix” because it projects the observed response vector, 
often denoted by y, onto a  predicted response vector, often denoted by -y in 
span(X): 
ModifyingAbove y With caret equals upper H y period-y = Hy.
(8.53) 
Because H is invariant, this projection is invariant to the choice of generalized 
inverse. (In the nonfull-rank case, however, we generally refrain from referring 
to the vector Hy as the “predicted response”; rather, we may call it the “ﬁtted 
response.”) 
The rank of H is  the same as the  rank  of  X, and its trace is the same as 
its rank (because it is idempotent). When X is of full column rank, we have 
normal t normal r left parenthesis upper H right parenthesis equals normal n normal u normal m normal b normal e normal r normal o normal f normal c normal o normal l normal u normal m normal n normal s normal o normal f upper X periodtr(H) = number of columns of X.
(8.54) 
(This can also be seen by using the invariance of the trace to permutations of 
the factors in a product as in Eq. (3.93).) 
In linear models, tr(H) is the model degrees of freedom, and the sum of 
squares due to the model is just yT Hy. 
The complementary projection matrix, 
upper I minus upper H commaI −H,
(8.55) 
also has interesting properties that relate to linear regression analysis. In 
geometrical terms, this matrix projects a vector onto N(XT ), the orthogonal 
complement of span(X). We have 
StartLayout 1st Row 1st Column y 2nd Column equals 3rd Column upper H y plus left parenthesis upper I minus upper H right parenthesis y 2nd Row 1st Column Blank 2nd Column equals 3rd Column ModifyingAbove y With caret plus r comma EndLayouty = Hy + (I −H)y
= -y + r,
(8.56)

8.6 Special Matrices Occurring in Data Analysis
401
where r = (I − H)y ∈N(XT ). The orthogonal complement is called the 
residual vector space, and r is called the residual vector. Both the rank and 
the trace of the orthogonal complement are the number of rows in X (i.e., 
the number of observations) minus the regression degrees of freedom. This 
quantity is the “residual degrees of freedom” (unadjusted). 
These two projection matrices (8.50) or (8.52) and  (8.55) partition the 
total sum of squares: 
y Superscript normal upper T Baseline y equals y Superscript normal upper T Baseline upper H y plus y Superscript normal upper T Baseline left parenthesis upper I minus upper H right parenthesis y periodyTy = yTHy + yT(I −H)y.
(8.57) 
This partitioning yields the total sum of squares into a sum of squares due 
to the ﬁtted relationship between y and X and a “residual” sum of squares. 
The analysis of these two sums of squares is one of the most fundamental 
and important techniques in statistics. Note that the second term in this 
partitioning is the Schur complement of XT X in [X y]T [X y] (see Eq. (3.205) 
on page 144). 
Smoothing Matrices 
The hat matrix, either from a full rank X as in Eq. (8.52) or formed by a  
generalized inverse as in Eq. (8.50), smoothes the vector y onto the hyperplane 
deﬁned by the column space of X. It is therefore a smoothing matrix. (Note 
that the rank of the column space of X is the same as the rank of XT X.) 
A useful variation of the cross-products matrix XT X is the matrix formed 
by adding a nonnegative (positive) deﬁnite matrix A to it. Because XT X is 
nonnegative (positive) deﬁnite, XT X + A is nonnegative deﬁnite, as we have 
seen (page 388), and hence XT X + A is a Gramian matrix. 
Because the square root of the nonnegative deﬁnite A exists, we can express 
the sum of the matrices as 
upper X Superscript normal upper T Baseline upper X plus upper A equals StartBinomialOrMatrix upper X Choose upper A Superscript one half Baseline EndBinomialOrMatrix Superscript normal upper T Baseline StartBinomialOrMatrix upper X Choose upper A Superscript one half Baseline EndBinomialOrMatrix periodXTX + A =
⎡X
A
1
2
⎤T ⎡X
A
1
2
⎤
.
(8.58) 
In a common application, a positive deﬁnite matrix λI, with  λ >  0, is 
added to XT X, and this new matrix is used as a smoothing matrix. The 
analogue of the hat matrix (8.52) is  
upper H Subscript lamda Baseline equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X plus lamda upper I right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline commaHλ = X(XTX + λI)−1XT,
(8.59) 
and the analogue of the ﬁtted response is 
ModifyingAbove y With caret Subscript lamda Baseline equals upper H Subscript lamda Baseline y period-yλ = Hλy.
(8.60) 
This has the eﬀect of shrinking the -y of Eq. (8.53) toward 0. (In regression 
analysis, this is called “ridge regression”; see page 401 or 477.) 
Any matrix such as  Hλ that is used to transform the observed vector y 
onto a given subspace is called a smoothing matrix.

402
8 Matrices with Special Properties
Eﬀective Degrees of Freedom 
Because of the shrinkage in ridge regression (i.e., because the ﬁtted model is 
less dependent just on the data in X), we say the “eﬀective” degree of freedom 
of a ridge regression model decreases with increasing λ. We can formally deﬁne 
the eﬀective model degrees of freedom of any linear ﬁt -y = Hλy as 
normal t normal r left parenthesis upper H Subscript lamda Baseline right parenthesis commatr(Hλ),
(8.61) 
analogous to the model degrees of freedom in linear regression above. This 
deﬁnition of eﬀective degrees of freedom applies generally in data smoothing. 
In fact, many smoothing matrices used in applications depend on a single 
smoothing parameter such as the λ in  ridge regression, and  so  the same no-
tation Hλ is often used for a general smoothing matrix. 
To evaluate the eﬀective degrees of freedom in the ridge regression model 
for a given λ and X, for example, using the singular value decomposition of 
X, X = UDV T , we have  
StartLayout 1st Row 1st Column normal t normal r left parenthesis upper X left parenthesis upper X Superscript normal upper T Baseline upper X plus lamda upper I right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline right parenthesis 2nd Row 1st Column equals 2nd Column normal t normal r left parenthesis upper U upper D upper V Superscript normal upper T Baseline left parenthesis upper V upper D squared upper V Superscript normal upper T Baseline plus lamda upper V upper V Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline upper V upper D upper U Superscript normal upper T Baseline right parenthesis 3rd Row 1st Column equals 2nd Column normal t normal r left parenthesis upper U upper D upper V Superscript normal upper T Baseline left parenthesis upper V left parenthesis upper D squared plus lamda upper I right parenthesis upper V Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline upper V upper D upper U Superscript normal upper T Baseline right parenthesis 4th Row 1st Column equals 2nd Column normal t normal r left parenthesis upper U upper D left parenthesis upper D squared plus lamda upper I right parenthesis Superscript negative 1 Baseline upper D upper U Superscript normal upper T Baseline right parenthesis 5th Row 1st Column equals 2nd Column normal t normal r left parenthesis upper D squared left parenthesis upper D squared plus lamda upper I right parenthesis Superscript negative 1 Baseline right parenthesis 6th Row 1st Column equals 2nd Column sigma summation StartFraction d Subscript i Superscript 2 Baseline Over d Subscript i Superscript 2 Baseline plus lamda EndFraction comma EndLayout
tr(X(XTX + λI)−1XT)
= tr
(
UDV T(V D2V T + λV V T)−1V DU T)
= tr
(
UDV T(V (D2 + λI)V T)−1V DU T)
= tr
(
UD(D2 + λI)−1DU T)
= tr
(
D2(D2 + λI)−1)
=
E
d2
i
d2
i + λ,
(8.62) 
where the di are the singular values of X. 
When λ = 0, this is the same as the ordinary model degrees of freedom, 
and when λ is positive, this quantity is smaller, as we would want it to be by 
the argument above. The d2 
i /(d2 
i + λ) are called shrinkage factors. 
If XT X is not of full rank, the addition of λI to it also has the eﬀect 
of yielding a full-rank matrix, if λ >  0, and so the inverse of XT X + λI 
exists even when that of XT X does not. In any event, the addition of λI to 
XT X yields a matrix with a better condition number, which we discussed in 
Sect. 5.1. (On page 266, we showed that the condition number of XT X + λI 
is better than that of XT X.) 
Residuals from Smoothed Data 
Just as in Eq. (8.56), we can write 
y equals ModifyingAbove y With caret Subscript lamda Baseline plus r Subscript lamda Baseline periody = -yλ + rλ.
(8.63) 
Notice, however, that in -yλ = Hλy, Hλ is not in general a projection matrix. 
Unless Hλ is a projection matrix, -yλ and rλ are not orthogonal in general

8.6 Special Matrices Occurring in Data Analysis
403
as are -y and r, and we do not have the additive partitioning of the sum of 
squares as in Eq. (8.57). 
The rank of Hλ is the same as the number of columns of X, but the trace, 
and hence the model degrees of freedom, is less than this number. 
8.6.3 Centered Matrices and Variance-Covariance Matrices 
In Sect. 2.3, we deﬁned the variance of a vector and the covariance of two vec-
tors. These are the same as the “sample variance” and “sample covariance” in 
statistical data analysis and are related to the variance and covariance of ran-
dom variables in probability theory. We now consider the variance-covariance 
matrix associated with a data matrix. We occasionally refer to the variance-
covariance matrix simply as the “variance matrix” or just as the “variance.” 
First, we consider centering and scaling data matrices. 
Centering and Scaling of Data Matrices 
When the elements in a vector represent similar measurements or observa-
tional data on a given phenomenon, summing or averaging the elements in the 
vector may yield meaningful statistics. In statistical applications, the columns 
in a matrix often represent measurements on the same feature or on the same 
variable over diﬀerent observational units as in Fig. 8.1, and so the mean of a 
column may be of interest. 
We may center the column by subtracting its mean from each element in 
the same manner as we centered vectors on page 59. The matrix formed by 
centering all of the columns of a given matrix is called a centered matrix, 
and if the original matrix is X, we represent the centered matrix as Xc in a 
notation analogous to what we introduced for centered vectors. If we represent 
the matrix whose ith column is the constant mean of the ith column of X as 
X, 
upper X Subscript normal c Baseline equals upper X minus upper X overbar periodXc = X −X.
(8.64) 
Here is an R statement to compute this: 
Xc <- X-rep(1,n)%*%t(apply(X,2,mean)) 
If the unit of a measurement is changed, all elements in a column of the 
data matrix in which the measurement is used will change. The amount of 
variation of elements within a column or the relative variation among diﬀerent 
columns ideally should not be measured in terms of the basic units of mea-
surement, which can diﬀer irreconcilably from one column to another. (For 
example, one column could represent scores on an exam and another column 
could represent weight.) 
In analyzing data, it is usually important to scale the variables so that 
their variations are comparable. We do this by using the standard deviation

404
8 Matrices with Special Properties
of the column. If we have also centered the columns, the column vectors are 
the centered and scaled vectors of the form of those in Eq. (2.72), 
x Subscript normal c normal s Baseline equals StartFraction x Subscript normal c Baseline Over s Subscript x Baseline EndFraction commaxcs = xc
sx
,
where sx is the standard deviation of x, 
s Subscript x Baseline equals StartFraction parallel to x Subscript normal c Baseline parallel to Over StartRoot n minus 1 EndRoot EndFraction periodsx =
||xc||
√n −1.
If all columns of the data matrix X are centered and scaled, we denote the 
resulting matrix as Xcs. If  si represents the standard deviation of the ith 
column, this matrix is formed as  
upper X Subscript normal c normal s Baseline equals upper X Subscript normal c Baseline normal d normal i normal a normal g left parenthesis 1 divided by s Subscript i Baseline right parenthesis periodXcs = Xcdiag(1/si).
(8.65) 
Here is an R statement to compute this: 
Xcs <- Xc%*%diag(1/apply(X,2,sd)) 
If the rows of X are taken as representatives of a population of similar vectors, 
it is often useful to center and scale any vector from that population in the 
manner of Eq. (8.65): 
x overTilde equals normal d normal i normal a normal g left parenthesis 1 divided by s Subscript i Baseline right parenthesis x Subscript normal c Baseline period˜x = diag(1/si)xc.
(8.66) 
(Note that xc is a vector of the same order as a row of Xc.) 
Gramian Matrices Formed from Centered Matrices: Covariance 
Matrices 
An important Gramian matrix is formed as the sums of squares and cross-
products matrix from a centered matrix and scaled by (n −1), where n is the 
number of rows of the original matrix: 
StartLayout 1st Row 1st Column upper S Subscript upper X 2nd Column equals 3rd Column StartFraction 1 Over n minus 1 EndFraction upper X Subscript normal c Superscript normal upper T Baseline upper X Subscript normal c 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis s Subscript i j Baseline right parenthesis period EndLayoutSX =
1
n −1XT
c Xc
= (sij).
(8.67) 
This matrix is called the variance-covariance matrix associated with the given 
matrix X, or the  sample variance-covariance matrix, to distinguish it from the 
variance-covariance matrix of the distribution given in Eq. (7.34) on page 343. 
We denote it by SX or just S. If  x∗i and x∗j are the vectors corresponding to 
the ith and jth columns of X, then  sij = Cov(x∗i, x∗j); that is, the oﬀ-diagonal 
elements are the covariances between the column vectors, as in Eq. (2.76), and 
the diagonal elements are variances of the column vectors. 
This matrix and others formed from it, such as RX in Eq. (8.69) below, are  
called association matrices because they are based on measures of association 
(covariance or correlation) among the columns of X. We could likewise deﬁne

8.6 Special Matrices Occurring in Data Analysis
405
a Gramian association matrix based on measures of association among the 
rows of X. 
A transformation using the Cholesky factor of SX or the square root of 
SX (assuming SX is full rank) results in a matrix whose associated variance-
covariance is the identity. We call this a sphered matrix: 
upper X Subscript normal s normal p normal h normal e normal r normal e normal d Baseline equals upper X Subscript normal c Baseline upper S Subscript upper X Superscript negative one half Baseline periodXsphered = XcS
−1
2
X .
(8.68) 
The matrix SX is a measure of the anisometry of the space of vectors 
represented by the rows of X as mentioned in Sect. 3.3.8. The inverse, S−1 
X , 
in some sense evens out the anisometry. Properties of vectors in the space 
represented by the rows of X are best assessed following a transformation as 
in Eq. (8.66). For example, rather than orthogonality of two vectors u and v, 
a more interesting relationship would be S−1 
X -conjugacy (see Eq. (3.106)): 
u Superscript normal upper T Baseline upper S Subscript upper X Superscript negative 1 Baseline v equals 0 perioduTS−1
X v = 0.
Also, the Mahalanobis distance,
/
(u − v)TS−1 
X (u − v), may be more relevant 
for measuring the diﬀerence in two vectors than the standard Euclidean dis-
tance. 
Gramian Matrices Formed from Scaled Centered Matrices: 
Correlation Matrices 
If the columns of a centered matrix are standardized (i.e., divided by their 
standard deviations, assuming that each is nonconstant, so that the stan-
dard deviation is positive), the scaled cross-products matrix is the correlation 
matrix, often denoted by RX or just R, 
StartLayout 1st Row 1st Column upper R Subscript upper X 2nd Column equals 3rd Column StartFraction 1 Over n minus 1 EndFraction upper X Subscript normal c normal s Superscript normal upper T Baseline upper X Subscript normal c normal s 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis r Subscript i j Baseline right parenthesis comma EndLayoutRX =
1
n −1XT
csXcs
= (rij),
(8.69) 
where if x∗i and x∗j are the vectors corresponding to the ith and jth columns 
of X, rij = Cor(x∗i, x∗j). The correlation matrix can also be expressed 
as RX = XT 
c DXc, where  D is the diagonal matrix whose kth diagonal is 
1/
/
V(x∗k), where V(x∗k) is the sample variance of the kth column; that 
is, V(x∗k) = E
i(xik − ¯x∗k)2 /(n − 1). This Gramian matrix RX is based on 
measures of association among the columns of X. 
The elements along the diagonal of the correlation matrix are all 1, and 
the oﬀ-diagonals are between −1 and 1, each being the correlation between a 
pair of column vectors, as in Eq. (2.78). The correlation matrix is nonnegative 
deﬁnite because it is a Gramian matrix. 
The trace of an n×n correlation matrix is n, and therefore, the eigenvalues, 
which are all nonnegative, sum to n.

406
8 Matrices with Special Properties
Without reference to a data matrix, any nonnegative deﬁnite matrix with 
1s on the diagonal and with all elements less than or equal to 1 in absolute 
value is called a correlation matrix. 
8.6.4 The Generalized Variance 
The diagonal elements of the variance-covariance matrix S associated with 
the n × m data matrix X are the second moments of the centered columns 
of X, and the oﬀ-diagonal elements are pairwise second central moments of 
the columns. Each element of the matrix provides some information about 
the spread of a single column or of two columns of X. The determinant of 
S provides a single overall measure of the spread of the columns of X. This  
measure, |S|, is called the generalized variance, or generalized sample variance, 
to distinguish it from an analogous measure of a distributional model. 
On page 94, we discussed the equivalence of a determinant and the vol-
ume of a parallelotope. The generalized variance captures this, and when the 
columns or rows of S are more orthogonal to each other, the volume of the 
parallelotope determined by the columns or rows of S is greater, as shown in 
Fig. 8.6 for m = 3.  
Figure 8.6. Generalized variances in terms of the columns of S 
The columns or rows of S are generally not of much interest in themselves. 
Our interest is in the relationship of the centered columns of the n × m data 
matrix X. Let us consider the case of m = 2.  Let  z∗1 and z∗2 represent the 
centered column vectors of X; that is,  for  z∗1, we have  z∗1i = x∗1i − ¯x1. 
Now, as in Eq. (3.47), consider the parallelogram formed by z∗1 and z∗2. For  
computing the area, consider z∗1 as forming the base. The length of the base 
is 
parallel to z Subscript asterisk 1 Baseline parallel to equals StartRoot left parenthesis n minus 1 right parenthesis s 11 EndRoot comma||z∗1|| =
/
(n −1)s11,
and the height is 
parallel to z Subscript asterisk 2 Baseline parallel to StartAbsoluteValue sine left parenthesis theta right parenthesis EndAbsoluteValue equals StartRoot left parenthesis n minus 1 right parenthesis s 22 left parenthesis 1 minus r 12 squared right parenthesis EndRoot period||z∗2||| sin(θ)| =
/
(n −1)s22(1 −r2
12).

8.6 Special Matrices Occurring in Data Analysis
407
(Recall the relationship between the angle and the correlation from Eq. (2.79).) 
The area of the parallelogram therefore is 
normal a normal r normal e normal a equals left parenthesis n minus 1 right parenthesis StartRoot s 11 s 22 left parenthesis 1 minus r 12 squared right parenthesis EndRoot periodarea = (n −1)
/
s11s22(1 −r2
12).
Now, consider S: 
StartLayout 1st Row 1st Column upper S 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column s 11 2nd Column s 21 2nd Row 1st Column s 12 2nd Column s 22 EndMatrix 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column s 11 2nd Column StartRoot s 11 s 22 EndRoot r 12 2nd Row 1st Column StartRoot s 11 s 22 EndRoot r 12 2nd Column s 22 EndMatrix period EndLayoutS =
⎡s11 s21
s12 s22
⎤
=
⎡
s11
√s11s22r12
√s11s22r12
s22
⎤
.
The determinant of S is therefore 
s 11 s 22 left parenthesis 1 minus r 12 squared right parenthesis commas11s22(1 −r2
12),
that is, 
StartAbsoluteValue upper S EndAbsoluteValue equals StartFraction 1 Over left parenthesis n minus 1 right parenthesis Superscript m Baseline EndFraction normal v normal o normal l normal u normal m normal e squared period|S| =
1
(n −1)m volume2.
(8.70) 
Although we considered only the case m = 2,  Eq. (8.70) holds generally, 
as can be seen by induction on m (see Anderson 2003). 
Comparing Variance-Covariance Matrices 
Many standard statistical procedures for comparing groups of data rely on the 
assumption that the population variance-covariance matrices of the groups are 
all the same. (The simplest example of this is the two-sample t-test, in which 
the concern is just that the population variances of the two groups be equal.) 
Occasionally, the data analyst wishes to test this assumption of homogeneity 
of variances. 
On page 199, we considered the problem of comparing two matrices of 
the same size. There we deﬁned a metric based on a matrix norm. For the 
problem of comparing variance-covariance matrices, a measure based on the 
generalized variances is more commonly used. 
In the typical situation, we have an n×m data matrix X in which the ﬁrst 
n1 rows represent observations from one group, the next n2 rows represent 
observations from another group, and the last ng rows represent observations 
from the gth group. For each group, we form a sample variance-covariance 
matrix Si as in Eq. (8.67) using  the  ith submatrix of X. Whenever we have 
individual variance-covariance matrices in situations similar to this, we deﬁne 
the pooled variance-covariance matrix: 
upper S Subscript normal p Baseline equals StartFraction 1 Over left parenthesis n minus g right parenthesis EndFraction sigma summation Underscript i equals 1 Overscript g Endscripts left parenthesis n Subscript i Baseline minus 1 right parenthesis upper S Subscript i Baseline periodSp =
1
(n −g)
g
E
i=1
(ni −1)Si.
(8.71)

408
8 Matrices with Special Properties
Bartlett suggested a test based on the determinants of (ni − 1)Si. (From  
Eq. (3.31), |(ni − 1)Si| = (ni − 1)m|Si|.) A similar test suggested by Box also 
uses the generalized variances. One form of the Box M statistic for testing for 
homogeneity of variances is 
upper M equals left parenthesis n minus g right parenthesis log left parenthesis StartAbsoluteValue upper S Subscript normal p Baseline EndAbsoluteValue right parenthesis minus sigma summation Underscript i equals 1 Overscript g Endscripts left parenthesis n Subscript i Baseline minus 1 right parenthesis log left parenthesis StartAbsoluteValue upper S Subscript i Baseline EndAbsoluteValue right parenthesis commaM = (n −g) log(|Sp|) −
g
E
i=1
(ni −1) log(|Si|),
(8.72) 
which under the null hypothesis of equal variance-covariance matrices has 
an approximate chi-squared distribution with m(m + 1)(g − 1) degrees of 
freedom. Both Bartlett’s and Box’s test (as well as modiﬁcations suggested in 
the literature) are sensitive to an assumption of normality, and the tests have 
very little power, even when the assumptions are satisﬁed. 
8.6.5 Similarity Matrices 
Covariance and correlation matrices are examples of similarity association ma-
trices: they measure the similarity or closeness of the columns of the matrices 
from which they are formed. 
The cosine of the angle between two vectors is related to the correlation 
between the vectors, so a matrix of the cosine of the angle between the columns 
of a given matrix would also be a similarity matrix. (The angle is exactly the 
same as the correlation if the vectors are centered; see Eq. (2.78).) 
Similarity matrices can be formed in many ways, and some are more useful 
in particular applications than in others. They may not even arise from a 
standard dataset in the familiar form in statistical applications. For example, 
we may be interested in comparing text documents. We might form a vector-
like object whose elements are the words in the document. The similarity 
between two such ordered tuples, generally of diﬀerent lengths, may be a 
count of the number of words, word pairs, or more general phrases in common 
between the two documents. 
It is generally reasonable that similarity between two objects be symmet-
ric; that is, the ﬁrst object is as close to the second as the second is to the ﬁrst. 
We reserve the term similarity matrix for matrices formed from such measures 
and, hence, that themselves are symmetric. Occasionally, for example, in psy-
chometric applications, the similarities are measured relative to rank order 
closeness within a set. In such a case, the measure of closeness may not be 
symmetric. A matrix formed from such measures is called a directed similarity 
matrix. 
8.6.6 Dissimilarity Matrices 
The elements of similarity generally increase with increasing “closeness.” We 
may also be interested in the dissimilarity. Clearly, any decreasing function of 
similarity could be taken as a reasonable measure of dissimilarity. There are,

8.7 Nonnegative and Positive Matrices
409
however, other measures of dissimilarity that often seem more appropriate. 
In particular, the properties of a metric (see page 41) may suggest that a 
dissimilarity be deﬁned in terms of a metric. 
In considering either similarity or dissimilarity for a data matrix, we could 
work with either rows or columns, but the common applications make one 
or the other more natural for the speciﬁc application. Because of the types 
of the common applications, we will discuss dissimilarities and distances in 
terms of rows instead of columns; thus, in this section, we will consider the 
dissimilarity of xi∗ and xj∗, the vectors corresponding to the ith and jth rows 
of X. 
Dissimilarity matrices are also association matrices in the general sense of 
Sect. 8.1.6. 
Dissimilarity or distance can be measured in various ways. A metric is 
the most obvious measure, although in certain applications other measures 
are appropriate. The measures may be based on some kind of ranking, for 
example. If the dissimilarity is based on a metric, the association matrix is 
often called a distance matrix. In most applications, the Euclidean distance,
||xi∗− xj∗||2, is the most commonly used metric, but others, especially ones 
based on L1 or L∞ norms, are often useful. 
Any hollow matrix with nonnegative elements is a directed dissimilarity 
matrix. A directed dissimilarity matrix is also called a cost matrix. If the  
matrix is symmetric, it is a dissimilarity matrix. 
An n × n matrix D = (dij) is an  m-dimensional distance matrix if there 
exist m-vectors x1 . . . , xn such that, for some metric Δ, dij = Δ(xi, xj). A 
distance matrix is necessarily a dissimilarity matrix. If the metric is the Eu-
clidean distance, the matrix D is called a Euclidean distance matrix. 
The matrix whose rows are the vectors xT 
1 , . . . , xT 
n is called an associated 
conﬁguration matrix of the given distance matrix. In metric multidimensional 
scaling, we are given a dissimilarity matrix and seek to ﬁnd a conﬁguration 
matrix whose associated distance matrix is closest to the dissimilarity matrix, 
usually in terms of the Frobenius norm of the diﬀerence of the matrices (see 
Trosset 2002, for basic deﬁnitions and extensions). 
8.7 Nonnegative and Positive Matrices 
A nonnegative matrix, as the name suggests, is a real matrix, all of whose 
elements are nonnegative, and a positive matrix is a real matrix, all of whose 
elements are positive. In some other literature, the latter type of matrix is 
called strictly positive, and a nonnegative matrix with a positive element is 
called positive. 
Many useful matrices are nonnegative, and we have already considered 
various kinds of nonnegative matrices. The adjacency or connectivity matrix of 
a graph is nonnegative. Dissimilarity matrices, including distance matrices, are 
nonnegative. Matrices used in modeling stochastic processes are nonnegative.

410
8 Matrices with Special Properties
While many of these matrices are square, we do not restrict the deﬁnitions to 
square matrices. 
If A is nonnegative, we write 
upper A greater than or equals 0 commaA ≥0,
(8.73) 
and if it is positive, we write 
upper A greater than 0 periodA > 0.
(8.74) 
Notice that A ≥0 and  A /= 0 together do not imply A >  0. 
We write 
upper A greater than or equals upper BA ≥B
to mean (A − B) ≥ 0 and  
upper A greater than upper BA > B
to mean (A − B) > 0. (Recall the deﬁnitions of nonnegative deﬁnite and pos-
itive deﬁnite matrices, and, from Eqs. (8.11) and  (8.16), the notation used to 
indicate those properties, A > 0 and  A > 0. Furthermore, notice that these 
deﬁnitions and this notation for nonnegative and positive matrices are con-
sistent with analogous deﬁnitions and notation involving vectors on page 25. 
Some authors, however, use the notation of Eqs. (8.73) and  (8.74) to mean  
“nonnegative deﬁnite” and “positive deﬁnite.” We should also note that some 
authors use somewhat diﬀerent terms for these and related properties. “Posi-
tive” for these authors means nonnegative with at least one positive element, 
and “strictly positive” means positive as we have deﬁned it.) 
Notice that positiveness (nonnegativeness) has nothing to do with posi-
tive (nonnegative) deﬁniteness. A positive or nonnegative matrix need not be 
symmetric or even square, although most such matrices useful in applications 
are square. A square positive matrix, unlike a positive deﬁnite matrix, need 
not be of full rank. 
The following properties are easily veriﬁed: 
1. If A ≥ 0 and  u ≥ v ≥ 0, then Au ≥ Av. 
2. If A ≥ 0, A /= 0,  and  u > v  >  0, then Au > Av. 
3. If A >  0 and  v ≥0, then Av ≥ 0. 
4. If A >  0 and  A is square, then ρ(A) > 0. 
Whereas most of the important matrices arising in the analysis of linear 
models are symmetric, and thus have the properties listed in Sect. 8.2.1 begin-
ning on page 378, many important nonnegative matrices, such as those used in 
studying stochastic processes, are not necessarily symmetric. The eigenvalues 
of real symmetric matrices are real, but the eigenvalues of real nonsymmet-
ric matrices may have imaginary components. In the following discussion, we 
must be careful to remember the meaning of the spectral radius. The deﬁni-
tion in Eq. (3.256) for the spectral radius of the matrix A with eigenvalues 
ci, 
rho left parenthesis upper A right parenthesis equals max StartAbsoluteValue c Subscript i Baseline EndAbsoluteValue commaρ(A) = max |ci|,

8.7 Nonnegative and Positive Matrices
411
is still correct, but the operator “| · |” must be interpreted as the modulus of 
a complex number. 
The Convex Cones of Nonnegative and Positive Matrices 
The class of all n × m nonnegative (positive) matrices is a cone because if 
X is a nonnegative (positive) matrix and a >  0, then aX is a nonnegative 
(positive) matrix (see page 53). Furthermore, it is a convex cone in IRn×m , 
because if X1 and X2 are n × m nonnegative (positive) matrices and a, b ≥ 0, 
then aX1 + bX2 is nonnegative (positive) so long as either a /= 0  or  b /= 0.  
Both of these classes are closed under Cayley multiplication, but they 
may not have inverses in the class (i.e., in particular, they are not groups 
with respect to that operation). 
8.7.1 Properties of Square Positive Matrices 
We have the following important properties for square positive matrices. These 
properties collectively are the conclusions of the Perron theorem. 
Let A be a square positive matrix and let r = ρ(A). Then: 
1. r is an eigenvalue of A. The eigenvalue r is called the Perron root. Note  
that the Perron root is real and positive (although other eigenvalues of A 
may not be). 
2. There is an eigenvector v associated with r such that v >  0. 
3. The Perron root is simple. (That is, the algebraic multiplicity of the Perron 
root is 1.) 
4. The dimension of the eigenspace of the Perron root is 1. (That is, the 
geometric multiplicity of ρ(A) is 1.) Hence, if v is an eigenvector associated 
with r, it is unique except for scaling. This associated eigenvector is called 
the Perron vector. Note that the Perron vector is real (although other 
eigenvectors of A may not be). The elements of the Perron vector all have 
the same sign, which we usually take to be positive; that is, v >  0. 
5. If ci is any other eigenvalue of A, then  |ci| < r. (That  is,  r is the only 
eigenvalue on the spectral circle of A.) 
We will give proofs only of properties 1 and 2 as examples. Proofs of all 
of these facts are available in Bapat and Raghavan (1997). 
To see properties 1 and 2, ﬁrst observe that a positive matrix must have 
at least one nonzero eigenvalue because the coeﬃcients and the constant in 
the characteristic equation must all be positive. Now scale the matrix so that 
its spectral radius is 1 (see page 164). So without loss of generality, let A be 
a scaled positive matrix with ρ(A) = 1.  Now  let  (c, x) be some eigenpair of A 
such that |c| =  1. First, we want to show,  for some such  c, that  c = ρ(A). 
Because all elements of A are positive, 
StartAbsoluteValue x EndAbsoluteValue equals StartAbsoluteValue upper A x EndAbsoluteValue less than or equals upper A StartAbsoluteValue x EndAbsoluteValue comma|x| = |Ax| ≤A|x|,

412
8 Matrices with Special Properties
and so 
upper A StartAbsoluteValue x EndAbsoluteValue minus StartAbsoluteValue x EndAbsoluteValue greater than or equals 0 periodA|x| −|x| ≥0.
(8.75) 
An eigenvector must be nonzero, so we also have 
upper A StartAbsoluteValue x EndAbsoluteValue greater than 0 periodA|x| > 0.
Now we want to show that  A|x| −|x| = 0. To that end, suppose the contrary; 
that is, suppose A|x|−|x| /= 0.  In that  case,  A(A|x|−|x|) > 0 from Eq. (8.75), 
and so there must be a positive number e such that 
StartFraction upper A Over 1 plus epsilon EndFraction upper A StartAbsoluteValue x EndAbsoluteValue greater than upper A StartAbsoluteValue x EndAbsoluteValue A
1 + eA|x| > A|x|
or 
upper B y greater than y commaBy > y,
where B = A/(1 + e) and  y = A|x|. Now successively multiplying both sides 
of this inequality by the positive matrix B, we have  
upper B Superscript k Baseline y greater than y for all k equals 1 comma 2 comma ellipsis periodBky > y
for all k = 1, 2, . . . .
Because ρ(B) =  ρ(A)/(1 + e) < 1, from Eq. (3.344) on page 196, we have  
limk→∞ Bk = 0; that is, limk→∞ Bk y = 0  > y. This contradicts the fact that 
y >  0. Because the supposition A|x| −|x| /= 0 led to this contradiction, we 
must have A|x| −|x| = 0. Therefore, 1 = ρ(A) must be an eigenvalue of A, 
and |x| must be an associated eigenvector; hence, with v = |x|, (ρ(A), v) is  
an eigenpair of A and v >  0, and this is the statement made in properties 1 
and 2. 
The Perron-Frobenius theorem, which we consider below, extends these 
results to a special class of square nonnegative matrices. (This class includes 
all positive matrices, so the Perron-Frobenius theorem is an extension of the 
Perron theorem.) 
8.7.2 Irreducible Square Nonnegative Matrices 
Nonnegativity of a matrix is not a very strong property. First of all, note that 
it includes the zero matrix; hence, clearly none of the properties of the Perron 
theorem can hold. Even a nondegenerate, full-rank nonnegative matrix does 
not necessarily possess those properties. A small full-rank nonnegative matrix 
provides a counterexample for properties 2, 3, and  5: 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 1 2nd Row 1st Column 0 2nd Column 1 EndMatrix periodA =
⎡
1 1
0 1
⎤
.
The eigenvalues are 1 and 1; that is, 1 with an algebraic multiplicity of 2 (so 
property 3 does not hold). There is only one nonnull eigenvector, (1, −1), (so 
property 2 does not hold, but property 4 holds), but the eigenvector is not

8.7 Nonnegative and Positive Matrices
413
positive (or even nonnegative). Of course property 5 cannot hold if property 3 
does not hold. 
We now consider irreducible square nonnegative matrices. This class in-
cludes positive matrices, and irreducibility yields some of the properties of pos-
itivity to matrices with some zero elements. (Heuristically, irreducibility puts 
some restrictions on the nonpositive elements of the matrix.) On page 374, we  
deﬁned reducibility of a nonnegative square matrix and we saw that a matrix 
is irreducible if and only if its digraph is strongly connected. 
To recall the deﬁnition, a nonnegative matrix is said to be reducible if by 
symmetric permutations it can be put into a block upper triangular matrix 
with square blocks along the diagonal; that is, the nonnegative matrix A is 
reducible if and only if there is a permutation matrix E(π) such that 
upper E Subscript left parenthesis pi right parenthesis Baseline upper A upper E Subscript left parenthesis pi right parenthesis Baseline equals Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column 0 2nd Column upper B 22 EndMatrix commaE(π)AE(π) =
⎡
B11 B12
0
B22
⎤
,
(8.76) 
where B11 and B22 are square. A matrix that cannot be put into that form 
is irreducible. An alternate term for reducible is decomposable, with the  as-
sociated term indecomposable. (There is an alternate meaning for the term 
“reducible” applied to a matrix. This alternate use of the term means that 
the matrix is capable of being expressed by a similarity transformation as the 
sum of two matrices whose columns are mutually orthogonal.) 
We see from the deﬁnition in Eq. (8.76) that a positive matrix is irreducible. 
Irreducible matrices have several interesting properties. An n × n nonneg-
ative matrix A is irreducible if and only if (I + A)n−1 is a positive matrix; 
that is, 
upper A is irreducible long left right double arrow left parenthesis upper I plus upper A right parenthesis Superscript n minus 1 Baseline greater than 0 periodA is irreducible ⇐⇒(I + A)n−1 > 0.
(8.77) 
To see this, ﬁrst assume (I +A)n−1 > 0; thus, (I +A)n−1 clearly is irreducible. 
If A is reducible, then there exists a permutation matrix E(π) such that 
upper E Subscript left parenthesis pi right parenthesis Baseline upper A upper E Subscript left parenthesis pi right parenthesis Baseline equals Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column 0 2nd Column upper B 22 EndMatrix commaE(π)AE(π) =
⎡B11 B12
0
B22
⎤
,
and so 
StartLayout 1st Row 1st Column upper E Subscript left parenthesis pi right parenthesis Baseline left parenthesis upper I plus upper A right parenthesis Superscript n minus 1 Baseline upper E Subscript left parenthesis pi right parenthesis 2nd Column equals 3rd Column left parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline left parenthesis upper I plus upper A right parenthesis upper E Subscript left parenthesis pi right parenthesis Baseline right parenthesis Superscript n minus 1 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis upper I plus upper E Subscript left parenthesis pi right parenthesis Baseline upper A upper E Subscript left parenthesis pi right parenthesis Baseline right parenthesis Superscript n minus 1 3rd Row 1st Column Blank 2nd Column equals 3rd Column Start 2 By 2 Matrix 1st Row 1st Column upper I Subscript n 1 Baseline plus upper B 11 2nd Column upper B 12 2nd Row 1st Column 0 2nd Column upper I Subscript n 2 Baseline plus upper B 22 EndMatrix period EndLayoutE(π)(I + A)n−1E(π) =
(
E(π)(I + A)E(π)
)n−1
=
(
I + E(π)AE(π)
)n−1
=
⎡
In1 + B11
B12
0
In2 + B22
⎤
.
This decomposition of (I +A)n−1 cannot exist because it is irreducible; hence, 
we conclude A is irreducible if (I + A)n−1 > 0. 
Now, if A is irreducible, we can see that (I + A)n−1 must be a positive 
matrix either by a strictly linear algebraic approach or by couching the argu-
ment in terms of the digraph G(A) formed by the matrix, as in the discussion

414
8 Matrices with Special Properties
on page 375 that showed that a digraph is strongly connected if (and only if) 
it is irreducible. We will use the latter approach in the spirit of applications 
of irreducibility in stochastic processes. 
For either approach, we ﬁrst observe that the (i, j)th element of (I +A)n−1 
can be expressed as 
left parenthesis left parenthesis upper I plus upper A right parenthesis Superscript n minus 1 Baseline right parenthesis Subscript i j Baseline equals left parenthesis sigma summation Underscript k equals 0 Overscript n minus 1 Endscripts StartBinomialOrMatrix n minus 1 Choose k EndBinomialOrMatrix upper A Superscript k Baseline right parenthesis Subscript i j Baseline period
(
(I + A)n−1)
ij =
(n−1
E
k=0
(n −1
k
)
Ak
)
ij
.
(8.78) 
Hence, for k = 1, . . . , n  − 1, we consider the (i, j)th entry of Ak . Let  a (k) 
ij 
represent this quantity. 
Given any pair (i, j), for some l1, l2, . . . , lk−1, we have  
a Subscript i j Superscript left parenthesis k right parenthesis Baseline equals sigma summation Underscript l 1 comma l 2 comma ellipsis comma l Subscript k minus 1 Baseline Endscripts a Subscript 1 l 1 Baseline a Subscript l 1 l 2 Baseline midline horizontal ellipsis a Subscript l Sub Subscript k minus 1 Subscript j Baseline perioda(k)
ij =
E
l1,l2,...,lk−1
a1l1al1l2 · · · alk−1j.
Now a (k) 
ij > 0 if and only if a1l1 , al1l2 , . . . , alk−1j are all positive, that is, if 
there is a path v1, vl1 , . . . , vlk−1 , vj in G(A). If A is irreducible, then G(A) is  
strongly connected, and hence the path exists. So, for any pair (i, j), we have 
from Eq. (8.78)
(
(I + A)n−1)
ij > 0; that is, (I + A)n−1 > 0. 
The positivity of (I + A)n−1 for an irreducible nonnegative matrix A is a 
very useful property because it allows us to extend some conclusions of the 
Perron theorem to irreducible nonnegative matrices. 
Properties of Square Irreducible Nonnegative Matrices; the 
Perron-Frobenius Theorem 
If A is a square irreducible nonnegative matrix, then we have the following 
properties, which are similar to properties 1 through 4 on page 411 for pos-
itive matrices. These following properties are the conclusions of the Perron-
Frobenius theorem. 
1. ρ(A) is an eigenvalue of A. This eigenvalue is called the Perron root, as  
before (and, as before, it is real and positive). 
2. The Perron root ρ(A) is simple. (That is, the algebraic multiplicity of the 
Perron root is 1.) 
3. The dimension of the eigenspace of the Perron root is 1. (That is, the 
geometric multiplicity of ρ(A) is 1.)  
4. The eigenvector associated with ρ(A) is positive. This eigenvector is called 
the Perron vector, as before. 
The relationship (8.77) allows us to prove properties 1 and 4 in a method 
similar to the proofs of properties 1 and 2 for positive matrices. (This is 
Exercise 8.9.) Complete proofs of all of these facts are available in Bapat and 
Raghavan (1997).

8.7 Nonnegative and Positive Matrices
415
The one property of square positive matrices that does not carry over 
to square irreducible nonnegative matrices is property 5: r = ρ(A) is the  
only eigenvalue on the spectral circle of A. For example, the small irreducible 
nonnegative matrix 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column 0 2nd Column 1 2nd Row 1st Column 1 2nd Column 0 EndMatrixA =
⎡
0 1
1 0
⎤
has eigenvalues 1 and −1, and so both are on the spectral circle. 
Primitive Matrices 
Square irreducible nonnegative matrices that have only one eigenvalue on the 
spectral circle, that is, that do satisfy property 5 on page 411, also have other 
interesting properties that are important, for example, in Markov chains. We 
therefore give a name to matrices with that property: 
A square irreducible nonnegative matrix is said to be primitive if it 
has only one eigenvalue on the spectral circle. 
Limiting Behavior of Primitive Matrices 
In modeling with Markov chains and other applications, the limiting behavior 
of Ak is an important property. 
On page 195, we saw that limk→∞ Ak = 0 if  ρ(A) < 1. For a primitive 
matrix, we also have a limit for Ak if ρ(A) = 1. (As we have done above, we 
can scale any matrix with a nonzero eigenvalue to a matrix with a spectral 
radius of 1.) 
If A is a primitive matrix, then we have the useful result 
limit Underscript k right arrow normal infinity Endscripts left parenthesis StartFraction upper A Over rho left parenthesis upper A right parenthesis EndFraction right parenthesis Superscript k Baseline equals v w Superscript normal upper T Baseline comma lim
k→∞
( A
ρ(A)
)k
= vwT,
(8.79) 
where v is an eigenvector of A associated with ρ(A) and  w is an eigenvector 
of AT associated with ρ(A), and w and v are scaled so that wT v = 1. (As  
we mentioned on page 181, such eigenvectors exist because ρ(A) is a simple  
eigenvalue. They also exist because of property 4; they are both positive. 
Note that A is not necessarily symmetric, and so its eigenvectors may include 
imaginary components; however, the eigenvectors associated with ρ(A) are  
real, and so we can write wT instead of wH .) 
To see Eq. (8.79), we consider
(
A −ρ(A)vwT)
. First, if (ci, vi) is an eigen-
pair of
(
A −ρ(A)vwT)
and ci /= 0,  then  (ci, vi) is an eigenpair of A. We can  
see this by multiplying both sides of the eigen-equation by vwT : 
StartLayout 1st Row 1st Column c Subscript i Baseline v w Superscript normal upper T Baseline v Subscript i 2nd Column equals 3rd Column v w Superscript normal upper T Baseline left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis v Subscript i 2nd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis v w Superscript normal upper T Baseline upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline v w Superscript normal upper T Baseline right parenthesis v Subscript i 3rd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis v Subscript i 4th Row 1st Column Blank 2nd Column equals 3rd Column 0 semicolon EndLayoutcivwTvi = vwT (
A −ρ(A)vwT)
vi
=
(
vwTA −ρ(A)vwTvwT)
vi
=
(
ρ(A)vwT −ρ(A)vwT)
vi
= 0;

416
8 Matrices with Special Properties
hence, 
StartLayout 1st Row 1st Column upper A v Subscript i 2nd Column equals 3rd Column left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis v Subscript i 2nd Row 1st Column Blank 2nd Column equals 3rd Column c Subscript i Baseline v Subscript i Baseline period EndLayoutAvi =
(
A −ρ(A)vwT)
vi
= civi.
Next, we show that 
rho left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis less than rho left parenthesis upper A right parenthesis periodρ
(
A −ρ(A)vwT)
< ρ(A).
(8.80) 
If ρ(A) were an eigenvalue of
(
A −ρ(A)vwT)
, then its associated eigenvector, 
say w, would also have to be an eigenvector of A, as we saw above. But since 
as an eigenvalue of A the geometric multiplicity of ρ(A) is 1, for some scalar 
s, w = sv. But this is impossible because that would yield 
StartLayout 1st Row 1st Column rho left parenthesis upper A right parenthesis s v 2nd Column equals 3rd Column left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis s v 2nd Row 1st Column Blank 2nd Column equals 3rd Column s upper A v minus s rho left parenthesis upper A right parenthesis v 3rd Row 1st Column Blank 2nd Column equals 3rd Column 0 comma EndLayoutρ(A)sv =
(
A −ρ(A)vwT)
sv
= sAv −sρ(A)v
= 0,
and neither ρ(A) nor  sv is zero. But as we saw above, any eigenvalue of
(
A − ρ(A)vwT)
is an eigenvalue of A and no eigenvalue of
(
A − ρ(A)vwT)
can be as large as ρ(A) in modulus; therefore, we have inequality (8.80). 
Finally, we recall Eq. (3.294), with w and v as deﬁned above, and with the 
eigenvalue ρ(A), 
left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis Superscript k Baseline equals upper A Superscript k Baseline minus left parenthesis rho left parenthesis upper A right parenthesis right parenthesis Superscript k Baseline v w Superscript normal upper T Baseline comma
(
A −ρ(A)vwT)k = Ak −(ρ(A))kvwT,
(8.81) 
for k = 1, 2, . . .. 
Dividing both sides of Eq. (8.81) by (ρ(A))k and rearranging terms, we 
have 
left parenthesis StartFraction upper A Over rho left parenthesis upper A right parenthesis EndFraction right parenthesis Superscript k Baseline equals v w Superscript normal upper T Baseline plus left parenthesis StartFraction left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis Over rho left parenthesis upper A right parenthesis EndFraction right parenthesis Superscript k Baseline period
( A
ρ(A)
)k
= vwT +
((
A −ρ(A)vwT)
ρ(A)
)k
.
(8.82) 
Now 
rho left parenthesis StartFraction left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis Over rho left parenthesis upper A right parenthesis EndFraction right parenthesis equals StartFraction rho left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis Over rho left parenthesis upper A right parenthesis EndFraction commaρ
((
A −ρ(A)vwT)
ρ(A)
)
= ρ
(
A −ρ(A)vwT)
ρ(A)
,
which is less than 1; hence, from Eq. (3.342) on page 195, we have  
limit Underscript k right arrow normal infinity Endscripts left parenthesis StartFraction left parenthesis upper A minus rho left parenthesis upper A right parenthesis v w Superscript normal upper T Baseline right parenthesis Over rho left parenthesis upper A right parenthesis EndFraction right parenthesis Superscript k Baseline equals 0 semicolon lim
k→∞
((
A −ρ(A)vwT)
ρ(A)
)k
= 0;
so, taking the limit in Eq. (8.82), we have Eq. (8.79). 
Applications of the Perron-Frobenius theorem are far-ranging. It has im-
plications for the convergence of some iterative algorithms, such as the power 
method discussed in Sect. 6.2. The most important applications in statistics 
are in the analysis of Markov chains, which we discuss in Sect. 9.5.1.

8.7 Nonnegative and Positive Matrices
417
8.7.3 Stochastic Matrices 
A nonnegative n × n matrix P such that 
upper P 1 Subscript n Baseline equals 1 Subscript nP1n = 1n
(8.83) 
is called a stochastic matrix. The deﬁnition means that (1, 1n) is an eigenpair 
of any stochastic matrix. It is also clear that if P is a stochastic matrix, 
then ||P||∞ = 1 (see page 189), and because ρ(P) ≤||P|| for any norm (see 
page 194) and 1 is an eigenvalue of P, we have  ρ(P) = 1.  
A stochastic matrix may not be positive, and it may be reducible or irre-
ducible. (Hence, (1, 1n) may not be the Perron root and Perron eigenvector.) 
If P is a stochastic matrix such that 
1 Subscript n Superscript normal upper T Baseline upper P equals 1 Subscript n Superscript normal upper T Baseline comma1T
nP = 1T
n,
(8.84) 
it is called a doubly stochastic matrix. If  P is a doubly stochastic matrix,
||P||1 = 1, and, of course, ||P||∞ = 1  and  ρ(P) = 1.  
A permutation matrix is a doubly stochastic matrix; in fact, it is the sim-
plest and one of the most commonly encountered doubly stochastic matrices. 
A permutation matrix is clearly reducible. 
Stochastic matrices are particularly interesting because of their use in 
deﬁning a discrete homogeneous Markov chain. In that application, a stochas-
tic matrix and distribution vectors play key roles. A distribution vector is a 
nonnegative matrix whose elements sum to 1; that is, a vector v such that 
1T 
nv = 1. In Markov chain models, the stochastic matrix is a probability tran-
sition matrix from a distribution at time t, πt, to the distribution at time 
t + 1,  
pi Subscript t plus 1 Baseline equals upper P pi Subscript t Baseline periodπt+1 = Pπt.
In Sect. 9.5.1, we deﬁne some basic properties of Markov chains. Those prop-
erties depend in large measure on whether the transition matrix is reducible 
or not. 
8.7.4 Leslie Matrices 
Another type of nonnegative transition matrix, often used in population stud-
ies, is a Leslie matrix, after P. H. Leslie, who used it in models in demography. 
A Leslie matrix is a matrix of the form 
Start 5 By 5 Matrix 1st Row 1st Column alpha 1 2nd Column alpha 2 3rd Column midline horizontal ellipsis 4th Column alpha Subscript m minus 1 Baseline 5th Column alpha Subscript m Baseline 2nd Row 1st Column sigma 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column sigma 2 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 5th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column sigma Subscript m minus 1 Baseline 5th Column 0 EndMatrix comma
⎡
⎢⎢⎢⎢⎢⎣
α1 α2 · · · αm−1 αm
σ1 0 · · ·
0
0
0 σ2 · · ·
0
0
...
...
...
...
...
0
0 · · · σm−1
0
⎤
⎥⎥⎥⎥⎥⎦
,
(8.85) 
where all elements are nonnegative, and additionally σi ≤ 1. 
A Leslie matrix is clearly reducible. Furthermore, a Leslie matrix has a 
single unique positive eigenvalue (see Exercise 8.10), which leads to some 
interesting properties (see Sect. 9.5.2).

418
8 Matrices with Special Properties
8.8 Other Matrices with Special Structures 
Matrices of a variety of special forms arise in statistical analyses and other 
applications. For some matrices with special structure, specialized algorithms 
can increase the speed of performing a given task considerably. Many tasks 
involving matrices require a number of computations of the order of n3 , where  
n is the number of rows or columns of the matrix. For some of the matrices 
discussed in this section, because of their special structure, the order of com-
putations may be n2 . The improvement from O(n3 ) to O(n2 ) is enough to 
make some tasks feasible that would otherwise be infeasible because of the 
time required to complete them. The collection of papers in Olshevsky (2003) 
describe various specialized algorithms for the kinds of matrices discussed in 
this section. 
8.8.1 Helmert Matrices 
A Helmert matrix is a square orthogonal matrix that partitions sums of 
squares. Its main use in statistics is in deﬁning contrasts in general linear 
models to compare the second level of a factor with the ﬁrst level, the third 
level with the average of the ﬁrst two, and so on. (There is another meaning 
of “Helmert matrix” that arises from so-called Helmert transformations used 
in geodesy.) 
For example, a partition of the sum En 
i=1 y2 
i into orthogonal sums, each 
involving ¯y2 
k and Ek 
i=1(yi − ¯yk)2 , is  
StartLayout 1st Row 1st Column y overTilde Subscript i 2nd Column equals 3rd Column left parenthesis i left parenthesis i plus 1 right parenthesis right parenthesis Superscript negative 1 divided by 2 Baseline left parenthesis sigma summation Underscript j equals 1 Overscript i plus 1 Endscripts y Subscript j Baseline minus left parenthesis i plus 1 right parenthesis y Subscript i plus 1 Baseline right parenthesis normal f normal o normal r i equals 1 comma ellipsis n minus 1 comma 2nd Row 1st Column Blank 3rd Row 1st Column y overTilde Subscript n 2nd Column equals 3rd Column n Superscript negative 1 divided by 2 Baseline sigma summation Underscript j equals 1 Overscript n Endscripts y Subscript j Baseline period EndLayout
˜yi = (i(i + 1))−1/2
⎛
⎝
i+1
E
j=1
yj −(i + 1)yi+1
⎞
⎠
for i = 1, . . . n −1,
˜yn = n−1/2
n
E
j=1
yj.
(8.86) 
These expressions lead to a computationally stable one-pass algorithm for 
computing the sample variance (see Eq. (10.8) on page 569). 
The Helmert matrix that corresponds to this partitioning has the form 
StartLayout 1st Row 1st Column upper H Subscript n 2nd Column equals 3rd Column Start 5 By 5 Matrix 1st Row 1st Column 1 divided by StartRoot n EndRoot 2nd Column 1 divided by StartRoot n EndRoot 3rd Column 1 divided by StartRoot n EndRoot 4th Column midline horizontal ellipsis 5th Column 1 divided by StartRoot n EndRoot 2nd Row 1st Column 1 divided by StartRoot 2 EndRoot 2nd Column negative 1 divided by StartRoot 2 EndRoot 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 0 3rd Row 1st Column 1 divided by StartRoot 6 EndRoot 2nd Column 1 divided by StartRoot 6 EndRoot 3rd Column negative 2 divided by StartRoot 6 EndRoot 4th Column midline horizontal ellipsis 5th Column 0 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 5th Row 1st Column StartFraction 1 Over StartRoot n left parenthesis n minus 1 right parenthesis EndRoot EndFraction 2nd Column StartFraction 1 Over StartRoot n left parenthesis n minus 1 right parenthesis EndRoot EndFraction 3rd Column StartFraction 1 Over StartRoot n left parenthesis n minus 1 right parenthesis EndRoot EndFraction 4th Column midline horizontal ellipsis 5th Column minus StartFraction left parenthesis n minus 1 right parenthesis Over StartRoot n left parenthesis n minus 1 right parenthesis EndRoot EndFraction EndMatrix 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartBinomialOrMatrix 1 divided by StartRoot n EndRoot 1 Subscript n Superscript normal upper T Baseline Choose upper K Subscript n minus 1 Baseline EndBinomialOrMatrix comma EndLayoutHn =
⎡
⎢⎢⎢⎢⎢⎢⎣
1/√n
1/√n
1/√n
· · ·
1/√n
1/
√
2
−1/
√
2
0
· · ·
0
1/
√
6
1/
√
6
−2/
√
6 · · ·
0
...
...
...
...
...
1
√
n(n−1)
1
√
n(n−1)
1
√
n(n−1) · · · −
(n−1)
√
n(n−1)
⎤
⎥⎥⎥⎥⎥⎥⎦
=
⎡
1/√n 1T
n
Kn−1
⎤
,
(8.87)

8.8 Other Matrices with Special Structures
419
where Kn−1 is the (n−1)×n matrix below the ﬁrst row. For the full n-vector 
y, we have  
StartLayout 1st Row 1st Column y Superscript normal upper T Baseline upper K Subscript n minus 1 Superscript normal upper T Baseline upper K Subscript n minus 1 Baseline y 2nd Column equals 3rd Column sigma summation left parenthesis y Subscript i Baseline minus y overbar right parenthesis squared 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation left parenthesis y Subscript i Baseline minus y overbar right parenthesis squared 3rd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis n minus 1 right parenthesis s Subscript y Superscript 2 Baseline period EndLayoutyTKT
n−1Kn−1y =
E
(yi −¯y)2
=
E
(yi −¯y)2
= (n −1)s2
y.
The rows of the matrix in Eq. (8.87) correspond to orthogonal contrasts in 
the analysis of linear models (see Sect. 9.2). 
Obviously, the sums of squares are never computed by forming the Helmert 
matrix explicitly and then computing the quadratic form, but the computa-
tions in partitioned Helmert matrices are performed indirectly in analysis of 
variance, and representation of the computations in terms of the matrix is 
often useful in the analysis of the computations. 
8.8.2 Vandermonde Matrices 
A Vandermonde matrix is an n × m matrix with columns that are deﬁned by 
monomials, 
upper V Subscript n times m Baseline equals Start 4 By 5 Matrix 1st Row 1st Column 1 2nd Column x 1 3rd Column x 1 squared 4th Column midline horizontal ellipsis 5th Column x 1 Superscript m minus 1 Baseline 2nd Row 1st Column 1 2nd Column x 2 3rd Column x 2 squared 4th Column midline horizontal ellipsis 5th Column x 2 Superscript m minus 1 Baseline 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 4th Row 1st Column 1 2nd Column x Subscript n Baseline 3rd Column x Subscript n Superscript 2 Baseline 4th Column midline horizontal ellipsis 5th Column x Subscript n Superscript m minus 1 Baseline EndMatrix commaVn×m =
⎡
⎢⎢⎢⎣
1 x1 x2
1 · · · xm−1
1
1 x2 x2
2 · · · xm−1
2
...
...
... ...
...
1 xn x2
n · · · xm−1
n
⎤
⎥⎥⎥⎦,
(8.88) 
where xi /= xj if i /= j. The Vandermonde matrix arises in polynomial regres-
sion analysis. For the model equation yi = β0 + β1xi + · · ·  + βpxp 
i + ei, given  
observations on y and x, a Vandermonde matrix is the matrix in the standard 
representation y = Xβ + e. 
Because of the relationships among the columns of a Vandermonde matrix, 
computations for polynomial regression analysis can be subject to numerical 
errors, and so sometimes we make transformations based on orthogonal poly-
nomials. The condition number (see Sect. 5.1, page 260) for a Vandermonde 
matrix is large. A Vandermonde matrix, however, can be used to form simple 
orthogonal vectors that correspond to orthogonal polynomials. For example, 
if the xs are chosen over a grid on [−1, 1], a QR factorization (see Sect. 4.6 on 
page 238) yields orthogonal vectors that correspond to Legendre polynomials. 
These vectors are called discrete Legendre polynomials. Although not used 
in regression analysis so often now, orthogonal vectors are useful in selecting 
settings in designed experiments. 
Vandermonde matrices also arise in the representation or approximation 
of a probability distribution in terms of its moments. 
The determinant of a square Vandermonde matrix has a particularly sim-
ple form (see Exercise  8.11).

420
8 Matrices with Special Properties
8.8.3 Hadamard Matrices and Orthogonal Arrays 
In a wide range of applications, including experimental design, cryptology, and 
other areas of combinatorics, we often encounter matrices whose elements are 
chosen from a set of only a few diﬀerent elements. In experimental design, 
the elements may correspond to the levels of the factors; in cryptology, they 
may represent the letters of an alphabet. In two-level factorial designs, the 
entries may be either 0 or 1. Matrices, all of whose entries are either 1 or 
−1, can represent the same layouts, and such matrices may have interesting 
mathematical properties. 
An n × n matrix with −1, 1 entries whose determinant is nn/2 is called a 
Hadamard matrix. Hadamard’s name is associated with this matrix because 
of the bound derived by Hadamard for the determinant of any matrix A 
with |aij| ≤ 1 for all i, j: |det(A)| ≤ nn/2 . A Hadamard matrix achieves this 
upper bound. A maximal determinant is often used as a criterion for a good 
experimental design. 
We often denote an n × n Hadamard matrix by Hn, which  is  the same no-
tation often used for a Helmert matrix, but in the case of Hadamard matrices, 
the matrix is not unique. All rows are orthogonal and so are all columns. The 
norm of each row or column is n, so  
upper H Subscript n Superscript normal upper T Baseline upper H Subscript n Baseline equals upper H Subscript n Baseline upper H Subscript n Superscript normal upper T Baseline equals n upper I periodHT
n Hn = HnHT
n = nI.
(8.89) 
It is clear that if Hn is a Hadamard matrix, then so is HT 
n , and a Hadamard 
matrix is a normal matrix (page 383). Symmetric Hadamard matrices are often 
of special interest. In a special type of n × n Hadamard matrix, one row and 
one column consist of all 1s; all n − 1 other rows and columns consist of n/2 
1s and n/2 −1s. Such a matrix is called a normalized Hadamard matrix. Most 
Hadamard matrices occurring in statistical applications are normalized, and 
also most have all 1s on the diagonal. 
A Hadamard matrix is often represented as a mosaic of black and white 
squares, as in Fig. 8.7. 
Figure 8.7. A 4  × 4 Hadamard matrix 
Hadamard matrices do not exist for all n. Clearly, n must be even because 
|Hn| = nn/2 , but some experimentation (or an exhaustive search) quickly 
shows that there is no Hadamard matrix for n = 6. It has been conjectured, 
but not proven, that Hadamard matrices exist for any n divisible by 4. Given 
any n × n Hadamard matrix, Hn, and  any  m × m Hadamard matrix, Hm, an

8.8 Other Matrices with Special Structures
421
nm × nm Hadamard  matrix  can be formed as a partitioned matrix in which  
each 1 in Hn is replaced by the block submatrix Hm and each −1 is replaced 
by the block submatrix −Hm. For example, the 4×4 Hadamard matrix shown 
in Fig. 8.7 is formed using the 2 × 2 Hadamard matrix 
Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column negative 1 2nd Row 1st Column 1 2nd Column 1 EndMatrix
⎡
1 −1
1
1
⎤
as both Hn and Hm. Since  an  H2 exists, this means that for any n = 2k , 
an n × n Hadamard matrix exists. Not all Hadamard matrices can be formed 
from other Hadamard matrices in this way, however; that is, they are not 
necessarily 2k × 2k . A Hadamard matrix exists for n = 12, for example. 
A related type of orthogonal matrix, called a conference matrix, is a hollow 
matrix Cn with −1, 1 entries on the oﬀ-diagonals, and such that 
upper C Subscript n Superscript normal upper T Baseline upper C Subscript n Baseline equals left parenthesis n minus 1 right parenthesis upper I periodCT
n Cn = (n −1)I.
(8.90) 
A conference matrix is said to be normalized if  the ﬁrst row  and the  ﬁrst  
column consist of all 1s, except for the (1, 1) element. Conference matrices 
arise in circuit design and other applications of graph theory. They are related 
to the adjacency matrices occurring in such applications. The (n−1)×(n−1) 
matrix formed by removing the ﬁrst row and ﬁrst column of a symmetric 
conference matrix is a Seidel adjacency matrix (page 372). 
A somewhat more general type of matrix corresponds to an n × m array 
with the elements in the jth column being members of a set of kj elements 
and such that, for some ﬁxed p ≤ m, in every  n × p submatrix all possible 
combinations of the elements of the m sets occur equally often as a row. 
(I make a distinction between the matrix and the array because often in 
applications the elements in the array are treated merely as symbols without 
the assumptions of an algebra of a ﬁeld. A terminology for orthogonal arrays 
has evolved that is diﬀerent from the terminology for matrices; for example, a 
symmetric orthogonal array is one in which k1 = · · ·  = km. On the other hand, 
treating the orthogonal arrays as matrices with real elements may provide 
solutions to combinatorial problems such as may arise in optimal design.) 
The 4 × 4 Hadamard matrix shown in Fig. 8.7 is a symmetric orthogonal 
array with k1 = · · ·  = k4 = 2  and  p =  4, so in the  array each of the  possible  
combinations of elements occurs exactly once. This array is a member of a 
simple class of symmetric orthogonal arrays that has the property that in any 
two rows each ordered pair of elements occurs exactly once. 
Orthogonal arrays are particularly useful in developing fractional factorial 
plans. (The robust designs of Taguchi correspond to orthogonal arrays.) 
8.8.4 Toeplitz Matrices 
A Toeplitz matrix is a square matrix with constant codiagonals:

422
8 Matrices with Special Properties
Start 5 By 5 Matrix 1st Row 1st Column d 2nd Column u 1 3rd Column u 2 4th Column midline horizontal ellipsis 5th Column u Subscript n minus 1 Baseline 2nd Row 1st Column l 1 2nd Column d 3rd Column u 1 4th Column midline horizontal ellipsis 5th Column u Subscript n minus 2 Baseline 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 4th Row 1st Column l Subscript n minus 2 Baseline 2nd Column l Subscript n minus 3 Baseline 3rd Column l Subscript n minus 4 Baseline 4th Column down right diagonal ellipsis 5th Column u 1 5th Row 1st Column l Subscript n minus 1 Baseline 2nd Column l Subscript n minus 2 Baseline 3rd Column l Subscript n minus 3 Baseline 4th Column midline horizontal ellipsis 5th Column d EndMatrix period
⎡
⎢⎢⎢⎢⎢⎢⎣
d
u1
u2 · · · un−1
l1
d
u1 · · · un−2
...
...
...
...
...
ln−2 ln−3 ln−4
...
u1
ln−1 ln−2 ln−3 · · ·
d
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(8.91) 
An n × n Toeplitz matrix is characterized by a diagonal element d and two 
(n − 1)-vectors, u and l, as indicated in the expression above. If u = 0,  the  
Toeplitz matrix is lower triangular; if l = 0, the matrix is upper triangular; and 
if u = l, the matrix is symmetric. A Toeplitz matrix is a banded matrix, but 
it may or may not be a “band matrix,” that is, one with many 0 codiagonals, 
which we discuss in Chaps. 3 and 12. 
Banded Toeplitz matrices arise frequently in time series studies. The 
covariance matrix in an ARMA(p, q) process, for example, is a symmetric 
Toeplitz matrix with 2 max(p, q) nonzero oﬀ-diagonal bands. See page 493 for 
an example and further discussion. 
Inverses of Certain Toeplitz Matrices and Other Banded Matrices 
A Toeplitz matrix that occurs often in stationary time series is the n × n 
variance-covariance matrix of the form 
upper V equals sigma squared Start 4 By 5 Matrix 1st Row 1st Column 1 2nd Column rho 3rd Column rho squared 4th Column midline horizontal ellipsis 5th Column rho Superscript n minus 1 Baseline 2nd Row 1st Column rho 2nd Column 1 3rd Column rho 4th Column midline horizontal ellipsis 5th Column rho Superscript n minus 2 Baseline 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column vertical ellipsis 5th Column vertical ellipsis 4th Row 1st Column rho Superscript n minus 1 Baseline 2nd Column rho Superscript n minus 2 Baseline 3rd Column rho Superscript n minus 3 Baseline 4th Column midline horizontal ellipsis 5th Column 1 EndMatrix periodV = σ2
⎡
⎢⎢⎢⎣
1
ρ
ρ2
· · · ρn−1
ρ
1
ρ
· · · ρn−2
...
...
...
...
...
ρn−1 ρn−2 ρn−3 · · ·
1
⎤
⎥⎥⎥⎦.
It is easy to see that V −1 exists if σ /= 0  and  |ρ| < 1 and that it is the type 2 
matrix 
upper V Superscript negative 1 Baseline equals StartFraction 1 Over left parenthesis 1 minus rho squared right parenthesis sigma squared EndFraction Start 5 By 5 Matrix 1st Row 1st Column 1 2nd Column negative rho 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 0 2nd Row 1st Column negative rho 2nd Column 1 plus rho squared 3rd Column negative rho 4th Column midline horizontal ellipsis 5th Column 0 3rd Row 1st Column 0 2nd Column negative rho 3rd Column 1 plus rho squared 4th Column midline horizontal ellipsis 5th Column 0 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column vertical ellipsis 5th Column vertical ellipsis 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 1 EndMatrix periodV −1 =
1
(1 −ρ2)σ2
⎡
⎢⎢⎢⎢⎢⎣
1
−ρ
0
· · · 0
−ρ 1 + ρ2
−ρ
· · · 0
0
−ρ
1 + ρ2 · · · 0
...
...
...
...
...
0
0
0
· · · 1
⎤
⎥⎥⎥⎥⎥⎦
.
(8.92) 
Type 2 matrices also occur as the inverses of other matrices with special 
patterns that arise in other common statistical applications (see Graybill 1983 
for examples). 
The inverses of all banded invertible matrices have some oﬀ-diagonal sub-
matrices that are zero or have low rank, depending on the bandwidth of the 
original matrix.

8.8 Other Matrices with Special Structures
423
8.8.5 Circulant Matrices 
A Toeplitz matrix having the form 
Start 5 By 6 Matrix 1st Row 1st Column c 1 2nd Column c 2 3rd Column c 3 4th Column midline horizontal ellipsis 5th Column c Subscript n minus 1 6th Column c Subscript n 2nd Row 1st Column c Subscript n 2nd Column c 1 3rd Column c 2 4th Column midline horizontal ellipsis 5th Column c Subscript n minus 2 6th Column c Subscript n minus 1 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 6th Column vertical ellipsis 4th Row 1st Column c 3 2nd Column c 4 3rd Column c 5 4th Column midline horizontal ellipsis 5th Column c 1 6th Column c 2 5th Row 1st Column c 2 2nd Column c 3 3rd Column c 4 4th Column midline horizontal ellipsis 5th Column c Subscript n 6th Column c 1 EndMatrix
⎡
⎢⎢⎢⎢⎢⎣
c1 c2 c3 · · · cn−1
cn
cn c1 c2 · · · cn−2 cn−1
...
...
... ...
...
...
c3 c4 c5 · · ·
c1
c2
c2 c3 c4 · · ·
cn
c1
⎤
⎥⎥⎥⎥⎥⎦
(8.93) 
is called a circulant matrix. Beginning with a ﬁxed ﬁrst row, each subsequent 
row is obtained by a right circular shift of the row just above it. 
A useful  n × n circulant matrix is the permutation matrix 
upper E Subscript left parenthesis n comma 1 comma 2 comma ellipsis comma n minus 1 right parenthesis Baseline equals product Underscript k equals 2 Overscript n Endscripts upper E Subscript k comma k minus 1 Baseline commaE(n,1,2,...,n−1) =
n
| |
k=2
Ek,k−1,
(8.94) 
which is the identity transformed by moving each row downward into the row 
below it and  the last row  into  the ﬁrst row. (See page  104.)  Let us denote this  
permutation as πc; hence, we denote the elementary circulant matrix in (8.94) 
as E(πc). 
By ﬁrst recalling that, for any permutation matrix, E−1 
(π) = ET 
(π), and then 
considering the eﬀects of the multiplications AE(π) and E(π)A, it is easy to  
see that A is circulant if and only if 
upper A equals upper E Subscript left parenthesis pi Sub Subscript normal c Subscript right parenthesis Baseline upper A upper E Subscript left parenthesis pi Sub Subscript normal c Subscript right parenthesis Superscript normal upper T Baseline periodA = E(πc)AET
(πc).
(8.95) 
Circulant matrices have several straightforward properties. If A is circu-
lant, then: 
• AT is circulant. 
• A2 is circulant. 
• If A is nonsingular, then A−1 is circulant. 
You are asked to prove these simple results in Exercise 8.13. 
Any linear combination of two circulant matrices of the same order is 
circulant (i.e., they form a vector space, see Exercise 8.14). 
If A and B are circulant matrices of the same order, then AB is circulant 
(Exercise 8.15). 
Another important property of a circulant matrix is that it is normal, as 
we can see by writing the (i, j) element  of  AAT and of AT A as a sum of 
products of elements of A (Exercise 8.16). This has an important implication: 
a circulant matrix is unitarily diagonalizable.

424
8 Matrices with Special Properties
8.8.6 Fourier Matrices and the Discrete Fourier Transform 
A special Vandermonde matrix (Eq. (8.88)) is an n × n matrix whose entries 
are the nth roots of unity, that is, {1, ω, ω2 , . . . , ωn−1}, where  
omega equals normal e Superscript 2 pi normal i divided by n Baseline equals cosine left parenthesis StartFraction 2 pi Over n EndFraction right parenthesis plus normal i sine left parenthesis StartFraction 2 pi Over n EndFraction right parenthesis periodω = e2πi/n = cos
(2π
n
)
+ i sin
(2π
n
)
.
(8.96) 
The matrix is called a Fourier matrix. The  (j, k) entry of a Fourier matrix is 
ω(j−1)(k−1) /√n: 
upper F Subscript n Baseline equals StartFraction 1 Over StartRoot n EndRoot EndFraction Start 5 By 6 Matrix 1st Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 5th Column midline horizontal ellipsis 6th Column 1 2nd Row 1st Column 1 2nd Column omega Superscript 1 Baseline 3rd Column omega squared 4th Column omega cubed 5th Column midline horizontal ellipsis 6th Column omega Superscript left parenthesis n minus 1 right parenthesis Baseline 3rd Row 1st Column 1 2nd Column omega squared 3rd Column omega Superscript 4 Baseline 4th Column omega Superscript 6 Baseline 5th Column midline horizontal ellipsis 6th Column omega Superscript 2 left parenthesis n minus 1 right parenthesis Baseline 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column vertical ellipsis 5th Column vertical ellipsis 6th Column vertical ellipsis 5th Row 1st Column 1 2nd Column omega Superscript n minus 1 Baseline 3rd Column omega Superscript 2 left parenthesis n minus 1 right parenthesis Baseline 4th Column omega Superscript 3 left parenthesis n minus 1 right parenthesis Baseline 5th Column midline horizontal ellipsis 6th Column omega Superscript left parenthesis n minus 1 right parenthesis left parenthesis n minus 1 right parenthesis Baseline EndMatrix periodFn =
1
√n
⎡
⎢⎢⎢⎢⎢⎣
1
1
1
1
· · ·
1
1
ω1
ω2
ω3
· · ·
ω(n−1)
1
ω2
ω4
ω6
· · ·
ω2(n−1)
...
...
...
...
...
...
1 ωn−1 ω2(n−1) ω3(n−1) · · · ω(n−1)(n−1)
⎤
⎥⎥⎥⎥⎥⎦
.
(8.97) 
(The Fourier matrix is sometimes deﬁned with a negative sign in the expo-
nents; that is, such that the (j, k)th entry is ω−(j−1)(k−1) /√n. The normal-
izing factor 1/√n is also sometimes omitted. In fact, in many applications 
of Fourier matrices and various Fourier forms, there is inconsequential, but 
possibly annoying, variation in the notation.) 
Notice that the Fourier matrix is symmetric, and any entry raised to the 
nth power is 1. Although the Fourier matrix is symmetric, its eigenvalues are 
not necessarily real, because it itself is not a real matrix. 
Fourier matrices whose order is a power of 2 tend to have a propensity of 
elements that are either ±1 or  ±i. For example, the 4 × 4 Fourier matrix is 
upper F 4 equals one half Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 2nd Row 1st Column 1 2nd Column normal i 3rd Column negative 1 4th Column negative normal i 3rd Row 1st Column 1 2nd Column negative 1 3rd Column 1 4th Column negative 1 4th Row 1st Column 1 2nd Column negative normal i 3rd Column negative 1 4th Column normal i EndMatrix periodF4 = 1
2
⎡
⎢⎢⎣
1
1
1
1
1
i −1 −i
1 −1
1 −1
1 −i −1
i
⎤
⎥⎥⎦.
(Recall that there are diﬀerent deﬁnitions of the elements of a Fourier matrix; 
for the 4 × 4, they all are as shown above, but the patterns of positive and 
negative values may be diﬀerent. Most of the elements of the 8 × 8 Fourier  
matrix are either ±1 or  ±i.  The 16 that are  not are  ±1/
√
2 ± i/
√
2. 
The Fourier matrix has many useful properties, such as being unitary; 
that is, row and columns are orthonormal: f H 
∗jf∗k = f H 
j∗fk∗ = 0  for  j /= k, and  
f H 
∗jf∗j = 1  (Exercise  8.17). 
The most interesting feature of the Fourier matrix is its relationship to the 
Fourier transform. For an integrable function f(x), the Fourier transform is 
script upper F f left parenthesis s right parenthesis equals integral Subscript negative normal infinity Superscript normal infinity Baseline normal e Superscript minus 2 pi normal i s x Baseline f left parenthesis x right parenthesis normal d x periodFf(s) =
f ∞
−∞
e−2πisxf(x) dx.
(8.98) 
(Note that the characteristic function in probability theory is this same trans-
form applied to a probability density function, with argument t = −2πs.)

8.8 Other Matrices with Special Structures
425
Fourier Matrices and Elementary Circulant Matrices 
The Fourier matrix and the elementary circulant matrix E(πc) of correspond-
ing order are closely related. Being a normal matrix, E(πc) is unitarily diago-
nalizable, and the Fourier matrix and its conjugate transpose are the diago-
nalizing matrices, and the diagonal matrix itself, that is, the eigenvalues, are 
elements of the Fourier matrix: 
upper E Subscript left parenthesis pi Sub Subscript normal c Subscript right parenthesis Baseline equals upper F Subscript n Superscript normal upper H Baseline normal d normal i normal a normal g left parenthesis left parenthesis 1 comma omega comma omega squared comma ellipsis comma omega Superscript n minus 1 Baseline right parenthesis right parenthesis upper F Subscript n Baseline periodE(πc) = F H
n diag((1, ω, ω2, . . . , ωn−1))Fn.
(8.99) 
This is easy to see by performing the multiplications on the right side of the 
equation, and you are asked to do this in Exercise 8.18. 
We see that the eigenvalues of E(πc) are what we would expect if we con-
tinued the development from page 159 where we determined the eigenvalues 
of an elementary permutation matrix. (An elementary permutation matrix of 
order 2 has the 2 eigenvalues 
√
1 and  −
√
1.) 
Notice that the modulus of all eigenvalues of E(πc) is the same, 1. Hence, 
all eigenvalues of E(πc) lie on its spectral circle. 
The Discrete Fourier  Transform  
Fourier transforms are invertible transformations of functions that often al-
low operations on those functions to be performed more easily. The Fourier 
transform shown in Eq. (8.98) may allow for simpler expressions of convo-
lutions, for example. An n-vector is equivalent to a function whose domain 
is {1, . . . , n}, and Fourier transforms of vectors are useful in various opera-
tions on the vectors. More importantly, if the vector represents observational 
data, certain properties of the data may become immediately apparent in the 
Fourier transform of the data vector. 
The Fourier transform, at a given value s, of the  function  as  shown in  
Eq. (8.98) is an integral. The argument of the transform s may or may not 
range over the same domain as x the argument of the function. The analogue 
of the Fourier transform of a vector is a sum, again at some given value, which 
is eﬀectively the argument of the transform. For an n-vector x, the discrete 
Fourier transform at n points is 
script upper F x equals upper F Subscript n Baseline x periodFx = Fnx.
(8.100) 
Transformations of this form are widely used in time series, where the vector x 
contains observations at equally spaced points in time. While the elements of 
x represent measurements at distinct points in time, the elements of the vector 
Fx represent values at diﬀerent points in the period of a sine and/or a cosine 
curve, as we see from the relation of the roots of unity given in Eq. (8.96). Many 
time series, especially those relating to measurement of waves propagating 
through matter or of vibrating objects, exhibit periodicities, which can be 
used to distinguish diﬀerent kinds of waves and possibly to locate their source.

426
8 Matrices with Special Properties
Many waves and other time series have multiple periodicities, at diﬀerent 
frequencies. The Fourier transform is sometimes called a frequency ﬁlter. 
Our purpose here is just to indicate the relation of Fourier transforms to 
matrices and to suggest how this might be useful in applications. There is a 
wealth of literature on Fourier transforms and their applications, but we will 
not pursue those topics here. 
As often as when writing expressions involving matrices, we must empha-
size that the form of a mathematical expression and the way the expression 
should be evaluated in actual practice may be quite diﬀerent. This is particu-
larly true in the case of the discrete Fourier transform. There would never be 
a reason to form the Fourier matrix for any computations, but more impor-
tantly, rather than evaluating the elements in the Fourier transform Fx using 
the right side of Eq. (8.100), we take advantage of properties of the powers 
of roots of unity to arrive at a faster method of computing them – so much 
faster, in fact, the method is called the “fast” Fourier transform, or FFT. The 
FFT is one of the most important computational methods in all of applied 
mathematics. I will not discuss it further here, however. Descriptions of it can 
be found throughout the literature on computational science (including other 
books that I have written). 
An Aside: Complex Matrices 
The Fourier matrix, I believe, is the only matrix I discuss in this book 
that has complex entries. The purpose is just to relate the discrete 
Fourier transform to matrix multiplication. We have already indicated 
various diﬀerences in operations on vectors and matrices over the com-
plex plane. We often form the conjugate of a complex number, which 
we denote by an overbar: ¯z. The conjugate of an object such as a vec-
tor or a matrix is the result of taking the conjugate of each element 
individually. Instead of a transpose, we usually work with a conjugate 
transpose, AH = AT . (See the discussion on pages 43 and 78.) We 
deﬁne the inner product of vectors x and y as <x, y> = ¯xT y; instead 
of symmetric matrices, we focus on Hermitian matrices, that is, ones 
such that the conjugate transpose is the same as the original matrices, 
and instead of orthogonal matrices, we focus on unitary matrices, that 
is, ones whose product with its conjugate transpose is the identity. All 
of the general results that we have stated for real matrices of course 
hold for complex matrices. Some of the analogous operations, how-
ever, have diﬀerent properties. For example, the property of a matrix 
being unitarily diagonalizable is an analogue of the property of being 
orthogonally diagonalizable, but, as we have seen, a wider class of ma-
trices (normal matrices) are unitarily diagonalizable than those that 
are orthogonally diagonalizable (symmetric matrices). 
Most scientiﬁc software systems support computations with complex 
numbers. Both R and MATLAB, for example, provide full capabili-
ties for working with complex numbers. The imaginary unit in both is

8.8 Other Matrices with Special Structures
427
denoted by “i,” which of course must be distinguished from “i” de-
noting a variable. In R, the function complex can be used to initialize  
a complex number; for example, 
z<-complex(re=3,im=2) 
assigns the value 3+2i to the variable z. (Another simple way of doing 
this is to juxtapose a numeric literal in front of the symbol i; in R, for  
example, z<-3+2i assigns the same value to z. I do not recommend the 
latter construct because of possible confusion with other expressions.) 
The jth row of an nth order Fourier matrix can be generated by the 
R expression 
c(1,exp(2*pi*complex(im=seq(j,j*(n-1),j)/n))) 
As mentioned above, there would almost never be a reason to form 
the Fourier matrix for any computations. It is instructive, however, to 
note its form and to do some simple manipulations with the Fourier 
matrix in R or some other software system. The MATLAB function 
dftmtx generates a Fourier matrix (with a slightly diﬀerent deﬁnition, 
resulting in a diﬀerent pattern of positives and negatives than what I 
have shown above). 
8.8.7 Hankel Matrices 
A Hankel matrix is a square matrix with constant “anti”-codiagonals: 
Start 5 By 5 Matrix 1st Row 1st Column u Subscript n minus 1 Baseline 2nd Column u Subscript n minus 2 Baseline 3rd Column midline horizontal ellipsis 4th Column u 1 5th Column d 2nd Row 1st Column u Subscript n minus 2 Baseline 2nd Column u Subscript n minus 3 Baseline 3rd Column midline horizontal ellipsis 4th Column d 5th Column l 1 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column vertical ellipsis 5th Column vertical ellipsis 4th Row 1st Column u 1 2nd Column d 3rd Column l 1 4th Column down right diagonal ellipsis 5th Column l Subscript n minus 2 Baseline 5th Row 1st Column d 2nd Column l 1 3rd Column l 2 4th Column midline horizontal ellipsis 5th Column l Subscript n minus 1 Baseline EndMatrix period
⎡
⎢⎢⎢⎢⎢⎢⎣
un−1 un−2 · · · u1
d
un−2 un−3 · · · d
l1
...
...
...
...
...
u1
d
l1
... ln−2
d
l1
l2 · · · ln−1
⎤
⎥⎥⎥⎥⎥⎥⎦
.
(8.101) 
Notice that a Hankel matrix is similar to a Toeplitz matrix, except that the 
axial diagonal is not the principal diagonal; rather, for an n × n Hankel ma-
trix H, it is the skew diagonal, or “anti-diagonal,” consisting of the elements 
h1,n, h2,n−1, . . . , hn,1. Although this “diagonal” is visually apparent, and sym-
metries about it are intuitively obvious, there is no commonly used term for 
this diagonal or for symmetries about it. We can also observe analogues of 
lower triangular, upper triangular, and symmetric matrices based on u = 0,  
l = 0,  and  u = l; but these are not the kinds of matrices that we have identi-
ﬁed with these terms. Words such as “anti” and “skew” are sometimes used 
to qualify names of these properties or objects. A triangular counterpart is 
sometimes called “skew diagonal.” There is no standard term for symmetry 
about the skew diagonal, however. The term “skew symmetric” would be an 
obvious choice, and is sometimes used, but that term has already been taken. 
(It is an antisymmetric matrix A, with  aij = −aji.)

428
8 Matrices with Special Properties
As with a Toeplitz matrix, a Hankel matrix is characterized by a “diago-
nal” element and two vectors and can be generalized to n × m matrices based 
on vectors of diﬀerent orders. As in the expression (8.101) above, an  n × n 
Toeplitz matrix can be deﬁned by a scalar d and two (n − 1)-vectors, u and 
l. There are other, perhaps more common, ways of putting the elements of a 
Hankel matrix into two vectors. In MATLAB, the function hankel produces 
a Hankel matrix, but the elements are speciﬁed in a diﬀerent way from that 
in expression (8.101). 
A common form of Hankel matrix is an n×n skew upper triangular matrix, 
and it is formed from the u vector only. The simplest form of the square skew 
upper triangular Hankel matrix is formed from the vector u = (n − 1, n  − 
2, . . . , 1) and d = n: 
Start 4 By 5 Matrix 1st Row 1st Column 1 2nd Column 2 3rd Column 3 4th Column midline horizontal ellipsis 5th Column n 2nd Row 1st Column 2 2nd Column 3 3rd Column 4 4th Column midline horizontal ellipsis 5th Column 0 3rd Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column midline horizontal ellipsis 5th Column vertical ellipsis 4th Row 1st Column n 2nd Column 0 3rd Column 0 4th Column midline horizontal ellipsis 5th Column 0 EndMatrix period
⎡
⎢⎢⎢⎣
1 2 3 · · · n
2 3 4 · · · 0
...
...
... · · ·
...
n 0 0 · · · 0
⎤
⎥⎥⎥⎦.
(8.102) 
A skew upper triangular Hankel matrix occurs in the spectral analysis of 
time series. If x(t) is a (discrete) time series, for t = 0, 1, 2, . . ., the Hankel 
matrix of the time series has as the (i, j) element  
StartLayout 1st Row 1st Column x left parenthesis i plus j minus 2 right parenthesis 2nd Column normal i normal f i plus j minus 1 less than or equals n comma 2nd Row 1st Column 0 2nd Column Blank 3rd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e period EndLayoutx(i + j −2)
if i + j −1 ≤n,
0
otherwise.
The L2 norm of the Hankel matrix of the time series is called the Hankel norm 
of the frequency ﬁlter response (the Fourier transform). 
8.8.8 Cauchy Matrices 
Another type of special n × m matrix whose elements are determined by a 
few n-vectors and m-vectors is a Cauchy-type matrix. The standard Cauchy 
matrix is built from two vectors, x and y. The more general form deﬁned 
below uses two additional vectors. 
A Cauchy matrix is an n × m matrix C(x, y, v, w) generated by n-vectors 
x and v and m-vectors y and w of the form 
upper C left parenthesis x comma y comma v comma w right parenthesis equals Start 3 By 3 Matrix 1st Row 1st Column StartFraction v 1 w 1 Over x 1 minus y 1 EndFraction 2nd Column midline horizontal ellipsis 3rd Column StartFraction v 1 w Subscript m Baseline Over x 1 minus y Subscript m Baseline EndFraction 2nd Row 1st Column vertical ellipsis 2nd Column midline horizontal ellipsis 3rd Column vertical ellipsis 3rd Row 1st Column StartFraction v Subscript n Baseline w 1 Over x Subscript n Baseline minus y 1 EndFraction 2nd Column midline horizontal ellipsis 3rd Column StartFraction v Subscript n Baseline w Subscript m Baseline Over x Subscript n Baseline minus y Subscript m Baseline EndFraction EndMatrix periodC(x, y, v, w) =
⎡
⎢⎢⎢⎣
v1w1
x1 −y1
· · ·
v1wm
x1 −ym
...
· · ·
...
vnw1
xn −y1
· · ·
vnwm
xn −ym
⎤
⎥⎥⎥⎦.
(8.103) 
Cauchy-type matrices often arise in the numerical solution of partial dif-
ferential equations (PDEs). For Cauchy matrices, the order of the number of 
computations for factorization or solutions of linear systems can be reduced 
from a power of 3 to a power of 2. This is a very signiﬁcant improvement for

8.8 Other Matrices with Special Structures
429
large matrices. In the PDE applications, the matrices are generally not large, 
but nevertheless, even in those applications, it is worthwhile to use algorithms 
that take advantage of the special structure. 
8.8.9 Matrices Useful in Graph Theory 
Many problems in statistics and applied mathematics can be posed as graphs, 
and various methods of graph theory can be used in their solution. 
Graph theory is particularly useful in cluster analysis or classiﬁcation. 
These involve the analysis of relationships of objects for the purpose of iden-
tifying similar groups of objects. The objects are associated with vertices of 
the graph, and an edge is generated if the relationship (measured somehow) 
between two objects is suﬃciently great. For example, suppose the question of 
interest is the authorship of some text documents. Each document is a vertex, 
and an edge between two vertices exists if there are enough words in common 
between the two documents. A similar application could be the determination 
of which computer user is associated with a given computer session. The ver-
tices would correspond to login sessions, and the edges would be established 
based on the commonality of programs invoked or ﬁles accessed. In applica-
tions such as these, there would typically be a training dataset consisting of 
text documents with known authors or consisting of session logs with known 
users. In both of these types of applications, decisions would have to be made 
about the extent of commonality of words, phrases, programs invoked, or ﬁles 
accessed in order to establish an edge between two documents or sessions. 
Unfortunately, as is often the case for an area of mathematics or statistics 
that developed from applications in diverse areas or through the eﬀorts of ap-
plied mathematicians somewhat outside of the mainstream of mathematics, 
there are major inconsistencies in the notation and terminology employed in 
graph theory. Thus, we often ﬁnd diﬀerent terms for the same object; for ex-
ample, adjacency matrix and connectivity matrix. This unpleasant situation, 
however, is not so disagreeable as a one-to-many inconsistency, such as the 
designation of the eigenvalues of a graph to be the eigenvalues of one type 
of matrix in some of the literature and the eigenvalues of diﬀerent types of 
matrices in other literature. 
Refer to Sect. 8.1.2 beginning on page 369 for terms and notation that we 
will use in the following discussion. As mentioned earlier, many of the matrices 
occurring in applications with graphical models are sparse. 
Adjacency Matrix: Connectivity Matrix 
We discussed adjacency or connectivity matrices on page 372. A matrix, such  
as an adjacency matrix, that consists of only 1s and 0s is called a Boolean 
matrix. 
Two vertices that are not connected and hence correspond to a 0 in a  
connectivity matrix are said to be independent.

430
8 Matrices with Special Properties
If no edges connect a vertex with itself, the adjacency matrix is a hollow 
matrix. 
Because the 1s in a connectivity matrix indicate a strong association, and 
we would naturally think of a vertex as having a strong association with 
itself, we sometimes modify the connectivity matrix so as to have 1s along the 
diagonal. Such a matrix is sometimes called an augmented connectivity matrix 
or augmented adjacency matrix. 
The eigenvalues of the adjacency matrix reveal some interesting properties 
of the graph and are sometimes called the eigenvalues of the graph. The eigen-
values of another matrix, which we discuss below, are more useful, however, 
and we will refer to those eigenvalues as the eigenvalues of the graph. 
Digraphs 
The digraph represented in Fig. 8.4 on page 373 is a network with ﬁve vertices, 
perhaps representing cities, and directed edges between some of the vertices. 
The edges could represent airline connections between the cities; for example, 
there are ﬂights from x to u and from u to x, and  from  y to z, but not from 
z to y. 
In a digraph, the relationships are directional. (An example of a directional 
relationship that might be of interest is when each observational unit has a 
diﬀerent number of measured features, and a relationship exists from vi to vj 
if a majority of the features of vi are identical to measured features of vj.) 
Use of the Connectivity Matrix 
The analysis of a network may begin by identifying which vertices are con-
nected with others, that is, by construction of the connectivity matrix. 
The connectivity matrix can then be used to analyze other levels of as-
sociation among the data represented by the graph or digraph. For example, 
from the connectivity matrix in Eq. (8.2) on page 373, we have  
upper A squared equals Start 5 By 5 Matrix 1st Row 1st Column 4 2nd Column 1 3rd Column 0 4th Column 0 5th Column 1 2nd Row 1st Column 0 2nd Column 1 3rd Column 1 4th Column 1 5th Column 1 3rd Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 5th Column 2 4th Row 1st Column 1 2nd Column 2 3rd Column 1 4th Column 1 5th Column 1 5th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 1 5th Column 1 EndMatrix periodA2 =
⎡
⎢⎢⎢⎢⎣
4 1 0 0 1
0 1 1 1 1
1 1 1 1 2
1 2 1 1 1
1 1 1 1 1
⎤
⎥⎥⎥⎥⎦
.
In terms of the application suggested on page 373 for airline connections, 
the matrix A2 represents the number of connections between the cities that 
consist of exactly two ﬂights. From A2 we see that there are two ways to go 
from city y to city w in just two ﬂights but only one way to go from w to y 
in two ﬂights. 
This property extends to multiple connections. If A is the adjacency matrix 
of a graph, then Ak 
ij is the number of paths of length k between nodes i and

8.8 Other Matrices with Special Structures
431
j in that graph. (See Exercise 8.20.) Important areas of application of this 
fact are in DNA sequence comparisons and measuring “centrality” of a node 
within a complex social network. 
The Laplacian Matrix of a Graph 
Spectral graph theory is concerned with the analysis of the eigenvalues of a 
graph. As mentioned above, there are two diﬀerent deﬁnitions of the eigenval-
ues of a graph. The more useful deﬁnition, and the one we use here, takes the 
eigenvalues of a graph to be the eigenvalues of a matrix, called the Laplacian 
matrix, formed from the adjacency matrix and a diagonal matrix consisting 
of the degrees of the vertices. 
Given the graph G, let  D(G) be a diagonal matrix consisting of the degrees 
of the vertices of G (i.e., D(G) = diag(d(G))) and let C(G) be the adjacency 
matrix of G. If there are no isolated vertices (i.e., if d(G) > 0), then the 
Laplacian matrix of the graph L(G) is given  by  
upper L left parenthesis script upper G right parenthesis equals upper I minus upper D left parenthesis script upper G right parenthesis Superscript negative one half Baseline upper C left parenthesis script upper G right parenthesis upper D left parenthesis script upper G right parenthesis Superscript negative one half Baseline periodL(G) = I −D(G)−1
2 C(G)D(G)−1
2 .
(8.104) 
Some authors deﬁne the Laplacian in other ways: 
upper L Subscript a Baseline left parenthesis script upper G right parenthesis equals upper I minus upper D left parenthesis script upper G right parenthesis Superscript negative 1 Baseline upper C left parenthesis script upper G right parenthesisLa(G) = I −D(G)−1C(G)
(8.105) 
or 
upper L Subscript b Baseline left parenthesis script upper G right parenthesis equals upper D left parenthesis script upper G right parenthesis minus upper C left parenthesis script upper G right parenthesis periodLb(G) = D(G) −C(G).
(8.106) 
The eigenvalues of the Laplacian matrix are the eigenvalues of a graph. The  
deﬁnition of the Laplacian matrix given in Eq. (8.104) seems to be more useful  
in terms of bounds on the eigenvalues of the graph. The set of unique eigen-
values (the spectrum of the matrix L) is called the spectrum of the graph. 
So long as d(G) > 0, L(G) =  D(G)−1 
2 La(G)D(G)−1 
2 . Unless the graph 
is regular, the matrix Lb(G) is not  symmetric. Note that if  G is k-regular, 
L(G) =  I − C(G)/k, and  Lb(G) =  L(G). 
For a digraph, the degrees are replaced by either the indegrees or the 
outdegrees. (Some authors deﬁne it one way and others the other way. The 
essential properties hold either way.) 
The Laplacian can be viewed as an operator on the space of functions 
f : V (G) →IR such that for the vertex v 
upper L left parenthesis f left parenthesis v right parenthesis right parenthesis equals StartFraction 1 Over StartRoot d Subscript v Baseline EndRoot EndFraction sigma summation Underscript w comma w tilde v Endscripts left parenthesis StartFraction f left parenthesis v right parenthesis Over StartRoot d Subscript v Baseline EndRoot EndFraction minus StartFraction f left parenthesis w right parenthesis Over StartRoot d Subscript w Baseline EndRoot EndFraction right parenthesis commaL(f(v)) =
1
√dv
E
w,w∼v
(f(v)
√dv
−f(w)
√dw
)
,
where w ∼ v means vertices w and v that are adjacent and du is the degree 
of the vertex u. 
For a symmetric graph, the Laplacian matrix is symmetric, so its eigenval-
ues are all real. We can see that the eigenvalues are all nonnegative by forming

432
8 Matrices with Special Properties
the Rayleigh quotient (Eq. (3.291)) using an arbitrary vector g, which  can be  
viewed as a real-valued function over the vertices, 
StartLayout 1st Row 1st Column upper R Subscript upper L Baseline left parenthesis g right parenthesis 2nd Column equals 3rd Column StartFraction left angle bracket g comma upper L g right angle bracket Over left angle bracket g comma g right angle bracket EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction left angle bracket g comma upper D Superscript negative one half Baseline upper L Subscript a Baseline upper D Superscript negative one half Baseline g right angle bracket Over left angle bracket g comma g right angle bracket EndFraction 3rd Row 1st Column Blank 2nd Column equals 3rd Column StartFraction left angle bracket f comma upper L Subscript a Baseline f right angle bracket Over left angle bracket upper D Superscript one half Baseline f comma upper D Superscript one half Baseline f right angle bracket EndFraction 4th Row 1st Column Blank 2nd Column equals 3rd Column StartFraction sigma summation Underscript v tilde w Endscripts left parenthesis f left parenthesis v right parenthesis minus f left parenthesis w right parenthesis right parenthesis squared Over f Superscript normal upper T Baseline upper D f EndFraction comma EndLayoutRL(g) = <g, Lg>
<g, g>
= <g, D−1
2 LaD−1
2 g>
<g, g>
=
<f, Laf>
<D
1
2 f, D
1
2 f>
=
E
v∼w(f(v) −f(w))2
f TDf
,
(8.107) 
where f = D−1 
2 g and f(u) is the element of the vector corresponding to 
vertex u. Because the Rayleigh quotient is nonnegative, all eigenvalues are 
nonnegative, and because there is an f /= 0 for which the Rayleigh quotient is 
0, we see that 0 is an eigenvalue of a graph. Furthermore, using the Cauchy-
Schwartz inequality, we see that the spectral radius is less than or equal to 
2. 
The eigenvalues of a matrix are the basic objects in spectral graph theory. 
They provide information about the properties of networks and other systems 
modeled by graphs. We will not explore these relations further here. 
If G is the graph represented in Fig. 8.2 on page 370, with  V (G) =  
{a, b, c, d, e}, the degrees of the vertices of the graph are d(G) = (4, 2, 2, 3, 3). 
Using the adjacency matrix given in Eq. (8.1), we have 
upper L left parenthesis script upper G right parenthesis equals Start 9 By 5 Matrix 1st Row 1st Column 1 2nd Column minus StartFraction StartRoot 2 EndRoot Over 4 EndFraction 3rd Column minus StartFraction StartRoot 2 EndRoot Over 4 EndFraction 4th Column minus StartFraction StartRoot 3 EndRoot Over 6 EndFraction 5th Column minus StartFraction StartRoot 3 EndRoot Over 6 EndFraction 2nd Row 1st Column Blank 3rd Row 1st Column minus StartFraction StartRoot 2 EndRoot Over 4 EndFraction 2nd Column 1 3rd Column 0 4th Column 0 5th Column minus StartFraction StartRoot 6 EndRoot Over 6 EndFraction 4th Row 1st Column Blank 5th Row 1st Column minus StartFraction StartRoot 2 EndRoot Over 4 EndFraction 2nd Column 0 3rd Column 1 4th Column minus StartFraction StartRoot 6 EndRoot Over 6 EndFraction 5th Column 0 6th Row 1st Column Blank 7th Row 1st Column minus StartFraction StartRoot 3 EndRoot Over 6 EndFraction 2nd Column 0 3rd Column minus StartFraction StartRoot 6 EndRoot Over 6 EndFraction 4th Column 1 5th Column negative one third 8th Row 1st Column Blank 9th Row 1st Column minus StartFraction StartRoot 3 EndRoot Over 6 EndFraction 2nd Column minus StartFraction StartRoot 6 EndRoot Over 6 EndFraction 3rd Column 0 4th Column negative one third 5th Column 1 EndMatrix periodL(G) =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 −
√
2
4 −
√
2
4 −
√
3
6 −
√
3
6
−
√
2
4
1
0
0 −
√
6
6
−
√
2
4
0
1 −
√
6
6
0
−
√
3
6
0 −
√
6
6
1
−1
3
−
√
3
6 −
√
6
6
0
−1
3
1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
.
(8.108) 
This matrix is singular, and the unnormalized eigenvector corresponding to 
the 0 eigenvalue is (2
√
14, 2
√
7, 2
√
7,
√
42,
√
42). 
8.8.10 Z-Matrices and M-Matrices 
In certain applications in physics and in the solution of systems of nonlinear 
diﬀerential equations, a class of matrices called M-matrices is important. 
The matrices in these applications have nonpositive oﬀ-diagonal elements. 
A square matrix, all of whose oﬀ-diagonal elements are nonpositive, is called 
a Z-matrix.

Exercises
433
A Z-matrix that is positive stable (see page 186) is called an M-matrix. A 
real symmetric M-matrix is positive deﬁnite. 
In addition to the properties that constitute the deﬁnition, M-matrices 
have a number of remarkable properties, which we state here without proof. 
If A is a real M-matrix, then 
• All principal minors of A are positive. 
• All diagonal elements of A are positive. 
• All diagonal elements of L and U in the LU decomposition of A are posi-
tive. 
• A is nonsingular and A−1 ≥ 0. 
Proofs of some of these facts can be found in Horn and Johnson (1991). 
Exercises 
8.1. Normal matrices. 
a) Show that a skew symmetric matrix is normal. 
b) Show that a skew Hermitian matrix is normal. 
8.2. Ordering of nonnegative deﬁnite matrices. 
a) A relation >< on a set is a partial ordering if, for elements a, b, and  
c, 
• 
It is reﬂexive: a >< a. 
• 
It is antisymmetric: a >< b >< a  =⇒ a = b. 
• 
It is transitive: a >< b >< c  =⇒ a >< c. 
Show that the relation > (Eq. (8.19)) is a partial ordering. 
b) Show that the relation > (Eq. (8.20)) is transitive. 
8.3. Let X be an n × m matrix with n > m  and with entries sampled 
independently from a continuous distribution (of a real-valued random 
variable). What is the probability that XT X is positive deﬁnite? 
8.4. a) Show that a (strictly) diagonally dominant symmetric matrix is pos-
itive deﬁnite. 
b) Show that if the real n × n symmetric matrix A is such that 
a Subscript i i Baseline greater than or equals sigma summation Underscript j not equals i Overscript n Endscripts StartAbsoluteValue a Subscript i j Baseline EndAbsoluteValue for each i equals 1 comma ellipsis comma naii ≥
n
E
j/=i
|aij|
for each i = 1, . . . , n
then A > 0. 
8.5. Show that the number of positive eigenvalues of an idempotent matrix 
is the rank of the matrix. 
8.6. Show that two idempotent matrices of the same rank are similar. 
8.7. Under the given conditions, show that properties (a) and (b) on 
page 395 imply property (c). 
8.8. Projections.

434
8 Matrices with Special Properties
a) Show that the matrix given in Eq. (8.42) (page 397) is a projection 
matrix. 
b) Write out the projection matrix for projecting a vector onto the 
plane formed by two vectors, x1 and x2, as indicated on page 397, 
and show that it is the same as the hat matrix of Eq. (8.52). 
8.9. Use the relationship (8.77) to prove properties 1 and 4 on page 414. 
8.10. Leslie matrices. 
a) Write the characteristic polynomial of the Leslie matrix, Eq. (8.85). 
b) Show that the Leslie matrix has a single, unique positive eigenvalue. 
8.11. Write out the determinant for an n × n Vandermonde matrix. 
Hint: The determinant of an n×n Vandermonde matrix as in Eq. (8.88) 
is (xn − x1)(xn − x2) · · · (xn − xn−1) times the determinant of the n − 
1 × n − 1 Vandermonde matrix formed by removing the last row and 
column. Show this by multiplying the original Vandermonde matrix by 
B = I + D, where  D is the matrix with 0s in all positions except for 
the ﬁrst supradiagonal, which consists of −xn, replicated n − 1 times.  
Clearly, the determinant of B is 1. 
8.12. Consider the 3 × 3 symmetric Toeplitz matrix with elements a, b, and  
c, that is, the matrix that looks like this: 
Start 3 By 3 Matrix 1st Row 1st Column 1 2nd Column b 3rd Column c 2nd Row 1st Column b 2nd Column 1 3rd Column b 3rd Row 1st Column c 2nd Column b 3rd Column 1 EndMatrix period
⎡
⎣
1 b c
b 1 b
c b 1
⎤
⎦.
a) Invert this matrix. 
See page 422. 
b) Determine conditions for which the matrix would be singular. 
8.13. If A is circulant, show that 
• 
AT is circulant. 
• 
A2 is circulant. 
• 
If A is nonsingular, then A−1 is circulant. 
Hint: Use Eq. (8.95). 
8.14. Show that the set of all n × n circulant matrices is a vector space along 
with the axpy operation. (Just show that it is closed with respect to 
that operation.) 
8.15. If A and B are circulant matrices of the same order, show AB is circu-
lant. 
8.16. Show that a circulant matrix is normal. 
8.17. Show that a Fourier matrix, as in Eq. (8.97), is unitary by showing 
f H 
∗jf∗k = f H 
j∗fk∗ = 0  for  j /= k, and  f H 
∗jf∗j = 1.  
8.18. Show that Eq. (8.99) is correct by performing the multiplications on the 
right side of the equation. 
8.19. Write out the determinant for the n × n skew upper triangular Hankel 
matrix in (8.102). 
8.20. Graphs. Let A be the adjacency matrix of an undirected graph.

Exercises
435
a) Show that A2 
ij is the number of paths of length 2 between nodes i 
and j. 
Hint: Construct a general diagram similar to Fig. 8.2 on page 370, 
and count the paths between two arbitrary nodes. 
b) Show that Ak 
ij is the number of paths of length k between nodes i 
and j. 
Hint: Use Exercise 8.20a and mathematical induction on k.

9 
Selected Applications in Statistics 
In this chapter, we consider some applications in statistics that use vectors and 
matrices. Vectors and matrices correspond naturally to the standard organi-
zation of statistical data, as discussed in Sect. 8.1.1. They also work nicely  
for storing statistical data in other structures, as discussed in Sects. 8.1.2 
and 8.1.3. Statistical data may sometimes be characterized by a temporal 
pattern, and again, vectors and matrices are well-suited to represent such 
relationships. 
The objective of this chapter is to illustrate how some of the properties of 
matrices and vectors that were covered in previous chapters relate to statistical 
models and to data analysis procedures. The ﬁeld of statistics is far too large 
for a single chapter on “applications” to cover more than just a small part 
of the area. Similarly, the topics covered previously are too extensive to give 
examples of applications of all of them. 
Several of the results in this chapter have already been presented in Chap. 8 
or even in previous chapters; for example, the smoothing matrix Hλ that we 
discuss in Sect. 9.1.9 has already been encountered on page 401 in Chap. 8. The  
purpose of the apparent redundancy is to present the results from a diﬀerent 
perspective. (None of the results are new; all are standard in the statistical 
literature.) 
Many statistical applications, including ﬁtting of models, experimental 
design, and construction of sampling procedures, involve optimization of a 
function. In Sect. 9.6 in this chapter, drawing heavily on Sect. 7.2, we discuss  
various applications that require optimization of some function. 
The appendix beginning on page 510 brieﬂy discusses R functions useful in 
various statistical applications. The matrix capabilities of R discussed in the 
appendix to Chap. 3, beginning on page 201, and the R functions for matrix 
operations discussed in the appendix to Chap. 4, beginning on page 251, may  
be useful in following some of the developments for analysis of linear models. 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 9 
437

438
9 Selected Applications in Statistics
Structure in Data and Statistical Data Analysis 
A common logical structure of statistical data is a two-dimensional array in 
which columns correspond to variables or measurable attributes and rows 
correspond to an observation on all attributes taken together. A matrix is 
obviously a convenient object for representing numeric data organized this 
way. An objective in analyzing data of this form is to uncover relationships 
among the variables or to characterize the distribution of the sample over 
IRm . Interesting relationships and patterns are called “structure” in the data. 
This is a diﬀerent meaning from that of the word used in the phrase “logical 
structure” or in the phrase “data structure” used in computer science. The 
special matrices discussed in Chap. 8 play an important role in this chapter. 
9.1 Linear Models 
Some of the most important applications of statistics involve the study of 
the relationship of one variable, often called a “response variable,” to other 
variables. The response variable is usually modeled as a random variable, 
which we often indicate by using a capital letter. A general model for the 
relationship of a variable, Y , to other variables, x (a vector), is 
upper Y almost equals f left parenthesis x right parenthesis periodY ≈f(x).
(9.1) 
In this asymmetric model and others like it, we call the response Y the depen-
dent variable and the elements of x the independent variables. Models such  
as these are called regression models. Here, I write Y in upper case and x in 
lower cases because we will treat Y as a random variable, but take x to be 
nonrandom. These distinctions are rarely of practical importance, and after 
having made the point, I will rarely use case to distinguish the types of the 
variables. 
We often introduce an adjustment or “error term” so as to make an equa-
tion out of expression (9.1), and because the functional form f of the rela-
tionship between Y and x may contain a parameter, we may  write the  model  
as 
upper Y equals f left parenthesis x semicolon theta right parenthesis plus upper E periodY = f(x; θ) + E.
(9.2) 
The adjustment term can have various interpretations. In statistical inference 
based on a linear model, it is considered to be a random variable, and its 
probability distribution is the basis for inference. In this form, the model 
consists of a systematic component expressing the relationship and an additive 
adjustment component or “additive error.” 
In a linear model the relationship between the dependent and independent 
variables can be represented as 
upper Y equals beta Superscript normal upper T Baseline f left parenthesis x right parenthesis plus upper E commaY = βTf(x) + E,
(9.3)

9.1 Linear Models
439
where the vector parameter β may contain unknown values and f(x) is a  
vector of functions of the independent variables that does not contain any 
nonobservable values. A model such as this is called a linear regression model. 
There are many useful variations of the model (9.3) that express various kinds 
of relationships between the response variable and the other variables. 
Note that “linear” refers to the form of β in the model; the multivariate 
f(x) may be nonlinear. 
Often in applications f(x) is just a vector of the independent values them-
selves, 
f left parenthesis x right parenthesis equals left parenthesis x 1 comma ellipsis comma x Subscript m Baseline right parenthesis commaf(x) = (x1, . . . , xm),
or, more commonly, 
f left parenthesis x right parenthesis equals left parenthesis 1 comma x 1 comma ellipsis comma x Subscript m Baseline right parenthesis commaf(x) = (1, x1, . . . , xm),
so that the linear model contains an intercept. In either case, we write the 
model as 
upper Y equals beta Superscript normal upper T Baseline x plus upper E commaY = βTx + E,
(9.4) 
which expresses the systematic component as a linear combination of the xs 
using the vector parameter β. 
The ambiguity between linear regression models with or without an inter-
cept will bedevil our subsequent discussions. In applications, it is likely that 
an intercept should be included in the model, but then the problem is that “x” 
contains a constant in addition to the independent variables. I will generally 
try to be clear about the intercept term, but the reader must be aware of the 
possible ambiguity. 
Notation 
I generally represent random variable in uppercase. In Eq. (9.4), for example, 
“E” is an uppercase epsilon representing a random variable. The lowercase “e” 
represents a realization, though an unobservable one, of the random variable. 
I will not always follow this convention, however; sometimes it is convenient 
to use the language more loosely and to speak of ei as a random variable. 
In data analysis with regression models, we have a set of observations 
{yi, xi}, where  xi is an m-vector. One of the primary tasks is to determine 
a reasonable value of the parameter. That is, in the linear regression model, 
for example, we think of β as an unknown variable (rather than as a ﬁxed 
constant or a realization of a random variable), and we want to ﬁnd a value 
of it such that the model ﬁts the observations well, 
y Subscript i Baseline equals beta Superscript normal upper T Baseline x Subscript i Baseline plus epsilon Subscript i Baseline commayi = βTxi + ei,
(9.5) 
where β and xi are m-vectors. (Note here the ambiguity alluded to above: 
does xi include a constant 1?) 
Given the observations {yi, xi}, we can represent the regression model and 
the data as

440
9 Selected Applications in Statistics
y equals upper X beta plus epsilon commay = Xβ + e,
(9.6) 
where X is the n × m matrix whose rows are the xis and e is the vector of 
deviations (“errors”) of the observations from the functional model. 
We assume that n > m, but we do not necessarily assume that rank(X) =  
m; that is, the model may not be of full rank. Whether or not the model is 
full rank, we will often refer to a “least squares ﬁt,” “least squares solution,” 
or “least squares estimator.” These are all based on the normal equations 
and the matrix (XT X)−1 , (XT X)+ , or (XT X)− (although, remember, we 
rarely actually compute those matrices). To facilitate the discussions, we will 
generally use the same notation throughout: 
•
-β is a solution to the normal equations based on any generalized inverse 
(XT X)− or any g12 inverse (XT X)∗. It is not necessarily unique. 
•
-β is the solution to the normal equations based on either the inverse in 
the case of a full-rank model or the Moore-Penrose inverse (XT X)+ . It is  
unique. 
We will often use the Moore-Penrose inverse even if the model is full rank. In 
that case of course the Moore-Penrose inverse is the inverse. 
We will occasionally refer to submatrices of the basic data matrix X using 
notation developed in Chap. 3. For example, X(i1,...,ik)(j1,...,jl) refers to the 
k × l matrix formed by retaining only the i1, . . . , ik rows and the j1, . . . , jl 
columns of X, and  X−(i1,...,ik)(j1,...,jl) refers to the matrix formed by deleting 
the i1, . . . , ik rows and the j1, . . . , jl columns of X. 
We also use the notation xi∗ to refer to the ith row of X (the row is a vector, 
a column vector), and x∗j to refer to the jth column of X. See page 663 for a 
summary of this notation. 
Statistical Inference 
The linear model is useful in any situations in which variables of interest, 
the dependent variables, take on values close to a hyperplane in a vector 
space endowed with a Cartesian coordinate system scaled with values of the 
independent variables. 
In many applications, the error term E can be ignored. Assuming E is 
small, we have 
upper Y almost equals beta Superscript normal upper T Baseline x periodY ≈βTx.
(9.7) 
Given some observed data, the objective may be just to ﬁt the model, that is, 
to determine a value of β such that the model (9.7) ﬁts the data fairly well. 
Fitting the model is the topic of this section. This is done without any 
further consideration of the error term E or e. 
The statistical problem is to  estimate elements of β. Estimation is mechan-
ically similar to ﬁtting, but it also involves some statement about the uncer-
tainty of the estimator and this depends on assumptions about the probability 
distribution of the random variable E or e.

9.1 Linear Models
441
We will discuss statistical inference in the linear model in Sect. 9.2, begin-
ning on page 458. 
9.1.1 Fitting the Model 
In this section, although we may occasionally refer to the error term as a 
random variable, we will concentrate on the algebraic aspects of ﬁtting the 
model without any assumptions about the probability distribution of the error 
term. 
In a model for a given dataset as in Eq. (9.6), although the errors are no 
longer random variables (they are realizations of random variables), they are 
not observable. 
To ﬁt the model, we replace the unknown constants with variables: β with 
b and e with r. While we usually refer to elements of as “errors,” we call the 
elements of r “residuals.” For any given value of b, all elements of r are known. 
The model with variables to be determined is 
y equals upper X b plus r periody = Xb + r.
(9.8) 
We then proceed by applying a chosen criterion for ﬁtting. 
In ﬁtting the model, we focus on the residuals r = y − Xb. An intuitive 
approach is to choose an appropriate norm on r and determine a value of b 
that minimizes that norm. This should yield a model equation ˆy = Xb, in  
which, for any X, ˆy is “close” to the observed y. 
Notice that we are not here posing a question of statistical inference, in 
which case we would estimate elements of β, and then necessarily go further 
to consider the statistical properties of the estimators. Our emphasis here is 
just on methods that minimize a norm on r. 
Ordinary Least Squares 
The r vector contains the distances of the observations on y from the values 
of the variable y deﬁned by the hyperplane bT x, measured  in the direction of 
the y axis. The objective is to determine a value of b that minimizes some 
norm of r. The use of the L2 norm is called “least squares.” The least squares 
ﬁt is the b that minimizes the dot product 
left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis equals sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus x Subscript i asterisk Superscript normal upper T Baseline b right parenthesis squared period(y −Xb)T(y −Xb) =
n
E
i=1
(yi −xT
i∗b)2.
(9.9) 
As we saw in Sect. 5.6 (where we used slightly diﬀerent notation), using 
elementary calculus to determine the minimum of Eq. (9.9) yields the “normal 
equations” 
upper X Superscript normal upper T Baseline upper X b equals upper X Superscript normal upper T Baseline y periodXTXb = XTy.
(9.10)

442
9 Selected Applications in Statistics
If the model includes an intercept, instead of X, a centered matrix Xc, as  
in Eq. (8.64) on page 403, is often used in the normal equations. 
The normal equations are useful in deriving properties of statistics that 
are relevant for least squares ﬁts, but it is usually better not to use the normal 
equations in computations for linear model ﬁtting. The condition number of 
XT X is the square of the condition number of X, however, and so any ill-
conditioning is exacerbated by the formation of the sums of squares and cross-
products matrix. The adjusted sums of squares and cross-products matrix, 
XT 
c Xc, tend to be better conditioned, so it is usually the one used in the 
normal equations, but of course the condition number of XT 
c Xc is the square 
of the condition number of Xc (see Sect. 5.1.1, beginning on page 261). 
Weighted Least Squares 
The elements of the residual vector may be weighted diﬀerently. This is ap-
propriate if, for instance, the variance of the error depends on the value of x; 
that is, in the notation of Eq. (9.3), V(E) =  g(x), where g is some function. If 
the function is known, we can address the problem almost identically as in the 
use of ordinary least squares, as we saw on page 289. Weighted least squares 
may also be appropriate if the observations in the sample are not indepen-
dent. In this case also, if we know the variance-covariance structure, after a 
simple transformation, we can use ordinary least squares. If the function g 
or the variance-covariance structure must be estimated, the ﬁtting problem is 
still straightforward, but formidable complications are introduced into other 
aspects of statistical inference. We discuss weighted least squares further in 
Sect. 9.1.7. 
Variations on the Criteria for Fitting 
Rather than minimizing a norm of r, there are many other approaches we could 
use to ﬁt the model to the data. Of course, just the choice of the norm yields 
diﬀerent approaches. Some of these approaches may depend on distributional 
assumptions, which we will not consider here. The point that we want to 
emphasize here, with little additional comment, is that the standard approach 
to regression modeling is not the only one. We mentioned some of these other 
approaches and the computational methods of dealing with them in Sect. 5.7. 
The ﬁts based on alternative criteria may be more “robust” or more resistant 
to the eﬀects of anomalous data or heavy-tailed probability distributions. 
Regularized Fits 
Some variations on the basic approach of minimizing residuals involve a kind of 
regularization that may take the form of an additive penalty on the objective 
function.Regularization often results in a shrinkage of the estimates toward 0.

9.1 Linear Models
443
One of the most common types of shrinkage estimate is the ridge regression 
estimate, which for the model y = Xβ + e is the solution of the modiﬁed 
normal equations (XT X + λI)b = XT y. We discuss this further in Sect. 9.4.4. 
Orthogonal Distances 
Another approach is to deﬁne an optimal value of β as one that minimizes a 
norm of the distances of the observed values of y from the vector Xβ. This is  
sometimes called “orthogonal distance regression.” The use of the L2 norm on 
this vector is sometimes called “total least squares.” This is a reasonable ap-
proach when it is assumed that the observations in X are realizations of some 
random variable; that is, an “errors-in-variables” model is appropriate. The 
model in Eq. (9.6) is modiﬁed to consist of two error terms: one for the errors 
in the variables and one for the error in the equation. The methods discussed 
in Sect. 5.7.4 can be used to ﬁt a model using a criterion of minimum  norm  
of orthogonal residuals. As we mentioned there, weighting of the orthogonal 
residuals can be easily accomplished in the usual way of handling weights on 
the diﬀerent observations. 
The weighting matrix often is formed as an inverse of a variance-covariance 
matrix Σ; hence, the modiﬁcation is to premultiply the matrix [X|y] in  
Eq. (5.56) by the Cholesky factor Σ−1 
C . In the case of errors-in-variables, how-
ever, there may be another variance-covariance structure to account for. If the 
variance-covariance matrix of the columns of X (i.e., the independent vari-
ables) together with y is T, then we handle the weighting for variances and 
covariances of the columns of X in the same way, except of course we postmul-
tiply the matrix [X|y] in Eq. (5.56) by  T −1 
C . This matrix is (m + 1) × (m + 1);  
however, it may be appropriate to assume any error in y is already accounted 
for, and so the last row and column of T may be 0 except for the (m+1, m+1) 
element, which would be 1. The appropriate model depends on the nature of 
the data, of course. 
Collinearity 
A major problem in regression analysis is collinearity (or “multicollinearity”), 
by which we mean a “near singularity” of the X matrix. This can be made 
more precise in terms of a condition number, as discussed in Sect. 5.1. Ill-
conditioning may not only present computational problems, but also may 
result in a least squares ﬁt with a very large variance. 
9.1.2 Least Squares Fit of Full-Rank Models 
One of the most commonly used models in statistical applications is the linear 
regression model, 
y Subscript i Baseline equals beta 0 plus beta 1 x Subscript 1 i Baseline plus midline horizontal ellipsis plus beta Subscript m plus 1 Baseline x Subscript m plus 1 comma i Baseline plus epsilon Subscript i Baseline commayi = β0 + β1x1i + · · · + βm+1xm+1,i + ei,

444
9 Selected Applications in Statistics
which, given data, results in the vector/matrix model y = Xβ +e in Eq. (9.6), 
after some adjustments of subscripts and proper interpretation of the inter-
cept, as we have mentioned. If X is of full column rank, then (XT X)−1 exists 
and -β = (XT X)−1 XT y is the unique least squares ﬁt of the model. 
If E(e) = 0, then 
normal upper E left parenthesis ModifyingAbove beta With caret right parenthesis equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline normal upper E left parenthesis y right parenthesis equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline upper X beta equals beta commaE
(
-β
)
= (XTX)−1XTE (y) = (XTX)−1XTXβ = β,
(9.11) 
and this is true for each element of β: E(-βj) =  βj. 
In this case, we will refer to -β as an “estimate” or an “estimator” of β. 
In the case of nonfull-rank models, we do not estimate β, but rather linear 
combinations of β (see Sect. 9.2.2). 
9.1.3 Least Squares Fits of Nonfull-Rank Models 
In a model not of full rank, (XT X)−1 does not exist. Instead we use some 
generalized inverse (XT X)− to ﬁt the model. Some relevant quantities are 
invariant to the choice of the generalized inverse, but the ﬁtted value of β, 
XT X)−1 XT y depends on the generalized inverses used. Perhaps surprisingly, 
one quantity that is invariant is the vector of residuals, r = y − Xb, for any 
value of b = (XT X)−XT y, that is, which is a solution to the normal equa-
tions. We see this from Eq. (8.48) on page 399 that says that X(XT X)−XT is 
invariant to choice of generalized inverse. In Sect. 9.2.2, we will identify some 
other quantities that invariant to choice of generalized inverse. 
Another model related to the regression model is the classiﬁcation model 
or “analysis of variance” model of the general form, 
y Subscript j ellipsis k i Baseline equals mu plus alpha Subscript j Baseline plus midline horizontal ellipsis plus beta Subscript k Baseline plus left bracket normal e normal t normal c normal e normal t normal e normal r normal a right bracket plus epsilon Subscript j ellipsis k i Baseline commayj...ki = μ + αj + · · · + βk + [etcetera] + ej...ki,
(9.12) 
where “μ” is some overall mean and “α” represents the diﬀerential eﬀects due 
to some class of treatments and “αj” speciﬁcally is the amount by which y 
changes when the jth level of the  “α” treatment is applied. This is a nonfull-
rank model. 
Let us consider a very simple instance of the model (9.12) to illustrate least 
squares ﬁts, properties of generalized estimators, and an important aspect of 
statistical inference, which we will pick up again in Sect. 9.2.2 on page 462. 
A Classiﬁcation Model: Numerical Example 
Consider the “two factor without interaction” model, 
y Subscript j k i Baseline equals mu plus alpha Subscript j Baseline plus gamma Subscript k Baseline plus epsilon Subscript j k i Baseline commayjki = μ + αj + γk + ejki,
(9.13) 
where j = 1, 2 and  k = 1, 2, and for each combination of j and k there are 
two observations. This model has ﬁve parameters that correspond to the βs

9.1 Linear Models
445
in the regression model, μ, α1, α2, γ1, γ2. Suppose the “truth” is that μ = 2,  
α1 = 3,  α2 = 1,  γ1 = 1,  and  γ2 = 4; the observed values of y may be similar 
to what is shown below. (These correspond to artiﬁcially generated numbers, 
but they were not actually observed in a real-world situation.) After arranging 
the “observations” in a systematic order, we may record the data as follows: 
y equals Start 8 By 1 Matrix 1st Row 6.6 2nd Row 6.9 3rd Row 8.8 4th Row 9.9 5th Row 4.2 6th Row 5.5 7th Row 7.3 8th Row 6.3 EndMatrix upper X equals Start 8 By 5 Matrix 1st Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 2nd Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 3rd Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 0 5th Column 1 4th Row 1st Column 1 2nd Column 1 3rd Column 0 4th Column 0 5th Column 1 5th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 1 5th Column 0 6th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 1 5th Column 0 7th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 8th Row 1st Column 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 EndMatrix upper X Superscript normal upper T Baseline upper X equals Start 5 By 5 Matrix 1st Row 1st Column 8 2nd Column 4 3rd Column 4 4th Column 4 5th Column 4 2nd Row 1st Column 4 2nd Column 4 3rd Column 0 4th Column 2 5th Column 2 3rd Row 1st Column 4 2nd Column 0 3rd Column 4 4th Column 2 5th Column 2 4th Row 1st Column 4 2nd Column 2 3rd Column 2 4th Column 4 5th Column 0 5th Row 1st Column 4 2nd Column 2 3rd Column 2 4th Column 0 5th Column 4 EndMatrix upper X Superscript normal upper T Baseline y equals Start 5 By 1 Matrix 1st Row 55.5 2nd Row 32.2 3rd Row 23.3 4th Row 23.2 5th Row 32.3 EndMatrix periody =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
6.6
6.9
8.8
9.9
4.2
5.5
7.3
6.3
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
X =
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
1 1 0 1 0
1 1 0 1 0
1 1 0 0 1
1 1 0 0 1
1 0 1 1 0
1 0 1 1 0
1 0 1 0 1
1 0 1 0 1
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
XTX =
⎡
⎢⎢⎢⎢⎣
8 4 4 4 4
4 4 0 2 2
4 0 4 2 2
4 2 2 4 0
4 2 2 0 4
⎤
⎥⎥⎥⎥⎦
XTy =
⎡
⎢⎢⎢⎢⎣
55.5
32.2
23.3
23.2
32.3
⎤
⎥⎥⎥⎥⎦
. (9.14) 
In the context of a linear model of the form y = Xβ + e, a least squares ﬁt is  
given by 
beta overTilde equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y comma-β = (XTX)−XTy,
(9.15) 
where (XT X)− is any generalized inverse of XT X. 
Fitting the Model Using Generalized Inverses 
As we have emphasized, we would probably never compute the generalized 
inverses, but I have computed some for the matrices in the toy example in 
Eq. (9.14). In Eq. (9.16) X∗ and (XT X)∗ are g12 generalized inverses com-
puted by Gaussian elimination, skipping over pivot values that are very small 
in absolute value. 
upper X Superscript asterisk Baseline equals Start 5 By 8 Matrix 1st Row 1st Column negative 1 2nd Column 0 3rd Column 1 4th Column 0 5th Column 1 6th Column 0 7th Column 0 8th Column 0 2nd Row 1st Column 1 2nd Column 0 3rd Column 0 4th Column 0 5th Column negative 1 6th Column 0 7th Column 0 8th Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 6th Column 0 7th Column 0 8th Column 0 4th Row 1st Column 1 2nd Column 0 3rd Column negative 1 4th Column 0 5th Column 0 6th Column 0 7th Column 0 8th Column 0 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 0 5th Column 0 6th Column 0 7th Column 0 8th Column 0 EndMatrix left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript asterisk Baseline equals Start 5 By 5 Matrix 1st Row 1st Column 0.375 2nd Column negative 0.25 3rd Column 0 4th Column negative 0.25 5th Column 0 2nd Row 1st Column negative 0.250 2nd Column 0.50 3rd Column 0 4th Column 0.00 5th Column 0 3rd Row 1st Column 0.000 2nd Column 0.00 3rd Column 0 4th Column 0.00 5th Column 0 4th Row 1st Column negative 0.250 2nd Column 0.00 3rd Column 0 4th Column 0.50 5th Column 0 5th Row 1st Column 0.000 2nd Column 0.00 3rd Column 0 4th Column 0.00 5th Column 0 EndMatrix periodX∗=
⎡
⎢⎢⎢⎢⎣
−1
0
1
0
1
0
0
0
1
0
0
0 −1
0
0
0
0
0
0
0
0
0
0
0
1
0 −1
0
0
0
0
0
0
0
0
0
0
0
0
0
⎤
⎥⎥⎥⎥⎦
(XTX)∗=
⎡
⎢⎢⎢⎢⎣
0.375 −0.25
0 −0.25
0
−0.250
0.50
0
0.00
0
0.000
0.00
0
0.00
0
−0.250
0.00
0
0.50
0
0.000
0.00
0
0.00
0
⎤
⎥⎥⎥⎥⎦
.
(9.16) 
The Moore-Penrose inverse of X is shown in Eq. (9.17). 
upper X Superscript plus Baseline equals Start 5 By 8 Matrix 1st Row 1st Column 0.06250 2nd Column 0.06250 3rd Column 0.06250 4th Column 0.06250 5th Column 0.06250 6th Column 0.06250 7th Column 0.06250 8th Column 0.06250 2nd Row 1st Column 0.15625 2nd Column 0.15625 3rd Column 0.15625 4th Column 0.15625 5th Column negative 0.09375 6th Column negative 0.09375 7th Column negative 0.09375 8th Column negative 0.09375 3rd Row 1st Column negative 0.09375 2nd Column negative 0.09375 3rd Column negative 0.09375 4th Column negative 0.09375 5th Column 0.15625 6th Column 0.15625 7th Column 0.15625 8th Column 0.15625 4th Row 1st Column 0.15625 2nd Column 0.15625 3rd Column negative 0.09375 4th Column negative 0.09375 5th Column 0.15625 6th Column 0.15625 7th Column negative 0.09375 8th Column negative 0.09375 5th Row 1st Column negative 0.09375 2nd Column negative 0.09375 3rd Column 0.15625 4th Column 0.15625 5th Column negative 0.09375 6th Column negative 0.09375 7th Column 0.15625 8th Column 0.15625 EndMatrix periodX+ =
⎡
⎢⎢⎢⎣
0.06250
0.06250
0.06250
0.06250
0.06250
0.06250
0.06250
0.06250
0.15625
0.15625
0.15625
0.15625 −0.09375 −0.09375 −0.09375 −0.09375
−0.09375 −0.09375 −0.09375 −0.09375
0.15625
0.15625
0.15625
0.15625
0.15625
0.15625 −0.09375 −0.09375
0.15625
0.15625 −0.09375 −0.09375
−0.09375 −0.09375
0.15625
0.15625 −0.09375 −0.09375
0.15625
0.15625
⎤
⎥⎥⎥⎦.
(9.17) 
Solutions to the normal equations are given by (XT X)∗XT y, 
by (XT X)+ XT y, and  by  X+ y (but not by X∗y). As we have shown (page 288), 
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y equals upper X Superscript plus Baseline y period(XTX)+XTy = X+y.

446
9 Selected Applications in Statistics
(The latter expression, of course, indicates how a solution to the normal equa-
tions should be computed; neither XT X nor XT y needs to be formed.)  
Use of the g12 generalized inverse shown above, rounded to two decimal 
places, yields 
beta overTilde equals left parenthesis 6.96 comma 2.23 comma 0.00 comma negative 2.28 comma 0.00 right parenthesis comma-β = (6.96, 2.23, 0.00, −2.28, 0.00),
and use of the Moore-Penrose inverse (again, rounded) yields 
ModifyingAbove beta With caret equals left parenthesis 3.47 comma 2.85 comma 0.62 comma 0.60 comma 2.87 right parenthesis period-β = (3.47, 2.85, 0.62, 0.60, 2.87).
The actual value of β used to generate this toy dataset is 
beta equals left parenthesis 2 comma 3 comma 1 comma 1 comma 4 right parenthesis periodβ = (2, 3, 1, 1, 4).
The two widely diﬀering solutions -β and -β indicate that we do not have the 
unbiased property that we have for the full-rank model in Eq. (9.11). That is 
indeed the case, and, in fact, we do not refer to -β and -β as “estimates.” We 
will return to this issue in Sect. 9.2.2. 
While -β and -β do not appear similar, they are both solutions to the least 
squares optimization problem 
min Underscript b Endscripts parallel to y minus upper X b parallel to Subscript 2 Baseline periodmin
b
||y −Xb||2.
Not only that, but all residuals are the same for both solutions: 
StartLayout 1st Row 1st Column r 2nd Column equals y minus upper X beta overTilde equals y minus upper X ModifyingAbove beta With caret 2nd Row 1st Column Blank 2nd Column equals left parenthesis negative 0.3125 comma negative 0.0125 comma negative 0.3875 comma 0.7125 comma negative 0.4875 comma 0.8125 comma 0.3375 comma negative 0.6625 right parenthesis comma EndLayoutr = y −X -β = y −X -β
= (−0.3125, −0.0125, −0.3875, 0.7125, −0.4875, 0.8125, 0.3375, −0.6625),
and, of course, in both cases, XT r = 0  (Eq. (5.37), page 285). 
We will return to this example on page 464. 
Uniqueness 
If X is not of full rank, the generalized inverse is not unique, so the solution 
in Eq. (9.15) is not unique. We will, nevertheless, often refer to it as “the” 
solution. We have seen that two very diﬀerent generalized inverses yield ex-
actly the same residuals. This is true, in fact, for any generalized inverse, as is 
easily seen. We will also see in Sect. 9.2 that most of the relevant statistics for 
inference in the linear model are invariant to the choice of generalized inverse. 
If X is not of full rank, we often use the unique Moore-Penrose inverse, 
(XT X)+ , in Eq. (9.15). Whether or not X is of full rank, a unique choice for 
β is 
ModifyingAbove beta With caret equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y period-β = (XTX)+XTy.
(9.18) 
Use of the Moore-Penrose inverse in the case of nonfull rank not only yields a 
unique solution, but -β in that case has the minimum L2-norm of all solutions 
given in Eq. (9.15) (see page 288).

9.1 Linear Models
447
As we saw in Eqs. (5.43) and  (5.44), we also have 
ModifyingAbove beta With caret equals upper X Superscript plus Baseline y period-β = X+y.
(9.19) 
On page 287, we derived this least squares solution by use of the QR decom-
position of X, and in Exercises 5.3a and 5.3b we mentioned two other ways 
to derive this important expression. 
9.1.4 Computing the Solution 
Most computational methods operate directly on the X matrix. Despite the 
possible consequent numerical problems, many computational approaches be-
gin with the formation of the Gramian matrix XT X that is the coeﬃcient 
matrix in the normal equations. In the case of a classiﬁcation model, rarely 
would an “X” matrix as in the example in Sect. 9.1.2 be formed. 
Direct Computations on X 
Equation (9.19) indicates an appropriate general way to compute -β. As we  
have seen many times before, however, we often use an expression without 
computing the individual terms. Instead of computing X+ in Eq. (9.19) ex-
plicitly, we use either Householder or Givens transformations to obtain the 
orthogonal decomposition 
upper X equals upper Q upper R commaX = QR,
or 
upper X equals upper Q upper R upper U Superscript normal upper TX = QRU T
if X is not of full rank. As we have seen, the QR decomposition of X can be 
performed row-wise using Givens transformations. This is especially useful if 
the data are available only one observation at a time. The equation used for 
computing -β is 
upper R ModifyingAbove beta With caret equals upper Q Superscript normal upper T Baseline y commaR-β = QTy,
(9.20) 
which can be solved by back-substitution in the triangular matrix R. 
Because 
upper X Superscript normal upper T Baseline upper X equals upper R Superscript normal upper T Baseline upper R commaXTX = RTR,
the quantities in XT X or its inverse, which are useful for making inferences 
using the regression model, can be obtained from the QR decomposition. 
The Normal Equations and the Sweep Operator 
The coeﬃcient matrix in the normal equations, XT X, or the adjusted version 
XT 
c Xc, where  Xc is the centered matrix as in Eq. (8.64) on page 403, is often 
of interest for reasons other than just to compute the least squares ﬁt.

448
9 Selected Applications in Statistics
A useful matrix can be formed from the normal equations by adjoining 
the m-vector XT y and the inner product yT y to the m × m matrix XT X: 
upper A equals Start 2 By 2 Matrix 1st Row 1st Column upper X Superscript normal upper T Baseline upper X 2nd Column upper X Superscript normal upper T Baseline y 2nd Row 1st Column y Superscript normal upper T Baseline upper X 2nd Column y Superscript normal upper T Baseline y EndMatrix periodA =
⎡XTX XTy
yTX yTy
⎤
.
(9.21) 
Applying m elementary operations on this matrix, we can get 
upper B equals Start 2 By 2 Matrix 1st Row 1st Column left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline 2nd Column upper X Superscript plus Baseline y 2nd Row 1st Column y Superscript normal upper T Baseline upper X Superscript plus normal upper T Baseline 2nd Column y Superscript normal upper T Baseline y minus y Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y EndMatrix periodB =
⎡(XTX)+
X+y
yTX+T yTy −yTX(XTX)+XTy
⎤
.
(9.22) 
If X is not of full rank, some elementary operations are just skipped, and a 
generalized inverse will be obtained, but in order to get the Moore-Penrose 
inverse in this expression, the elementary operations must be applied in a 
ﬁxed manner; otherwise, we get a diﬀerent generalized inverse. 
The matrix in the upper left of the partition (9.22) is related to the es-
timated variance-covariance matrix of the particular solution of the normal 
equations, and it can be used to get an estimate of the variance-covariance 
matrix of estimates of any independent set of linearly estimable functions of 
β. The vector in the upper right of the partition is the unique minimum-
length solution to the normal equations, -β. The scalar in the lower right 
partition, which is the Schur complement of the full inverse (see Eqs. (3.204) 
and (3.236)), is the square of the residual norm. The squared residual norm 
provides an estimate of the variance of the errors in Eq. (9.6) after proper 
scaling. 
The elementary operations can be grouped into a larger operation, called 
a “sweep operation,” which is performed for a given row. The sweep operation 
on row i, Si, of the nonnegative deﬁnite matrix A to yield the matrix B, which  
we denote by 
normal upper S Subscript i Baseline left parenthesis upper A right parenthesis equals upper B commaSi(A) = B,
is deﬁned in Algorithm 9.1. 
Algorithm 9.1 Gaussian Sweep of the ith Row 
1. If aii = 0, skip the following operations. 
2. Set bii = a−1 
ii . 
3. For j /= i, set  bij = a−1 
ii aij. 
4. For k /= i, set  bkj = akj − akia−1 
ii aij. 
(A sweep operation more generally applies a linear operation to the rows or 
columns of a matrix. The R function sweep implements this general opera-
tion.) 
The Gaussian sweep operator is its own inverse: 
normal upper S Subscript i Baseline left parenthesis normal upper S Subscript i Baseline left parenthesis upper A right parenthesis right parenthesis equals upper A periodSi(Si(A)) = A.

9.1 Linear Models
449
The Gaussian sweep operator applied to the matrix (9.21) corresponds to 
adding or removing the ith variable (column) of the X matrix to the regression 
equation. 
Computations for Analysis of Variance 
The main objective in analyzing data in a classiﬁcation model is often just 
to determine whether the variation among the observed responses among dif-
ferent treatments is signiﬁcantly greater than the variation within observa-
tions that receive the same treatment (hence, the name “analysis of variance” 
model). Although a classiﬁcation model can be formulated in the form of a 
general linear model y = Xβ + e, as we did in the numerical example in 
Sect. 9.1.3, the relevant computations can be performed more eﬃciently by 
direct formation of the sums of squared diﬀerences from the mean responses 
within the various classes. In any event, although solutions to the normal 
equations yield unique residuals, they are only directly relevant in linear com-
binations that are estimable, in the sense that we deﬁne in Sect. 9.2.2. 
Although computations in classiﬁcation models may not follow the pattern 
of computations for full-rank linear models, analysis of classiﬁcation models 
from the standpoint of a general linear model yields insights into their statis-
tical properties. 
9.1.5 Properties of a Least Squares Fit 
Whether or not the least squares ﬁt is unique, the ﬁts have several interesting 
properties in common. 
Geometrical Properties 
The vector -y = X -β is the projection of the n-vector y onto a space of dimen-
sion equal to the (column) rank of X, which we denote by rX. The vector 
of the model, E(Y ) =  Xβ, is also in the  rX-dimensional space span(X). The 
projection matrix I −X(XT X)+ XT projects y onto an (n −rX)-dimensional 
residual space that is orthogonal to span(X). Figure 9.1 represents these sub-
spaces and the vectors in them. 
Recall from page 285 that orthogonality of the residuals to span(X) is not  
only a property of a least squares solution, it actually characterizes a least 
squares solution; that is, if b is such that XT (y − Xb) = 0, then b is a least 
squares solution using some generalized inverse of XT X. 
In the (rX + 1)-order vector space of the variables, the hyperplane de-
ﬁned by -βT x is the ﬁtted model (assuming -β /= 0; otherwise, the space is of 
order rX).

450
9 Selected Applications in Statistics
Figure 9.1. The linear least squares ﬁt of y with X 
Degrees of Freedom 
In general, the vector y can range freely over an n-dimensional space. We say 
the degree of freedom of y, or the  total degrees of freedom, is  n. If we ﬁx the  
mean of y, then the adjusted total degree of freedom is n − 1. 
The model Xβ can range over a space with dimension equal to the (col-
umn) rank of X, that is,  rX. We say that the model degree of freedom is rX. 
Note that the space of X -β is the same as the space of Xβ. 
Finally, the space orthogonal to X -β (i.e., the space of the residuals y−X -β) 
has dimension n−rX. We say that the residual (or error) degree of freedom is 
n − rX. (Note that the error vector e can range over an n-dimensional space, 
but because -β is a least squares ﬁt, the residual vector, y−X -β, can only range 
over an (n − rX)-dimensional space.) 
The Hat Matrix and Leverage 
The projection matrix H = X(XT X)+ XT is sometimes called the “hat ma-
trix” because 
StartLayout 1st Row 1st Column ModifyingAbove y With caret 2nd Column equals 3rd Column upper X ModifyingAbove beta With caret 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y 3rd Row 1st Column Blank 2nd Column equals 3rd Column upper H y comma EndLayout-y = X -β
= X(XTX)+XTy
= Hy,
(9.23) 
that is, it projects y onto -y in the span of X. Notice that the hat matrix can 
be computed without knowledge of the observations in y.

9.1 Linear Models
451
The elements of H are useful in assessing the eﬀect of the particular pattern 
of the regressors on the predicted values of the response. The extent to which 
a given point in the row space of X aﬀects the regression ﬁt is called its 
“leverage.” The leverage of the ith observation is 
h Subscript i i Baseline equals x Subscript i asterisk Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline x Subscript i asterisk Baseline periodhii = xT
i∗(XTX)+xi∗.
(9.24) 
This is just the partial derivative of ˆyi with respect to yi (Exercise 9.1). A 
relatively large value of hii compared with the other diagonal elements of the 
hat matrix means that the ith observed response, yi, has a correspondingly 
relatively large eﬀect on the regression ﬁt. 
9.1.6 Linear Least Squares Subject to Linear Equality Constraints 
In the regression model (9.6), it may be known that β satisﬁes certain con-
straints, such as that all the elements be nonnegative. For constraints of the 
form g(β) ∈ C, where  C is some m-dimensional space, we may estimate β 
by constrained least squares, that is, the vector -βC that minimizes the dot 
product (9.9) among  all  b that satisfy g(b) ∈ C. 
The nature of the constraints may or may not make drastic changes to the 
computational problem. (The constraints also change the statistical inference 
problem in various ways, but we do not address that here.) If the constraints 
are nonlinear, or if the constraints are inequality constraints (such as that all 
the elements be nonnegative), there is no general closed-form solution. 
It is easy to handle linear equality constraints of the form 
StartLayout 1st Row 1st Column g left parenthesis beta right parenthesis 2nd Column equals 3rd Column upper L beta 2nd Row 1st Column Blank 2nd Column equals 3rd Column c comma EndLayoutg(β) = Lβ
= c,
where L is a q × m matrix of full rank. The solution is 
ModifyingAbove beta With caret Subscript upper C Baseline equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y plus left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper L Superscript normal upper T Baseline left parenthesis upper L left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper L Superscript normal upper T Baseline right parenthesis Superscript plus Baseline left parenthesis c minus upper L left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline y right parenthesis period-βC = (XTX)+XTy + (XTX)+LT(L(XTX)+LT)+(c −L(XTX)+XTy).
(9.25) 
When X is of full rank, this result can be derived by using Lagrange multipliers 
and the derivative of the norm (9.9) (see Exercise  9.3 on page 517). When X 
is not of full rank, it is slightly more diﬃcult to show this, but it is still true. 
The restricted least squares estimate, -βC, can be obtained (in the (1, 2) 
block) by performing m + q sweep operations on the matrix, 
Start 3 By 3 Matrix 1st Row 1st Column upper X Superscript normal upper T Baseline upper X 2nd Column upper X Superscript normal upper T Baseline y 3rd Column upper L Superscript normal upper T Baseline 2nd Row 1st Column y Superscript normal upper T Baseline upper X 2nd Column y Superscript normal upper T Baseline y 3rd Column c Superscript normal upper T Baseline 3rd Row 1st Column upper L 2nd Column c 3rd Column 0 EndMatrix comma
⎡
⎣
XTX XTy LT
yTX yTy cT
L
c
0
⎤
⎦,
(9.26) 
analogous to matrix (9.21).

452
9 Selected Applications in Statistics
9.1.7 Weighted Least Squares 
In ﬁtting the regression model y ≈ Xβ, it is often desirable to weight the 
observations diﬀerently, and so instead of minimizing Eq. (9.9), we minimize 
sigma summation w Subscript i Baseline left parenthesis y Subscript i Baseline minus x Subscript i asterisk Superscript normal upper T Baseline b right parenthesis squared comma
E
wi(yi −xT
i∗b)2,
where wi represents a nonnegative weight to be applied to the ith observation. 
One purpose of the weight may be to control the eﬀect of a given observation 
on  the overall  ﬁt. If a model of the  form  of  Eq. (9.6), 
y equals upper X beta plus epsilon commay = Xβ + e,
is assumed, and e is taken to be a random variable such that ei has variance σ2 
i , 
an appropriate value of wi may be 1/σ2 
i . (Statisticians almost always naturally 
assume that e is a random variable. Although usually it is modeled this way, 
here we are allowing for more general interpretations and more general motives 
in ﬁtting the model.) 
The normal equations can be written as 
left parenthesis upper X Superscript normal upper T Baseline normal d normal i normal a normal g left parenthesis left parenthesis w 1 comma w 2 comma ellipsis comma w Subscript n Baseline right parenthesis right parenthesis upper X right parenthesis b equals upper X Superscript normal upper T Baseline normal d normal i normal a normal g left parenthesis left parenthesis w 1 comma w 2 comma ellipsis comma w Subscript n Baseline right parenthesis right parenthesis y period
(
XTdiag((w1, w2, . . . , wn))X
)
b = XTdiag((w1, w2, . . . , wn))y.
More generally, we can consider W to be a weight matrix that is not 
necessarily diagonal. We have the same set of normal equations: 
left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis b equals upper X Superscript normal upper T Baseline upper W y period(XTWX)b = XTWy.
(9.27) 
When W is a diagonal matrix, the problem is called “weighted least squares.” 
Use of a nondiagonal W is also called weighted least squares or sometimes, 
“generalized least squares.” The weight matrix is symmetric and generally 
positive deﬁnite or at least nonnegative deﬁnite. The unique weighted least 
squares ﬁt is 
ModifyingAbove beta With caret Subscript upper W Baseline equals left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline upper W y period-βW = (XTWX)+XTWy.
As we have mentioned many times, an expression such as this is not necessarily 
a formula for computation. The matrix factorizations discussed above for the 
unweighted case can also be used for computing weighted least squares ﬁts. 
In a model y = Xβ + e, where e is taken to be a random variable with 
variance-covariance matrix Σ, the choice of W as Σ−1 yields estimators with 
certain desirable statistical properties. (Because this is a natural choice for 
many models, statisticians sometimes choose the weighting matrix without 
fully considering the reasons for the choice.) As we pointed out on page 289, 
weighted least squares can be handled by premultiplication of both y and 
X by the Cholesky factor of the weight matrix. In the case of an assumed 
variance-covariance matrix Σ, we transform each side by Σ−1 
C , where  ΣC is 
the Cholesky factor of Σ. The residuals whose squares are to be minimized 
are Σ−1 
C (y − Xb). Under the assumptions, the variance-covariance matrix of 
the residuals is I.

9.1 Linear Models
453
9.1.8 Updating Linear Regression Statistics 
In Sect. 5.6.5 on page 289, we discussed the general problem of updating a 
least squares solution to an overdetermined system when either the number 
of equations (rows) or the number of variables (columns) is changed. In the 
linear regression problem these changes correspond to adding or deleting ob-
servations and adding or deleting terms in the linear model, respectively. 
Adding More Variables 
Suppose ﬁrst that more variables are added, so the regression model is 
y almost equals Start 1 By 2 Matrix 1st Row 1st Column upper X 2nd Column upper X Subscript plus Superscript Baseline EndMatrix theta commay ≈
⎡
X X+
⎤
θ,
where X+ represents the observations on the additional variables. (We use θ 
to represent the parameter vector; because the model is diﬀerent, it is not just 
β with some additional elements.) 
If XT X has been formed and the sweep operator is being used to perform 
the regression computations, it can be used easily to add or delete variables 
from the model, as we mentioned above. The Sherman-Morrison-Woodbury 
formulas (5.28) and  (5.30) and the Hemes formula (5.31) (see page 282) can  
also be used to update the solution. 
In regression analysis, one of the most important questions is the identiﬁ-
cation of independent variables from a set of potential explanatory variables 
that should be in the model. This aspect of the analysis involves adding and 
deleting variables. We discuss this further in Sect. 9.4.2. 
Adding More Observations 
If we have obtained more observations, the regression model is 
StartBinomialOrMatrix y Choose y Subscript plus Superscript Baseline EndBinomialOrMatrix almost equals StartBinomialOrMatrix upper X Choose upper X Subscript plus Superscript Baseline EndBinomialOrMatrix beta comma
⎡y
y+
⎤
≈
⎡X
X+
⎤
β,
where y+ and X+ represent the additional observations. 
We ﬁrst note some properties of the new XT X matrix, although we will 
make direct use of the new X matrix, as usual. We see that 
StartBinomialOrMatrix upper X Choose upper X Subscript plus Superscript Baseline EndBinomialOrMatrix Superscript normal upper T Baseline StartBinomialOrMatrix upper X Choose upper X Subscript plus Superscript Baseline EndBinomialOrMatrix equals upper X Superscript normal upper T Baseline upper X plus upper X Subscript plus Superscript normal upper T Baseline upper X Subscript plus Superscript Baseline period
⎡X
X+
⎤T ⎡X
X+
⎤
= XTX + XT
+X+.
The relation of the  inverse of  XT X + XT 
+X+ to the inverse of XT X can be 
seen in Eq. (3.191) on page 141 or in Eq. (3.199) for the vector corresponding 
to a single additional row. 
If the QR decomposition of X is available, we simply augment it as in 
Eq. (5.47):

454
9 Selected Applications in Statistics
Start 3 By 2 Matrix 1st Row 1st Column upper R 2nd Column c 1 2nd Row 1st Column 0 2nd Column c 2 3rd Row 1st Column upper X Subscript plus Baseline 2nd Column y Subscript plus Baseline EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column upper Q Superscript normal upper T Baseline 2nd Column 0 2nd Row 1st Column 0 2nd Column upper I EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper X 2nd Column y 2nd Row 1st Column upper X Subscript plus Baseline 2nd Column y Subscript plus Baseline EndMatrix period
⎡
⎣
R
c1
0
c2
X+ y+
⎤
⎦=
⎡
QT 0
0 I
⎤⎡
X
y
X+ y+
⎤
.
We now apply orthogonal transformations to this to zero out the last rows 
and produce 
Start 2 By 2 Matrix 1st Row 1st Column upper R Subscript asterisk Baseline 2nd Column c Subscript 1 asterisk Baseline 2nd Row 1st Column 0 2nd Column c Subscript 2 asterisk Baseline EndMatrix comma
⎡
R∗c1∗
0 c2∗
⎤
,
where R∗ is an m × m upper triangular matrix and c1∗ is an m-vector as 
before, but c2∗ is an (n −m + k)-vector. We then have an equation of the 
form (9.20) and we use back-substitution to solve it. 
Adding More Observations Using Weights 
Another way of approaching the problem of adding or deleting observations 
is by viewing the problem as weighted least squares. In this approach, we also 
have more general results for updating regression statistics. Following Escobar 
and Moser (1993), we can consider two weighted least squares problems: one 
with weight matrix W and one with weight matrix V . Suppose we have the 
solutions -βW and -βV . Now  let  
normal upper Delta equals upper V minus upper W commaΔ = V −W,
and use the subscript ∗ on any matrix or vector to denote the subarray that 
corresponds only to the nonnull rows of Δ. The  symbol  Δ∗, for example, is 
the square subarray of Δ consisting of all of the nonzero rows and columns of 
Δ, and  X∗ is the subarray of X consisting of all the columns of X and only 
the rows of X that correspond to Δ∗. From the normal equations (9.27) using  
W and V , and with the solutions -βW and -βV plugged in, we have 
left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis ModifyingAbove beta With caret Subscript upper V Baseline plus left parenthesis upper X Superscript normal upper T Baseline normal upper Delta upper X right parenthesis ModifyingAbove beta With caret Subscript upper V Baseline equals upper X Superscript normal upper T Baseline upper W y plus upper X Superscript normal upper T Baseline normal upper Delta y comma(XTWX)-βV + (XTΔX)-βV = XTWy + XTΔy,
and so 
ModifyingAbove beta With caret Subscript upper V Baseline minus ModifyingAbove beta With caret Subscript upper W Baseline equals left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline normal upper Delta Subscript asterisk Superscript Baseline left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper V Baseline right parenthesis Subscript asterisk Superscript Baseline period-βV −-βW = (XTWX)+XT
∗Δ∗(y −X -βV )∗.
This gives 
left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper V Baseline right parenthesis Subscript asterisk Superscript Baseline equals left parenthesis upper I plus upper X left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline normal upper Delta Subscript asterisk Superscript Baseline right parenthesis Superscript plus Baseline left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper W Baseline right parenthesis Subscript asterisk Superscript Baseline comma(y −X -βV )∗= (I + X(XTWX)+XT
∗Δ∗)+(y −X -βW )∗,
and ﬁnally 
ModifyingAbove beta With caret Subscript upper V Baseline equals ModifyingAbove beta With caret Subscript upper W Baseline plus left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline normal upper Delta Subscript asterisk Superscript Baseline left parenthesis upper I plus upper X Subscript asterisk Superscript Baseline left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline normal upper Delta Subscript asterisk Superscript Baseline right parenthesis Superscript plus Baseline left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper W Baseline right parenthesis Subscript asterisk Superscript Baseline period-βV = -βW + (XTWX)+XT
∗Δ∗
(
I + X∗(XTWX)+XT
∗Δ∗
)+
(y −X -βW )∗.
If Δ∗ can be written as ±GGT , using this equation and Eq. (3.190) on  
page 141 (which also apply to pseudoinverses), we have

9.1 Linear Models
455
ModifyingAbove beta With caret Subscript upper V Baseline equals ModifyingAbove beta With caret Subscript upper W Baseline plus or minus left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline upper G left parenthesis upper I plus or minus upper G Superscript normal upper T Baseline upper X Subscript asterisk Superscript Baseline left parenthesis upper X Superscript normal upper T Baseline upper W upper X right parenthesis Superscript plus Baseline upper X Subscript asterisk Superscript normal upper T Baseline upper G right parenthesis Superscript plus Baseline upper G Superscript normal upper T Baseline left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper W Baseline right parenthesis Subscript asterisk Superscript Baseline period-βV = -βW ± (XTWX)+XT
∗G(I ± GTX∗(XTWX)+XT
∗G)+GT(y −X -βW )∗.
(9.28) 
The sign of GGT is positive when observations are added and negative when 
they are deleted. 
Equation (9.28) is particularly simple in the case where W and V are 
identity matrices (of diﬀerent sizes, of course). Suppose that we have ob-
tained more observations in y+ and X+. (In the following, the reader must 
be careful to distinguish “+” as a subscript to represent more data and “+” 
as a superscript with its usual meaning of a Moore-Penrose inverse.) Suppose 
we already have the least squares solution for y ≈ Xβ, say -βW . Now -βW is 
the weighted least squares solution to the model with the additional data and 
with weight matrix 
upper W equals Start 2 By 2 Matrix 1st Row 1st Column upper I 2nd Column 0 2nd Row 1st Column 0 2nd Column 0 EndMatrix periodW =
⎡I 0
0 0
⎤
.
We now seek the solution to the same system with weight matrix V , which  is  
a larger identity matrix. From Eq. (9.28), the solution is 
ModifyingAbove beta With caret equals ModifyingAbove beta With caret Subscript upper W Baseline plus left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Subscript plus Superscript normal upper T Baseline left parenthesis upper I plus upper X Subscript plus Superscript Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Subscript plus Superscript normal upper T Baseline right parenthesis Superscript plus Baseline left parenthesis y minus upper X ModifyingAbove beta With caret Subscript upper W Baseline right parenthesis Subscript asterisk Superscript Baseline period-β = -βW + (XTX)+XT
+(I + X+(XTX)+XT
+)+(y −X -βW )∗.
(9.29) 
9.1.9 Linear Smoothing 
The interesting reasons for doing regression analysis are to understand rela-
tionships and to predict a value of the dependent value given a value of the 
independent variable. As a side beneﬁt, a model with a smooth equation f(x) 
“smoothes” the observed responses; that is, the elements in ˆy = -
f(x) exhibit 
less variation than the elements in y. Of course, the important fact for our 
purposes is that ||y −ˆy|| is smaller than ||y|| or ||y − ¯y||. 
The use of the hat matrix emphasizes the smoothing perspective as a 
projection of the original y: 
ModifyingAbove y With caret equals upper H y periodˆy = Hy.
The concept of a smoothing matrix was discussed in Sect. 8.6.2. From this  
perspective, using H, we project y onto a vector in span(H), and that vector 
has a smaller variation than y; that is,  H has smoothed y. It does not matter 
what the speciﬁc values in the vector y are so long as they are associated with 
the same values of the independent variables. 
We can extend this idea to a general n × n smoothing matrix Hλ: 
y overTilde equals upper H Subscript lamda Baseline y period˜y = Hλy.
The smoothing matrix depends only on the kind and extent of smoothing to 
be performed and on the observed values of the independent variables. The 
extent of the smoothing may be indicated by the indexing parameter λ. Once

456
9 Selected Applications in Statistics
the smoothing matrix is obtained, it does not matter how the independent 
variables are related to the model. 
In Sect. 5.7.2, we discussed regularized solutions of overdetermined systems 
of equations, which in the present case is equivalent to solving 
min Underscript b Endscripts left parenthesis left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis plus lamda b Superscript normal upper T Baseline b right parenthesis periodmin
b
(
(y −Xb)T(y −Xb) + λbTb
)
.
The solution of this yields the smoothing matrix 
upper S Subscript lamda Baseline equals upper X left parenthesis upper X Superscript normal upper T Baseline upper X plus lamda upper I right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline commaSλ = X(XTX + λI)−1XT,
as we have seen on page 401. This has the eﬀect of shrinking the -y toward 0. 
(In regression analysis, this is called “ridge regression.”) 
We discuss ridge regression and general shrinkage estimation in Sect. 9.4.4. 
9.1.10 Multivariate Linear Models 
A simple modiﬁcation of the model (9.5), yi = βT xi +ei, on page 439, extends 
the scalar responses to vector responses; that is, yi is a vector, and of course, 
the vector of parameters β must be replaced by a matrix. Let d be the order 
of yi. Similarly, ei is a d-vector. 
This is a “multivariate” linear model, meaning among other things, that 
the error term has a multivariate distribution (it is not a set of iid scalars). 
A major diﬀerence in the multivariate linear model arises from the struc-
ture of the vector ei. It may be appropriate to assume that the eis are indepen-
dent from one observation to another, but it is not likely that the individual 
elements within an ei vector are independent from each other or even that 
they have zero correlations. A reasonable assumption to complete the model 
is that the vectors eis are independently and identically distributed with mean 
0 and variance-covariance matrix Σ. It might be reasonable also to assume 
that they have a normal distribution. 
In statistical applications in which univariate responses are modeled, in-
stead of the model for a single observation, we are more likely to write the 
model for a set of observations on y and x in the form of Eq. (9.6), y = Xβ+e, 
in which y and e are d-vectors, and X is a matrix in which the rows correspond 
to the individual xi. Extending this form to the multivariate model, we write 
upper Y equals upper X upper B plus upper E commaY = XB + E,
(9.30) 
where now Y is an n × d matrix, X is an n × m matrix as before, B is an 
m × d matrix, and E is an n × d matrix. 
We will discuss issues of statistical inference for multivariate linear models 
in Sect. 9.2.6, beginning on page 468.

9.1 Linear Models
457
Fitting the Model 
Fitting multivariate linear models is done in the same way as ﬁtting univariate 
linear models. The most common criterion for ﬁtting is least squares. This is 
the same ﬁtting problem that we considered in Sect. 9.1.1 in this chapter or 
earlier in Sect. 5.6 on page 282. 
In an approach similar to the development in Sect. 5.6, for a given choice 
of B, say -B, we have, corresponding to Eq. (5.33), 
upper X upper B overTilde equals upper Y minus upper R commaX -B = Y −R,
(9.31) 
where R is an n × d matrix of residuals. 
A least squares solution -B is one that minimizes the sum of squares of the 
residuals (or, equivalently, the square root of the sum of the squares, that is,
||R||F). Hence, we have the optimization problem 
min Underscript upper B overTilde Endscripts double vertical bar upper Y minus upper X upper B overTilde double vertical bar Subscript normal upper F Baseline period min
-
B
||||||Y −X -B
||||||
F .
(9.32) 
As in Sect. 5.6, we rewrite the square of this norm, using Eq. (3.321) from  
page 191, as  
normal t normal r left parenthesis left parenthesis upper Y minus upper X upper B overTilde right parenthesis Superscript normal upper T Baseline left parenthesis upper Y minus upper X upper B overTilde right parenthesis right parenthesis periodtr
(
(Y −X -B)T(Y −X -B)
)
.
(9.33) 
This is similar to Eq. (5.35), which, as before, we diﬀerentiate and set equal 
to zero, getting the normal equations in the vector -B, 
upper X Superscript normal upper T Baseline upper X ModifyingAbove upper B With caret equals upper X Superscript normal upper T Baseline upper YXTX -B = XTY
(9.34) 
(Exercise 9.5). 
We note that the columns of the matrices in these equations are each the 
same as the univariate normal equations (5.36): 
upper X Superscript normal upper T Baseline upper X left bracket ModifyingAbove upper B With caret Subscript asterisk 1 Baseline comma ellipsis comma ModifyingAbove upper B With caret Subscript asterisk d Baseline right bracket equals upper X Superscript normal upper T Baseline left bracket upper Y Subscript asterisk 1 Baseline comma ellipsis comma upper Y Subscript asterisk d Baseline right bracket periodXTX[ -B∗1, . . . , -B∗d] = XT[Y∗1, . . . , Y∗d].
Partitioning the Sum of Squares 
On page 401, we discussed the partitioning of the sum of squares of an ob-
served vector of data, yT y. We did this in the context of the Gramian of the 
partitioned matrix [X y]. In the multivariate case, ﬁrst of all, instead of the 
sum of squares yT y, we have the matrix of sums of squares and cross-products, 
Y T Y . We now consider the Gramian matrix [X Y  ]T [X Y  ], and partition it as 
in expression (9.21), 
Start 2 By 3 Matrix 1st Row 1st Column upper X Superscript normal upper T Baseline upper X 2nd Column Blank 3rd Column upper X Superscript normal upper T Baseline upper Y 2nd Row 1st Column upper Y Superscript normal upper T Baseline upper X 2nd Column Blank 3rd Column upper Y Superscript normal upper T Baseline upper Y EndMatrix period
⎡
XTX XTY
Y TX Y TY
⎤
.
(9.35) 
From this we can get 
Start 2 By 3 Matrix 1st Row 1st Column left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline 2nd Column Blank 3rd Column upper X Superscript plus Baseline upper Y 2nd Row 1st Column upper Y Superscript normal upper T Baseline upper X Superscript plus normal upper T Baseline 2nd Column Blank 3rd Column upper Y Superscript normal upper T Baseline upper Y minus upper Y Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline upper Y EndMatrix period
⎡
(XTX)+
X+Y
Y TX+T
Y TY −Y TX(XTX)+XTY
⎤
.
(9.36) 
Note that the term in the lower right side in this partitioning is the Schur 
complement of XT X in [X Y  ]T [X Y  ] (see Eq. (3.205) on page 144).

458
9 Selected Applications in Statistics
9.2 Statistical Inference in Linear Models 
In the previous section we considered computational issues in ﬁtting the model 
y equals upper X beta plus epsilon commay = Xβ + e,
that is, determining a value -β such that ˆy = X -β is a good approximation to 
the observed y. 
The computational methods discussed in this section are similar, but here, 
“ﬁtting” becomes statistical “estimation,” and we are concerned with statis-
tical properties of the inference procedures. 
Statistical Properties of Estimators 
An early step in statistical inference is usually estimation of some parameter 
in a model. In the linear model, we want to estimate β, or some linear combi-
nation, qT β. There are many possible estimators, and we evaluate them based 
on such things as unbiasedness, small variance, or  small mean squared error. 
These properties taken alone do not have much value. Consider unbiasedness, 
for example. If T is an unbiased estimator of some parameter, then T + U, 
where U = 1 if a toss of an “unbiased” coin is heads and U = −1 if it is  
tails, is also unbiased for that same parameter. Now, consider small variance. 
The estimator T = c, where  c is a constant –any constant– has the smallest 
variance possible, but it is probably not a good estimator. 
In linear models, we often consider another property of estimators, namely, 
linearity. A point estimator T is linear if it can be expressed as gT y for some 
vector g, which may depend on X or other observables. An estimator of a 
vector is linear if it can be expressed as My for some matrix M. 
Statistical inference in the linear model also includes testing statistical 
hypotheses of the form 
upper H 0 colon l Superscript normal upper T Baseline beta equals 0 commaH0 : lTβ = 0,
for a given vector l and setting conﬁdence intervals for some vector Lβ, where  
L is some given matrix. 
These considerations require knowledge of (or assumptions about) the 
probability distribution of the vector e and any other random variables in 
the model. 
In the following, we will assume that β is constant (but unknown) and X 
is constant (and observed). 
Full-Rank and Nonfull-Rank Models 
Two general types of models of the form y = Xβ + e are often distinguished, 
“regression models” in which X is  assumed to be of full (column) rank and  
“analysis of variance models” in which X is not of full rank. In the following, I

9.2 Statistical Inference in Linear Models
459
will often assume the general case in which X may not be of full rank; hence, 
I use generalized inverses X− or X+ . For “regression models” of full rank, 
wherever I have written − or + for a square matrix, just substitute −1 . 
In the case of nonfull-rank models, various quantities may depend on the 
speciﬁc form of the generalized inverse. It is important to identify estimators 
or other statistics that are invariant to the choice of generalized inverse. (We 
have noted, for example, that the residuals are invariant to the choice of 
generalized inverse.) 
9.2.1 The Probability Distribution of e
The probability distribution of observable random variables determines the 
nature of statistical inference based on observed data. The observable random 
variable in the linear model is y, but since y = Xβ +e, we usually focus on the 
probability distribution of e. Various assumptions about this distribution allow 
diﬀerent statements about estimators in analyzing the model and diﬀerent 
kinds of statistical inference about parameters in the model. 
Expectation of e
We make a simple assumption about the distribution of e. We assume its mean 
given X and β is 0: 
normal upper E left parenthesis epsilon right parenthesis equals 0 periodE(e) = 0.
This implies 
normal upper E left parenthesis y right parenthesis equals upper X beta periodE(y) = Xβ.
If the model has an intercept, this assumption can be justiﬁed by noting that 
if E(e) is not zero, it could be added to the constant term, and in the resulting 
model, the mean of the errors would be 0. 
With just this assumption, we can identify two important properties, es-
timability and testability of functions of β. We will consider estimability and 
testability in Sects. 9.2.2 and 9.2.4, focusing on linear combinations of the 
elements of β. 
Variances of e and of the Least Squares Fits 
In statistical inference we are interested in the variances and covariances of the 
relevant statistics and estimators. To determine these, we must make further 
assumptions about the distribution of e; in particular, we must make some 
assumptions about V(e) or V(y). The variances of interest are the variances 
conditional on X and β, but since we are assuming X and β are constant, 
they are the same as the unconditional variances. Also, because X and β are 
assumed constant, it is assumed that V(y) = V(e). 
We will generally express the variance of any estimator or other statistic 
in terms of V(e).

460
9 Selected Applications in Statistics
The variance of any linear transformation of y, say  Ay, can be expressed 
in terms of the variance of y: 
normal upper V left parenthesis upper A y right parenthesis equals upper A normal upper V left parenthesis epsilon right parenthesis upper A Superscript normal upper T Baseline periodV(Ay) = AV(e)AT.
(9.37) 
Three commonly assumed forms of V(e) are  
StartLayout 1st Row 1st Column bullet Constant variance and 0 covariance colon 2nd Column normal upper V left parenthesis epsilon right parenthesis equals sigma squared upper I 2nd Row 1st Column bullet Heteroscedastic observations with 0 covariance colon 2nd Column normal upper V left parenthesis epsilon right parenthesis equals normal d normal i normal a normal g left parenthesis sigma 1 squared comma ellipsis comma sigma Subscript n Superscript 2 Baseline right parenthesis 3rd Row 1st Column bullet General positive definite variance hyphen covariance colon 2nd Column normal upper V left parenthesis epsilon right parenthesis equals normal upper Sigma EndLayout
• Constant variance and 0 covariance:
V(e) = σ2I
• Heteroscedastic observations with 0 covariance: V(e) = diag(σ2
1, . . . , σ2
n)
• General positive deﬁnite variance-covariance:
V(e) = Σ
We must emphasize that these variances are all conditional on the correct 
model. 
Many of the desirable properties of least squares estimators depend on 
the error variance having the  form  σ2 I, and so it is often assumed that the 
variances are constant and that the observations have 0 covariance. In the 
following, we will assume a variance of the form σ2 I. In Sect. 9.2.5 beginning 
on page 467, we will brieﬂy consider other forms. 
Under the assumption V(e) =  σ2 I, in the case of a full-rank model, 
Eq. (9.37) yields the variance of the least squares estimator as 
normal upper V left parenthesis ModifyingAbove beta With caret right parenthesis equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline sigma squared periodV(-β) = (XTX)−1σ2.
(9.38) 
In a nonfull-rank model, the variance of the least squares ﬁt depends on 
the generalized inverse that yields the ﬁts. In general, it is 
normal upper V left parenthesis beta overTilde right parenthesis equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline sigma squared periodV(-β) = (XTX)−XTX(XTX)−σ2.
(9.39) 
The properties of the Moore-Penrose inverse (page 151) yield the variance of 
the least squares ﬁt using it as 
normal upper V left parenthesis ModifyingAbove beta With caret right parenthesis equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline sigma squared commaV(-β) = (XTX)+σ2,
(9.40) 
similar to that of the full-rank model. 
Diﬀerent generalized inverses yield diﬀerent values of V(-β) in Eq. (9.39). 
Normality: e ∼ Nn(0, σ2 In) 
To perform tests, set conﬁdence regions, or make other statements about 
estimators and other statistics, we may need to make stronger assumptions 
about the distribution of e. One common assumption is that the elements of
e are iid normal with a mean of 0, that is, e ∼ Nn(0, σ2 In). Under this strong 
assumption, statistics of interest may have t, chi-squared, or F distributions. 
Under the assumption e ∼ Nn(0, σ2 In), we have the result that y ∼ 
Nn(Xβ, σ2 In). We continue with the assumption that X and β are constant 
and X is observable. Since -β = (XT X)+ XT y, we can now work out the dis-
tribution of -β from the distribution of y. It obviously depends on (XT X)+ . 
For a full-rank model, (XT X)+ = (XT X)−1 , and we have

9.2 Statistical Inference in Linear Models
461
ModifyingAbove beta With caret tilde normal upper N Subscript n Baseline left parenthesis beta comma sigma squared upper I Subscript n Baseline right parenthesis period-β ∼Nn(β, σ2In).
(9.41) 
This is a simple application of result (7.44) on page 348. 
For models not of full rank, we estimate linear combinations of β. Under 
the assumption of normality for e, we can obtain the distributions of estimators 
of the estimable linear combinations. 
Maximum Likelihood Estimators 
The assumption of a normal distribution also yields the result that the least 
squares estimators are maximum likelihood estimators. We see this by writing 
the log-likelihood function for e = y −Xβ; we have (see Sect. 9.6.4, beginning 
on page 502): 
l left parenthesis beta comma sigma squared semicolon y comma upper X right parenthesis equals c minus StartFraction n Over 2 EndFraction log left parenthesis sigma squared right parenthesis minus StartFraction 1 Over 2 sigma squared EndFraction left parenthesis y minus upper X beta right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X beta right parenthesis commal(β, σ2; y, X) = c −n
2 log(σ2) −
1
2σ2 (y −Xβ)T(y −Xβ),
(9.42) 
where we have used  c to represent the constant portion. 
For any value of σ2 , we see that the log-likelihood function is maximized 
with β such that 
left parenthesis y minus upper X beta right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X beta right parenthesis(y −Xβ)T(y −Xβ)
(9.43) 
is minimized. This is the same as the sum of squares of the residuals. Diﬀer-
entiating with respect to β and setting equal to 0, we get the same normal 
equations as in the least squares ﬁt. For e ∼ Nn(0, σ2 In), a maximum likeli-
hood estimator (MLE) of β is a least squares (OLS) estimator. Any solution 
to the normal equations is an MLE. As before, if the model is of full rank, the 
estimator is unique. 
We denote an MLE by -β. 
Setting the partial derivative of l(β, σ2 ; y, X) with respect to σ2 to 0, and 
solving the equation, we obtain an MLE of σ2 , 
ModifyingAbove sigma With caret squared equals StartFraction 1 Over n EndFraction left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis period-σ2 = 1
n(y −X -β)T(y −X -β).
(9.44) 
As we have seen, this is invariant to -β, so long as -β minimizes (y −Xβ)T (y − 
Xβ). Although it is biased (because of the n−1 factor), the estimator is con-
sistent, as are most MLEs. 
The equivalence of least squares and maximum likelihood in the case of 
normally distributed errors is relevant because of the important statistical 
properties of maximum likelihood estimators, especially asymptotic proper-
ties. An example is the use of MLEs in forming likelihood ratio tests; see 
page 466.

462
9 Selected Applications in Statistics
9.2.2 Estimability 
If X is of full (column) rank, then we can form unbiased estimators of any 
element in β or indeed of the vector β itself. In nonfull-rank models, however, 
this is not the case. 
One of the most important questions for statistical inference in linear 
models involves estimating or testing some linear combination of the elements 
of the parameter β; for example, we may wish to estimate β1 −β2 or to test 
the hypothesis that β1 − β2 = c1 for some constant c1. In general, we will 
consider the linear combination lT β. 
A function of a model parameter is said to be estimable if there exists 
a function of a statistic whose expected value is the given function of the 
parameter. (A statistic is a function of observable variables that does not 
involve any unknown parameter; thus, estimators are statistics.) Estimability 
is thus related to unbiased estimators. 
Estimability does not depend on knowing the distribution of the errors. 
Estimability depends only on the simplest distributional assumption about the 
model, that is, that some terms in the linear model have a known expectation. 
The most common assumption is that E(e) = 0, and hence E(y) =  Xβ. 
Now consider linear functions of E(y) and formally deﬁne a linear combi-
nation lT β to be linearly estimable if there exists a vector t such that 
t Superscript normal upper T Baseline normal upper E left parenthesis y right parenthesis equals l Superscript normal upper T Baseline betatTE(y) = lTβ
(9.45) 
for any β. 
If X is of full column rank, lT β is obviously linearly estimable for any l. 
Because E(y) =  Xβ, it is clear that lT β is linearly estimable for any l ∈ 
span(XT ), and furthermore lT β is linearly estimable only for l ∈ span(XT ). 
For l ∈ span(XT ), each of the following linear systems is consistent: 
upper X Superscript normal upper T Baseline t equals l periodXTt = l.
(9.46) 
upper X Superscript normal upper T Baseline upper X t equals lXTXt = l
(9.47) 
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline t equals l period(XTX)−t = l.
(9.48) 
The solution to each system obviously may be diﬀerent, but expressions of 
these forms allow us to derive properties of the estimators of lT β. The  t 
vector in Eq. (9.45) is a solution to the underdetermined consistent linear 
system (9.46). 
In the context of the linear model, we call an estimator of lT β a linear 
estimator if it can be expressed as aT y for some vector a. It is clear that a 
least squares estimator lT -β is a linear estimator. 
When X is not of full rank, we often are interested in an orthogonal basis 
for span(XT ). If X includes a column of 1s, the elements of any vector in 
the basis must sum to 0. Such vectors are called contrasts. The second and

9.2 Statistical Inference in Linear Models
463
subsequent rows of the Helmert matrix (see Sect. 8.8.1 on page 418) are  con-
trasts that are often of interest because of their regular patterns and their 
interpretability in applications involving the analysis of levels of factors in 
experiments. 
Uniqueness and Unbiasedness of Least Squares Estimators 
The least squares estimator of an estimable linear combination lT β is unique 
and unbiased, even if β itself is not estimable. 
The estimator of lT β based on a solution to the normal equations is 
lT (XT X)−XT y. From Eq. (9.47), let ˜t be such that l = XT X˜t, and  let  G 
be any generalized inverse of XT X. We have  
StartLayout 1st Row 1st Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y 2nd Column equals 3rd Column t overTilde Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y 2nd Row 1st Column Blank 2nd Column equals 3rd Column t overTilde Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X upper G upper X Superscript normal upper T Baseline y 3rd Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline upper G upper X Superscript normal upper T Baseline y comma EndLayoutlT(XTX)−XTy = ˜tTXTX(XTX)−XTy
= ˜tTXTXGXTy
= lTGXTy,
from Eq. (3.232) on page 151; that is, the estimator of an estimable linear 
combination is unique. It is invariant to choice of the generalized inverse. 
The estimator of lT β based on a solution to the normal equations is unbi-
ased. We see that this is true because 
StartLayout 1st Row 1st Column normal upper E left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis 2nd Column equals 3rd Column normal upper E left parenthesis l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column t overTilde Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X beta 3rd Row 1st Column Blank 2nd Column equals 3rd Column t overTilde Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X beta 4th Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline beta period EndLayoutE(lT -β) = E(lT(XTX)−XTy)
= ˜tTXTX(XTX)−XTXβ
= ˜tTXTXβ
= lTβ.
(9.49) 
If lT β is estimable, lT -β is invariant to the choice of the generalized inverse 
that yields -β, and it is unbiased. See the example below. 
Variance of Least Squares Estimators of Estimable Combinations 
While the unbiasedness of a linear estimator of lT β depends only on an as-
sumption concerning the expectation of e, any statements about the variance 
of a linear estimator depend on assumptions concerning the variance. In the 
following, we make the common assumption that V(y) = V(e) =  σ2 I, although 
the reasoning could be extended to more general forms of the variance. 
From Eq. (7.36) on page 343, we have, for any generalized inverse of XT X, 
StartLayout 1st Row 1st Column normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis 2nd Column equals 3rd Column left parenthesis l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline right parenthesis normal upper V left parenthesis y right parenthesis left parenthesis upper X left parenthesis left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline right parenthesis Superscript normal upper T Baseline l right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X left parenthesis left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline right parenthesis Superscript normal upper T Baseline l sigma squared period EndLayoutV(lT -β) =
(
lT(XTX)−XT)
V(y)
(
X
(
(XTX)−)T l
)
= lT(XTX)−XTX
(
(XTX)−)T lσ2.
(9.50) 
Now writing l = XT˜t for some ˜t (since l ∈ span(XT )), and using 
Eqs. (3.230) and  (3.231) on page 150, we have

464
9 Selected Applications in Statistics
StartLayout 1st Row 1st Column normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis 2nd Column equals 3rd Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline upper X left parenthesis left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline right parenthesis Superscript normal upper T Baseline upper X Superscript normal upper T Baseline t overTilde sigma squared 2nd Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline t overTilde sigma squared 3rd Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline l sigma squared period EndLayoutV(lT -β) = lT(XTX)−XTX
(
(XTX)−)T XT˜tσ2
= lT(XTX)−XT˜tσ2
= lT(XTX)−lσ2.
(9.51) 
We see that variance-covariance of the unbiased least square estimator
-β depends on the generalized inverse, as in Eqs. (9.39) and  (9.40), but for 
an estimable function, this variance is the same no matter what generalized 
inverse is used, as we can see from the same argument as above from the 
expression in the intermediate expression, lT (XT X)−XT˜tσ2 , after replacing 
lT with ˜tT X. 
The Gauss-Markov theorem below states an optimality property of this 
variance. First, we revisit a numerical example. 
The Classiﬁcation Model Numerical Example (Continued from 
Page 444) 
In the toy example based on the model 
y Subscript j k i Baseline equals mu plus alpha Subscript j Baseline plus gamma Subscript k Baseline plus epsilon Subscript j k i Baseline commayjki = μ + αj + γk + ejki,
where j = 1, 2 and  k = 1, 2, and for each combination of j and k there are two 
observations, in which β = (μ, α1, α2, γ1, γ2) using  a  g12 generalized inverse, 
we obtained 
beta overTilde equals left parenthesis 6.96 comma 2.23 comma 0.00 comma negative 2.28 comma 0.00 right parenthesis comma-β = (6.96, 2.23, 0.00, −2.28, 0.00),
and using the Moore-Penrose inverse, we obtained 
ModifyingAbove beta With caret equals left parenthesis 3.47 comma 2.85 comma 0.62 comma 0.60 comma 2.87 right parenthesis period-β = (3.47, 2.85, 0.62, 0.60, 2.87).
The actual value of β that was used to generate this toy dataset was 
beta equals left parenthesis 2 comma 3 comma 1 comma 1 comma 4 right parenthesis periodβ = (2, 3, 1, 1, 4).
Neither -β nor -β is an estimate of β; β is not estimable. 
Suppose, however, we want to estimate α1 − α2, which  in  the  β notation 
is β2 −β3. (In the classiﬁcation model, this is the diﬀerence in the eﬀect of 
the α1 treatment and the α2 treatment.) 
Let t = (1, 0, 0, 0, −1, 0, 0, 0), and let l = XT t. (This forms a linear combi-
nation of the ﬁrst and ﬁfth rows of X, and  l = (0, 1, −1, 0, 0).) Therefore, the 
contrast lT β = α1 − α2 is estimable. 
We get the same estimate for any estimable function, just as we would 
expect from the manipulations in Eq. (9.49). Use of either the g12 inverse ﬁt 
or the Moore-Penrose inverse ﬁt yields 
l Superscript normal upper T Baseline beta overTilde equals l Superscript normal upper T Baseline ModifyingAbove beta With caret equals 2.225 periodlT -β = lT -β = 2.225.

9.2 Statistical Inference in Linear Models
465
The actual value in the quantities that were used to generate the data is 
3 − 1 = 2.  
The variance of the estimator of lT β does not depend on the generalized 
inverse used. From Eq. (9.51), we have 
normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis equals l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript asterisk Baseline l equals 0.5 equals normal upper V left parenthesis l Superscript normal upper T Baseline ModifyingAbove beta With caret right parenthesis equals l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline l periodV(lT -β) = lT(XTX)∗l =
0.5
= V(lT -β) = lT(XTX)+l.
9.2.3 The Gauss-Markov Theorem 
The Gauss-Markov theorem provides a restricted optimality property for es-
timators of estimable functions of β under the condition that E(e) = 0  and  
V(e) =  σ2 I; that is, in addition to the assumption of zero expectation, which 
we have used above, we also assume that the elements of e have constant vari-
ance and that their covariances are zero. (We are not assuming independence 
or normality, as we do in order to develop tests of hypotheses.) 
Let y = Xβ + e and E(e) = 0  and  V(e) =  σ2 I, and  let -β be a solution  
to the normal equations XT X -β = XT y. The Gauss-Markov theorem states 
that lT -β is the unique best linear unbiased estimator (BLUE) of the estimable 
function lT β. 
A “linear” estimator in this context means a linear combination of y, that  
is, an estimator in the form aT y. “Best” in this context means that its variance 
is no greater than any other estimator that satisﬁes the requirements. 
It is clear that lT -β is linear, and we have already seen that it is unbiased 
for lT β. Hence, to prove the theorem, we only need to show that lT -β has 
minimum variance among all unbiased linear estimators. In Eq. (9.51), we 
showed the variance of lT -β to be lT (XT X)−lσ2 . 
Let aT y be any unbiased linear estimator of lT β. Since  E(aT y) =  lT β, it  
must  be  the case that  aT X = lT . Now, consider the variance of the diﬀerence 
of aT y and lT -β: 
normal upper V left parenthesis a Superscript normal upper T Baseline y minus l Superscript normal upper T Baseline beta overTilde right parenthesis equals normal upper V left parenthesis a Superscript normal upper T Baseline y right parenthesis plus normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis minus 2 normal upper C normal o normal v left parenthesis a Superscript normal upper T Baseline y comma l Superscript normal upper T Baseline beta overTilde right parenthesis periodV(aTy −lT -β) = V(aTy) + V(lT -β) −2Cov(aTy, lT -β).
(9.52) 
For the covariance, we have 
StartLayout 1st Row 1st Column normal upper C normal o normal v left parenthesis a Superscript normal upper T Baseline y comma l Superscript normal upper T Baseline beta overTilde right parenthesis 2nd Column equals 3rd Column normal upper C normal o normal v left parenthesis a Superscript normal upper T Baseline y comma l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline y right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column a Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline l sigma squared 3rd Row 1st Column Blank 2nd Column equals 3rd Column l Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline l sigma squared semicolon EndLayoutCov(aTy, lT -β) = Cov(aTy, lT(XTX)−XTy)
= aTX(XTX)−lσ2
= lT(XTX)−lσ2;
hence, substituting into Eq. (9.52), we have 
normal upper V left parenthesis a Superscript normal upper T Baseline y minus l Superscript normal upper T Baseline beta overTilde right parenthesis equals normal upper V left parenthesis a Superscript normal upper T Baseline y right parenthesis minus normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis commaV(aTy −lT -β) = V(aTy) −V(lT -β),
(9.53) 
and since V(aT y − lT -β) is nonnegative, we conclude 
normal upper V left parenthesis a Superscript normal upper T Baseline y right parenthesis greater than or equals normal upper V left parenthesis l Superscript normal upper T Baseline beta overTilde right parenthesis semicolonV(aTy) ≥V(lT -β);

466
9 Selected Applications in Statistics
that is, lT -β is the BLUE of lT β. 
We have already seen that the estimator based on a solution to the normal 
equations is unique. We can see this in another way by considering the case in 
which V(aT y) = V(lT -β). For this to be true, we must have V(aT y−tT XT y) =  
0, and for this variance to equal 0, it must be the case that aT − tT XT = 0  
or aT y = tT XT y = lT -β. This implies that lT -β is the unique linear unbiased 
estimator that achieves the minimum variance. 
If we assume further that e ∼ Nn(0, σ2 In), we can show that lT -β is the 
uniformly minimum variance unbiased estimator (UMVUE) for lT β. This is  
because (XT y, (y − X -β)T (y −X -β)) is complete and suﬃcient for (β, σ2 ). 
This line of reasoning also implies that (y − X -β)T (y −X -β)/(n − r), where 
r = rank(X), is UMVUE for σ2 . We will not go through the details here. 
The interested reader is referred to a text on mathematical statistics, such as 
Gentle (2003) at  
https://mason.gmu.edu/~jgentle/books/MathStat.pdf. 
9.2.4 Testing Linear Hypotheses 
We deﬁne a linear hypothesis lT β = c1 as testable if lT β is estimable. We 
generally restrict our attention to testable hypotheses. 
It is often of interest to test multiple hypotheses concerning linear combi-
nations of the elements of β. For the model (9.6), the general linear hypothesis 
is 
normal upper H 0 colon upper L Superscript normal upper T Baseline beta equals c commaH0 : LTβ = c,
where L is m × q, of rank  q, and such that span(L) ⊆ span(X). 
The test for a hypothesis depends on the distributions of the random 
variables in the model. 
If we assume that the elements of e are iid normal with a mean of 0, then 
the general linear hypothesis is tested using an F statistic whose numerator 
is the diﬀerence in the residual sum of squares from ﬁtting the model with 
the restriction LT β = c and the residual sum of squares from ﬁtting the 
unrestricted model. This reduced sum of squares is 
left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis Superscript normal upper T Baseline left parenthesis upper L Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper L right parenthesis Superscript negative 1 Baseline left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis comma(LT -β −c)T (LT(XTX)−L)−1 (LT -β −c),
(9.54) 
where (XT X)− is any generalized inverse of XT X. This test is a likelihood 
ratio test. 
To compute the quantity in expression (9.54), ﬁrst observe 
upper L Superscript normal upper T Baseline left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper L equals left parenthesis upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper L right parenthesis Superscript normal upper T Baseline left parenthesis upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper L right parenthesis periodLT(XTX)−L = (X(XTX)−L)T (X(XTX)−L).
(9.55) 
Now, if X(XT X)−L, which has rank q, is decomposed as 
upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper L equals upper P StartBinomialOrMatrix upper T Choose 0 EndBinomialOrMatrix commaX(XTX)−L = P
⎡
T
0
⎤
,

9.2 Statistical Inference in Linear Models
467
where P is an m × m orthogonal matrix and T is a q × q upper triangular 
matrix, we can write the reduced sum of squares (9.54) as  
left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis Superscript normal upper T Baseline left parenthesis upper T Superscript normal upper T Baseline upper T right parenthesis Superscript negative 1 Baseline left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis(LT -β −c)T (T TT)−1 (LT -β −c)
or 
left parenthesis upper T Superscript negative normal upper T Baseline left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis right parenthesis Superscript normal upper T Baseline left parenthesis upper T Superscript negative normal upper T Baseline left parenthesis upper L Superscript normal upper T Baseline beta overTilde minus c right parenthesis right parenthesis
(
T −T(LT -β −c)
)T (
T −T(LT -β −c)
)
or 
v Superscript normal upper T Baseline v periodvTv.
(9.56) 
To compute v, we solve  
upper T Superscript normal upper T Baseline v equals upper L Superscript normal upper T Baseline beta overTilde minus cT Tv = LT -β −c
(9.57) 
for v, and the reduced sum of squares is then formed as vT v. 
9.2.5 Statistical Inference in Linear Models with Heteroscedastic 
or Correlated Errors 
On page 460 we listed various assumptions for the variance V(e) of the errors 
in the linear model. In the past several pages, we have concentrated on one 
case, namely, V(e) =  σ2 I, that is, constant variance and 0 covariance. 
Recall that estimability of linear functions of β does not depend on the 
variance of e. Furthermore, the least squares estimators of estimable functions 
remain unbiased no matter what variance of e is so long as its conditional 
expectation is 0. What is aﬀected, however, are the variances of the estimators 
and the subsequent use of the estimators the in statistical tests and conﬁdence 
regions. 
If we know the underlying variance, we can make transformations using 
V(e) =  Σ to achieve the same model as one with V(e) =  σ2 I. Of course,  it  
would be rather unusual to have complete knowledge of a general positive 
deﬁnite variance-covariance matrix, and while approximations using partial 
knowledge of relationships may be appropriate, their eﬀects on the inference 
would be unknown. 
If the correlations among the observations are zero, but the individual 
variances may be diﬀerent, we might be able to develop reasonable inferential 
procedures using individual residuals. For heteroscedastic observations with 0 
covariances, we have V(e) = diag(σ2 
1, . . . , σ2 
n). This situation may be relatively 
common. It is often assumed in econometric applications. 
White (1980) suggested a consistent estimator of the variance of lT -β and 
other summary statistics that uses the individual residuals, ri. For expressions 
of the form (XT X)−XT X
(
(XT X)−)T σ2 based on a constant variance, such 
as occurs in Eq. (9.50), for example, we allow for heteroscedasticity by using 
the individual residuals ri along with their corresponding values of the xi∗: 
left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline upper X Superscript normal upper T Baseline normal d normal i normal a normal g left parenthesis r 1 squared comma ellipsis comma r Subscript n Superscript 2 Baseline right parenthesis upper X left parenthesis left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript minus Baseline right parenthesis Superscript normal upper T Baseline period(XTX)−XTdiag(r2
1, . . . , r2
n)X
(
(XTX)−)T .
(9.58)

468
9 Selected Applications in Statistics
In linear models with the heteroscedastic variance of the form V(e) =  
diag(σ2 
1, . . . , σ2 
n), estimators and test statistics based on expression (9.58) are  
consistent. Unless the data are highly heteroscedastic, the values tend to be 
fairly close to those based on the homoscedastic assumption. Estimators of the 
general form of expression (9.58) are sometimes called “sandwich” estimators. 
9.2.6 Statistical Inference for Multivariate Linear Models 
Statistical inference for the multivariate linear model, 
upper Y equals upper X upper B plus upper E commaY = XB + E,
(9.59) 
is similar to that in the univariate linear model with some obvious changes and 
extensions. First-order properties of distributions of the analogous statistics 
are almost the same. Second-order properties (variances), however, are rather 
diﬀerent. 
In Eq. (9.59), Y is an n×d matrix, X is an n×m matrix as in the univariate 
model, B is an m × d matrix, and E is an n × d matrix. 
The most common criterion for ﬁtting is least squares, which as we have 
pointed out before is the same as a maximum likelihood criterion if the errors 
are identically and independently normally distributed (which follows from 
the identity matrix I in expression (9.60)). 
Under the assumptions on the distribution of the vector ei above, and 
including the assumption of normality, E in (9.30) has a matrix normal dis-
tribution (see expression (7.46) on page 349): 
upper E tilde normal upper N Subscript n comma d Baseline left parenthesis 0 comma upper I comma normal upper Sigma right parenthesis commaE ∼Nn,d(0, I, Σ),
(9.60) 
or in the form of (7.47), 
normal v normal e normal c left parenthesis upper E Superscript normal upper T Baseline right parenthesis tilde normal upper N Subscript d n Baseline left parenthesis 0 comma normal d normal i normal a normal g left parenthesis normal upper Sigma comma ellipsis comma normal upper Sigma right parenthesis right parenthesis periodvec
(
ET)
∼Ndn (0, diag(Σ, . . . , Σ)) .
Note that the variance-covariance matrix in this distribution has Kronecker 
structure, since diag(Σ, . . . , Σ) =  I ⊗ Σ (see also Eq. (3.116)). 
The solution of the normal equations -B has a matrix normal distribution 
with expectation B. 
The matrix of residual sums of squares and cross-products shown in ex-
pression (9.36) provides a maximum likelihood estimator of Σ: 
ModifyingAbove normal upper Sigma With caret equals left parenthesis upper Y Superscript normal upper T Baseline upper Y minus upper Y Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript plus Baseline upper X Superscript normal upper T Baseline upper Y right parenthesis divided by n period -Σ =
(
Y TY −Y TX(XTX)+XTY
)
/n.
(9.61) 
If the normalizing factor is 1/(n−d) instead of 1/n, the estimator is unbiased. 
This partitioning breaks the matrix of total sums of squares and cross-
products into a sum of a matrix of sums of squares and cross-products due 
to the ﬁtted relationship between Y and X and a matrix of residual sums 
of squares and cross-products. The analysis of these two matrices of sums

9.2 Statistical Inference in Linear Models
469
of squares and cross-products is one of the most fundamental and important 
techniques in multivariate statistics. 
The matrix in the upper left of the partition (9.36) can be used to get an 
estimate of the variance-covariance matrix of estimates of any independent 
set of linearly estimable functions of B. The matrix in the upper right of 
the partition is the solution to the normal equations, -B. The matrix in the 
lower right partition, which is the Schur complement of the full inverse (see 
Eqs. (3.204) and  (3.236)), is the matrix of sums of squares and cross-products 
of the residuals. With proper scaling, it provides an estimate of the variance-
covariance Σ of each row of E in Eq. (9.30). 
The scaled matrix of sums of squares and cross-products of the residuals, 
call it -Σ, has a Wishart distribution with parameter Σ. 
The basic null hypothesis of interest that the distribution of Y is not 
dependent on X is essentially the same as in the univariate case. The F-test 
in the corresponding univariate case, which is the ratio of two independent chi-
squared random variables, has an analogue in a comparison of two matrices 
of sums of squares and cross-products. In the multivariate case, the basis for 
statistical inference is -Σ, and it can be used in various ways. The relevant 
fact is that -Σ ∼ Wd(Σ, n − d), that is, it has a Wishart distribution with 
variance-covariance matrix Σ and parameters d and n − d (see Exercise 7.9 
on page 361). 
In hypothesis testing, depending on the null hypothesis, there are other 
matrices that have Wishart distributions. There are various scalar transfor-
mations of Wishart matrices whose distributions are known (or which have 
been approximated). One of the most common ones, and which even has some 
of the ﬂavor of an F statistic, is Wilk’s Λ, 
normal upper Lamda equals StartFraction normal d normal e normal t left parenthesis ModifyingAbove normal upper Sigma With caret right parenthesis Over normal d normal e normal t left parenthesis ModifyingAbove normal upper Sigma With caret Subscript 0 Baseline right parenthesis EndFraction commaΛ =
det
(
-Σ
)
det
(
-Σ0
),
(9.62) 
where -Σ0 is a scaled Wishart matrix yielding a maximum of the likelihood 
under a null hypothesis. Other related test statistics involving -Σ are Pillai’s 
trace, the Lawley-Hotelling trace (and Hotelling’s T 2 ), and Roy’s maximum 
root. We will not discuss these here, and the interested reader is referred to a 
text on multivariate analysis. 
In multivariate analysis, there are other properties of the model that are 
subject to statistical inference. For example, we may wish to estimate or test 
the rank of the coeﬃcient matrix, B. Even in the case of a single multivariate 
random variable, we may wish to test whether the variance-covariance matrix 
is of full rank. If it is not, there are ﬁxed relationships among the elements of 
the random variable, and the distribution is said to be singular. We will discuss 
the problem of testing the rank of a matrix brieﬂy in Sect. 9.4.5, beginning on 
page 479, but for more discussion on issues of statistical inference, we again 
refer the reader to a text on multivariate statistical inference.

470
9 Selected Applications in Statistics
9.3 Principal Components 
The analysis of multivariate data involves various linear transformations that 
help in understanding the relationships among the features that the data 
represent. The second moments of the data are used to accommodate the 
diﬀerences in the scales of the individual variables and the covariances among 
pairs of variables. 
If X is the matrix containing the data stored in the usual way, a useful 
statistic is the sums of squares and cross-products matrix, XT X, or the “ad-
justed” squares and cross-products matrix, XT 
c Xc, where  Xc is the centered 
matrix formed by subtracting from each element of X the mean of the col-
umn containing that element. The sample variance-covariance matrix, as in 
Eq. (8.67), is the Gramian matrix 
upper S Subscript upper X Baseline equals StartFraction 1 Over n minus 1 EndFraction upper X Subscript normal c Superscript normal upper T Baseline upper X Subscript normal c Baseline commaSX =
1
n −1XT
c Xc,
(9.63) 
where n is the number of observations (the number of rows in X). 
In data analysis, the sample variance-covariance matrix SX in Eq. (9.63) 
plays an important role. In more formal statistical inference, it is a consis-
tent estimator of the population variance-covariance matrix (if it is positive 
deﬁnite), and under assumptions of independent sampling from a normal dis-
tribution, it has a known distribution. It also has important numerical prop-
erties; it is symmetric and positive deﬁnite (or, at least, nonnegative deﬁnite; 
see Sect. 8.6). Other estimates of the variance-covariance matrix or the cor-
relation matrix of the underlying distribution may not be positive deﬁnite, 
however, and in Sect. 9.4.6 and Exercise 9.22 we describe possible ways of 
adjusting a matrix to be positive deﬁnite. 
9.3.1 Principal Components of a Random Vector 
It is often of interest to transform a given random vector into a vector whose el-
ements are independent. We may also be interested in which of those elements 
of the transformed random vector have the largest variances. The transformed 
vector may be more useful in making inferences about the population. In more 
informal data analysis, it may allow use of smaller observational vectors with-
out much loss in information. 
Stating this more formally, if Y is a random d-vector with variance-
covariance matrix Σ, we seek a transformation matrix A such that -Y = AY 
has a diagonal variance-covariance matrix. We are additionally interested in 
a transformation aT Y that has maximal variance for a given ||a||. 
Because the variance of aT Y is V(aT Y ) =  aT Σa, we have already obtained 
the solution in Eq. (3.290). The vector a is the eigenvector corresponding to 
the maximum eigenvalue of Σ, and  if  a is normalized, the variance of aT Y is 
the maximum eigenvalue.

9.3 Principal Components
471
Because Σ is symmetric, it is orthogonally diagonalizable and the proper-
ties discussed in Sect. 3.9.9 on page 176 not only provide the transformation 
immediately but also indicate which elements of -Y have the largest variances. 
We write the orthogonal diagonalization of Σ as (see Eq. (3.276)) 
normal upper Sigma equals normal upper Gamma normal upper Lamda normal upper Gamma Superscript normal upper T Baseline commaΣ = ΓΛΓ T,
(9.64) 
where ΓΓ T = Γ T Γ = I and Λ is diagonal with elements λ1 ≥ · · ·  ≥ λm ≥ 0 
(because a variance-covariance matrix is nonnegative deﬁnite). Choosing the 
transformation as 
upper Y overTilde equals normal upper Gamma Superscript normal upper T Baseline upper Y comma-Y = Γ TY,
(9.65) 
we have V(-Y ) =  Λ; that is,  the  ith element of -Y has variance λi, and  
normal upper C normal o normal v left parenthesis upper Y overTilde Subscript i Baseline comma upper Y overTilde Subscript j Baseline right parenthesis equals 0 if i not equals j periodCov(-Yi, -Yj) = 0
if i /= j.
The elements of -Y are called the principal components of Y . The  ﬁrst principal 
component, -Y1, which is the signed magnitude of the projection of Y in the 
direction of the eigenvector corresponding to the maximum eigenvalue, has 
the maximum variance of any of the elements of -Y , and  V(-Y1) =  λ1. (It  is,  
of course, possible that the maximum eigenvalue is not simple. In that case, 
there is no one-dimensional ﬁrst principal component. If m1 is the multiplicity 
of λ1, all one-dimensional projections within the m1-dimensional eigenspace 
corresponding to λ1 have the same variance, and m1 projections can be chosen 
as mutually independent.) 
The second and third principal components, and so on, are likewise deter-
mined directly from the spectral decomposition. 
9.3.2 Principal Components of Data 
The same ideas of principal components in probability models carry over to 
observational data. Given an n × d data matrix X, we seek a transformation 
as above that will yield the linear combination of the columns that has max-
imum sample variance, and other linear combinations that are independent. 
This means that we work with the centered matrix Xc (Eq. (8.64)) and the 
variance-covariance matrix SX, as above, or the centered and scaled matrix 
Xcs (Eq. (8.65)) and the correlation matrix RX (Eq. (8.69)). See Section 3.3 
in Jolliﬀe (2002) for discussions of the diﬀerences in using the centered but 
not scaled matrix and using the centered and scaled matrix. 
In the following, we will use SX, which plays a role similar to Σ for the ran-
dom variable. (This role could be stated more formally in terms of statistical 
estimation. Additionally, the scaling may require more careful consideration. 
The issue of scaling naturally arises from the arbitrariness of units of mea-
surement in data. Random variables discussed in Sect. 9.3.1 have no units of 
measurement.)

472
9 Selected Applications in Statistics
In data analysis, we seek a normalized transformation vector a to apply 
to any centered observation xc, so that the sample variance of aT xc, that is,  
a Superscript normal upper T Baseline upper S Subscript upper X Baseline a commaaTSXa,
(9.66) 
is maximized. 
From Eq. (3.290) or the spectral decomposition equation (3.280), we know 
that the solution to this maximization problem is the eigenvector, v1, corre-
sponding to the largest eigenvalue, c1, of  SX, and the value of the expres-
sion (9.66); that is, vT 
1 SXv1 at the maximum is the largest eigenvalue. In 
applications, this vector is used to transform the rows of Xc into scalars. If 
we think of a generic row of Xc as the vector x, we call  vT 
1 x the ﬁrst principal 
component of x. There is some ambiguity about the precise meaning of “prin-
cipal component.” The deﬁnition just given is a scalar, that is, a combination 
of values of a vector of variables. This is consistent with the deﬁnition that 
arises in the population model in Sect. 9.3.1. Sometimes, however, the normal-
ized eigenvector v1 itself is referred to as the ﬁrst principal component. More 
often, the vector Xcv1 of linear combinations of the columns of Xc is called 
the ﬁrst principal component. We will often use the term in this latter sense. 
The vector z1 shown in Fig. 9.2 is Xcv1, and is the ﬁrst principal component 
of the dataset shown in the ﬁgure. 
If the largest eigenvalue, c1, is of algebraic multiplicity m1 > 1, we have 
seen that we can choose m1 orthogonal eigenvectors that correspond to c1 
(because SX, being symmetric, is simple). Any one of these vectors may be 
called a ﬁrst principal component of X. 
The second and third principal components, and so on, are likewise de-
termined directly from the nonzero eigenvalues in the spectral decomposition 
of SX. Because the eigenvectors are orthogonal (or can be chosen to be), the 
principal components have the property 
z Subscript i Superscript normal upper T Baseline upper S Subscript upper X Baseline z Subscript j Baseline equals z Subscript i Superscript normal upper T Baseline z Subscript j Baseline equals 0 comma normal f normal o normal r i not equals j periodzT
i SXzj = zT
i zj = 0,
for i /= j.
The full set of principal components of Xc, analogous to Eq. (9.65) except 
that here the random vectors correspond to the rows in Xc, is  
upper Z equals upper X Subscript normal c Baseline upper V commaZ = XcV,
(9.67) 
where V has rX columns. (As before, rX is the rank of X.) 
Principal Components Directly from the Data Matrix 
Formation of the SX matrix emphasizes the role that the sample covariances 
play in principal components analysis. However, there is no reason to form 
a matrix such as  XT 
c Xc, and indeed we may introduce signiﬁcant rounding 
errors by doing so. (Recall our previous discussions of the condition numbers 
of XT X and X.)

9.3 Principal Components
473
Figure 9.2. Principal components 
The singular value decomposition of the n×m matrix Xc yields the square 
roots of the eigenvalues of XT 
c Xc and the same eigenvectors. (The eigenvalues 
of XT 
c Xc are (n − 1) times the eigenvalues of SX.) We will assume that there 
are more observations than variables (i.e., that n > m). In the SVD of the 
centered data matrix Xc = UAV T , U is an n × rX matrix with orthogonal 
columns, V is an m × rX matrix whose ﬁrst rX columns are orthogonal and 
the rest are 0, and A is an rX × rX diagonal matrix whose entries are the 
nonnegative singular values of X − X. (As  before,  rX is the column rank of 
X.) 
The spectral decomposition in terms of the singular values and outer prod-
ucts of the columns of the factor matrices is 
upper X Subscript normal c Baseline equals sigma summation Underscript i Overscript r Subscript upper X Baseline Endscripts sigma Subscript i Baseline u Subscript i Baseline v Subscript i Superscript normal upper T Baseline periodXc =
rX
E
i
σiuivT
i .
(9.68) 
The vectors vi are the same as the eigenvectors of SX. 
Dimension Reduction 
If the columns of a data matrix X are viewed as variables or features that are 
measured for each of several observational units, which correspond to rows 
in the data matrix, an objective in principal components analysis may be to 
determine some small number of linear combinations of the columns of X that 
contain almost as much information as the full set of columns. (Here we are 
not using “information” in a precise sense; in a general sense, it means having 
similar statistical properties.) Instead of a space of dimension equal to the

474
9 Selected Applications in Statistics
(column) rank of X (i.e., rX), we seek a subspace of span(X) with rank less  
than rX that approximates the full space (in some sense). As we discussed on 
page 199, the best approximation in terms of the usual norm (the Frobenius 
norm) of Xc by a matrix of rank p is 
upper X overTilde Subscript p Baseline equals sigma summation Underscript i Overscript p Endscripts sigma Subscript i Baseline u Subscript i Baseline v Subscript i Superscript normal upper T -
Xp =
p
E
i
σiuivT
i
(9.69) 
for some p <  min(n, m). 
Principal components analysis is often used for “dimension reduction” by 
using the ﬁrst few principal components in place of the original data. There 
are various ways of choosing the number of principal components (i.e., p in 
Eq. (9.69)). There are also other approaches to dimension reduction. A general 
reference on this topic is Mizuta (2012). 
9.4 Condition of Models and Data 
In Sect. 5.1, we describe the concept of “condition” of a matrix for certain 
kinds of computations. In Sect. 5.1.3, we discuss how a large condition num-
ber may indicate the level of numerical accuracy in the solution of a system 
of linear equations, and on page 286 we extend this discussion to overdeter-
mined systems such as those encountered in regression analysis. (We return 
to the topic of condition in Sect. 11.2 with even more emphasis on the numer-
ical computations.) The condition of the X matrices has implications for the 
accuracy we can expect in the numerical computations for regression analysis. 
There are other connections between the condition of the data and statis-
tical analysis that go beyond just the purely computational issues. Analysis 
involves more than just computations. Ill-conditioned data also make inter-
pretation of relationships diﬃcult because we may be concerned with both 
conditional and marginal relationships. In ill-conditioned data, the relation-
ships between any two variables may be quite diﬀerent depending on whether 
or not the relationships are conditioned on relationships with other variables 
in the dataset. 
9.4.1 Ill-Conditioning in Statistical Applications 
We have described ill-conditioning heuristically as a situation in which small 
changes in the input data may result in large changes in the solution. Ill-
conditioning in statistical modeling is often the result of high correlations 
among the independent variables. When such correlations exist, the compu-
tations may be subject to severe rounding error. This was a problem in using 
computer software many years ago, as Longley (1967) pointed out. When 
there are large correlations among the independent variables, the model itself 
must be examined, as Beaton et al. (1976) emphasize in reviewing the analy-
sis performed by Longley. Although the work of Beaton, Rubin, and Barone

9.4 Condition of Models and Data
475
was criticized for not paying proper respect to high-accuracy computations, 
ultimately it is the utility of the ﬁtted model that counts, not the accuracy of 
the computations. 
Large correlations are reﬂected in the condition number of the X matrix. 
A large condition number may indicate the possibility of harmful numerical 
errors. Some of the techniques for assessing the accuracy of a computed result 
may be useful. One general way of doing this is to perturb the problem in a 
way that the eﬀect of the perturbation is known, rerun the computations, and 
then compare results. In the regression of y on x1, . . . , xm, we might modify 
the response by adding a known constant d multiple of one or more of the 
independent variables and then regress y + dxj on the original variable. We 
would then compare the results. If the diﬀerences in the computations do not 
correspond to the known diﬀerences, then we know that there were numerical 
errors. 
Some types of ill-conditioning may be rather subtle. Large variations in 
the leverages may be the cause of ill-conditioning. 
Often, numerical problems in regression computations indicate that the 
linear model may not be entirely satisfactory for the phenomenon being stud-
ied. Ill-conditioning in statistical data analysis often means that the approach 
or the model is not appropriate. 
9.4.2 Variable Selection 
Starting with a model such as Eq. (9.4), 
upper Y equals beta Superscript normal upper T Baseline x plus upper E commaY = βTx + E,
we are ignoring the most fundamental problem in data analysis: which vari-
ables are really related to Y , and  how are they related? 
We often begin with the premise that a linear relationship is at least a good 
approximation locally, that is, with restricted ranges of the variables. This 
leaves us with one of the most important tasks in linear regression analysis: 
selection of the variables to include in the model. There are many statistical 
issues that must be taken into consideration. Some aspects of the statistical 
analysis involve tests of linear hypotheses, such as discussed in Sect. 9.2. There  
is a major diﬀerence, however; those tests were based on knowledge of the 
correct model. 
The basic problem in variable selection is that we do not know the correct 
model. Most reasonable procedures to determine the correct model yield bi-
ased statistics. Looking at a variable and then making a decision to exclude 
that variable from the model can bias further analyses. On the other hand, if 
a variable that has nonzero correlation with the response is omitted from the 
model, the estimators may be biased by this omission. This is called omitted 
variable bias. 
We generally approach the variable selection problem by writing the model 
with the data as

476
9 Selected Applications in Statistics
y equals upper X Subscript normal i Baseline beta Subscript normal i Baseline plus upper X Subscript normal o Baseline beta Subscript normal o Baseline plus epsilon commay = Xiβi + Xoβo + e,
(9.70) 
where Xi and Xo are matrices that form some permutation of the columns of 
X, Xi|Xo = X, and  βi and βo are vectors consisting of corresponding elements 
from β. (The i and o are “in” and “out.”) We then consider the model 
y equals upper X Subscript normal i Baseline beta Subscript normal i Baseline plus epsilon Subscript normal i Baseline periody = Xiβi + ei.
(9.71) 
It is interesting to note that the least squares estimate of βi in the 
model (9.71) is the same as the least squares estimate in the model 
ModifyingAbove y With caret Subscript normal i normal o Baseline equals upper X Subscript normal i Baseline beta Subscript normal i Baseline plus epsilon Subscript normal i Baseline commaˆyio = Xiβi + ei,
where ˆyio is the vector of predicted values obtained by ﬁtting the full 
model (9.70). An interpretation of this fact is that ﬁtting the model (9.71) 
that includes only a subset of the variables is the same as using that subset 
to approximate the predictions of the full model. The fact itself can be seen 
from the normal equations associated with these two models. We have 
upper X Subscript normal i Superscript normal upper T Baseline upper X left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline equals upper X Subscript normal i Superscript normal upper T Baseline periodXT
i X(XTX)−1XT = XT
i .
(9.72) 
This follows from the fact that X(XT X)−1 XT is a projection matrix, and Xi 
consists of a set of columns of X (see Sect. 8.5 and Exercise 9.8 on page 519). 
As mentioned above, there are many diﬃcult statistical issues in the vari-
able selection problem. The exact methods of statistical inference generally 
do not apply (because they are based on a model, and we are trying to choose 
a model). In variable selection, as in any statistical analysis that involves the 
choice of a model, the eﬀect of the given dataset may be greater than war-
ranted, resulting in overﬁtting. One way of dealing with this kind of problem is 
to use part of the dataset for ﬁtting and part for validation of the ﬁt. There are 
many variations on exactly how to do this, but in general, “cross-validation” 
is an important part of any analysis that involves building a model. 
The computations involved in variable selection are the same as those 
discussed in Sect. 9.1.8. 
9.4.3 Principal Components Regression 
A somewhat diﬀerent approach to the problem of variable selection involves 
selecting some linear combinations of all of the variables. The ﬁrst p princi-
pal components of X cover the space of span(X) optimally (in some sense), 
and so these linear combinations themselves may be considered as the “best” 
variables to include in a regression model. If Vp is the ﬁrst p columns from V 
in the full set of principal components of X, Eq. (9.67), we use the regression 
model 
y almost equals upper Z Subscript p Baseline gamma commay ≈Zpγ,
(9.73) 
where

9.4 Condition of Models and Data
477
upper Z Subscript p Baseline equals upper X upper V Subscript p Baseline periodZp = XVp.
(9.74) 
This is the idea of principal components regression. 
In principal components regression, even if p < m  (which is the case, of 
course; otherwise principal components regression would make no sense), all 
of the original variables are included in the model. Any linear combination 
forming a principal component may include all of the original variables. The 
weighting on the original variables tends to be such that the coeﬃcients of 
the original variables that have extreme values in the ordinary least squares 
regression are attenuated in the principal components regression using only 
the ﬁrst p principal components. 
The principal components do not involve y, so it may  not be obvious  that  a  
model using only a set of principal components selected without reference to y 
would yield a useful regression model. Indeed, sometimes important indepen-
dent variables do not get suﬃcient weight in principal components regression. 
9.4.4 Shrinkage Estimation 
As mentioned in the previous section, instead of selecting speciﬁc independent 
variables to include in the regression model, we may take the approach of 
shrinking the coeﬃcient estimates toward zero. This of course has the eﬀect of 
introducing a bias into the estimates (in the case of a true model being used), 
but in the process of reducing the inherent instability due to collinearity in 
the independent variables, it may also reduce the mean squared error of linear 
combinations of the coeﬃcient estimates. This is one approach to the problem 
of overﬁtting. 
The shrinkage can also be accomplished by a regularization of the ﬁtting 
criterion. If the ﬁtting criterion is minimization of a norm of the residuals, we 
add a norm of the coeﬃcient estimates to minimize 
parallel to r left parenthesis b right parenthesis parallel to Subscript f Baseline plus lamda parallel to b parallel to Subscript b Baseline comma||r(b)||f + λ||b||b,
(9.75) 
where λ is a tuning parameter that allows control over the relative weight 
given to the two components of the objective function. This regularization is 
also related to the variable selection problem by the association of superﬂuous 
variables with the individual elements of the optimal b that are close to zero. 
Ridge Regression 
If the ﬁtting criterion is least squares, we may also choose an L2 norm on b, 
and we have the ﬁtting problem 
min Underscript b Endscripts left parenthesis left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis plus lamda parallel to b parallel to Subscript 2 Baseline right parenthesis period min
b
(
(y −Xb)T(y −Xb) + λ||b||2
)
.
(9.76) 
This is called Tikhonov regularization (from A. N. Tikhonov) or ridge regres-
sion, and it is by far the most commonly used regularization. This minimiza-
tion problem yields the modiﬁed normal equations

478
9 Selected Applications in Statistics
left parenthesis upper X Superscript normal upper T Baseline upper X plus lamda upper I right parenthesis b equals upper X Superscript normal upper T Baseline y comma(XTX + λI)b = XTy,
(9.77) 
obtained by adding λI to the sums of squares and cross-products matrix. This 
is the ridge regression we discussed on page 401, and as we saw in Sect. 5.1, the  
addition of this positive deﬁnite matrix has the eﬀect of reducing numerical 
ill-conditioning. 
Rather than strictly minimizing this expression, we could formulate a con-
strained optimization problem 
min Underscript parallel to b parallel to Subscript 2 Baseline less than t Endscripts left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis comma min
||b||2<t(y −Xb)T(y −Xb),
for some tuning constant t. This is a quadratic programming problem. 
Interestingly, the normal equations (9.77) correspond to a least squares 
approximation for 
Start 3 By 1 Matrix 1st Row y 2nd Row Blank 3rd Row 0 EndMatrix almost equals Start 3 By 1 Matrix 1st Row upper X 2nd Row Blank 3rd Row StartRoot lamda EndRoot upper I EndMatrix beta period
⎛
⎝
y
0
⎞
⎠≈
⎡
⎣
X
√
λI
⎤
⎦β.
(9.78) 
(See Exercise 9.7.) The shrinkage toward 0 is evident in this formulation. 
Because of this, we say the “eﬀective” degrees of freedom of a ridge regres-
sion model decrease with increasing λ. In Eq. (8.61), we formally deﬁned the 
eﬀective model degrees of freedom of any linear ﬁt 
ModifyingAbove y With caret equals upper S Subscript lamda Baseline y-y = Sλy
as 
normal t normal r left parenthesis upper S Subscript lamda Baseline right parenthesis commatr(Sλ),
and we saw in Eq. (8.62) that indeed it does decrease with increasing λ. 
Even if all variables are left in the model, the ridge regression approach 
may alleviate some of the deleterious eﬀects of collinearity in the independent 
variables. 
Lasso Regression 
The norm for the regularization in expression (9.75) does not  have  to  be  
the same as the norm applied to the model residuals. An alternative ﬁtting 
criterion, for example, is to use an L1 norm in expression (9.76), 
min Underscript b Endscripts left parenthesis left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis plus lamda parallel to b parallel to Subscript 1 Baseline right parenthesis period min
b
(
(y −Xb)T(y −Xb) + λ||b||1
)
.
(9.79) 
Rather than strictly minimizing this expression, we can formulate a con-
strained optimization problem 
min Underscript parallel to b parallel to Subscript 1 Baseline less than t Endscripts left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis comma min
||b||1<t(y −Xb)T(y −Xb),
(9.80)

9.4 Condition of Models and Data
479
for some tuning constant t. The solution of this quadratic programming prob-
lem yields a b with some elements identically 0, depending on t. As  t de-
creases, more elements of the optimal b are identically 0, and thus this is an 
eﬀective method for variable selection. The use of expression (9.80) is called 
lasso regression. (“Lasso” stands for “least absolute shrinkage and selection 
operator.”) 
Lasso regression is computationally expensive if several values of t are 
explored. Efron et al. (2004) propose “least angle regression” (LAR), the steps 
of which eﬀectively yield the entire lasso regularization path. 
Elastic Net 
An obvious modiﬁcation for regularization of the least squares ﬁt is a com-
bination of the penalties in ridge or Tikhonov shrinkage, Eq. 9.76, and lasso 
shrinkage, Eq. 9.79. The two penalties can be adjusted independently in an 
“elastic net” penalty, 
lamda 1 parallel to b parallel to Subscript 1 Baseline plus lamda 2 parallel to b parallel to Subscript 2 Baseline commaλ1||b||1 + λ2||b||2,
(9.81) 
for λ1, λ2 ≥0. 
9.4.5 Statistical Inference About the Rank of a Matrix 
An interesting problem in numerical linear algebra is to approximate the rank 
of a given matrix. A related problem in statistical inference is to estimate or 
to test a hypothesis concerning the rank of an unknown matrix. For example, 
in the multivariate regression model discussed in Sect. 9.1.10 (beginning on 
page 456), we may wish to test whether the coeﬃcient matrix B is of full 
rank. 
In statistical inference, we use observed data to make inferences about a 
model, but we do not “estimate” or “test a hypothesis” concerning the rank 
of a given matrix of data. 
Numerical Approximation and Statistical Inference 
The rank of a matrix is not a continuous function of the elements of the matrix. 
It is often diﬃcult to compute the rank of a given matrix; hence, we often seek 
to approximate the rank. We alluded to the problem of approximating the rank 
of a matrix on page 241 and indicated that a QR factorization of the given 
matrix might be an appropriate approach to the problem. (In Sect. 11.4, we  
discuss the rank-revealing QR (or LU) method for approximating the rank of 
a matrix.)  
The SVD  can also be used to approximate the  rank  of  a given  n × m 
matrix. The approximation would be based on a decision that either the rank 
is min(n, m) or that the rank is r because di = 0  for  i > r  in the decomposition 
UDV T given in Eq. (3.299) on page 183.

480
9 Selected Applications in Statistics
Although we sometimes refer to the problem as one of “estimating the 
rank of a matrix,” “estimation” in the numerical-analytical sense refers to 
“approximation,” rather than to statistical estimation. This is an important 
distinction that is often lost. Estimation and testing in a statistical sense do 
not apply to a given entity; these methods of inference apply to properties 
of a random variable. We use observed realizations of the random variable to 
make inferences about unobserved properties or parameters that describe the 
distribution of the random variable. 
A statistical test is a decision rule for rejection of a hypothesis, about 
which empirical evidence is available. The empirical evidence consists of ob-
servations on some random variable, and the hypothesis is a statement about 
the distribution of the random variable. In simple cases of hypothesis testing, 
the distribution is assumed to be characterized by a parameter, and the hy-
pothesis merely speciﬁes the value of that parameter. The statistical test is 
based on the distribution of the underlying random variable if the hypothesis 
is true. 
Statistical Tests of the Rank of a Class of Matrices 
Most common statistical tests involve hypotheses concerning a scalar param-
eter. We have encountered two examples that involve tests of hypotheses con-
cerning matrix parameters. One involved tests of the variance-covariance ma-
trix Σ in a multivariate distribution (Exercise 7.9 on page 361), and the other 
was for tests of the coeﬃcient matrix B in multivariate linear regression (see 
Sect. 9.2.6, page 468). The tests of the variance-covariance matrix are based 
on a Wishart matrix W, but for a speciﬁc hypothesis, the test statistic is 
a chi-squared statistic. In the multivariate linear regression testing problem, 
the least squares estimator of the coeﬃcient matrix, which has a matrix nor-
mal distribution, is used to form two matrices that have independent Wishart 
distributions. The hypotheses of interest are that certain elements of the coef-
ﬁcient matrix are zero, and the test statistics involve functions of the Wishart 
matrices, such as Wilk’s Λ, which is the ratio of the determinants. 
In multivariate linear regression, given n observations on the vectors y and 
x, we use the model for the data given in Eq. (9.30), on page 456, 
upper Y equals upper X upper B plus upper E commaY = XB + E,
where Y is an n×d matrix and X is an n×m matrix of observations, B is an 
m×d unknown matrix, and E is an n×d matrix of n unobserved realizations of 
a d-variate random variable. The canonical problem in statistical applications 
is to test whether B = 0, that is, whether there is any linear relationship 
between y and x. A related but less encompassing question is whether B is of 
full rank. (If B = 0, its rank is zero.) Testing whether B is of full rank is similar 
to the familiar univariate statistical problem of testing if some elements of β 
in the model y = xT β + e are zero. In the multivariate case, this is sometimes

9.4 Condition of Models and Data
481
referred to as the “reduced rank regression” problem. The null hypothesis of 
interest is 
upper H 0 colon normal r normal a normal n normal k left parenthesis upper B right parenthesis less than or equals min left parenthesis m comma d right parenthesis minus 1 periodH0 : rank(B) ≤min(m, d) −1.
One approach is to test sequentially the null hypotheses H0i : rank(B) =  i 
for i = 1, . . . min(m, d) − 1. 
The other problem referred to above is, given n d-vectors y1, . . . yn assumed 
to be independent realizations of random vectors distributed as Nd(μ, Σ), to 
test whether Σ is of full rank (i.e., whether the multivariate normal distribu-
tion is singular; see page 347). 
In other applications, in vector stochastic processes, the matrix of interest 
is one that speciﬁes the relationship of one time series to another. In such 
applications the issue of stationarity is important and may be one of the 
reasons for performing the rank test. 
The appropriate statistical models in these settings are diﬀerent, and the 
forms of the models in the diﬀerent applications aﬀect the distributions of any 
test statistics. The nature of the subject of the hypothesis, that is, the rank of 
a matrix, poses some diﬃculty. Much of the statistical theory on hypothesis 
testing involves an open parameter space over a dense set of reals, but of course 
the rank is an integer. Because of this, even if for no other reason, we would 
not expect to be able to work out an exact distribution of any estimator or 
test statistic for the rank. At best we would seek an estimator or test statistic 
for which we could derive, or at least approximate, an asymptotic distribution. 
Problems of testing the rank of a matrix have been addressed in the sta-
tistical literature for some time; see, for example, Anderson (1951), Gill and 
Lewbel (1992), and Cragg and Donald (1996). They have also been discussed 
frequently in econometric applications; see, for example, Robin and Smith 
(2000) and Kleibergen and Paap (2006). In some of the literature, it is not 
clear whether or not the authors are describing a test for the rank of a given 
matrix, which, as pointed out above, is not a statistical procedure, even if a 
“test statistic” and a “null probability distribution” are involved. 
My purpose in this section is not to review the various approaches to 
statistical inference about the rank of matrices or to discuss the “best” tests 
under various scenarios, but rather to describe one test in order to give the 
ﬂavor of the approaches. 
Statistical Tests of the Rank Based on an LDU Factorization 
Gill and Lewbel (1992) and Cragg and Donald (1996) describe tests of the 
rank of a matrix that use factors from an LDU factorization. For an m × d 
matrix Θ, the tests are of the null hypothesis H0 : rank(Θ) =  r, where  
r <  min(m, d). (There are various ways an alternative hypothesis could be 
phrased, but we will not specify one here.) 
We ﬁrst decompose the unknown matrix Θ as in Eq. (4.30), using per-
mutation matrices so that the diagonal elements of D are nonincreasing in 
absolute value: E(π1)ΘE(π2) = LDU.

482
9 Selected Applications in Statistics
The m × d matrix Θ (with m ≥ d without loss of generality) can be 
decomposed as 
StartLayout 1st Row 1st Column upper E Subscript left parenthesis pi 1 right parenthesis Baseline normal upper Theta upper E Subscript left parenthesis pi 2 right parenthesis 2nd Column equals 3rd Column upper L upper D upper U 2nd Row 1st Column Blank 2nd Column equals 3rd Column Start 3 By 3 Matrix 1st Row 1st Column upper L 11 2nd Column 0 3rd Column 0 2nd Row 1st Column upper L 21 2nd Column upper L 22 3rd Column 0 3rd Row 1st Column upper L 31 2nd Column upper L 32 3rd Column upper I Subscript m minus d Baseline EndMatrix Start 3 By 3 Matrix 1st Row 1st Column upper D 1 2nd Column 0 3rd Column 0 2nd Row 1st Column 0 2nd Column upper D 2 3rd Column 0 3rd Row 1st Column 0 2nd Column 0 3rd Column 0 EndMatrix Start 3 By 2 Matrix 1st Row 1st Column upper U 11 2nd Column upper U 12 2nd Row 1st Column 0 2nd Column upper U 22 3rd Row 1st Column 0 2nd Column 0 EndMatrix comma EndLayoutE(π1)ΘE(π2) = LDU
=
⎡
⎣
L11
0
0
L21 L22
0
L31 L32 Im−d
⎤
⎦
⎡
⎣
D1 0 0
0 D2 0
0
0 0
⎤
⎦
⎡
⎣
U11 U12
0
U22
0
0
⎤
⎦,
(9.82) 
where the unknown matrices L11, U11, and  D1 are r × r, and  the elements of  
the diagonal submatrices D1 and D2 are arranged in nonincreasing order. If 
the rank of Θ is r, D2 = 0, but no diagonal element of D1 is 0. 
For a statistical test of the rank of Θ, we need to identify an observable 
random variable (random vector) whose distribution depends on Θ. To pro-
ceed, we take a sample of realizations of this random variable. We let -Θ be 
an estimate of Θ based on n such realizations, and assume the central limit 
property, 
StartRoot k EndRoot normal v normal e normal c left parenthesis ModifyingAbove normal upper Theta With caret minus normal upper Theta right parenthesis right arrow Subscript normal d Baseline normal upper N left parenthesis 0 comma upper V right parenthesis comma
√
k vec( -Θ −Θ) →d N(0, V ),
(9.83) 
where V is nm×nm and positive deﬁnite. (For example, if B is the coeﬃcient 
matrix in the multivariate linear regression model (9.30) and -B is the least 
squares estimator from expression (9.32), then -B and B have this property. 
If Σ is the variance-covariance matrix in the multivariate normal distribu-
tion, expression (7.39), then we use the sample variance-covariance matrix, 
Eq. (8.67), page 404; but in this case, the analogous asymptotic distribution 
relating -Σ and Σ is a Wishart distribution.) 
Now if D2 = 0 (i.e., if Θ has rank r) and -Θ is decomposed in the same 
way as Θ in Eq. (9.82), then 
StartRoot k EndRoot normal d normal i normal a normal g left parenthesis ModifyingAbove upper D With caret Subscript 2 Baseline right parenthesis right arrow Subscript normal d Baseline normal upper N left parenthesis 0 comma upper W right parenthesis
√
k diag( -D2) →d N(0, W)
for some positive deﬁnite matrix W, and the quantity 
n ModifyingAbove d With caret Subscript 2 Superscript normal upper T Baseline upper W Superscript negative 1 Baseline ModifyingAbove d With caret Subscript 2 Baseline comman-dT
2 W −1 -d2,
(9.84) 
where 
ModifyingAbove d With caret Subscript 2 Baseline equals normal d normal i normal a normal g left parenthesis ModifyingAbove upper D With caret Subscript 2 Baseline right parenthesis-d2 = diag( -D2)
has an asymptotic chi-squared distribution with (m −r) degrees of freedom. 
If a consistent and independent estimator of W, say -
W, is used in place  of  W 
in the expression (9.84), this would be a test statistic for the hypothesis that 
the rank of Θ is r. (Note that W is m − r × m − r.) 
Gill and Lewbel (1992) derive a consistent estimator to use in expres-
sion (9.84) as a test statistic. Following their derivation, ﬁrst let -V be a con-
sistent estimator of V . (It would typically be a sample variance-covariance 
matrix.) Then 
left parenthesis ModifyingAbove upper Q With caret Superscript normal upper T Baseline circled times ModifyingAbove upper P With caret right parenthesis ModifyingAbove upper V With caret left parenthesis ModifyingAbove upper Q With caret circled times ModifyingAbove upper P With caret Superscript normal upper T Baseline right parenthesis
(
-QT ⊗-P
)
-V
(
-Q ⊗-P T)

9.4 Condition of Models and Data
483
is a consistent estimator of the variance-covariance of vec( -P( -Θ −Θ) -Q). Next, 
deﬁne the matrices 
ModifyingAbove upper H With caret equals left bracket minus ModifyingAbove upper L With caret Subscript 22 Superscript negative 1 Baseline ModifyingAbove upper L With caret Subscript 21 Baseline ModifyingAbove upper L With caret Subscript 11 Superscript negative 1 Baseline StartAbsoluteValue ModifyingAbove upper L With caret Subscript 22 Superscript negative 1 Baseline EndAbsoluteValue 0 right bracket comma-H =
⎡
−-L−1
22 -L21-L−1
11
||| -L−1
22
||| 0
⎤
,
ModifyingAbove upper K With caret equals StartBinomialOrMatrix minus ModifyingAbove upper U With caret Subscript 11 Superscript negative 1 Baseline ModifyingAbove upper U With caret Subscript 12 Baseline ModifyingAbove upper U With caret Subscript 22 Superscript negative 1 Baseline Choose ModifyingAbove upper U With caret Subscript 22 Superscript negative 1 Baseline EndBinomialOrMatrix comma-K =
⎡
−-U −1
11 -U12 -U −1
22
-U −1
22
⎤
,
and T such that 
normal v normal e normal c left parenthesis ModifyingAbove upper D With caret Subscript 2 Baseline right parenthesis equals upper T ModifyingAbove d With caret Subscript 2 Baseline periodvec( -D2) = T -d2.
The matrix T is (m −r)2 × (m −r), consisting of a stack of square matrices 
with 0s in all positions except for a 1 in one diagonal element. The matrix is 
orthogonal; that is, 
upper T Superscript normal upper T Baseline upper T equals upper I Subscript m minus r Baseline periodT TT = Im−r.
The matrix 
left parenthesis ModifyingAbove upper K With caret circled times ModifyingAbove upper H With caret Superscript normal upper T Baseline right parenthesis upper T( -K ⊗-HT)T
transforms vec( -P( -Θ−Θ) -Q) into -d2; hence, the variance-covariance estimator, 
( -QT ⊗-P)-V ( -Q ⊗-P T ), is adjusted by this matrix. The estimator -
W therefore 
is given by 
ModifyingAbove upper W With caret equals upper T Superscript normal upper T Baseline left parenthesis ModifyingAbove upper K With caret Superscript normal upper T Baseline circled times ModifyingAbove upper H With caret right parenthesis left parenthesis ModifyingAbove upper Q With caret Superscript normal upper T Baseline circled times ModifyingAbove upper P With caret right parenthesis ModifyingAbove upper V With caret left parenthesis ModifyingAbove upper Q With caret circled times ModifyingAbove upper P With caret Superscript normal upper T Baseline right parenthesis left parenthesis ModifyingAbove upper K With caret circled times ModifyingAbove upper H With caret Superscript normal upper T Baseline right parenthesis upper T period-
W = T T( -KT ⊗-H)( -QT ⊗-P)-V ( -Q ⊗-P T)( -K ⊗-HT)T.
The test statistic is 
n ModifyingAbove d With caret Subscript 2 Superscript normal upper T Baseline ModifyingAbove upper W With caret Superscript negative 1 Baseline ModifyingAbove d With caret Subscript 2 Baseline comman-dT
2 -
W −1 -d2,
(9.85) 
with an approximate chi-squared distribution with (m−r) degrees of freedom. 
Cragg and Donald (1996), however, have pointed out that the indetermi-
nacy of the LDU decomposition casts doubts on the central limiting distribu-
tion in (9.83). Kleibergen and Paap (2006) proposed a related test for a certain 
class of matrices based on the SVD. Because the SVD is unique (within the 
limitations mentioned on page 184), it does not suﬀer from the indeterminacy. 
9.4.6 Incomplete Data 
Missing values in a dataset can not only result in ill-conditioned problems but 
can cause some matrix statistics to lack their standard properties, such as 
covariance or correlation matrices formed from the available data not being 
positive deﬁnite. 
In the standard ﬂat data ﬁle represented in Fig. 8.1, where a row holds 
data from a given observation and a column represents a speciﬁc variable or 
feature, it is often the case that some values are missing for some observa-
tion/variable combination. This can occur for various reasons, such as a failure

484
9 Selected Applications in Statistics
of a measuring device, refusal to answer a question in a survey, or an indeter-
minate or inﬁnite value for a derived variable (e.g., a coeﬃcient of variation 
when the mean is 0). This causes problems for our standard storage of data 
in a matrix. The values for some cells are not available. 
The need to make provisions for missing data is one of the important 
diﬀerences between statistical numerical processing and ordinary numerical 
analysis. First of all, we need a method for representing a “not available” 
(NA) value, and then we need a mechanism for avoiding computations with 
this NA value. There are various ways of doing this, including the use of 
special computer numbers (see pages 530 and 541). 
The layout of the data may be of the form 
upper X equals Start 4 By 3 Matrix 1st Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper N monospace upper A 2nd Row 1st Column monospace upper X 2nd Column monospace upper N monospace upper A 3rd Column monospace upper N monospace upper A 3rd Row 1st Column monospace upper X 2nd Column monospace upper N monospace upper A 3rd Column monospace upper X 4th Row 1st Column monospace upper X 2nd Column monospace upper X 3rd Column monospace upper X EndMatrix periodX =
⎡
⎢⎢⎣
X X NA
X NA NA
X NA X
X X
X
⎤
⎥⎥⎦.
(9.86) 
In  the data matrix of Eq. (9.86), all rows could be used for summary statistics 
relating to the ﬁrst variable, but only two rows could be used for summary 
statistics relating to the second and third variables. For summary statistics 
such as the mean or variance for any one variable, it would seem to make 
sense to use all of the available data. 
The picture is not so clear, however, for statistics on two variables, such as 
the covariance. If all observations that contain data on both variables are used 
for computing the covariance, then the covariance matrix may not be positive 
deﬁnite. If the correlation matrix is computed using covariances computed in 
this way but variances computed on all of the data, some oﬀ-diagonal elements 
may be larger than 1. If the correlation matrix is computed using covariances 
from all available pairs and variances computed only from the data in complete 
pairs (i.e., the variances used in computing correlations involving a given 
variable are diﬀerent for diﬀerent variables), then no oﬀ-diagonal element can 
be larger than 1, but the correlation matrix may not be nonnegative deﬁnite. 
An alternative, of course, is to use only data in records that are complete. 
This is called “casewise deletion,” whereas the use of all available data for 
bivariate statistics is called “pairwise deletion.” One must be very careful in 
computing bivariate statistics from data with missing values; see Exercise 9.9. 
Estimated or approximate variance-covariance or correlation matrices that 
are not positive deﬁnite can arise in other ways in applications. For example, 
the data analyst may have an estimate of the correlation matrix that was not 
based on a single sample. 
Various approaches to handling an approximate correlation matrix that is 
not positive deﬁnite have been considered. Devlin, Gnanadesikan, and Ket-
tenring (1975) describe a method of shrinking the given R toward a chosen 
positive deﬁnite matrix, R1, which may be an estimator of a correlation matrix 
computed in other ways (perhaps a robust estimator) or may just be chosen

9.4 Condition of Models and Data
485
arbitrarily; for example, R1 may just be the identity matrix. The method is 
to choose the largest value α in [0, 1] such that the matrix 
upper R overTilde equals alpha upper R plus left parenthesis 1 minus alpha right parenthesis upper R 1 -R = αR + (1 −α)R1
(9.87) 
is positive deﬁnite. This optimization problem can be solved iteratively start-
ing with α = 1 and decreasing α in small steps while checking whether -R is 
positive deﬁnite. (The checks may require several computations.) A related 
method is to use a modiﬁed Cholesky decomposition. If the symmetric matrix 
S is not positive deﬁnite, a diagonal matrix D can be determined so that 
S + D is positive deﬁnite. Eskow and Schnabel (1991), for example, describe 
one way to determine D with values near zero and to compute a Cholesky 
decomposition of S + D. 
Devlin et al. (1975) also describe nonlinear shrinking methods in which 
all of the oﬀ-diagonal elements rij are replaced iteratively, beginning with 
r (0) 
ij = rij and proceeding with 
r Subscript i j Superscript left parenthesis k right parenthesis Baseline equals StartLayout Enlarged left brace 1st Row 1st Column f Superscript negative 1 Baseline left parenthesis f left parenthesis r Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis plus delta right parenthesis 2nd Column normal i f r Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline less than minus f Superscript negative 1 Baseline left parenthesis delta right parenthesis 2nd Row 1st Column Blank 3rd Row 1st Column 0 2nd Column normal i f StartAbsoluteValue r Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline EndAbsoluteValue less than or equals f Superscript negative 1 Baseline left parenthesis delta right parenthesis 4th Row 1st Column Blank 5th Row 1st Column f Superscript negative 1 Baseline left parenthesis f left parenthesis r Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis minus delta right parenthesis 2nd Column normal i f r Subscript i j Superscript left parenthesis k minus 1 right parenthesis Baseline greater than f Superscript negative 1 Baseline left parenthesis delta right parenthesis EndLayoutr(k)
ij
=
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
f −1 (
f
(
r(k−1)
ij
)
+ δ
)
if r(k−1)
ij
< −f −1(δ)
0
if
|||r(k−1)
ij
||| ≤f −1(δ)
f −1 (
f
(
r(k−1)
ij
)
−δ
)
if r(k−1)
ij
> f −1(δ)
(9.88) 
for some invertible positive-valued function f and some small positive con-
stant δ (e.g., 0.05). The function f may be chosen in various ways; one sug-
gested function is the hyperbolic tangent, which makes f −1 Fisher’s variance-
stabilizing function for a correlation coeﬃcient; see Exercise 9.25b. 
Rousseeuw and Molenberghs (1993) suggest a method in which some ap-
proximate correlation matrices can be adjusted to a nearby correlation matrix, 
where closeness is determined by the Frobenius norm. Their method applies to 
pseudo-correlation matrices. Recall that any symmetric nonnegative deﬁnite 
matrix with ones on the diagonal is a correlation matrix. A pseudo-correlation 
matrix is a symmetric matrix R with positive diagonal elements (but not nec-
essarily 1s) and such that r2 
ij ≤ riirjj. (This is inequality (8.12), which is a 
necessary but not suﬃcient condition for the matrix to be nonnegative deﬁ-
nite.) 
The method of Rousseeuw and Molenberghs adjusts an m × m pseudo-
correlation matrix R to the closest correlation matrix -R, where closeness is 
determined by the Frobenius norm; that is, we seek -R such that 
parallel to upper R minus upper R overTilde parallel to Subscript normal upper F||R −-R||F
(9.89) 
is minimum over all choices of -R that are correlation matrices (i.e., matrices 
with 1s on the diagonal that are positive deﬁnite). The solution to this opti-
mization problem is not as easy as the solution to the problem we consider on

486
9 Selected Applications in Statistics
page 199 of ﬁnding the best approximate matrix of a given rank. Rousseeuw 
and Molenberghs describe a computational method for ﬁnding -R to minimize 
expression (9.89). A correlation matrix -R can be formed as a Gramian matrix 
formed from a matrix U whose columns, u1, . . . , um, are normalized vectors, 
where 
r overTilde Subscript i j Baseline equals u Subscript i Superscript normal upper T Baseline u Subscript j Baseline period˜rij = uT
i uj.
If we choose the vector ui so that only the ﬁrst i elements are nonzero, then 
they form the Cholesky factor elements of -R with nonnegative diagonal ele-
ments, 
upper R overTilde equals upper U Superscript normal upper T Baseline upper U comma-R = U TU,
and each ui can be completely represented in IRi . We can associate the m(m− 
1)/2 unknown elements of U with the angles in their spherical coordinates. In 
ui, the  jth element is 0 if j >  i  and otherwise is 
sine left parenthesis theta Subscript i Baseline 1 Baseline right parenthesis midline horizontal ellipsis sine left parenthesis theta Subscript i comma i minus j Baseline right parenthesis cosine left parenthesis theta Subscript i comma i minus j plus 1 Baseline right parenthesis commasin(θi1) · · · sin(θi,i−j) cos(θi,i−j+1),
where θi1, . . . , θi,i−j, θi,i−j+1 are the unknown angles that are the variables in 
the optimization problem for the Frobenius norm (9.89). The problem now is 
to solve 
min sigma summation Underscript i equals 1 Overscript m Endscripts sigma summation Underscript j equals 1 Overscript i Endscripts left parenthesis r Subscript i j Baseline minus sine left parenthesis theta Subscript i Baseline 1 Baseline right parenthesis midline horizontal ellipsis sine left parenthesis theta Subscript i comma i minus j Baseline right parenthesis cosine left parenthesis theta Subscript i comma i minus j plus 1 Baseline right parenthesis right parenthesis squared period min
m
E
i=1
i
E
j=1
(rij −sin(θi1) · · · sin(θi,i−j) cos(θi,i−j+1))2.
(9.90) 
This optimization problem is well-behaved and can be solved by steepest de-
scent (see page  496). Rousseeuw and Molenberghs (1993) also mention that 
a weighted least squares problem in place of Eq. (9.90) may be more appro-
priate if the elements of the pseudo-correlation matrix R result from diﬀerent 
numbers of observations. 
In Exercise 9.22, we describe another way of converting an approximate 
correlation matrix that is not positive deﬁnite into a correlation matrix by 
iteratively replacing negative eigenvalues with positive ones. 
9.5 Stochastic Processes 
Many stochastic processes are modeled by a “state vector” and rules for up-
dating the state vector through a sequence of discrete steps. At time t, the  
elements of the state vector xt are values of various characteristics of the sys-
tem. A model for the stochastic process is a probabilistic prescription for xta 
in terms of xtb , where  ta > tb; that is, given observations on the state vec-
tor prior to some point in time, the model gives probabilities for, or predicts 
values of, the state vector at later times. 
A stochastic process is distinguished in terms of the countability of the 
space of states, X, and the index of the state (i.e., the parameter space, T ).

9.5 Stochastic Processes
487
If the parameter space is continuous (uncountable), the process is called a 
diﬀusion process. If the parameter space is countable, we usually consider it 
to consist of the nonnegative integers. 
If the properties of a stochastic process do not depend on the index, the 
process is said to be stationary. If the properties also do not depend on any 
initial state, the process is said to be time homogeneous or homogeneous with 
respect to the parameter space. (We usually refer to such processes simply as 
“homogeneous.”) 
9.5.1 Markov Chains 
The Markov (or Markovian) property in a stochastic process is the condition in 
which the current state does not depend on any states prior to the immediately 
previous state; that is, the process is memoryless. If the transitions occur at 
discrete intervals, the Markov property is the condition where the probability 
distribution of the state at time t + 1 depends only on the state at time t. 
In what follows, we will brieﬂy consider some Markov processes in which 
the set of states is countable and the transitions occur at discrete intervals 
(discrete times). Such a process is called a Markov chain. (Some authors’ 
use of the term “Markov chain” allows the state space to be continuous, and 
others’ allows time to be continuous; here we are not deﬁning the term. We 
will be concerned with only a subclass of Markov chains, whichever way they 
are deﬁned. The models for this subclass are easily formulated in terms of 
vectors and matrices.) 
If the state space is countable, it is equivalent to X = {1, 2, . . .}. If  X is a 
random variable from some sample space to X, and  
pi Subscript i Baseline equals normal upper P normal r left parenthesis upper X equals i right parenthesis commaπi = Pr(X = i),
then the vector π deﬁnes a distribution of X on X. (A vector of nonnegative 
numbers that sum to 1 is a distribution.) 
Formally, we deﬁne a Markov chain (of random variables) X0, X1, . . .  in 
terms of an initial distribution π and a conditional distribution for Xt+1 given 
Xt. Let  X0 have distribution π, and  given  Xt = i, let  Xt+1 have distribution 
(pij; j ∈X); that is, pij is the probability of a transition from state i at time 
t to state j at time t + 1.  Let  
upper P equals left parenthesis p Subscript i j Baseline right parenthesis periodP = (pij).
This square matrix is called the transition matrix of the chain. It is clear that 
P is a stochastic matrix (it is nonnegative and the elements in any row sum to 
1), and hence ρ(P) = ||P||∞ = 1,  and  (1, 1) is an eigenpair of P (see page 417). 
If P does not depend on the time (and our notation indicates that we are 
assuming this), the Markov chain is stationary. 
The initial distribution π and the transition matrix P characterize the 
chain, which we sometimes denote as Markov(π, P).

488
9 Selected Applications in Statistics
If the set of states is countably inﬁnite, the vectors and matrices have 
inﬁnite order; that is, they have “inﬁnite dimension.” (Note that this use of 
“dimension” is diﬀerent from our standard deﬁnition that is based on linear 
independence.) 
We denote the distribution at time t by π(t) and hence often write the 
initial distribution as π(0) . A distribution at time t can be expressed in terms 
of π and P if we extend the deﬁnition of (Cayley) matrix multiplication in 
Eq. (3.48) in the obvious way to handle any countable number of elements so 
that PP or P 2 is the matrix deﬁned by 
left parenthesis upper P squared right parenthesis Subscript i j Baseline equals sigma summation Underscript k element of script upper X Endscripts p Subscript i k Baseline p Subscript k j Baseline period(P 2)ij =
E
k∈X
pikpkj.
We see immediately that 
pi Superscript left parenthesis t right parenthesis Baseline equals left parenthesis upper P Superscript t Baseline right parenthesis Superscript normal upper T Baseline pi Superscript left parenthesis 0 right parenthesis Baseline periodπ(t) = (P t)Tπ(0).
(9.91) 
Because of Eq. (9.91), P t is often called the t-step transition matrix. (The  
somewhat awkward notation with the transpose results from the historical 
convention in Markov chain theory of expressing distributions as “row vec-
tors.”) 
Properties of Markov Chains 
The transition matrix determines various relationships among the states of a 
Markov chain. State j is said to be accessible from state i if it can be reached 
from state i in a ﬁnite number of steps. This is equivalent to (P t )ij > 0 
for some t. If state j is accessible from state i and state i is accessible from 
state j, states j and i are said to communicate. Communication is clearly 
an equivalence relation. (A binary relation ∼ is an equivalence relation over 
some set S if for x, y, z ∈ S, (1)  x ∼ x, (2)  x ∼ y ⇒ y ∼ x, and  (3)  
x ∼ y ∧ y ∼ z ⇒ x ∼ z; that is, it is reﬂexive, symmetric, and transitive.) 
The set of all states that communicate with each other is an equivalence class. 
States belonging to diﬀerent equivalence classes do not communicate, although 
a state in one class may be accessible from a state in a diﬀerent class. 
Identiﬁcation and analysis of states that communicate can be done by the 
reduction of the transition matrix in the manner discussed on page 413 and 
illustrated in Eq. (8.76), in which by permutations of the rows and columns, 
square 0 submatrices are formed. If the transition matrix is irreducible, that 
is, if no such 0 submatrices can be formed, then all states in a Markov chain 
are in a single equivalence class. In that case the chain is said to be irreducible. 
Irreducible matrices are discussed in Sect. 8.7.2, beginning on page 412, and  
the implication (8.77) in that section provides a simple characterization of 
irreducibility. Reducibility of Markov chains is also clearly related to the re-
ducibility in graphs that we discussed in Sect. 8.1.2. (In graphs, the connec-
tivity matrix is similar to the transition matrix in Markov chains.)

9.5 Stochastic Processes
489
If the transition matrix is primitive (i.e., it is irreducible and its eigenvalue 
with maximum modulus has algebraic multiplicity of 1; see page 415), then 
the Markov chain is said to be primitive. 
Primitivity and irreducibility are important concepts in analysis of Markov 
chains because they imply interesting limiting behavior of the chains. 
Limiting Behavior of Markov Chains 
The limiting behavior of the Markov chain is of interest. This of course can be 
analyzed in terms of limt→∞ P t . Whether or not this limit exists depends on 
the properties of P. If  P is primitive and irreducible, we can make use of the 
results in Sect. 8.7.2. In particular, because 1 is an eigenvalue and the vector 
1 is the eigenvector associated with 1, from Eq. (8.79), we have 
limit Underscript t right arrow normal infinity Endscripts upper P Superscript t Baseline equals 1 pi Subscript s Superscript normal upper T Baseline comma lim
t→∞P t = 1πT
s ,
(9.92) 
where πs is the Perron vector of P T . 
This also gives us the limiting distribution for an irreducible, primitive 
Markov chain, 
limit Underscript t right arrow normal infinity Endscripts pi Superscript left parenthesis t right parenthesis Baseline equals pi Subscript s Baseline period lim
t→∞π(t) = πs.
The Perron vector has the property πs = P T πs of course, so this distribu-
tion is the invariant distribution of the chain. This invariance is a necessary 
condition for most uses of Markov chains in Monte Carlo methods for gener-
ating posterior distributions in Bayesian statistical analysis. These methods 
are called Markov chain Monte Carlo (MCMC) methods and are widely used 
in Bayesian analyses. 
There are many other interesting properties of Markov chains that follow 
from various properties of nonnegative matrices that we discuss in Sect. 8.7, 
but rather than continuing the discussion here, we refer the interested reader 
to a text on Markov chains, such as Meyn and Tweedie (2009). 
9.5.2 Markovian Population Models 
A simple but useful model for population growth measured at discrete points 
in time, t, t + 1, . . ., is constructed as follows. We identify k age groupings for 
the members of the population; we determine the number of members in each 
age group at time t, calling this p(t) , 
p Superscript left parenthesis t right parenthesis Baseline equals left parenthesis p 1 Superscript left parenthesis t right parenthesis Baseline comma ellipsis comma p Subscript k Superscript left parenthesis t right parenthesis Baseline right parenthesis semicolonp(t) =
(
p(t)
1 , . . . , p(t)
k
)
;
determine the reproductive rate in each age group, calling this α, 
alpha equals left parenthesis alpha 1 comma ellipsis comma alpha Subscript k Baseline right parenthesis semicolonα = (α1, . . . , αk);

490
9 Selected Applications in Statistics
and determine the survival rate in each of the ﬁrst k − 1 age groups, calling 
this σ, 
sigma equals left parenthesis sigma 1 comma ellipsis comma sigma Subscript k minus 1 Baseline right parenthesis periodσ = (σ1, . . . , σk−1).
It is assumed that the reproductive rate and the survival rate are constant 
in time. (There are interesting statistical estimation problems here that are 
described in standard texts in demography or in animal population models.) 
The survival rate σi is the proportion of members in age group i at time t 
who survive to age group i + 1. (It is assumed that the members in the last 
age group do not survive from time t to time t + 1.) The total size of the 
population at time t is N (t) = 1T p(t) . (The use of the capital letter N for 
a scalar variable is consistent with the notation used in the study of ﬁnite 
populations.) 
If the population in each age group is relatively large, then given the sizes 
of the population age groups at time t, the approximate sizes at time t + 1  
are given by 
p Superscript left parenthesis t plus 1 right parenthesis Baseline equals upper A p Superscript left parenthesis t right parenthesis Baseline commap(t+1) = Ap(t),
(9.93) 
where A is a Leslie matrix as in Eq. (8.85), 
upper A equals Start 5 By 5 Matrix 1st Row 1st Column alpha 1 2nd Column alpha 2 3rd Column midline horizontal ellipsis 4th Column alpha Subscript m minus 1 Baseline 5th Column alpha Subscript m Baseline 2nd Row 1st Column sigma 1 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column sigma 2 3rd Column midline horizontal ellipsis 4th Column 0 5th Column 0 4th Row 1st Column vertical ellipsis 2nd Column vertical ellipsis 3rd Column vertical ellipsis 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 5th Row 1st Column 0 2nd Column 0 3rd Column midline horizontal ellipsis 4th Column sigma Subscript m minus 1 Baseline 5th Column 0 EndMatrix commaA =
⎡
⎢⎢⎢⎢⎢⎣
α1 α2 · · · αm−1 αm
σ1 0 · · ·
0
0
0 σ2 · · ·
0
0
...
...
...
...
...
0
0 · · · σm−1
0
⎤
⎥⎥⎥⎥⎥⎦
,
(9.94) 
where 0 ≤ αi and 0 ≤ σi ≤ 1. 
The Leslie population model can be useful in studying various species of 
plants or animals. The parameters in the model determine the vitality of the 
species. For biological realism, at least one αi and all σi must be positive. This 
model provides a simple approach to the study and simulation of population 
dynamics. The model depends critically on the eigenvalues of A. 
As we have seen (Exercise 8.10), the Leslie matrix has a single unique 
positive eigenvalue. If that positive eigenvalue is strictly greater in modulus 
than any other eigenvalue, then given some initial population size, p(0) , the  
model yields a few damping oscillations and then an exponential growth, 
p Superscript left parenthesis t 0 plus t right parenthesis Baseline equals p Superscript left parenthesis t 0 right parenthesis Baseline normal e Superscript r t Baseline commap(t0+t) = p(t0)ert,
(9.95) 
where r is the rate constant. The vector p(t0) (or any scalar multiple) is called 
the stable age distribution. (You are  asked to show this in Exercise  9.15a.) If 1 
is an eigenvalue and all other eigenvalues are strictly less than 1 in modulus, 
then the population eventually becomes constant; that is, there is a stable 
population. (You are asked to show this in Exercise 9.15b.) 
The survival rates and reproductive rates constitute an age-dependent life 
table, which is widely used in studying population growth. The age groups

9.5 Stochastic Processes
491
in life tables for higher-order animals are often deﬁned in years, and the pa-
rameters often are deﬁned only for females. The ﬁrst age group is generally 
age 0, and so α1 = 0. The  net reproductive rate, r0, is the average number 
of (female) oﬀspring born to a given (female) member of the population over 
the lifetime of that member; that is, 
r 0 equals sigma summation Underscript i equals 2 Overscript m Endscripts alpha Subscript i Baseline sigma Subscript i minus 1 Baseline periodr0 =
m
E
i=2
αiσi−1.
(9.96) 
The average generation time, T, is given  by  
upper T equals sigma summation Underscript i equals 2 Overscript m Endscripts i alpha Subscript i Baseline sigma Subscript i minus 1 Baseline divided by r 0 periodT =
m
E
i=2
iαiσi−1/r0.
(9.97) 
The net reproductive rate, average generation time, and exponential growth 
rate constant are related by 
r equals log left parenthesis r 0 right parenthesis divided by upper T periodr = log(r0)/T.
(9.98) 
(You are asked to show this in Exercise 9.15c.) 
Because the process being modeled is continuous in time and this model 
is discrete, there are certain averaging approximations that must be made. 
There are various reﬁnements of this basic model to account for continuous 
time. There are also reﬁnements to allow for time-varying parameters and for 
the intervention of exogenous events. Of course, from a statistical perspective, 
the most interesting questions involve the estimation of the parameters. See 
Cullen (1985), for example, for further discussions of this modeling problem. 
Various starting age distributions can be used in this model to study the 
population dynamics. 
9.5.3 Autoregressive Processes 
An interesting type of stochastic process is the pth-order autoregressive time 
series, deﬁned by the stochastic diﬀerence equation 
x Subscript t Baseline equals phi 0 plus phi 1 x Subscript t minus 1 Baseline plus midline horizontal ellipsis plus phi Subscript p Baseline x Subscript t minus p Baseline plus e Subscript t Baseline commaxt = φ0 + φ1xt−1 + · · · + φpxt−p + et,
(9.99) 
where φp /= 0,  the  et have constant mean of 0 and constant variance of σ2 > 0, 
and any two diﬀerent es and et have zero correlation. This model is called an 
autoregressive model of order p or AR(p). 
Comments on the model and notation: 
I have implied that et is considered to be a random variable, even 
though it is not written in upper case, and of course, if et is a random 
variable then xt is also, even though it is not written in upper case 
either; likewise, I will sometimes consider xt−1 and the other xt−j to

492
9 Selected Applications in Statistics
be random variables. 
Stationarity (i.e., constancy over time) is an issue. In the simple model 
above all of the simple parameters are constant; however, unless cer-
tain conditions are met, the moments of xt can grow without bounds. 
(This is related to the “unit roots” mentioned below.) Some authors 
require that some other conditions be satisﬁed in an AR(p) model  so  
that moments of the process do not grow without bounds. 
Also, many authors omit the φ0 term in the AR(p) model.  
The most important properties of this process arise from the autocorrela-
tions, Cor(xs, xt). If these autocorrelations depend on s and t only through 
|s − t| and if for given h = |s − t| the autocorrelation, 
rho Subscript h Baseline equals normal upper C normal o normal r left parenthesis x Subscript t plus h Baseline comma x Subscript t Baseline right parenthesis commaρh = Cor(xt+h, xt),
is constant, the autoregressive process has some simple, but useful properties. 
The model (9.99) is a little more complicated than it appears. This is 
because the speciﬁcation of xt is conditional on xt−1, . . . , xt−p. Presumably,  
also, xt−1 is dependent on xt−2, . . . , xt−p−1, and so on. There are no marginal 
(unconditional) properties of the xs that are speciﬁed in the model; that is, 
we have not speciﬁed a starting point. 
The stationarity of the et (constant mean and constant variance) does not 
imply that the xt are stationary. We can make the model more speciﬁc by 
adding a condition of stationarity on the xt. Let us assume that the xt have 
constant and ﬁnite means and variances; that is, the {xt} process is (weakly)  
stationary. 
To continue the analysis, consider the AR(1) model. If the xt have constant 
means and variances, then 
normal upper E left parenthesis x Subscript t Baseline right parenthesis equals StartFraction phi 0 Over 1 minus phi 1 EndFractionE(xt) =
φ0
1 −φ1
(9.100) 
and 
normal upper V left parenthesis x Subscript t Baseline right parenthesis equals StartFraction sigma squared Over 1 minus phi 1 squared EndFraction periodV(xt) =
σ2
1 −φ2
1
.
(9.101) 
The ﬁrst equation indicates that we cannot have φ1 = 1 and the second 
equation makes sense only if |φ1| < 1. For AR(p) models in general, similar 
situations can occur. The denominators in the expressions for the mean and 
variance involve p-degree polynomials, similar to the ﬁrst degree polynomials 
in the denominators of Eqs. (9.100) and  (9.101). 
We call f(z) = 1 − φ1z1 −· · · − φpzp the associated polynomial. If  f(z) =  
0, we have situations similar to a 0 in the denominator of Eq. (9.101). If a 
root of f(z) = 0 is 1, the expression for a variance is inﬁnite (which we see 
immediately from Eq. (9.101) for  the  AR(1) model). This situation is called a 
“unit root.” If some root is greater than 1, we have an expression for a variance 
that is negative. Hence, in order for the model to make sense in all respects,

9.5 Stochastic Processes
493
all roots of of the associated polynomial must be less than 1 in modulus. (Note 
some roots can contain imaginary components.) 
Although many of the mechanical manipulations in the analysis of the 
model may be unaﬀected by unit roots, they have serious implications for the 
interpretation of the model. 
Relation of the Autocorrelations to the Autoregressive Coeﬃcients 
From the model (9.99) we can  see that  ρh = 0  for  h > p, and  ρ1, . . . ρp are 
determined by φ1, . . . φp by the relationship 
upper R phi equals rho commaRφ = ρ,
(9.102) 
where φ and ρ are the p-vectors of the φis (i /= 0)  and  the  ρis, and R is the 
Toeplitz matrix (see Sect. 8.8.4) 
upper R equals Start 5 By 5 Matrix 1st Row 1st Column 1 2nd Column rho 1 3rd Column rho 2 4th Column midline horizontal ellipsis 5th Column rho Subscript p minus 1 Baseline 2nd Row 1st Column rho 1 2nd Column 1 3rd Column rho 1 4th Column midline horizontal ellipsis 5th Column rho Subscript p minus 2 Baseline 3rd Row 1st Column rho 2 2nd Column rho 1 3rd Column 1 4th Column midline horizontal ellipsis 5th Column rho Subscript p minus 3 Baseline 4th Row 1st Column vertical ellipsis 2nd Column Blank 3rd Column Blank 4th Column down right diagonal ellipsis 5th Column vertical ellipsis 5th Row 1st Column rho Subscript p minus 1 Baseline 2nd Column rho Subscript p minus 2 Baseline 3rd Column rho Subscript p minus 3 Baseline 4th Column midline horizontal ellipsis 5th Column 1 EndMatrix periodR =
⎡
⎢⎢⎢⎢⎢⎣
1
ρ1
ρ2
· · · ρp−1
ρ1
1
ρ1
· · · ρp−2
ρ2
ρ1
1
· · · ρp−3
...
...
...
ρp−1 ρp−2 ρp−3 · · ·
1
⎤
⎥⎥⎥⎥⎥⎦
.
This system of equations is called the Yule-Walker equations. Notice the re-
lationship of the Yule-Walker equations to the unit root problem mentioned 
above. 
For a given set of ρs, possibly estimated from some observations on the 
time series, Algorithm 9.2 can be used to solve the system (9.102). 
Algorithm 9.2 Solution of the Yule-Walker System (9.102) 
1. Set k = 0;  φ (k) 
1 
= −ρ1; b(k) = 1;  and  a(k) = −ρ1. 
2. Set k = k + 1.  
3. Set b(k) =
(
1 −
(
a(k−1))2)
b(k−1) . 
4. Set a(k) = −
(
ρk+1 + Ek 
i=1 ρk+1−iφ (k−1) 
1
)
/b(k) . 
5. For i = 1, 2, . . . , k  
set yi = φ (k−1) 
i
+ a(k) φ (k−1) 
k+1−i. 
6. For i = 1, 2, . . . , k  
set φ (k) 
i 
= yi. 
7. Set φ (k) 
k+1 = a(k) . 
8. If k <  p  − 1, go to step 1; otherwise terminate. 
This algorithm is O(p) (see Golub and Van Loan (1996)). 
The Yule-Walker equations arise in many places in the analysis of stochas-
tic processes. Multivariate versions of the equations are used for a vector time 
series (see Fuller 1995, for example).

494
9 Selected Applications in Statistics
9.6 Optimization of Scalar-Valued Functions 
A common problem in statistics, as well as in science generally, is to ﬁnd the 
minimum or maximum of some scalar-valued function. In statistical applica-
tions, for example, we may seek to ﬁt a model to data by determining the 
values of model parameters that yield the minimum of the sum of squares 
of residuals, as we discussed in Sect. 9.1.1. Alternatively, we may seek to de-
termine the values of the parameters in a likelihood function that yield the 
maximum of the likelihood function for a given set of data, as we will discuss 
below in Sect. 9.6.4. In Sect. 3.12, we considered approximation of a matrix 
by another matrix, and in Sect. 4.8, we considered the problem of approxi-
mate factorization of a matrix. In both cases, we formulated the problems as 
optimization problems. 
In optimization problems, we refer to the function of interest as the ob-
jective function. In the least squares ﬁtting problem, the objective function 
is the sum of squares, which, for given data, is a function of the parameters 
in the model. In the maximum likelihood estimation problem, the objective 
function is the likelihood function, which, for given data, is a function of the 
parameters in the probability model. 
Because the function may have many ups and downs, we often use the 
phrases local minimum and global minimum (or global maximum or global 
optimum), with obvious meanings. Our discussion focuses on local optima, 
and we will not consider the problem of ﬁnding a global optimum of a function 
with multiple local optima. 
Since maximizing a scalar function f(x) is equivalent to minimizing its 
negative, −f(x), we can always just consider the basic problem to be min-
imization of a function. Thus, we can use terminology for the problem of 
ﬁnding a minimum of the objective function, and write the general problem 
as 
min Underscript x element of upper D Endscripts f left parenthesis x right parenthesis period min
x∈D f(x).
(9.103) 
Often, D in the expression above is just the full domain of f, and often that 
is IRm for some m. In the next few subsections, we will consider D to be 
the full domain of f. Problems in which D is the full domain of f are called 
unconstrained optimization problems. We consider them ﬁrst, and then in 
Sect. 9.6.5 we will brieﬂy discuss constrained optimization problems. 
Methods to approach the optimization problem obviously depend on the 
nature of the objective function. Whether or not the function is convex and 
whether or not it is diﬀerentiable are important distinctions. 
In this section we consider only scalar-valued objective functions that are 
twice-diﬀerentiable. For a scalar-valued objective function f(x) of  m variables, 
we will denote the m-vector of ﬁrst derivatives, that is, the gradient, as gf(x), 
and the m × m matrix of second derivatives, the Hessian, as Hf(x), as above. 
For a twice-diﬀerentiable function, the Hessian is nonnegative (positive) def-

9.6 Optimization of Scalar-Valued Functions
495
inite everywhere on a given domain if and only if the function is (strictly) 
convex on that domain. 
Because a derivative measures the rate of change of a function, a point at 
which the ﬁrst derivative is equal to 0 is a stationary point, which may be a 
maximum or a minimum of the function. Diﬀerentiation is therefore a very 
useful tool for ﬁnding the optima of functions, and so, for a given function 
f(x), the gradient vector function, gf(x), and the Hessian matrix function, 
Hf(x), play important roles in optimization methods. 
Because except in the very simplest of cases, determining a solution to the 
equation deﬁning a stationary point, gf(x) = 0 cannot be done in closed form, 
the optimization method itself must be iterative, moving through a sequence 
of points, x(0) , x(1) , x(2) , . . ., that approaches the optimum point arbitrarily 
closely. At the point x(k) , the direction of steepest descent is clearly −gf(x(k) ), 
but because this direction may be continuously changing, the steepest-descent 
direction may not be the best direction in which to seek the next point, x(k+1) . 
In most cases, however, we move in a downward path in the general direction 
of the gradient. We call any such method a gradient-descent method. 
9.6.1 Stationary Points of Functions 
The optimum point is a stationary point (assuming that it occurs at an interior 
point of the domain), but a stationary point is not necessarily an optimum 
point. 
The ﬁrst derivative of the objective function helps only in ﬁnding a station-
ary point. The matrix of second derivatives, the Hessian, provides information 
about the nature of the stationary point, which may be a local minimum or 
maximum, a saddlepoint, or only an inﬂection point. 
The so-called second-order optimality conditions are the following (see a 
general text on optimization, such as Griva, Nash, and Sofer 2009, for  the  
proofs): 
• If (but not only if) the stationary point is a local minimum, then the 
Hessian is nonnegative deﬁnite at the stationary point. 
• If the Hessian is positive deﬁnite at the stationary point, then the station-
ary point is a local minimum. 
• Likewise, if the stationary point is a local maximum, then the Hessian 
is nonpositive deﬁnite, and if the Hessian is negative deﬁnite, then the 
stationary point is a local maximum. 
• If the Hessian has both positive and negative eigenvalues at the stationary 
point, then the stationary point is a saddlepoint. 
9.6.2 Newton’s Method 
We consider a twice-diﬀerentiable scalar-valued function of a vector argument, 
f(x). By a Taylor series expansion about a stationary point x∗, truncated after 
the second-order term

496
9 Selected Applications in Statistics
f left parenthesis x right parenthesis almost equals f left parenthesis x Subscript asterisk Baseline right parenthesis plus left parenthesis x minus x Subscript asterisk Baseline right parenthesis Superscript normal upper T Baseline normal g Subscript f Baseline left parenthesis x Subscript asterisk Baseline right parenthesis plus one half left parenthesis x minus x Subscript asterisk Baseline right parenthesis Superscript normal upper T Baseline normal upper H Subscript f Baseline left parenthesis x Subscript asterisk Baseline right parenthesis left parenthesis x minus x Subscript asterisk Baseline right parenthesis commaf(x) ≈f(x∗) + (x −x∗)Tgf
(
x∗
)
+ 1
2(x −x∗)THf
(
x∗
)
(x −x∗),
(9.104) 
because gf
(
x∗
)
= 0, we have a general method of ﬁnding a stationary point 
for the function f(·), called Newton’s method. If x is an m-vector, gf(x) is an  
m-vector and Hf(x) is an  m × m matrix. 
Newton’s method is to choose a starting point x(0) , then, for k = 0, 1, . . ., 
to solve the linear systems 
normal upper H Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis p Superscript left parenthesis k right parenthesis Baseline equals minus normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesisHf
(
x(k))
p(k) = −gf
(
x(k))
(9.105) 
for p(k) , and then to update the point in the domain of f(·) by  
x Superscript left parenthesis k plus 1 right parenthesis Baseline equals x Superscript left parenthesis k right parenthesis Baseline plus alpha Superscript left parenthesis k right parenthesis Baseline p Superscript left parenthesis k right parenthesis Baseline commax(k+1) = x(k) + α(k)p(k),
(9.106) 
where α(k) is a scalar such that f(x(k+1) ) < f(x(k) ). The two steps are re-
peated until there is essentially no change from one iteration to the next. 
These two steps have a very simple form for a function of one variable (see 
Exercise 9.26a). 
We can stop the iterative progression based on either the change in the 
domain or the amount of change in the value of the function. For speciﬁed 
constants e1 and e2, we can stop when 
double vertical bar x Superscript left parenthesis k plus 1 right parenthesis Baseline minus x Superscript left parenthesis k right parenthesis Baseline double vertical bar less than epsilon 1
||||||x(k+1) −x(k)|||||| < e1
(9.107) 
or when 
StartAbsoluteValue f left parenthesis x Superscript left parenthesis k plus 1 right parenthesis Baseline right parenthesis minus f left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis EndAbsoluteValue less than epsilon 2 period
|||f
(
x(k+1))
−f
(
x(k))||| < e2.
(9.108) 
If f(·) is a quadratic function, the solution using Newton’s method is not 
iterative; it is obtained in one step because Eq. (9.104) is exact. 
Quasi-Newton Methods 
All gradient-descent methods determine the path p(k) to take toward the next 
point by a system of equations of the form 
upper R Superscript left parenthesis k right parenthesis Baseline p Superscript left parenthesis k right parenthesis Baseline equals minus normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis periodR(k)p(k) = −gf
(
x(k))
.
In the steepest-descent method, R(k) is the identity, I, in these equations. 
For functions with eccentric contours, the steepest-descent method traverses 
a zigzag path to the minimum. In Newton’s method (Eq. (9.105)), R(k) is the 
Hessian evaluated at the previous point, Hf
(
x(k))
, which results in a more 
direct path to the minimum than the steepest descent follows. 
Aside from the issue of consistency of the resulting equation, a major dis-
advantage of Newton’s method is the computational burden of computing the 
Hessian, which requires O(m2 ) function evaluations, and solving the system, 
which requires O(m3 ) arithmetic operations, at each iteration.

9.6 Optimization of Scalar-Valued Functions
497
Instead of using the Hessian at each iteration, we may use an approxima-
tion, B(k) . We may choose approximations that are simpler to update and/or 
that allow the equations for the step to be solved more easily. Methods us-
ing such approximations are called quasi-Newton methods or variable metric 
methods. 
One simple approximation to the Hessian is based on the fact that 
normal upper H Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis left parenthesis x Superscript left parenthesis k right parenthesis Baseline minus x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis almost equals normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis minus normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis semicolonHf
(
x(k))(
x(k) −x(k−1))
≈gf
(
x(k))
−gf
(
x(k−1))
;
hence, we choose B(k) so that 
upper B Superscript left parenthesis k right parenthesis Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline minus x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis equals normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis minus normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis periodB(k)(
x(k) −x(k−1))
= gf
(
x(k))
−gf
(
x(k−1))
.
(9.109) 
This is called the secant condition. 
We express the secant condition as 
upper B Superscript left parenthesis k right parenthesis Baseline s Superscript left parenthesis k right parenthesis Baseline equals y Superscript left parenthesis k right parenthesis Baseline commaB(k)s(k) = y(k),
(9.110) 
where 
s Superscript left parenthesis k right parenthesis Baseline equals x Superscript left parenthesis k right parenthesis Baseline minus x Superscript left parenthesis k minus 1 right parenthesiss(k) = x(k) −x(k−1)
and 
y Superscript left parenthesis k right parenthesis Baseline equals normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k right parenthesis Baseline right parenthesis minus normal g Subscript f Baseline left parenthesis x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis commay(k) = gf(x(k)) −gf(x(k−1)),
as above. 
The system of equations in (9.110) does not fully determine B(k) of course. 
Because B(k) should approximate the Hessian, we may require that it be 
symmetric and positive deﬁnite. 
The most common approach in quasi-Newton methods is ﬁrst to choose 
a reasonable starting matrix B(0) and then to choose subsequent matrices by 
additive updates, 
upper B Superscript left parenthesis k plus 1 right parenthesis Baseline equals upper B Superscript left parenthesis k right parenthesis Baseline plus upper B Subscript a Superscript left parenthesis k right parenthesis Baseline commaB(k+1) = B(k) + B(k)
a ,
(9.111) 
subject to preservation of symmetry and positive deﬁniteness. An approximate 
Hessian B(k) may be used for several iterations before it is updated; that is, 
B (k) 
a 
may be taken as 0 for several successive iterations. 
9.6.3 Least Squares 
One of the most important applications that involve minimization is the ﬁtting 
of a model to data. In this problem, we have a function s that relates one 
variable, say y, to other variables, say the m-vector x. The function involves 
some unknown parameters, say the d-vector θ, so the model is 
y almost equals s left parenthesis x semicolon theta right parenthesis periody ≈s(x; θ).
(9.112) 
The data consists of n observations on the variables y and x.

498
9 Selected Applications in Statistics
Fitting the model is usually done by minimizing some norm of the vector 
of residuals 
r Subscript i Baseline left parenthesis theta right parenthesis equals y Subscript i Baseline minus s left parenthesis x Subscript i Baseline semicolon theta right parenthesis periodri(θ) = yi −s(xi; θ).
(9.113) 
The decision variables are the parameters θ. The optimal values of θ, often 
denoted as -θ, are called “estimates.” 
Because the data are observed and therefore are constants, the residuals 
are functions of θ only. The vector-valued function r(θ) maps IRd into IRn . 
The most common norm to minimize to obtain a ﬁt is the L2 or Euclidean 
norm. The scalar-valued objective function then is 
StartLayout 1st Row 1st Column f left parenthesis theta right parenthesis 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus s left parenthesis x Subscript i Baseline semicolon theta right parenthesis right parenthesis squared 2nd Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis r Subscript i Baseline left parenthesis theta right parenthesis right parenthesis squared 3rd Row 1st Column Blank 2nd Column equals 3rd Column left parenthesis r left parenthesis theta right parenthesis right parenthesis Superscript normal upper T Baseline r left parenthesis theta right parenthesis period EndLayoutf(θ) =
n
E
i=1
(
yi −s(xi; θ)
)2
=
n
E
i=1
(
ri(θ)
)2
(9.114) 
=
(
r(θ)
)T r(θ). 
This problem is called least squares regression. If the function s is nonlinear 
in θ, the functions ri are also nonlinear in θ, and the problem is called nonlinear 
least squares regression. 
Linear Least Squares 
A very common form of the model (9.112) is the linear regression model 
y almost equals x Superscript normal upper T Baseline theta periody ≈xTθ.
In this form of the model, we usually use β in place of θ, and we write the model 
as an equality with an additive “error” term instead of the approximation. In 
this case also, the order of β is  the same as that of  x, which we will continue 
to denote as m. 
In statistical applications, we generally have n observations of pairs of y 
and x. We form an  n-vector of the y observations and an n × m matrix X of 
the x observations. Thus, we form the linear model of the data 
y equals upper X beta plus epsilon commay = Xβ + e,
(9.115) 
where y is an n-vector, X is an n × m matrix, β is an m-vector, and e is an 
n-vector. 
For any value of β, say  b, we have the residual vector 
r equals y minus upper X b periodr = y −Xb.
(9.116) 
For a least squares ﬁt of the regression model, we minimize its Euclidean 
norm,

9.6 Optimization of Scalar-Valued Functions
499
f left parenthesis b right parenthesis equals r Superscript normal upper T Baseline r commaf(b) = rTr,
(9.117) 
with respect to the variable b. We can solve this optimization problem by 
taking the derivative of this sum of squares and equating it to zero. Doing 
this, we get 
StartLayout 1st Row 1st Column StartFraction normal d left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis Over normal d b EndFraction 2nd Column equals 3rd Column StartFraction normal d left parenthesis y Superscript normal upper T Baseline y minus 2 b Superscript normal upper T Baseline upper X Superscript normal upper T Baseline y plus b Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X b right parenthesis Over normal d b EndFraction 2nd Row 1st Column Blank 3rd Row 1st Column Blank 2nd Column equals 3rd Column minus 2 upper X Superscript normal upper T Baseline y plus 2 upper X Superscript normal upper T Baseline upper X b 4th Row 1st Column Blank 2nd Column equals 3rd Column 0 comma EndLayoutd(y −Xb)T(y −Xb)
db
= d(yTy −2bTXTy + bTXTXb)
db
= −2XTy + 2XTXb
= 0,
which yields the normal equations 
upper X Superscript normal upper T Baseline upper X b equals upper X Superscript normal upper T Baseline y periodXTXb = XTy.
(9.118) 
The solution to the normal equations is a stationary point of the func-
tion (9.117). The Hessian of (y − Xb)T (y − Xb) with respect to b is 2XT X 
and 
upper X Superscript normal upper T Baseline upper X succeeds above single line equals sign 0 periodXTX > 0.
Because the matrix of second derivatives is nonnegative deﬁnite, the value of 
b that solves the system of equations arising from the ﬁrst derivatives is a 
local minimum of Eq. (9.117). We discuss these equations further in Sects. 5.6 
and 9.1.1. 
The normal equations also imply one of the most important properties of 
a linear least squares solution. The residuals are orthogonal to the column 
space of X: 
upper X Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis equals 0 periodXT(y −Xb) = 0.
(9.119) 
Notice, of course, that this is just the gradient of the objective function, and 
the equation just states that the gradient is 0 at the optimum. 
Nonlinear Least Squares: The Gauss-Newton Method 
If the function in the model (9.112) is not linear, the problem may be quite 
diﬀerent, both for solving the least squares problem and for issues involved 
in statistical inference. We will not consider the inference problems here, but 
rather, just discuss methods for obtaining the least squares ﬁt. 
The gradient and the Hessian for a least squares problem have special 
structures that involve the Jacobian of the residuals, which is a vector function 
of the parameters. The gradient of f(θ) is  
normal g Subscript f Baseline left parenthesis theta right parenthesis equals left parenthesis normal upper J Subscript r Baseline left parenthesis theta right parenthesis right parenthesis Superscript normal upper T Baseline r left parenthesis theta right parenthesis periodgf(θ) =
(
Jr(θ)
)Tr(θ).
The Jacobian of r is also part of the Hessian of f:

500
9 Selected Applications in Statistics
normal upper H Subscript f Baseline left parenthesis theta right parenthesis equals left parenthesis normal upper J Subscript r Baseline left parenthesis theta right parenthesis right parenthesis Superscript normal upper T Baseline normal upper J Subscript r Baseline left parenthesis theta right parenthesis plus sigma summation Underscript i equals 1 Overscript n Endscripts r Subscript i Baseline left parenthesis theta right parenthesis normal upper H Subscript r Sub Subscript i Subscript Baseline left parenthesis theta right parenthesis periodHf(θ) =
(
Jr(θ)
)TJr(θ) +
n
E
i=1
ri(θ)Hri(θ).
(9.120) 
In this maze of notation the reader should pause to remember the shapes 
of these arrays and their meanings in the context of ﬁtting a model to data. 
Notice, in particular, that the dimension of the space of the optimization 
problem is d, instead of m as in the previous problems. We purposely chose 
a diﬀerent letter to represent the dimension so as to emphasize that the de-
cision variables may have a diﬀerent dimension from that of the independent 
(observable) variables. The space of an observation has dimension m + 1 (the 
m elements of x, plus the response y); and the space of the observations as 
points yi and corresponding model values s(xi, θ) has dimension n. 
• xi is an m-vector. In the modeling context, these are the independent 
variables. 
• y is an n-vector and together with the n xi vectors is a constant in the 
optimization problem. In the modeling context, these are observations. 
• θ is a d-vector. This is the vector of parameters. 
• r(·) is an  n-vector. This is the vector of residuals. 
• Jr(·) is an  n × d matrix. 
• Hri (·) is a  d × d matrix. 
• f(·) is a scalar. This is the objective function for the data-ﬁtting criterion. 
• gf(·) is a  d-vector. 
• Hf(·) is a  d × d matrix. 
In the vicinity of the solution θ∗, the residuals ri(θ) should be small, and 
Hf(θ) may be approximated by neglecting the second term in Eq. (9.120). 
Using this approximation and the gradient-descent equation, we have 
left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis p Superscript left parenthesis k right parenthesis Baseline equals minus left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis period
(
Jr(θ(k))
)TJr(θ(k)) p(k) = −
(
Jr(θ(k))
)Tr(θ(k)).
(9.121) 
It is clear that the solution p(k) is a descent direction and so gf(θ(k) ) /= 0,  and  
StartLayout 1st Row 1st Column left parenthesis p Superscript left parenthesis k right parenthesis Baseline right parenthesis Superscript normal upper T Baseline normal g Subscript f Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis 2nd Column equals 3rd Column minus left parenthesis left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline p Superscript left parenthesis k right parenthesis Baseline right parenthesis Superscript normal upper T Baseline left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline p Superscript left parenthesis k right parenthesis 2nd Row 1st Column Blank 2nd Column less than 3rd Column 0 period EndLayout(p(k))Tgf(θ(k)) = −
((
Jr(θ(k))
)Tp(k))T (
Jr(θ(k))
)Tp(k)
< 0.
The update step is determined by a line search in the direction of the 
solution of Eq. (9.121): 
x Superscript left parenthesis k plus 1 right parenthesis Baseline minus x Superscript left parenthesis k right parenthesis Baseline equals alpha Superscript left parenthesis k right parenthesis Baseline p Superscript left parenthesis k right parenthesis Baseline periodx(k+1) −x(k) = α(k)p(k).
This method is called the Gauss-Newton algorithm. Because years ago the 
step was often taken simply as p(k) , a method that uses a variable step length 
factor α(k) is sometimes called a “modiﬁed Gauss-Newton algorithm.” It is 
the only kind to use, so we just call it the “Gauss-Newton algorithm.”

9.6 Optimization of Scalar-Valued Functions
501
If the residuals are small and if the Jacobian is nonsingular, the Gauss-
Newton method behaves much like Newton’s method near the solution. The 
major advantage is that second derivatives are not computed. 
If the residuals are not small or if Jr(θ(k) ) is poorly conditioned, the Gauss-
Newton method can perform very poorly. If Jr(θ(k) ) is not of full rank, we 
could choose the solution corresponding to the Moore-Penrose inverse: 
p Superscript left parenthesis k right parenthesis Baseline equals left parenthesis left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline right parenthesis Superscript plus Baseline r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis periodp(k) =
((
Jr(θ(k))
)T)+
r(θ(k)).
(9.122) 
If the matrix is nonsingular, the Moore-Penrose inverse is the usual inverse. 
(We consider the linear case more thoroughly in Sect. 5.6.3, beginning on 
page 287, and show that the Moore-Penrose yields the solution that has the 
shortest Euclidean length.) 
In the case of a linear model, the data, consisting of n observations on y 
and the m-vector x, result in an  n-vector of residuals, 
r equals y minus upper X b commar = y −Xb,
where X is the n×m matrix whose rows are the observed xT s (and  b is used in 
place of θ). The Gauss-Newton algorithm, which is the same as the ordinary 
Newton method for this linear least squares problem, yields the solution in 
one step. 
Levenberg-Marquardt Method 
Another possibility is to add a conditioning matrix to
(
Jr(θ(k) )
)T Jr(θ(k) ) on  
the left side of Eq. (9.121). A simple choice is τ (k) Id for some scalar τ (k) , and  
the equation for the update direction becomes 
left parenthesis left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis plus tau Superscript left parenthesis k right parenthesis Baseline upper I Subscript d Baseline right parenthesis p Superscript left parenthesis k right parenthesis Baseline equals minus left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis period
((
Jr(θ(k))
)TJr(θ(k)) + τ (k)Id
)
p(k) = −
(
Jr(θ(k))
)Tr(θ(k)).
A better choice may be a scaling matrix, S(k) , that takes into account the 
variability in the columns of Jr(θ(k) );  hence we have for  the update  
left parenthesis left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis plus lamda Superscript left parenthesis k right parenthesis Baseline left parenthesis upper S Superscript left parenthesis k right parenthesis Baseline right parenthesis Superscript normal upper T Baseline upper S Superscript left parenthesis k right parenthesis Baseline right parenthesis p Superscript left parenthesis k right parenthesis Baseline equals minus left parenthesis normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis right parenthesis Superscript normal upper T Baseline r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis period
((
Jr(θ(k))
)TJr(θ(k)) + λ(k)(
S(k))TS(k))
p(k) = −
(
Jr(θ(k))
)Tr(θ(k)).
(9.123) 
The basic requirement for the matrix
(
S(k))T S(k) is that it improves the con-
dition of the coeﬃcient matrix. There are various ways of choosing this ma-
trix. One is to transform the matrix
(
Jr(θ(k) )
)T Jr(θ(k) ) so it has 1s along the 
diagonal (this is equivalent to forming a correlation matrix from a variance-
covariance matrix), and to use the scaling vector to form S(k) . The nonneg-
ative factor λ(k) can be chosen to control the extent of the adjustment. The 
sequence λ(k) obviously must go to 0 for the solution sequence to converge to 
the optimum.

502
9 Selected Applications in Statistics
Use of an adjustment such as in Eq. (9.123) is called the Levenberg-
Marquardt method. This is probably the most widely used method for nonlin-
ear least squares. 
The Levenberg-Marquardt adjustment is similar to the regularization done 
in ridge regression that we will discuss from time to time (see, e.g., Eq. (8.59) 
on page 401). Just as in ridge regression the computations for Eq. (9.123) can  
be performed eﬃciently by recognizing that the system is the normal equations 
for the least squares ﬁt of 
Start 3 By 1 Matrix 1st Row r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis 2nd Row Blank 3rd Row 0 EndMatrix almost equals Start 3 By 1 Matrix 1st Row normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis 2nd Row Blank 3rd Row StartRoot lamda Superscript left parenthesis k right parenthesis Baseline EndRoot left parenthesis upper S Superscript left parenthesis k right parenthesis Baseline right parenthesis EndMatrix p period
⎛
⎝
r(θ(k))
0
⎞
⎠≈
⎡
⎣
Jr(θ(k))
√
λ(k)(
S(k))
⎤
⎦p.
(See page 478 and Exercise 9.7.) 
Equation (9.123), as indeed regularized solutions such as ridge regression 
generally, can be thought of as a Lagrange multiplier formulation of a con-
strained problem, as we will discuss in Sect. 9.6.5, beginning on page 503. 
9.6.4 Maximum Likelihood 
For a sample y = (y1, . . . , yn) from a probability distribution with probability 
density function p(·; θ), the likelihood function is 
upper L left parenthesis theta semicolon y right parenthesis equals product Underscript i equals 1 Overscript n Endscripts p left parenthesis y Subscript i Baseline semicolon theta right parenthesis commaL(θ; y) =
n
| |
i=1
p(yi; θ),
(9.124) 
and the log-likelihood function is l(θ; y) = log(L(θ; y)). It is often easier to 
work with the log-likelihood function. 
The log-likelihood is an important quantity in information theory and in 
unbiased estimation. If Y is a random variable having the given probability 
density function with the r-vector parameter θ, the  Fisher information matrix 
that Y contains about θ is the r × r matrix 
upper I left parenthesis theta right parenthesis equals normal upper C normal o normal v Subscript theta Baseline left parenthesis StartFraction partial differential l left parenthesis t comma upper Y right parenthesis Over partial differential t Subscript i Baseline EndFraction comma StartFraction partial differential l left parenthesis t comma upper Y right parenthesis Over partial differential t Subscript j Baseline EndFraction right parenthesis commaI(θ) = Covθ
(∂l(t, Y )
∂ti
, ∂l(t, Y )
∂tj
)
,
(9.125) 
where Covθ represents the variance-covariance matrix of the functions of Y 
formed by taking expectations for the given θ. (I use  diﬀerent  symbols here  
because the derivatives are taken with respect to a variable, but the θ in Covθ 
cannot be the variable of the diﬀerentiation. This distinction is somewhat 
pedantic, and sometimes I follow the more common practice of using the 
same symbol in an expression that involves both Covθ and ∂l(θ, Y )/∂θi.) 
For example, if the distribution of Y is the d-variate normal distribution 
with mean d-vector μ and d × d positive deﬁnite variance-covariance matrix

9.6 Optimization of Scalar-Valued Functions
503
Σ, from the multivariate normal PDF, Eq. (7.38), the likelihood, Eq. (9.124), 
is 
upper L left parenthesis mu comma normal upper Sigma semicolon y right parenthesis equals StartFraction 1 Over left parenthesis left parenthesis 2 pi right parenthesis Superscript d divided by 2 Baseline StartAbsoluteValue normal upper Sigma EndAbsoluteValue Superscript 1 divided by 2 Baseline right parenthesis Superscript n Baseline EndFraction exp left parenthesis minus one half sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus mu right parenthesis Superscript normal upper T Baseline normal upper Sigma Superscript negative 1 Baseline left parenthesis y Subscript i Baseline minus mu right parenthesis right parenthesis periodL(μ, Σ; y) =
1
(
(2π)d/2|Σ|1/2)n exp
(
−1
2
n
E
i=1
(yi −μ)TΣ−1(yi −μ)
)
.
(Note that |Σ|1/2 = |Σ 
1 
2 |, where here I am using | · |  to represent the de-
terminant. The square root matrix Σ 
1 
2 is often useful in transformations of 
variables.) 
Anytime we have a quadratic form that we need to simplify, we should re-
call Eq. (3.103): xT Ax = tr(AxxT ). Using this, and because the log-likelihood 
is easier to work with here, we write 
l left parenthesis mu comma normal upper Sigma semicolon y right parenthesis equals c minus StartFraction n Over 2 EndFraction log StartAbsoluteValue normal upper Sigma EndAbsoluteValue minus one half normal t normal r left parenthesis normal upper Sigma Superscript negative 1 Baseline sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus mu right parenthesis left parenthesis y Subscript i Baseline minus mu right parenthesis Superscript normal upper T Baseline right parenthesis commal(μ, Σ; y) = c −n
2 log |Σ| −1
2tr
(
Σ−1
n
E
i=1
(yi −μ)(yi −μ)T
)
,
(9.126) 
where we have used  c to represent the constant portion. Next, we use the 
Pythagorean equation (2.68) or Eq. (3.105) on the outer product to get 
StartLayout 1st Row l left parenthesis mu comma normal upper Sigma semicolon y right parenthesis equals c minus StartFraction n Over 2 EndFraction log StartAbsoluteValue normal upper Sigma EndAbsoluteValue minus one half normal t normal r left parenthesis normal upper Sigma Superscript negative 1 Baseline sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus y overbar right parenthesis left parenthesis y Subscript i Baseline minus y overbar right parenthesis Superscript normal upper T Baseline right parenthesis 2nd Row minus StartFraction n Over 2 EndFraction normal t normal r left parenthesis normal upper Sigma Superscript negative 1 Baseline left parenthesis y overbar minus mu right parenthesis left parenthesis y overbar minus mu right parenthesis Superscript normal upper T Baseline right parenthesis period EndLayoutl(μ, Σ; y) = c −n
2 log |Σ| −1
2tr
(
Σ−1
n
E
i=1
(yi −¯y)(yi −¯y)T
)
−n
2 tr
(
Σ−1(¯y −μ)(¯y −μ)T)
.
(9.127) 
In Sect. 9.2.1 we encountered this optimization problem in the context of 
linear regression and saw that a least squares ﬁt is the same as a maximum 
likelihood estimate when the distribution of the errors is normal. 
In maximum likelihood estimation, we seek the maximum of the likeli-
hood function (9.124) with respect to θ while we consider y to be ﬁxed. If the 
maximum occurs within an open set and if the likelihood is diﬀerentiable, we 
might be able to ﬁnd the maximum likelihood estimates by diﬀerentiation. In 
the log-likelihood for the d-variate normal distribution, we consider the pa-
rameters μ and Σ to be variables. To emphasize that perspective, we replace 
the parameters μ and Σ by the variables ˆμ and -Σ. Now, to determine the 
maximum, we could take derivatives with respect to ˆμ and -Σ, set them equal 
to 0, and solve for the maximum likelihood estimates. Some subtle problems 
arise that depend on the fact that for any constant vector a and scalar b, 
Pr(aT X = b) = 0, but we do not interpret the likelihood as a probability. 
In Exercise 9.12b you are asked to determine the values of ˆμ and -Σ using 
properties of traces and positive deﬁnite matrices without resorting to dif-
ferentiation. (This approach does not avoid the subtle problems, however.) 
9.6.5 Optimization of Functions with Constraints 
Instead of the problem shown in expression (9.103) in which we seek a mini-
mum point anywhere in the domain of the objective function, which often is

504
9 Selected Applications in Statistics
IRm , we may constrain the search for the minimum point to some subset of 
the domain. The problem is 
min Underscript x element of upper C Endscripts f left parenthesis x right parenthesis comma min
x∈C f(x),
(9.128) 
where C ⊂ D, the domain of f. This is a  constrained optimization problem. 
To emphasize the constraints or to make them more explicit, we often write 
the problem as 
StartLayout 1st Row 1st Column min Underscript x Endscripts 2nd Column f left parenthesis x right parenthesis 2nd Row 1st Column normal s period normal t period 2nd Column x element of upper C EndLayoutmin
x
f(x)
s.t. x ∈C
(9.129) 
Optimization is a large area. It is our intent here only to cover some of the 
subareas most closely related to our main subject of matrix algebra. Hence, we 
will consider only a simple type of equality-constrained optimization problem. 
Equality-Constrained Linear Least Squares Problems 
Instead of the simple least squares problem of determining a value of b that 
minimizes the sum of squares, we may have some restrictions that b must 
satisfy; for example, we may have the requirement that the elements of b 
sum to 1. More generally, consider the least squares problem for the linear 
model (9.115) with the requirement that b satisfy some set of linear restric-
tions, Ab = c, where  A is a full-rank k × m matrix (with k ≤ m). (The rank 
of A must be less than m or else the constraints completely determine the 
solution to the problem. If the rank of A is less than k, however, some rows 
of A and some elements of b could be combined into a smaller number of 
constraints. We can therefore assume A is of full row rank. Furthermore, we 
assume the linear system is consistent (i.e., rank([A|c]) = k) for otherwise 
there could be no solution.) We call any point b that satisﬁes Ab = c a feasible 
point. 
We write the equality-constrained least squares optimization problem as 
StartLayout 1st Row 1st Column min Underscript b Endscripts 2nd Column f left parenthesis b right parenthesis equals left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis 2nd Row 1st Column normal s period normal t period 2nd Column upper A b equals c period EndLayout
min
b
f(b) = (y −Xb)T(y −Xb)
s.t. Ab = c.
(9.130) 
If bc is any feasible point (i.e., Abc = c), then any other feasible point can 
be represented as bc + p, where  p is any vector in the null space of A, N(A). 
From our discussion in Sect. 3.6.2, we know that the dimension of N(A) is  
m −k, and its order is m. If  N is an m × (m − k) matrix whose  columns  
form a basis for N(A), all feasible points can be generated by bc + Nz, where  
z ∈ IRm−k . Hence, we need only consider the restricted variables 
b equals b Subscript c Baseline plus upper N zb = bc + Nz
and the “reduced” function 
h left parenthesis z right parenthesis equals f left parenthesis b Subscript c Baseline plus upper N z right parenthesis periodh(z) = f(bc + Nz).

9.6 Optimization of Scalar-Valued Functions
505
The argument of this function is a vector with only m − k elements instead 
of m elements as in the unconstrained problem. It is clear, however, that an 
unconstrained minimum point, z∗, of  h(z) yields a solution, b∗ = bc + Nz∗, of  
the original constrained minimization problem. 
The Reduced Gradient and Reduced Hessian 
If we assume diﬀerentiability, the gradient and Hessian of the reduced function 
can be expressed in terms of the original function: 
StartLayout 1st Row 1st Column normal g Subscript h Baseline left parenthesis z right parenthesis 2nd Column equals 3rd Column upper N Superscript normal upper T Baseline normal g Subscript f Baseline left parenthesis b Subscript c Baseline plus upper N z right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper N Superscript normal upper T Baseline normal g Subscript f Baseline left parenthesis b right parenthesis EndLayoutgh(z) = N Tgf(bc + Nz)
= N Tgf(b)
(9.131) 
and 
StartLayout 1st Row 1st Column normal upper H Subscript h Baseline left parenthesis z right parenthesis 2nd Column equals 3rd Column upper N Superscript normal upper T Baseline normal upper H Subscript f Baseline left parenthesis b Subscript c Baseline plus upper N z right parenthesis upper N 2nd Row 1st Column Blank 2nd Column equals 3rd Column upper N Superscript normal upper T Baseline normal upper H Subscript f Baseline left parenthesis b right parenthesis upper N period EndLayoutHh(z) = N THf(bc + Nz)N
= N THf(b)N.
(9.132) 
In Eq. (9.131), N T gf(b) is called the reduced gradient or projected gradient, and  
N T Hf(b)N in Eq. (9.132) is called the reduced Hessian or projected Hessian. 
The properties of stationary points referred to above are the conditions 
that determine a minimum of this reduced objective function; that is, b∗ is a 
minimum if and only if 
upper N Superscript normal upper T Baseline normal g Subscript f Baseline left parenthesis b Subscript asterisk Baseline right parenthesis equals 0 commaN Tgf(b∗) = 0,
(9.133) 
upper N Superscript normal upper T Baseline normal upper H Subscript f Baseline left parenthesis b Subscript asterisk Baseline right parenthesis upper N succeeds 0 commaN THf(b∗)N > 0,
(9.134) 
and 
upper A b Subscript asterisk Baseline equals c periodAb∗= c.
(9.135) 
These relationships then provide the basis for the solution of the optimiza-
tion problem. 
Lagrange Multipliers 
Because the m × m matrix [N|AT ] spans IRm , we can represent the vector 
gf(b∗) as a linear combination of the columns of N and AT , that is,  
StartLayout 1st Row 1st Column normal g Subscript f Baseline left parenthesis b Subscript asterisk Baseline right parenthesis 2nd Column equals 3rd Column left bracket upper N vertical bar upper A Superscript normal upper T Baseline right bracket StartBinomialOrMatrix z Subscript asterisk Choose lamda Subscript asterisk EndBinomialOrMatrix 2nd Row 1st Column Blank 2nd Column equals 3rd Column StartBinomialOrMatrix upper N z Subscript asterisk Baseline Choose upper A Superscript normal upper T Baseline lamda Subscript asterisk Baseline EndBinomialOrMatrix comma EndLayoutgf(b∗) = [N|AT]
( z∗
λ∗
)
=
( Nz∗
ATλ∗
)
,
where z∗ is an (m − k)-vector and λ∗ is a k-vector. Because gh(z∗) = 0,  Nz∗ 
must also vanish (i.e., Nz∗ = 0), and thus, at the optimum, the nonzero 
elements of the gradient of the objective function are linear combinations of 
the rows of the constraint matrix, AT λ∗. 
The k elements of the linear combination vector λ∗ are called Lagrange 
multipliers.

506
9 Selected Applications in Statistics
The Lagrangian 
Let us now consider a simple generalization of the constrained problem above 
and an abstraction of the results above so as to develop a general method. We 
consider the problem 
StartLayout 1st Row 1st Column min Underscript x Endscripts 2nd Column f left parenthesis x right parenthesis 2nd Row 1st Column normal s period normal t period 2nd Column c left parenthesis x right parenthesis equals 0 comma EndLayoutmin
x
f(x)
s.t. c(x) = 0,
(9.136) 
where f is a scalar-valued function of an m-vector variable and c is a k-
vector-valued function of the variable. There are some issues concerning the 
equation c(x) = 0 that we will not go into here. Obviously, we have the same 
concerns as before; that is, whether c(x) = 0 is consistent and whether the 
individual equations ci(x) = 0 are independent. Let us just assume they are, 
and proceed. (Again, we refer the interested reader to a more general text on 
optimization.) 
Motivated by the results above, we form a function that incorporates a 
dot product of Lagrange multipliers and the function c(x): 
upper F left parenthesis x right parenthesis equals f left parenthesis x right parenthesis plus lamda Superscript normal upper T Baseline c left parenthesis x right parenthesis periodF(x) = f(x) + λTc(x).
(9.137) 
This function is called the Lagrangian. The  solution, (x∗, λ∗), of the optimiza-
tion problem occurs at a stationary point of the Lagrangian, 
normal g Subscript f Baseline left parenthesis x Subscript asterisk Baseline right parenthesis equals StartBinomialOrMatrix 0 Choose normal upper J Subscript c Baseline left parenthesis x Subscript asterisk Baseline right parenthesis Superscript normal upper T Baseline lamda Subscript asterisk Baseline EndBinomialOrMatrix periodgf(x∗) =
(
0
Jc(x∗)Tλ∗
)
.
(9.138) 
Thus, at the optimum, the gradient of the objective function is a linear com-
bination of the columns of the Jacobian of the constraints. 
Another Example: The Rayleigh Quotient 
The important equation (3.290) on page 179 can also be derived by using 
diﬀerentiation. This equation involves maximization of the Rayleigh quotient 
(Eq. (3.291)): 
x Superscript normal upper T Baseline upper A x divided by x Superscript normal upper T Baseline xxTAx/xTx
under the constraint that x /= 0. In this function, this constraint is equivalent 
to the constraint that xT x is equal to a ﬁxed nonzero constant, which is 
canceled in the numerator and denominator. We can arbitrarily require that 
xT x = 1, and the problem is now to determine the maximum of xT Ax subject 
to the constraint xT x = 1. We now formulate the Lagrangian 
x Superscript normal upper T Baseline upper A x minus lamda left parenthesis x Superscript normal upper T Baseline x minus 1 right parenthesis commaxTAx −λ(xTx −1),
(9.139) 
diﬀerentiate, and set it equal to 0, yielding 
upper A x minus lamda x equals 0 periodAx −λx = 0.

9.6 Optimization of Scalar-Valued Functions
507
This implies that a stationary point of the Lagrangian occurs at an eigenvector 
and that the value of xT Ax is an eigenvalue. This leads to the conclusion that 
the maximum of the ratio is the maximum eigenvalue. We also see that the 
second-order necessary condition for a local maximum is satisﬁed; A − λI 
is nonpositive deﬁnite when λ is the maximum eigenvalue. (We can see this 
using the spectral decomposition of A and then subtracting λI.) Note that we 
do not have the suﬃcient condition that A − λI is negative deﬁnite (A −λI 
is obviously singular), but the fact that it is a maximum is established by 
inspection of the ﬁnite set of stationary points. 
Optimization of Functions with Inequality Constraints 
Inequality constraints are much more diﬃcult to deal with than equality con-
straints. The feasible region, instead of being a manifold of dimension m −1 
or less, may be a subset of the domain with dimension m. 
Inequality-Constrained Linear Least Squares Problems 
Similarly to the equality-constrained least squares optimization problem 
(9.130), an inequality-constrained least squares optimization problem can be 
written as 
StartLayout 1st Row 1st Column min Underscript b Endscripts 2nd Column f left parenthesis b right parenthesis equals left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis 2nd Row 1st Column normal s period normal t period 2nd Column upper A b less than or equals c comma EndLayout
min
b
f(b) = (y −Xb)T(y −Xb)
s.t. Ab ≤c,
(9.140) 
where A is a k × m matrix. There are no restrictions on k or on the rank 
of A as before, except as required to make the feasible region convex. (If the 
feasible region is not convex or it is not connected, we may need to decompose 
the optimization problem into a set of optimization problems.) In the most 
common cases the region is a convex cone. 
A simple instance of an inequality-constrained least squares problem is one 
with constraints that the parameters be nonnegative; that is, b ≥ 0. This is 
the problem we discussed in Sect. 5.7.3. As we mentioned there, Lawson and 
Hanson (1974, 1995) gave an algorithm to solve this constrained least squares 
problem and the R function nnls implements that algorithm. 
The analysis leading to a reduced gradient and a reduced Hessian and 
to the conditions given on page 505 that characterize a solution does not 
apply. In an equality-constrained problem, all of the constraints are active, 
in the sense that a feasible point can change only in limited ways and still 
satisfy the constraint. Geometrically, the feasible region lies in a hyperplane. 
(Of course, it may be restricted even further within the hyperplane.) In an 
inequality-constrained problem, however, the feasible region deﬁned by Ab ≤c 
consists of the intersection of halfspaces deﬁned by the rows of A. For  the  ith 
row of A, ai, the halfspace consists of all b satisfying aT 
i b ≤ ci. The outward-
pointing normal to the hyperplane deﬁning the halfspace is ai. For any point 
b(0) in the feasible region, a particular constraint, say aT 
i b ≤ ci, is “active” if

508
9 Selected Applications in Statistics
aT 
i b(0) = ci; otherwise, that particular constraint is not active, and points in 
a neighborhood of b(0) are also feasible.  
If b∗ is a solution to the constrained optimization problem (9.140), then 
the gradient at b∗, XT (y − Xb∗), would be zero except for the active con-
straints. The negative of the gradient therefore can be represented as a linear 
combination of the normals to the hyperplanes deﬁning the constraints that 
are active. Now, deﬁne a k-vector e such that ei ≥0 if  aT 
i b∗ = ci and ei = 0  if  
aT 
i b∗ < ci. Let  S (for “slack”) represent the set of all i such that ei = 0,  and  
let E (for “equal”) represent the set of all i such that ei ≥ 0. Then there are 
values of ei as deﬁned above such that 
sigma summation Underscript i Endscripts e Subscript i Baseline a Subscript i Baseline equals upper X Superscript normal upper T Baseline left parenthesis y minus upper X b Subscript asterisk Baseline right parenthesis period
E
i
eiai = XT(y −Xb∗).
(9.141) 
Now consider a perturbation of the optimal b∗, δb∗. In order  for  b∗ + 
δb∗ to remain feasible, the perturbation must satisfy δbT 
∗ ai ≤ 0 for all i ∈ 
E. Multiplying both sides of Eq. (9.141) by  δbT 
∗ , because ei ≥ 0, we have 
δbT 
∗ XT (y −Xb∗) ≤ 0. Because 
StartLayout 1st Row left parenthesis y minus upper X left parenthesis b Subscript asterisk Baseline plus delta b Subscript asterisk Baseline right parenthesis right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X left parenthesis b Subscript asterisk Baseline plus delta b Subscript asterisk Baseline right parenthesis right parenthesis equals 2nd Row left parenthesis y minus upper X b Subscript asterisk Baseline right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b Subscript asterisk Baseline right parenthesis minus 2 delta b Subscript asterisk Superscript normal upper T Baseline upper X Superscript normal upper T Baseline left parenthesis y minus upper X b Subscript asterisk Baseline right parenthesis plus left parenthesis upper X delta b Subscript asterisk Baseline right parenthesis Superscript normal upper T Baseline upper X delta b Subscript asterisk Baseline comma EndLayout(y −X(b∗+ δb∗))T(y −X(b∗+ δb∗)) =
(y −Xb∗)T(y −Xb∗) −2δbT
∗XT(y −Xb∗) + (Xδb∗)TXδb∗,
no feasible solution will further decrease the value of the objective function 
(y − Xb)T (y − Xb); that is, b∗ is optimal. 
We can summarize this discussion by the statement that b∗ is a solution 
for the problem (9.140) if and only if there exists a k-vector e such that ei ≥ 0 
if aT 
i b∗ = ci and ei = 0  if  aT 
i b∗ < ci, and  
upper A Superscript normal upper T Baseline e equals upper X Superscript normal upper T Baseline left parenthesis y minus upper X b Subscript asterisk Baseline right parenthesis periodATe = XT(y −Xb∗).
(9.142) 
These conditions are special cases of the Kuhn-Tucker or Karush-Kuhn-Tucker 
conditions for constrained optimization problems (see, e.g., Griva, Nash, and 
Sofer 2009). 
The variables in e are called dual variables. The nonzero dual variables are 
slack variables. 
Nonlinear Least Squares as an Inequality-Constrained Problem 
The Levenberg-Marquardt adjustment in Eq. (9.123) can be thought of as a 
Lagrangian multiplier formulation of the constrained problem: 
StartLayout 1st Row 1st Column min Underscript x Endscripts 2nd Column one half double vertical bar normal upper J Subscript r Baseline left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis x plus r left parenthesis theta Superscript left parenthesis k right parenthesis Baseline right parenthesis double vertical bar 3rd Column Blank 2nd Row 1st Column normal s period normal t period 2nd Column double vertical bar upper S Superscript left parenthesis k right parenthesis Baseline x double vertical bar less than or equals delta Subscript k Baseline period EndLayout min
x
1
2
||||Jr(θ(k))x + r(θ(k))
||||
(9.143) 
s.t.
||||S(k) x
|||| ≤δk. 
See Exercise 9.13. 
The Lagrange multiplier λ(k) is zero if p(k) from Eq. (9.122) satisﬁes
||p(k)|| ≤ δk; otherwise, it is chosen so that
||||S(k) p(k)|||| = δk.

9.6 Optimization of Scalar-Valued Functions
509
9.6.6 Optimization Without Diﬀerentiation 
Diﬀerentiation is often an eﬀective method for solving an optimization prob-
lem. It may lead us to a stationary point, and then in optimization problems, 
we must establish that the stationary point is indeed a maximum or minimum, 
which we do by use of second-order derivatives or else by inspection. 
If the objective function is not diﬀerentiable, then of course diﬀerentiation 
cannot be used. Also, even if the function is diﬀerentiable, if the optimum 
occurs on the boundary of the domain of the objective function, then diﬀer-
entiation may not ﬁnd it. Before attempting to solve an optimization problem, 
we should do some preliminary analysis of the problem. Often in working out 
maximum likelihood estimates, for example, students immediately think of 
diﬀerentiating, setting to 0, and solving. This requires that the likelihood 
function be diﬀerentiable, that it be concave, and that the maximum occur 
at an interior point of the parameter space. Keeping in mind exactly what 
the problem is –one of ﬁnding a maximum– often leads to the correct solution 
more quickly. 
Sometimes in optimization problems, even in the simple cases of diﬀeren-
tiable objective functions, it may be easier to use other methods to determine 
the optimum, that is, methods that do not depend on derivatives. For example, 
the Rayleigh quotient for which we determined the maximum by diﬀerentia-
tion above could be determined easily without diﬀerentiation, as we did on 
page 179. 
For another example, a constrained minimization problem we encounter 
occasionally is 
min Underscript upper X succeeds 0 Endscripts left parenthesis log StartAbsoluteValue upper X EndAbsoluteValue plus normal t normal r left parenthesis upper X Superscript negative 1 Baseline upper A right parenthesis right parenthesis min
X>0
(
log |X| + tr(X−1A)
)
(9.144) 
for a given positive deﬁnite matrix A and subject to X being positive deﬁnite. 
(The canonical problem giving rise to this minimization problem is the use of 
likelihood methods with a multivariate normal distribution. Also recall that 
an alternate notation for det(X) is  |X|, which I am using here, as I often do 
in working with the multivariate normal PDF, for example, in Eq. (7.38) on  
page 346.) 
The derivatives given in Tables 7.2 and 7.3 could be used to minimize the 
function in (9.144). The derivatives set equal to 0 immediately yield X = 
A. This means that X = A is a stationary point, but whether or not it is 
a minimum would require further analysis. As is often the case with such 
problems, an alternate approach leaves no such pesky complications. Let A 
and X be n×n positive deﬁnite matrices, and let c1, . . . , cn be the eigenvalues 
of X−1 A. Now, by property 8 on page 158 these are also the eigenvalues of 
X−1/2 AX−1/2 , which is positive deﬁnite (see inequality (3.172) on page 133). 
Now, consider the expression (9.144) with general X minus the expression 
with X = A:

510
9 Selected Applications in Statistics
StartLayout 1st Row 1st Column log StartAbsoluteValue upper X EndAbsoluteValue plus normal t normal r left parenthesis upper X Superscript negative 1 Baseline upper A right parenthesis minus log StartAbsoluteValue upper A EndAbsoluteValue minus normal t normal r left parenthesis upper A Superscript negative 1 Baseline upper A right parenthesis 2nd Column equals 3rd Column log StartAbsoluteValue upper X upper A Superscript negative 1 Baseline EndAbsoluteValue plus normal t normal r left parenthesis upper X Superscript negative 1 Baseline upper A right parenthesis minus normal t normal r left parenthesis upper I right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column minus log StartAbsoluteValue upper X Superscript negative 1 Baseline upper A EndAbsoluteValue plus normal t normal r left parenthesis upper X Superscript negative 1 Baseline upper A right parenthesis minus n 3rd Row 1st Column Blank 2nd Column equals 3rd Column minus log left parenthesis product Underscript i Endscripts c Subscript i Baseline right parenthesis plus sigma summation Underscript i Endscripts c Subscript i minus n 4th Row 1st Column Blank 2nd Column equals 3rd Column sigma summation Underscript i Endscripts left parenthesis minus log c Subscript i Baseline plus c Subscript i Baseline minus 1 right parenthesis 5th Row 1st Column Blank 2nd Column greater than or equals 3rd Column 0 EndLayoutlog |X| + tr(X−1A) −log |A| −tr(A−1A) = log |XA−1| + tr(X−1A) −tr(I)
= −log |X−1A| + tr(X−1A) −n
= −log
(| |
i
ci
)
+
E
i
ci −n
=
E
i
(−log ci + ci −1)
≥0
because if c >  0, then log c ≤ c −1, and the minimum occurs when each 
ci = 1,  that  is,  when  X−1 A = I. Thus, the minimum of expression (9.144) 
occurs uniquely at X = A. 
Appendix: R for Applications in Statistics 
In previous chapters we have discussed the use of R for computations in nu-
merical linear algebra. Many of the computations for statistical applications 
are just operations in numerical linear algebra, but the main thing that dis-
tinguishes a statistical software system from a big computer for linear algebra 
is the software’s provisions for dealing with data. 
Statistical data generally ﬁts well into the linear or rectangular structure 
of vectors and matrices, but some important characteristics of statistical data 
go beyond the basic structure. Important properties of statistical data and 
issues that must be considered in data analysis that go beyond the simple 
considerations of numerical linear algebra include the following. 
• Handling of missing data 
• Provision for nonnumeric data types, such as character strings and dates 
• Provision of an organization of data consistent with the basic characteris-
tics of the ﬂat ﬁle structure of a statistical dataset 
• Provision of language constructs to specify statistical models of data 
Systems for statistical data analysis provide for these in various ways. In the 
next few sections, I will brieﬂy describe the features of R that take it beyond 
a simple computational engine for linear algebra. 
Missing Data 
Statistical data often contain missing values; in a survey a respondent may 
refuse to answer a question; in an experimental procedure, an experimental 
unit may die or may just leave the procedure. 
An important issue for a statistical analysis system is how to represent 
a value that is missing. The proper way to do this is to use a computer bit 
pattern that is never used for a legitimate datum. The standard ways that

9.6 Optimization of Scalar-Valued Functions
511
data are represented in the computer allow some bit patterns not to represent 
any number or character. Such a bit pattern is called “not a number,” or NaN 
(see page 541). In R, the NaN reserved for a missing value is denoted NA. 
The next issue is what to do with a missing value. Many functions in R 
provide a standard way of requesting that this be done, by means of the logical 
argument na.rm. If  na.rm=TRUE, then the function removes any observations 
that contain a missing datum, and performs its operations on the complete 
valid observations. This is “casewise deletion.” If na.rm=FALSE, then the func-
tion performs its operations on all data, meaning that there may be some NAs 
in the output. 
Obviously, a simple arithmetic operation on a missing value should yield 
a missing value. 
> x <- NA  
> 5*x 
[1] NA 
> x+5 
[1] NA 
Often when data contain missing values, however, we wish to process all of 
the valid data, but not use NAs in any computations. See Sect. 9.4.6, begin-
ning on page 483, where we illustrate these issues in the computation of a 
variance-covariance matrix using data with some missing values. The use ar-
gument in the var, cov, and  cor functions determines whether observations 
that contain a missing datum are completely eliminated from the computa-
tions, as in na.rm=TRUE, whether the function performs its operations on all 
data, meaning that there may be some NAs in the output, or whether any 
binary operation involving a missing value is not performed, but all others 
are performed (“pairwise deletion”). 
A logical operation involving an NA does not identify the NA. R provides 
a special function, is.na, to test whether or not a value is NA. Thus, we have 
> x <- NA  
> x==NA 
[1] NA 
> x!=NA 
[1] NA 
> is.na(x) 
[1] TRUE 
See also Exercise  10.24 on page 589.

512
9 Selected Applications in Statistics
Nonnumeric Data Types 
Statistical data is often not numeric. The most common nonnumeric data are 
character strings, “a,” “green,” “Smith,” and so on. R provides a character 
data type and several functions to work with character data. 
In some statistical applications, especially in designed experiments, a vari-
able may represent a “factor,” that is, a setting of some quantity that may 
aﬀect the response under investigation. Generally a factor will take on only 
a small number of diﬀerent values, which are called “levels” and are often 
designated by integers. R provides a factor data type. 
Another common data type is a date. R handles dates in a uniform way, 
but it allows various formats for specifying and displaying dates. 
R Data Frames 
The basic structure of a statistical dataset is a rectangular structure, similar 
to a matrix, in which rows represent multivariate observations and columns 
represent variables or features of the observational units. 
A matrix may be adequate for storing a statistical dataset in some cases, 
but a matrix has many limitations. One obvious limitation is the inability to 
handle nonnumeric data. 
R provides a rectangular structure, called a data frame, that can accommo-
date mixtures of data types, both numeric and nonnumeric. An R data frame 
is of class data.frame and it has additional features related to a statistical 
dataset. 
A data frame is essentially a list of one-dimensional lists, all of the same 
length. The names of the lists, which may be vectors or one-dimensional ar-
rays, are the names of the statistical variables in the dataset. A data frame 
can be created from the individual lists or it can be created from a matrix. 
The data.frame function is used to create a data frame. 
The data.frame class inherits from the list class (see page 251), so the 
names function and the $ extractor can be used with data frames. 
> Name <- c("Bob","Jane","Tom") 
> Height <- c(72,64,69) 
> Weight <- c(200,130,191) 
> D <- data.frame(Name,Height,Weight) 
> D  
Name Height Weight 
1 Bob
72
200 
2 Jane
64
130 
3 Tom
69
191 
> names(D) 
[1] "Name"
"Height" "Weight" 
> D$Height 
[1] 72 64 69

9.6 Optimization of Scalar-Valued Functions
513
A list of class character in earlier versions of R would be converted to 
class factor. That is why you often see stringsAsFactors=FALSE in the 
data.frame function in old R scripts. FALSE is the default now. 
Data frames can be combined or updated using cbind and rbind, just as  
with matrices. Data frames can also be combined in other meaningful ways 
using the merge function. 
Other classes of objects can be created as enhancements of R data frames. 
A class that is useful in working with time series data is xts, which inherits 
from data.frame, but has the additional feature that the row indexes are of 
class date, and subarrays of the xts dataset can be formed using operations 
on dates. 
A variation of a data frame is a tibble, which is an object of class tibble. 
A tibble is similar to a data frame, but some operations such as printing and 
subsetting work diﬀerently. The dplyr package provides several functions for 
manipulating and searching data in a tibble. Tibbles are said to be a “modern” 
version of data frames. 
Linear Models 
As we have seen, R provides the standard univariate and bivariate statistical 
operations, such as computing means, standard deviations, and so on, and 
functions for graphical display of univariate and bivariate data. 
One of the biggest strengths of R is in the analysis of data in a linear 
model, 
y almost equals upper X beta commay ≈Xβ,
where y is an n-vector of observations and X is a matrix of n observed m 
covariates. While the expression Xβ is rather clear, there are various speciﬁc 
forms that could be represented by the right-hand side Xβ; for example, does 
it have an intercept—is one column of X a column of 1s?  
The modiﬁer “linear” refers to the form of β in the model and the coeﬃ-
cients of β can be nonlinear functions as in the model (9.3), 
upper Y equals beta Superscript normal upper T Baseline f left parenthesis x right parenthesis plus upper E commaY = βTf(x) + E,
where f(x) is a vector of functions of the independent variables and E is a 
random variable with a continuous distribution. 
The term “general linear model” was sometimes used to refer to a linear 
model that is not of full rank. Such a model is a combination of an ordinary 
regression model and an analysis of variance (ANOVA) model, and use of such 
a model is sometimes called analysis of covariance, or ANCOVA. 
The basic R function for analysis of linear models is lm. 
The R package lmtest provides a collection of tests for analysis of linear 
models using output from lm. The  lmtest package includes tests for diagnostic 
checking of model assumptions. It also contains data sets and examples for 
linear regression models.

514
9 Selected Applications in Statistics
Generalized Linear Models 
The form of the linear model is not suitable for discrete response variables. 
The response may be binary, “yes” or “no,” for example. It may still be the 
cases, however, that the response depends approximately on a linear function 
of some parameters, β. In the case of a binary response, which we would model 
with a Bernoulli distribution with parameter π, it may  be  the case that the  log  
odds, log(π/(1 −π)), depend on some covariates and parameters in a similar 
fashion as in the basic linear model. This leads to a “generalized linear model.” 
The form of a generalized linear model depends on the family of probability 
distributions of the response, Bernoulli (or binomial), Poisson, and so on. The 
form of the linearizing transformation is called the link function. If the family 
of distributions is binomial, for example, the link is the logit, or log odds, 
function. 
The generalized linear model is ﬁt by maximum likelihood based on the 
assumed distribution. As we have mentioned, the least squares ﬁt in a lin-
ear model in which the response has a normal distribution is a maximum 
likelihood ﬁt. 
The basic R function for analysis of a generalized linear model is glm. 
An argument in glm is family, which is used to specify the distribution 
family. Choices include binomial, poisson, and  gaussian. The model in the 
gaussian (normal) family is the basic general linear model, as in the lm R 
function. The link function in that case is the identity. 
A basic concept in both lm and glm is that of a formula, which is used to 
specify the form of the linear part of the model, βT f(x). 
The formula Object 
Statistical data analysis begins with a model, which is a description of the 
data and the method of analysis. The statistical model may be very complete 
and detailed, specifying probability distributions and relationships among the 
variables, or the model may be very general, only stating, for example, that 
some variable has a ﬁnite median. The range of possible models is so great 
that it would be diﬃcult to specify any and all models in a software analysis 
system. 
A simple model  y = Xβ + e can be speciﬁed with a vector for y and 
a matrix or vector for X, as in the input argument for the R functions lm 
and glm, or for the functions lsfit, l1fit, and  nnls mentioned in the R 
Appendix for Chap. 5, beginning on page 299. 
It is relatively simple, however, to specify a large class of regression models, 
and most statistical software systems provide a syntax for deﬁning linear 
regression models of the form, 
upper Y almost equals beta Superscript normal upper T Baseline f left parenthesis x right parenthesis commaY ≈βTf(x),

9.6 Optimization of Scalar-Valued Functions
515
in which the vector parameter β, which contains unknown values, enters the 
model linearly, and f(x) is a vector of functions of the independent variables 
and does not contain any nonobservable values. 
Assuming that a dataset with named variables has been deﬁned in the soft-
ware system, the necessary information that must be conveyed in the model 
statement is the name of the response variable and the components of the 
vector f(x), that is, which variables are involved and their functional forms. 
To ﬁt the model using a least squares criterion, the only information about 
the random error term is that it has 0 mean, it has constant variance, and it 
has 0 correlation from one observation to another. Hence, to use least squares, 
the model statement needs no information about the error term. Of course 
any statistics that the software may produce relating to statistical inference, 
such as conﬁdence intervals, p-values, and so on, depend on the distribution 
of the error term. Most statistical software systems just compute the statistics 
that are appropriate under the further assumption that the error term has a 
normal distribution. 
Statistical models in R are speciﬁed by a formula, which is an R object of 
class formula. A formula class object consists of three parts, 
monospace r monospace e monospace s monospace p monospace o monospace n monospace s monospace e monospace tilde monospace t monospace e monospace r monospace m monospace s commaresponse
~
terms,
(9.145) 
where “response” is a numeric response vector and “terms” is a series of vec-
tors or transformations of variables in a data frame specifying the predictors 
and their form in the model. The symbol ~ separates the response from the 
predictor portion of the model. 
The form of the predictor portion of the model is indicated by operators 
such as “+”, “*”, and “:”. The model is assumed to contain an additive 
constant, unless a “no-intercept” model is speciﬁed by “0+” or “-1”. For 
example, the R formula 
y ~ x1+x2 
may specify the model 
y Subscript i Baseline equals beta 0 plus beta 1 x Subscript 1 i Baseline plus beta 2 x Subscript 2 i Baseline plus epsilon Subscript i Baseline commayi = β0 + β1x1i + β2x2i + ei,
and 
y ~ 0+x1+x2 
may specify the model 
y Subscript i Baseline equals beta 1 x Subscript 1 i Baseline plus beta 2 x Subscript 2 i Baseline plus epsilon Subscript i Baseline periodyi = β1x1i + β2x2i + ei.
Mathematical functions and operators can be included in the formula, 
except for the exponentiation operator, which, because of its special meaning 
(see example below), must be escaped using the I function (see the example). 
Table 9.1 shows some examples, using the standard statistical notation and

516
9 Selected Applications in Statistics
Table 9.1. Model formulas in R 
y ~ x1+x2-1
β1x1 + β2x2 
y ~ x1+x2+x1:x2
β0 + β1x1 + β2x2 + β3x1x2 
y ~ x1*x2
β0 + β1x1 + β2x2 + β3x1x2 
y ~ (x1+x2+x3)^2
β0 + β1x1 + β2x2 + β3x3+ 
β4x1x2 + β5x1x3 + β6x2x3 
y ~ x1+I(x1^2)+log(x2) β0 + β1x1 + β2x2 
1 + β3 log(x2) 
with the obvious interpretation of the R objects as statistical variables. The 
operators “*”, “:”, and “^” are used primarily in classiﬁcation (AOV) models. 
The full meaning of the models in Table 9.1 depends on the usage. In a 
linear model, in each case, the response y is interpreted as the variable on the 
left side of the model, and an error term e is added to the right side of the 
model. 
A formula object can be constructed by the formula function and then 
that object can be used in other R functions that require a formula to be 
speciﬁed. 
The formula object may be interpreted diﬀerently in diﬀerent R functions. 
For example, in a generalized linear model (using the function glm), a family 
keyword is provided to specify the type of the generalized linear model. If 
family=binomial, for example, the response is interpreted as the logit of 
the variable speciﬁed, and terms following the ~ is the linear portion of the 
link function in the model. 
Regularized Fits 
The R function package glmnet by Friedman, Hastie, Narasimhan, Simon, 
Tay, and Tibshirani provides several functions for doing regularized regression 
ﬁts. The function glmnet in that package is the basic function for using an 
elastic net on linear models and generalized linear models. The model interface 
is similar to that of glm, in which  family=gaussian deﬁnes a linear model. 
The shrinkage penalty is similar to that of expression 9.81, but instead of λ1 
and λ2, there is a single λ and 0 ≤ α ≤ 1 to provide a weighting of the two 
factors. 
The R function package lars by Efron et al. (2004) provides several func-
tions for doing least angle regression ﬁts. The function lars in that package 
is the basic function. It uses a model speciﬁcation similar to lsfit, l1fit, 
and nnls instead of a formula object. 
Heteroscedastic-Robust Procedures 
Since linear models of economic data tend to be characterized by heteroscedas-
ticity, it is common for econometricians to use inferential procedures based

Exercises
517
on the general approach discussed on page 467. The R package sandwich 
contains functions for computing heteroscedasticity-robust estimates. The es-
timates have components of the “sandwich” form of expression (9.58). The two 
main functions are vcovHC, which computes the sandwich estimate of the full 
covariance matrix of the coeﬃcient estimates in a regression model (assuming 
full rank), and meatHC, which computes just the “meat” of the sandwich, that 
is, just a quantity of the form XT diag(r2 
1, . . . , r2 
n)X. 
The tests in the lmtest can use the output of vcovHC or meatHC to perform 
heteroscedasticity-robust tests. 
library(lmtest} 
library(sandwich} 
# Example: robust t-tests using model y = Xb + e 
... initialize dataframe yX with variables y, x1, x2, x3 
fit <- lm(data=yX, formula=y~x1 +x2 +x3) 
coeftest(fit, vcov=vcovHC(fit, type = ’HC0’)) 
The bptest function in the lmtest package performs White’s test to de-
termine if heteroscedasticity is present. 
Other Statistical Applications 
R also provides various functions for many other statistical applications such 
as time series analysis, survival analysis, and so on. Descriptions of these can 
be found in standard texts on statistical analysis in the various application 
areas. 
There are also general R functions for numerical optimization; see Nash 
(2022) for  a review.  
Exercises 
9.1. From Eq. (9.23), we have ˆyi = yT X(XT X)+ xi∗. Show that hii in 
Eq. (9.24) is  ∂ˆyi/∂yi. 
9.2. Formally prove from the deﬁnition that the sweep operator is its own 
inverse. 
9.3. Consider the regression model 
y equals upper X beta plus epsilony = Xβ + e
(9.146) 
subject to the linear equality constraints 
upper L beta equals c commaLβ = c,
(9.147) 
and assume that X is of full column rank.

518
9 Selected Applications in Statistics
a) Let λ be the vector of Lagrange multipliers. Form 
left parenthesis b Superscript normal upper T Baseline upper L Superscript normal upper T Baseline minus c Superscript normal upper T Baseline right parenthesis lamda(bTLT −cT)λ
and 
left parenthesis y minus upper X b right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X b right parenthesis plus left parenthesis b Superscript normal upper T Baseline upper L Superscript normal upper T Baseline minus c Superscript normal upper T Baseline right parenthesis lamda period(y −Xb)T(y −Xb) + (bTLT −cT)λ.
Now diﬀerentiate these two expressions with respect to λ and b, 
respectively, set the derivatives equal to zero, and solve to obtain 
StartLayout 1st Row 1st Column ModifyingAbove beta With caret Subscript upper C 2nd Column equals 3rd Column left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline y minus one half left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper L Superscript normal upper T Baseline ModifyingAbove lamda With caret Subscript upper C 2nd Row 1st Column Blank 2nd Column equals 3rd Column ModifyingAbove beta With caret minus one half left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper L Superscript normal upper T Baseline ModifyingAbove lamda With caret Subscript upper C EndLayout-βC = (XTX)−1XTy −1
2(XTX)−1LT-λC
= -β −1
2(XTX)−1LT-λC
and 
ModifyingAbove lamda With caret Subscript upper C Baseline equals minus 2 left parenthesis upper L left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper L Superscript normal upper T Baseline right parenthesis Superscript negative 1 Baseline left parenthesis c minus upper L ModifyingAbove beta With caret right parenthesis period-λC = −2(L(XTX)−1LT)−1(c −L-β).
Now combine and simplify these expressions to obtain expres-
sion (9.25) (on page 451). 
b) Prove that the stationary point obtained in Exercise 9.3a actually 
minimizes the residual sum of squares subject to the equality con-
straints. 
Hint: First express the residual sum of squares as 
left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis plus left parenthesis ModifyingAbove beta With caret minus b right parenthesis Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X left parenthesis ModifyingAbove beta With caret minus b right parenthesis comma(y −X -β)T(y −X -β) + (-β −b)TXTX(-β −b),
and show that is equal to 
left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis Superscript normal upper T Baseline left parenthesis y minus upper X ModifyingAbove beta With caret right parenthesis plus left parenthesis ModifyingAbove beta With caret minus ModifyingAbove beta With caret Subscript upper C Baseline right parenthesis Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X left parenthesis ModifyingAbove beta With caret minus ModifyingAbove beta With caret Subscript upper C Baseline right parenthesis plus left parenthesis ModifyingAbove beta With caret Subscript upper C Baseline minus b right parenthesis Superscript normal upper T Baseline upper X Superscript normal upper T Baseline upper X left parenthesis ModifyingAbove beta With caret Subscript upper C Baseline minus b right parenthesis comma(y−X -β)T(y−X -β)+(-β−-βC)TXTX(-β−-βC)+(-βC−b)TXTX(-βC−b),
which is minimized when b = -βC. 
c) Show that sweep operations applied to the matrix (9.26) on page 451 
yield the restricted least squares estimate in the (1,2) block. 
d) For the weighting matrix W, derive the expression, analogous to 
Eq. (9.25), for the generalized or weighted least squares estimator 
for β in Eq. (9.146) subject to the equality constraints (9.147). 
9.4. Derive a formula similar to Eq. (9.29) on page 455 to update -β due to 
the deletion of the ith observation. 
9.5. By diﬀerentiating expression (9.33), derive the normal equations (9.34) 
for the multivariate linear model. 
9.6. On page 506, we used Lagrange multipliers to determine the normalized 
vector x that maximized xT Ax. If  A is SX, this is the ﬁrst principal 
component. We also know the principal components from the spectral 
decomposition. We could also ﬁnd them by sequential solutions of La-
grangians. After ﬁnding the ﬁrst principal component, we would seek the 
linear combination z such that Xcz has maximum variance among all

Exercises
519
normalized z that are orthogonal to the space spanned by the ﬁrst prin-
cipal component; that is, such that z and the ﬁrst principal component 
are XT 
c Xc-conjugate (see Eq. (3.106) on page 115). If V1 is the matrix 
whose columns are the eigenvectors associated with the largest eigen-
vector, this is equivalent to ﬁnding z so as to maximize zT Sz subject to 
V T 
1 z = 0. Using the method of Lagrange multipliers as in Eq. (9.137), 
we form the Lagrangian corresponding to Eq. (9.139) as  
z Superscript normal upper T Baseline upper S z minus lamda left parenthesis z Superscript normal upper T Baseline z minus 1 right parenthesis minus phi upper V 1 Superscript normal upper T Baseline z commazTSz −λ(zTz −1) −φV T
1 z,
where λ is the Lagrange multiplier associated with the normalization 
requirement zT z = 1  and  φ is the Lagrange multiplier associated with 
the orthogonality requirement. Solve this for the second principal com-
ponent and show that it is the same as the eigenvector corresponding 
to the second-largest eigenvalue. 
9.7. Consider the least squares regression estimator (9.15) for full rank n×m 
matrix X (n > m): 
ModifyingAbove beta With caret equals left parenthesis upper X Superscript normal upper T Baseline upper X right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline y period-β = (XTX)−1XTy.
a) Compare this with the ridge estimator 
ModifyingAbove beta With caret Subscript upper R left parenthesis d right parenthesis Baseline equals left parenthesis upper X Superscript normal upper T Baseline upper X plus d upper I Subscript m Baseline right parenthesis Superscript negative 1 Baseline upper X Superscript normal upper T Baseline y-βR(d) = (XTX + dIm)−1XTy
for d ≥ 0. Show that 
parallel to ModifyingAbove beta With caret Subscript upper R left parenthesis d right parenthesis Baseline parallel to less than or equals parallel to ModifyingAbove beta With caret parallel to period||-βR(d)|| ≤||-β||.
b) Show that -βR(d) is the least squares solution to the regression model 
similar to y = Xβ + e except with some additional artiﬁcial data; 
that is, y is replaced with 
StartBinomialOrMatrix y Choose 0 EndBinomialOrMatrix comma
(
y
0
)
,
where 0 is an m-vector of 0s and X is replaced with 
StartBinomialOrMatrix upper X Choose d upper I Subscript m Baseline EndBinomialOrMatrix period
⎡X
dIm
⎤
.
(9.148) 
Now explain why -βR(d) is shorter than -β. 
9.8. Use the Schur decomposition (Eq. (3.204), page 144) of the inverse of 
(XT X) to prove  Eq. (9.72) on page 476. 
9.9. Construct a 9×2 matrix  X with some missing values, such that SX com-
puted using all available data for the covariance or correlation matrix is 
not nonnegative deﬁnite. 
9.10. Suppose the method in Exercise 9.22 converges to a positive deﬁnite 
matrix R(n) . Prove that all oﬀ-diagonal elements of R(n) are less than 
1 in absolute value. (This is true for any positive deﬁnite matrix with 
1s on the diagonal.)

520
9 Selected Applications in Statistics
9.11. Show that the matrices generated in Algorithm 7.1 are correlation ma-
trices. (They are clearly nonnegative deﬁnite, but how do we know that 
they have 1s on the diagonal?) 
9.12. Consider the log-likelihood l(μ, Σ; y) for  the  d-variate normal distribu-
tion, Eq. (9.126). Be aware of the subtle issue referred to in the text. It 
has to do with whether En 
i=1(yi − ¯y)(yi − ¯y)T is positive deﬁnite. 
a) Replace the parameters μ and Σ by the variables ˆμ and -Σ, take  
derivatives with respect to ˆμ and -Σ, set them equal to 0, and solve 
for the maximum likelihood estimates. What assumptions do you 
have to make about n and d? 
b) Another approach to maximizing the expression in Eq. (9.126) is  
to maximize the last term with respect to ˆμ (this is the only term 
involving μ) and then, with the maximizing value substituted, to 
maximize 
minus StartFraction n Over 2 EndFraction log StartAbsoluteValue normal upper Sigma EndAbsoluteValue minus one half normal t normal r left parenthesis normal upper Sigma Superscript negative 1 Baseline sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis y Subscript i Baseline minus y overbar right parenthesis left parenthesis y Subscript i Baseline minus y overbar right parenthesis Superscript normal upper T Baseline right parenthesis period−n
2 log |Σ| −1
2tr
(
Σ−1
n
E
i=1
(yi −¯y)(yi −¯y)T
)
.
Use this approach to determine the maximum likelihood estimates 
ˆμ and -Σ. 
9.13. Compare the constrained optimization problem (9.143) with the Levenberg-
Marquardt regularization in Eq. (9.123). Carefully identify the corre-
sponding terms in the two diﬀerent expressions. 
9.14. Consider a two-state Markov chain with transition matrix 
upper P equals Start 2 By 2 Matrix 1st Row 1st Column 1 minus alpha 2nd Column alpha 2nd Row 1st Column beta 2nd Column 1 minus beta EndMatrixP =
⎡
1 −α
α
β
1 −β
⎤
for 0 < α <  1 and  0  < β  <  1. Does an invariant distribution exist, and 
if so what is it? 
9.15. Recall from Exercise 8.10 that a Leslie matrix has a single unique posi-
tive eigenvalue. 
a) What are the conditions on a Leslie matrix A that allow a stable 
age distribution? Prove your assertion. 
Hint: Review the development of the power method in Eqs. (6.9) 
and (6.10). 
b) What are the conditions on a Leslie matrix A that allow a stable 
population, that is, for some xt, xt+1 = xt? 
c) Derive Eq. (9.98). (Recall that there are approximations that result 
from the use of a discrete model of a continuous process.) 
9.16. Derive Eqs. (9.100) and  (9.101) under the stationarity assumptions for 
the model (9.99). 
9.17. Derive the Yule-Walker equations (9.102) for the model (9.99).

Exercises
521
Computational Exercises 
These exercises are intended to work using R, but you may choose to use other 
software. 
9.18. The Gaussian sweep algorithm. 
a) Write an R function to implement Algorithm 9.1 (page 448) for  a  
given symmetric matrix and a given set of rows. Use the function 
deﬁnition 
Gsweep <- function(A,rows){ 
where rows is a vector indicating the rows of A to which Gaussian 
sweeps are to be performed. 
b) Generate a random 5 × 5 symmetric matrix A. (The elements may  
come from any probability distribution that you choose.) Sweep the 
ﬁrst three rows. Then sweep the ﬁrst three rows of the resulting 
matrix. 
Notice that the sweep operator is its own inverse. 
c) Sweep the ﬁrst four rows in the matrix A from Exercise 9.18b. 
9.19. When data are used to ﬁt a model such as y = Xβ + e, a large leverage 
of an observation is generally undesirable. If an observation with large 
leverage just happens not to ﬁt the “true” model well, it will cause -β to 
be farther from β than a similar observation with smaller leverage. 
a) Use artiﬁcial data to study inﬂuence. There are two main aspects 
to consider in choosing the data: the pattern of X and the values 
of the residuals in e. The true values of β are not too important, 
so β can be chosen as 1. Use 20 observations. First, use just one 
independent variable (yi = β0 + β1xi + ei). Generate 20 xis more or  
less equally spaced between 0 and 10, generate 20 eis, and form the 
corresponding yis. Fit the model, and plot the data and the model. 
Now, set x20 = 20, set e20 to various values, form the yis, and ﬁt 
the model for each value. Notice the inﬂuence of x20. 
Now, do similar studies with three independent variables. (Do not 
plot the data, but perform the computations and observe the eﬀect.) 
Carefully write up a clear description of your study with tables and 
plots. 
b) Heuristically, the leverage of a point arises from the distance from 
the point to a fulcrum. In the case of a linear regression model, the 
measure of the distance of observation i is 
upper Delta left parenthesis x Subscript i Baseline comma upper X Baseline 1 divided by n right parenthesis equals parallel to x Subscript i Baseline comma upper X Baseline 1 divided by n parallel to periodΔ(xi, X1/n) = ||xi, X1/n||.
(This is not the same quantity from the hat matrix that is deﬁned as 
the leverage on page 450, but it should be clear that the inﬂuence of 
a point  for which  Δ(xi, X1/n) is large is greater than that of a point

522
9 Selected Applications in Statistics
for which the quantity is small.) It may be possible to overcome some 
of the undesirable eﬀects of diﬀerential leverage by using weighted 
least squares to ﬁt the model. The weight wi would be a decreasing 
function of Δ(xi, X1/n). 
Now, using datasets similar to those used in the previous part of this 
exercise, study the use of various weighting schemes to control the 
inﬂuence. Weight functions that may be interesting to try include 
w Subscript i Baseline equals e Superscript minus upper Delta left parenthesis x Super Subscript i Superscript comma upper X Baseline 1 divided by n right parenthesiswi = e−Δ(xi,X1/n)
and 
w Subscript i Baseline equals max left parenthesis w Subscript normal m normal a normal x Baseline comma parallel to upper Delta left parenthesis x Subscript i Baseline comma upper X Baseline 1 divided by n right parenthesis parallel to Superscript negative p Baseline right parenthesiswi = max(wmax, ||Δ(xi, X1/n)||−p)
for some wmax and some p >  0. (Use your imagination!) 
Carefully write up a clear description of your study with tables and 
plots. 
c) Now repeat Exercise 9.19b except use a decreasing function of the 
leverage, hii from the hat matrix in Eq. (9.23) instead of the function 
Δ(xi, X1/n). 
Carefully write up a clear description of this study, and compare it 
with the results from Exercise 9.19b. 
9.20. Obtain the “Longley data.” (It is a dataset in R.) Each observation is for 
a year from 1947 to 1962 and consists of the number of people employed, 
ﬁve other economic variables, and the year itself. Longley (1967) ﬁtted 
the number of people employed to a linear combination of the other 
variables, including the year. 
a) Use a regression program to obtain the ﬁt. 
b) Now consider the year variable. The other variables are measured 
(estimated) at various times of the year, so replace the year vari-
able with a “midyear” variable (i.e., add 1 
2 to each year). Redo the 
regression. How do your estimates compare? 
c) Compute the L2 condition number of the matrix of independent 
variables. Now add a ridge regression diagonal matrix, as in the 
matrix (9.148), and compute the condition number of the resulting 
matrix. How do the two condition numbers compare? 
9.21. Given the matrix 
upper A equals Start 4 By 3 Matrix 1st Row 1st Column 2 2nd Column 1 3rd Column 3 2nd Row 1st Column 1 2nd Column 2 3rd Column 3 3rd Row 1st Column 1 2nd Column 1 3rd Column 1 4th Row 1st Column 1 2nd Column 0 3rd Column 1 EndMatrix commaA =
⎡
⎢⎢⎣
2 1 3
1 2 3
1 1 1
1 0 1
⎤
⎥⎥⎦,
assume the random 3 × 2 matrix  X is such that 
normal v normal e normal c left parenthesis upper X minus upper A right parenthesisvec(X −A)
has a N(0, V  ) distribution, where V is block diagonal with the matrix

Exercises
523
Start 4 By 4 Matrix 1st Row 1st Column 2 2nd Column 1 3rd Column 1 4th Column 1 2nd Row 1st Column 1 2nd Column 2 3rd Column 1 4th Column 1 3rd Row 1st Column 1 2nd Column 1 3rd Column 2 4th Column 1 4th Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 2 EndMatrix
⎡
⎢⎢⎣
2 1 1 1
1 2 1 1
1 1 2 1
1 1 1 2
⎤
⎥⎥⎦
along the diagonal. Generate ten realizations of X matrices, and use 
them to test that the rank of A is 2. Use the test statistic (9.85) on  
page 483. 
9.22. Consider an m×m symmetric nonsingular matrix, R, with 1s on the di-
agonal and with all oﬀ-diagonal elements less than 1 in absolute value. 
If this matrix is positive deﬁnite, it is a correlation matrix. Suppose, 
however, that some of the eigenvalues are negative. Iman and Daven-
port (1982) describe a method of adjusting the matrix to a “near-by” 
matrix that is positive deﬁnite. (See Ronald L. Iman and James M. Dav-
enport, 1982, An Iterative Algorithm to Produce a Positive Deﬁnite Cor-
relation Matrix from an “Approximate Correlation Matrix,” Sandia Re-
port SAND81-1376, Sandia National Laboratories, Albuquerque, New 
Mexico.) For their method, they assumed the eigenvalues are unique, 
but this is not necessary in the algorithm. 
Before beginning the algorithm, choose a small positive quantity, e, to  
use in the adjustments, set k = 0,  and  set  R(k) = R. 
1. Compute the eigenvalues of R(k) , 
c 1 greater than or equals c 2 greater than or equals ellipsis greater than or equals c Subscript m Baseline commac1 ≥c2 ≥. . . ≥cm,
and let p be the number of eigenvalues that are negative. If p = 0,  
stop. Otherwise, set 
c Subscript i Superscript asterisk Baseline equals StartLayout Enlarged left brace 1st Row 1st Column epsilon 2nd Column normal i normal f c Subscript i Baseline less than epsilon 2nd Row 1st Column c Subscript i Baseline 2nd Column normal o normal t normal h normal e normal r normal w normal i normal s normal e EndLayout normal f normal o normal r i equals p 1 comma ellipsis comma m minus p commac∗
i =
{e if ci < e
ci otherwise
for i = p1, . . . , m −p,
(9.149) 
where p1 = max(1, m − 2p). 
2. Let 
sigma summation Underscript i Endscripts c Subscript i Baseline v Subscript i Baseline v Subscript i Superscript normal upper T
E
i
civivT
i
be the spectral decomposition of R (Eq. (3.280), page 177), and form 
the matrix R∗: 
upper R Superscript asterisk Baseline equals sigma summation Underscript i equals 1 Overscript p 1 Endscripts c Subscript i Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline plus sigma summation Underscript i equals p 1 plus 1 Overscript m minus p Endscripts c Subscript i Superscript asterisk Baseline v Subscript i Baseline v Subscript i Superscript normal upper T Baseline plus sigma summation Underscript i equals m minus p plus 1 Overscript m Endscripts epsilon v Subscript i Baseline v Subscript i Superscript normal upper T Baseline periodR∗=
p1
E
i=1
civivT
i +
m−p
E
i=p1+1
c∗
i vivT
i +
m
E
i=m−p+1
evivT
i .
3. Form R(k) from R∗ by setting all diagonal elements to 1. 
4. Set k = k + 1, and go to step 1. (The algorithm iterates on k until 
p = 0.)  
Write a program to implement this adjustment algorithm. Write your 
program to accept any size matrix and a user-chosen value for e. Test  
your program on the correlation matrix from Exercise 9.9.

524
9 Selected Applications in Statistics
9.23. Consider some variations of the method in Exercise 9.22. For exam-
ple, do not make the adjustments as in Eq. (9.149) or make diﬀerent 
ones. Consider diﬀerent adjustments of R∗; for example, adjust any oﬀ-
diagonal elements that are greater than 1 in absolute value. 
Compare the performance of the variations. 
9.24. Investigate the convergence of the method in Exercise 9.22. Note that  
there are several ways the method could converge. 
9.25. Shrinkage adjustments of approximate correlation matrices. 
a) Write a program to implement the linear shrinkage adjustment of 
Eq. (9.87). Test your program on the correlation matrix from Exer-
cise 9.9. 
b) Write a program to implement the nonlinear shrinkage adjustment 
of Eq. (9.88). Let δ = 0.05 and 
f left parenthesis x right parenthesis equals hyperbolic tangent left parenthesis x right parenthesis periodf(x) = tanh(x).
Test your program on the correlation matrix from Exercise 9.9. 
c) Write a program to implement the scaling adjustment of Eq. (9.89). 
Recall that this method applies to an approximate correlation ma-
trix that is a pseudo-correlation matrix. Test your program on the 
correlation matrix from Exercise 9.9. 
9.26. Newton’s method. 
You should not, of course, just blindly pick a starting point and begin 
iterating. How can you be sure that your solution is a local optimum? 
Can you be sure that your solution is a global optimum? It is often a 
good idea to make some plots of the function. In the case of a function 
of a single variable, you may want to make plots in diﬀerent scales. For 
functions of more than one variable, proﬁle plots may be useful (i.e., 
plots of the function in one variable with all the other variables held 
constant). 
a) Use Newton’s method to determine the maximum of the function 
f(x) = sin(4x) − x4 /12. 
b) Use Newton’s method to determine the minimum of 
f left parenthesis x 1 comma x 2 right parenthesis equals 2 x 1 Superscript 4 Baseline plus 3 x 1 cubed plus 2 x 1 squared plus x 2 squared minus 4 x 1 x 2 periodf(x1, x2) = 2x4
1 + 3x3
1 + 2x2
1 + x2
2 −4x1x2.
What is the Hessian at the minimum?

Part III 
Numerical Methods and Software

10 
Numerical Methods 
The computer is a tool for storage, manipulation, and presentation of data. 
The data may be numbers, text, or images, but no matter what the data are, 
they must be coded into a sequence of 0s and 1s because that is what the 
computer stores. For each type of data, there are several ways of coding. For 
any unique coding scheme, the primary considerations are eﬃciency in storage, 
retrieval, and computations. Each of these considerations may depend on the 
computing system to be used. Another important consideration is coding that 
can be shared or transported to other systems. 
How much a computer user needs to know about the way the computer 
works depends on the complexity of the use and the extent to which the 
necessary operations of the computer have been encapsulated in software that 
is oriented toward the speciﬁc application. This chapter covers many of the 
basics of how digital computers represent data and perform operations on the 
data. Although some of the speciﬁc details we discuss will not be important 
for a computational scientist or for someone doing statistical computing, the 
consequences of those details are important, and the serious computer user 
must be at least vaguely aware of the consequences. 
There are some interesting facts that should cause anyone who programs 
a computer to take care: 
• Adding a positive number to a given positive number may not change that 
positive number 
• Performing two multiplications in one order may yield a diﬀerent result 
from doing the multiplications in the other order 
• For any given number, there is a “next” number 
• The next number after 1 is farther from 1 than the next number before 1 
• The next number after 1 is closer to 1 than the next number after 2 is to 2 
. . . and  so  on. . .  
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 10 
527

528
10 Numerical Methods
Some of the material in this chapter and the next chapter is covered more 
extensively in Gentle (2009), Chapters 2 through 6. 
10.1 Software Development 
From the perspective of economic theory, software does not ﬁt any of the 
standard molds of a “good.” The production cost is primarily in the initial 
development; production of copies and distribution is essentially without cost. 
The good can be modiﬁed and distributed with almost zero cost, other than 
the redevelopment expense. 
Software is an intellectual good, which is protected by copyrights, patents 
(for algorithms), or license agreements. Many software products are free. The 
Free Software Foundation maintains the legal underpinning for software li-
censes that guarantee free software to remain so. The R Foundation, which 
coordinates maintenance of R, and the Linux Foundation, which is ultimately 
responsible for the Linux operating system, are both members of the Free 
Software Foundation. 
A related movement among software developers and users is to ensure that 
the underlying coder-level code be available to inspect and modify, subject to 
agreements that require that any modiﬁcations be moved forward under the 
same agreements. The Open Source Initiative is a US 501(c)(3) nonproﬁt cor-
poration that deﬁnes “open source” and promotes it in software development. 
The Apache Software Foundation, likewise a US 501(c)(3) nonproﬁt corpo-
ration, has produced and made available many software products with open 
source, such as Hadoop and Spark. R is open source. 
10.1.1 Standards 
Standards are agreed-upon descriptions of processes, languages, or hardware. 
They facilitate development of computers and software. Most engineers are 
very familiar with standards for various types of hardware, such as electronic 
components, in which case the standard may specify a range for input voltage 
and current and a range for generated heat. In computing, standards may 
allow a software developer to know exactly what a piece of Fortran code will 
do, for example. 
Development of a standard often begins with one or more working groups 
of experts, then evolves through a process of public comment, and ﬁnally is 
adopted by a standards agency. Working groups are often formed by profes-
sional societies, for example, the International Federation for Information Pro-
cessing has a number of working groups, including the IFIP Working Group 
2.5 on Numerical Software. Standards agencies may be formed by professional 
societies or by governments. How useful a standard is depends on how widely 
the standard is followed; hence, larger, stronger, and fewer standards agencies 
are desirable.

10.1 Software Development
529
In this and the next two chapters we will mention a number of standards, 
the most important of which are the IEEE standards for computer represen-
tation of numeric values and computations with them and the ISO (formerly 
ANSI) standards for computer languages. ANSI published their standards in 
open documents, but ISO copyrights their publications and sells them. 
10.1.2 Coding Systems 
Data of whatever form are represented by groups of 0s and 1s, called bits 
from the words “binary” and “digits.” (The word was coined by John Tukey.) 
For representing simple text (that is, strings of characters each of which has 
no separate visual distinctions), the bits are usually taken in groups of eight, 
called bytes, and associated with a speciﬁc character according to a ﬁxed 
coding rule. Because of the common association of a byte with a character, 
those two words are often used synonymously. 
The most widely used code for representing characters in bytes is “ASCII” 
(pronounced “askey,” from American Standard Code for Information Inter-
change). Because the code is so widely used, the phrase “ASCII data” is some-
times used as a synonym for “text data” or “character data.” The ASCII code 
for the character “A”, for example, is 01000001; for “a” it is 01100001; and for 
“5” it is 00110101. Humans can more easily read shorter strings with several 
diﬀerent characters than they can longer strings, even if those longer strings 
consist of only two characters. Bits, therefore, are often grouped into strings 
of fours; a four-bit string is equivalent to a hexadecimal digit, 1, 2, ellipsis. . . , 9, A,  
B, ellipsis. . ., or F. Thus, the ASCII codes above could be written in hexadecimal 
notation as 41 (“A”), 61 (“a”), and 35 (“5”). 
Because the common character sets diﬀer from one language to another 
(both natural languages and computer languages), there are several modi-
ﬁcations of the basic ASCII code set. Also, when there is a need for more 
diﬀerent characters than can be represented in a byte (2 Superscript 828), codes to associate 
characters with larger groups of bits are necessary. For compatibility with the 
commonly used ASCII codes using groups of 8 bits, these codes usually are for 
groups of 16 bits. These codes for “16-bit characters” are useful for represent-
ing characters in some Oriental languages, for example. Unicode Consortium 
(1990, 1992) has developed a 16-bit standard, called Unicode, that is widely 
used for representing characters from a variety of languages. For any ASCII 
character, the Unicode representation uses eight leading 0s and then the same 
eight bits as the ASCII representation. 
A standard scheme for representing data is very important when data are 
moved from one computer system to another or when researchers at diﬀerent 
sites want to share data. Except for some bits that indicate how other bits 
are to be formed into groups (such as an indicator of the end of a ﬁle, or the 
delimiters of a record within a ﬁle), a set of data in ASCII representation is the 
same on diﬀerent computer systems. Software systems that process documents 
either are speciﬁc to a given computer system or must have some standard 
coding to allow portability. The Java system, for example, uses Unicode to 
represent characters so as to ensure that documents can be shared among 
widely disparate platforms.

530
10 Numerical Methods
In addition to standard schemes for representing the individual data el-
ements, there are some standard formats for organizing and storing sets of 
data. These standards specify properties of “data structure.” 
10.1.3 Types of Data 
Bytes that correspond to characters are often concatenated to form character 
string data (or just “strings”). Strings represent text without regard to the 
appearance of the text if it were to be printed. Thus, a string representing 
“ABC” does not distinguish between “ABC”, “ABC”, and “ABC”. The ap-
pearance of the printed character must be indicated some other way, perhaps 
by additional bit strings designating a font. 
The appearance of characters or other visual entities such as graphs or 
pictures is often represented more directly as a “bitmap.” Images on a display 
medium such as paper or a computer screen consist of an arrangement of 
small dots, possibly of various colors. The dots must be coded into a sequence 
of bits, and there are various coding schemes in use, such as JPEG (for Joint 
Photographic Experts Group). Image representations of “ABC”, “ABC”, and 
“ABC” would all be diﬀerent. The computer’s internal representation may 
correspond directly to the dots that are displayed or it may be a formula to 
generate the dots, but in each case, the data are represented as a set of dots 
located with respect to some coordinate system. More dots would be turned 
on to represent “ABC” than to represent  “ABC”.  The location of the  dots  
and the distance between the dots depend on the coordinate system; thus, the 
image can be repositioned or rescaled. 
Computers initially were used primarily to process numeric data, and num-
bers are still the most important type of data in statistical computing. There 
are important diﬀerences between the numerical quantities with which the 
computer works and the numerical quantities of everyday experience. The 
fact that numbers in the computer must have a ﬁnite representation has very 
important consequences. 
10.1.4 Missing Data 
In statistical computing and in any applications of the data sciences, we must 
deal with the reality of missing data. For any of a number of reasons, certain 
pieces of a dataset to be analyzed may be missing. Exactly how to proceed 
with an analysis when some of the data elements are missing depends on the 
nature of the data and the analysis to be performed and which data elements 
are missing. For example, there are three diﬀerent ways to compute a sample 
variance-covariance matrix from a dataset with missing elements, as discussed 
in Sect. 9.4.6, beginning on page 483. 
In this chapter, we are concerned with computer representation of data, 
so the ﬁrst issue is what to store in a computer memory location when the 
value that should go there is missing. We need a method for representing

10.1 Software Development
531
a “not available” (NA) value, and then we need a mechanism for avoiding 
operations with this NA value. There are various ways of doing this, and 
diﬀerent software systems provide diﬀerent methods. The standard computer 
number system itself provides for special computer numbers, as we will discuss 
on page 541. 
10.1.5 Data Structures 
The user’s interactions with the computing system may include data in-
put/output and other transmissions and various manipulations of the data. 
The organization of data is called the data structure, and there are various 
standard types of data structures that are useful for diﬀerent types of compu-
tations. In many numerical computations, such as those involving vectors and 
matrices, the data structures are usually very simple and correspond closely 
to the ordinary mathematical representation of the entities. In some cases, 
however, the data may be organized so as to optimize for sparsity or in or-
der to take advantage of some aspect of the computer’s architecture, such as 
interleaved memory banks. In large-scale computational problems, the data 
may be stored in separate ﬁles that reside on separate computing systems. 
For more general databases, commercial software vendors have deﬁned 
data structures and developed data management systems to support access-
ing and updating the datasets. Some of the details of those systems may be 
proprietary. 
There are also some open systems that are widely used in science appli-
cations. One is the Common Data Format (CDF), developed by the NASA’s 
National Space Science Data Center, and dates from the 1980s. It is essentially 
a programming interface for storage and manipulation of multidimensional 
data. A related system, NetCDF, is built on CDF. There are R packages 
(ncdf.tools and ncdf4) to read and write NetCDF ﬁles. NetCDF is the 
standard data structure for some software systems for spatial statistics. 
Another standard structure is the Hierarchical Data Format (HDF), devel-
oped by the National Center for Supercomputing Applications. The current 
version of the Hierarchical Data Format, HDF5, which is built on NetCDF, 
is supported in many software systems including R and MATLAB, as well as 
Fortran, C, Java, and Python. 
Both CDF and HDF standards allow a variety of types and structures of 
data; the standardization is in the descriptions that accompany the datasets. 
10.1.6 Computer Architectures and File Systems 
The user interacts with the computing system at one level through an operat-
ing system, such as Linux, Microsoft Windows, or Apple iOS, and at diﬀerent 
levels through applications software or other types of software systems. The 
general organizational structure or architecture of the computing system ulti-
mately determines how the user interacts with the computing system. We call

532
10 Numerical Methods
the way that the user interacts with the computing system the programming 
model, and it generally consists of multiple simpler programming models. The 
simplest and probably most common programming model applies to a single 
user on a single computing system running a single process. In this model, 
the computing system appears to the user as a single entity, although the 
processing unit itself may consist of multiple processors (or “cores”). Multiple 
processors allow for various types of parallel processing. Even if the comput-
ing system actually consists of several separate “computers,” the same model 
can apply. If the computing system allows for multiple computations to occur 
simultaneously or when more than one computer comprises the system, a pro-
gramming model that allows control of the separate processors may be more 
appropriate. This is especially true if there are diﬀerent computers that are 
linked to make up the computer system. This kind of setup is called distributed 
computing. 
How data input and output are performed depends on a ﬁle system. A  
single computer may have a simple ﬁle system that the user is often not even 
aware of. 
More interesting computing applications may involve a distributed envi-
ronment across many computers or clusters of computers and many program-
ming models. Instead of an ad hoc top-level programming model, it is desirable 
to have a model that scales from a single system to many machines, each with 
its own local computing and storage resources. The challenge is to access each 
resource seamlessly. MapReduce is an eﬃcient method for doing this that was 
developed by Google, Inc. (now Alphabet, Inc.). We will discuss MapReduce 
on page 581 and again brieﬂy on page 601. 
Hadoop is an open-source system based on MapReduce methods that pro-
vides a scalable programming model. The basic feature of Hadoop is a ﬁle 
system, called the Hadoop Distributed File System or HDFS. Hadoop was 
produced by the Apache Software Foundation, as was Apache Spark, a ﬁle 
system that combines features of MapReduce and Hadoop. 
10.2 Digital Representation of Numeric Data 
For representing a number in a ﬁnite number of digits or bits, the two most 
relevant things are the magnitude of the number and the precision with which 
the number is to be represented. Whenever a set of numbers  are to be used  
in the same context, we must ﬁnd a method of representing the numbers that 
will accommodate their full range and will carry enough precision for all of 
the numbers in the set. 
Another important aspect in the choice of a method to represent data is the 
way data are communicated within a computer and between the computer and 
peripheral components such as data storage units. Data are usually treated 
as a ﬁxed-length sequence of bits. The basic grouping of bits in a computer 
is sometimes called a “word” or a “storage unit.” The length of words or

10.2 Digital Representation of Numeric Data
533
storage units commonly used in computers is 64 bits, although some systems 
use 32 bits or other lengths. (In the following examples, I will use 32 bits for 
simplicity.) 
It is desirable to have diﬀerent kinds of representations for diﬀerent sets of 
numbers, even on the same computer. Like the ASCII standard for characters, 
however, there are some standards for representation of, and operations on, 
numeric data. The Institute of Electrical and Electronics Engineers (IEEE) 
and, subsequently, the International Electrotechnical Commission (IEC) have 
been active in promulgating these standards, and the standards themselves 
are designated by an IEEE number and/or an IEC number. 
The two mathematical models that are often used for numeric data are the 
ring of integers, normal upper Z normal upper ZZZ, and the ﬁeld of reals, normal upper I normal upper RIR. We use two computer models, normal upper I normal upper III
and normal upper I normal upper FIF, to simulate these mathematical entities. (Neither normal upper I normal upper III nor normal upper I normal upper FIF is a simple 
mathematical construct such as a ring or ﬁeld.) 
10.2.1 The Fixed-Point Number System 
Because an important set of numbers is a ﬁnite set of reasonably sized inte-
gers, eﬃcient schemes for representing these special numbers are available in 
most computing systems. The scheme is usually some form of a base 2 repre-
sentation and may use one storage unit (this is most common), two storage 
units, or one half of a storage unit. For example, if a storage unit consists 
of 32 bits and one storage unit is used to represent an integer, the integer 5 
may be represented in binary notation using the low-order bits, as shown in 
Fig. 10.1. 
Figure 10.1. The value 5 in a binary representation 
The sequence of bits in Fig. 10.1 represents the value 5, using one storage 
unit. The character “5” is represented in the ASCII code shown previously, 
00110101. 
If the set of integers includes the negative numbers also, some way of 
indicating the sign must be available. The ﬁrst bit in the bit sequence (usually 
one storage unit) representing an integer is usually used to indicate the sign; 
if it is 0, a positive number is represented; if it is 1, a negative number is 
represented. In a common method for representing negative integers, called 
“two’s-complement representation,” the sign bit is set to 1 and the remaining 
bits are set to their opposite values (0 for 1; 1 for 0), and then 1 is added to 
the result. If the bits for 5 are . . . 00101, the bits for negative 5−5 would be . . . 11010 
plus+ 1, or . . . 11011. If there are k bits in a storage unit (and one storage unit 
is used to represent a single integer), the integers from 0 through 2 Superscript k minus 1 Baseline minus 12k−1 −1
would be represented in ordinary binary notation using k minus 1k −1 bits. An integer

534
10 Numerical Methods
i in the interval left bracket minus 2 Superscript k minus 1 Baseline comma negative 1 right bracket[−2k−1, −1] would be represented by the same bit pattern 
by which the nonnegative integer 2 Superscript k minus 1 Baseline plus i2k−1 + i is represented, except the sign bit 
would be 1. 
The sequence of bits in Fig. 10.2 represents the value negative 5−5 using two’s-
complement notation in 32 bits, with the leftmost bit being the sign bit and 
the rightmost bit being the least signiﬁcant bit, that is, the 1 position. The 
ASCII code for “negative 5−5” consists of the codes for “minus−” and “5”, that is, 00101101 
00110101. 
Figure 10.2. The value negative 5−5 in a two’s-complement representation 
The special representations for numeric data are usually chosen so as to 
facilitate manipulation of data. The two’s-complement representation makes 
arithmetic operations particularly simple. It is easy to see that the largest 
integer that can be represented in the two’s-complement form is 2 Superscript k minus 1 Baseline minus 12k−1 −1 and 
that the smallest integer is minus 2 Superscript k minus 1−2k−1. 
A representation scheme such as that described above is called ﬁxed-point 
representation or integer representation, and the set of such numbers is de-
noted by normal upper I normal upper III. The notation normal upper I normal upper III is also used to denote the system built on this set. 
This system is similar in some ways to a ring, which is what the integers normal upper Z normal upper ZZZ
are. 
Software Representation and Big Integers 
The description above relates directly to how the bits in computer memory 
are stored in a ﬁxed-point representation. The number of bits used and the 
method of representing negative numbers are two aspects that may vary from 
one computer to another. Various software systems provide other options, 
such as “long,” “short,” “signed,” and “unsigned,” so that even within a 
single computer system, the number of bits used in ﬁxed-point representation 
may vary. The common number of bits is typically one or two storage units 
or half of a storage unit. 
A diﬀerent approach implemented in the software is to use as much mem-
ory as necessary to represent any integer value, if the memory is available. 
Such a scheme is called big integer representation. The representation is also 
known as multiple precision, arbitrary precision, or inﬁnite precision integer. 
GMP, for “GNU Multiple Precision,” is a C and C++ library that provides 
mathematical operations involving big integers, as well as multiple-precision 
ﬂoating-point numbers. 
Both the gmp package and the Rmpfr package in R (see page 582) are  based  
on the GMP software library.

10.2 Digital Representation of Numeric Data
535
We discuss the operations with numbers in the ﬁxed-point system in 
Sect. 10.3.1. 
10.2.2 The Floating-Point Model for Real Numbers 
In a ﬁxed-point representation, all bits represent values greater than or equal 
to 1; the base point or radix point is at the far right, before the ﬁrst bit. In 
a ﬁxed-point representation scheme using k bits, the range of representable 
numbers is of the  order of  2 Superscript k2k, usually from approximately minus 2 Superscript k minus 1−2k−1 to 2 Superscript k minus 12k−1. 
Numbers outside of this range cannot be represented directly in the ﬁxed-
point scheme. Likewise, nonintegral numbers cannot be represented. Large 
numbers and fractional numbers are generally represented in a scheme similar 
to what is sometimes called “scientiﬁc notation” or in a type of logarithmic 
notation. Because within a ﬁxed number of digits the radix point is not ﬁxed, 
this scheme is called ﬂoating-point representation, and the set of such numbers 
is denoted by normal upper I normal upper FIF. The notation normal upper I normal upper FIF is also used to denote the system built on 
this set. 
In a misplaced analogy to the real numbers, a ﬂoating-point number is also 
called “real.” Both computer “integers,” normal upper I normal upper III, and “reals,” normal upper I normal upper FIF, represent useful 
subsets of the corresponding mathematical entities, normal upper Z normal upper ZZZ and normal upper I normal upper RIR, but while the 
computer numbers called “integers” do constitute a fairly simple subset of the 
integers, the computer numbers called “real” do not correspond to the real 
numbers in a natural way. In particular, the ﬂoating-point numbers do not 
occur uniformly over the real number line. 
Within the allowable range, a mathematical integer is exactly represented 
by a computer ﬁxed-point number, but a given real number, even a rational, 
of any size may or may not have an exact representation by a ﬂoating-point 
number. This is the familiar situation where fractions such as one third1
3 have no ﬁnite 
representation in base 10. The simple rule, of course, is that the number must 
be a rational number whose denominator in reduced form factors into only 
primes that appear in the factorization of the base. In base 10, for exam-
ple, only rational numbers whose factored denominators contain only 2s and 
5s have an exact, ﬁnite representation, and in base 2, only rational numbers 
whose factored denominators contain only 2s have an exact, ﬁnite represen-
tation. 
For a given real number x, we will occasionally use the notation 
left bracket x right bracket Subscript normal c[x]c
to indicate the ﬂoating-point number used to approximate x, and we will refer 
to the exact value of a ﬂoating-point number as a computer number. We will 
also use the phrase “computer number” to refer to the value of a computer 
ﬁxed-point number. It is important to understand that the sets of computer 
numbers normal upper I normal upper III and normal upper I normal upper FIF are ﬁnite. The set of ﬁxed-point numbers normal upper I normal upper III is a proper 
subset of normal upper Z normal upper ZZZ. The set of ﬂoating-point numbers is almost a proper subset of

536
10 Numerical Methods
normal upper I normal upper RIR, but it is not a subset because it contains some numbers not in normal upper I normal upper RIR; see the 
special ﬂoating-point numbers discussed on page 541. 
Our main task in using computers, of course, is not to evaluate functions of 
the set of computer ﬂoating-point numbers or the set of computer integers; the 
main immediate task usually is to perform operations in the ﬁeld of real (or 
complex) numbers or occasionally in the ring of integers. Doing computations 
on the computer, then, involves using the sets of computer numbers to simulate 
the sets of reals or integers. 
The Parameters of the Floating-Point Representation 
The parameters necessary to deﬁne a ﬂoating-point representation are the 
base or radix, the range of the mantissa or signiﬁcand, and the range of the 
exponent. Because the number is to be represented in a ﬁxed number of bits, 
such as one storage unit or word, the ranges of the signiﬁcand and exponent 
must be chosen judiciously so as to ﬁt within the number of bits available. If 
the radix is b and the integer digits d Subscript idi are such that 0 less than or equals d Subscript i Baseline less than b0 ≤di < b, and there 
are enough bits in the signiﬁcand to represent p digits, then a real number is 
approximated by 
plus or minus 0 period d 1 d 2 midline horizontal ellipsis d Subscript p Baseline times b Superscript e Baseline comma ± 0.d1d2 · · · dp × be,
(10.1) 
where e is an integer. This is the standard model for the ﬂoating-point repre-
sentation. (The d Subscript idi are called “digits” from the common use of base 10.) 
The number of bits allocated to the exponent e must be suﬃcient to rep-
resent numbers within a reasonable range of magnitudes; that is, so that the 
smallest number in magnitude that may be of interest is approximately b Superscript e Super Subscript normal m normal i normal nbemin
and the largest number of interest is approximately b Superscript e Super Subscript normal m normal a normal xbemax, where  e Subscript normal m normal i normal nemin and e Subscript normal m normal a normal xemax
are, respectively, the smallest and the largest allowable values of the exponent. 
Because e Subscript normal m normal i normal nemin is likely negative and e Subscript normal m normal a normal xemax is positive, the exponent requires a 
sign. In practice, most computer systems handle the sign of the exponent by 
deﬁning a bias and then subtracting the bias from the value of the exponent 
evaluated without regard to a sign. 
The parameters b, p, and  e Subscript normal m normal i normal nemin and e Subscript normal m normal a normal xemax are so fundamental to the oper-
ations of the computer that on most computers they are ﬁxed, except for a 
choice of two or three values for p and maybe two choices for the range of e. 
In order to ensure a unique representation for all numbers (except 0), 
most ﬂoating-point systems require that the leading digit in the signiﬁcand 
be nonzero unless the magnitude is less than b Superscript e Super Subscript normal m normal i normal nbemin. A number with a nonzero 
leading digit in the signiﬁcand is said to be normalized. 
The most common value of the base b is 2, although 16 and even 10 are 
sometimes used. If the base is 2, in a normalized representation, the ﬁrst 
digit in the signiﬁcand is always 1; therefore, it is not necessary to ﬁll that 
bit position, and so we eﬀectively have an extra bit in the signiﬁcand. The 
leading bit, which is not represented, is called a “hidden bit.” This requires a 
special representation for the number 0, however.

10.2 Digital Representation of Numeric Data
537
In a typical computer using a base of 2 and 64 bits to represent one ﬂoating-
point number, 1 bit may be designated as the sign bit, 52 bits allocated to the 
signiﬁcand, and 11 bits allocated to the exponent. The arrangement of these 
bits is somewhat arbitrary, and of course the physical arrangement on some 
kind of storage medium would be diﬀerent from the “logical” arrangement. 
A common logical arrangement assigns the ﬁrst bit as the sign bit, the next 
11 bits as the exponent, and the last 52 bits as the signiﬁcand. (Computer 
engineers sometimes label these bits as 0 comma 1 comma ellipsis0, 1, . . . and then get confused as to 
which is the i normal t normal hith bit. When we say “ﬁrst,” we mean “ﬁrst,” whether an engineer 
calls it the “0 normal t normal h0th” or the  “1 normal s normal t1st.”) The range of exponents for the base of 2 in this 
typical computer would be 2,048. If this range is split evenly between positive 
and negative values, the range of orders of magnitude of representable numbers 
would be from negative 308−308 to 308. The bits allocated to the signiﬁcand would provide 
roughly 16 decimal places of precision. 
Figure 10.3 shows the bit pattern to represent the number 5, using b equals 2b = 2, 
p equals 24p = 24, e Subscript normal m normal i normal n Baseline equals negative 126emin = −126, and a bias of 127, in a word of 32 bits. The ﬁrst bit on the 
left is the sign bit; the next 8 bits represent the exponent, 129, in ordinary base 
2 with a bias; and the remaining 23 bits represent the signiﬁcand beyond the 
leading bit, known to be 1. (The binary point is to the right of the leading bit 
that is not represented.) The value is therefore plus 1.01 times 2 squared+1.01 × 22 in binary notation. 
Figure 10.3. The value 5 in a ﬂoating-point representation 
While in ﬁxed-point two’s-complement representations there are consider-
able diﬀerences between the representation of a given integer and the nega-
tive of that integer (see Figs. 10.1 and 10.2), the only diﬀerence between the 
ﬂoating-point representation of a number and its additive inverse is usually 
just in one bit. In the example of Fig. 10.3, only the ﬁrst bit would be changed 
to represent the number negative 5−5. 
As mentioned above, the set of ﬂoating-point numbers is not uniformly 
distributed over the ordered set of the reals. There are the same number of 
ﬂoating-point numbers in the interval left bracket b Superscript i Baseline comma b Superscript i plus 1 Baseline right bracket[bi, bi+1] as in the interval left bracket b Superscript i plus 1 Baseline comma b Superscript i plus 2 Baseline right bracket[bi+1, bi+2], 
even though the second interval is b times as long as the ﬁrst. Figures 10.4, 
10.5, and  10.6 illustrate this. 
Figure 10.4. The ﬂoating-point number line, nonnegative half 
The ﬁxed-point numbers, on the other hand, are uniformly distributed 
over their range, as illustrated in Fig. 10.7. 
The density of the ﬂoating-point numbers is generally greater closer to 
zero. Notice that if ﬂoating-point numbers are all normalized, the spacing

538
10 Numerical Methods
Figure 10.5. The ﬂoating-point number line, nonpositive half 
Figure 10.6. The ﬂoating-point number line, nonnegative half; another view 
Figure 10.7. The ﬁxed-point number line, nonnegative half 
between 0 and b Superscript e Super Subscript normal m normal i normal nbemin is b Superscript e Super Subscript normal m normal i normal nbemin (that is, there is no ﬂoating-point number in that 
open interval), whereas the spacing between b Superscript e Super Subscript normal m normal i normal nbemin and b Superscript e Super Subscript normal m normal i normal n Superscript plus 1bemin+1 is b Superscript e Super Subscript normal m normal i normal n Superscript minus p plus 1bemin−p+1. 
Most systems do not require ﬂoating-point numbers less than b Superscript e Super Subscript normal m normal i normal nbemin in mag-
nitude to be normalized. This means that the spacing between 0 and b Superscript e Super Subscript normal m normal i normal nbemin can 
be b Superscript e Super Subscript normal m normal i normal n Superscript minus pbemin−p, which is more consistent with the spacing just above b Superscript e Super Subscript normal m normal i normal nbemin. When  
these nonnormalized numbers are the result of arithmetic operations, the re-
sult is called “graceful” or “gradual” underﬂow. 
The spacing between ﬂoating-point numbers has some interesting (and, 
for the novice computer user, surprising!) consequences. For example, if 1 is 
repeatedly added to x, by the recursion 
x Superscript left parenthesis k plus 1 right parenthesis Baseline equals x Superscript left parenthesis k right parenthesis Baseline plus 1 commax(k+1) = x(k) + 1,
the resulting quantity does not continue to get ever larger. Obviously, it could 
not increase without bound because of the ﬁnite representation. It does not 
even approach the largest number representable, however! (This is assuming 
that the parameters of the ﬂoating-point representation are reasonable ones.) 
In fact, if x is initially smaller in absolute value than b Superscript e Super Subscript normal m normal a normal x Superscript minus pbemax−p (approximately), 
the recursion 
x Superscript left parenthesis k plus 1 right parenthesis Baseline equals x Superscript left parenthesis k right parenthesis Baseline plus cx(k+1) = x(k) + c
will converge to a stationary point for any value of c smaller in absolute value 
than b Superscript e Super Subscript normal m normal a normal x Superscript minus pbemax−p. 
The way the arithmetic is performed would determine these values pre-
cisely; as we shall see below, arithmetic operations may utilize more bits than 
are used in the representation of the individual operands. 
The spacings of numbers just smaller than 1 and just larger than 1 are 
particularly interesting. This is because we can determine the relative spac-
ing at any point by knowing the spacing around 1. These spacings at 1 are 
sometimes called the “machine epsilons,” denoted epsilon Subscript normal m normal i normal nemin and epsilon Subscript normal m normal a normal xemax (not to be 
confused with e Subscript normal m normal i normal nemin and e Subscript normal m normal a normal xemax deﬁned earlier). It is easy to see from the model 
for ﬂoating-point numbers on page 536 that 
epsilon Subscript normal m normal i normal n Baseline equals b Superscript negative pemin = b−p

10.2 Digital Representation of Numeric Data
539
and 
epsilon Subscript normal m normal a normal x Baseline equals b Superscript 1 minus p Baseline semicolonemax = b1−p;
see Fig. 10.8. The more conservative value, epsilon Subscript normal m normal a normal xemax, sometimes called “the ma-
chine epsilon,” epsilone or epsilon Subscript normal m normal a normal c normal hemach, provides an upper bound on the rounding that 
occurs when a ﬂoating-point number is chosen to represent a real number. A 
ﬂoating-point number near 1 can be chosen within epsilon Subscript normal m normal a normal x Baseline divided by 2emax/2 of a real number 
that is near 1. This bound, one half b Superscript 1 minus p 1
2b1−p, is called the unit roundoﬀ. 
Figure 10.8. Relative spacings at 1: “machine epsilons” 
These machine epsilons are also called the “smallest relative spacing” and 
the “largest relative spacing” because they can be used to determine the 
relative spacing at the point x (Fig. 10.9). 
Figure 10.9. Relative spacings 
If x is not zero, the relative spacing at x is approximately 
StartFraction x minus left parenthesis 1 minus epsilon Subscript normal m normal i normal n Baseline right parenthesis x Over x EndFractionx −(1 −emin)x
x
or 
StartFraction left parenthesis 1 plus epsilon Subscript normal m normal a normal x Baseline right parenthesis x minus x Over x EndFraction period(1 + emax)x −x
x
.
Notice that we say “approximately.” First of all, we do not even know that x 
is representable. Although left parenthesis 1 minus epsilon Subscript normal m normal i normal n Baseline right parenthesis(1 −emin) and left parenthesis 1 plus epsilon Subscript normal m normal a normal x Baseline right parenthesis(1 + emax) are members of the set of 
ﬂoating-point numbers by deﬁnition, that does not guarantee that the product 
of either of these numbers and left bracket x right bracket Subscript normal c[x]c is also a member of the set of ﬂoating-point 
numbers. However, the quantities left bracket left parenthesis 1 minus epsilon Subscript normal m normal i normal n Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c[(1−emin)[x]c]c and left bracket left parenthesis 1 plus epsilon Subscript normal m normal a normal x Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c[(1+emax)[x]c]c are rep-
resentable (by the deﬁnition of left bracket dot right bracket Subscript normal c[·]c as a ﬂoating-point number approximating 
the quantity within the brackets), and, in fact, they are respectively the next 
smallest number than left bracket x right bracket Subscript normal c[x]c (if left bracket x right bracket Subscript normal c[x]c is positive, or the next largest number other-
wise) and the next largest number than left bracket x right bracket Subscript normal c[x]c (if left bracket x right bracket Subscript normal c[x]c is positive). The spacings 
at left bracket x right bracket Subscript normal c[x]c therefore are 
left bracket x right bracket Subscript normal c Baseline minus left bracket left parenthesis 1 minus epsilon Subscript normal m normal i normal n Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c[x]c −[(1 −emin)[x]c]c
and

540
10 Numerical Methods
left bracket left parenthesis 1 plus epsilon Subscript normal m normal a normal x Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline minus left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c Baseline period[(1 + emax)[x]c −[x]c]c.
As an aside, note that this implies it is probable that 
left bracket left parenthesis 1 minus epsilon Subscript normal m normal i normal n Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c Baseline equals left bracket left parenthesis 1 plus epsilon Subscript normal m normal i normal n Baseline right parenthesis left bracket x right bracket Subscript normal c Baseline right bracket Subscript normal c Baseline period[(1 −emin)[x]c]c = [(1 + emin)[x]c]c.
In practice, to compare two numbers x and y, we must compare  left bracket x right bracket Subscript normal c[x]c and 
left bracket y right bracket Subscript normal c[y]c. We consider  x and y diﬀerent if 
left bracket StartAbsoluteValue y EndAbsoluteValue right bracket Subscript normal c Baseline less than left bracket StartAbsoluteValue x EndAbsoluteValue right bracket Subscript normal c Baseline minus left bracket epsilon Subscript normal m normal i normal n Baseline left bracket StartAbsoluteValue x EndAbsoluteValue right bracket Subscript normal c Baseline right bracket Subscript normal c[|y|]c < [|x|]c −[emin[|x|]c]c
or if 
left bracket StartAbsoluteValue y EndAbsoluteValue right bracket Subscript normal c Baseline greater than left bracket StartAbsoluteValue x EndAbsoluteValue right bracket Subscript normal c Baseline plus left bracket epsilon Subscript normal m normal a normal x Baseline left bracket StartAbsoluteValue x EndAbsoluteValue right bracket Subscript normal c Baseline right bracket Subscript normal c Baseline period[|y|]c > [|x|]c + [emax[|x|]c]c.
The relative spacing at any point obviously depends on the value repre-
sented by the least signiﬁcant digit in the signiﬁcand. This digit (or bit) is 
called the “unit in the last place,” or “ulp.” The magnitude of a ulp depends 
of course on the magnitude of the number being represented. Any real number 
within the range allowed by the exponent can be approximated within one half1
2 ulp 
by a ﬂoating-point number. 
The subsets of numbers that we need in the computer depend on the kinds 
of numbers that are of interest for the problem at hand. Often, however, the 
kinds of numbers of interest change dramatically within a given problem. 
For example, we may begin with integer data in the range from 1 to 50. 
Most simple operations, such as addition, squaring, and so on, with these 
data would allow a single paradigm for their representation. The ﬁxed-point 
representation should work very nicely for such manipulations. 
Something as simple as a factorial, however, immediately changes the 
paradigm. It is unlikely that the ﬁxed-point representation would be able to 
handle the resulting large numbers. When we signiﬁcantly change the range 
of numbers that must be accommodated, another change that occurs is the 
ability to represent the numbers exactly. If the beginning data are integers be-
tween 1 and 50, and no divisions or operations leading to irrational numbers 
are performed, one storage unit would almost surely be suﬃcient to represent 
all values exactly. If factorials are evaluated, however, the results cannot be 
represented exactly in one storage unit and so must be approximated (even 
though the results are integers). When data are not integers, it is usually ob-
vious that we must use approximations, but it may also be true for integer 
data. 
Standardization of Floating-Point Representation 
Over the past several years, standards for ﬂoating-point representation have 
evolved. In the mid-1980s, IEEE promulgated a standard, called the IEEE 
Standard 754, that speciﬁes the exact layout of the bits in ﬂoating-point rep-
resentations. This standard, which also became the IEC 60559 Standard, was

10.2 Digital Representation of Numeric Data
541
modiﬁed slightly in 2008 (IEEE 2008) and is now sometimes referred to as 
IEEE Standard 754-2008. 
The IEEE Standard 754 speciﬁes characteristics for two diﬀerent preci-
sions, “single” and “double.” In both cases, the standard requires that the 
radix be 2; hence, it is sometimes called the “binary standard.” For single 
precision, p must be 24, e Subscript normal m normal a normal xemax must be 127, and e Subscript normal m normal i normal nemin must be negative 126−126. For dou-
ble precision, p must be 53, e Subscript normal m normal a normal xemax must be 1023, and e Subscript normal m normal i normal nemin must be negative 1022−1022. The  
standard also deﬁnes two additional precisions, “single extended” and “double 
extended.” For each of the extended precisions, the standard sets bound on 
the precision and exponent ranges rather than specifying them exactly. The 
extended precisions have larger exponent ranges and greater precision than 
the corresponding precision that is not “extended.” 
The standard also speciﬁes how a number is to be represented when it can-
not be represented exactly, that is, how rounding should occur. In practice, 
this is most relevant in deﬁning the result of a numerical operation. The stan-
dard allows for round-to-nearest (with ties going to the smallest in absolute 
value), round-up, and round-down, but it requires that the default rounding 
be round-to-nearest. 
Most of the computers developed in the past few years comply with the 
standards, but it is up to the computer manufacturers to conform voluntarily 
to these standards. We would hope that the marketplace would penalize the 
manufacturers who do not conform. 
Additional information about the IEEE standards for ﬂoating-point num-
bers can be found in Overton (2001). 
Special Floating-Point Numbers 
It is necessary to be able to represent certain special entities, such as inﬁnity, 
indeterminate (0 divided by 00/0), or simply missing, which do not have ordinary represen-
tations in any base-digit system. Although 8 bits are available for the exponent 
in the single-precision IEEE binary standard, e Subscript normal m normal a normal x Baseline equals 127emax = 127 and e Subscript normal m normal i normal n Baseline equals negative 126emin = −126. 
This means there are two unused possible values for the exponent; likewise, 
for the double-precision standard, there are two unused possible values for 
the exponent. These extra possible values for the exponent allow us to rep-
resent certain special ﬂoating-point numbers. An exponent of e Subscript normal m normal i normal n Baseline minus 1emin −1 allows 
us to handle 0 and the numbers between 0 and b Superscript e Super Subscript normal m normal i normal nbemin unambiguously even 
though there is a hidden bit (see the discussion above about normalization 
and gradual underﬂow). The special number 0 is represented with an exponent 
of e Subscript normal m normal i normal n Baseline minus 1emin −1 and a signiﬁcand of 00 ellipsis 000 . . . 0. 
An exponent of e Subscript normal m normal a normal x Baseline plus 1emax + 1 allows us to represent plus or minus normal infinity±∞and indeterminate or 
missing values. A ﬂoating-point number with this exponent and a signiﬁcand 
of 0 represents plus or minus normal infinity±∞(the sign bit determines the sign, as usual). A ﬂoating-
point number with this exponent and a nonzero signiﬁcand represents an 
indeterminate value such as StartFraction 0 Over 0 EndFraction0
0. This value is called “not-a-number,” or NaN. 
There are various ways of representing NaNs. In statistical data processing, a

542
10 Numerical Methods
NaN is sometimes used to represent a missing value, but a particular software 
system may use a speciﬁc NaN representation to distinguish diﬀerent kinds 
of indeterminate values. (R does this, for example, using a special coding to 
represent “not available,” NA. The concept of missing values can also apply 
to other things that are not numbers, such as character data.) 
Because a NaN is an indeterminate value, if a variable x has a value of 
NaN, it is neither true that x equals xx = x nor that x not equals xx /= x. Also, because a NaN can 
be represented in diﬀerent ways, a programmer must be careful in testing for 
NaNs. Some software systems provide explicit functions for testing for a NaN. 
The IEEE Binary Standard recommended that a function isnan be provided 
to test for a NaN. Cody and Coonen (1993) provide C programs for isnan 
and other functions useful in working with ﬂoating-point numbers. (R provides 
is.nan for this purpose, but also provides is.na for values that have been 
designated as missing, that is, as not available or NA. Both is.na(0/0) and 
is.nan(0/0) are true, but if x = NA, then  is.na(x) is true, but is.nan(x) 
is false.) 
10.2.3 Language Constructs for Representing Numeric Data 
Most general-purpose computer programming languages provide constructs 
for the user to specify the type of representation for numeric quantities. Within 
the two basic types supported by the computer hardware and described in the 
preceding sections, the computer language may support additional types of 
numeric quantities, such as complex numbers. 
These speciﬁcations of the type are made in declaration statements that 
are made at the beginning of some section of the program for which they 
apply, or they are made by “casting” functions or operations. 
The diﬀerence between ﬁxed-point and ﬂoating-point representations has 
a conceptual basis that may correspond to the problem being addressed. The 
diﬀerences between other kinds of representations often are not because of 
conceptual diﬀerences; rather, they are the results of increasingly irrelevant 
limitations of the computer. The reasons there are “short” and “long,” or 
“signed” and “unsigned,” representations do not arise from the problem the 
user wishes to solve; the representations are to allow more eﬃcient use of 
computer resources. The software designer nowadays generally eschews the 
space-saving constructs that apply to only a relatively small proportion of 
the data. In some applications, however, the short representations of numeric 
data still have a place. 
C 
In C, the types of all variables must be speciﬁed with a basic declarator, which 
may be qualiﬁed further. For variables containing numeric data, the possible 
types are shown in Table 10.1.

10.2 Digital Representation of Numeric Data
543
Table 10.1. Numeric data types in C 
Basic type
Basic
Fully qualiﬁed 
declarator
declarator 
Fixed-point
int
signed short int 
unsigned short int 
signed long int 
unsigned long int 
signed long long int 
unsigned long long int 
Floating-point float 
double
double 
long double 
Exactly what these types mean is not speciﬁed by the language but de-
pends on the speciﬁc implementation, which associates each type with some 
natural type supported by the speciﬁc computer. Common storage for a ﬁxed-
point variable of type short int uses 16 bits and for type long int uses 32 
bits. An unsigned quantity of either type speciﬁes that no bit is to be used as a 
sign bit, which eﬀectively doubles the largest representable number. Of course, 
this is essentially irrelevant for scientiﬁc computations, so unsigned integers 
are generally just a nuisance. If neither short nor long is speciﬁed, there is a 
default interpretation that is implementation-dependent. The default always 
favors signed over unsigned. There is a movement toward standardization 
of the meanings of these types. The American National Standards Institute 
(ANSI) and its international counterpart, the International Organization for 
Standardization (ISO), have speciﬁed standard deﬁnitions of several program-
ming languages. ANSI (1989) is a speciﬁcation of the C language, which we 
will refer to as “ANSI C.” The ISO has promulgated revisions of the C stan-
dard, called “C99” and “C11,” corresponding to the years the standards were 
adopted. These standards are essentially backward-compatible with ANSI C, 
which remains the most widely used version. C11 has added an additional 
numeric type, called long long int. 
Standards for computer languages usually provide minimum speciﬁcations, 
rather than exact meanings of the various data types. ANSI C and C99 and 
C11 require that short int use at least 16 bits, that long int use at least 
32 bits, and that long int be at least as long as int, which  in  turn  must  
be least as long as short int. The  long double type may or may not have 
more precision and a larger range than the double type. C11 has added an 
additional numeric type, called long long int, which must use at least 64 
bits. 
The object-oriented hybrid language built on C, C++ (ANSI, 1998), pro-
vides the user with the ability also to deﬁne operator functions, so that the four 
simple arithmetic operations can be implemented by the operators “+”, “minus−”,

544
10 Numerical Methods
“asterisk∗”, and “/”. There is no good way of deﬁning an exponentiation operator, 
however, because the user-deﬁned operators are limited to extended versions 
of the operators already deﬁned in the language. (The ISO has also promul-
gated revisions of the C++ standard, called “C++03,” “C++11,” “C++14,” 
and “C++17,” corresponding to the years the standards were adopted. The 
revisions add types among other things, but ANSI C++ remains the most 
widely used version.) 
Fortran 
Fortran provides three intrinsic numeric data types (and also two additional 
intrinsic types for logical and character quantities) and within each type pro-
vides kinds. One intrinsic type, INTEGER, corresponds to a standard ﬁxed-point 
entity, and another intrinsic type, REAL, corresponds to a standard ﬂoating-
point entity. The exact form of the storage, including the number of bits, can 
vary from one platform to another. A third intrinsic type, COMPLEX, corre-
sponds to a doubleton of two units of type REAL. Arithmetic over numbers 
of type COMPLEX, including also numbers of type REAL, conform to the usual 
rules of arithmetic in the complex ﬁeld, within the usual limitations of arith-
metic in normal upper I normal upper FIF. Within each of the ﬁve intrinsic types (including LOGICAL and 
CHARACTER), Fortran provides for various kinds; for example within the real 
type, there must be at least two kinds available, a default kind and a kind 
with greater precision (usually “double precision”). The declarator for double 
precision is of the form 
REAL(KIND=LONG) 
There are many other kinds available for the various data types. 
In Fortran, variables have a default numeric type that depends on the ﬁrst 
letter in the name of the variable. The type can be explicitly declared (and, in 
fact, should be in careful programming). The signed and unsigned qualiﬁers 
of C, which have very little use in scientiﬁc computing, are missing in Fortran. 
Basic types for variables containing numeric data are shown in Table 10.2. 
Table 10.2. Numeric data types in Fortran 
Basic type
Basic
Default 
declarator
variable name 
Fixed-point
INTEGER
begin with i–n or I–N 
Floating-point REAL
begin with a–h or o–z 
or with A–H or O–Z 
REAL(KIND=LONG)
no default, although 
d or D is sometimes used 
Complex
COMPLEX
no default, although 
c or C is sometimes used 
COMPLEX(KIND=LONG) 

10.2 Digital Representation of Numeric Data
545
Although the standard organizations have deﬁned these constructs for the 
Fortran language, just as is the case with C, exactly what these types mean 
is not speciﬁed by the language but depends on the speciﬁc implementation, 
although in most implementations, the number of bytes to be used for storage 
can be speciﬁed through the KIND keyword. 
The kind is a qualiﬁer for the basic type; thus, a ﬁxed-point number may be 
an INTEGER of kind 1 or kind 2, for example. The actual value of the qualiﬁer 
kind may diﬀer from one compiler to another, so the user deﬁnes a program pa-
rameter to be the kind that is appropriate to the range and precision required 
for a given variable. Fortran provides the functions SELECTED INT KIND and 
SELECTED REAL KIND to do this. Thus, to declare some ﬁxed-point variables 
that have at least three decimal digits and some more ﬁxed-point variables 
that have at least eight decimal digits, the user may write the following state-
ments: 
INTEGER, PARAMETER :: little = SELECTED_INT_KIND(3) 
INTEGER, PARAMETER :: big = SELECTED_INT_KIND(8) 
INTEGER (little) :: ismall, jsmall 
INTEGER (big) :: itotal_accounts, igain 
The variables little and big would have integer values, chosen by the com-
piler designer, that could be used in the program to qualify integer types to en-
sure that range of numbers could be handled. Thus, ismall and jsmall would 
be ﬁxed-point numbers that could represent integers between negative 999−999 and 999, 
and itotal accounts and igain would be ﬁxed-point numbers that could 
represent integers between negative 99 comma 999 comma 999−99,999,999 and 99 comma 999 comma 99999,999,999. Depending on the ba-
sic hardware, the compiler may assign two bytes as kind = little, meaning 
that integers between negative 32 comma 768−32,768 and 32 comma 76732,767 could probably be accommodated 
by any variable, such as ismall, that is declared as integer (little). Like-
wise, it is probable that the range of variables declared as integer (big) 
could handle numbers in the range negative 2 comma 147 comma 483 comma 648−2,147,483,648 and 2 comma 147 comma 483 comma 6472,147,483,647. For  
declaring ﬂoating-point numbers, the user can specify a minimum range and 
precision with the function SELECTED REAL KIND, which takes two arguments, 
the number of decimal digits of precision and the exponent of 10 for the range. 
Thus, the statements 
INTEGER, PARAMETER :: real4 = SELECTED_REAL_KIND(6,37) 
INTEGER, PARAMETER :: real8 = SELECTED_REAL_KIND(15,307) 
would yield designators of ﬂoating-point types that would have either six dec-
imals of precision and a range up to 10 Superscript 371037 or 15 decimals of precision and a 
range up to 10 Superscript 30710307. The statements 
REAL (real4) :: x, y 
REAL (real8) :: dx, dy

546
10 Numerical Methods
declare x and y as variables corresponding roughly to REAL on most systems 
and dx and dy as variables corresponding roughly to REAL(KIND=LONG (or 
DOUBLE PRECISION). 
If the system cannot provide types matching the requirements speciﬁed in 
SELECTED INT KIND or SELECTED REAL KIND, these functions return negative 1−1. Be-
cause it is not possible to handle such an error situation in the declaration 
statements, the user should know in advance the available ranges. Fortran pro-
vides a number of intrinsic functions, such as EPSILON, RRSPACING, and  HUGE, 
to use in obtaining information about the ﬁxed- and ﬂoating-point numbers 
provided by the system. 
As with other language standards, the Fortran standard provides minimum 
speciﬁcations, rather than exact meanings of the various data types. Metcalf 
et al. (2018) summarize the Fortran standard speciﬁcations for the various 
data types. 
Fortran also provides a number of intrinsic functions for dealing with bits. 
These functions are essentially those speciﬁed in the MIL-STD-1753 Standard 
of the US Department of Defense. These bit functions, which have been a part 
of many Fortran implementations for years, provide for shifting bits within 
a string, extracting bits, exclusive or inclusive oring of bits, and so on. (See 
ANSI, 1992, or Lemmon and Schafer, 2005, for more extensive discussions of 
the intrinsic functions provided in Fortran.) 
Determining the Numerical Characteristics of a Particular 
Computer 
The environmental inquiry program MACHAR by Cody (1988) can  be  used  
to determine the characteristics of a speciﬁc computer’s ﬂoating-point repre-
sentation and its arithmetic. Although it may be of interest to examine the 
source code to see how methods to determine these characteristics work (the 
program is available in CALGO from netlib), modern systems for numerical 
computations provide functions to determine the characteristics directly. 
In R, the numerical characteristics of a given system are stored in the 
variable .Machine. 
As mentioned above, modern Fortran provides a number of functions for 
inquiring about many of the characteristics of the computer. 
Many higher-level languages and application software packages do not give 
the user a choice of how to represent numeric data. The software system may 
consistently use a type thought to be appropriate for the kinds of applications 
addressed. For example, many statistical analysis application packages choose 
to use a ﬂoating-point representation with about 64 bits for all numeric data. 
Making a choice such as this yields more comparable results across a range of 
computer platforms on which the software system may be implemented.

10.2 Digital Representation of Numeric Data
547
Whenever the user chooses the type and precision of variables, it is a 
good idea to use some convention to name the variable in such a way as to 
indicate the type and precision. Books or courses on elementary programming 
suggest using mnemonic names, such as “time,” for a variable that holds the 
measure of time. If the variable takes ﬁxed-point values, a better name might 
be “itime.” It still has the mnemonic value of “time,” but it also helps us to 
remember that, in the computer, itime/length may not be the same thing as 
time/xlength. Although the variables are declared in the program to be of a 
speciﬁc type, the programmer can beneﬁt from a reminder of the type. Even 
as we “humanize” computing, we must remember that there are details about 
the computer that matter. (The operator “/” is said to be “overloaded”: in 
a general way, it means “divide,” but it means diﬀerent things depending on 
the contexts of the two expressions above.) Whether a quantity is a member 
of normal upper I normal upper III or normal upper I normal upper FIF may have major consequences for the computations, and a careful 
choice of notation can help to remind us of that, even if the notation may look 
old-fashioned. 
Numerical analysts sometimes use the phrase “full precision” to refer to 
a precision of about 16 decimal digits and the phrase “half precision”to re-
fer to a precision of about seven decimal digits. These terms are not deﬁned 
precisely, but they do allow us to speak of the precision in roughly equivalent 
ways for diﬀerent computer systems without specifying the precision exactly. 
Full precision is roughly equivalent to Fortran REAL(KIND=LONG) (or DOUBLE 
PRECISION) on 32-bit computers and to Fortran REAL on 64-bit machines. 
Half precision corresponds roughly to Fortran REAL on 32-bit machines. Full 
and half precision can be handled in a portable way in Fortran. The following 
statements declare a variable x to be one with full precision: 
INTEGER, PARAMETER :: full = SELECTED\_REAL\_KIND(15,307) 
REAL(full) :: x 
In a construct of this kind, the user can deﬁne “full” or “half” as appropriate. 
10.2.4 Other Variations in the Representation of Data: Portability 
of Data 
As we have indicated already, computer designers have a great deal of latitude 
in how they choose to represent data. The ASCII standards of ANSI and 
ISO have provided a common representation for individual characters. The 
IEEE Standard 754-2008 referred to previously (IEEE, 2008) brought some 
standardization to the representation of ﬂoating-point data, but do not specify 
how the available bits are to be allocated among the sign, exponent, and 
signiﬁcand.

548
10 Numerical Methods
Because the number of bits used as the basic storage unit has generally 
increased over time, some computer designers have arranged small groups 
of bits, such as bytes, together in strange ways to form words. There are 
two common schemes of organizing bits into bytes and bytes into words. In 
one scheme, called “big end” or “big endian,” the bits are indexed from the 
“left,” or most signiﬁcant, end of the byte, and bytes are indexed within 
words and words are indexed within groups of words in the same direction 
(see Fig. 10.10). 
In another scheme, called “little end” or “little endian,” the bytes are 
indexed within the word in the opposite direction. Figures 10.11 and 10.12 
illustrate some of the diﬀerences, using the program shown in Fig. 10.10. The  R  
function .Platform provides information on the type of endian of the given 
machine on which the program is running. 
Figure 10.10. A Fortran program illustrating bit and byte organization 
Figure 10.11. Output from a Little Endian System 
These diﬀerences are important only when accessing the individual bits 
and bytes, when making data type transformations directly, or when mov-
ing data from one machine to another without interpreting the data in the 
process (“binary transfer”). One lesson to be learned from observing such 
subtle diﬀerences in the way the same quantities are treated in diﬀerent com-
puter systems is that programs should rarely rely on the inner workings of

10.3 Computer Operations on Numeric Data
549
the computer. A program that does will not be portable; that is, it will not 
give the same results on diﬀerent computer systems. Programs that are not 
portable may work well on one system, and the developers of the programs 
may never intend for them to be used anywhere else. As time passes, however, 
systems change or users change systems. When that happens, the programs 
that were not portable may cost more than they ever saved by making use of 
computer-speciﬁc features. 
Figure 10.12. Output from a big endian system 
The external data representation, or XDR, developed by Sun Microsys-
tems for use in remote procedure calls, is a widely used machine-independent 
standard for binary data structures. XDR uses a base unit of 4 bytes in big-
endian order. Multiple XDR units can be combined if necessary. (Note that 
“XDR” is also a standard acronym used in computer security systems.) 
10.3 Computer Operations on Numeric Data 
As we have emphasized above, the numerical quantities represented in the 
computer are used to simulate or approximate more interesting quantities, 
namely, the real numbers or perhaps the integers. Obviously, because the sets 
(computer numbers and real numbers) are not the same, we could not de-
ﬁne operations on the computer numbers that would yield the same ﬁeld as 
the familiar ﬁeld of the reals. In fact, because of the nonuniform spacing of 
ﬂoating-point numbers, we would suspect that some of the fundamental prop-
erties of a ﬁeld may not hold. Depending on the magnitudes of the quantities 
involved, it is possible, for example, that if we compute ab and ac and then 
a b plus a cab + ac, we may not get the same thing as if we compute left parenthesis b plus c right parenthesis(b + c) and then 
a left parenthesis b plus c right parenthesisa(b + c). Just as we use the computer quantities to simulate real quantities, 
we deﬁne operations on the computer quantities to simulate the familiar oper-
ations on real quantities. Designers of computers attempt to deﬁne computer 
operations so as to correspond closely to operations on real numbers, but we 
must not lose sight of the fact that the computer uses a diﬀerent arithmetic 
system. 
The basic operational objective in numerical computing, of course, is that 
a computer operation, when applied to computer numbers, yields computer 
numbers that approximate the number that would be yielded by a certain

550
10 Numerical Methods
mathematical operation applied to the numbers approximated by the original 
computer numbers. Just as we introduced the notation 
left bracket x right bracket Subscript normal c[x]c
on page 535 to denote the computer ﬂoating-point number approximation to 
the real number x, we occasionally use the notation 
left bracket ring right bracket Subscript normal c[◦]c
to refer to a computer operation that simulates the mathematical operation ring◦. 
Thus, 
left bracket plus right bracket Subscript normal c[+]c
represents an operation similar to addition but that yields a result in a set of 
computer numbers. (We use this notation only where necessary for emphasis, 
however, because it is somewhat awkward to use it consistently.) The failure 
of the familiar laws of the ﬁeld of the reals, such as the distributive law cited 
above, can be anticipated by noting that 
left bracket a right bracket Subscript normal c Baseline left bracket plus right bracket Subscript normal c Baseline left bracket b right bracket Subscript normal c Baseline not equals left bracket a plus b right bracket Subscript normal c Baseline comma[a]c [+]c [b]c /= [a + b]c,
or by considering the simple example in which all numbers are rounded to one 
decimal place and so one third plus one third not equals two thirds 1
3 + 1
3 /= 2
3 (that is, .3 plus .3 not equals .7.3 + .3 /= .7). 
The three familiar laws of the ﬁeld of the reals (commutativity of addition 
and multiplication, associativity of addition and multiplication, and distribu-
tion of multiplication over addition) result in the independence of the order 
in which operations are performed; the failure of these laws implies that the 
order of the operations may make a diﬀerence. When computer operations 
are performed sequentially, we can usually deﬁne and control the sequence 
fairly easily. If the computer performs operations in parallel, the resulting 
diﬀerences in the orders in which some operations may be performed can 
occasionally yield unexpected results. 
Because the operations are not closed, special notice may need to be taken 
when the operation would yield a number not in the set. Adding two num-
bers, for example, may yield a number too large to be represented well by 
a computer number, either ﬁxed-point or ﬂoating-point. When an operation 
yields such an anomalous result, an exception is said to exist. 
The computer operations for the two diﬀerent types of computer numbers 
are diﬀerent, and we discuss them separately. 
10.3.1 Fixed-Point Operations 
The operations of addition, subtraction, and multiplication for ﬁxed-point 
numbers are performed in an obvious way that corresponds to the similar 
operations on the ring of integers. Subtraction is addition of the additive 
inverse. (In the usual two’s-complement representation we described earlier,

10.3 Computer Operations on Numeric Data
551
all ﬁxed-point numbers have additive inverses except minus 2 Superscript k minus 1−2k−1.) Because there is 
no multiplicative inverse, however, division is not multiplication by the inverse. 
The result of division with ﬁxed-point numbers is the result of division with 
the corresponding real numbers rounded toward zero. This is not considered 
an exception. 
As we indicated above, the set of ﬁxed-point numbers together with addi-
tion and multiplication is not the same as the ring of integers, if for no other 
reason than that the set is ﬁnite. Under the ordinary deﬁnitions of addition 
and multiplication, the set is not closed under either operation. The computer 
operations of addition and multiplication, however, are deﬁned so that the set 
is closed. These operations occur as if there were additional higher-order bits 
and the sign bit were interpreted as a regular numeric bit. The result is then 
whatever would be in the standard number of lower-order bits. If the lost 
higher-order bits are necessary, the operation is said to overﬂow. If ﬁxed-
point overﬂow occurs, the result is not correct under the usual interpretation 
of the operation, so an error situation, or an exception, has occurred. Most 
computer systems allow this error condition to be detected, but most software 
systems do not take note of the exception. The result, of course, depends on 
the speciﬁc computer architecture. On many systems, aside from the interpre-
tation of the sign bit, the result is essentially the same as would result from a 
modular reduction. There are some special-purpose algorithms that actually 
use this modiﬁed modular reduction, although such algorithms would not be 
portable across diﬀerent computer systems. 
The gmp package in R allows operations on larger integers in the bigz 
class. The number of digits allowed is platform speciﬁc. 
10.3.2 Floating-Point Operations 
As we have seen, real numbers within the allowable range may or may not 
have an exact ﬂoating-point operation, and the computer operations on the 
computer numbers may or may not yield numbers that represent exactly the 
real number that would result from mathematical operations on the numbers. 
If the true result is r, the best we could hope for would be left bracket r right bracket Subscript normal c[r]c. As we have  
mentioned, however, the computer operation may not be exactly the same as 
the mathematical operation being simulated, and furthermore, there may be 
several operations involved in arriving at the result. Hence, we expect some 
error in the result. 
Errors 
If the computed value is r overTilde˜r (for the true value r), we speak of the absolute 
error, 
StartAbsoluteValue r overTilde minus r EndAbsoluteValue comma|˜r −r|,
and the relative error,

552
10 Numerical Methods
StartFraction StartAbsoluteValue r overTilde minus r EndAbsoluteValue Over StartAbsoluteValue r EndAbsoluteValue EndFraction|˜r −r|
|r|
(so long as r not equals 0r /= 0). An important objective in numerical computation obviously 
is to ensure that the error in the result is small. 
We will discuss error in ﬂoating-point computations further in Sect. 10.4.1. 
Guard Digits and Chained Operations 
Ideally, the result of an operation on two ﬂoating-point numbers would be the 
same as if the operation were performed exactly on the two operands (consid-
ering them to be exact also) and the result was then rounded. Attempting to 
do this would be very expensive in both computational time and complexity 
of the software. If care is not taken, however, the relative error can be very 
large. Consider, for example, a ﬂoating-point number system with b equals 2b = 2 and 
p equals 4p = 4. Suppose we want to add 8 and negative 7.5−7.5. In the ﬂoating-point system, we 
would be faced with the problem 
StartLayout 1st Row 1st Column 8 colon 2nd Column 1.000 3rd Column times 4th Column 2 cubed 2nd Row 1st Column 7.5 colon 2nd Column 1.111 3rd Column times 4th Column 2 squared period EndLayout 8 : 1.000 × 23
7.5 : 1.111 × 22.
To make the exponents the same, we have 
StartLayout 1st Row 1st Column 8 colon 2nd Column 1.000 3rd Column times 4th Column 2 cubed 5th Column Blank 2nd Row 1st Column 7.5 colon 2nd Column 0.111 3rd Column times 4th Column 2 cubed 5th Column Blank EndLayout normal o normal r StartLayout 1st Row 1st Column Blank 2nd Column 8 colon 3rd Column 1.000 4th Column times 5th Column 2 cubed 2nd Row 1st Column Blank 2nd Column 7.5 colon 3rd Column 1.000 4th Column times 5th Column 2 cubed period EndLayout 8 : 1.000 × 23
7.5 : 0.111 × 23 or
8 : 1.000 × 23
7.5 : 1.000 × 23.
The subtraction will yield either 0.000 Subscript 20.0002 or 1.000 Subscript 2 Baseline times 2 Superscript 01.0002 × 20, whereas the correct 
value is 1.000 Subscript 2 Baseline times 2 Superscript negative 11.0002 × 2−1. Either way, the absolute error is 0.5 Subscript 100.510, and the relative 
error is 1. Every bit in the signiﬁcand is wrong. The magnitude of the error 
is the same as the magnitude of the result. This is not acceptable. (More 
generally, we could show that the relative error in a similar computation could 
be as large as b minus 1b −1 for any base b.) The solution to this problem is to use one 
or more guard digits. A guard digit is an extra digit in the signiﬁcand that 
participates in the arithmetic operation. If one guard digit is used (and this 
is the most common situation), the operands each have p plus 1p + 1 digits in the 
signiﬁcand. In the example above, we would have 
StartLayout 1st Row 1st Column 8 colon 2nd Column 1.0000 3rd Column times 4th Column 2 cubed 2nd Row 1st Column 7.5 colon 2nd Column 0.1111 3rd Column times 4th Column 2 cubed comma EndLayout 8 : 1.0000 × 23
7.5 : 0.1111 × 23,
and the result is exact. In general, one guard digit can ensure that the relative 
error is less than  2 epsilon Subscript normal m normal a normal x2emax. The use of guard digits requires that the operands 
be stored in special storage units. Whenever multiple operations are to be 
performed together, the operands and intermediate results can all be kept 
in the special registers to take advantage of the guard digits or even longer 
storage units. This is called chaining of operations.

10.3 Computer Operations on Numeric Data
553
Addition of Several Numbers 
When several numbers x Subscript ixi are to be summed, it is likely that as the operations 
proceed serially, the magnitudes of the partial sum and the next summand 
will be quite diﬀerent. In such a case, the full precision of the next summand 
is lost. This is especially true if the numbers are of the same sign. As we 
mentioned earlier, a computer program to implement serially the algorithm 
implied by sigma summation Underscript i equals 1 Overscript normal infinity Endscripts iE∞
i=1 i will converge to some number much smaller than the largest 
ﬂoating-point number. 
If the numbers to be summed are not all the same constant (and if they 
are constant, just use multiplication!), the accuracy of the summation can 
be increased by ﬁrst sorting the numbers and summing them in order of 
increasing magnitude. If the numbers are all of the same sign and have roughly 
the same magnitude, a pairwise “fan-in” method may yield good accuracy. In 
the fan-in method, the n numbers to be summed are added two at a time to 
yield left ceiling n divided by 2 right ceiling[n/2] partial sums. The partial sums are then added two at a time, and 
so on, until all sums are completed. The name “fan-in” comes from the tree 
diagram of the separate steps of the computations: 
StartLayout 1st Row 1st Column s 1 Superscript left parenthesis 1 right parenthesis Baseline equals x 1 plus x 2 2nd Column s 2 Superscript left parenthesis 1 right parenthesis Baseline equals x 3 plus x 4 3rd Column ellipsis 4th Column s Subscript 2 m minus 1 Superscript left parenthesis 1 right parenthesis Baseline equals x Subscript 4 m minus 3 Baseline plus x Subscript 4 m minus 2 Baseline 5th Column s Subscript 2 m Superscript left parenthesis 1 right parenthesis Baseline equals 6th Column ellipsis 2nd Row 1st Column down right arrow 2nd Column down left arrow 3rd Column ellipsis 4th Column down right arrow 5th Column down left arrow 6th Column ellipsis 3rd Row 1st Column s 1 Superscript left parenthesis 2 right parenthesis Baseline equals s 1 Superscript left parenthesis 1 right parenthesis Baseline plus s 2 Superscript left parenthesis 1 right parenthesis Baseline 2nd Column ellipsis 3rd Column s Subscript m Superscript left parenthesis 2 right parenthesis Baseline equals s Subscript 2 m minus 1 Superscript left parenthesis 1 right parenthesis Baseline plus s Subscript 2 m Superscript left parenthesis 1 right parenthesis Baseline 4th Column ellipsis 4th Row 1st Column down right arrow 2nd Column Blank 3rd Column ellipsis 4th Column down arrow 5th Column ellipsis 5th Row 1st Column s 1 Superscript left parenthesis 3 right parenthesis Baseline equals s 1 Superscript left parenthesis 2 right parenthesis Baseline plus s 2 Superscript left parenthesis 2 right parenthesis Baseline 2nd Column ellipsis 3rd Column ellipsis 4th Column ellipsis EndLayout
s(1)
1
= x1 + x2
s(1)
2
= x3 + x4 . . . s(1)
2m−1 = x4m−3 + x4m−2 s(1)
2m = . . .
-
-
. . .
-
-
. . .
s(2)
1
= s(1)
1
+ s(1)
2
. . .
s(2)
m = s(1)
2m−1 + s(1)
2m
. . .
-
. . .
↓
. . .
s(3)
1
= s(2)
1
+ s(2)
2
. . .
. . .
. . .
It is likely that the numbers to be added will be of roughly the same magnitude 
at each stage. Remember we are assuming they have the same sign initially; 
this would be the case, for example, if the summands are squares. 
Compensated Summation 
Another way of summing many numbers of varying magnitudes is due to W. 
Kahan. It is called compensated summation, and  for  x 1 comma ellipsis comma x Subscript n Baselinex1, . . . , xn, it follows these 
steps: 
StartLayout 1st Row s equals x 1 2nd Row a equals 0 3rd Row normal f normal o normal r i equals 2 comma ellipsis comma n 4th Row left brace 5th Row y equals x Subscript i Baseline minus a 6th Row t equals s plus y 7th Row a equals left parenthesis t minus s right parenthesis minus y 8th Row s equals t 9th Row right brace period EndLayout
s = x1
a = 0
for i = 2, . . . , n
{
y = xi −a
t = s + y
a = (t −s) −y
s = t
}.
(10.2) 
Much of the work on improving the accuracy of summation was motivated 
by the problem of computation of normal upper L 2L2 norms, or more generally computation of

554
10 Numerical Methods
dot products. The exact dot product (EDP) procedure, described on page 561 
addresses this problem. Both compensated summation and EDP implemented 
in the hardware have been used in some versions of the BLAS. (See Sect. 12.2.1 
on page 624 for descriptions of the BLAS.) 
In Sect. 10.3.5 we discuss exact computation, and obviously any of the 
methods of exact computation can also be used to increase the accuracy of 
summations, and, indeed, in most cases, to make the summations exact. 
Catastrophic Cancellation 
Another kind of error that can result because of the ﬁnite precision used 
for ﬂoating-point numbers is catastrophic cancellation. This can occur when 
two rounded values of approximately equal magnitude and opposite signs are 
added. (If the values are exact, cancellation can also occur, but it is benign.) 
After catastrophic cancellation, the digits left are just the digits that repre-
sented the rounding. Suppose x almost equals yx ≈y and that left bracket x right bracket Subscript normal c Baseline equals left bracket y right bracket Subscript normal c[x]c = [y]c. The computed result 
will be zero, whereas the correct (rounded) result is left bracket x minus y right bracket Subscript normal c[x−y]c. The relative error 
is 100%. This error is caused by rounding, but it is diﬀerent from the “round-
ing error” discussed above. Although the loss of information arising from the 
rounding error is the culprit, the rounding would be of little consequence were 
it not for the cancellation. 
To avoid catastrophic cancellation, watch for possible additions of quanti-
ties of approximately equal magnitude and opposite signs, and rearrange the 
computations if possible. Consider the problem of computing the roots of a 
quadratic polynomial, a x squared plus b x plus cax2 + bx + c. In the quadratic formula 
x equals StartFraction negative b plus or minus StartRoot b squared minus 4 a c EndRoot Over 2 a EndFraction commax = −b ±
√
b2 −4ac
2a
,
(10.3) 
the square root of the discriminant, left parenthesis b squared minus 4 a c right parenthesis(b2 −4ac), may be approximately equal to 
b in magnitude, meaning that one of the roots is close to zero and, in fact, may 
be computed as zero. The solution is to compute only one of the roots, x 1x1, by  
the formula (the “minus−” root if  b is positive and the “plus+” root if  b is negative) 
and then compute the other root, x 2x2 by the relationship x 1 x 2 equals c divided by ax1x2 = c/a. 
In discussing Householder reﬂections, we have seen another example where 
catastrophic cancellation could occur and how we can avoid it, in particular, 
in Eq. (4.8) on page 226. 
Standards for Floating-Point Operations 
The IEEE Binary Standard 754 (IEEE, 2008) applies not only to the represen-
tation of ﬂoating-point numbers but also to certain operations on those num-
bers. The standard requires correct rounded results for addition, subtraction, 
multiplication, division, remaindering, and extraction of the square root. It 
also requires that conversion between ﬁxed-point numbers and ﬂoating-point 
numbers yield correct rounded results.

10.3 Computer Operations on Numeric Data
555
An inexact operation is one for which the result must be rounded. For 
example, in a multiplication operation, if all p bits of the signiﬁcand are 
required to represent both the multiplier and multiplicand, approximately 2p 
bits would be required to represent the product. Because only p are available, 
however, the result generally must be rounded. 
If a ﬂoating-point operation does not yield the exact result of the cor-
responding mathematical operation, then an exception is said to occur. In 
this sense, inexact operations are exceptions, but that kind of exception is 
generally ignored. 
The IEEE Binary Standard deﬁnes how exceptions should be handled. The 
exceptions are divided into four types: division of a nonzero number by zero, 
overﬂow, underﬂow, and invalid operation. 
Division by zero results in a special number if the dividend is nonzero. The 
result is either normal infinity∞or negative normal infinity−∞, and these have special representations, as we have 
seen. 
If an operation on ﬂoating-point numbers would result in a number beyond 
the range of representable ﬂoating-point numbers, the exception, called over-
ﬂow, is generally very serious. (Overﬂow is serious in ﬁxed-point operations 
also if it is unplanned. Because we have the alternative of using ﬂoating-point 
numbers if the magnitude of the numbers is likely to exceed what is repre-
sentable in ﬁxed-point numbers, the user is expected to use this alternative. If 
the magnitude exceeds what is representable in ﬂoating-point numbers, how-
ever, the user must resort to some indirect means, such as scaling, to solve 
the problem.) 
Underﬂow occurs whenever the result is too small to be represented as a 
normalized ﬂoating-point number. As we have seen, a nonnormalized repre-
sentation can be used to allow a gradual underﬂow. 
An invalid operation is one for which the result is not deﬁned because 
of the values of the operands. The invalid operations are addition of normal infinity∞to 
negative normal infinity−∞, multiplication of plus or minus normal infinity±∞and 0, 0 divided by 0 or by plus or minus normal infinity±∞, plus or minus normal infinity±∞divided 
by 0 or by plus or minus normal infinity±∞, extraction of the square root of a negative number (some 
systems, such as Fortran, R, and MATLAB, have a special type for complex 
numbers and deal correctly with them), and remaindering any quantity with 
0 or remaindering plus or minus normal infinity±∞with any quantity. An invalid operation results in a 
NaN. The IEEE Binary Standard 754 provides for two classes of NaNs, a 
“quiet NaN” and a “signaling NaN,” for which the processing unit will raise 
an exception, that is, set a condition that can be queried by a program. (As 
it turns out, this feature is rarely useful except in debugging programs.) 
Conformance to the IEEE Binary Standard 754 does not ensure that the 
results of multiple ﬂoating-point computations will be the same on all com-
puters. The standard does not specify the order of the computations, and 
diﬀerences in the order can change the results. The slight diﬀerences are usu-
ally unimportant, but Blackford et al. (1997a) describe some examples of 
problems that occurred when computations were performed in parallel using

556
10 Numerical Methods
a heterogeneous network of computers all of which conformed to the IEEE 
standard. 
Operations Involving Special Floating-Point Numbers 
Operations involving any of the special ﬂoating-point numbers result in values 
consistent with the meaning of the special value. For example, plus or minus normal infinity plus 1 equals plus or minus normal infinity±∞+1 = ±∞, 
plus or minus normal infinity asterisk 1 equals plus or minus normal infinity±∞∗1 = ±∞, and  plus or minus normal infinity asterisk left parenthesis negative 1 right parenthesis equals minus or plus normal infinity±∞∗(−1) = ∓∞. 
Any operation involving a single NaN results in a NaN of that same type. 
In systems that distinguish various types of NaNs, an operation involving 
diﬀerent types results in a value consistent with a hierarchy of NaNs. (In such 
cases, the terminology may diﬀer from that of the IEEE Binary Standard. For 
example, in R, any NaN is an NA, but an NA is not a NaN. Adding an NaN 
to an NA results in an NA, consistent with the relationship of not-available 
to not-a-number.) 
Comparison of Real Numbers and Floating-Point Numbers 
For most applications, the system of ﬂoating-point numbers simulates the ﬁeld 
of the reals very well. It is important, however, to be aware of some of the 
diﬀerences in the two systems. There is a very obvious useful measure for 
the reals, namely the Lebesgue measure, based on lengths of open intervals. 
An approximation of this measure is appropriate for ﬂoating-point numbers, 
even though the set of ﬂoating-point numbers within any interval is ﬁnite. 
(Because the set is ﬁnite, we might consider basing a measure on a counting 
measure, but a counting measure does not work well at all. For one thing, 
the ﬁniteness of the set of ﬂoating-point numbers means that there may be a 
diﬀerence in the cardinality of an open interval and a closed interval with the 
same endpoints. Also, the uneven distribution of ﬂoating-point values relative 
to the reals (Figs. 10.4 and 10.5) means that the cardinalities of two interval-
bounded sets with the same interval length may be diﬀerent.) 
Some general diﬀerences in the real numbers and the ﬂoating-point sys-
tem are exhibited in Table 10.3. The last four properties in Table 10.3 are 
properties of a ﬁeld. The important facts are that normal upper I normal upper RIR is an uncountable ﬁeld 
and that normal upper I normal upper FIF is a more complicated ﬁnite mathematical structure. 
10.3.3 Language Constructs for Operations on Numeric Data 
Most general-purpose computer programming languages provide constructs 
for operations that correspond to the common operations on scalar numeric 
data, such as “+”, “-”, “*” (multiplication), and “/”. These operators simulate 
the corresponding mathematical operations. As we mentioned on page 550, 
we will occasionally use notation such as 
left bracket plus right bracket Subscript normal c[+]c

10.3 Computer Operations on Numeric Data
557
Table 10.3. Diﬀerences in real numbers and ﬂoating-point numbers 
normal upper I normal upper RIR
normal upper I normal upper FIF
Cardinality:
Uncountable
Finite 
Measure:
mu left parenthesis left parenthesis x comma y right parenthesis right parenthesis equals StartAbsoluteValue x minus y EndAbsoluteValueμ((x, y)) = |x −y|
nu left parenthesis left parenthesis monospace x comma monospace y right parenthesis right parenthesis equals nu left parenthesis left bracket monospace x comma monospace y right bracket right parenthesis equals StartAbsoluteValue monospace x minus monospace y EndAbsoluteValueν((x, y)) = ν([x, y]) = |x −y|
mu left parenthesis left parenthesis x comma y right parenthesis right parenthesis equals mu left parenthesis left bracket x comma y right bracket right parenthesisμ((x, y)) = μ([x, y])
there exists monospace x comma monospace y comma monospace z comma monospace w contains as member StartAbsoluteValue monospace x minus monospace y EndAbsoluteValue equals StartAbsoluteValue monospace z minus monospace w EndAbsoluteValue∃x, y, z, w ∃|x −y| = |z −w|, 
but number sign left parenthesis monospace x comma monospace y right parenthesis not equals number sign left parenthesis monospace z comma monospace w right parenthesis#(x, y) /= #(z, w)
Continuity:
If x less than y comma there exists z contains as member x less than z less than yx < y, ∃z ∃x < z < y
monospace x less than monospace yx < y, but no monospace z contains as member monospace x less than monospace z less than monospace yz ∃x < z < y
and
and 
mu left parenthesis left bracket x comma y right bracket right parenthesis equals mu left parenthesis left parenthesis x comma y right parenthesis right parenthesisμ([x, y]) = μ((x, y))
number sign left bracket monospace x comma monospace y right bracket greater than number sign left parenthesis monospace x comma monospace y right parenthesis#[x, y] > #(x, y)
Convergence
sigma summation Underscript x equals 1 Overscript normal infinity Endscripts xE∞
x=1 x
diverges
sigma summation Underscript monospace x equals 1 Overscript normal infinity Endscripts monospace xE∞
x=1 x
converges, 
if interpreted as 
left parenthesis midline horizontal ellipsis left parenthesis left parenthesis 1 plus 2 right parenthesis plus 3 right parenthesis midline horizontal ellipsis right parenthesis(· · · ((1 + 2) + 3) · · · )
Closure:
x comma y element of normal upper I normal upper R right double arrow x plus y element of normal upper I normal upper Rx, y ∈IR ⇒x + y ∈IR
Not closed wrt addition 
x comma y element of normal upper I normal upper R right double arrow x y element of normal upper I normal upper Rx, y ∈IR ⇒xy ∈IR
Not closed wrt multiplication 
(exclusive of inﬁnities) 
Operations
a equals 0a = 0, unique
monospace a plus monospace x equals monospace b plus monospace xa + x = b + x, but monospace b not equals monospace ab /= a
with an
a plus x equals xa + x = x, for any x
monospace a plus monospace x equals monospace xa + x = x, but monospace a plus monospace y not equals monospace ya + y /= y
identity, a or a:
x minus x equals ax −x = a, for any x
monospace a plus monospace x equals monospace xa + x = x, but monospace x minus monospace x not equals monospace ax −x /= a
Associativity:
x comma y comma z element of normal upper I normal upper R right double arrowx, y, z ∈IR ⇒
left parenthesis x plus y right parenthesis plus z equals x plus left parenthesis y plus z right parenthesis(x + y) + z = x + (y + z)
Not associative 
left parenthesis x y right parenthesis z equals x left parenthesis y z right parenthesis(xy)z = x(yz)
Not associative 
Distributivity:
x comma y comma z element of normal upper I normal upper R right double arrowx, y, z ∈IR ⇒
x left parenthesis y plus z right parenthesis equals x y plus x zx(y + z) = xy + xz
Not distributive

558
10 Numerical Methods
to indicate the computer operator. The operators have slightly diﬀerent mean-
ings depending on the operand objects; that is, the operations are “over-
loaded.” Most of these operators are binary inﬁx operators, meaning that the 
operator is written between the two operands. 
Most languages provide operations beyond the four basic scalar arithmetic 
operations, especially for exponentiation, usually speciﬁed as “**” or “^”. (In 
C, exponentiation is handled by a function provided in a standard supple-
mental library <math.h>.) Exactly what the exponentiation operator means 
may be slightly diﬀerent. Some versions of Fortran, for example, interpret the 
operator always to mean: 
1. Take log 
2. Multiply by power 
3. Exponentiate 
if the base and the power are both ﬂoating-point types. This, of course, will 
not work if the base is negative, even if the power is an integer. Most ver-
sions of Fortran will determine at run time if the power is an integer and use 
repeated multiplication if it is. 
C provides some specialized operations, such as the unary preﬁx and post-
ﬁx increment “++” and decrement “--” operators, for adding or subtracting 
1. Although these operations can easily be performed directly by standard 
operators, the operators look cool, and some other languages have adopted 
them. Related augmented assignment operators, “+=”, “-=”, and so on, were 
also deﬁned in C and adopted by some other languages. 
C also overloads the basic multiplication operator so that it can indicate 
a change of meaning of a variable in addition to indicating the multiplication 
of two scalar numbers. A standard library in C (<signal.h>) allows easy 
handling of arithmetic exceptions. With this facility, for example, the user 
can distinguish a quiet NaN from a signaling NaN. 
Modern Fortran also provides numeric operators for vectors and matrices. 
The usual vector/matrix operators are implemented as intrinsic functions or 
as preﬁx operators in Fortran. 
In addition to the basic arithmetic operators, the common programming 
languages provide several other types of operators, including relational oper-
ators and operators for manipulating structures of data. 
10.3.4 Software Methods for Extending the Precision 
Software packages have been built to extend the numerical accuracy of com-
putations, possibly to the extent that the computations are exact. There are 
four ways in which this is done: 
• Multiple precision 
• Rational fractions 
• Interval arithmetic 
• Residue arithmetic

10.3 Computer Operations on Numeric Data
559
We discuss the ﬁrst three of these in this section and then in the next sec-
tion brieﬂy discuss residue arithmetic, which may also be implemented at the 
microcode or hardware level. 
Multiple Precision 
Multiple-precision operations are performed in the software by combining 
more than one computer-storage unit to represent a single number. For ex-
ample, to operate on x and y, we may represent x as a dot 10 Superscript p plus ba · 10p + b and y as 
c dot 10 Superscript p plus dc·10p +d. The product xy then is formed as a c dot 10 Superscript 2 p plus left parenthesis a d plus b c right parenthesis dot 10 Superscript p plus b dac·102p +(ad+bc)·10p +bd. The  
representation is chosen so that any of the coeﬃcients of the scaling factors 
(in this case powers of 10) can be represented to within the desired accuracy. 
Multiple precision is diﬀerent from “extended precision,” discussed earlier; 
extended precision is implemented at the hardware level or at the microcode 
level. A multiple-precision package may allow the user to specify the number 
of digits to use in representing data and performing computations. 
Bailey (1993, 1995) gives software for instrumenting Fortran code to use 
multiple-precision operations. The C and C++ library GMP, mentioned ear-
lier, provides support for multiple-precision ﬂoating-point numbers in its mpf 
package. Both the gmp package and the Rmpfr package in R (see page 582) 
are based on the GMP software library. 
Rational Fractions 
Rational fractions are ratios of two integers. If the input data can be rep-
resented exactly as rational fractions, it may be possible to preserve exact 
values of the results of computations. Using rational fractions allows avoid-
ance of reciprocation, which is the operation that most commonly yields a 
nonrepresentable value from one that is representable. Of course, any addi-
tion or multiplication that increases the magnitude of an integer in a rational 
fraction beyond a value that can be represented exactly (that is, beyond ap-
proximately 2 Superscript 23223, 2 Superscript 31231, or  2 Superscript 53253, depending on the computing system) may break 
the error-free chain of operations. 
Computations with rational fractions are often performed using a ﬁxed-
point representation. As mentioned earlier, the GMP library supports mathe-
matical operations involving big integers. This facility is what is needed to use 
rational fractions to achieve high accuracy or even exactness in computations. 
The gmp package in R facilitates use of big integers in operations involving 
rational fractions. 
Interval Arithmetic 
Interval arithmetic maintains intervals in which the data and results of compu-
tations are known to lie. Instead of working with single-point approximations, 
for which we used notation such as

560
10 Numerical Methods
left bracket x right bracket Subscript normal c[x]c
on page 535 for the value of the ﬂoating-point approximation to the real 
number x and 
left bracket ring right bracket Subscript normal c[◦]c
on page 550 for the simulated operation ring◦, we can approach the problem by 
identifying a closed interval in which x lies and a closed interval in which the 
result of the operation ring◦lies. We denote the interval operation as 
left bracket ring right bracket Subscript upper I Baseline period[◦]I.
For the real number x, we identify two ﬂoating-point numbers, x Subscript lxl and x Subscript uxu, 
such that x Subscript l Baseline less than or equals x less than or equals x Subscript uxl ≤x ≤xu. (This relationship also implies x Subscript l Baseline less than or equals left bracket x right bracket Subscript normal c Baseline less than or equals x Subscript uxl ≤[x]c ≤xu.) The 
real number x is then considered to be the interval left bracket x Subscript l Baseline comma x Subscript u Baseline right bracket[xl, xu]. For this approach 
to be useful, of course, we seek tight bounds. If x equals left bracket x right bracket Subscript normal cx = [x]c, the best interval is 
degenerate. In some other cases, either x Subscript lxl or x Subscript uxu is left bracket x right bracket Subscript normal c[x]c, and the length of the 
interval is the ﬂoating-point spacing from left bracket x right bracket Subscript normal c[x]c in the appropriate direction. 
Addition and multiplication in interval arithmetic yield the intervals 
x left bracket plus right bracket Subscript upper I Baseline y equals left bracket x Subscript l Baseline plus y Subscript l Baseline comma x Subscript u Baseline plus y Subscript u Baseline right bracketx [+]I y = [xl + yl, xu + yu]
and 
x left bracket asterisk right bracket Subscript upper I Baseline y equals left bracket min left parenthesis x Subscript l Baseline y Subscript l Baseline comma x Subscript l Baseline y Subscript u Baseline comma x Subscript u Baseline y Subscript l Baseline comma x Subscript u Baseline y Subscript u Baseline right parenthesis comma max left parenthesis x Subscript l Baseline y Subscript l Baseline comma x Subscript l Baseline y Subscript u Baseline comma x Subscript u Baseline y Subscript l Baseline comma x Subscript u Baseline y Subscript u Baseline right parenthesis right bracket periodx [∗]I y = [min(xlyl, xlyu, xuyl, xuyu), max(xlyl, xlyu, xuyl, xuyu)].
A change of sign results in left bracket minus x Subscript u Baseline comma minus x Subscript l Baseline right bracket[−xu, −xl], and  if  0 not element of left bracket x Subscript l Baseline comma x Subscript u Baseline right bracket0 /∈[xl, xu], reciprocation re-
sults in left bracket 1 divided by x Subscript u Baseline comma 1 divided by x Subscript l Baseline right bracket[1/xu, 1/xl]. See Moore (1979) or Alefeld and Herzberger (1983) for 
discussions of these kinds of operations and an extensive treatment of interval 
arithmetic. The journal Reliable Computing is devoted to interval computa-
tions. The book edited by Kearfott and Kreinovich (1996) addresses various 
aspects of interval arithmetic. One chapter in that book, by Walster (1996), 
discusses how both hardware and system software could be designed to im-
plement interval arithmetic. 
Most software support for interval arithmetic is provided through subrou-
tine libraries. The ACRITH package of IBM (see Jansen and Weidner, 1986) is 
a library of Fortran subroutines that perform computations in interval arith-
metic and also in multiple precision. Kearfott et al. (1994) have produced a 
portable Fortran library of basic arithmetic operations and elementary func-
tions in interval arithmetic, and Kearfott (1996) gives a Fortran 90 module 
deﬁning an interval data type. Jaulin et al. (2001) describe additional sources 
of software. Sun Microsystems Inc. provided full intrinsic support for interval 
data types in their Fortran compiler SunTM ONE Studio Fortran 95 (now 
distributed and maintained by Oracle, Inc.); see Walster (2005) for a descrip-
tion of the compiler extensions. The IEEE Standard P1788 speciﬁes interval 
arithmetic operations based on intervals whose endpoints are IEEE binary64 
ﬂoating-point numbers. The standard ensures exact propagation of properties 
of the computed results.

10.4 Numerical Algorithms and Analysis
561
10.3.5 Exact Computations 
Computations involving integer quantities can easily be performed exactly if 
the number of available bits to represent the integers is large enough. The use 
of rational fractions described above is the most common way of achieving 
exact computations. 
Another way of performing exact computations is by using residue arith-
metic, in which each quantity is represented as a vector of residues, all from 
a vector of relatively prime integer moduli. 
Exact Dot Product (EDP) 
As mentioned before, a common computational problem that motivates ef-
forts for exact (or at least highly accurate) computations is computing a dot 
product. The ability to compute an exact dot product, or EDP, is desirable. 
One approach, as indicated above, is to store every bit of the input ﬂoating-
point numbers in very long vectors of bits, called accumulators. Kulisch (2011) 
proposed accumulators of 4288 bits each. The Kulisch accumulator can handle 
the exact accumulation of products of 64-bit IEEE ﬂoating-point values. This 
approach incurs a large memory overhead, however. Also, unless it is accom-
plished directly in the hardware, vectorization of computations would be very 
diﬃcult. The IFIP Working Group 2.5 on Numerical Software has proposed 
that the EDP be incorporated into a new IEEE 754 Standard (see page 540). 
10.4 Numerical Algorithms and Analysis 
We will use the term “algorithm” rather loosely but always in the general 
sense of a method or a set of instructions for doing something. (Formally, 
an “algorithm” must terminate; however, respecting that deﬁnition would 
not allow us to refer to a method as an algorithm until it has been proven 
to terminate.) Algorithms are sometimes distinguished as “numerical,” “semi-
numerical,” and “nonnumerical,” depending on the extent to which operations 
on real numbers are simulated. 
Algorithms and Programs 
Algorithms are expressed by means of a ﬂowchart, a series of steps, or in a 
computer language or pseudolanguage. The expression in a computer language 
is a source program or module; hence, we sometimes use the words “algorithm” 
and “program” synonymously. 
The program is the set of computer instructions that implement the algo-
rithm. A poor implementation can render a good algorithm useless. A good 
implementation will preserve the algorithm’s accuracy and eﬃciency and will

562
10 Numerical Methods
detect data that are inappropriate for the algorithm. Robustness is more a 
property of the program than of the algorithm. 
The exact way an algorithm is implemented in a program depends of course 
on the programming language, but it also may depend on the computer and 
associated system software. A program that will run on most systems without 
modiﬁcation is said to be portable. 
The two most important aspects of a computer algorithm are its accuracy 
and its eﬃciency. Although each of these concepts appears rather simple on 
the surface, both are actually fairly complicated, as we shall see. 
10.4.1 Error in Numerical Computations 
An “accurate” algorithm is one that gets the “right” answer. Knowing that 
the right answer may not be representable and that rounding within a set of 
operations may result in variations in the answer, we often must settle for an 
answer that is “close.” As we have discussed previously, we measure error, or 
closeness, as either the absolute error or the relative error of a computation. 
Another way of considering the concept of “closeness” is by looking back-
ward from the computed answer and asking what perturbation of the original 
problem would yield the computed answer exactly. This approach, developed 
by Wilkinson (1963), is called backward error analysis. The backward anal-
ysis is followed by an assessment of the eﬀect of the perturbation on the 
solution. Although backward error analysis may not seem as natural as “for-
ward” analysis (in which we assess the diﬀerence between the computed and 
true solutions), it is easier to perform because all operations in the backward 
analysis are performed in normal upper I normal upper FIF instead of in normal upper I normal upper RIR. Each step in the backward anal-
ysis involves numbers in the set normal upper I normal upper FIF, that is, numbers that could actually have 
participated in the computations that were performed. Because the properties 
of the arithmetic operations in normal upper I normal upper RIR do not hold and, at any step in the sequence 
of computations, the result in normal upper I normal upper RIR may not exist in normal upper I normal upper FIF, it is very diﬃcult to carry 
out a forward error analysis. 
There are other complications in assessing errors in computations. Sup-
pose the value to be computed is a vector, such as a solution to a linear 
system. What norm do we use to compare the closeness of vectors? Another, 
more complicated situation for which assessing correctness may be diﬃcult 
is random number generation. It would be diﬃcult to assign a meaning to 
“accuracy” for such a problem. 
The basic source of error in numerical computations is the inability to work 
with the reals. The ﬁeld of reals is simulated with a ﬁnite set. This has several 
consequences. A real number is rounded to a ﬂoating-point number, the result 
of an operation on two ﬂoating-point numbers is rounded to another ﬂoating-
point number, and passage to the limit, which is a fundamental concept in 
the ﬁeld of reals, is not possible in the computer. 
Rounding errors that occur just because the result of an operation is not 
representable in the computer’s set of ﬂoating-point numbers are usually not

10.4 Numerical Algorithms and Analysis
563
too bad. Of course, if they accumulate through the course of many operations, 
the ﬁnal result may have an unacceptably large rounding error. 
A natural approach to studying errors in ﬂoating-point computations is 
to deﬁne random variables for the rounding at all stages, from the initial 
representation of the operands, through any intermediate computations, to 
the ﬁnal result. Given a probability model for the rounding error in the rep-
resentation of the input data, a statistical analysis of rounding errors can 
be performed. Wilkinson (1963) introduced a uniform probability model for 
rounding of input and derived distributions for computed results based on 
that model. Linnainmaa (1975) discusses the eﬀects of accumulated errors in 
ﬂoating-point computations based on a more general model of the rounding 
for the input. This approach leads to a forward error analysis that provides a 
probability distribution for the error in the ﬁnal result. 
The obvious probability model for ﬂoating-point representations is that 
the reals within an interval between any two ﬂoating-point numbers have 
a uniform distribution (see Fig. 10.4 on page 537). A probability model for 
the real line can be built up as a mixture of the uniform distributions (see 
Exercise 10.9 on page 586). The density is obviously 0 in the tails. While 
a model based on simple distributions may be appropriate for the rounding 
error due to the ﬁnite-precision representation of real numbers, probability 
models for rounding errors in ﬂoating-point computations are not so simple. 
This is because the rounding errors in computations are not random. 
Another, more pernicious, eﬀect of rounding can occur in a single oper-
ation, resulting in catastrophic cancellation, as we have discussed previously 
(see page 554). 
Measures of Error and Bounds for Errors 
For the simple case of representing the real number r by an approximation 
r overTilde˜r, we deﬁne absolute error, StartAbsoluteValue r overTilde minus r EndAbsoluteValue|˜r −r|, and relative error, StartAbsoluteValue r overTilde minus r EndAbsoluteValue divided by StartAbsoluteValue r EndAbsoluteValue|˜r −r|/|r| (so long 
as r not equals 0r /= 0). These same types of measures are used to express the errors in 
numerical computations. As we indicated above, however, the result may not 
be a simple real number; it may consist of several real numbers. For example, 
in statistical data analysis, the numerical result, r overTilde˜r, may consist of estimates 
of several regression coeﬃcients, various sums of squares and their ratio, and 
several other quantities. We may then be interested in some more general 
measure of the diﬀerence of r overTilde˜r and r, 
upper Delta left parenthesis r overTilde comma r right parenthesis commaΔ(˜r, r),
where upper Delta left parenthesis dot comma dot right parenthesisΔ(·, ·) is a nonnegative, real-valued function. This is the absolute error, 
and the relative error is the ratio of the absolute error to upper Delta left parenthesis r comma r 0 right parenthesisΔ(r, r0), where  r 0r0
is a baseline value, such as 0. When r, instead of just being a single number, 
consists of several components, we must measure error diﬀerently. If r is a 
vector, the measure may be based on some norm, and in that case, upper Delta left parenthesis r overTilde comma r right parenthesisΔ(˜r, r)

564
10 Numerical Methods
may be denoted by parallel to left parenthesis r overTilde minus r right parenthesis parallel to||(˜r −r)||. A norm tends to become larger as the number 
of elements increases, so instead of using a raw norm, it may be appropriate 
to scale the norm to reﬂect the number of elements being computed. 
However the error is measured, for a given algorithm, we would like to have 
some knowledge of the amount of error to expect or at least some bound on the 
error. Unfortunately, almost any measure contains terms that depend on the 
quantity being evaluated. Given this limitation, however, often we can develop 
an upper bound on the error. In other cases, we can develop an estimate of an 
“average error” based on some assumed probability distribution of the data 
comprising the problem. 
In a Monte Carlo method, we estimate the solution based on a “random” 
sample, so one source of error in addition to any numerical computational 
errors, is the “sampling error.” Just as in ordinary statistical estimation, we 
are concerned about the variance of the estimate, which we relate to the 
sampling error. We can usually derive expressions for the variance of the 
estimator in terms of the quantity being evaluated, and of course we can 
estimate the variance of the estimator using the realized random sample. The 
standard deviation of the estimator provides an indication of the distance 
around the computed quantity within which we may have some conﬁdence that 
the true value lies. The standard deviation is sometimes called the “standard 
error,” or the “probabilistic error bound.” 
It is often useful to identify the “order of the error” whether we are con-
cerned about error bounds, average expected error, or the standard deviation 
of an estimator. In general, we speak of the order of one function in terms of 
another function as a common argument of the functions approaches a given 
value. A function f left parenthesis t right parenthesisf(t) is said to be of order g left parenthesis t right parenthesisg(t) at t 0t0, written normal upper O left parenthesis g left parenthesis t right parenthesis right parenthesisO(g(t)) (“big normal upper OO
of g left parenthesis t right parenthesisg(t)”), if there exists a positive constant M such that 
StartAbsoluteValue f left parenthesis t right parenthesis EndAbsoluteValue less than or equals upper M StartAbsoluteValue g left parenthesis t right parenthesis EndAbsoluteValue normal a normal s t right arrow t 0 period|f(t)| ≤M|g(t)|
as t →t0.
This is the order of convergence of one function to another function at a given 
point. 
If our objective is to compute f left parenthesis t right parenthesisf(t) and we use an approximation ModifyingAbove f With tilde left parenthesis t right parenthesis ˜f(t), the  
order of the error due to the approximation is the order of the convergence. 
In this case, the argument of the order of the error may be some variable that 
deﬁnes the approximation. For example, if ModifyingAbove f With tilde left parenthesis t right parenthesis ˜f(t) is a ﬁnite series approximation 
to f left parenthesis t right parenthesisf(t) using, say, k terms, we may express the error as normal upper O left parenthesis h left parenthesis k right parenthesis right parenthesisO(h(k)) for some func-
tion h left parenthesis k right parenthesish(k). Typical orders of errors due to the approximation may be normal upper O left parenthesis 1 divided by k right parenthesisO(1/k), 
normal upper O left parenthesis 1 divided by k squared right parenthesisO(1/k2), or  normal upper O left parenthesis 1 divided by k factorial right parenthesisO(1/k!). An approximation with order of error normal upper O left parenthesis 1 divided by k factorial right parenthesisO(1/k!) is to be 
preferred over one order of error normal upper O left parenthesis 1 divided by k right parenthesisO(1/k) because the error is decreasing more 
rapidly. The order of error due to the approximation is only one aspect to 
consider; roundoﬀ error in the representation of any intermediate quantities 
must also be considered. 
We will discuss the order of error in iterative algorithms further in 
Sect. 10.4.3 beginning on page 576. (We will discuss order also in measuring 
the speed of an algorithm in Sect. 10.4.2.)

10.4 Numerical Algorithms and Analysis
565
The special case of convergence to the constant zero is often of interest. A 
function f left parenthesis t right parenthesisf(t) is said to be “little normal oo of g left parenthesis t right parenthesisg(t)” at  t 0t0, written normal o left parenthesis g left parenthesis t right parenthesis right parenthesiso(g(t)), if  
f left parenthesis t right parenthesis divided by g left parenthesis t right parenthesis right arrow 0 normal a normal s t right arrow t 0 periodf(t)/g(t) →0
as t →t0.
If the function f left parenthesis t right parenthesisf(t) approaches 0 at t 0t0, g left parenthesis t right parenthesisg(t) can be taken as a constant and 
f left parenthesis t right parenthesisf(t) is said to be normal o left parenthesis 1 right parenthesiso(1). 
Big O and little o convergences are deﬁned in terms of dominating func-
tions. In the analysis of algorithms, it is often useful to consider analogous 
types of convergence in which the function of interest dominates another func-
tion. This type of relationship is similar to a lower bound. A function f left parenthesis t right parenthesisf(t) is 
said to be normal upper Omega left parenthesis g left parenthesis t right parenthesis right parenthesisΩ(g(t)) (“big omega of g left parenthesis t right parenthesisg(t)”) if there exists a positive constant m 
such that 
StartAbsoluteValue f left parenthesis t right parenthesis EndAbsoluteValue greater than or equals m StartAbsoluteValue g left parenthesis t right parenthesis EndAbsoluteValue normal a normal s t right arrow t 0 period|f(t)| ≥m|g(t)|
as t →t0.
Likewise, a function f left parenthesis t right parenthesisf(t) is said to be “little omega of g left parenthesis t right parenthesisg(t)” at  t 0t0, written 
omega left parenthesis g left parenthesis t right parenthesis right parenthesisω(g(t)), if  
g left parenthesis t right parenthesis divided by f left parenthesis t right parenthesis right arrow 0 normal a normal s t right arrow t 0 periodg(t)/f(t) →0
as t →t0.
Usually the limit on t in order expressions is either 0 or normal infinity∞, and because it 
is obvious from the context, mention of it is omitted. The order of the error 
in numerical computations usually provides a measure in terms of something 
that can be controlled in the algorithm, such as the point at which an inﬁnite 
series is truncated in the computations. The measure of the error usually also 
contains expressions that depend on the quantity being evaluated, however. 
Error of Approximation 
Some algorithms are exact, such as an algorithm to multiply two matrices 
that just uses the deﬁnition of matrix multiplication. In this case, there may 
be numerical errors due to rounding or other computational events, but no 
errors due to the computational approach. Here we focus on additional errors 
that are due to the algorithms not being exact. 
Algorithms may be approximate either because the algorithm itself is iter-
ative, such as one using the Gauss-Seidel method (page 274), or because the 
algorithm makes use of approximations to the desired value, possibly because 
the result to be computed does not have a ﬁnite closed-form expression. An 
example of the latter is the evaluation of the normal cumulative distribution 
function. One way of evaluating this is by using a rational polynomial approxi-
mation to the distribution function. Such an expression may be evaluated with 
very little rounding error, but the expression has an error of approximation. 
On the other hand, the component of the error in an iterative algorithm due 
to eventually having to halt the iterations is an error of truncation. The trun-
cation error is also an error of approximation. 
When solving a diﬀerential equation on the computer, the diﬀerential equa-
tion is often approximated by a diﬀerence equation. Even though the diﬀer-
ences used may not be constant, they are ﬁnite and the passage to the limit

566
10 Numerical Methods
can never be eﬀected. This kind of approximation leads to a discretization 
error. The amount of the discretization error has nothing to do with rounding 
error. If the last diﬀerences used in the algorithm are delta tδt, then the error is 
usually of order normal upper O left parenthesis delta t right parenthesisO(δt), even if the computations are performed exactly. 
The type of error of approximation that occurs when an algorithm uses 
a series expansion is similar to the error that occurs in using an iterative 
algorithm. The series may be exact, and in principle the evaluation of all 
terms would yield an exact result. The algorithm uses only a ﬁnite number 
of terms, and the resulting component of the error is truncation error. This  
is the type of error we discussed in connection with Fourier expansions on 
pages 52 and 120. Often the exact expansion is an inﬁnite series, and we 
approximate it with a ﬁnite series. When a truncated Taylor series is used to 
evaluate a function at a given point x 0x0, the order of the truncation error is 
the derivative of the function that would appear in the ﬁrst unused term of 
the series, evaluated at x 0x0. 
We need to have some knowledge of the magnitude of the error. For al-
gorithms that use approximations, it is often useful to express the order of 
the error in terms of some quantity used in the algorithm or in terms of some 
aspect of the problem itself. We must be aware, however, of the limitations 
of such measures of the errors or error bounds. For an oscillating function, 
for example, the truncation error may never approach zero over any nonzero 
interval. 
Algorithms and Data 
The performance of an algorithm may depend on the data. We have seen that 
even the simple problem of computing the roots of a quadratic polynomial, 
a x squared plus b x plus cax2+bx+c, using the quadratic formula, Eq. (10.3), can lead to severe cancel-
lation. For many values of a, b, and  c, the quadratic formula works perfectly 
well. Data that are likely to cause computational problems are referred to as 
ill-conditioned data, and, more generally, we speak of the “condition” of data. 
The concept of condition is understood in the context of a particular set of 
operations. Heuristically, data for a given problem are ill-conditioned if small 
changes in the data may yield large changes in the solution. 
Consider the problem of ﬁnding the roots of a high-degree polynomial, 
for example. Wilkinson (1959) gave an example of a polynomial that is very 
simple on the surface yet whose solution is very sensitive to small changes of 
the values of the coeﬃcients: 
StartLayout 1st Row 1st Column f left parenthesis x right parenthesis 2nd Column equals 3rd Column left parenthesis x minus 1 right parenthesis left parenthesis x minus 2 right parenthesis midline horizontal ellipsis left parenthesis x minus 20 right parenthesis 2nd Row 1st Column Blank 2nd Column equals 3rd Column x Superscript 20 Baseline minus 210 x Superscript 19 Baseline plus midline horizontal ellipsis plus 20 factorial period EndLayoutf(x) = (x −1)(x −2) · · · (x −20)
= x20 −210x19 + · · · + 20!.
(10.4) 
While the solution is easy to see from the factored form, the solution is very 
sensitive to perturbations of the coeﬃcients. For example, changing the coef-
ﬁcient 210 to 210 plus 2 Superscript negative 23210+2−23 changes the roots drastically; in fact, ten of them are

10.4 Numerical Algorithms and Analysis
567
now complex. Of course, the extreme variation in the magnitudes of the coeﬃ-
cients should give us some indication that the problem may be ill-conditioned. 
Condition of Data 
We attempt to quantify the condition of a set of data for a particular set of 
operations by means of a condition number. Condition numbers are deﬁned 
to be positive and in such a way that large values of the numbers mean 
that the data or problems are ill-conditioned. A useful condition number for 
the problem of ﬁnding roots of a diﬀerentiable function can be deﬁned to 
be increasing as the reciprocal of the absolute value of the derivative of the 
function in the vicinity of a root. 
In the solution of a linear system of equations, the coeﬃcient matrix de-
termines the condition of the problem. The most commonly used condition 
number for this problem is the one based on ratio of eigenvalues (or singular 
values) that we discussed in Sect. 5.1.1 on page 261. 
Condition numbers are only indicators of possible numerical diﬃculties for 
a given problem. They must be used with some care. For example, according 
to the condition number for ﬁnding roots based on the derivative, Wilkinson’s 
polynomial is well-conditioned. 
Robustness of Algorithms 
The ability of an algorithm to handle a wide range of data and either to solve 
the problem as requested or to determine that the condition of the data does 
not allow the algorithm to be used is called the robustness of the algorithm. 
Stability of Algorithms 
Another concept that is quite diﬀerent from robustness is stability. An algo-
rithm is said to be  stable if it always yields a solution that is an exact solution 
to a perturbed problem; that is, for the problem of computing f left parenthesis x right parenthesisf(x) using the 
input data x, an algorithm is stable if the result it yields, ModifyingAbove f With tilde left parenthesis x right parenthesis ˜f(x), is  
f left parenthesis x plus delta x right parenthesisf(x + δx)
for some (bounded) perturbation delta xδx of x. Stated another way, an algorithm 
is stable if small perturbations in the input or in intermediate computations 
do not result in large diﬀerences in the results. 
The concept of stability for an algorithm should be contrasted with the 
concept of condition for a problem or a dataset. If a problem is ill-conditioned, 
a stable algorithm (a “good algorithm”) will produce results with large dif-
ferences for small diﬀerences in the speciﬁcation of the problem. This is be-
cause the exact results have large diﬀerences. An algorithm that is not sta-
ble, however, may produce large diﬀerences for small diﬀerences in the com-
puter description of the problem, which may involve rounding, truncation,

568
10 Numerical Methods
or discretization, or for small diﬀerences in the intermediate computations 
performed by the algorithm. 
The concept of stability arises from backward error analysis. The stability 
of an algorithm may depend on how continuous quantities are discretized, 
such as when a range is gridded for solving a diﬀerential equation. 
Reducing the Error in Numerical Computations 
An objective in designing an algorithm to evaluate some quantity is to avoid 
accumulated rounding error and to avoid catastrophic cancellation. In the dis-
cussion of ﬂoating-point operations above, we have seen two examples of how 
an algorithm can be constructed to mitigate the eﬀect of accumulated round-
ing error (using Eqs. (10.2) on page 553 for computing a sum) and to avoid 
possible catastrophic cancellation in the evaluation of the expression (10.3) 
for the roots of a quadratic equation. 
Another example familiar to statisticians is the computation of the sample 
sum of squares: 
sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis x Subscript i Baseline minus x overbar right parenthesis squared equals sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i Superscript 2 Baseline minus n x overbar squared period
n
E
i=1
(xi −¯x)2 =
n
E
i=1
x2
i −n¯x2.
(10.5) 
This quantity is left parenthesis n minus 1 right parenthesis s squared(n −1)s2, where  s squareds2 is the sample variance. 
Either expression in Eq. (10.5) can be thought of as describing an algo-
rithm. The expression on the left-hand side implies the “two-pass” algorithm: 
StartLayout 1st Row a equals x 1 2nd Row normal f normal o normal r i equals 2 comma ellipsis comma n 3rd Row left brace 4th Row a equals x Subscript i Baseline plus a 5th Row right brace 6th Row a equals a divided by n 7th Row b equals left parenthesis x 1 minus a right parenthesis squared 8th Row normal f normal o normal r i equals 2 comma ellipsis comma n 9th Row left brace 10th Row b equals left parenthesis x Subscript i Baseline minus a right parenthesis squared plus b 11th Row right brace period EndLayout
a = x1
for i = 2, . . . , n
{
a = xi + a
}
a = a/n
b = (x1 −a)2
for i = 2, . . . , n
{
b = (xi −a)2 + b
}.
(10.6) 
This algorithm yields x overbar equals a¯x = a and left parenthesis n minus 1 right parenthesis s squared equals b(n −1)s2 = b. Each of the sums computed 
in this algorithm may be improved by using Eqs. (10.2). A problem with this 
algorithm is the fact that it requires two passes through the data. Because 
the quantities in the second summation are squares of residuals, they are 
likely to be of relatively equal magnitude. They are of the same sign, so there 
will be no catastrophic cancellation in the early stages when the terms being 
accumulated are close in size to the current value of b. There will be some 
accuracy loss as the sum b grows, but the addends left parenthesis x Subscript i Baseline minus a right parenthesis squared(xi −a)2 remain roughly 
the same size. The accumulated rounding error, however, may not be too bad.

10.4 Numerical Algorithms and Analysis
569
The expression on the right-hand side of Eq. (10.5) implies the “one-pass” 
algorithm: 
StartLayout 1st Row a equals x 1 2nd Row b equals x 1 squared 3rd Row normal f normal o normal r i equals 2 comma ellipsis comma n 4th Row left brace 5th Row a equals x Subscript i Baseline plus a 6th Row b equals x Subscript i Superscript 2 Baseline plus b 7th Row right brace 8th Row a equals a divided by n 9th Row b equals b minus n a squared period EndLayout
a = x1
b = x2
1
for i = 2, . . . , n
{
a = xi + a
b = x2
i + b
}
a = a/n
b = b −na2.
(10.7) 
This algorithm requires only one pass through the data, but if the x Subscript ixis have  
magnitudes larger than 1, the algorithm has built up two relatively large 
quantities, b and n a squaredna2. These quantities may be of roughly equal magnitudes; 
subtracting one from the other may lead to catastrophic cancellation (see 
Exercise 10.16, page 587). 
Another algorithm is shown in Eqs. (10.8). It requires just one pass through 
the data, and the individual terms are generally accumulated fairly accurately. 
Equations (10.8) are a form of the Kalman ﬁlter. 
StartLayout 1st Row a equals x 1 2nd Row b equals 0 3rd Row normal f normal o normal r i equals 2 comma ellipsis comma n 4th Row left brace 5th Row d equals left parenthesis x Subscript i Baseline minus a right parenthesis divided by i 6th Row a equals d plus a 7th Row b equals i left parenthesis i minus 1 right parenthesis d squared plus b 8th Row right brace period EndLayout
a = x1
b = 0
for i = 2, . . . , n
{
d = (xi −a)/i
a = d + a
b = i(i −1)d2 + b
}.
(10.8) 
A useful measure to quantify the sensitivity of s, the sample standard 
deviation, to the data, the x Subscript ixis, is the condition number 
kappa equals StartFraction sigma summation Underscript i equals 1 Overscript n Endscripts x Subscript i Superscript 2 Baseline Over StartRoot n minus 1 EndRoot s EndFraction periodκ =
En
i=1 x2
i
√n −1s.
(10.9) 
This is a measure of the “stiﬀness” of the data. It is clear that if the mean 
is large relative to the variance, this condition number will be large. (Recall 
that large condition numbers imply ill-conditioning, and also recall that condi-
tion numbers must be interpreted with some care.) Notice that this condition 
number achieves its minimum value of 1 for the data x Subscript i Baseline minus x overbarxi −¯x, so if the compu-
tations for x overbar¯x and x Subscript i Baseline minus x overbarxi −¯x were exact, the data in the last part of the algorithm 
in Eqs. (10.6) would be perfectly conditioned. A dataset with a large mean 
relative to the variance is said to be stiﬀ. 
Often when a ﬁnite series is to be evaluated, it is necessary to accumulate 
a set of terms of the series that have similar magnitudes and then combine 
this with similar partial sums. It may also be necessary to scale the individual

570
10 Numerical Methods
terms by some very large or very small multiplicative constant while the terms 
are being accumulated and then remove the scale after some computations 
have been performed. 
Chan et al. (1982) propose a modiﬁcation of the algorithm in Eqs. (10.8) 
to use pairwise accumulations (as in the fan-in method discussed previously). 
Chan et al. (1983) make extensive comparisons of the methods and give error 
bounds based on the condition number. 
10.4.2 Eﬃciency 
The eﬃciency of an algorithm refers to its usage of computer resources. The 
two most important resources are the processing units and the memory. The 
amount of time the processing units are in use and the amount of memory 
required are the key measures of eﬃciency. A limiting factor for the time 
the processing units are in use is the number and type of operations required. 
Some operations take longer than others; for example, the operation of adding 
ﬂoating-point numbers may take more time than the operation of adding ﬁxed-
point numbers. This, of course, depends on the computer system and on what 
kinds of ﬂoating-point or ﬁxed-point numbers we are dealing with. If we have 
a measure of the size of the problem, we can characterize the performance of 
a given algorithm by specifying the number of operations of each type or just 
the number of operations of the slowest type. 
Measuring Eﬃciency: Counting Computations 
The number of computations required by an algorithm ultimately determines 
its eﬃciency or the time it requires for completion of a given task. 
Often, instead of the exact number of operations, we use the order of the 
number of operations in terms of the measure of problem size. If n is some 
measure of the size of the problem, an algorithm has order normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesisO(f(n)) if, as 
n right arrow normal infinityn →∞, the number of computations right arrow c f left parenthesis n right parenthesis→cf(n), where  c is some constant 
that does not depend on n. For example, to multiply two n times nn × n matrices 
in the obvious way requires normal upper O left parenthesis n cubed right parenthesisO(n3) multiplications and additions; to multiply 
an n times mn × m matrix and an m times pm × p matrix requires normal upper O left parenthesis n m p right parenthesisO(nmp) multiplications and 
additions. In the latter case, n, m, and  p are all measures of the size of the 
problem. 
Notice that in the deﬁnition of order there is a constant c. Two algorithms 
that have the same order may have diﬀerent constants and in that case are 
said to “diﬀer only in the constant.” The order of an algorithm is a measure 
of how well the algorithm “scales,” that is, the extent to which the algorithm 
can deal with truly large problems. 
Let n be a measure of the problem size, and let b and q be positive 
constants. An algorithm of order normal upper O left parenthesis b Superscript n Baseline right parenthesisO(bn) has exponential order, one of order 
normal upper O left parenthesis n Superscript q Baseline right parenthesisO(nq) has polynomial order, and one of order  normal upper O left parenthesis log n right parenthesisO(log n) has log order. No-
tice that for log order it does not matter what the base is. Also, notice that

10.4 Numerical Algorithms and Analysis
571
normal upper O left parenthesis log n Superscript q Baseline right parenthesis equals normal upper O left parenthesis log n right parenthesisO(log nq) = O(log n). For a given task with an obvious algorithm that has 
polynomial order, it is often possible to modify the algorithm to address parts 
of the problem so that in the order of the resulting algorithm one n factor is 
replaced by a factor of log nlog n. 
Although it is often relatively easy to determine the order of an algo-
rithm, an interesting question in algorithm design involves the order of the 
problem, that is, the order of the most eﬃcient algorithm possible. A problem 
of polynomial order is usually considered tractable, whereas one of exponen-
tial order may require a prohibitively excessive amount of time for its solution. 
An interesting class of problems are those for which a solution can be veriﬁed 
in polynomial time yet for which no polynomial algorithm is known to ex-
ist. Such a problem is called a nondeterministic polynomial, or NP,  problem.  
“Nondeterministic” does not imply any randomness; it refers to the fact that 
no polynomial algorithm for determining the solution is known. Most inter-
esting NP problems can be shown to be equivalent to each other in order by 
reductions that require polynomial time. Any problem in this subclass of NP 
problems is equivalent in some sense to all other problems in the subclass and 
so such a problem is said to be NP-complete. 
For many problems it is useful to measure the size of a problem in some 
standard way and then to identify the order of an algorithm for the problem 
with separate components. A common measure of the size of a problem is L, 
the length of the stream of data elements. An n times nn×n matrix would have length 
proportional to upper L equals n squaredL = n2, for example. To multiply two n times nn × n matrices in the 
obvious way requires normal upper O left parenthesis upper L Superscript 3 divided by 2 Baseline right parenthesisO(L3/2) multiplications and additions, as we mentioned 
above. 
In analyzing algorithms for more complicated problems, we may wish to 
determine the order in the form 
normal upper O left parenthesis f left parenthesis n right parenthesis g left parenthesis upper L right parenthesis right parenthesisO(f(n)g(L))
because L is an essential measure of the problem size and n may depend on 
how the computations are performed. For example, in the linear programming 
problem, with n variables and m constraints with a dense coeﬃcient matrix, 
there are order nm data elements. Algorithms for solving this problem gen-
erally depend on the limit on n, so we may speak of a linear programming 
algorithm as being normal upper O left parenthesis n cubed upper L right parenthesisO(n3L), for example, or of some other algorithm as being 
normal upper O left parenthesis StartRoot n EndRoot upper L right parenthesisO(√nL). (In deﬁning L, it is common to consider the magnitudes of the data 
elements or the precision with which the data are represented, so that L is 
the order of the total number of bits required to represent the data. This level 
of detail can usually be ignored, however, because the limits involved in the 
order are generally not taken on the magnitude of the data but only on the 
number of data elements.) 
The order of an algorithm (or, more precisely, the “order of operations of 
an algorithm”) is an asymptotic measure of the operation count as the size 
of the problem goes to inﬁnity. The order of an algorithm is important, but 
in practice the actual count of the operations is also important. In practice,

572
10 Numerical Methods
an algorithm whose operation count is approximately n squaredn2 may be more useful 
than one whose count is 1000 left parenthesis n log n plus n right parenthesis1000(n log n + n), although the latter would have 
order normal upper O left parenthesis n log n right parenthesisO(n log n), which is much better than that of the former, normal upper O left parenthesis n squared right parenthesisO(n2). When  
an algorithm is given a ﬁxed-size task many times, the ﬁnite eﬃciency of the 
algorithm becomes very important. 
The number of computations required to perform some tasks depends not 
only on the size of the problem but also on the data. For example, for most 
sorting algorithms, it takes fewer computations (comparisons) to sort data 
that are already almost sorted than it does to sort data that are completely 
unsorted. We sometimes speak of the average time and the worst-case time 
of an algorithm. For some algorithms, these may be very diﬀerent, whereas 
for other algorithms or for some problems, these two may be essentially the 
same. 
Our main interest is usually not in how many computations occur but 
rather in how long it takes to perform the computations. Because some com-
putations can take place simultaneously, even if all kinds of computations 
required the same amount of time, the order of time could be diﬀerent from 
the order of the number of computations. 
The actual number of ﬂoating-point operations divided by the time in 
seconds required to perform the operations is called the FLOPS (ﬂoating-point 
operations per second) rate. Confusingly, “FLOP” also means “ﬂoating-point 
operation,” and “FLOPs” is the plural of “FLOP.” Of course, as we tend to 
use lowercase more often, we must use the context to distinguish “ﬂops” as a 
rate from “ﬂops” the plural of “ﬂop.” 
In addition to the actual processing, the data may need to be copied from 
one storage position to another. Data movement slows the algorithm and may 
cause it not to use the processing units to their fullest capacity. When groups 
of data are being used together, blocks of data may be moved from ordinary 
storage locations to an area from which they can be accessed more rapidly. The 
eﬃciency of a program is enhanced if all operations that are to be performed 
on a given block of data are performed one right after the other. Sometimes a 
higher-level language prevents this from happening. For example, to add two 
arrays (matrices) in modern Fortran, a single statement is suﬃcient: 
A = B + C  
Now, if we also want to add B to the array E, we may  write  
A = B + C  
D = B + E  
These two Fortran statements together may be less eﬃcient than writing a 
traditional loop in Fortran or in C because the array B may be accessed a 
second time needlessly. (Of course, this is relevant only if these arrays are 
very large.)

10.4 Numerical Algorithms and Analysis
573
Measuring Eﬃciency: Timing Computations 
A computing system has a system clock that measures time used by various 
computing resources in the system. Exactly how it works and what is measured 
depends on the speciﬁc system, but typically three times are measured, “user 
time,” “system time,” and “elapsed time.” User time is the amount of time the 
core processor or processors used in executing the user’s instructions. Diﬀerent 
systems may account for usage by multiple core processors diﬀerently. System 
time is the time the system used in managing resources on behalf of the calling 
process. Elapsed time, or “wall time,” is the total time from the the start of 
the R system program. 
Improving Eﬃciency 
There are many ways to attempt to improve the eﬃciency of an algorithm. 
Often the best way is just to look at the task from a higher level of detail and 
attempt to construct a new algorithm. Many obvious algorithms are serial 
methods that would be used for hand computations and so are not the best 
for use on the computer. 
An eﬀective general method of developing an eﬃcient algorithm is called 
divide and conquer. In this method, the problem is broken into subproblems, 
each of which is solved, and then the subproblem solutions are combined into 
a solution for the original problem. In some cases, this can result in a net 
savings either in the number of computations, resulting in an improved order 
of computations, or in the number of computations that must be performed 
serially, resulting in an improved order of time. 
Let the time required to solve a problem of size n be t left parenthesis n right parenthesist(n), and consider 
the recurrence relation 
t left parenthesis n right parenthesis equals p t left parenthesis n divided by p right parenthesis plus c nt(n) = pt(n/p) + cn
for p positive and c nonnegative. Then t left parenthesis n right parenthesis element of normal upper O left parenthesis n log n right parenthesist(n) ∈O(n log n) (see Exercise 10.18, 
page 588). Divide and conquer strategies can sometimes be used together with 
a simple method that would be normal upper O left parenthesis n squared right parenthesisO(n2) if applied directly to the full problem 
to reduce the order to normal upper O left parenthesis n log n right parenthesisO(n log n). 
The “fan-in algorithm” (see page 553) is an example of a divide and con-
quer strategy that allows normal upper O left parenthesis n right parenthesisO(n) operations to be performed in normal upper O left parenthesis log n right parenthesisO(log n) time 
if the operations can be performed simultaneously. The number of operations 
does not change materially; the improvement is in the time. 
Although there have been orders of magnitude improvements in the speed 
of computers because the hardware is better, the order of time required to 
solve a problem is almost entirely dependent on the algorithm. The improve-
ments in eﬃciency resulting from hardware improvements are generally dif-
ferences only in the constant. The practical meaning of the order of the time 
must be considered, however, and so the constant may be important. In the 
fan-in algorithm, for example, the improvement in order is dependent on the

574
10 Numerical Methods
unrealistic assumption that as the problem size increases without bound, the 
number of processors also increases without bound. Divide and conquer strate-
gies do not require multiple processors for their implementation, of course. 
Some algorithms are designed so that each step is as eﬃcient as possi-
ble, without regard to what future steps may be part of the algorithm. An 
algorithm that follows this principle is called a greedy algorithm. A greedy 
algorithm is often useful in the early stages of computation for a problem or 
when a problem lacks an understandable structure. 
Scalability 
We expect to devote more resources to solving large-scale problems than to 
solving smaller problems. Whether or not the additional resources allow the 
larger problems to be solved in an acceptable length of time depends on the 
system or process being used to solve the problem, as well as on the nature of 
the problem itself. For example, in adding a set of numbers, the fan-in algo-
rithm requires additional adders as the size of the problem grows. Assuming 
that these additional resources can be provided, the time required is of log 
order. 
A system or process is called scalable if, as the size of the problem increases, 
additional resources can be provided to the system or process so that its 
performance is not badly degraded. The addition of resources is called scaling 
the system. 
Scaling a system can be done in various ways. More random-access memory 
can be added to a computer or more cores can be added to the CPU, for 
example. This is sometimes called scaling up the system, because the basic 
structure of the system does not change. In very large-scale problems, the 
system itself can be expanded into one that consists of distributed computing 
systems. This is called scaling out the system 
As a general concept, the words scalable and scalability are useful, but 
because of the imprecision, I do not use the words often. The order of compu-
tations or of time expressed as a function of the size of the problem and the 
resources available are more meaningful measures, although, as we have seen 
above, there may be inherent ambiguities in those measures. 
Bottlenecks and Limits 
There is a maximum FLOPS rate possible for a given computer system. This 
rate depends on how fast the individual processing units are, how many pro-
cessing units there are, and how fast data can be moved around in the system. 
The more eﬃcient an algorithm is, the closer its achieved FLOPS rate is to 
the maximum FLOPS rate. 
For a given computer system, there is also a maximum FLOPS rate possi-
ble for a given problem. This has to do with the nature of the tasks within the 
given problem. Some kinds of tasks can utilize various system resources more

10.4 Numerical Algorithms and Analysis
575
easily than other tasks. If a problem can be broken into two tasks, upper T 1T1 and 
upper T 2T2, such that upper T 1T1 must be brought to completion before upper T 2T2 can be performed, 
the total time required for the problem depends more on the task that takes 
longer. This tautology (which is sometimes called someone’s “law”) has im-
portant implications for the limits of eﬃciency of algorithms. The speedup of 
problems that consist of both tasks that must be performed sequentially and 
tasks that can be performed in parallel is limited by the time required for the 
sequential tasks. 
The eﬃciency of an algorithm may depend on the organization of the 
computer, the implementation of the algorithm in a programming language, 
and the way the program is compiled. 
High-Performance Computing 
In “high-performance” computing major emphasis is placed on computational 
eﬃciency. The architecture of the computer becomes very important, and the 
software is designed to take advantage of the particular characteristics of the 
computer on which it is to run. 
The three main architectural elements are memory, central processing 
units, and communication paths. A controlling unit oversees how these el-
ements work together. 
There are various ways memory can be organized. There is usually a hier-
archy of types of memory with diﬀerent speeds of access. The various levels 
can also be organized into banks with separate communication links to the 
processing units. 
The processing units can be constructed and organized in various ways. A 
single processor can be “vectorized,” so that it can perform the same operation 
on all elements of two vectors at the same time. Vectorized processing is very 
important in numerical linear algebra, because so many of the computations 
naturally are performed on vectors. An EDP processor (see page 561) would  
also be very useful. 
There may be multiple central processing units. The units may consist of 
multiple cores within the same processor. The processing units may include 
vector processors. 
If more than one processing unit is available, it may be possible to perform 
diﬀerent kinds of operations simultaneously. In this case, the amount of time 
required may be drastically smaller for an eﬃcient parallel algorithm than it 
would for the most eﬃcient serial algorithm that utilizes only one processor at 
a time. An analysis of the eﬃciency must take into consideration how many 
processors are available, how many computations can be performed in parallel, 
and how often they can be performed in parallel. 
Computations in Parallel 
The most eﬀective way of decreasing the time required for solving a computa-
tional problem is to perform the computations in parallel if possible. There are

576
10 Numerical Methods
some computations that are essentially serial, but in almost any problem there 
are subtasks that are independent of each other and can be performed in any 
order. Parallel computing remains an important research area. In an increas-
ing number of problems in the data sciences, distributed computing, which 
can be considered an extreme form of parallel computing, is used because the 
data for the problems reside on diﬀerent servers. 
10.4.3 Iterations and Convergence 
Many numerical algorithms are iterative; that is, groups of computations form 
successive approximations to the desired solution. In a program this usually 
means a loop through a common set of instructions in which each pass through 
the loop changes the initial values of operands in the instructions. 
We will generally use the notation x Superscript left parenthesis k right parenthesisx(k) to refer to the computed value of 
x at the k normal t normal hkth iteration. 
An iterative algorithm terminates when some convergence criterion or 
stopping criterion is satisﬁed. An example is to declare that an algorithm 
has converged when 
upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesis less than or equals epsilon commaΔ(x(k), x(k−1)) ≤e,
where upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesisΔ(x(k), x(k−1)) is some measure of the diﬀerence of x Superscript left parenthesis k right parenthesisx(k) and x Superscript left parenthesis k minus 1 right parenthesisx(k−1) and 
epsilone is a small positive number. Because x may not be a single number, we must 
consider general measures of the diﬀerence of x Superscript left parenthesis k right parenthesisx(k) and x Superscript left parenthesis k minus 1 right parenthesisx(k−1). For example, if 
x is a vector, the measure may be some metric, such as we discuss in Chap. 2. 
In that case, upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x Superscript left parenthesis k minus 1 right parenthesis Baseline right parenthesisΔ(x(k), x(k−1)) may be denoted by parallel to x Superscript left parenthesis k right parenthesis Baseline minus x Superscript left parenthesis k minus 1 right parenthesis Baseline parallel to||x(k) −x(k−1)||. 
An iterative algorithm may have more than one stopping criterion. Often, 
a maximum number of iterations is set so that the algorithm will be sure to 
terminate whether it converges or not. (Some people deﬁne the term “algo-
rithm” to refer only to methods that converge. Under this deﬁnition, whether 
or not a method is an “algorithm” may depend on the input data unless a 
stopping rule based on something independent of the data, such as the num-
ber of iterations, is applied. In any event, it is always a good idea, in addition 
to stopping criteria based on convergence of the solution, to have a stopping 
criterion that is independent of convergence and that limits the number of 
operations.) 
The convergence ratio of the sequence x Superscript left parenthesis k right parenthesisx(k) to a constant x 0x0 is 
limit Underscript k right arrow normal infinity Endscripts StartFraction upper Delta left parenthesis x Superscript left parenthesis k plus 1 right parenthesis Baseline comma x 0 right parenthesis Over upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x 0 right parenthesis EndFraction lim
k→∞
Δ(x(k+1), x0)
Δ(x(k), x0)
if this limit exists. If the convergence ratio is greater than 0 and less than 
1, the sequence is said to converge linearly. If the convergence ratio is 0, the 
sequence is said to converge superlinearly. 
Other measures of the rate of convergence are based on 
limit Underscript k right arrow normal infinity Endscripts StartFraction upper Delta left parenthesis x Superscript left parenthesis k plus 1 right parenthesis Baseline comma x 0 right parenthesis Over left parenthesis upper Delta left parenthesis x Superscript left parenthesis k right parenthesis Baseline comma x 0 right parenthesis right parenthesis Superscript r Baseline EndFraction equals c lim
k→∞
Δ(x(k+1), x0)
(Δ(x(k), x0))r = c
(10.10)

10.4 Numerical Algorithms and Analysis
577
(again, assuming the limit exists; i.e., c less than normal infinityc < ∞). In Eq. (10.10), the exponent r 
is called the rate of convergence, and the limit c is called the rate constant. 
If r equals 2r = 2 (and c is ﬁnite), the sequence is said to converge quadratically. It is  
clear that for any r greater than 1r > 1 (and ﬁnite c), the convergence is superlinear. 
Convergence deﬁned in terms of Eq. (10.10) is sometimes referred to as 
“Q-convergence” because the criterion is a quotient. Types of convergence 
may then be referred to as “Q-linear,” “Q-quadratic,” and so on. 
The convergence rate is often a function of k, say  h left parenthesis k right parenthesish(k). The convergence 
is then expressed as an order in k, normal upper O left parenthesis h left parenthesis k right parenthesis right parenthesisO(h(k)). 
Extrapolation 
As we have noted, many numerical computations are performed on a discrete 
set that approximates the reals or normal upper I normal upper R Superscript dIRd, resulting in discretization errors. By  
“discretization error,” we do not mean a rounding error resulting from the 
computer’s ﬁnite representation of numbers. The discrete set used in com-
puting some quantity such as an integral is often a grid. If h is the interval 
width of the grid, the computations may have errors that can be expressed 
as a function of h. For example, if the true value is x and, because of the 
discretization, the exact value that would be computed is x Subscript hxh, then we can  
write 
x equals x Subscript h Baseline plus e left parenthesis h right parenthesis periodx = xh + e(h).
For a given algorithm, suppose the error e left parenthesis h right parenthesise(h) is proportional to some power 
of h, say  h Superscript nhn, and so we can write 
x equals x Subscript h Baseline plus c h Superscript nx = xh + chn
(10.11) 
for some constant c. Now, suppose we use a diﬀerent discretization, with 
interval length rh having 0 less than r less than 10 < r < 1. We have  
x equals x Subscript r h Baseline plus c left parenthesis r h right parenthesis Superscript nx = xrh + c(rh)n
and, after subtracting from Eq. (10.11), 
0 equals x Subscript h Baseline minus x Subscript r h Baseline plus c left parenthesis h Superscript n Baseline minus left parenthesis r h right parenthesis Superscript n Baseline right parenthesis0 = xh −xrh + c(hn −(rh)n)
or 
c h Superscript n Baseline equals StartFraction left parenthesis x Subscript h Baseline minus x Subscript r h Baseline right parenthesis Over r Superscript n Baseline minus 1 EndFraction periodchn = (xh −xrh)
rn −1
.
(10.12) 
This analysis relies on the assumption that the error in the discrete algo-
rithm is proportional to h Superscript nhn. Under this assumption, c h Superscript nchn in Eq. (10.12) is  
the discretization error in computing x, using exact computations, and is an 
estimate of the error due to discretization in actual computations. A more 
realistic regularity assumption is that the error is normal upper O left parenthesis h Superscript n Baseline right parenthesisO(hn) as h right arrow 0h →0; that is,  
instead of (10.11), we have

578
10 Numerical Methods
x equals x Subscript h Baseline plus c h Superscript n Baseline plus normal upper O left parenthesis h Superscript n plus alpha Baseline right parenthesisx = xh + chn + O(hn+α)
for alpha greater than 0α > 0. 
Whenever this regularity assumption is satisﬁed, Eq. (10.12) provides us 
with an inexpensive improved estimate of x: 
x Subscript upper R Baseline equals StartFraction x Subscript r h Baseline minus r Superscript n Baseline x Subscript h Baseline Over 1 minus r Superscript n Baseline EndFraction periodxR = xrh −rnxh
1 −rn
.
(10.13) 
It is easy to see that StartAbsoluteValue x minus x Subscript upper R Baseline EndAbsoluteValue|x −xR| is less than the absolute error using an interval 
size of either h or rh. 
The process described above is called Richardson extrapolation, and  the  
value in Eq. (10.13) is called the Richardson extrapolation estimate. Richard-
son extrapolation is also called “Richardson’s deferred approach to the limit.” 
It has general applications in numerical analysis, but is most widely used in 
numerical quadrature. Bickel and Yahav (1988) use Richardson extrapolation 
to reduce the computations in a bootstrap. Extrapolation can be extended 
beyond just one step, as in the presentation above. 
Reducing the computational burden by using extrapolation is very impor-
tant in higher dimensions. In many cases, for example, in direct extensions 
of quadrature rules, the computational burden grows exponentially with the 
number of dimensions. This is sometimes called “the curse of dimensionality” 
and can render a fairly straightforward problem in one or two dimensions 
unsolvable in higher dimensions. 
A direct extension of Richardson extrapolation in higher dimensions would 
involve extrapolation in each direction, with an exponential increase in the 
amount of computation. An approach that is particularly appealing in higher 
dimensions is splitting extrapolation, which avoids independent extrapolations 
in all directions. 
10.4.4 Computations Without Storing Data 
For computations involving large sets of data, it is desirable to have algorithms 
that sequentially use a single data record, update some cumulative data, and 
then discard the data record. Such an algorithm is called a real-time algorithm, 
and operation of such an algorithm is called online processing. An algorithm 
that has all of the data available throughout the computations is called a batch 
algorithm. 
An algorithm that generally processes data sequentially in a similar man-
ner as a real-time algorithm but may have subsequent access to the same data 
is called an online algorithm or an “out-of-core” algorithm. (This latter name 
derives from the erstwhile use of “core” to refer to computer memory.) Any 
real-time algorithm is an online or out-of-core algorithm, but an online or 
out-of-core algorithm may make more than one pass through the data. (Some 
people restrict “online” to mean “real-time” as we have deﬁned it above.)

10.4 Numerical Algorithms and Analysis
579
If the quantity t is to be computed from the data x 1 comma x 2 comma ellipsis comma x Subscript n Baselinex1, x2, . . . , xn, a real-time  
algorithm begins with a quantity t Superscript left parenthesis 0 right parenthesist(0) and from t Superscript left parenthesis 0 right parenthesist(0) and x 1x1 computes t Superscript left parenthesis 1 right parenthesist(1). The  
algorithm proceeds to compute t Superscript left parenthesis 2 right parenthesist(2) using x 2x2 and so on, never retaining more 
than just the current value, t Superscript left parenthesis k right parenthesist(k). The quantities t Superscript left parenthesis k right parenthesist(k) may of course consist of 
multiple elements, but the point is that the number of elements in each t Superscript left parenthesis k right parenthesist(k)
is independent of n. 
Many summary statistics can be computed in online or real-time pro-
cesses. For example, the algorithms discussed beginning on page 569 for com-
puting the sample sum of squares are real-time algorithms. The algorithm 
in Eqs. (10.6) requires two passes through the data so it is not a real-time 
algorithm, although it is out-of-core in each pass. There are stable online al-
gorithms for other similar statistics, such as the sample variance-covariance 
matrix. The least squares linear regression estimates can also be computed by 
a stable one-pass algorithm that, incidentally, does not involve computation 
of the variance-covariance matrix (or the sums of squares and cross products 
matrix). There is no real-time algorithm for ﬁnding the median. The number 
of data records that must be retained and reexamined depends on n. 
It is interesting to note that any suﬃcient statistic can be computed by 
an out-of-core algorithm. 
In addition to the reduced storage burden, a real-time algorithm allows 
a statistic computed from one sample to be updated using data from a new 
sample. A real-time algorithm is necessarily normal upper O left parenthesis n right parenthesisO(n). 
10.4.5 Other Computational Techniques 
In addition to techniques to improve the eﬃciency and the accuracy of com-
putations, there are also special methods that relate to the way we build 
programs or store and access data. 
Recursion 
The algorithms for many computations perform some operation, update the 
operands, and perform the operation again: 
1. perform operation 
2. test for exit 
3. update operands 
4. go to 1 
If we give this algorithm the name doit and represent its operands by x, we  
could write the algorithm as 
Algorithm doit(x) 
1. operate on x 
2. test for exit 
3. update x: x primex'
4. doit(x primex')

580
10 Numerical Methods
The algorithm for computing the mean and the sum of squares (10.8) on  
page 569 can be derived as a recursion. Suppose we have the mean a Subscript kak and 
the sum of squares s Subscript ksk for k elements x 1 comma x 2 comma ellipsis comma x Subscript k Baselinex1, x2, . . . , xk, and we have a new value 
x Subscript k plus 1xk+1 and wish to compute a Subscript k plus 1ak+1 and s Subscript k plus 1sk+1. The obvious solution is 
a Subscript k plus 1 Baseline equals a Subscript k Baseline plus StartFraction x Subscript k plus 1 Baseline minus a Subscript k Baseline Over k plus 1 EndFractionak+1 = ak + xk+1 −ak
k + 1
and 
s Subscript k plus 1 Baseline equals s Subscript k Baseline plus StartFraction k left parenthesis x Subscript k plus 1 Baseline minus a Subscript k Baseline right parenthesis squared Over k plus 1 EndFraction periodsk+1 = sk + k(xk+1 −ak)2
k + 1
.
These are the same computations as in Eqs. (10.8) on page 569. 
Another example of how viewing the problem as an update problem can 
result in an eﬃcient algorithm is in the evaluation of a polynomial of degree 
d, 
p Subscript d Baseline left parenthesis x right parenthesis equals c Subscript d Baseline x Superscript d Baseline plus c Subscript d minus 1 Baseline x Superscript d minus 1 Baseline plus midline horizontal ellipsis plus c 1 x plus c 0 periodpd(x) = cdxd + cd−1xd−1 + · · · + c1x + c0.
Doing this in a naive way would require d minus 1d−1 multiplications to get the powers 
of x, d additional multiplications for the coeﬃcients, and d additions. If we 
write the polynomial as 
p Subscript d Baseline left parenthesis x right parenthesis equals x left parenthesis c Subscript d Baseline x Superscript d minus 1 Baseline plus c Subscript d minus 1 Baseline x Superscript d minus 2 Baseline plus midline horizontal ellipsis plus c 1 right parenthesis plus c 0 commapd(x) = x(cdxd−1 + cd−1xd−2 + · · · + c1) + c0,
we see a polynomial of degree d minus 1d−1 from which our polynomial of degree d can 
be obtained with but one multiplication and one addition; that is, the number 
of multiplications is equal to the increase in the degree—not two times the 
increase in the degree. Generalizing, we have 
p Subscript d Baseline left parenthesis x right parenthesis equals x left parenthesis midline horizontal ellipsis x left parenthesis x left parenthesis c Subscript d Baseline x plus c Subscript d minus 1 Baseline right parenthesis plus midline horizontal ellipsis right parenthesis plus c 1 right parenthesis plus c 0 commapd(x) = x(· · · x(x(cdx + cd−1) + · · · ) + c1) + c0,
(10.14) 
which has a total of d multiplications and d additions. The method for eval-
uating polynomials in Eq. (10.14) is called Horner’s method. 
A computer subprogram that implements recursion invokes itself. Not only 
must the programmer be careful in writing the recursive subprogram, but the 
programming system must maintain call tables and other data properly to 
allow for recursion. Once a programmer begins to understand recursion, there 
may be a tendency to overuse it. To compute a factorial, for example, the 
inexperienced C programmer may write 
float Factorial(int n) 
{ 
if(n==0) 
return 1; 
else 
return n*Factorial(n-1); 
}

10.4 Numerical Algorithms and Analysis
581
The problem is that this is implemented by storing a stack of statements. 
Because n may be relatively large, the stack may become quite large and 
ineﬃcient. It is just as easy to write the function as a simple loop, and it 
would be a much better piece of code. 
Fortran, C, R, and Python all allow for recursion. 
MapReduce 
In very-large-scale computational problems, the data may be stored in dif-
ferent locations and in diﬀerent formats. In order to process the data in any 
systematic way, we need to scale out the processing system. To do this, we ﬁrst 
need to map it into some common structure and then combine the individual 
pieces. One standard way of doing this is called MapReduce, because of these 
two separate types of operations. 
In an application of MapReduce, it is ﬁrst assumed that the problem con-
sists of, or can be divided into, separate parts. The mapping phase of MapRe-
duce operates on individual parts of the problem, and within each part of the 
problem, it assigns identifying keys to the separate values. The result of this 
phase is a collection of sets of key-value pairs. 
The next step is to “shuﬄe” the elements in the sets of key-value pairs to 
form a new collection of sets each of which has a single key. These individ-
ual sets are then “reduced,” that is, the actual computations are performed. 
Finally, the individual computed results are combined. 
Here we will consider a simpler example, so that the individual steps are 
clear. Consider the problem of counting how many diﬀerent numbers there are 
in a given matrix. In Fig. 10.13, we show a  4 times 44 × 4 matrix with three diﬀerent 
elements, negative 1−1, 0, and 1. The problem is to determine how many elements of 
each value are in the matrix. In the MapReduce method, each key-value pair 
is an element count. 
The implementation of this procedure in a distributed computing environ-
ment would entail consideration of many details about the component com-
puting environments and the interaction with the ﬁle system. As mentioned 
earlier, the Hadoop Distributed File System (HDFS) is designed for this kind 
of process. 
This kind of problem does not arise often in numerical computations, but 
it illustrates a scalable approach that could be used in a similar problem in 
which the matrix is composed of elements distributed over multiple computer 
systems. 
Our purpose here is only to get an overview of the big picture, not to 
discuss the details of the implementation. We will consider the MapReduce 
method in the speciﬁc context of matrix multiplication on page 601.

582
10 Numerical Methods
Figure 10.13. MapReduce to count the number of diﬀerent elements in a matrix 
Appendix: Numerical Computations in R 
R provides for special numbers and expressions such as normal infinity∞, negative normal infinity−∞, and  0 divided by 00/0 in 
ways that are consistent with our expectations. An indeterminate numeric 
expression such as 0 divided by 00/0 is not a number and obviously cannot participate in 
arithmetic operations, but it cannot be compared with other values or even 
with itself. R provides for a special constant, NaN to denote a quantity similar 
to a numeric value, but which is not a number. 
R also provides for a special missing value constant, NA (“not applicable”). 
An object with a value NA, similar to an object with a value NaN, cannot 
participate in arithmetic operations, but it cannot be compared with other 
values or even with itself. R provides two functions is.nan and is.na to 
test for NaN or NA. A  NaN is considered to be a NA (since it is “missing”), 
but a NA may not be a NaN since it may have no relation to numbers. (See 
Exercise 10.24.) 
As mentioned previously, in R, the numerical characteristics of a given 
system are stored in the variable .Machine. Other R objects that provide 
information on a computer’s characteristics are the variable .Platform and 
the function capabilities. 
Multiple Precision in R 
There are R packages, gmp and Rmpfr, that interface with the GMP C and 
C++ library for multiple-precision computations. The gmp package in R de-
ﬁnes the bigz class, in which larger integers can be stored and operated on. 
The gmp package facilitates use of big integers in operations involving rational 
fractions (see page 559). 
The R package Rmpfr by Martin M¨achler provides the mpfr class, which 
allows assignment of and operations on ﬂoating-point numbers of arbitrary

Exercises
583
precision up to a limit that is platform dependent. The basic function to 
assign a value to a variable is mpfr, in which the value is speciﬁed, followed 
by an argument specifying the precision in bits. 
> pimpf <- mpfr(pi, prec=50*log2(10)) # pi is builtin 
constant 
> pimpf 
1 ’mpfr’ number of precision 166 bits 
[1] 3.141592653589793115997963468544185161590576171875 
Most R functions use multiple-precision arithmetic when the arguments 
are of class mpfr (see Exercise 10.27). 
Timing in R 
R provides access to the computer clock to time computational procedures. 
The function proc.time determines how much real and CPU time (in seconds) 
the currently running R process has used. The function (with no arguments) 
returns the “user” time, the “system” time, and the “elapsed” time since 
the R program began execution; hence, to determine the time that a given 
set of computations used, proc.time must be invoked prior to and after the 
computations are performed, and the diﬀerence in the times determined. 
> startt <- proc.time() 
... computations ... 
> endt <- proc.time() 
> endt - startt 
user system elapsed 
0.12 0.00 0.12 
The system.time function determines the time used in evaluating an R 
expression. It invokes proc.time and performs the subtractions. 
Exercises 
10.1. An important attitude in the computational sciences is that the com-
puter is to be used as a tool for exploration and discovery. The com-
puter should be used to check out “hunches” or conjectures, which then 
later should be subjected to analysis in the traditional manner. There 
are limits to this approach, however. An example is in limiting pro-
cesses. Because the computer deals with ﬁnite quantities, the results 
of a computation may be misleading. Explore each of the situations 
below using C or Fortran. A few minutes or even seconds of computing 
should be enough to give you a feel for the nature of the computations.

584
10 Numerical Methods
In these exercises, you may write computer programs in which you per-
form tests for equality. A word of warning is in order about such tests. 
If a test involving a quantity x is executed soon after the computation 
of x, the test may be invalid within the set of ﬂoating-point numbers 
with which the computer nominally works. This is because the test 
may be performed using the extended precision of the computational 
registers. 
a) Consider the question of the convergence of the series 
sigma summation Underscript i equals 1 Overscript normal infinity Endscripts i period
∞
E
i=1
i.
Obviously, this series does not converge in IR. Suppose, however, 
that we begin summing this series using ﬂoating-point numbers. 
Will the computations overﬂow? If so, at what value of i (approx-
imately)? Or will the series converge in IF? If so, to what value 
and at what value of i (approximately)? In either case, state your 
answer in terms of the standard parameters of the ﬂoating-point 
model, b, p, emin, and  emax (page 536). 
b) Consider the question of the convergence of the series 
sigma summation Underscript i equals 1 Overscript normal infinity Endscripts 2 Superscript minus 2 i
∞
E
i=1
2−2i
and answer the same questions as in Exercise 10.1a. 
c) Consider the question of the convergence of the series 
sigma summation Underscript i equals 1 Overscript normal infinity Endscripts StartFraction 1 Over i EndFraction
∞
E
i=1
1
i
and answer the same questions as in Exercise 10.1a. 
d) Consider the question of the convergence of the series 
sigma summation Underscript i equals 1 Overscript normal infinity Endscripts StartFraction 1 Over i Superscript x Baseline EndFraction comma
∞
E
i=1
1
ix ,
for x ≥ 1. Answer the same questions as in Exercise 10.1a, except 
address the variable x. 
10.2. We know, of course, that the harmonic series in Exercise 10.1c does 
not converge (although the naive program to compute it does). It is, 
in fact, true that 
StartLayout 1st Row 1st Column upper H Subscript n 2nd Column equals 3rd Column sigma summation Underscript i equals 1 Overscript n Endscripts StartFraction 1 Over i EndFraction 2nd Row 1st Column Blank 2nd Column equals 3rd Column f left parenthesis n right parenthesis plus gamma plus normal o left parenthesis 1 right parenthesis comma EndLayoutHn =
n
E
i=1
1
i
= f(n) + γ + o(1),

Exercises
585
where f is an increasing function and γ is Euler’s constant. 
For various n, compute Hn. Determine a function f that provides a 
good ﬁt and obtain an approximation of Euler’s constant. 
10.3. Machine characteristics. 
a) Write a program to determine the smallest and largest relative 
spacings. Use it to determine them on the machine you are using. 
b) Write a program to determine whether your computer system im-
plements gradual underﬂow. 
c) Write a program to determine the bit patterns of +∞, −∞, and  
NaN on a computer that implements the IEEE Binary Standard. 
(This may be more diﬃcult than it seems.) 
d) 
i. Obtain the program MACHAR (Cody 1988) and use it to de-
termine the machine epsilon and the smallest positive ﬂoating-
point number on the computer you are using. (MACHAR is 
included in CALGO, which is available from netlib.) 
ii. Alternatively, or in addition, determine these values on the 
computer you are using by use of the Fortran intrinsic func-
tions, such as EPSILON and TINY. 
10.4. Write a program in Fortran or C to determine the bit patterns of ﬁxed-
point numbers, ﬂoating-point numbers, and character strings. Run your 
program on diﬀerent computers, and compare your results with those 
shown in Figs. 10.1, 10.2, 10.3 and Figs. 10.11 and 10.12. 
10.5. What is the numerical value of the rounding unit ( 1 
2 ulp) in the IEEE 
Standard 754 double precision? 
10.6. Consider the standard model (10.1) for the ﬂoating-point representa-
tion, 
plus or minus 0 period d 1 d 2 midline horizontal ellipsis d Subscript p Baseline times b Superscript e Baseline comma± 0.d1d2 · · · dp × be,
with emin ≤ e ≤ emax. Your answers to the following questions may 
depend on an additional assumption or two. Either choice of (standard) 
assumptions is acceptable. 
a) How many ﬂoating-point numbers are there? 
b) What is the smallest positive number? 
c) What is the smallest number larger than 1? 
d) What is the smallest number X such that X + 1  =  X? 
e) Suppose p = 4  and  b = 2  (and  emin is very small and emax is very 
large). What is the next number after 20 in this number system? 
10.7. a) Deﬁne parameters of a ﬂoating-point model so that the number 
of numbers in the system is less than the largest number in the 
system. 
b) Deﬁne parameters of a ﬂoating-point model so that the number of 
numbers in the system is greater than the largest number in the 
system. 
10.8. Suppose that a certain computer represents ﬂoating-point numbers in 
base 10 using eight decimal places for the mantissa, two decimal places

586
10 Numerical Methods
for the exponent, one decimal place for the sign of the exponent, and 
one decimal place for the sign of the number. 
a) What are the “smallest relative spacing” and the “largest relative 
spacing?” (Your answer may depend on certain additional assump-
tions about the representation; state any assumptions.) 
b) What is the largest number g such that 417 + g = 417? 
c) Discuss the associativity of addition using numbers represented in 
this system. Give an example of three numbers, a, b, and  c, such  
that using this representation (a + b) +  c /= a + (b + c) unless 
the operations are chained. Then show how chaining could make 
associativity hold for some more numbers but still not hold for 
others. 
d) Compare the maximum rounding error in the computation x+x+ 
x + x with that in 4 ∗ x. (Again, you may wish to mention the 
possibilities of chaining operations.) 
10.9. Consider the same ﬂoating-point system as in Exercise 10.8. 
a) Let X be a random variable uniformly distributed over the interval 
left bracket 1 minus .000001 comma 1 plus .000001 right bracket period[1 −.000001, 1 + .000001].
Develop a probability model for the representation [X]c. (This is a 
discrete random variable with 111 mass points.) 
b) Let X and Y be random variables uniformly distributed over the 
same interval as above. Develop a probability model for the rep-
resentation [X + Y ]c. (This is a discrete random variable with 41 
mass points.) 
c) Develop a probability model for [X]c [+]c [Y ]c. (This is also a 
discrete random variable with 41 mass points.) 
10.10. Give an example to show that the sum of three ﬂoating-point numbers 
can have a very large relative error. 
10.11. Write a single program in Fortran or C to compute the following 
a) 
sigma summation Underscript i equals 0 Overscript 5 Endscripts StartBinomialOrMatrix 10 Choose i EndBinomialOrMatrix 0.25 Superscript i Baseline 0.75 Superscript 20 minus i Baseline period
5
E
i=0
(
10
i
)
0.25i0.7520−i.
b) 
sigma summation Underscript i equals 0 Overscript 10 Endscripts StartBinomialOrMatrix 20 Choose i EndBinomialOrMatrix 0.25 Superscript i Baseline 0.75 Superscript 20 minus i Baseline period
10
E
i=0
( 20
i
)
0.25i0.7520−i.
c) 
sigma summation Underscript i equals 0 Overscript 50 Endscripts StartBinomialOrMatrix 100 Choose i EndBinomialOrMatrix 0.25 Superscript i Baseline 0.75 Superscript 20 minus i Baseline period
50
E
i=0
(100
i
)
0.25i0.7520−i.

Exercises
587
10.12. In standard mathematical libraries, there are functions for log(x) and  
exp(x) called log and exp, respectively. There is a function in the IMSL 
Libraries to evaluate log(1 + x) and  one to evaluate (exp(x) − 1)/x. 
(The names in Fortran for single precision are alnrel and exprl.) 
a) Explain why the designers of the libraries included those functions, 
even though log and exp are available. 
b) Give an example in which the standard log loses precision. Evaluate 
it using log in the standard math library of Fortran or C. Now 
evaluate it using a Taylor series expansion of log(1 + x). 
10.13. Suppose you have a program to compute the cumulative distribution 
function for the chi-squared distribution. The input for the program 
is x and df, and the output is Pr(X ≤ x). Suppose you are interested 
in probabilities in the extreme upper range and high accuracy is very 
important. What is wrong with the design of the program for this 
problem? What kind of program would be better? 
10.14. Write a program in Fortran or C to compute e−12 using a Taylor series 
directly, and then compute e−12 as the reciprocal of e12 , which  is  also  
computed using a Taylor series. Discuss the reasons for the diﬀerences 
in the results. To what extent is truncation error a problem? 
10.15. Errors in computations. 
a) Explain the diﬀerence in truncation and cancellation. 
b) Why is cancellation not a problem in multiplication? 
10.16. Assume we have a computer system that can maintain seven digits of 
precision. Evaluate the sum of squares for the dataset {9000, 9001, 9002}. 
a) Use the algorithm in Eqs. (10.6) on page 568. 
b) Use the algorithm in Eqs. (10.7) on page 569. 
c) Now assume there is one guard digit. Would the answers change? 
10.17. Develop algorithms similar to Eqs. (10.8) on page 569 to evaluate the 
following: 
a) The weighted sum of squares 
sigma summation Underscript i equals 1 Overscript n Endscripts w Subscript i Baseline left parenthesis x Subscript i Baseline minus x overbar right parenthesis squared period
n
E
i=1
wi(xi −¯x)2.
b) The third central moment 
sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis x Subscript i Baseline minus x overbar right parenthesis cubed period
n
E
i=1
(xi −¯x)3.
c) The sum of cross products 
sigma summation Underscript i equals 1 Overscript n Endscripts left parenthesis x Subscript i Baseline minus x overbar right parenthesis left parenthesis y Subscript i Baseline minus y overbar right parenthesis period
n
E
i=1
(xi −¯x)(yi −¯y).

588
10 Numerical Methods
Hint: Look at the diﬀerence in partial sums, 
sigma summation Underscript i equals 1 Overscript j Endscripts left parenthesis dot right parenthesis minus sigma summation Underscript i equals 1 Overscript j minus 1 Endscripts left parenthesis dot right parenthesis period
j
E
i=1
(·) −
j−1
E
i=1
(·).
10.18. Given the recurrence relation 
t left parenthesis n right parenthesis equals p t left parenthesis n divided by p right parenthesis plus c nt(n) = pt(n/p) + cn
for p positive and c nonnegative, show that t(n) is O(n log n). 
Hint: First assume n is a power of p. 
10.19. In statistical data analysis, it is common to have some missing data. 
This may be because of nonresponse in a survey questionnaire or be-
cause an experimental or observational unit dies or discontinues partic-
ipation in the study. When the data are recorded, some form of missing-
data indicator must be used. Discuss the use of NaN as a missing-value 
indicator. What are some of its advantages and disadvantages? 
10.20. Consider the four properties of a dot product listed on page 33. For  
each one, state whether the property holds in computer arithmetic. 
Give examples to support your answers. 
10.21. Assuming the model (10.1) on page 536 for the ﬂoating-point number 
system, give an example of a nonsingular 2 × 2 matrix that is algorith-
mically singular. 
10.22. A Monte Carlo study of condition number and size of the matrix. 
For n = 5, 10, . . . , 30, generate 100 n×n matrices whose elements have 
independent N(0, 1) distributions. For each, compute the L2 condition 
number and plot the mean condition number versus the size of the ma-
trix. At each point, plot error bars representing the sample “standard 
error” (the standard deviation of the sample mean at that point). How 
would you describe the relationship between the condition number and 
the size? 
In any such Monte Carlo study we must consider the extent to which 
the random samples represent situations of interest. (How often do we 
have matrices whose elements have independent N(0, 1) distributions?) 
10.23. Solving an overdetermined system Xb = y, where  X is n × m. 
a) Count how many multiplications and additions are required to form 
XT X. (A multiplication or addition such as this is performed in 
ﬂoating point on a computer, so the operation is called a “ﬂop”. 
Sometimes a ﬂop is considered a combined operation of multipli-
cation and addition; at other times, each is considered a separate 
ﬂop. See page 572. The distinction is not important here; just count 
the total number.) 
b) Count how many ﬂops are required to form XT y. 
c) Count how many ﬂops are required to solve XT Xb = XT y using a 
Cholesky decomposition.

Exercises
589
d) Count how many ﬂops are required to form a QR decomposition 
of X using reﬂectors. 
e) Count how many ﬂops are required to form a QT y. 
f) Count how many ﬂops are required to solve R1b = c1 (Eq. (5.41), 
page 287). 
g) If n is large relative to m, what is the ratio of the total number 
of ﬂops required to form and solve the normal equations using the 
Cholesky method to the total number required to solve the sys-
tem using a QR decomposition? Why is the QR method generally 
preferred? 
R Exercises 
10.24. Special numbers in R. 
What are the results of evaluating the following expressions in R? 
a) 1/0,
-5(1/0), (1/0)(1/0), 1/0 
? =5(1/0) 
b) 0/0,
-5(0/0), 
0/0 
? =NaN,
x<-NaN, 
x 
? =NaN, 
is.nan(x), 
is.na(x) 
c) x<-NA, x 
? =NA, is.nan(x), is.na(x) 
10.25. Machine characteristics. 
a) From the R variable .Machine, determine the smallest and largest 
relative spacings, the machine epsilon, and the smallest positive 
ﬂoating-point number on the computer you are using. 
Compare the values with the ones you determined in Exercises 
10.3aa and  10.3dd. 
b) Using values obtained from .Machine, construct examples to illus-
trate the anomalies of computer arithmetic listed on page 527. 
i. Adding a positive number to a given positive number may not 
change that positive number. 
ii. Performing two multiplications in one order may yield a diﬀer-
ent result from doing the multiplications in the other order. 
iii. For any given number, there is a “next” number. 
iv. The next number after 1 is farther from 1 than the next number 
before 1. 
v. The next number after 1 is closer to 1 than the next number 
after  2 is to 2.  
10.26. Install and load the gmp package in R. Store 112000 and 351500 as bigz 
values and compute their product. How many digits are in the product? 
10.27. Install and load the Rmpfr package in R. Compute and print the exact 
value of 24! using factorial. 
First, by decomposing 24, determine how many digits 24! has. Also 
determine what are the lowest-order digits, so that you can make a 
quick visual check on your solution.

11 
Numerical Linear Algebra 
Many scientiﬁc computational problems in various areas of application involve 
vectors and matrices. Programming languages such as C provide the capabili-
ties for working with the individual elements but not directly with the arrays. 
Modern Fortran and higher-level languages such as Octave or MATLAB and 
R allow direct manipulation of objects that represent vectors and matrices. 
The vectors and matrices are arrays of ﬂoating-point numbers. 
The distinction between the set of real numbers, IR, and the set of ﬂoating-
point numbers, IF, that we use in the computer has important implications for 
numerical computations. As we discussed in Sect. 10.3, beginning on page 549, 
an element x of a vector or matrix is approximated by a computer number [x]c, 
and a mathematical operation ◦ is simulated by a computer operation [◦]c. 
The familiar laws of algebra for the ﬁeld of the reals do not hold in IF; see Ta-
ble 10.3 on page 557. This is especially true if uncontrolled parallel operations 
are allowed. These distinctions, of course, carry over to arrays of ﬂoating-point 
numbers that represent real numbers, and the properties of vectors and ma-
trices that we discussed in earlier chapters may not hold for their computer 
counterparts. For example, the dot product of a nonzero vector with itself is 
positive (see page 33), but <xc, xc>c = 0 does not imply xc = 0.  
11.1 Computer Storage of Vectors and Matrices 
The elements of vectors and matrices are represented as ordinary numeric 
data, as we described in Sect. 10.2, in either ﬁxed-point or ﬂoating-point rep-
resentation. 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 11 
591

592
11 Numerical Linear Algebra
11.1.1 Storage Modes 
The elements of vectors and matrices are generally stored in a logically con-
tiguous area of the computer’s memory. What is logically contiguous may not 
be physically contiguous, however. 
Accessing data from memory in a single pipeline may take more computer 
time than the computations themselves. For this reason, computer memory 
may be organized into separate modules, or banks, with separate paths to the 
central processing unit. Logical memory is interleaved through the banks; that 
is, two consecutive logical memory locations are in separate banks. In order 
to take maximum advantage of the computing power, it may be necessary to 
be aware of how many interleaved banks the computer system has. 
There are no convenient mappings of computer memory that would allow 
matrices to be stored in a logical rectangular grid, so matrices are usually 
stored either as columns strung end to end (a “column-major” storage) or as 
rows strung end to end (a “row-major” storage). In using a computer language 
or a software package, sometimes it is necessary to know which way the matrix 
is stored. The type of matrix computation to be performed may determine 
whether a vectorized processor should operate on rows or on columns. 
For some software to deal with matrices of varying sizes, the user must 
specify the length of one dimension of the array containing the matrix. (In 
general, the user must specify the lengths of all dimensions of the array except 
one.) In Fortran subroutines, it is common to have an argument specifying 
the leading dimension (number of rows), and in C functions it is common 
to have an argument specifying the column dimension. (See the examples in 
Fig. 12.1 on page 633 and Fig. 12.2 on page 634 for illustrations of the leading 
dimension argument.) 
11.1.2 Strides 
Sometimes in accessing a partition of a given matrix, the elements occur at 
ﬁxed distances from each other. If the storage is row-major for an n × m 
matrix, for example, the elements of a given column occur at a ﬁxed distance 
of m from each other. This distance is called the “stride,” and it is often more 
eﬃcient to access elements that occur with a ﬁxed stride than it is to access 
elements randomly scattered. 
Just accessing data from the computer’s memory contributes signiﬁcantly 
to the time it takes to perform computations. A stride that is not a multi-
ple of the number of banks in an interleaved bank memory organization can 
measurably increase the computational time in high-performance computing. 
11.1.3 Sparsity 
If a matrix has many elements that are zeros and if the positions of those 
zeros are easily identiﬁed, many operations on the matrix can be speeded up. 
Matrices with many zero elements are called sparse matrices. They occur of-
ten in certain types of problems, for example, in the solution of diﬀerential

11.2 General Computational Considerations
593
equations, in statistical designs of experiments, in graphical models, and in 
term-document studies. The ﬁrst consideration is how to represent the matrix 
and then how to store the matrix and the location information. Diﬀerent soft-
ware systems may use diﬀerent schemes to store sparse matrices, as described 
on page 618. The methods used in R are described on page 644. An important  
consideration in using sparse matrices is how to preserve the sparsity during 
intermediate computations. 
11.2 General Computational Considerations for Vectors 
and Matrices 
All of the computational methods discussed in Chap. 10 apply to vectors and 
matrices, but there are some additional general considerations for vectors and 
matrices. 
11.2.1 Relative Magnitudes of Operands 
One common situation that gives rise to numerical errors in computer opera-
tions is when a quantity x is transformed to t(x) but the value computed is 
unchanged: 
left bracket t left parenthesis x right parenthesis right bracket Subscript normal c Baseline equals left bracket x right bracket Subscript normal c Baseline semicolon[t(x)]c = [x]c;
(11.1) 
that is, the operation actually accomplishes nothing. A type of transformation 
that has this problem is 
t left parenthesis x right parenthesis equals x plus epsilon commat(x) = x + e,
(11.2) 
where |e| is much smaller than |x|. If all we wish to compute is x + e, the  
fact that [x + e]c = [x]c is probably not important. Usually, of course, this 
simple computation is part of some larger set of computations in which e was 
computed. This, therefore, is the situation we want to anticipate and avoid. 
Another type of problem is the addition to x of a computed quantity y 
that overwhelms x in magnitude. In this case, we may have 
left bracket x plus y right bracket Subscript normal c Baseline equals left bracket y right bracket Subscript normal c Baseline period[x + y]c = [y]c.
(11.3) 
Again, this is a situation we want to anticipate and avoid. 
Condition 
A measure of the worst-case numerical error in numerical computation in-
volving a given mathematical entity is the “condition” of that entity for the 
particular computations. The condition number of a matrix is the most gener-
ally useful such measure. For the matrix A, we denote the condition number 
as κ(A). We discussed the condition number in Sect. 5.1 and illustrated it 
in the toy example of Eq. (5.1). The condition number provides a bound on 
the relative norms of a “correct” solution to a linear system and a solution 
to a nearby problem. A speciﬁc condition number therefore depends on the

594
11 Numerical Linear Algebra
norm, and we deﬁned κ1, κ2, and  κ∞ condition numbers (and saw that they 
are generally roughly of the same magnitude). We saw in Eq. (5.10) that the  
L2 condition number, κ2(A), is the ratio of magnitudes of the two extreme 
eigenvalues of A. 
The condition of data depends on the particular computations to be per-
formed. The relative magnitudes of other eigenvalues (or singular values) may 
be more relevant for some types of computations. Also, we saw in Sect. 10.4.1 
that the “stiﬀness” measure in Eq. (10.4.1) is a more appropriate measure of 
the extent of the numerical error to be expected in computing variances. 
Pivoting 
Pivoting, discussed on page 270, is a method for avoiding a situation like that 
in Eq. (11.3). In Gaussian elimination, for example, we do an addition, x + y, 
where the y is the result of having divided some element of the matrix by 
some other element and x is some other element in the matrix. If the divisor 
is very small in magnitude, y is large and may overwhelm x as in Eq. (11.3). 
“Modiﬁed” and “Classical” Gram-Schmidt Transformations 
Another example of how to avoid a situation similar to that in Eq. (11.1) is  
the use of the correct form of the Gram-Schmidt transformations. 
The orthogonalizing transformations shown in Eqs. (2.55) on page 48 are 
the basis for Gram-Schmidt transformations of matrices. These transforma-
tions in turn are the basis for other computations, such as the QR factoriza-
tion. (Exercise 4.9 required you to apply Gram-Schmidt transformations to 
develop a QR factorization.) 
As mentioned on page 48, there are two ways we can extend Eqs. (2.55) to  
more than two vectors, and the method given in Algorithm 2.1 is the correct 
way to do it. At the kth stage of the Gram-Schmidt method, the vector x (k) 
k 
is taken as x (k−1) 
k
and the vectors x (k) 
k+1, x  (k) 
k+2, . . . , x  (k) 
m are all made orthog-
onal to x (k) 
k . After the ﬁrst stage, all vectors have been transformed. This 
method is sometimes called “modiﬁed Gram-Schmidt” because some people 
have performed the basic transformations in a diﬀerent way, so that at the 
kth iteration, starting at k = 2, the ﬁrst k −1 vectors are unchanged (i.e., 
x (k) 
i 
= x (k−1) 
i
for i = 1, 2, . . . , k −1), and x (k) 
k 
is made orthogonal to the k −1 
previously orthogonalized vectors x (k) 
1 , x  (k) 
2 , . . . , x  (k) 
k−1. This method is called 
“classical Gram-Schmidt” for no particular reason. The “classical” method 
is not as stable and should not be used. In this book, “Gram-Schmidt” is 
the same as what is sometimes called “modiﬁed Gram-Schmidt.” In Exer-
cise 11.1, you are asked to experiment with the relative numerical accuracy of 
the “classical Gram-Schmidt” and the correct Gram-Schmidt. The problems 
with the former method show up with the simple set of vectors x1 = (1, e, e), 
x2 = (1, e, 0), and x3 = (1, 0, e), with e small enough that

11.2 General Computational Considerations
595
left bracket 1 plus epsilon squared right bracket Subscript normal c Baseline equals 1 period[1 + e2]c = 1.
11.2.2 Iterative Methods 
As we saw in Chap. 5, we often have a choice between direct methods (that is, 
methods that compute a closed-form solution) and iterative methods. Iterative 
methods are usually to be favored for large, sparse systems. 
Iterative methods are based on a sequence of approximations that (it is 
hoped) converge to the correct solution. The fundamental trade-oﬀ in iter-
ative methods is between the amount of work expended in getting a good 
approximation at each step and the number of steps required for convergence. 
Preconditioning 
In order to achieve acceptable rates of convergence for iterative algorithms, it 
is often necessary to precondition the system, that is, to replace the system 
Ax = b by the system 
upper M Superscript negative 1 Baseline upper A x equals upper M Superscript negative 1 Baseline bM −1Ax = M −1b
for some suitable matrix M. As we indicated in Chaps. 5 and 6, the choice of 
M involves some art, and we will not consider any of the results further here. 
Restarting and Rescaling 
In many iterative methods, not all components of the computations are up-
dated in each iteration. An approximation to a given matrix or vector may be 
adequate during some sequence of computations without change, but then at 
some point the approximation is no longer close enough, and a new approxi-
mation must be computed. An example of this is in the use of quasi-Newton 
methods in optimization in which an approximate Hessian is updated, as in-
dicated in Eq. (9.111) on page 497. We may, for example, just compute an 
approximation to the Hessian every few iterations, perhaps using second dif-
ferences, and then use that approximate matrix for a few subsequent itera-
tions. 
Another example of the need to restart or to rescale is in the use of fast 
Givens rotations. As we mentioned on page 231 when we described the fast 
Givens rotations, the diagonal elements in the accumulated C matrices in 
the fast Givens rotations can become widely diﬀerent in absolute values, so 
to avoid excessive loss of accuracy, it is usually necessary to rescale the el-
ements periodically. Anda and Park (1994, 1996) describe methods of doing 
the rescaling dynamically. Their methods involve adjusting the ﬁrst diagonal 
element by multiplication by the square of the cosine and adjusting the second 
diagonal element by division by the square of the cosine. Bindel et al. (2002) 
discuss in detail techniques for performing Givens rotations eﬃciently while 
still maintaining accuracy. (The BLAS routines [see Sect. 12.2.1] rotmg and 
rotm, respectively, set up and apply fast Givens rotations.)

596
11 Numerical Linear Algebra
Preservation of Sparsity 
In computations involving large sparse systems, we may want to preserve the 
sparsity, even if that requires using approximations, as discussed in Sect. 4.8.2. 
Fill-in (when a zero position in a sparse matrix becomes nonzero) would cause 
loss of the computational and storage eﬃciencies of software for sparse matri-
ces. 
In forming a preconditioner for a sparse matrix A, for example, we may 
choose a matrix M = -L-U, where -L and -U are approximations to the matrices 
in an LU decomposition of A, as in Eq. (4.49). These matrices are constructed 
as indicated in Eq. (4.50) so as to have zeros everywhere A has, and A ≈
-L-U. This is called incomplete factorization, and often, instead of an exact 
factorization, an approximate factorization may be more useful because of 
computational eﬃciency. 
Iterative Reﬁnement 
Even if we are using a direct method, it may be useful to reﬁne the solution by 
one step computed in extended precision. A method for iterative reﬁnement 
of a solution of a linear system is given in Algorithm 5.3. 
11.2.3 Assessing Computational Errors 
As we discuss in Sect. 10.3.2 on page 551, we measure error by a scalar quan-
tity, either as absolute error, |˜r −r|, where  r is the true value and ˜r is the 
computed or rounded value, or as relative error, |˜r − r|/r (as long as r /= 0).  
The errors in vectors or matrices are generally expressed in terms of norms; 
for example, the relative error in the representation of the vector v, or as a  
result of computing v, may be expressed as ||˜v − v||/||v|| (as long as ||v|| /= 0),  
where ˜v is the computed vector. We often use the notation ˜v = v + δv, and  
so ||δv||/||v|| is the relative error. The choice of which vector norm to use may 
depend on practical considerations about the errors in the individual elements. 
The L∞ norm, for example, gives weight only to the element with the largest 
single error, while the L1 norm gives weights to all magnitudes equally. 
Assessing Errors in Given Computations 
In real-life applications, the correct solution is not known, but we would still 
like to have some way of assessing the accuracy using the data themselves. 
Sometimes a convenient way to do this in a given problem is to perform inter-
nal consistency tests. An internal consistency test may be an assessment of the 
agreement of various parts of the output. Relationships among the output are 
exploited to ensure that the individually computed quantities satisfy these re-
lationships. Other internal consistency tests may be performed by comparing 
the results of the solutions of two problems with a known relationship.

11.3 Multiplication of Vectors and Matrices
597
The solution to the linear system Ax = b has a simple relationship to 
the solution to the linear system Ax = b + caj, where  aj is the jth column 
of A and c is a constant. A useful check on the accuracy of a computed 
solution to Ax = b is to compare it with a computed solution to the modiﬁed 
system. Of course, if the expected relationship does not hold, we do not know 
which solution is incorrect, but it is probably not a good idea to trust either. 
To test the accuracy of the computed regression coeﬃcients for regressing 
y on x1, . . . , xm, they suggest comparing them to the computed regression 
coeﬃcients for regressing y + dxj on x1, . . . , xm. If the expected relationships 
do not obtain, the analyst has strong reason to doubt the accuracy of the 
computations. 
Another simple modiﬁcation of the problem of solving a linear system with 
a known exact eﬀect is the permutation of the rows or columns. Although this 
perturbation of the problem does not change the solution, it does sometimes 
result in a change in the computations, and hence, it may result in a diﬀerent 
computed solution. This obviously would alert the user to problems in the 
computations. 
A simple internal consistency test that is applicable to many problems is 
to use two levels of precision in some of the computations. In using this test, 
one must be careful to make sure that the input data are the same. Rounding 
of the input data may cause incorrect output to result, but that is not the 
fault of the computational algorithm. 
Internal consistency tests cannot conﬁrm that the results are correct; they 
can only give an indication that the results are incorrect. 
11.3 Multiplication of Vectors and Matrices 
Arithmetic on vectors and matrices involves arithmetic on the individual el-
ements. The arithmetic on the individual elements is performed as we have 
discussed in Sect. 10.3. 
The way the storage of the individual elements is organized is very impor-
tant for the eﬃciency of computations. Also, the way the computer memory is 
organized and the nature of the numerical processors aﬀect the eﬃciency and 
may be an important consideration in the design of algorithms for working 
with vectors and matrices. 
The best methods for performing operations on vectors and matrices in 
the computer may not be the methods that are suggested by the deﬁnitions 
of the operations. 
In most numerical computations with vectors and matrices, there is more 
than one way of performing the operations on the scalar elements. Consider 
the problem of evaluating the matrix times vector product, c = Ab, where  A 
is n × m. There are two obvious ways of doing this:

598
11 Numerical Linear Algebra
• Compute each of the n elements of c, one at a time, as an inner product 
of m-vectors, ci = aT 
i b = E
j aijbj 
• Update the computation of all of the elements of c simultaneously as fol-
lows: 
1. For i = 1, . . . , n, let  c (0) 
i 
= 0.  
2. For j = 1, . . . , m, 
{ 
for i = 1, . . . , n, 
{ 
let c (i) 
i = c (i−1) 
i
+ aijbj. 
} 
} 
If there are p processors available for parallel processing, we could use a fan-in 
algorithm (see page 553) to evaluate  Ax as a set of inner products: 
StartLayout 1st Row 1st Column c 1 Superscript left parenthesis 1 right parenthesis Baseline equals 2nd Column c 2 Superscript left parenthesis 1 right parenthesis Baseline equals 3rd Column ellipsis 4th Column c Subscript 2 m minus 1 Superscript left parenthesis 1 right parenthesis Baseline equals 5th Column c Subscript 2 m Superscript left parenthesis 1 right parenthesis Baseline equals 6th Column ellipsis 2nd Row 1st Column a Subscript i Baseline 1 Baseline b 1 plus a Subscript i Baseline 2 Baseline b 2 2nd Column a Subscript i Baseline 3 Baseline b 3 plus a Subscript i Baseline 4 Baseline b 4 3rd Column ellipsis 4th Column a Subscript i comma 4 m minus 3 Baseline b Subscript 4 m minus 3 plus a Subscript i comma 4 m minus 2 Baseline b Subscript 4 m minus 2 5th Column ellipsis 6th Column ellipsis 3rd Row 1st Column down right arrow 2nd Column down left arrow 3rd Column ellipsis 4th Column down right arrow 5th Column down left arrow 6th Column ellipsis 4th Row 1st Column c 1 Superscript left parenthesis 2 right parenthesis Baseline equals 2nd Column ellipsis 3rd Column c Subscript m Superscript left parenthesis 2 right parenthesis Baseline equals 4th Column ellipsis 5th Row 1st Column c 1 Superscript left parenthesis 1 right parenthesis Baseline plus c 2 Superscript left parenthesis 1 right parenthesis 2nd Column ellipsis 3rd Column c Subscript 2 m minus 1 Superscript left parenthesis 1 right parenthesis Baseline plus c Subscript 2 m Superscript left parenthesis 1 right parenthesis Baseline 4th Column ellipsis 6th Row 1st Column down right arrow 2nd Column Blank 3rd Column ellipsis 4th Column down arrow 5th Column ellipsis 7th Row 1st Column c 1 Superscript left parenthesis 3 right parenthesis Baseline equals c 1 Superscript left parenthesis 2 right parenthesis Baseline plus c 2 Superscript left parenthesis 2 right parenthesis Baseline 2nd Column ellipsis 3rd Column ellipsis 4th Column ellipsis EndLayout
c(1)
1
=
c(1)
2
=
. . . c(1)
2m−1 =
c(1)
2m = . . .
ai1b1 + ai2b2
ai3b3 + ai4b4 . . . ai,4m−3b4m−3 + ai,4m−2b4m−2
. . .
. . .
-
-
. . .
-
-
. . .
c(2)
1
=
. . .
c(2)
m =
. . .
c(1)
1
+ c(1)
2
. . .
c(1)
2m−1 + c(1)
2m
. . .
-
. . .
↓
. . .
c(3)
1
= c(2)
1
+ c(2)
2
. . .
. . .
. . .
The order of the computations is nm (or n2 ). 
Multiplying two matrices A and B can be considered as a problem of mul-
tiplying several vectors bi by a matrix A, as described above. In the following 
we will assume A is n × m and B is m × p, and we will use the notation 
ai to represent the ith column of A, aT 
i to represent the ith row of A, bi to 
represent the ith column of B, ci to represent the ith column of C = AB, and  
so on. (This notation is somewhat confusing because here we are not using aT 
i 
to represent the transpose of ai as we normally do. The notation should be 
clear in the context of the diagrams below, however.) Using the inner product 
method above results in the ﬁrst step of the matrix multiplication forming 
Start 4 By 4 Matrix 1st Row 1st Column Blank 2nd Column Blank 3rd Column a 1 Superscript normal upper T Baseline 4th Column Blank 2nd Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix Start 4 By 4 Matrix 1st Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 2nd Row 1st Column b 1 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column Blank 2nd Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix long right arrow Start 4 By 4 Matrix 1st Row 1st Column c 11 equals a 1 Superscript normal upper T Baseline b 1 2nd Column midline horizontal ellipsis 2nd Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column vertical ellipsis 2nd Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix period
⎡
⎢⎢⎢⎣
aT
1
· · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
· · ·
b1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎢⎣
c11 = aT
1 b1
· · ·
· · ·
...
...
· · ·
⎤
⎥⎥⎥⎦.

11.3 Multiplication of Vectors and Matrices
599
Using the second method above, in which the elements of the product vec-
tor are updated all at once, results in the ﬁrst step of the matrix multiplication 
forming 
Start 4 By 4 Matrix 1st Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 2nd Row 1st Column a 1 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column Blank 2nd Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix Start 4 By 4 Matrix 1st Row 1st Column b 11 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 2nd Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column vertical ellipsis 2nd Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix long right arrow Start 4 By 4 Matrix 1st Row 1st Column c 11 Superscript left parenthesis 1 right parenthesis Baseline equals a 11 b 11 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 2nd Row 1st Column c 21 Superscript left parenthesis 1 right parenthesis Baseline equals a 21 b 11 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column vertical ellipsis 2nd Column down right diagonal ellipsis 4th Row 1st Column c Subscript n Baseline 1 Superscript left parenthesis 1 right parenthesis Baseline equals a Subscript n Baseline 1 Baseline b 11 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix period
⎡
⎢⎢⎢⎣
· · ·
a1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
b11 · · ·
· · ·
...
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎢⎢⎣
c(1)
11 = a11b11 · · ·
c(1)
21 = a21b11 · · ·
...
...
c(1)
n1 = an1b11 · · ·
⎤
⎥⎥⎥⎥⎦
.
The next and each successive step in this method are axpy operations: 
c 1 Superscript left parenthesis k plus 1 right parenthesis Baseline equals b Subscript left parenthesis k plus 1 right parenthesis comma 1 Baseline a 1 plus c 1 Superscript left parenthesis k right parenthesis Baseline commac(k+1)
1
= b(k+1),1a1 + c(k)
1 ,
for k going to m − 1. 
Another method for matrix multiplication is to perform axpy operations 
using all of the elements of bT 
1 before completing the computations for any of 
the columns of C. In this method, the elements of the product are built as 
the sum of the outer products aibT 
i . In the notation used above for the other 
methods, we have 
Start 4 By 4 Matrix 1st Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 2nd Row 1st Column a 1 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column Blank 2nd Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix Start 4 By 4 Matrix 1st Row 1st Column Blank 2nd Column Blank 3rd Column b 1 Superscript normal upper T Baseline 4th Column Blank 2nd Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank 3rd Row 1st Column down right diagonal ellipsis 4th Row 1st Column Blank 2nd Column Blank 3rd Column midline horizontal ellipsis 4th Column Blank EndMatrix long right arrow Start 4 By 0 Matrix 1st Row Blank 2nd Row c Subscript i j Superscript left parenthesis 1 right parenthesis Baseline equals a 1 b 1 Superscript normal upper T Baseline 3rd Row Blank 4th Row Blank EndMatrix comma
⎡
⎢⎢⎢⎣
· · ·
a1 · · ·
...
· · ·
⎤
⎥⎥⎥⎦
⎡
⎢⎢⎢⎣
bT
1
· · ·
...
· · ·
⎤
⎥⎥⎥⎦−→
⎡
⎢⎢⎣
c(1)
ij = a1bT
1
⎤
⎥⎥⎦,
and the update is 
c Subscript i j Superscript left parenthesis k plus 1 right parenthesis Baseline equals a Subscript k plus 1 Baseline b Subscript k plus 1 Superscript normal upper T Baseline plus c Subscript i j Superscript left parenthesis k right parenthesis Baseline periodc(k+1)
ij
= ak+1bT
k+1 + c(k)
ij .
The order of computations for any of these methods is O(nmp), or just 
O(n3 ), if the dimensions are all approximately the same. Strassen’s method, 
discussed next, reduces the order of the computations. 
11.3.1 Strassen’s Algorithm 
Another method for multiplying matrices that can be faster for large matrices 
is the so-called Strassen algorithm (from Strassen 1969). Suppose A and B 
are square matrices with equal and even dimensions. Partition them into sub-
matrices of equal size, and consider the block representation of the product, 
Start 2 By 2 Matrix 1st Row 1st Column upper C 11 2nd Column upper C 12 2nd Row 1st Column upper C 21 2nd Column upper C 22 EndMatrix equals Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column upper B 21 2nd Column upper B 22 EndMatrix comma
[ C11 C12
C21 C22
]
=
[ A11 A12
A21 A22
] [ B11 B12
B21 B22
]
,
where all blocks are of equal size. Form

600
11 Numerical Linear Algebra
StartLayout 1st Row 1st Column upper P 1 2nd Column equals 3rd Column left parenthesis upper A 11 plus upper A 22 right parenthesis left parenthesis upper B 11 plus upper B 22 right parenthesis comma 2nd Row 1st Column upper P 2 2nd Column equals 3rd Column left parenthesis upper A 21 plus upper A 22 right parenthesis upper B 11 comma 3rd Row 1st Column upper P 3 2nd Column equals 3rd Column upper A 11 left parenthesis upper B 12 minus upper B 22 right parenthesis comma 4th Row 1st Column upper P 4 2nd Column equals 3rd Column upper A 22 left parenthesis upper B 21 minus upper B 11 right parenthesis comma 5th Row 1st Column upper P 5 2nd Column equals 3rd Column left parenthesis upper A 11 plus upper A 12 right parenthesis upper B 22 comma 6th Row 1st Column upper P 6 2nd Column equals 3rd Column left parenthesis upper A 21 minus upper A 11 right parenthesis left parenthesis upper B 11 plus upper B 12 right parenthesis comma 7th Row 1st Column upper P 7 2nd Column equals 3rd Column left parenthesis upper A 12 minus upper A 22 right parenthesis left parenthesis upper B 21 plus upper B 22 right parenthesis period EndLayoutP1 = (A11 + A22)(B11 + B22),
P2 = (A21 + A22)B11,
P3 = A11(B12 −B22),
P4 = A22(B21 −B11),
P5 = (A11 + A12)B22,
P6 = (A21 −A11)(B11 + B12),
P7 = (A12 −A22)(B21 + B22).
Then we have (see the discussion on partitioned matrices in Sect. 3.1) 
StartLayout 1st Row 1st Column upper C 11 2nd Column equals 3rd Column upper P 1 plus upper P 4 minus upper P 5 plus upper P 7 comma 2nd Row 1st Column upper C 12 2nd Column equals 3rd Column upper P 3 plus upper P 5 comma 3rd Row 1st Column upper C 21 2nd Column equals 3rd Column upper P 2 plus upper P 4 comma 4th Row 1st Column upper C 22 2nd Column equals 3rd Column upper P 1 plus upper P 3 minus upper P 2 plus upper P 6 period EndLayoutC11 = P1 + P4 −P5 + P7,
C12 = P3 + P5,
C21 = P2 + P4,
C22 = P1 + P3 −P2 + P6.
Notice that the total number of multiplications is 7 instead of the 8 it would 
be in forming 
Start 2 By 2 Matrix 1st Row 1st Column upper A 11 2nd Column upper A 12 2nd Row 1st Column upper A 21 2nd Column upper A 22 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column upper B 11 2nd Column upper B 12 2nd Row 1st Column upper B 21 2nd Column upper B 22 EndMatrix
[
A11 A12
A21 A22
] [
B11 B12
B21 B22
]
directly. Whether the blocks are matrices or scalars, the same analysis holds. 
Of course, in either case there are more additions. The addition of two k × 
k matrices is O(k2 ), so for a large enough value of n, the total number of 
operations using the Strassen algorithm is less than the number required for 
performing the multiplication in the usual way. 
The partitioning of the matrix factors can also be used recursively, that 
is, in the formation of the P matrices. If the dimension, n, contains a factor 
2e , the algorithm can be used directly e times, and then conventional matrix 
multiplication can be used on any submatrix of dimension ≤ n/2e .) If the 
dimension of the matrices is not even, or if the matrices are not square, it 
may be worthwhile to pad the matrices with zeros and then use the Strassen 
algorithm recursively. 
The order of computations of the Strassen algorithm is O(nlog2 7 ), instead 
of O(n3 ) as in the ordinary method (log2 7 = 2.81). The algorithm can be 
implemented in parallel (see Bailey, Lee, and Simon 1990), and this algorithm 
is actually used in some software systems. 
Several algorithms have been developed that use similar ideas to Strassen’s 
algorithm and are asymptotically faster, that is, with order of computations 
O(nk ) where  k <  log2 7). (Notice that k must be at least 2 because there 
are n2 elements.) None of the algorithms that are asymptotically faster than 
Strassen’s are competitive in practice, however, because they all have much 
larger start-up costs.

11.4 Other Matrix Computations
601
11.3.2 Matrix Multiplication Using MapReduce 
While methods such as Strassen’s algorithm achieve speedup by decreasing 
the total number of computations, other methods increase the overall speed 
by performing computations in parallel. Although not all computations can be 
performed in parallel and there is some overhead in additional computations 
for setting up the job, when multiple processors are available, the total number 
of computations may not be very important. One of the major tasks in parallel 
processing is just keeping track of the individual computations. MapReduce 
(see page 581) can sometimes be used in coordinating these operations. 
For the matrix multiplication AB, in the view that the multiplication is a 
set of inner products, for i running over the indexes of the rows of A and j 
running over the indexes of the columns of B, we merely access the ith row of 
A, ai∗, and  the  jth column of B, b∗j, and form the inner product aT 
i∗b∗j as the 
(i, j)th element of the product AB. In the language of relational databases in 
which the two matrices are sets of data with row and column identiﬁers, this 
amounts to accessing the rows of A and the columns of B one by one, matching 
the elements of the row and the column so that the column designator of the 
row element matches the row designator of the column element, summing the 
product of the A row elements and the B column elements, and then grouping 
the sums of the products (that is, the inner products) by the A row designators 
and the B column designators. In SQL, it is 
SELECT A.row, B.col 
SUM(A.value*B.value) FROM A,B WHERE A.col=B.row 
GROUP BY A.row, B.col; 
In a distributed computing environment, MapReduce could be used to 
perform these operations. However the matrices are stored, possibly each over 
multiple environments, MapReduce would ﬁrst map the matrix elements using 
their respective row and column indices as keys. It would then make the 
appropriate associations of row element from A with the column elements 
from B and perform the multiplications and the sum. Finally, the sums of 
the multiplications (that is, the inner products) would be associated with the 
appropriate keys for the output. This process is described in many elementary 
descriptions of Hadoop, such as in Leskovec et al. (2014) (Chapter 2). 
11.4 Other Matrix Computations 
Many other matrix computations depend on a matrix factorization. The most 
useful factorization is the QR factorization. It can be computed stably using 
either Householder reﬂections, Givens rotations, or the Gram-Schmidt proce-
dure, as described in Sects. 4.6.6, 4.6.7, and  4.6.8 (beginning on page 242), 
respectively. This is one time when the computational methods can follow

602
11 Numerical Linear Algebra
the mathematical descriptions rather closely. Iterations using the QR factor-
ization are used in a variety of matrix computations; for example, they are 
used in the most common method for evaluating eigenvalues, as described in 
Sect. 6.4, beginning on page 314. 
Another very useful factorization is the singular value decomposition 
(SVD). The computations for SVD described in Sect. 6.7 beginning on page 318 
are eﬃcient and preserve numerical accuracy. A major diﬀerence in the QR 
factorization and the SVD is that the computations for SVD are necessarily 
iterative (recall the remarks at the beginning of Chap. 6). 
11.4.1 Rank Determination 
It is often easy to determine that a matrix is of full rank. If the matrix is 
not of full rank, however, or if it is very ill-conditioned, it may be diﬃcult to 
determine its rank. This is because the computations to determine the rank 
eventually approximate 0. It is diﬃcult to approximate 0; the relative error 
(if deﬁned) would be either 0 or inﬁnite. The rank-revealing QR factoriza-
tion (Eq. (4.41), page 241) is the preferred method for estimating the rank. 
(Although I refer to this as “estimation,” it more properly should be called 
“approximation.” “Estimation” and the related term “testing,” as used in sta-
tistical applications, apply to an unknown object, as in estimating or testing 
the rank of a model matrix as discussed in Sect. 9.4.5, beginning on page 479.) 
When this decomposition is used to estimate the rank, it is recommended that 
complete pivoting be used in computing the decomposition. The LDU decom-
position, described on page 232, can be modiﬁed the same way we used the 
modiﬁed QR to estimate the rank of a matrix. Again, it is recommended that 
complete pivoting be used in computing the decomposition. 
The singular value decomposition (SVD) shown in Eq. (3.299) on page 183 
also provides an indication of the rank of the matrix. For the n × m matrix 
A, the SVD is 
upper A equals upper U upper D upper V Superscript normal upper T Baseline commaA = UDV T,
where U is an n×n orthogonal matrix, V is an m×m orthogonal matrix, and 
D is a diagonal matrix of the singular values. The number of nonzero singular 
values is the rank of the matrix. Of course, again, the question is whether or 
not the singular values are zero. It is unlikely that the values computed are 
exactly zero. 
A problem related to rank determination is to approximate the matrix 
A with a matrix Ar of rank r ≤ rank(A). The singular value decomposition 
provides an easy way to do this, 
upper A Subscript r Baseline equals upper U upper D Subscript r Baseline upper V Superscript normal upper T Baseline commaAr = UDrV T,
where Dr is  the same as  D, except with zeros replacing all but the r largest 
singular values. A result of Eckart and Young (1936) guarantees Ar is the 
rank r matrix closest to A as measured by the Frobenius norm,

11.4 Other Matrix Computations
603
parallel to upper A minus upper A Subscript r Baseline parallel to Subscript normal upper F Baseline comma||A −Ar||F,
(see Sect. 3.12). This kind of matrix approximation is the basis for dimension 
reduction by principal components. 
11.4.2 Computing the Determinant 
The determinant of a square matrix can be obtained easily as the product of 
the diagonal elements of the triangular matrix in any factorization that yields 
an orthogonal matrix times a triangular matrix. As we have stated before, 
however, it is not often that the determinant need be computed. 
One application in statistics is in optimal experimental designs. The D-
optimal criterion, for example, chooses the design matrix, X, such that |XT X| 
is maximized. 
11.4.3 Computing the Condition Number 
The computation of a condition number of a matrix can be quite involved. 
Clearly, we would not want to use the deﬁnition, κ(A) = ||A|| ||A−1||, directly. 
Although the choice of the norm aﬀects the condition number, recalling the 
discussion in Sect. 5.1, we choose whichever condition number is easiest to 
compute or estimate. 
Various methods have been proposed to estimate the condition number 
using relatively simple computations. Cline et al. (1979) suggest a method 
that is easy to perform and is widely used. For a given matrix A and some 
vector v, solve  
upper A Superscript normal upper T Baseline x equals vATx = v
and then 
upper A y equals x periodAy = x.
By tracking the computations in the solution of these systems, Cline et al. 
conclude that 
StartFraction parallel to y parallel to Over parallel to x parallel to EndFraction||y||
||x||
is approximately equal to, but less than, ||A−1||. This estimate is used with  
respect to the L1 norm in the LINPACK software library (see page 627 and 
Dongarra et al. 1979), but the approximation is valid for any norm. Solving the 
two systems above probably does not require much additional work because 
the original problem was likely to solve Ax = b, and solving a system with 
multiple right-hand sides can be done eﬃciently using the solution to one 
of the right-hand sides. The approximation is better if v is chosen so that
||x|| is as large as possible relative to ||v||, but the LINPACK estimator can 
underestimate the true condition number considerably. Modiﬁcations of the 
L1 LINPACK condition number that are based on an L2 norm also perform 
poorly.

604
11 Numerical Linear Algebra
Hager (1984) gives another method for an L1 condition number. Higham 
(1988) provides an improvement of Hager’s method, given as Algorithm 11.1 
below, which is used in the LAPACK software library (Anderson et al. 2000). 
Algorithm 11.1 The Hager/Higham LAPACK Condition Number 
Estimator γ of the n × n Matrix A 
Assume n >  1; otherwise set γ = ||A||. (All norms are L1 unless speciﬁed 
otherwise.) 
0. Set k = 1;  v(k) = 1 
nA1; γ(k) = ||v(k)||; and  x(k) = AT sign(v(k) ). 
1. Set j = min{i, s.t. |x (k) 
i | = ||x(k)||∞}. 
2. Set k = k + 1.  
3. Set v(k) = Aej. 
4. Set γ(k) = ||v(k)||. 
5. If sign(v(k) ) = sign(v(k−1) ) or  γ(k) ≤ γ(k−1) , then go to step  8. 
6. Set x(k) = AT sign(v(k) ). 
7. If ||x(k)||∞/= x (k) 
j 
and k ≤ kmax, then go to step  1.  
8. For i = 1, 2, . . . , n, set  xi = (−1)i+1 (
1 +  i−1 
n−1
)
. 
9. Set x = Ax. 
10. If 2||x||
(3n) > γ(k) , set  γ(k) = 2||x||
(3n) . 
11. Set γ = γ(k) . 
Higham (1990) gives a survey and comparison of various ways of estimating 
and computing condition numbers. You are asked to study the performance 
of the LAPACK estimate using Monte Carlo methods in Exercise 11.5 on 
page 606. 
Exercises 
11.1. Gram-Schmidt orthonormalization. 
a) Write a program module (in Fortran, C, R, Octave or MATLAB, 
or whatever language you choose) to implement Gram-Schmidt or-
thonormalization using Algorithm 2.1. Your program should be for 
an arbitrary order and for an arbitrary set of linearly independent 
vectors. 
b) Write a program module to implement Gram-Schmidt orthonormal-
ization using Eqs. (2.55) and  (2.56). 
c) Experiment with your programs. Do they usually give the same re-
sults? Try them on a linearly independent set of vectors all of which 
point “almost” in the same direction. Do you see any diﬀerence in 
the accuracy? Think of some systematic way of forming a set of 
vectors that point in almost the same direction. One way of doing 
this would be, for a given x, to form  x + eei for i = 1, . . . , n  − 1,

Exercises
605
where ei is the ith unit vector and e is a small positive number. The 
diﬀerence can even be seen in hand computations for n = 3.  Take  
x1 = (1, 10−6 , 10−6 ), x2 = (1, 10−6 , 0), and x3 = (1, 0, 10−6 ). 
11.2. Given the n × k matrix A and the k-vector b (where n and k are large), 
consider the problem of evaluating c = Ab. As we have mentioned, there 
are two obvious ways of doing this: (1) compute each element of c, one  
at a time, as an inner product ci = aT 
i b = E
j aijbj, or (2) update the 
computation of all of the elements of c in the inner loop. 
a) What is the order of computation of the two algorithms? 
b) Why would the relative eﬃciencies of these two algorithms be dif-
ferent for diﬀerent programming languages, such as Fortran and C? 
c) Suppose there are p processors available and the fan-in algorithm 
on page 598 is used to evaluate Ax as a set of inner products. What 
is the order of time of the algorithm? 
d) Give a heuristic explanation of why the computation of the inner 
products by a fan-in algorithm is likely to have less roundoﬀ error 
than computing the inner products by a standard serial algorithm. 
(This does not have anything to do with the parallelism.) 
e) Describe how the following approach could be parallelized. (This is 
the second general algorithm mentioned above.) 
StartLayout 1st Row normal f normal o normal r i equals 1 comma ellipsis comma n 2nd Row left brace 3rd Row c Subscript i Baseline equals 0 4th Row normal f normal o normal r j equals 1 comma ellipsis comma k 5th Row left brace 6th Row c Subscript i Baseline equals c Subscript i Baseline plus a Subscript i j Baseline b Subscript j Baseline 7th Row right brace 8th Row right brace EndLayout
for i = 1, . . . , n
{
ci = 0
for j = 1, . . . , k
{
ci = ci + aijbj
}
}
f) What is the order of time of the algorithms you described? 
11.3. Consider the problem of evaluating C = AB, where  A is n × m and B 
is m × q. Notice that this multiplication can be viewed as a set of ma-
trix/vector multiplications, so either of the algorithms in Exercise 11.2d 
above would be applicable. There is, however, another way of performing 
this multiplication, in which all of the elements of C could be evaluated 
simultaneously. 
a) Write pseudocode for an algorithm in which the nq elements of C 
could be evaluated simultaneously. Do not be concerned with the 
parallelization in this part of the question. 
b) Now suppose there are nmq processors available. Describe how the 
matrix multiplication could be accomplished in O(m) steps (where 
a step may be a multiplication and an addition). 
Hint: Use a fan-in algorithm. 
11.4. Write a Fortran or C program to compute an estimate of the L1 LA-
PACK condition number γ using Algorithm 11.1 on page 604.

606
11 Numerical Linear Algebra
11.5. Design and conduct a Monte Carlo study to assess the performance of 
the LAPACK estimator of the L1 condition number using your program 
from Exercise 11.4. Consider a few diﬀerent sizes of matrices, say 5 × 
5, 10 × 10, and 20 × 20, and consider a range of condition numbers, 
say 10, 104 , and  108 . In order to assess the accuracy of the condition 
number estimator, the random matrices in your study must have known 
condition numbers. It is easy to construct a diagonal matrix with a 
given condition number. The condition number of the diagonal matrix 
D, with nonzero elements d1, . . . , dn, is max |di|/ min |di|. It is not  so  
clear how to construct a general (square) matrix with a given condition 
number. The L2 condition number of the matrix UDV , where  U and 
V are orthogonal matrices, is the same as the L2 condition number 
of U. We can therefore construct a wide range of matrices with given 
L2 condition numbers. In your Monte Carlo study, use matrices with 
known L2 condition numbers. The next question is what kind of random 
matrices to generate. Again, make a choice of convenience. Generate 
random diagonal matrices D, subject to ﬁxed κ(D) = max |di|/ min |di|. 
Then generate random orthogonal matrices as described in Exercise 7.12 
on page 362. Any conclusions made on the basis of a Monte Carlo study, 
of course, must be restricted to the domain of the sampling of the study. 
(See Stewart 1980, for a Monte Carlo study of the performance of the 
LINPACK condition number estimator.)

12 
Software for Numerical Linear Algebra 
There is a variety of computer software available to perform the operations 
on vectors and matrices discussed in Chap. 11 and previous chapters. I have 
discussed and illustrated R for these operations, and it is the main system I 
use nowadays. I also use Fortran occasionally. I use other systems only rarely, 
and even so, I generally use them through R. 
In this chapter, I describe some of the general properties of software sys-
tems, especially as they would be used in numerical linear algebra. I then 
brieﬂy describe some of the other software systems that I have used. 
We can distinguish software based on various dimensions, including the 
kinds of applications that the software emphasizes, the level of the objects 
it works with directly, and whether or not it is interactive. We can also dis-
tinguish software based on who “owns” the software and its availability to 
other users. Many commercial software systems are available from the de-
velopers/owners through licensing agreements, and the rights of the user are 
restricted by the terms of the license, in addition to any copyright. 
Some software is designed only to perform a limited number of functions, 
such as for eigenanalysis, while other software provides a wide range of com-
putations for linear algebra. Some software supports only real matrices and 
real associated values, such as eigenvalues. In some software systems, the ba-
sic units must be scalars, and so operations on matrices or vectors must be 
performed on individual elements. In these systems, higher-level functions to 
work directly on the arrays are often built and stored in libraries or sup-
plemental software packages. In other software systems, the array itself is a 
fundamental operand. Finally, some software for linear algebra is interactive 
and computations are performed immediately in response to the user’s input, 
while other software systems provide a language for writing procedural de-
scriptions that are to be compiled for later invocation through some interface 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 12 
607

608
12 Software for Numerical Linear Algebra
within the same system or through linkage from a diﬀerent software system. 
We often refer to the languages of interactive systems as “interpretive” or 
“scripting” languages. 
12.1 General Considerations 
A person’s interaction with a software system represents a series of invest-
ments. The ﬁrst investment is in getting access to the system. This may involve 
spending money, spending time downloading ﬁles, and installing the software. 
The investment to acquire access involves “learning” the system: how to start 
it, how to input data, how to do something, how to output results, and how to 
shut it down. The next type of investment is usually just the time to use the 
system to solve problems. The returns on these investments are the solutions 
to the problems, or just the personal satisfaction in having solved them. A 
continuing investment is in becoming proﬁcient in use of the software. The 
return is in time saved. Another important investment is the development 
of a set of reusable functions and scripts. Functions can often be used in a 
wide variety of settings far beyond the one that prompted the writing of the 
function in the ﬁrst place. Scripts for a particular task can serve as templates 
for programs to solve a diﬀerent problem. The return is in time saved and, 
possibly, improved accuracy, in solving future problems. 
As with other investments, an objective is to maximize returns. Although 
the computing environment is constantly changing, as with other investments, 
some degree of stability of the investment environment is desirable in order 
to protect the returns. Formal standards promulgated by organizations such 
as ANSI, IEEE, and ISO help to ensure this. 
The software user’s primary goal is not investment (usually), but never-
theless, the user is making an investment and with a little forethought can 
increase the returns on this investment. 
Investors’ choices depend on the stage of their investment career. If a 
software investor has not become proﬁcient in any language, that investor has 
diﬀerent investment choices from an investor who is very ﬂuent in C, maybe 
knows a little Python, but does not know a word of Fortran. 
12.1.1 Software Development and Open-Source Software 
An important distinction among software systems is the nature of the user’s 
access to the system. Software, as intellectual property, is protected by copy-
right, just as any other written work (as opposed to an algorithm, which may 
be protected by patent). Copyright is an unusual type of legal title. In most le-
gal jurisdictions, it is initiated without any explicit registration by the holder; 
it just comes into being at the time of the creation of the original work. 
There are various ways that a copyright of software can be modiﬁed. At one 
extreme is complete revocation of copyright by the author; that is, the author

12.1 General Considerations
609
puts the work in the “public domain.” Even if the copyright is retained, the 
owner may allow free use, either with or without the rights to modify. We will 
not delve into the variations in these schemes, but rather use the phrase “open 
source” to refer to systems to which the authors have give users free access 
to the full source code of the system. Many important systems ranging from 
operating systems, such as Linux, to programming and application-oriented 
systems, such as R, are open source. 
12.1.2 Integrated Development, Collaborative Research, and 
Version Control 
Results of computations are generally accompanied by explanatory text. There 
are software systems that allow computer code incorporated with ordinary 
text in such a way that the code can be executed and the results saved of 
that the text can be typeset, ignoring the computer code, or displaying it in 
a special font or in a ﬂoating ﬁgure. Such systems are based on markup text 
processors, such as TEX or LATEX and on a front end to the code processor 
that ignores the ordinary text. 
Some systems designed for such an integrated development are imple-
mented as R packages, including sweave, knitr, and  markdown, all of which 
are available from the cran website. The packages are simple to learn, and 
many users think that they are worth the trouble. Alternatively, Overleaf is 
a web-based system that allows incorporation of R code and LATEX text in a 
single ﬁle. The R code can be executed and the results processed and saved, 
and the LATEX text along with R code or output can be typeset. 
Larger software development projects may involve several diﬀerent peo-
ple who must coordinate their separate contributions and changes. Overleaf, 
being web-based, is particularly useful for general collaborative document de-
velopment. The TEX processor in Overleaf has most of the common packages 
attached. It also provides PDF rendering. The website is 
https://www.overleaf.com/ 
Another good system for collaborative work is GitHub. 
https://github.com/ 
In collaborative development, version-control systems are especially useful. 
Git is probably one of the best systems for version control. (It was built 
by Linus Torvalds, the creator of Linux.) GitHub provides a user-friendly 
interface to the software as well as a hosting service. The software itself is free 
and open source. 
The Reproducible R Toolkit (see page 646) is useful for version control in 
collaborative work as well as for a single user to deal with diﬀerent versions 
of R and R packages. 
12.1.3 Finding Software 
The quickest way to ﬁnd software is usually to use a web search program. The 
R Seek organization provides a restricted web search for R software:

610
12 Software for Numerical Linear Algebra
https://rseek.org/ 
An index of R software for speciﬁc tasks is the views site at cran: 
https://cran.r-project.org/web/views/ 
The Guide to Available Mathematical Software (GAMS) is a good source 
of information about software. This guide is organized by types of compu-
tations. Computations for linear algebra are in Class D, for example. The 
website is 
https://gams.nist.gov/cgi-bin/serve.cgi/Class/D/ 
Much of the software is available through the netlib repository: 
https://netlib.org/ 
12.1.4 Software Design 
There are many properties of software that aﬀect its usefulness. Although we 
emphasize eﬃcient use of computer resources (fast algorithms and minimal 
memory usage), it is more important that the user’s time and eﬀorts are not 
wasted. If the general design of the software is consistent with the user’s ways 
of thinking about the application, then the user is more likely to be eﬃcient 
in the use of the software and to make fewer errors. 
Interoperability 
For some types of software, it is important to be aware of the way the data are 
stored in the computer, as we discussed in Sect. 11.1 beginning on page 591. 
This may include such things as whether the storage is row-major or column-
major, which will determine the stride and may determine the details of an 
algorithm so as to enhance the eﬃciency. Software written in a language such 
as Fortran or C often requires the speciﬁcation of the number of rows (in 
Fortran) or columns (in C) that have been allocated for the storage of a 
matrix. The amount of space allocated for the storage of a matrix may not 
correspond exactly to the size of the matrix. 
There are many issues to consider in evaluating software or to be aware 
of when developing software. The portability of the software is an important 
consideration because a user’s programs are often moved from one computing 
environment to another. 
Eﬃciency 
Computational eﬃciency, the topic of Sect. 10.4.2, is a very important con-
sideration in the development and use of numerical methods. In that context, 
eﬃciency refers to the use of computing resources, which ones are used and for 
how long they are used for a speciﬁc task. In the selection and use of software,

12.1 General Considerations
611
the eﬃciency of the methods implemented in the software is an important 
consideration, but, depending on the task and the available resources, the 
user’s time to set up the computations and access the results is often more 
important. Higher-level software and more application-speciﬁc software, while 
the actual computations may be slower, may be more eﬃcient overall. 
Some situations require special software that is more eﬃcient than general-
purpose software would be for the given task. Software for sparse matrices, 
for example, is specialized to take advantage of the zero entries. 
Writing Mathematics and Writing Programs 
Although one of the messages of this book is that we should not ignore compu-
tational details, there are some trivial details that should not have to enter our 
conscious thought processes. In well-designed software systems, these details 
ﬂow naturally in the process of converting mathematics to computer code. 
In writing either mathematics or programs, it is generally best to think of 
objects at the highest level that is appropriate for the problem at hand. The 
details of some computational procedure may be of the form 
StartSet normal f normal o normal r i equals 1 colon n normal d normal o StartSet normal f normal o normal r j equals 1 colon p normal d normal o sigma summation Underscript k equals 1 Overscript m Endscripts a Subscript i k Baseline x Subscript k j Baseline EndSet EndSet period
{
for i = 1 : n do
{
for j = 1 : p do
m
E
k=1
aikxkj
}}
.
(12.1) 
We sometimes think of the computations in this form because we have pro-
grammed them in some low-level language at some time. 
More likely, we have learned mathematical deﬁnitions and have seen a 
formula in that form (expression (12.1) deﬁnes Cayley multiplication of the 
n times mn × m matrix left parenthesis a Subscript i k Baseline right parenthesis(aik) and the m times pm × p matrix left parenthesis x Subscript k j Baseline right parenthesis(xkj)). But we do not program 
it that way; this is an instance of a principle that we encounter repeatedly: 
the form of a mathematical expression and the way the expression should be 
evaluated in actual practice may be quite diﬀerent. 
In some cases, it is important to look at the computations in this form, 
but usually it is better to think of the computations at a higher level, say 
upper A Superscript normal upper T Baseline upper X periodATX.
(12.2) 
The compactness of the expression is not the issue (although it certainly is 
more pleasant to read). The issue is that expression (12.1) leads us to think of 
some nested computational loops, while expression (12.2) leads us to look for 
more eﬃcient computational modules, such as the BLAS, which we discuss 
below. In a higher-level language system such as R, the latter expression is 
more likely to cause us to use the system more eﬃciently. 
Numerical Mathematical Objects and Computer Objects 
Some diﬃcult design decisions must be made when building systems that 
provide objects that simulate mathematical objects. One issue is how to treat

612
12 Software for Numerical Linear Algebra
diﬀerent mathematical objects that are based on the real number system, such 
as scalars, vectors, and matrices, when their sizes happen to coincide. 
• Is a vector with one element a scalar? 
• Is a 1 times 11 × 1 matrix a scalar? 
• Is a 1 times n1 × n matrix a “row vector?” 
• Is an n times 1n × 1 matrix a “column vector?” 
• Is a “column vector” diﬀerent from a “row vector?” 
While the obvious answer to all these questions is “no,” it is often convenient 
to design software systems as if the answer, at least to some of the questions 
some of the time, is “yes.” The answer to any such software design question 
always must be made in the context of the purpose and intended use (and 
users) of the software. The issue is not the purity of a mathematical deﬁnition. 
We have already seen that most computer objects and operators do not behave 
exactly like the mathematical entities they simulate anyway. 
The experience of most people engaged in scientiﬁc computations over 
many years has shown that the convenience resulting from the software’s 
equivalent treatment of such diﬀerent objects as a 1 times 11 × 1 matrix and a scalar 
outweighs the programming error detection that could be possible if the ob-
jects were made to behave as nearly as possible to the way the mathematical 
entities they simulate behave. 
Consider, for example, the following arrays of numbers: 
upper A equals left bracket 1 2 right bracket comma upper B equals StartBinomialOrMatrix 1 Choose 2 EndBinomialOrMatrix comma upper C equals Start 3 By 1 Matrix 1st Row 1 2nd Row 2 3rd Row 3 EndMatrix periodA = [1 2],
B =
[ 1
2
]
,
C =
⎡
⎣
1
2
3
⎤
⎦.
(12.3) 
If these arrays are matrices with the usual matrix algebra, then ABC, where  
juxtaposition indicates Cayley multiplication, is not a valid expression. 
If, however, we are willing to allow mathematical objects to change types, 
we come up with a reasonable interpretation of ABC. If the  1 times 11 × 1 matrix AB 
is interpreted as the scalar 5, then the expression left parenthesis upper A upper B right parenthesis upper C(AB)C can be interpreted as 
5C, that is, a scalar times a matrix. (Under Cayley multiplication, of course, 
we do not need to indicate the order of the operations because the operation 
is associative.) 
There is no (reasonable) interpretation that would make the expression 
upper A left parenthesis upper B upper C right parenthesisA(BC) valid. 
If A is a “row vector” and B is a “column vector,” it hardly makes sense 
to deﬁne an operation on them that would yield another vector. A vector 
space cannot consist of such mixtures. Under a strict interpretation of the 
operations, left parenthesis upper A upper B right parenthesis upper C(AB)C is not a valid expression. 
We often think of the “transpose” of a vector (although this may not be 
a viable concept in a vector space), and we denote a dot product in a vector 
space as x Superscript normal upper T Baseline yxTy. If we therefore interpret a row vector such as A in (12.3) as  
x Superscript normal upper TxT for some x in the vector space of which B is a member, then AB can be

12.1 General Considerations
613
interpreted as a dot product (that is, as a scalar) and again left parenthesis upper A upper B right parenthesis upper C(AB)C is a valid 
expression. 
Many software packages have a natural atomic numerical type. For exam-
ple, in Fortran and C the atomic numeric type is a scalar. Other structures 
such as vectors and matrices are built as simple arrays from scalars. Other 
software systems are array-oriented. The basic atomic numeric type in MAT-
LAB is a matrix. In MATLAB, a scalar is a 1 times 11 × 1 matrix. A one-dimensional 
array in MATLAB is a matrix with either one row or one column. 
The basic atomic numeric type in R is a vector. In R, a scalar is a vector 
of length 1. A 1 times n1 × n matrix and an n times 1n × 1 matrix may be cast as vectors (that 
is, one-dimensional arrays). This is often exactly what we would want, but 
sometimes we want to preserve the matrix structure. 
Other Mathematical Objects and Computer Objects 
Most mathematical operations performed on a computer are numerical com-
putations, and most of our interests in linear algebra are focused on numerical 
computations. Occasionally, however, we want to do other kinds of math-
ematical operations or operations on a dataset. One of the most common 
non-computational operation is sorting. 
Operations on abstract variables are supported by some software packages. 
These operations are called “symbolic computations,” and the computer sys-
tems that support them are called “computer algebra systems.” Maple and 
Mathematica are probably the best-known computer algebra systems. 
Operations on sets are also supported by some software systems, including 
Python notably. 
Software for Statistical Applications 
As mentioned in Chap. 9 in the appendix on page 510, statistical applica-
tions have requirements that go beyond simple linear algebra. The two most 
common additional requirements are for: 
• Handling metadata 
• Accommodating missing data 
Software packages designed for data analysis, such as SAS and R, generally 
provide for metadata and missing values. Fortran/C libraries generally do not 
provide for metadata or for handling missing data. 
Two other needs that often arise in statistical analysis but often are not 
dealt with adequately in available software are: 
• Graceful handling of nonfull rank matrices 
• Working with nonsquare matrices

614
12 Software for Numerical Linear Algebra
Aside from these general capabilities, of course, software packages for sta-
tistical applications, even if they are designed for some speciﬁc type of anal-
ysis, should provide the common operations such as computation of simple 
univariate statistics, linear regression computations, and some simple graph-
ing capabilities. 
Robustness 
Operations on matrices are often viewed from the narrow perspective of the 
numerical analyst rather than from the broader perspective of a user with 
a task to perform. For example, the user may seek a solution to the linear 
system upper A x equals bAx = b. Many software packages to solve a linear system requires A 
to be square and of full rank. If this is not the case, then there are three 
possibilities: the system has no solution, the system has multiple solutions, 
or the system has a unique solution. Software to solve a linear system of 
equations that requires A to be square and of full rank does not distinguish 
among these possibilities but rather always refuses to provide any solution. 
This can be quite annoying to a user who wants to solve a large number 
of diﬀerent systems with diﬀerent properties using the same code. A better 
design of the software would allow solution of consistent non-square systems 
and of consistent nonfull-rank systems and report an inconsistency of any 
structure as such. 
A related problem occurs when A is a scalar. Some software to solve the 
linear system will fail on this degenerate case. (Note, for example, solve in 
R handles this correctly, although it does not handle the nonsquare case.) 
Good software is robust both to extended cases and to degenerate cases. 
Computing Paradigms: Parallel Processing 
A computational task may involve several sequences of computations, some of 
which can be performed independently of others. We speak of the separate se-
quences of operations as threads. If the threads can be executed independently, 
they can easily be performed at the same time, that is, in parallel. Even if they 
cannot be performed independently, by sharing results among the threads, it 
may be possible to execute them in parallel. Parallel processing is one of the 
most important ways of speeding up computations. 
Many computational jobs follow a very simple pattern; all data and all 
processors are together in the same system. The programming model is simple. 
A single computational job may consist of many individual computations, 
some of which depend on the results of previous computations and others of 
sequences of which are completely independent of others. If multiple process-
ing units are available, it may be possible to perform some of the computations 
simultaneously in diﬀerent units. The computations can be split out in two 
ways, via sockets, in which a separate version of the program is launched on

12.1 General Considerations
615
each processor, or by forking, in which the entire executing program on one 
processor is replicated on each processor in turn. 
Most computers, even personal computers, nowadays have multiple pro-
cessing units, called “cores.” These cores can execute a program in parallel. 
For many interesting problems, the data are stored in diﬀerent places and 
in a variety of formats. A simple approach, of course, is to collect and organize 
the data prior to any actual processing. 
In many large-scale problems it is not practical to collect and organize the 
data prior to any actual processing, due either to the size of the data or to 
the fact that the process of data generation is ongoing. If the data are not 
collected in one place, the processing may occur at diﬀerent locations also. 
Such distributed computing, which can be thought of as an extreme form of 
parallel computing, requires a completely diﬀerent paradigm. 
Spark and Hadoop provide such a paradigm. Hadoop and MapReduce, 
discussed on page 581, together are ideally suited to distributed computing 
tasks. We will not discuss these methods further here, but rather refer the 
reader to White (2015) who discusses Hadoop and gives several examples of 
its use and to Karau et al. (2015) who provide a good introduction to Spark. 
For most computations in numerical linear algebra, these considerations 
are not relevant, but as we address larger problems, these issues become much 
more important. 
Array Structures and Indexes 
Numerical operations on vectors and matrices in most general-purpose pro-
cedural languages are performed either within loops of operations on the in-
dividual elements or by invocation of a separate program module. A natural 
way of representing vectors and matrices is as array variables with indexes. 
One of the main diﬀerences in various computing systems is how they 
handle arrays. The most common impact on the user is the indexing of the 
arrays, and here the most common diﬀerences are in where the index begins (0 
or 1) and how sequences are handled. To process a mathematical expression 
f left parenthesis x Subscript i Baseline right parenthesisf(xi) over n values of a vector x, we use programming statements of the form 
of either 
for i = 1:n, compute f(x[i]) 
or 
for i = 0:(n-1), compute f(x[i]) 
depending on the beginning index. 
Indexing in Fortran (by default), MATLAB, and R begins with “1”, just 
as in most mathematical notation (as throughout this book, for example). 
Fortran allows indexing to start at any integer. Indexing for ordinary arrays 
begins with “0” in Python, C, and C++.

616
12 Software for Numerical Linear Algebra
Fortran handles arrays as multiply indexed memory locations, consistent 
with the nature of the object. The storage of two-dimensional arrays in Fortran 
is column-major; that is, the array A is stored as normal v normal e normal c left parenthesis upper A right parenthesisvec(A). To reference the 
contiguous memory locations, the ﬁrst subscript varies fastest. In general-
purpose software consisting of Fortran subprograms, it is often necessary to 
specify the lengths of all dimensions of a Fortran array except the last one. 
An array in C is an ordered set of memory locations referenced by a pointer 
or by a name and an index. The indexes are enclosed in rectangular brackets 
following the variable name. An element of a multidimensional array in C is 
indexed by multiple indexes, each within rectangular brackets. If the 3 times 43 × 4
matrix A is as stored in the C array A, the  left parenthesis 2 comma 3 right parenthesis(2, 3) element upper A Subscript 2 comma 3A2,3 is referenced as 
A[1][2]. This disconnect between the usual mathematical representations and 
the C representations results from the historical development of C by computer 
scientists, who deal with arrays, rather than by mathematical scientists, who 
deal with matrices and vectors. 
Multidimensional arrays in C are arrays of arrays, in which the array con-
structors operate from right to left. This results in two-dimensional C arrays 
being stored in row-major order, that is, the array A is stored as normal v normal e normal c left parenthesis upper A Superscript normal upper T Baseline right parenthesisvec(AT). To  
reference the contiguous memory locations, the last subscript varies fastest. 
In general-purpose software consisting of C functions, it is often necessary to 
specify the lengths of all dimensions of a C array except the ﬁrst one. 
Python provides an array construct similar to that of C, but in addition it 
also provides several other constructs for multiple values, including one that 
is eﬀectively a mathematical set; that is, a collection of unique elements. 
Subarrays are formed slightly diﬀerently in diﬀerent systems. Consider the 
vector x with ten elements, which we have stored in a linear array called x. 
The number of elements in the array is len, 10 in this case. Suppose we want 
to reference various elements within the array. In diﬀerent systems this is done 
as  shown in Table  12.1. 
Table 12.1. Subarrays 
The last ﬁve elements All but last ﬁve elements First, third, ... elements 
Fortran
x(len-4:)
x(:len-5)
x(::2) 
MATLAB
x(len-4:len)
x(1:len-5)
x(1:2:len) 
R
x[(len-4):len]
x[1:(len-5)]
x[seq(1,len,2)] 
Python
x[len-5:]
x[:len-5]
x[::2] 
For people who use diﬀerent systems, the diﬀerences in the indexing is not 
just annoying; the main problem is that it leads to programming errors. (The 
reader who does not know Python may also be somewhat perplexed by the 
ﬁrst two entries for Python in Table 12.1.)

12.1 General Considerations
617
Matrix Storage Modes 
There are various ways of indexing matrices that have special structure. Al-
though one of the purposes of special storage modes is to use less computer 
memory, sometimes it is more natural to think of the elements of a matrix 
with a special structure in a way that conforms to that structure. 
Matrices that have multiple elements with the same value can often be 
stored in the computer in such a way that the individual elements do not all 
have separate locations. Symmetric matrices and matrices with many zeros, 
such as the upper or lower triangular matrices of the various factorizations we 
have discussed, are examples of matrices that do not require full rectangular 
arrays for their storage. There are several special modes for storage of matrices 
with repeated elements, especially if those elements are zeroes: 
• Symmetric mode 
• Hermitian mode 
• Triangular mode 
• Band mode 
• Sparse storage mode 
For a symmetric or Hermitian matrix or a triangular matrix, only about 
half of the elements need to be stored. Although the amount of space saved 
by not storing the full symmetric matrix is only about one-half of the amount 
of space required, the use of rank 1 arrays rather than rank 2 arrays can yield 
some reference eﬃciencies. (Recall that in discussions of computer software 
objects, “rank” usually means the number of dimensions.) For band matrices 
and other sparse matrices, the savings in storage can be much larger. 
For a symmetric matrix such as 
upper A equals Start 4 By 4 Matrix 1st Row 1st Column 1 2nd Column 2 3rd Column 4 4th Column midline horizontal ellipsis 2nd Row 1st Column 2 2nd Column 3 3rd Column 5 4th Column midline horizontal ellipsis 3rd Row 1st Column 4 2nd Column 5 3rd Column 6 4th Column midline horizontal ellipsis 4th Row 1st Column midline horizontal ellipsis EndMatrix commaA =
⎡
⎢⎢⎣
1 2 4 · · ·
2 3 5 · · ·
4 5 6 · · ·
· · ·
⎤
⎥⎥⎦,
(12.4) 
only the unique elements 
v equals left parenthesis 1 comma 2 comma 3 comma 4 comma 5 comma 6 comma midline horizontal ellipsis right parenthesisv = (1, 2, 3, 4, 5, 6, · · · )
(12.5) 
need to be stored. 
A special indexing method for storing symmetric matrices, called sym-
metric storage mode, uses a linear array to store only the unique elements. 
Symmetric storage mode can be set up in various ways. 
One form of symmetric storage mode corresponds directly to an arrange-
ment of the vector v in Eq. (12.5) to hold the data in matrix A in Eq. (12.4). 
This symmetric storage mode is a much more eﬃcient and useful method of 
storing a symmetric matrix than would be achieved by a normal v normal e normal c normal h left parenthesis dot right parenthesisvech(·) operator be-
cause with symmetric storage mode, the size of the matrix aﬀects only the

618
12 Software for Numerical Linear Algebra
elements of the vector near the end. If the number of rows and columns of the 
matrix is increased, the length of the vector is increased, but the elements are 
not rearranged. 
For an n times nn×n symmetric matrix A, the correspondence with the n left parenthesis n plus 1 right parenthesis divided by 2n(n+1)/2-
vector v is 
v Subscript i left parenthesis i minus 1 right parenthesis divided by 2 plus j Baseline equals a Subscript i comma j Baseline comma normal f normal o normal r i greater than or equals j periodvi(i−1)/2+j = ai,j,
for i ≥j.
Notice that the relationship does not involve n. For  i greater than or equals ji ≥j, in Fortran, it is 
v(i*(i-1)/2+j) = a(i,j) 
In C, because the indexing begins at 0, the correspondence in C arrays is 
v[i*(i+1)/2+j] = a[i][j] 
In R, a symmetric matrix such as A in Eq. (12.4) is often represented in a 
vector of the form 
v overTilde equals left parenthesis 1 comma 2 comma 4 comma midline horizontal ellipsis comma 3 comma 5 comma midline horizontal ellipsis comma 6 comma midline horizontal ellipsis right parenthesis period˜v = (1, 2, 4, · · · , 3, 5, · · · , 6, · · · ).
(12.6) 
Notice that this is exactly what the normal v normal e normal c normal h left parenthesis dot right parenthesisvech(·) operator yields. (There is no vech 
function in any of the common R packages, however.) For an n times nn×n symmetric 
matrix A, the correspondence with the n left parenthesis n plus 1 right parenthesis divided by 2n(n + 1)/2-vector v overTilde˜v is 
v overTilde Subscript i plus n left parenthesis j minus 1 right parenthesis minus j left parenthesis j minus 1 right parenthesis divided by 2 Baseline equals a Subscript i comma j Baseline comma normal f normal o normal r i greater than or equals j period˜vi+n(j−1)−j(j−1)/2 = ai,j,
for i ≥j.
Notice that the relationship involves n. For a hollow matrix, such as a dis-
tance matrix (produced by the dist function in R, for example), the diagonal 
elements are not stored. 
In a band storage mode, for the n times mn×m band matrix A with lower bandwidth 
w Subscript lwl and upper bandwidth w Subscript uwu, an w Subscript l Baseline plus w Subscript u times mwl+wu×m array is used to store the elements. 
The elements are stored in the same column of the array, say A, as they are  
in the matrix; that is, 
monospace upper A left parenthesis i minus j plus w Subscript u Baseline plus 1 comma j right parenthesis equals a Subscript i comma jA(i −j + wu + 1, j) = ai,j
for i equals 1 comma 2 comma ellipsis comma w Subscript l Baseline plus w Subscript u Baseline plus 1i = 1, 2, . . . , wl + wu + 1. Band symmetric, band Hermitian, and band 
triangular modes are all deﬁned similarly. In each case, only the upper or 
lower bands are referenced. 
Storage Schemes for Sparse Matrices 
There are various schemes for representing sparse matrices. The most common 
way is the index-index-value method. This method uses three arrays, each of 
rank 1 and with length equal to the number of nonzero elements. The integer 
array i contains the row indicator, the integer array j contains the column 
indicator, and the ﬂoating-point array a contains the corresponding values; 
that is, the (i(k), j(k)) element of the matrix is stored in a(k). The arrays are 
generally sorted to improve access eﬃciency. The ﬂoating-point array can be

12.1 General Considerations
619
stored either column-major or row-major. This is also called the triplet format 
method. 
Another common scheme is the compressed sparse column format, which 
is a modiﬁcation of the index-index-value method. The nonzero values are 
stored in column-major order. A row index vector corresponds to each nonzero 
value in order, and another vector points to the beginning of each column. 
The column pointer is of length m plus 1m + 1, where  m is the number of columns 
(hence, “compressed” relative to the column indexes in the index-index-value 
method). The values in the column-pointer vector are the indexes in the row 
index vector where the given column starts, which is equivalent to the total 
number of nonzeros before the given column plus 1 (or plus 0, if the ﬁrst 
element is the zeroth element). The compressed sparse column format is com-
monly used in single-cell RNA sequencing. 
The trivial but extremely annoying problem of the beginning index (zeroth 
or ﬁrst) must often be dealt with when more than one platform is used. 
The sparse matrices in Matrix Market (page 621) use the index-index-value 
method. MATLAB generally uses the compressed sparse column format. The 
IMSL Libraries use the index-index-value method. The level 3 BLAS (see 
page 627) for sparse matrices have an argument to allow the user to specify 
the type of storage mode. The methods used in R are described on page 644. 
12.1.5 Software Development, Maintenance, and Testing 
In software development as in other production processes, good initial design 
of both the product and the process reduces the need for “acceptance sam-
pling.” Nevertheless, there is a need for thorough and systematic testing of 
software. Tests should be part of the development phase, but can continue 
throughout the life of the software. 
Test Data 
Testbeds for software consist of test datasets that vary in condition but have 
known solutions or for which there is an easy way of verifying the solution. 
Test data may be ﬁxed datasets or randomly generated datasets over some 
population with known and controllable properties. 
For testing software for some matrix computations, a very common matrix 
is the special Hankel matrix, called the Hilbert matrix H that has elements 
h Subscript i j Baseline equals StartFraction 1 Over i plus j minus 1 EndFraction periodhij =
1
i + j −1.
Hilbert matrices have large condition numbers; for example, the 10 times 1010×10 Hilbert 
matrix has condition number of order 10 Superscript 131013. The MATLAB function hilb(n) 
generates an n times nn × n Hilbert matrix. A Hilbert matrix is also a special case of 
a square Cauchy matrix with x equals left parenthesis n plus 1 comma n plus 2 comma ellipsis comma 2 n right parenthesisx = (n + 1, n + 2, . . . , 2n), y equals left parenthesis n comma n minus 1 comma ellipsis comma 1 right parenthesisy = (n, n −1, . . . , 1), 
v equals 1v = 1, and  w equals 1w = 1 (see Sect. 8.8.8 on page 428).

620
12 Software for Numerical Linear Algebra
Randomly generated test data can provide general information about the 
performance of a computational method over a range of datasets with spec-
iﬁed characteristics. Examples of studies using randomly generated datasets 
are the paper by Birkhoﬀ and Gulati (1979) on the accuracy of computed 
solutions x Subscript cxc of the linear system upper A x equals bAx = b, where  A is n times nn × n from a BMvN 
distribution, and the paper by Stewart (1980) using random matrices from a 
Haar distribution to study approximations to condition numbers (see page 349 
and Exercise 7.12). As it turns out, matrices from the BMvN distribution are 
not suﬃciently ill-conditioned often enough to be useful in studies of the accu-
racy of solutions of linear systems. Birkhoﬀ and Gulati developed a procedure 
to construct arbitrarily ill-conditioned matrices from ones with a BMvN dis-
tribution and then used these matrices in their empirical studies. 
Ericksen (1985) describes a method to generate matrices with known in-
verses in such a way that the condition numbers vary widely. To generate an 
n times nn×n matrix A, choose  x 1 comma x 2 comma ellipsis comma x Subscript n Baselinex1, x2, . . . , xn arbitrarily, except such that x 1 not equals 0x1 /= 0, and  
then take 
StartLayout 1st Row 1st Column a Subscript 1 j Baseline equals x 1 2nd Column normal f normal o normal r j equals 1 comma ellipsis comma n comma 2nd Row 1st Column a Subscript i Baseline 1 Baseline equals x Subscript i Baseline 2nd Column normal f normal o normal r i equals 2 comma ellipsis comma n comma 3rd Row 1st Column a Subscript i j Baseline equals a Subscript i comma j minus 1 Baseline plus a Subscript i minus 1 comma j minus 1 Baseline 2nd Column normal f normal o normal r i comma j equals 2 comma ellipsis comma n period EndLayout
a1j = x1
for j = 1, . . . , n,
ai1 = xi
for i = 2, . . . , n,
aij = ai,j−1 + ai−1,j−1 for i, j = 2, . . . , n.
(12.7) 
To represent the elements of the inverse, ﬁrst deﬁne y 1 equals x 1 Superscript negative 1y1 = x−1
1 , and  for  i equals 2 comma ellipsis comma ni =
2, . . . , n, 
y Subscript i Baseline equals minus y 1 sigma summation Underscript k equals 0 Overscript i minus 1 Endscripts x Subscript i minus k Baseline y Subscript k Baseline periodyi = −y1
i−1
E
k=0
xi−kyk.
Then the elements of the inverse of A, upper B equals left parenthesis b Subscript i j Baseline right parenthesisB = (bij), are  given by  
StartLayout 1st Row 1st Column b Subscript i n Baseline equals left parenthesis negative 1 right parenthesis Superscript i plus k Baseline StartBinomialOrMatrix n minus 1 Choose i minus 1 EndBinomialOrMatrix y 1 2nd Column normal f normal o normal r i equals 1 comma ellipsis comma n comma 2nd Row 1st Column Blank 3rd Row 1st Column b Subscript n j Baseline equals y Subscript n plus 1 minus j Baseline 2nd Column normal f normal o normal r j equals 1 comma ellipsis comma n minus 1 comma 4th Row 1st Column Blank 5th Row 1st Column b Subscript i j Baseline equals x 1 b Subscript i n Baseline b Subscript n j Baseline plus sigma summation Underscript k equals i plus 1 Overscript n Endscripts b Subscript k comma j plus 1 Baseline 2nd Column normal f normal o normal r i comma j equals 1 comma ellipsis comma n minus 1 comma EndLayout
bin = (−1)i+k
( n −1
i −1
)
y1
for i = 1, . . . , n,
bnj = yn+1−j
for j = 1, . . . , n −1,
bij = x1binbnj + En
k=i+1 bk,j+1 for i, j = 1, . . . , n −1,
(12.8) 
where the binomial coeﬃcient, StartBinomialOrMatrix k Choose m EndBinomialOrMatrix
(
k
m
)
, is deﬁned to be 0 if  k less than mk < m or m less than 0m < 0. 
The nonzero elements of L and U in the LU decomposition of A are eas-
ily seen to be l Subscript i j Baseline equals x Subscript i plus 1 minus jlij = xi+1−j and u Subscript i j Baseline equals StartBinomialOrMatrix j minus 1 Choose i minus 1 EndBinomialOrMatrixuij =
(
j −1
i −1
)
. The nonzero elements of 
the inverses of L and U are then seen to have left parenthesis i comma j right parenthesis(i, j) elements y Subscript i plus 1 minus jyi+1−j and 
left parenthesis negative 1 right parenthesis Superscript i minus j Baseline StartBinomialOrMatrix j minus 1 Choose i minus 1 EndBinomialOrMatrix(−1)i−j
(
j −1
i −1
)
. The determinant of A is x 1 Superscript nxn
1. 
For some choices of x 1 comma ellipsis comma x Subscript n Baselinex1, . . . , xn, it is easy to determine the condition num-
bers, especially with respect to the normal upper L 1L1 norm, of the matrices A generated in 
this way. Ericksen (1985) suggests that the xs be chosen as  
x 1 equals 2 Superscript m Baseline normal f normal o normal r m less than or equals 0x1 = 2m
for m ≤0

12.1 General Considerations
621
and 
x Subscript i Baseline equals StartBinomialOrMatrix k Choose i minus 1 EndBinomialOrMatrix normal f normal o normal r i equals 2 comma ellipsis comma n normal a normal n normal d k greater than or equals 2 commaxi =
(
k
i −1
)
for i = 2, . . . , n
and k ≥2,
in which case the normal upper L 1L1 condition number of 10 times 1010 × 10 matrices will range from 
about 10 Superscript 7107 to 10 Superscript 171017 as n ranges  from  2 to 20 for  m equals 0m = 0 and will range from 
about 10 Superscript 111011 to 10 Superscript 231023 as n ranges  from  2 to 20 for  m equals negative 1m = −1. 
For testing algorithms for computing eigenvalues, a useful matrix is a 
Wilkinson matrix, which is a symmetric, tridiagonal matrix with 1s on the 
oﬀ-diagonals. For an n times nn × n Wilkinson matrix, the diagonal elements are 
StartFraction n minus 1 Over 2 EndFraction comma StartFraction n minus 3 Over 2 EndFraction comma StartFraction n minus 5 Over 2 EndFraction comma ellipsis comma StartFraction n minus 5 Over 2 EndFraction comma StartFraction n minus 3 Over 2 EndFraction comma StartFraction n minus 1 Over 2 EndFraction periodn −1
2
, n −3
2
, n −5
2
, . . . , n −5
2
, n −3
2
, n −1
2
.
If n is odd, the diagonal includes 0; otherwise, all of the diagonal elements are 
positive. The 5 times 55 × 5 Wilkinson matrix, for example, is 
Start 5 By 5 Matrix 1st Row 1st Column 2 2nd Column 1 3rd Column 0 4th Column 0 5th Column 0 2nd Row 1st Column 1 2nd Column 1 3rd Column 1 4th Column 0 5th Column 0 3rd Row 1st Column 0 2nd Column 1 3rd Column 0 4th Column 1 5th Column 0 4th Row 1st Column 0 2nd Column 0 3rd Column 1 4th Column 1 5th Column 1 5th Row 1st Column 0 2nd Column 0 3rd Column 0 4th Column 1 5th Column 2 EndMatrix period
⎡
⎢⎢⎢⎢⎣
2 1 0 0 0
1 1 1 0 0
0 1 0 1 0
0 0 1 1 1
0 0 0 1 2
⎤
⎥⎥⎥⎥⎦
.
The two largest eigenvalues of a Wilkinson matrix are very nearly equal. Other 
pairs are likewise almost equal to each other: the third and fourth largest 
eigenvalues are also close in size, the ﬁfth and sixth largest are likewise, and 
so on. The largest pair is closest in size, and each smaller pair is less close 
in size. The MATLAB function wilkinson(n) generates an n times nn × n Wilkinson 
matrix. 
Another test matrix available in MATLAB is the Rosser test matrix, which 
is an 8 times 88 × 8 matrix with an eigenvalue of multiplicity 2 and three nearly equal 
eigenvalues. It is constructed by the MATLAB function rosser. 
A well-known, large, and wide-ranging set of test matrices for computa-
tional algorithms for various problems in linear algebra was compiled and 
described by Gregory and Karney (1969). Higham (1991, 2002) also describes 
a set of test matrices and provides MATLAB programs to generate the ma-
trices. 
Another set of test matrices is available through the “Matrix Market,” 
designed and developed by Boisvert, Pozo, and Remington of the US National 
Institute of Standards and Technology with contributions by various other 
people. The test matrices can be accessed at 
https://math.nist.gov/MatrixMarket 
The database can be searched by specifying characteristics of the test matrix, 
such as size, symmetry, and so on. Once a particular matrix is found, its 
sparsity pattern can be viewed at various levels of detail, and other pertinent 
data can be reviewed. If the matrix seems to be what the user wants, it

622
12 Software for Numerical Linear Algebra
can be downloaded. The Matrix Market contains almost 500 test problems, 
many from the Harwell-Boeing Sparse Matrix Collection. The Matrix Market 
also includes data generators for generating matrices with various speciﬁed 
properties. 
A set of test datasets for statistical analyses has been developed by the 
National Institute of Standards and Technology. This set, called “statistical 
reference datasets” (StRD), includes test datasets for linear regression, anal-
ysis of variance, nonlinear regression, Markov chain Monte Carlo estimation, 
and univariate summary statistics. It is available at 
https://www.itl.nist.gov/div898/strd/ 
Assessing the Accuracy of a Computed Result 
In real-life applications, the correct solution is not known, and this may also 
be the case for randomly generated test datasets. If the correct solution is not 
known, internal consistency tests as discussed in Sect. 11.2.3 may be used to 
assess the accuracy of the computations in a given problem. 
Software Reviews 
Reviews of available software play an important role in alerting the user to 
both good software to be used and bad software to be avoided. Software 
reviews also often have the salutary eﬀect of causing the software producers 
to improve their products. 
Software reviews vary in quality. Some reviews are rather superﬁcial and 
reﬂect mainly the overall impressions of the reviewer. For software described in 
the peer-reviewed literature, eﬀorts like the ACM’s Replicated Computational 
Results review process can improve the quality of reviews (see below). 
12.1.6 Reproducible Research 
As the research of statisticians, computer scientists, and applied mathemati-
cians has become ever more computationally oriented, the reproducibility of 
the research has become much more of an issue. Research involving only logi-
cal reasoning or mathematical manipulations is immediately reproducible by 
its essential nature. Research involving mathematical manipulations that also 
include some assumptions about underlying physical phenomena must be val-
idated by observational studies. Experiments or observational studies in the 
natural sciences, however, by long-standing tradition, must also be repro-
ducible by experimentation or through other observational studies in order to 
be accepted by other scientists. 
Reproducibility of research based on computational results requires access 
to the original data or to data substantially similar and access to the entire

12.1 General Considerations
623
workﬂow, including particularly the computational methods. Data may have 
substantial value, and the owner of the data, in order to protect that value, 
may impose strict limits on its usage and distribution. Data may also have 
conﬁdential, possibly personal, information that must be protected. Often the 
only way to ensure protection is to restrict the distribution of the data. Just 
because there are impediments to the sharing of data, however, does not mean 
that sharing of data or at least of substantially similar data or anonymized 
data is not necessary. 
The actual computations may depend on the computing system, both 
the hardware and the software. It is an easy matter to include all of the 
technical speciﬁcations in a description of the research. These include version 
numbers of published software and exact copies of any ad hoc software. In 
the case of simulation studies, reproducibility requires speciﬁcation of the 
random number generator and any settings used. GitHub, mentioned above, 
is not only a convenient tool for collaborative research; it also provides a good 
system of version control and a repository for software (and other documents) 
to be part of the published research. 
One of the main issues in ensuring that research is reproducible is the 
simple housekeeping involved in keeping correct versions of software and doc-
uments together. This can be accomplished by use of a markup language such 
as LATEX that provides rich typesetting capabilities and also has directives 
to pass code to a processing system. There are various tools that allow for 
mixing code with source text for typesetting. Two systems that are helpful in 
integrating R scripts with LATEX documents are Sweave and knitr. Xie  (2015) 
describes knitr which also works with a simple markup system called Mark-
down and distributed by RStudio. Stodden et al. (2014) and Gandrud (2015) 
discuss Sweave, knitr, and other tools for integrating programs with docu-
ments and also provide general recommendations for ensuring reproducibility 
of research. The Reproducible R Toolkit (see page 646) is useful for version 
control. 
Another major way in which the quality of research is ensured is through 
the peer review and editorial process. There is a variety of publication outlets, 
and in many, this process is not carried out in an ideal fashion. The publication 
of algorithms and computer programs is often taken rather lightly. One form 
of publication, for example, is an R package and associated documentation in 
CRAN, the Comprehensive R Archive Network. This repository is extremely 
useful, not only because of the number of high-quality packages it contains, 
but in the maintenance of the system provided by the R Project team. As the 
various packages are used in diﬀerent settings and because of the open-source 
nature of the software, there is a certain amount of quality control. Software 
bugs, however, are notoriously diﬃcult to ferret out, and some R packages are 
less than reliable. 
The ACM, through its journals such as Transactions on Mathematical 
Software (TOMS), publishes software that has at least been through a stan-
dard peer-review refereeing process. The refereeing has been enhanced with a

624
12 Software for Numerical Linear Algebra
“Replicated Computational Results” or “RCR” review process, in which a ref-
eree performs a more extensive review that includes execution of the program 
in various settings. The RCR reviewer writes a report that is accompanied by 
the name of the reviewer. The paper/software itself receives a special RCR 
designation in TOMS. See  Heroux  (2015) for a description of the process. 
12.2 Software Libraries 
There are a number of libraries of subprograms for carrying out common oper-
ations in linear algebra and other scientiﬁc computations. The libraries vary in 
several ways: free or with licensing costs or user fees; low-level computational 
modules or higher-level, more application-oriented programs; specialized or 
general purpose; and quality, from high to low. Many of these subprograms 
were originally written in Fortran and later translated into C. Now many of 
the common libraries are available in other general-purpose programming lan-
guages, such as Python. The basic libraries have also been adapted for use 
in higher-level software systems such as R and MATLAB. In the higher-level 
systems and in Python, libraries are usually called “packages.” 
In any system, when multiple packages or libraries are installed, there 
may be multiple versions of some of the components. Management of the sep-
arate packages in a single system can be carried out by a package manager 
such as Conda. Conda is written in Python and generally oriented toward 
Python packages but it can be used to manage packages for other software 
systems, including R. A version of Conda, called Anaconda, includes several 
Python packages in addition to the package manager. Anaconda and informa-
tion about it are available at 
https://www.continuum.io/ 
12.2.1 BLAS 
There are several basic computations for vectors and matrices that are very 
common across a wide range of scientiﬁc applications. Computing the dot 
product of two vectors, for example, is a task that may occur in such diverse 
areas as ﬁtting a linear model to data or determining the maximum value 
of a function. While the dot product is relatively simple, the details of how 
the computations are performed and the order in which they are performed 
can have eﬀects on both the eﬃciency and the accuracy; see the discussion 
beginning on page 553 about the order of summing a list of numbers. 
There is a widely used standard set of routines called “basic linear alge-
bra subprograms” (BLAS) that implement many of the standard operations 
for vectors and matrices. The BLAS represent a very signiﬁcant step toward 
software standardization because the deﬁnitions of the tasks and the user in-
terface are the same on all computing platforms. The BLAS can be thought of

12.2 Software Libraries
625
as “software parts” (Rice 1993). They are described in terms of the interface 
to a general-purpose programming language (an “application programming 
interface,” or API). A software part, similar to an electrical component in an 
electronic device, conforms to functional speciﬁcations, which allow an engi-
neer to construct a larger system using that part, possibly together with other 
parts. The speciﬁc “binding,” or name and list of arguments, may be diﬀerent 
for diﬀerent software systems, and certainly the actual coding may be quite 
diﬀerent to take advantage of special features of the hardware or underlying 
software such as compilers. 
The operations performed by the BLAS often cause an input variable to be 
updated. For example, in a Givens rotation, two input vectors are rotated into 
two new vectors (see rot below). In this case, it is natural and eﬃcient just to 
replace the input values with the output values. A natural implementation of 
such an operation is to use an argument that is both input and output. In some 
programming paradigms, such a “side eﬀect” can be somewhat confusing, but 
the value of this implementation outweighs the undesirable properties. 
There is a consistency of the interface among the BLAS routines. The 
nature of the arguments and their order in the reference are similar from one 
routine to the next. The general order of the arguments is: 
1. The size or shape of the vector or matrix 
2. The array itself, which may be either input or output 
3. The stride, 
4. Other input arguments 
The ﬁrst and second types of arguments are repeated as necessary for each of 
the operand arrays and the resultant array. 
A BLAS routine is identiﬁed by a root character string that indicates 
the operation, for example, dot or axpy. The name of the BLAS program 
module may depend on the programming language. In some languages, the 
root may be preﬁxed by s to indicate single precision, by d to indicate double 
precision, or by c to indicate complex, for example. If the language allows 
generic function and subroutine references, just the root of the name is used. 
The axpy operation we referred to on page 20 multiplies one vector by 
a constant and then adds another vector (a x plus yax + y). The BLAS routine axpy 
performs this operation. The interface is 
axpy(n, a, x, incx, y, incy) 
where 
n,
the number of elements in each vector 
a,
the scalar constant 
x,
input/output one-dimensional array that contains 
the elements of the vector x 
incx, the stride in the array x that deﬁnes the vector

626
12 Software for Numerical Linear Algebra
y,
the input/output one-dimensional array that contains 
the elements of the vector y 
incy, the stride in the array y that deﬁnes the vector 
As another example, the routine rot to apply a Givens rotation has the 
interface 
rot(n, x, incx, y, incy, c, s) 
where 
n,
the number of elements in each vector 
x,
the input/output one-dimensional array that contains 
the elements of the vector x 
incx, the stride in the array x that deﬁnes the vector 
y,
the input/output one-dimensional array that contains 
the elements of the vector y 
incy, the stride in the array y that deﬁnes the vector, 
c,
the cosine of the rotation 
s,
the sine of the rotation 
This routine is invoked after rotg has been called to determine the cosine and 
the sine of the rotation (see Exercise 12.5, page 647). 
Source programs and additional information about the BLAS can be ob-
tained at 
https://www.netlib.org/blas/ 
There is a software suite called ATLAS (Automatically Tuned Linear Al-
gebra Software) that provides Fortran and C interfaces to a portable BLAS 
binding as well as to other software for linear algebra for various processors. 
Information about the ATLAS software can be obtained at 
https://math-atlas.sourceforge.net/ 
Intel Math Kernel Library (MKL) is a library of optimized routines that 
takes advantage of the architecture of Intel processors, or other processors 
built on the same architecture. The functions include the BLAS, LAPACK (see 
next section), FFT, and various miscellaneous computations. The BLAS and 
LAPACK routines use the standard interfaces, so programs using BLAS and 
LAPACK can simply be relinked with the MKL. The functions are designed 
to perform multithreaded operations using all of the available cores in an 
Intel CPU. There is an R package RevoUtilsMath which provides for the 
incorporation of the Math Kernel Library into R on systems running on Intel 
or compatible processors.

12.2 Software Libraries
627
12.2.2 Level 2 and Level 3 BLAS, LAPACK, and Related Libraries 
The level 1 BLAS or BLAS-1, the original set of the BLAS, are for vector 
operations. They were deﬁned by Lawson et al. (1979). The basic operation 
is axpy, for vectors and scalars: 
alpha x plus y periodαx + y.
(Here I have adopted the notation commonly used now by the numerical an-
alysts who work on the BLAS.) General operations on a matrix and a vector 
or on two matrices can be put together using the BLAS-1. Depending on the 
architecture of the computer, greater eﬃciency can be achieved by tighter 
coupling of the computations involving all elements in a matrix without re-
gard to whether they are in the same row or same column. Dongarra et al. 
(1988) deﬁned a set of the BLAS, called level 2 or the BLAS-2, for opera-
tions involving a matrix and a vector. The basic operation is “gemv” (general 
matrix-vector), 
alpha upper A x plus beta y periodαAx + βy.
Later, a set called the level 3 BLAS or the BLAS-3, for operations involving 
two dense matrices, was deﬁned by Dongarra et al. (1990), and a set of the level 
3 BLAS for sparse matrices was proposed by Duﬀ et al. (1997). An updated 
set of BLAS is described by Blackford et al. (2002). The basic operation is 
“gemm” (general matrix-matrix), 
alpha upper A upper B plus beta upper C periodαAB + βC.
Duﬀ and V¨omel (2002) provide a set of Fortran BLAS for sparse matrices. 
When work was being done on the BLAS-1 in the 1970s, those lower-level 
routines were being incorporated into a higher-level set of Fortran routines for 
matrix eigensystem analysis called EISPACK (Smith et al. 1976) and  into  a  
higher-level set of Fortran routines for solutions of linear systems called LIN-
PACK (Dongarra et al. 1979). As work progressed on the BLAS-2 and BLAS-3 
in the 1980s and later, a uniﬁed set of Fortran routines for both eigenvalue 
problems and solutions of linear systems was developed, called LAPACK (An-
derson et al. 2000). A Fortran 95 version, LAPACK95, is described by Barker 
et al. (2001). Current information about LAPACK is available at 
https://www.netlib.org/lapack/ 
There is a graphical user interface to help the user navigate the LAPACK site 
and download LAPACK routines. 
There are several other libraries that provide functions not only for numer-
ical linear algebra but also for a wide range of scientiﬁc computations. Two 
of the most widely used proprietary Fortran and C libraries are the IMSL 
Libraries and the Nag Library. The GNU Scientiﬁc Library (GSL) is a widely 
used and freely distributed C library. See Galassi et al. (2021) and the website

628
12 Software for Numerical Linear Algebra
https://www.gnu.org/gsl/ 
Many of the GSL functions, as well as others, have been incorporated 
into the Python package called NumPy. The free Anaconda system will quickly 
install Python along with the packages. The NumPy library can be accessed 
from R with the R package reticulate by Allaire, Ushey, and Tang. 
All of these libraries provide large numbers of routines for numerical lin-
ear algebra, ranging from very basic computations as provided in the BLAS 
through complete routines for solving various types of systems of equations 
and for performing eigenanalysis. 
12.2.3 Libraries for High-Performance Computing 
Because computations for linear algebra are so pervasive in scientiﬁc appli-
cations, it is important to have very eﬃcient software for carrying out these 
computations. We have discussed several considerations for software eﬃciency 
in previous chapters. 
If the matrices in large-scale problems are sparse, it is important to take 
advantage of that sparsity both in the storage and in all computations. Soft-
ware for performing computations on sparse matrices is specialized to take 
advantage of the zero entries. Duﬀ, Heroux, and Pozo (2002) discuss issues of 
preserving sparsity during computations with sparse matrices. 
Libraries for Parallel Processing 
In parallel processing, the main issues are how to divide up the computations 
and how to share the results. A widely used standard for communication 
among processors is the message-passing interface (MPI) developed in the 
1990s. MPI allows for standardized message passing across languages and 
systems. See Gropp, Lusk, and Skjellum (2014) for a description of the MPI 
system. Current information is available at 
https://www.open-mpi.org/ 
IBM has built the Message Passing Library (MPL) in both Fortran and 
C, which provides message-passing kernels. 
A system such as MPL that implements the MPI paradigm is essentially a 
library of functions. Another approach to parallel processing is to incorporate 
language constructs into the programming language itself, thus providing a 
higher-level facility. OpenMP is a language extension for expressing parallel 
operations. See Chapman, Jost, and van der Pas (2007) for a description of 
OpenMP. Current information is available at 
https://www.openmp.org/ 
A standard set of library routines that are more application-oriented, called 
the BLACS (Basic Linear Algebra Communication Subroutines), provides a

12.2 Software Libraries
629
portable message-passing interface primarily for linear algebra computations 
with a user interface similar to that of the BLAS. A slightly higher-level set 
of routines, the PBLAS, combine both the data communication and compu-
tation into one routine, also with a user interface similar to that of the BLAS. 
Filippone and Colajanni (2000) provide a set of parallel BLAS for sparse ma-
trices. Their system, called PSBLAS, shares the general design of the PBLAS 
for dense matrices and the design of the level 3 BLAS for sparse matrices. 
ScaLAPACK, described by Blackford et al. (1997b), is a distributed mem-
ory version of LAPACK that uses the BLACS and the PBLAS modules. The 
computations in ScaLAPACK are organized as if performed in a “distributed 
linear algebra machine” (DLAM), which is constructed by interconnecting 
BLAS with a BLACS network. The BLAS perform the usual basic computa-
tions and the BLACS network exchanges data using primitive message-passing 
operations. The DLAM can be constructed either with or without a host pro-
cess. If a host process is present, it would act like a server in receiving a 
user request, creating the BLACS network, distributing the data, starting 
the BLAS processes, and collecting the results. ScaLAPACK has routines for 
LU, Cholesky, and QR decompositions and for computing eigenvalues of a 
symmetric matrix. The routines are similar to the corresponding routines in 
LAPACK. Even the names are similar, for example, in Fortran (Table 12.2): 
Table 12.2. LAPACK and ScaLAPACK 
LAPACK ScaLAPACK 
dgetrf
pdgetrf 
LU factorization 
dpotrf
pdpotrf 
Cholesky factorization 
dgeqrf
pdgeqrf 
QR factorization 
dsyevx
pdsyevx 
Eigenvalues/vectors of symmetric matrix 
Three other packages for numerical linear algebra that deserve mention 
are Trilinos, Blaze, and PLASMA. Trilinos is a collection of compatible soft-
ware packages that support parallel linear algebra computations, solution of 
linear and nonlinear equations and eigensystems of equations and related ca-
pabilities. The majority of packages are written in C++ using object-oriented 
techniques. All packages are self-contained, with the Trilinos top layer provid-
ing a common look and feel and infrastructure. 
The main Trilinos website is 
https://software.sandia.gov/trilinos/ 
All of the Trilinos packages are available on a range of platforms, especially 
on high-performance computers. 
Blaze is an open-source, high-performance C++ library for dense and 
sparse matrices. Blaze consists of a number of LAPACK wrapper functions 
that simplify the use of LAPACK.

630
12 Software for Numerical Linear Algebra
The main Blaze website is 
https://bitbucket.org/blaze-lib/blaze/ 
There is also an R package  RcppBlaze which provides for the incorporation of 
the Blaze software into R using the Rcpp package for use of C++ code in R. 
The Parallel Linear Algebra for Scalable Multicore Architectures 
(PLASMA) system is based on OpenMP multithreading directives. It in-
cludes linear systems solvers, mixed precision iterative reﬁnement, matrix 
norms (fast, multithreaded), a full set of multithreaded BLAS 3, least squares, 
and band linear solvers. See Buttari et al. (2009) for a description of the overall 
design. The functionality of PLASMA continues to increase. 
The constructs of an array-oriented language such as R or modern Fortran 
are helpful in thinking of operations in such a way that they are naturally 
parallelized. While the addition of arrays in FORTRAN 77 or C is an operation 
that leads to loops of sequential scalar operations, in modern Fortran it is 
thought of as a single higher-level operation. How to perform operations in 
parallel eﬃciently is still not a natural activity, however. For example, the 
two modern Fortran statements to add the arrays a and b and then to add  a 
and c 
d = a + b  
e = a + c  
may be less eﬃcient than loops because the array a may be accessed twice. 
Parallel Computations in R 
Parallel computations in R can be allowed via the parallel package, which 
is a component of the base system. The package contains various functions to 
manage the available cores and to determine how computations are performed. 
It implements both sockets and forking, but of course setting these up depends 
on the operating system. The Microsoft Windows operating system only allows 
sockets. Some of the functions, mclapply and mcmapply, for example, depend 
on forks and so in Windows versions of R, serial versions of these functions 
are provided. 
The code segment below, which should be self-explanatory, illustrates some 
of the functions in parallel. The code was executed on an Intel-based system 
with 8 cores running Windows. 
> library(parallel) 
> numCores <- detectCores() 
> numCores 
[1] 8 
The makeCluster function can be used to assign any number of the avail-
able cores to the executing program. The makeCluster function takes an

12.2 Software Libraries
631
argument type which can be either PSOCK or FORK. (In Windows versions of 
R, type is set automatically to FORK.) 
Graphical Processing Units 
A typical core, or central processing unit (CPU), consists of registers into 
which operands are brought, manipulated as necessary, and then combined 
in a manner simulating the mathematical operation to be performed. The 
registers themselves may be organized into a small number of “cores,” each of 
which can perform separate arithmetical operations. In the late 1990s, Nvidia 
extended this type of design to put thousands of cores on a single unit. This 
was done initially so as to update graphical images, which only requires simple 
operations on data representing states of pixels. This type of processing unit 
is called a graphical processing unit or GPU. 
Over time, the cores of a GPU were developed further so as to be able 
to perform more general arithmetic operations. Nvidia integrated GPUs with 
CPUs in a “compute uniﬁed device architecture,” or CUDA. CUDA now refers 
to a programming interface at both a low and a high level to allow Fortran 
or C programs to utilize an architecture that combines GPUs and CPUs. 
Various CUDA libraries are available, such as BLAS (cuBLAS) and routines 
for computations with sparse matrices (cuSPARSE). 
Cheng, Grossman, and McKercher (2014) provide an introduction to 
CUDA programming in C. They discuss matrix multiplication in CUDA on 
both shared-memory and unshared-memory architectures. In a CUDA imple-
mentation, each core within the GPU accumulates one entry of the product 
matrix. 
Clusters of Computers and Cloud Computing 
A cluster of computers is a very cost-eﬀective method for high-performance 
computing. A standard technology for building a cluster of Unix or Linux 
computers is called Beowulf (see Gropp, Lusk, and Sterling 2003). The indi-
vidual machines are usually identical. A standard message-passing interface 
(MPI) is used to move data (including commands) among the machines. A 
system called Pooch is available for linking Apple computers into clusters (see 
Dauger and Decyk 2005). A cluster of computers can be considered to be a 
single virtual machine. 
An extension of a cluster of computers is a cloud of computers, possibly of 
diﬀerent types and at dispersed locations. Cloud computing is characterized 
by an on-demand self-service provision of computing capabilities, which re-
quires adequate network access. The computing service is accessed through an 
API running on the user’s local machine. Cloud computing requires dynamic 
pooling of resources, including storage, memory, network bandwidth, and also

632
12 Software for Numerical Linear Algebra
any virtual machines deﬁned within the system. The resources are dynam-
ically assigned and reassigned according to consumer demand. The pooling 
must be done rapidly; that is, latency must be minimized. 
The cloud of computing resources may be owned by an organization that 
provides the cloud for a fee; hence, there must be a provision for monitoring 
and controlling resource usage. There also needs to be a reporting mechanism 
that is transparent for both the provider and the consumer. 
Amazon’s AWS and Microsoft’s Azure are the most commonly used com-
mercial cloud computing services, and it is a simple matter to set up a user 
account at either. Both cloud services provide broad ranges of computing 
resources, programs, and storage. 
12.2.4 The IMSL Libraries 
The IMSL Libraries are available in both Fortran and C versions and in both 
single and double precisions. These libraries use the BLAS and other software 
from LAPACK. Sparse matrices are generally stored in the index-index-value 
scheme, although other storage modes are also implemented. 
Examples of Use of the IMSL Libraries 
There are separate IMSL routines for single and double precisions. The names 
of the Fortran routines share a common root; the double-precision version 
has a D as its ﬁrst character, usually just placed in front of the common 
root. Functions that return a ﬂoating-point number but whose mnemonic 
root begins with an I through an N have an A in front of the mnemonic root 
for the single-precision version and have a D in front of the mnemonic root for 
the double-precision version. Likewise, the names of the C functions share a 
common root. The function name is of the form imsl f root name for single 
precision and imsl d root name for double precision. 
Consider the problem of solving the system of linear equations 
StartLayout 1st Row 1st Column x 1 plus 4 x 2 plus 7 x 3 2nd Column equals 3rd Column 10 comma 2nd Row 1st Column 2 x 1 plus 5 x 2 plus 8 x 3 2nd Column equals 3rd Column 11 comma 3rd Row 1st Column 3 x 1 plus 6 x 2 plus 9 x 3 2nd Column equals 3rd Column 12 period EndLayout x1 + 4x2 + 7x3 = 10,
2x1 + 5x2 + 8x3 = 11,
3x1 + 6x2 + 9x3 = 12.
Write the system as upper A x equals bAx = b. The coeﬃcient matrix A is real (not necessarily 
REAL) and square. We can use various IMSL subroutines to solve this problem. 
The two simplest basic routines are lslrg/dlslrg and lsarg/dlsarg. Both  
have the same set of arguments: 
n,
the problem size 
A,
the coeﬃcient matrix 
lda,
the leading dimension of A (A can be deﬁned to be bigger

12.2 Software Libraries
633
than it actually is in the given problem) 
b,
the right-hand sides 
ipath, an indicator of whether upper A x equals bAx = b or upper A Superscript normal t Baseline x equals bAtx = b is to be solved 
x,
the
 
solution
 
The diﬀerence in the two routines is whether or not they do iterative reﬁne-
ment. A program to solve the system using lsarg (without iterative reﬁne-
ment) is shown in Fig. 12.1. 
Figure 12.1. IMSL Fortran program to solve the system of linear equations 
The IMSL C function to solve this problem is lin sol gen, which is avail-
able as ﬂoat *imsl f lin sol gen or double *imsl d lin sol gen. The  only  
required arguments for *imsl f lin sol gen are: 
int n,
the problem size 
ﬂoat a[], the coeﬃcient matrix 
ﬂoat b[], the right-hand sides 
Either function will allow the array a to be larger than n, in which case the 
number of columns in a must be supplied in an optional argument. Other 
optional arguments allow the speciﬁcation of whether upper A x equals bAx = b or upper A Superscript normal upper T Baseline x equals bATx = b is 
to be solved (corresponding to the argument ipath in the Fortran subroutines 
lslrg/dlslrg and lsarg/dlsarg), the storage of the LU factorization, the 
storage of the inverse, and so on. A program to solve the system is shown in 
Fig. 12.2. Note the diﬀerence between the column orientation of Fortran and 
the row orientation of C.

634
12 Software for Numerical Linear Algebra
Figure 12.2. IMSL C program to solve the system of linear equations 
The argument IMSL A COL DIM is optional, taking the value of n, the  num-
ber of equations, if it is not speciﬁed. It is used in Fig. 12.2 only for illustration. 
12.3 General-Purpose Languages and Programming 
Systems 
Fortran, C, and Python are the most commonly used procedural languages 
for scientiﬁc computation. 
The International Organization for Standardization (ISO) has speciﬁed 
standard deﬁnitions of Fortran and C, so users’ and developers’ investments 
in these two languages have a certain amount of protection. A standard version 
of Python is controlled by the nonproﬁt Python Software Foundation. 
Many of the computations for linear algebra are implemented as simple 
operators on vectors and matrices in some interactive systems. 
Occasionally, we need to operate on vectors or matrices whose elements are 
variables. Software for symbolic manipulation, such as Maple or Mathematica, 
can perform vector/matrix operations on variables. See Exercise 12.11 on 
page 649. 
The development of accepted standards for computer languages began 
with the American National Standards Institute (ANSI). Now its international 
counterpart, ISO, is the main body developing and promoting standards for 
computer languages. Whenever ANSI and ISO both have a standard for a 
given version of a language, the standards are the same.

12.3 General-Purpose Languages and Programming Systems
635
There are various dialects of both Fortran and C, most of which result 
from “extensions” provided by writers of compilers. While these extensions 
may make program development easier and occasionally provide modest en-
hancements to execution eﬃciency, a major eﬀect of the extensions is to lock 
the user into a speciﬁc compiler. Because users usually outlive compilers, it 
is best to eschew the extensions and to program according to the ANSI/ISO 
standards. Several libraries of program modules for numerical linear algebra 
are available both in standard Fortran and in standard C. 
C began as a low-level language that provided many of the capabilities 
of a higher-level language together with more direct access to the operating 
system. It still lacks some of the facilities that are very useful in scientiﬁc 
computation. 
C++ is an object-oriented programming language built on C. The object-
oriented features make it much more useful in computing with vectors and 
matrices or other arrays and more complicated data structures. Class libraries 
can be built in C++ to provide capabilities similar to those available in For-
tran. There are also ANSI (or ISO) standard versions of C++. 
An advantage of C over Fortran is that it provides for easier communica-
tion between program units, so it is often used when larger program systems 
are being put together. Other advantages of C are the existence of widely 
available, inexpensive compilers, and the fact that it is widely taught as a 
programming language in beginning courses in computer science. 
Fortran has evolved over many years of use by scientists and engineers. 
Both ANSI and ISO have speciﬁed standard deﬁnitions of various versions of 
Fortran. A version called FORTRAN was deﬁned in 1977 (see ANSI 1978). 
We refer to this version along with a modest number of extensions as FOR-
TRAN 77. If we mean to exclude any extensions or modiﬁcations, we refer 
to it as ANSI FORTRAN 77. (Although the formal name of that version of 
the language is written in uppercase, most people and I generally write it 
in lowercase except for the ﬁrst letter.) A new standard (not a replacement 
standard) was adopted in 1990 by ANSI, at the insistence of ISO. This stan-
dard language is called ANSI Fortran 90 (written in lowercase except for the 
ﬁrst letter) or ISO Fortran 90 (see ANSI 1992). It has a number of features 
that extend its usefulness, especially in numerical linear algebra. There have 
been a few revisions of Fortran 90 in the past several years. There are only 
small diﬀerences between Fortran 90 and subsequent versions, which are called 
Fortran 95, Fortran 2000, Fortran 2003, Fortran 2008, Fortran 2015, and For-
tran 2018. Fortran 2008 introduced the concept of multiple images for parallel 
processing. Each image can be thought of as a separate program executing in 
its own environment. Data are shared by a special construct, called a coarray. 
Most of the features I discuss are in all versions after Fortran 95, which I 
will generally refer to just as “Fortran” or, possibly, “modern Fortran.” 
Modern Fortran provides additional facilities for working directly with 
arrays. For example, to add matrices A and B we can write the Fortran 
expression A+B.

636
12 Software for Numerical Linear Algebra
Compilers for Fortran are often more expensive and less widely available 
than compilers for C/C++. An open-source free compiler for Fortran 95 is 
available at 
https://www.g95.org/ 
This compiler also implements some of the newer features, such as coarrays, 
which were introduced in the later versions of Fortran 2003 and Fortran 2008. 
A disadvantage of Fortran compared with C/C++ is that fewer people 
outside of the numerical computing community know the language. 
12.3.1 Programming Considerations 
Sometimes within the execution of an iterative algorithm it is necessary to 
perform some operation outside of the basic algorithm itself. A common ex-
ample of this is in an online algorithm, in which more data must be brought 
in between the operations of the online algorithm. A simple example of this 
is the online computation of a correlation matrix using an algorithm simi-
lar to Eqs. (10.8) on page 569. When the ﬁrst observation is passed to the 
program doing the computations, that program must be told that this is the 
ﬁrst observation (or, more generally, the ﬁrst n 1n1 observations). Then, for each 
subsequent observation (or set of observations), the program must be told 
that these are intermediate observations. Finally, when the last observation 
(or set of observations, or even a null set of observations) is passed to the 
computational program, the program must be told that these are the last 
observations, and wrap-up computations must be performed (computing co-
variances and correlations from sums of squares). Between the ﬁrst and last 
invocations of the computational program, it may preserve intermediate re-
sults that are not passed back to the calling program. In this simple example, 
the communication is one-way, from calling routine to called routine. 
Reverse Communication in Iterative Algorithms 
In more complicated cases using an iterative algorithm, the computational 
routine may need more general input or auxiliary computations, and hence, 
there may be two-way communication between the calling routine and the 
called routine. A two-way communication between the routines is sometimes 
called reverse communication. An example is the repetition of a precondition-
ing step in a routine using a conjugate gradient method; as the computations 
proceed, the computational routine may detect a need for rescaling and so 
return to a calling routine to perform those services. Barrett et al. (1994) and  
Dongarra and Eijkhout (2000) describe a variety of uses of reverse communi-
cation in software for numerical linear algebra.

12.3 General-Purpose Languages and Programming Systems
637
Computational Eﬃciency 
Two seemingly trivial things can have major eﬀects on computational ef-
ﬁciency. One is movement of data between the computer’s memory and the 
computational units. How quickly this movement occurs depends, among other 
things, on the organization of the data in the computer. Multiple elements of 
an array can be retrieved from memory more quickly if they are in contiguous 
memory locations. (Location in computer memory does not necessarily refer 
to a physical place; in fact, memory is often divided into banks, and adjacent 
“locations” are in alternate banks. Memory is organized to optimize access.) 
The main reason that storage of data in contiguous memory locations aﬀects 
eﬃciency involves the diﬀerent levels of computer memory. A computer often 
has three general levels of randomly accessible memory, ranging from “cache” 
memory, which is very fast, to “disk” memory, which is relatively slower. 
When data are used in computations, they may be moved in blocks, or pages, 
from contiguous locations in one level of memory to a higher level. This allows 
faster subsequent access to other data in the same page. When one block of 
data is moved into the higher level of memory, another block is moved out. 
The movement of data (or program segments, which are also data) from one 
level of memory to another is called “paging.” 
In Fortran, a column of a matrix occupies contiguous locations, so when 
paging occurs, elements in the same column are moved. Hence, a column of 
a matrix can often be operated on more quickly in Fortran than a row of a 
matrix. In C, a row can be operated on more quickly for similar reasons. 
Some computers have array processors that provide basic arithmetic oper-
ations for vectors. The processing units are called vector registers and typically 
hold 128 or 256 full-precision ﬂoating-point numbers (see Sect. 10.2). For soft-
ware to achieve high levels of eﬃciency, computations must be organized to 
match the length of the vector processors as often as possible. 
Another thing that aﬀects the performance of software is the execution of 
loops. In the simple loop 
do i = 1, n 
sx(i) = sin(x(i)) 
end do 
it may appear that the only computing is just the evaluation of the sine of 
the elements in the vector x. In fact, a nonnegligible amount of time may be 
spent in keeping track of the loop index and in accessing memory. A compiler 
on a vector computer may organize the computations so that they are done 
in groups corresponding to the length of the vector registers. On a computer 
that does not have vector processors, a technique called “unrolling do-loops” 
is sometimes used. For the code segment above, unrolling the do-loop to a 
depth of 7, for example, would yield the following code: 
do i = 1, n, 7 
sx(i) = sin(x(i))

638
12 Software for Numerical Linear Algebra
sx(i+1) = sin(x(i+1)) 
sx(i+2) = sin(x(i+2)) 
sx(i+3) = sin(x(i+3)) 
sx(i+4) = sin(x(i+4)) 
sx(i+5) = sin(x(i+5)) 
sx(i+6) = sin(x(i+6)) 
end do 
plus a short loop for any additional elements in x beyond 7 left floor n divided by 7 right floor7[n/7]. Obviously, 
this kind of programming eﬀort is warranted only when n is large and when the 
code segment is expected to be executed many times. The extra programming 
is deﬁnitely worthwhile for programs that are to be widely distributed and 
used, such as the BLAS. 
For matrices with special patterns, the storage mode (see page 617) can  
have a major eﬀect on the computational eﬃciency, both in memory access 
and in the number of computations that can be avoided. 
12.3.2 Modern Fortran 
Fortran began as a simple language to translate common mathematical ex-
pressions into computer instructions. This heritage ensures that much of the 
language “looks like” what most people write in standard mathematics. Early 
versions of Fortran had a lot of “computerese” restrictions, however (parts of 
statements had to be in certain columns and so on). Statement grouping was 
done in an old-fashioned way, using numbers. More seriously, many things 
that we might want to do were very diﬃcult. 
Fortran has evolved, and modern Fortran is now one of the most useful 
general-purpose compiler languages for numerical computations. Some books 
that describe modern Fortran are Clerman and Spector (2012), Hanson and 
Hopkins (2013), Lemmon and Schafer (2005), Markus (2012), and Metcalf, 
Reid, and Cohen (2018). Although the current version is Fortran 2018, the 
version called Fortran 95 remains the version in common use. 
For the scientiﬁc programmer, one of the most useful features of mod-
ern Fortran is the provision of primitive constructs for vectors and matrices. 
Whereas all of the FORTRAN 77 intrinsics are scalar-valued functions, For-
tran 95 provides intrinsic array-valued functions. For example, if A and B 
represent matrices conformable for addition, the statement D = A+B performs 
the operation; if they are conformable for multiplication, the statement 
C = MATMUL(A, B) 
yields the Cayley product in C. The  MATMUL function also allows multiplication 
of vectors and matrices. 
Indexing of arrays starts at 1 by default (any starting value can be speci-
ﬁed, however), and storage is column-major. 
Space must be allocated for arrays in modern Fortran, but this can be done 
at run time. An array can be initialized either in the statement allocating the

12.3 General-Purpose Languages and Programming Systems
639
space or in a regular assignment statement. A vector can be initialized by 
listing the elements between “(/” and “/)”. This list can be generated in 
various ways. The RESHAPE function can be used to initialize matrices. 
For example, a modern Fortran statement to declare that the variable A is 
to be used as a 3 times 43 × 4 array and to allocate the necessary space is 
REAL, DIMENSION(3,4) :: A 
A Fortran statement to initialize A with the matrix 
Start 3 By 4 Matrix 1st Row 1st Column 1 2nd Column 4 3rd Column 7 4th Column 10 2nd Row 1st Column 2 2nd Column 5 3rd Column 8 4th Column 11 3rd Row 1st Column 3 2nd Column 6 3rd Column 9 4th Column 12 EndMatrix
⎡
⎣
1 4 7 10
2 5 8 11
3 6 9 12
⎤
⎦
is 
A = RESHAPE( (/ 1., 2., 3., & 
4., 5., 6., & 
7., 8., 9., & 
10.,11.,12./), & 
(/3,4/) ) 
Fortran has an intuitive syntax for referencing subarrays, shown in Ta-
ble 12.3. (See also Table 12.1 on page 616.) 
Table 12.3. Subarrays in modern Fortran 
A(2:3,1:3) The 2 times 32 × 3 submatrix in rows 2 and 3 
and columns 1 to 3 of A 
A(:,1:4:2) Refers to the submatrix with all three rows 
and the ﬁrst and third columns of A 
A(:,4)
Refers to the column vector that is the fourth column of A 
Notice that because the indexing starts with 1 (instead of 0), the corre-
spondence between the computer objects and the mathematical objects is a 
natural one. The subarrays can be used directly in functions. For example, if 
A is as shown above, and B is the matrix 
Start 4 By 2 Matrix 1st Row 1st Column 1 2nd Column 5 2nd Row 1st Column 2 2nd Column 6 3rd Row 1st Column 3 2nd Column 7 4th Row 1st Column 4 2nd Column 8 EndMatrix comma
⎡
⎢⎢⎣
1 5
2 6
3 7
4 8
⎤
⎥⎥⎦,
the Fortran function reference 
MATMUL(A(1:2,2:3), B(3:4,:))

640
12 Software for Numerical Linear Algebra
yields the Cayley product 
Start 2 By 2 Matrix 1st Row 1st Column 4 2nd Column 7 2nd Row 1st Column 5 2nd Column 8 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column 3 2nd Column 7 2nd Row 1st Column 4 2nd Column 8 EndMatrix period
[
4 7
5 8
] [
3 7
4 8
]
.
(12.9) 
Fortran also contains some of the constructs, such as FORALL, that have  
evolved to support parallel processing. 
More extensive later revisions (Fortran 2000 and subsequent versions) in-
clude such features as exception handling, better interoperability with C, al-
locatable components, parameterized derived types, object-oriented program-
ming, and coarrays. 
A good index of freely available Fortran software (and also C and C++ 
software) is 
https://www.netlib.org/utk/people/JackDongarra/la-sw.html 
12.3.3 C and C++ 
C is a computer language ideally suited to computer system operation and 
management. It is stable and widely used, meaning that an investment in C 
software is likely to continue being worthwhile. There are many libraries of C 
programs for a wide range of applications. 
Indexes of arrays in C begin at 0. (That is the way that positions in 
hardware registers are normally counted, but not the way we usually index 
mathematical entities.) Two-dimensional arrays in C are row-major, and in 
general, the indexes in multidimensional arrays as a mapping to computer 
memory vary fastest from right to left. A programmer mixing Fortran code 
with C code must be constantly aware of these two properties. 
C++ is an object-oriented dialect of C. In an object-oriented language, it 
is useful to deﬁne classes corresponding to matrices and vectors. Operators 
and/or functions corresponding to the usual operations in linear algebra can 
be deﬁned so as to allow use of simple expressions to perform these operations. 
A class library in C++ can be deﬁned in such a way that the computer 
code corresponds more closely to mathematical code. The indexes to the arrays 
can be deﬁned to start at 1, and the double index of a matrix can be written 
within a single pair of parentheses. For example, in a C++ class deﬁned for 
use in scientiﬁc computations, the left parenthesis 10 comma 10 right parenthesis(10, 10) element of the matrix A (that is, 
a Subscript 10 comma 10a10,10) can be referenced in a natural way as 
A(10,10) 
instead of as 
A[9][9] 
as it would be in ordinary C. Many computer engineers prefer the latter 
notation, however.

12.3 General-Purpose Languages and Programming Systems
641
There are various C++ class libraries or templates for matrix and vector 
computations. One is the Armadillo C++ Matrix Library, which provides a 
syntax similar to that of MATLAB. 
The Template Numerical Toolkit 
https://math.nist.gov/tnt/ 
and the Matrix Template Library 
https://www.osl.iu.edu/research/mtl/ 
are templates based on the design approach of the C++ Standard Template 
Library 
https://www.sgi.com/tech/stl/ 
Use of a C++ class library for linear algebra computations may carry a 
computational overhead that is unacceptable for large arrays. Both the Tem-
plate Numerical Toolkit and the Matrix Template Library are very eﬃcient 
computationally. 
12.3.4 Python 
Python, which is an interpretive system, is one of the most popular languages. 
It is currently the most commonly used language in academic courses to in-
troduce students to programming. There are several free Python interpreters 
for a wide range of platforms. There is an active development community that 
has produced a large number of open-source Python functions for scientiﬁc 
applications. While all language systems, such as Fortran and C, undergo re-
visions from time to time, the rate and nature of the revisions to Python have 
exceeded those of other systems. Some of these changes have been disruptive 
because of lack of backward compatibility. Many changes are useful enhance-
ments, of course. In a recent update, a Cayley multiplication operator, “@” 
(of all symbols!), was introduced. 
For persons who program in other languages, the many diﬀerences in sym-
bols and in syntax are annoying and induce programming errors. 
Python has a number of array constructs, some that act similarly to vec-
tors, some that are like sets, and so on. The native index for all of them begins 
at 0. 
One of the most useful freely distributed Python libraries is NumPy, which  
supports a wide range of functionality including general numerical linear alge-
bra, statistical applications, optimization, solutions of diﬀerential equations, 
and so on. The free Anaconda system will quickly install Python along with 
the packages. (See page 624.) 
There is also a set of Python wrappers for the Armadillo C++ Matrix 
Library.

642
12 Software for Numerical Linear Algebra
12.3.5 MATLAB and Octave 
MATLAB
R
O, or MATLAB
R
O, is a proprietary software package distributed 
by The Mathworks, Inc. It is built on an interactive, interpretive expression 
language and is one of the most widely used computer systems supporting 
numerical linear algebra and other scientiﬁc computations. 
Octave is a free open-source package that provides essentially the same core 
functionality in the same language as MATLAB. The graphical interfaces for 
Octave are more primitive than those for MATLAB and do not interact as 
seamlessly with the operating system. Most of the discussion in this section 
applies to Octave as well as to MATLAB. 
The basic object in MATLAB and Octave is a rectangular array of numbers 
(possibly complex). Scalars (even indices) are 1 times 11 × 1 matrices; equivalently, a 
1 times 11×1 matrix can be treated as a scalar. A vector is a matrix with one column. 
Statements in MATLAB are line-oriented. A statement is assumed to end 
at the end of the line, unless the last three characters on the line are periods 
(...) or a matrix is being initialized. If an assignment statement in MATLAB 
is not terminated with a semicolon, the matrix on the left-hand side of the 
assignment is printed. If a statement consists only of the name of a matrix, 
the object is printed to the standard output device (which is likely to be the 
monitor). 
The indexing of arrays in MATLAB starts with 1, and indexes are indi-
cated by “( )”. 
A matrix is initialized in MATLAB by listing the elements row-wise within 
brackets and with a semicolon marking the end of a row. (MATLAB also has 
a reshape function similar to that of Fortran that treats the matrix in a 
column-major fashion.) 
MATLAB has a number of functions to operate on vectors and matrices 
to perform such operations as various matrix factorizations, computations of 
eigenvalues and eigenvectors, solution of linear systems, and so on. Many op-
erations are performed by operators of the language. In general, the operators 
in the MATLAB language refer to the common vector/matrix operations. For 
example, the usual multiplication symbol, “*”, means Cayley multiplication 
when the operands are matrices. The meaning of an operator can often be 
changed to become the corresponding element-by-element operation by pre-
ceding the operator with a period; for example, the symbol “.*” indicates the 
Hadamard product of two matrices. The expression 
A * B  
indicates the Cayley product of the matrices, where the number of columns 
of A must be the same as the number of rows of B and the expression 
A .* B  
indicates the Hadamard product of the matrices, where the number of rows 
and columns of A must be the same as the number of rows and columns of B.

12.3 General-Purpose Languages and Programming Systems
643
The transpose of a vector or matrix is obtained by using a postﬁx operator 
“prime'”, which is the same ASCII character as the apostrophe: 
A’ 
MATLAB has special operators “\” and  “/” for solving linear systems or for 
multiplying one matrix by the inverse of another. The operator “/” means  
“divide,” as in upper A divided by upper B equals upper A upper B Superscript negative 1A/B = AB−1, while “\” means “divide from the left,” as in 
A\upper B equals upper A Superscript negative 1 Baseline upper BB = A−1B. 
While the statement 
A\B 
refers to a quantity that has the same value as the quantity indicated by 
inv(A)*B 
the computations performed are diﬀerent (and, hence, the values produced 
may be diﬀerent). The second expression is evaluated by performing the two 
operations indicated: A is inverted, and the inverse is then used as the left 
factor in matrix or matrix/vector multiplication. The ﬁrst expression, A\B, 
indicates that the appropriate computations to evaluate x in upper A x equals bAx = b should 
be performed to evaluate the expression. (Here, x and b may be matrices 
or vectors.) Another diﬀerence between the two expressions is that inv(A) 
requires A to be square and algorithmically nonsingular, whereas A\B produces 
a value that simulates upper A Superscript minus Baseline bA−b. 
MATLAB distinguishes between row vectors and column vectors. A row 
vector is a matrix whose ﬁrst dimension is 1, and a column vector is a ma-
trix whose second dimension is 1. In either case, an element of the vector is 
referenced by a single index. 
Subarrays in MATLAB are deﬁned in much the same way as in modern 
Fortran. The subarrays can be used directly in expressions. For example, the 
expression 
A(1:2,2:3) * B(3:4,:) 
yields the product 
Start 2 By 2 Matrix 1st Row 1st Column 4 2nd Column 7 2nd Row 1st Column 5 2nd Column 8 EndMatrix Start 2 By 2 Matrix 1st Row 1st Column 3 2nd Column 7 2nd Row 1st Column 4 2nd Column 8 EndMatrix
[
4 7
5 8
] [
3 7
4 8
]
as on page 640. There is one major diﬀerence in how MATLAB and Fortran 
denote sequences with a stride greater than 1, however. The upper limit and 
the stride are reversed in the triplet. For example, if x contains the elements 
1,2,3,4,5, in MATLAB, x(1:2:5) is the list 1,3,5. The same subarray in For-
tran is obtained by x(1:5:2). 
There are a number of books on MATLAB, including, for example, Att-
away (2016). Many books on numerical linear algebra use the MATLAB lan-
guage in the expositions. For example, the widely used book by Coleman and 
Van Loan (1988), while not speciﬁcally on MATLAB, shows how to perform 
matrix computations in MATLAB.

644
12 Software for Numerical Linear Algebra
Appendix: R Software for Numerical Linear Algebra 
We have discussed various features of the R software system in various places 
in this book, and a number of exercises have required the reader to use the 
system. In this section, I will brieﬂy discuss a few more features of the system. 
Sparse Matrices in R 
The Matrix R package, by Douglas Bates primarily, implements the class 
dgTMatrix, which stores sparse matrices in the index-index-value (triplet) for-
mat, and the class dgCMatrix, which stores sparse matrices in the compressed 
column format, and the class dgRMatrix, which stores sparse matrices in the 
compressed column format (see page 618). The dgCMatrix class is usually  the  
default. The initialization functions have a logical argument index1, which  if  
TRUE indicate that the indexing begins at 1 instead of at 0. 
The sparseMatrix function is used below to initialize the toy sparse ma-
trix in Eq. (8.3) on page 374. This is just to illustrate some of the R functions. 
As mentioned in the discussion on sparse matrices above, connectivity ma-
trices of this form are special because all of the nonzero values are 1. There 
are classes in Matrix that accommodate this special value (meaning it is not 
stored), but we will illustrate the general form here. 
> library(Matrix) 
> C <- sparseMatrix(i=c(4, 1,5, 1, 3, 1,4), 
+
j=c(1, 2,2, 3, 4, 5,5), 
+
x=c(1, 1,1, 1, 1, 1,1)) 
> C  
5 x 5 sparse Matrix of class "dgCMatrix" 
[1,] . 1 1 . 1 
[2,] . . . . . 
[3,] . . . 1 . 
[4,] 1 . . . 1 
[5,] . 1 . . . 
The Matrix package provides several functions to work with sparse ma-
trices in the class dgCMatrix. Some of these are specialized versions of base 
R functions, for example, qr{Matrix} performs a QR decomposition of a 
sparse matrix stored in the dgCMatrix format and returns an object of class 
sparseQR that contains matrices of class dgCMatrix along with index vectors. 
Other Mathematical and Computer Objects in R 
One of the most common non-computational operation is sorting. R does sort-
ing very eﬃciently with the sort function. A useful R function is is.unsorted, 
which checks if an object is sorted without sorting it.

12.3 General-Purpose Languages and Programming Systems
645
Symbolic computations can be performed in R using an R package, Ryacas, 
that provides access to Yacas (Yet Another Computer Algebra System) that 
is a free and open-source general-purpose computer algebra system. 
R support operations on sets. R does not have an object class of “set” 
(although we could create one), but R does provide functions for the standard 
set operations such as union and intersection and the logical operators of 
set equality and inclusion. These functions treat vectors or one-dimensional 
arrays as sets; that is, the contents are unordered, and any object is either in 
the set or not (meaning that an element cannot be in “twice”). 
> S1 <- c(1,2,3,2) 
> S2 <- c(3,2,1) 
> S3 <- c(4,3,2,1) 
> setequal(S1,S2) 
[1] TRUE 
> union(S1,S3) 
[1] 1 2 3 4 
> union(S1,S1) 
[1] 1 2 3 
> intersect(S1,S3) 
[1] 1 2 3 
> intersect(S1,S1) 
[1] 1 2 3 
> 1 %in% S1 
[1] TRUE 
> 5 %in% S1 
[1] FALSE 
The R function unique is useful for removing duplicate elements in an R 
object. 
Sharable Libraries and Rcpp 
The functionality of R can also be extended by linking functions written in 
general-purpose compiled languages into R. The general way this is done is 
by building a sharable library (or “dynamic” library) in the language and 
then loading it into R using the R function dyn.load. (The sharable library 
can be built within the R script by use of R CMD.) For Fortran and C there 
are R functions, .Fortran and .C, to access any program unit in the shared 
library after it is loaded. The arguments of these functions are the program 
unit name followed by the names of the program unit’s arguments set to the 
appropriately cast R variables. For example, if a Fortran subroutine is named 
myFun and it has a double precision argument x and an integer argument n, 
and it has been compiled and linked into a sharable library called myLib.so, 
it could be invoked in R by the statements

646
12 Software for Numerical Linear Algebra
dyn.load("myLib.so") 
... initialize relevant R variables, say xr and nr 
ans <- .Fortran("myFun", x=as.double(xr), n=as.integer(nr)) 
There is an R package, Rcpp, that facilitates interoperability between C++ 
and R. This package allows the R user to write C++ code directly in the R 
script, so it is much simpler than the sharable library approach described 
above. This package can be used to produce other R packages written in 
C++ or other compiler languages. See Eddelbuettel (2013) and also Cham-
bers (2016) and Wickham (2019) for discussion and examples of the use of 
Rcpp. There is also an R package, RcppArmadillo, that provides access to the 
functions in the Armadillo C++ Matrix Library. 
There is a Python package, RPy, that allows access of the R system from 
Python. There is also an R package, reticulate by Allaire, Ushey, and Tang, 
that allows access of the Python (and, importantly NumPy) from R.  
Other Versions of R and Other Distributions 
In addition to the standard distribution versions of R available from CRAN, 
there are various add-ons and enhancements, some also available from CRAN. 
One of the most notable of these is RStudio, which is an integrated R devel-
opment system including a sophisticated program editor. 
Microsoft R Open (MRO), formerly known as Revolution R Open (RRO), 
is an enhanced distribution of R from Microsoft Corporation. Microsoft R 
Open interfaces easily with Microsoft SQL Server, which may be used to 
manage data in an SQL environment. An important feature of Microsoft R 
Open is the use of the Intel Math Kernel Library (MKL) to allow multiple 
threads if the CPU follows the Intel multicore architecture. This can result in 
signiﬁcant speedup of standard computations for linear algebra. 
Another useful feature of Microsoft R Open is the Reproducible R Toolkit, 
which can ensure that the same version of all R packages is used consistently. 
This is done by means of a CRAN repository snapshot, so that even if some 
packages are changed, the same computations of an R program on a given 
system at a given time can be performed again on a diﬀerent system at a 
diﬀerent time. A related component of the Reproducible R Toolkit is the 
checkpoint package, which  allows  the user to go back and  forth to retrieve  
the exact versions of R packages. The user can specify the version of a package 
directly in the R code.

Exercises
647
Exercises 
12.1. In Exercise 4.14a, you were to write a function in R to ﬁnd the square 
root of a symmetric nonnegative deﬁnite matrix (see page 187). Now, 
if necessary, modify your function so that it will determine the square 
root of a nonnegative 1 × 1 matrix; add checks to your function so 
that if the matrix is not square or is not nonnegative deﬁnite, it prints 
an appropriate message and returns an appropriate value. Use your 
modiﬁed function on the matrices in Exercises 4.14b through 4.14d. 
12.2. Write a recursive function in Fortran, C, or C++ to multiply two square 
matrices using the Strassen algorithm (page 599). Write the function 
so that it uses an ordinary multiplication method if the size of the 
matrices is below a threshold that is supplied by the user. 
12.3. Set up an account on GitHub, and upload the function you wrote in 
Exercise 12.2 to your account. Share this function with another person. 
(This would probably be the instructor if you are using this book as a 
textbook in a course.) 
12.4. There are various ways to evaluate the eﬃciency of a program: count-
ing operations, checking the “wall time,” using a shell level timer, and 
using a call within the program. In C, the timing routine is ctime, 
and in modern Fortran the subroutine system clock. FORTRAN 77 
does not have a built-in timing routine, but the IMSL Fortran Library 
provides one. For this exercise, you are to write six short C programs 
and six short Fortran programs. The programs in all cases are to ini-
tialize an n × m matrix so that the entries are equal to the column 
numbers; that is, all elements in the ﬁrst column are 1s, all in the sec-
ond column are 2s, etc. The six programs arise from three matrices of 
diﬀerent sizes 10,000 × 10,000, 100 × 1,000,000, and 1,000,000 × 100 
and from two diﬀerent ways of nesting the loops: for each size matrix, 
ﬁrst nest the row loop within the column loop and then reverse the 
loops. The number of operations is the same for all programs. For each 
program, use both a shell level timer (e.g., in Unix or Linux, use time) 
and a timer called from within your program. Make a table of the times: 
10000 × 10000 100 × 1000000 1000000 × 100 
Fortran column-in-row
—
—
— 
row-in-column
—
—
— 
C
column-in-row
—
—
— 
row-in-column
—
—
— 
12.5. Obtain the BLAS routines rotg and rot for constructing and applying 
a Givens rotation. These routines exist in both Fortran and C; they are 
available in the IMSL Libraries or from CALGO (Collected Algorithms 
of the ACM; see the Bibliography).

648
12 Software for Numerical Linear Algebra
a) Using these two routines, apply a Givens rotation to the matrix used 
in Exercise 4.8 in Chap. 4, 
upper A equals Start 4 By 3 Matrix 1st Row 1st Column 3 2nd Column 5 3rd Column 6 2nd Row 1st Column 6 2nd Column 1 3rd Column 2 3rd Row 1st Column 8 2nd Column 6 3rd Column 7 4th Row 1st Column 2 2nd Column 3 3rd Column 1 EndMatrix commaA =
⎡
⎢⎢⎣
3 5 6
6 1 2
8 6 7
2 3 1
⎤
⎥⎥⎦,
so that the second column becomes (5, ˜a22, 6, 0). 
b) Write a routine in Fortran or C that accepts as input a matrix and its 
dimensions and uses the BLAS routines rotg and rot to produce 
its QR decomposition. There are several design issues you should 
address: how the output is returned (for purposes of this exercise, 
just return two arrays or pointers to the arrays in full storage mode), 
how to handle nonfull rank matrices (for this exercise, assume that 
the matrix is of full rank, so return an error message in this case), 
how to handle other input errors (what do you do if the user inputs 
a negative number for a dimension?), and others. 
12.6. Using the BLAS routines rotg and rot for constructing and applying 
a Givens rotation and the program you wrote in Exercise 12.5, write  
a Fortran or C routine that accepts a simple symmetric matrix and 
computes its eigenvalues using the mobile Jacobi scheme. The outer 
loop of your routine consists of the steps shown on page 313, and  the  
multiple actions of each of those steps can be implemented in a loop 
in serial mode. The importance of this algorithm, however, is realized 
when the actions in the individual steps on page 313 are performed in 
parallel. 
12.7. Sparse matrices. 
a) Represent the order 10 identity matrix I100 using the index-index-
value notation. (You may want to use R or the IMSL Libraries to 
ensure the correctness of the representation.) 
b) Represent the 10 × 10 type 2 tridiagonal matrix of Eq. (8.92) on  
page 422 using the index-index-value notation. 
12.8. a) Show that the Ericksen matrix (expression (12.7)) has the inverse 
shown (expression (12.8)). 
b) Show that the Ericksen matrix has determinant xn 
1. 
c) For m = −1 and  k = 2, form a 10 × 10 Ericksen matrix using the 
suggested values for the xs. (Recall the condition on the binomial 
coeﬃcients.) 
Compute the inverse, the determinant, and the L2 condition number. 
12.9. Compute the two largest eigenvalues of the 21 × 21 Wilkinson matrix 
to 15 digits. 
12.10. Develop a class library in C++ for matrix and vector operations. Dis-
cuss carefully the issues you consider in designing the class constructors. 
Design them in such a way that the references

Exercises
649
xx(1) 
YY(1,1) 
refer to the implied mathematical entities. Design the operators “+” 
and “*” so that the references 
aa + bb 
aa * bb 
will determine whether a and b are matrices and/or vectors con-
formable for the implied mathematical operations and, if so, will pro-
duce the object corresponding to the implied mathematical entity rep-
resented by the expression. 
See Eubank and Kupresanin (2012), Appendix D, for a full-blown so-
lution to this exercise. 
12.11. Use a symbolic manipulation software package such as Maple to deter-
mine the inverse of the matrix: 
Start 3 By 3 Matrix 1st Row 1st Column a 2nd Column b 3rd Column c 2nd Row 1st Column d 2nd Column e 3rd Column f 3rd Row 1st Column g 2nd Column h 3rd Column i EndMatrix period
⎡
⎣
a b c
d e f
g h i
⎤
⎦.
Determine conditions for which the matrix would be singular. (You 
can use the solve() function in Maple on certain expressions in the 
symbolic solution you obtained.) 
See also Exercise  12.15. 
12.12. Consider the 3 × 3 symmetric Toeplitz matrix with elements a, b, and  
c; that is, the matrix that looks like this: 
Start 3 By 3 Matrix 1st Row 1st Column a 2nd Column b 3rd Column c 2nd Row 1st Column b 2nd Column a 3rd Column b 3rd Row 1st Column c 2nd Column b 3rd Column a EndMatrix period
⎡
⎣
a b c
b a b
c b a
⎤
⎦.
See Exercise 8.12 on page 434. 
a) Use a symbolic manipulation software package such as Maple to 
determine the inverse of this matrix. 
See page 422. 
b) Determine conditions for which the matrix would be singular. 
R Exercises 
12.13. a) Incorporate the function you developed in Exercise 12.2 into R. 
If you used Fortran or C, ﬁrst build a sharable library. (You gen-
erally do this with a compiler directive.) Then use dyn.load and 
.Fortran or .C. If you used C++ in Exercise 12.2, use  Rcpp. 
b) Now write an R function to multiple two matrices using the 
Strassen algorithm as in Exercise 12.2.

650
12 Software for Numerical Linear Algebra
c) Now compare the eﬃciency of your R function with the Fortran, 
C, or C++ function invoked from R and with the standard R mul-
tiplication operator, %*%. (The  proc.time function in the R base 
package is probably the easiest way to do the timing.) 
12.14. Try to deﬁne an R operator "%\%" that corresponds to the "\" in 
MATLAB. What is the point of this exercise (that is, what do you 
learn)? 
12.15. Symbolic computations. 
Use R and the package Ryacas to determine the inverse of the matrix 
in Exercise 12.11. 
12.16. Sparse matrices and S4 classes. 
The Matrix package uses S4 classes. There are several diﬀerences in 
objects of an S4 class from objects with an S3 class or other R objects. 
The components of an S4 object are called slots and are referenced 
using “@”. (The @ extractor is analogous to the $ extractor for lists.) 
For more information on S4 classes, see Chambers (2008). 
Load the Matrix package, and initialize the sparse matrix C, as on 
page 644, to be of class  "dgCMatrix". 
Now compute the QR decomposition of C using the Matrix version of 
qr. This produces an object of class "sparseQR". 
Rearrange the elements in the V and R slots of the result to obtain the 
Q and R factors. (This is similar to Exercise 4.15b on page 258 for a 
QR decomposition of a dense matrix.)

Appendices

A 
Notation and Deﬁnitions 
All notations used in this work are “standard,” and I have endeavored to use 
notation consistently. I have opted for a simple notation, which, of course, 
results in a one-to-many map of notations to object classes. Within a given 
context, however, the overloaded notation is generally unambiguous. 
This appendix is not intended to be a comprehensive listing of deﬁnitions. 
The Index is a more reliable set of pointers to deﬁnitions, except for symbols 
that are not words. 
A.1 General Notation 
Uppercase italic Latin and Greek letters, such as A, B, E, normal upper LamdaΛ, etc., are generally 
used to represent either matrices or random variables. Random variables are 
usually denoted by letters nearer the end of the Latin alphabet, such X, Y , and  
Z, and by the Greek letter E. Parameters in models (that is, unobservables 
in the models), whether or not they are considered to be random variables, 
are generally represented by lowercase Greek letters. Uppercase Latin and 
Greek letters are also used to represent cumulative distribution functions. 
Also, uppercase Latin letters are used to denote sets. 
Lowercase Latin and Greek letters are used to represent ordinary scalar or 
vector variables and functions. No distinction in the notation is made 
between scalars and vectors; thus,  betaβ may represent a vector and beta Subscript iβi may 
represent the i Superscript normal t normal hith element of the vector betaβ. In another context, however, betaβ may 
represent a scalar. All vectors are considered to be column vectors, although 
we may write a vector as x equals left parenthesis x 1 comma x 2 comma ellipsis comma x Subscript n Baseline right parenthesisx = (x1, x2, . . . , xn). Transposition of a vector or a 
matrix is denoted by the superscript “Superscript normal upper TT”. 
Uppercase calligraphic Latin letters, such as script upper DD, script upper VV, and  script upper WW, are generally 
used to represent either vector spaces or transforms (functionals). 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 
653

654
Appendix A. Notation and Deﬁnitions 
Subscripts generally represent indexes to a larger structure; for example, 
x Subscript i jxij may represent the left parenthesis i comma j right parenthesis normal t normal h(i, j)th element of a matrix, X. A subscript in paren-
theses represents an order statistic. A superscript in parentheses represents 
an iteration; for example, x Subscript i Superscript left parenthesis k right parenthesisx(k)
i
may represent the value of x Subscript ixi at the k normal t normal hkth step 
of an iterative process. 
x Subscript ixi
The i normal t normal hith element of a structure (including a sample, 
which is a multiset). 
x Subscript left parenthesis i right parenthesisx(i)
The i normal t normal hith-order statistic. 
x Superscript left parenthesis i right parenthesisx(i)
The value of x at the i normal t normal hith iteration. 
Realizations of random variables and placeholders in functions associated 
with random variables are usually represented by lowercase letters correspond-
ing to the uppercase letters; thus, epsilonε may represent a realization of the random 
variable E. 
A single symbol in an italic font is used to represent a single variable. A 
Roman font or a special font is often used to represent a standard operator 
or a standard mathematical structure. Sometimes a string of symbols in a 
Roman font is used to represent an operator (or a standard function); for 
example, normal e normal x normal p left parenthesis dot right parenthesisexp(·) represents the exponential function. But a string of symbols 
in an italic font on the same baseline should be interpreted as representing 
a composition (probably by multiplication) of separate objects; for example, 
exp represents the product of e, x, and  p. Likewise a string of symbols in 
a Roman font (usually a single symbol) is used to represent a fundamental 
constant; for example, normal ee represents the base of the natural logarithm, while e 
represents a variable. 
A ﬁxed-width font is used to represent computer input or output, for ex-
ample, 
a = bx + sin(c). 
In computer text, a string of letters or numerals with no intervening spaces 
or other characters, such as bx above, represents a single object, and there is 
no distinction in the font to indicate the type of object. 
Some important mathematical structures and other objects are: 
IR
The ﬁeld of reals or the set over which that ﬁeld is 
deﬁned. 
Subscript plusIR+
The set of positive reals.

A.2 Computer Number Systems
655
normal upper I overbar normal upper R Subscript plus¯IR+
The nonnegative reals; normal upper I overbar normal upper R Subscript plus Baseline equals Subscript plus Baseline union StartSet 0 EndSet¯IR+ = IR+ ∪{0}. 
Superscript dIRd
The usual d-dimensional vector space over the reals or 
the set of all d-tuples with elements in IR. 
Superscript n times mIRn×m
The vector space of real n times mn × m matrices. 
normal upper Z normal upper ZZZ
The ring of integers or the set over which that ring is 
deﬁned. 
script upper G upper L left parenthesis n right parenthesisGL(n)
The general linear group, that is, the group of n times nn × n
full rank (real) matrices with Cayley multiplication. 
script upper O left parenthesis n right parenthesisO(n)
The orthogonal group; that is, the group of n times nn × n or-
thogonal (orthonormal) matrices with Cayley multipli-
cation. 
normal ee
The base of the natural logarithm. This is a constant; 
e may be used to represent a variable. (Note the dif-
ference in the font.) 
normal ii
The imaginary unit, StartRoot negative 1 EndRoot√−1. This is a constant; i may 
be used to represent a variable. (Note the diﬀerence in 
the font.) 
A.2 Computer Number Systems 
Computer number systems are used to simulate the more commonly used 
number systems. It is important to realize that they have diﬀerent properties, 
however. Some notation for computer number systems follows: 
normal upper I normal upper FIF
The set of ﬂoating-point numbers with a given preci-
sion, on a given computer system, or this set together 
with the four operators +, -, *, and  /. (normal upper I normal upper FIF is similar to 
IR in some useful ways; see Sect. 10.2.2 and Table 10.3 
on page 557.) 
normal upper I normal upper III
The set of ﬁxed-point numbers with a given length, on 
a given computer system, or this set together with the 
four operators +, -, *, and  /. (normal upper I normal upper III is similar to normal upper Z normal upper ZZZ in some 
useful ways; see Sect. 10.2.1.)

656
Appendix A. Notation and Deﬁnitions 
e Subscript normal m normal i normal nemin and e Subscript normal m normal a normal xemax
The minimum and maximum values of the exponent in 
the set of ﬂoating-point numbers with a given length 
(see page 536). 
epsilon Subscript normal m normal i normal nεmin and epsilon Subscript normal m normal a normal xεmax
The minimum and maximum spacings around 1 in the 
set of ﬂoating-point numbers with a given length (see 
page 538). 
epsilonε or epsilon Subscript normal m normal a normal c normal hεmach
The machine epsilon, the same as epsilon Subscript normal m normal i normal nεmin (see page 539). 
left bracket dot right bracket Subscript c[·]c
The computer version of the object dot· (see page 550). 
NA
Not available; a missing-value indicator. 
NaN
Not-a-number (see page 541). 
A.3 General Mathematical Functions and Operators 
Functions such as sin, max, span, and so on that are commonly associated 
with groups of Latin letters are generally represented by those letters in a 
Roman font. 
Operators such as d (the diﬀerential operator) that are commonly associ-
ated with a Latin letter are generally represented by that letter in a Roman 
font. 
Note that some symbols, such as StartAbsoluteValue dot EndAbsoluteValue| · |, are overloaded; such symbols are 
generally listed together below: 
times×
Cartesian or cross product of sets, or multiplication of 
elements of a ﬁeld or ring. 
StartAbsoluteValue x EndAbsoluteValue|x|
The modulus of the real or complex number x; if  x is 
real, StartAbsoluteValue x EndAbsoluteValue|x| is the absolute value of x. 
left ceiling x right ceiling[x]
The ceiling function evaluated at the real number x: 
left ceiling x right ceiling[x] is the smallest integer greater than or equal to x. 
For any x, left floor x right floor less than or equals x less than or equals left ceiling x right ceiling[x] ≤x ≤[x]. 
left floor x right floor[x]
The ﬂoor function evaluated at the real number x: left floor x right floor[x]
is the largest integer less than or equal to x. 
x factorialx!
The factorial of x. If  x is a positive integer, x factorial equals x left parenthesis x minus 1 right parenthesis midline horizontal ellipsis 2 dot 1x! = x(x −
1) · · · 2·1. For all other values except negative integers, 
x factorialx! is deﬁned by x factorial equals upper Gamma left parenthesis x plus 1 right parenthesisx! = Γ(x + 1).

A.3 General Mathematical Functions and Operators
657
normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesisO(f(n))
The order class big normal upper OO with respect to f left parenthesis n right parenthesisf(n). 
g left parenthesis n right parenthesis element of normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesisg(n) ∈O(f(n))
means there exists some ﬁxed c such that parallel to g left parenthesis n right parenthesis parallel to less than or equals c parallel to f left parenthesis n right parenthesis parallel to for all n||g(n)|| ≤
c||f(n)|| ∀n. In particular, g left parenthesis n right parenthesis element of normal upper O left parenthesis 1 right parenthesisg(n) ∈O(1) means g left parenthesis n right parenthesisg(n) is 
bounded. 
In one special case, we will use normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesisO(f(n)) to represent 
some unspeciﬁed scalar or vector x element of normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesisx ∈O(f(n)). This is  
the case of a convergent series. An example is 
s equals f 1 left parenthesis n right parenthesis plus midline horizontal ellipsis plus f Subscript k Baseline left parenthesis n right parenthesis plus normal upper O left parenthesis f left parenthesis n right parenthesis right parenthesis commas = f1(n) + · · · + fk(n) + O(f(n)),
where f 1 left parenthesis n right parenthesis comma ellipsis comma f Subscript k Baseline left parenthesis n right parenthesisf1(n), . . . , fk(n) are ﬁnite constants. 
We may also express the order class deﬁned by con-
vergence as x right arrow ax →a as normal upper O left parenthesis f left parenthesis x right parenthesis right parenthesis Subscript x right arrow aO(f(x))x→a (where a may be 
inﬁnite). Hence, g element of normal upper O left parenthesis f left parenthesis x right parenthesis right parenthesis Subscript x right arrow ag ∈O(f(x))x→a iﬀ 
limit sup Underscript x right arrow a Endscripts parallel to g left parenthesis n right parenthesis parallel to divided by parallel to f left parenthesis n right parenthesis parallel to less than normal infinity periodlim sup
x→a
||g(n)||/||f(n)|| < ∞.
normal o left parenthesis f left parenthesis n right parenthesis right parenthesiso(f(n))
Little normal oo; g left parenthesis n right parenthesis element of normal o left parenthesis f left parenthesis n right parenthesis right parenthesisg(n) ∈o(f(n)) means for all c greater than 0c > 0 there exists 
some ﬁxed N such that 0 less than or equals g left parenthesis n right parenthesis less than c f left parenthesis n right parenthesis for all n greater than or equals upper N0 ≤g(n) < cf(n) ∀n ≥N. 
(The functions f and g and the constant c could all 
also be negative, with a reversal of the inequalities.) 
Hence, g left parenthesis n right parenthesis element of normal o left parenthesis f left parenthesis n right parenthesis right parenthesisg(n) ∈o(f(n)) means parallel to g left parenthesis n right parenthesis parallel to divided by parallel to f left parenthesis n right parenthesis parallel to right arrow 0||g(n)||/||f(n)|| →0 as 
n right arrow normal infinityn →∞. 
In particular, g left parenthesis n right parenthesis element of normal o left parenthesis 1 right parenthesisg(n) ∈o(1) means g left parenthesis n right parenthesis right arrow 0g(n) →0. 
We also use normal o left parenthesis f left parenthesis n right parenthesis right parenthesiso(f(n)) to represent some unspeciﬁed 
scalar or vector x element of normal o left parenthesis f left parenthesis n right parenthesis right parenthesisx ∈o(f(n)) in special case of a con-
vergent series, as above: 
s equals f 1 left parenthesis n right parenthesis plus midline horizontal ellipsis plus f Subscript k Baseline left parenthesis n right parenthesis plus normal o left parenthesis f left parenthesis n right parenthesis right parenthesis periods = f1(n) + · · · + fk(n) + o(f(n)).
We may also express this kind of convergence in the 
form g element of normal o left parenthesis f left parenthesis x right parenthesis right parenthesis Subscript x right arrow ag ∈o(f(x))x→a as x right arrow ax →a (where a may be inﬁ-
nite). 
normal upper O Subscript upper P Baseline left parenthesis f left parenthesis n right parenthesis right parenthesisOP (f(n))
Bounded convergence in probability; upper X left parenthesis n right parenthesis element of normal upper O Subscript upper P Baseline left parenthesis f left parenthesis n right parenthesis right parenthesisX(n) ∈OP (f(n))
means that for any positive epsilonε, there is a constant upper C Subscript epsilonCε
such that sup Underscript n Endscripts normal upper P normal r left parenthesis parallel to upper X left parenthesis n right parenthesis parallel to greater than or equals upper C Subscript epsilon Baseline parallel to f left parenthesis n right parenthesis parallel to right parenthesis less than epsilonsupn Pr(||X(n)|| ≥Cε||f(n)||) < ε. 
normal o Subscript upper P Baseline left parenthesis f left parenthesis n right parenthesis right parenthesisoP (f(n))
Convergent in probability; upper X left parenthesis n right parenthesis element of normal o Subscript upper P Baseline left parenthesis f left parenthesis n right parenthesis right parenthesisX(n) ∈oP (f(n)) means 
that for any positive epsilonε, normal upper P normal r left parenthesis parallel to upper X left parenthesis n right parenthesis minus f left parenthesis n right parenthesis parallel to greater than epsilon right parenthesis right arrow 0Pr(||X(n) −f(n)|| > ε) →0
as n right arrow normal infinityn →∞. 
normal dd
The diﬀerential operator.

658
Appendix A. Notation and Deﬁnitions 
upper DeltaΔ or deltaδ
A perturbation operator; upper Delta xΔx (or delta xδx) represents a per-
turbation of x and not a multiplication of x by upper DeltaΔ (or deltaδ), 
even if x is a type of object for which a multiplication 
is deﬁned. 
upper Delta left parenthesis dot comma dot right parenthesisΔ(·, ·)
A real-valued diﬀerence function; upper Delta left parenthesis x comma y right parenthesisΔ(x, y) is a mea-
sure of the diﬀerence of x and y. For simple objects, 
upper Delta left parenthesis x comma y right parenthesis equals StartAbsoluteValue x minus y EndAbsoluteValueΔ(x, y) = |x −y|. For more complicated objects, a 
subtraction operator may not be deﬁned, and upper DeltaΔ is a 
generalized diﬀerence. 
x overTilde˜x
A perturbation of the object x; 
upper Delta left parenthesis x comma x overTilde right parenthesis equals upper Delta xΔ(x, ˜x) = Δx. 
x overTilde˜x
An average of a sample of objects generically denoted 
by x. 
x overbar¯x
The mean of a sample of objects generically denoted 
by x. 
x overbar¯x
The complex conjugate of the complex number x; that  
is, if x equals r plus normal i cx = r + ic, then  x overbar equals r minus normal i c¯x = r −ic. 
normal s normal i normal g normal n left parenthesis x right parenthesissign(x)
For the vector x, a vector of units corresponding to the 
signs: 
StartLayout 1st Row 1st Column normal s normal i normal g normal n left parenthesis x right parenthesis Subscript i 2nd Column equals 3rd Column 1 normal i normal f x Subscript i Baseline greater than 0 comma 2nd Row 1st Column Blank 2nd Column equals 3rd Column 0 normal i normal f x Subscript i Baseline equals 0 comma 3rd Row 1st Column Blank 2nd Column equals 3rd Column negative 1 normal i normal f x Subscript i Baseline less than 0 comma EndLayout
sign(x)i =
1
if xi > 0,
=
0
if xi = 0,
= −1
if xi < 0,
with a similar meaning for a scalar. 
Special Functions 
A good general reference on special functions in mathematics is the NIST 
Handbook of Mathematical Functions, edited by Olver et al. (2010). An-
other good reference on special functions is the venerable book edited by 
Abramowitz and Stegun (1964), which has been kept in print by Dover Pub-
lications. 
log xlog x
The natural logarithm evaluated at x. 
sine xsin x
The sine evaluated at x (in radians) and similarly for 
other trigonometric functions.

A.4 Linear Spaces and Matrices
659
upper Gamma left parenthesis x right parenthesisΓ(x)
The complete gamma function: upper Gamma left parenthesis x right parenthesis equals integral Subscript 0 Superscript normal infinity Baseline t Superscript x minus 1 Baseline normal e Superscript negative t Baseline normal d tΓ(x) =
f ∞
0
tx−1e−tdt. 
(This is called Euler’s integral.) Integration by parts 
immediately gives the replication formula upper Gamma left parenthesis x plus 1 right parenthesis equals x upper Gamma left parenthesis x right parenthesisΓ(x + 1) =
xΓ(x), and  so  if  x is a positive integer, upper Gamma left parenthesis x plus 1 right parenthesis equals x factorialΓ(x + 1) = x!, 
and more generally, upper Gamma left parenthesis x plus 1 right parenthesisΓ(x + 1) deﬁnes x factorialx! for all x ex-
cept negative integers. Direct evaluation of the integral 
yields upper Gamma left parenthesis 1 divided by 2 right parenthesis equals StartRoot pi EndRootΓ(1/2) = √π. Using this and the replication for-
mula, with some manipulation we get for the positive 
integer j 
upper Gamma left parenthesis j plus 1 divided by 2 right parenthesis equals StartFraction 1 dot 2 midline horizontal ellipsis left parenthesis 2 j minus 1 right parenthesis Over 2 Superscript j Baseline EndFraction StartRoot pi EndRoot periodΓ(j + 1/2) = 1 · 2 · · · (2j −1)
2j
√π.
The integral does not exist, and thus, the gamma func-
tion is not deﬁned at the nonpositive integers. 
The notation upper Gamma Subscript d Baseline left parenthesis x right parenthesisΓd(x) denotes the multivariate gamma 
function (page 235), although in other literature this 
notation denotes the incomplete univariate gamma 
function, integral Subscript 0 Superscript d Baseline t Superscript x minus 1 Baseline normal e Superscript negative t normal d t
f d
0 tx−1e−tdt. 
A.4 Linear Spaces and Matrices 
V(G)
For the set of vectors (all of the same order) G, the  
vector space generated by that set. 
V(X)
For the matrix X, the vector space generated by the 
columns of X. 
dim(V)
The dimension of the vector space V; that is,  the max-
imum number of linearly independent vectors in the 
vector space. 
span(Y )
For
 
Y either a set of vectors or a matrix, the vector 
space V(Y ) 
. 
⊥
Orthogonality relationship (vectors, see page 43; vector 
spaces, see page 44). 
V⊥
The orthogonal complement of the vector space V (see 
page 44).

660
Appendix A. Notation and Deﬁnitions 
N(A)
The null space of the matrix A; that is, the set of vec-
tors generated by all solutions, z, of the homogeneous 
system Az = 0;  N(A) is the orthogonal complement of 
V(AT ). 
tr(A)
The trace of the square matrix A, that is, the sum of 
the diagonal elements. 
rank(A)
The rank of the matrix A, that is, the maximum num-
ber of independent rows (or columns) of A. 
σ(A)
The spectrum of the matrix A (the set of [unique] 
eigenvalues). 
ρ(A)
The spectral radius of the matrix A (the maximum 
absolute value of its eigenvalues). 
A >  0 
A ≥0 
If A is a matrix, this notation means, respectively, that 
each element of A is positive or nonnegative. 
A > 0 
A > 0 
This notation means that A is a symmetric matrix and 
that it is, respectively, positive deﬁnite or nonnegative 
deﬁnite. 
AT 
For the matrix A, its transpose (also used for a vector 
to represent the corresponding row vector). 
AH 
The conjugate transpose, also called the adjoint, of the 
matrix A; 
AH = ¯AT = AT. 
A−1 
The inverse of the square, nonsingular matrix A. 
A−R 
The right inverse of the n × m matrix A, of rank  n; 
AA−R = In. The right inverse is m × n and of full 
column rank. 
A−L 
The left inverse of the  n × m matrix A, of rank  m; 
A−L A = Im. The right inverse is m × n and of full row  
rank. 
A−T 
The inverse of the transpose of the square, nonsingular 
matrix A. 
A+ 
The g4 inverse, the Moore-Penrose inverse, or the pseu-
doinverse of the matrix A (see page 151).

A.4 Linear Spaces and Matrices
661
A−
A g1, or generalized, inverse of the matrix A (see 
page 149). 
A 
1 
2
The square root of a nonnegative deﬁnite or positive 
deﬁnite matrix A; (A 
1 
2 )2 = A. 
A−1 
2
The square root of the inverse of a positive deﬁnite 
matrix A; (A−1 
2 )2 = A−1 .
O
Hadamard multiplication (see page 116). 
⊗
Kronecker multiplication (see page 116). 
⊕
The direct sum of two matrices; A ⊕ B = diag(A, B) 
(see page 82). 
⊕
Direct sum of vector spaces (see page 28). 
A.4.1 Norms and Inner Products 
Lp
For real p ≥ 1, a norm formed by accumulating the 
pth powers of the moduli of individual elements in an 
object and then taking the (1/p)th power of the result 
(see page 37).
|| · ||
In general, the norm of the object ·.
|| · ||p
In general, the Lp norm of the object ·.
||x||p
For the vector x, the  Lp norm 
parallel to x parallel to Subscript p Baseline equals left parenthesis sigma summation StartAbsoluteValue x Subscript i Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript StartFraction 1 Over p EndFraction||x||p =
(E
|xi|p) 1
p
(see page 37).
||X||p
For the matrix X, the  Lp norm 
parallel to upper X parallel to Subscript p Baseline equals max Underscript parallel to v parallel to Subscript p Baseline equals 1 Endscripts parallel to upper X v parallel to Subscript p||X||p = max
||v||p=1 ||Xv||p
(see page 189).

662
Appendix A. Notation and Deﬁnitions
||X||F
For the matrix X, the Frobenius norm 
parallel to upper X parallel to Subscript normal upper F Baseline equals StartRoot sigma summation Underscript i comma j Endscripts x Subscript i j Superscript 2 Baseline EndRoot||X||F =
/E
i,j
x2
ij
(see page 191).
||X||Fp 
For the matrix X, the  Frobenius  p norm 
parallel to upper X parallel to Subscript normal upper F Sub Subscript p Subscript Baseline equals left parenthesis sigma summation Underscript i comma j Endscripts StartAbsoluteValue x Subscript i j Baseline EndAbsoluteValue Superscript p Baseline right parenthesis Superscript 1 divided by p Baseline period||X||Fp =
⎛
⎝E
i,j
|xij|p
⎞
⎠
1/p
.
||X||Sp 
For the n × m matrix X, the Schatten p norm 
parallel to upper X parallel to Subscript normal upper S Sub Subscript p Subscript Baseline equals left parenthesis sigma summation Underscript i equals 1 Overscript min left parenthesis n comma m right parenthesis Endscripts d Subscript i Superscript p Baseline right parenthesis Superscript 1 divided by p Baseline comma||X||Sp =
⎛
⎝
min(n,m)
E
i=1
dp
i
⎞
⎠
1/p
,
where the di are the singular values of X.
<x, y>
The inner product or dot product of x and y (see 
page 34; and see page 118 for matrices). 
κp(A)
The
 
Lp condition number of the nonsingular square 
matrix A with respect to inversion (see page 263). 
A.4.2 Matrix Shaping Notation 
vecdiag(A) 
or 
diag(A) 
For the matrix A, the vector consisting of the elements 
of the principal diagonal of A; 
normal d normal i normal a normal g left parenthesis upper A right parenthesis equals normal v normal e normal c normal d normal i normal a normal g left parenthesis upper A right parenthesis equals left parenthesis a 11 comma ellipsis comma a Subscript k k Baseline right parenthesis commadiag(A) = vecdiag(A) = (a11, . . . , akk),
where k is the minimum of the number of rows and the 
number of columns of A. 
diag(v)
For the vector v, the diagonal matrix whose nonzero 
elements are those of v; that is, the square matrix, A, 
such that Aii = vi and for i /= j, Aij = 0.  
diag(A1, A2, . . . , Ak) The block diagonal matrix whose submatrices along 
the diagonal are A1, A2, . . . , Ak.

A.4 Linear Spaces and Matrices
663
vec(A)
The vector consisting of the columns of the matrix A 
all strung into one vector; if the column vectors of A 
are a1, a2, . . . , am, then  
normal v normal e normal c left parenthesis upper A right parenthesis equals left parenthesis a 1 Superscript normal upper T Baseline comma a 2 Superscript normal upper T Baseline comma ellipsis comma a Subscript m Superscript normal upper T Baseline right parenthesis periodvec(A) = (aT
1 , aT
2 , . . . , aT
m).
vech(A)
For
 
the
 
m × m symmetric matrix A, the vector con-
sisting of the lower triangular elements all strung into 
one vector: 
normal v normal e normal c normal h left parenthesis upper A right parenthesis equals left parenthesis a 11 comma a 21 comma ellipsis comma a Subscript m Baseline 1 Baseline comma a 22 comma ellipsis comma a Subscript m Baseline 2 Baseline comma ellipsis comma a Subscript m m Baseline right parenthesis periodvech(A) = (a11, a21, . . . , am1, a22, . . . , am2, . . . , amm).
A(i1,...,ik)
The matrix formed from rows i1, . . . , ik and columns 
i1, . . . , ik from a given matrix A. This kind of subma-
trix and the ones below occur often when working with 
determinants (for square matrices). If A is square, the 
determinants of these submatrices are called minors 
(see page 89). Because the principal diagonal elements 
of this matrix are principal diagonal elements of A, it  
is called a principal submatrix of A. Generally, but not 
necessarily, ij < ij+1. 
A(i1,...,ik)(j1,...,jl)
The submatrix of a given matrix A formed from rows 
i1, . . . , ik and columns j1, . . . , jl from A. 
A(i1,...,ik)(∗) 
or 
A(∗)(j1,...,jl) 
The submatrix of a given matrix A formed from rows 
i1, . . . , ik and all columns or else all rows and columns 
j1, . . . , jl from A. 
A−(i1,...,ik)(j1,...,jl)
The submatrix formed from a given matrix A by delet-
ing rows i1, . . . , ik and columns j1, . . . , jl. 
A−(i1,...,ik)() 
or 
A−()(j1,...,jl) 
The submatrix formed from a given matrix A by delet-
ing rows i1, . . . , ik (and keeping all columns) or else by 
deleting columns j1, . . . , jl from A.

664
Appendix A. Notation and Deﬁnitions 
A.4.3 Notation for Rows or Columns of Matrices 
ai∗
The vector that corresponds to the ith row of the ma-
trix A. As with all vectors, this is a column vector, so 
it often appears in the form aT 
i∗. 
a∗j
The vector that corresponds to the jth column of the 
matrix A. 
A.4.4 Notation Relating to Matrix Determinants 
|A|
The determinant of the square matrix A, 
|A| = 
det(A). 
det(A)
The determinant of the square matrix A,
det(A) =  
|A|. 
det
(
A(i1,...,ik)
)
A principal minor of a square matrix A; in this case,  it  
is the minor corresponding to the matrix formed from 
rows i1, . . . , ik and columns i1, . . . , ik from a given ma-
trix A. The notation |A(i1,...,ik)| is also used synony-
mously. 
det
(
A−(i)(j)
)
The minor associated with the (i, j)th element of a 
square matrix A. The notation |A−(i)(j)| is also used 
synonymously. 
a(ij)
The cofactor associated with the (i, j)th element of a 
square matrix A; that is,  a(ij) = (−1)i+j|A−(i)(j)|. 
adj(A)
The adjugate, also called the classical adjoint, of the 
square matrix A: adj(A) = (a(ji)); that is, the matrix 
of the same size as A formed from the cofactors of the 
elements of AT . 
A.4.5 Matrix-Vector Diﬀerentiation 
dt
The diﬀerential operator on the scalar, vector, or ma-
trix t. This is an operator; d may be used to represent  
a variable. (Note the diﬀerence in the font.)

A.4 Linear Spaces and Matrices
665
gf 
or ∇f 
For the scalar-valued function f of a vector variable, 
the vector whose ith element is ∂f/∂xi. This is the 
gradient, also often denoted as gf. 
∇f
For the vector-valued function f of a vector variable, 
the matrix whose element in position (i, j) is  
StartFraction partial differential f Subscript j Baseline left parenthesis x right parenthesis Over partial differential x Subscript i Baseline EndFraction period∂fj(x)
∂xi
.
This is also written as ∂f T /∂x or just as ∂f/∂x. This  
is the transpose of the Jacobian of f. 
Jf
For the vector-valued function f of a vector variable, 
the Jacobian of f denoted as Jf. The element in posi-
tion (i, j) is  
StartFraction partial differential f Subscript i Baseline left parenthesis x right parenthesis Over partial differential x Subscript j Baseline EndFraction period∂fi(x)
∂xj
.
This is the transpose of (∇f): Jf = (∇f)T . 
Hf 
or ∇∇f 
or ∇2 f 
The Hessian of the scalar-valued function f of a vector 
variable. The Hessian is the transpose of the Jacobian 
of the gradient. Except in pathological cases, it is sym-
metric. The element in position (i, j) is  
StartFraction partial differential squared f left parenthesis x right parenthesis Over partial differential x Subscript i Baseline partial differential x Subscript j Baseline EndFraction period∂2f(x)
∂xi∂xj
.
The symbol ∇2 f is sometimes also used to denote the 
trace of the Hessian, in which case it is called the 
Laplace operator. 
A.4.6 Special Vectors and Matrices 
1 or 1n
A vector (of length n) whose elements are all 1s. 
0 or 0n
A vector (of length n) whose elements are all 0s. 
I or In
The (n × n) identity matrix. 
ei
The ith unit vector (with implied length) (see page 24).

666
Appendix A. Notation and Deﬁnitions 
A.4.7 Elementary Operator Matrices 
Epq
The (p, q)th elementary permutation matrix (see 
page 103). 
E(π)
The permutation matrix that permutes the rows ac-
cording to the  permutation  π. 
Ep(a)
The
 
pth elementary scalar multiplication matrix (see 
page 105). 
Epq(a)
The
 
(p, q)th elementary axpy matrix (see page 105). 
A.5 Models and Data 
A form of model used often in statistics and applied mathematics has three 
parts: a left-hand side representing an object of primary interest; a function 
of another variable and a parameter, each of which is likely to be a vector; 
and an adjustment term to make the right-hand side equal the left-hand side. 
The notation varies depending on the meaning of the terms. One of the most 
common models used in statistics, the linear regression model with normal 
errors, is written as 
upper Y equals beta Superscript normal upper T Baseline x plus upper E periodY = βTx + E.
(A.1) 
The adjustment term is a random variable, denoted by an uppercase epsilon. 
The term on the left-hand side is also a random variable. This model does not 
represent observations or data. A slightly more general form is 
upper Y equals f left parenthesis x semicolon theta right parenthesis plus upper E periodY = f(x; θ) + E.
(A.2) 
A single observation or a single data item that corresponds to model (A.1) 
may be written as 
y equals beta Superscript normal upper T Baseline x plus epsilon commay = βTx + ε,
or, if it is one  of  several,  
y Subscript i Baseline equals beta Superscript normal upper T Baseline x Subscript i Baseline plus epsilon Subscript i Baseline periodyi = βTxi + εi.
Similar expressions are used for a single data item that corresponds to 
model (A.2). 
In these cases, rather than being a random variable, epsilonε or epsilon Subscript iεi may be a 
realization of a random variable, or it may just be an adjustment factor with 
no assumptions about its origin. 
A set  of  n such observations is usually represented in an n-vector y, a  
matrix X with n rows, and an n-vector epsilonε: 
y equals upper X beta plus epsilony = Xβ + ε
or 
y equals f left parenthesis upper X semicolon theta right parenthesis plus epsilon periody = f(X; θ) + ε.

Bibliography 
The following bibliography obviously covers a wide range of topics in statis-
tical computing and computational statistics. Except for a few of the general 
references, all of these entries have been cited in the text. 
Abramowitz, Milton, and Irene A. Stegun, eds. 1964. Handbook of Mathemat-
ical Functions with Formulas, Graphs, and Mathematical Tables. National  
Bureau of Standards (NIST) (Reprinted in 1965 by Dover Publications, 
Inc.) 
Ammann, Larry, and John Van Ness. 1988. A routine for converting regres-
sion algorithms into corresponding orthogonal regression algorithms. ACM 
Transactions on Mathematical Software 14: 76–87. 
Anda, Andrew A., and Haesun Park. 1994. Fast plane rotations with dynamic 
scaling. SIAM Journal of Matrix Analysis and Applications 15: 162–174. 
Anda, Andrew A., and Haesun Park. 1996. Self-scaling fast rotations for stiﬀ 
least squares problems. Linear Algebra and Its Applications 234: 137–162. 
Anderson, E., Z. Bai, C. Bischof, L.S. Blackford, J. Demmel, J. Dongarra, J. 
Du Croz, A. Greenhaum, S. Hammarling, A. McKenney, and D. Sorensen. 
2000. LAPACK Users’ Guide, 3rd ed. Society for Industrial and Applied 
Mathematics. 
Anderson, T.W. 1951. Estimating linear restrictions on regression coeﬃcients 
for multivariate normal distributions. Annals of Mathematical Statistics 22: 
327–351. 
Anderson, T.W. 2003. An Introduction to Multivariate Statistical Analysis, 
3rd ed. Wiley. 
ANSI. 1978. American National Standard for Information Systems— 
Programming Language FORTRAN, Document X3.9-1978. American Na-
tional Standards Institute. 
ANSI. 1989. American National Standard for Information Systems— 
Programming Language C, Document X3.159-1989. American National 
Standards Institute. 
© The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 
667

668
Bibliography
ANSI. 1992. American National Standard for Information Systems— 
Programming Language Fortran-90, Document X3.9-1992. American Na-
tional Standards Institute. 
ANSI. 1998. American National Standard for Information Systems— 
Programming Language C++, Document ISO/IEC 14882-1998. American 
National Standards Institute. 
Attaway, Stormy. 2016. Matlab: A Practical Introduction to Programming 
and Problem Solving, 4th ed. Butterworth-Heinemann. 
Bailey, David H. 1993. Algorithm 719: Multiprecision translation and execu-
tion of FORTRAN programs. ACM Transactions on Mathematical Software 
19: 288–319. 
Bailey, David H. 1995. A Fortran 90-based multiprecision system. ACM Trans-
actions on Mathematical Software 21: 379–387. 
Bailey, David H., King Lee, and Horst D. Simon. 1990. Using Strassen’s al-
gorithm to accelerate the solution of linear systems. Journal of Supercom-
puting 4: 358–371. 
Bapat, R.B., and T.E.S. Raghavan. 1997. Nonnegative Matrices and Applica-
tions. Cambridge University Press. 
Barker, V.A., L.S. Blackford, J. Dongarra, J. Du Croz, S. Hammarling, M. 
Marinova, J. Wasniewsk, and P. Yalamov. 2001. LAPACK95 Users’ Guide. 
Society for Industrial and Applied Mathematics. 
Barrett, R., M. Berry, T.F. Chan, J. Demmel, J. Donato, J. Dongarra, V. 
Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst. 1994. Templates for 
the Solution of Linear Systems: Building Blocks for Iterative Methods, 2nd  
ed. Society for Industrial and Applied Mathematics. 
Beaton, Albert E., Donald B. Rubin, and John L. Barone. 1976. The ac-
ceptability of regression solutions: Another look at computational accuracy. 
Journal of the American Statistical Association 71: 158–168. 
Bickel, Peter J., and Joseph A. Yahav. 1988. Richardson extrapolation and 
the bootstrap. Journal of the American Statistical Association 83: 387–393. 
Bindel, David, James Demmel, William Kahan, and Osni Marques. 2002. On 
computing Givens rotations reliably and eﬃciently. ACM Transactions on 
Mathematical Software 28: 206–238. 
Birkhoﬀ, Garrett, and Surender Gulati. 1979. Isotropic distributions of test 
matrices. Journal of Applied Mathematics and Physics (ZAMP) 30: 148– 
158. 
Bischof, Christian H. 1990. Incremental condition estimation. SIAM Journal 
of Matrix Analysis and Applications 11: 312–322. 
Bischof, Christian H., and Gregorio Quintana-Ort´ı˙1998a. Computing rank-
revealing QR factorizations. ACM Transactions on Mathematical Software 
24: 226–253. 
Bischof, Christian H., and Gregorio Quintana-Ort´ı˙1998b. Algorithm 782: 
Codes for rank-revealing QR factorizations of dense matrices. ACM Trans-
actions on Mathematical Software 24: 254–257.

Bibliography
669
Bj¨orck, ˚Ake. 1996. Numerical Methods for Least Squares Problems. Society  
for Industrial and Applied Mathematics. 
Blackford, L.S., J. Choi, A. Cleary, E. D’Azevedo, J. Demmel, I. Dhillon, J. 
Dongarra, S. Hammarling, G. Henry, A. Petitet, K. Stanley, D. Walker, and 
R.C. Whaley. 1997a. ScaLAPACK Users’ Guide. Society for Industrial and 
Applied Mathematics. 
Blackford, L.S., A. Cleary, A. Petitet, R.C. Whaley, J. Demmel, I. Dhillon, H. 
Ren, K. Stanley, J. Dongarra, and S. Hammarling. 1997b. Practical expe-
rience in the numerical dangers of heterogeneous computing. ACM Trans-
actions on Mathematical Software 23: 133–147. 
Blackford, L. Susan, Antoine Petitet, Roldan Pozo, Karin Remington, R. Clint 
Whaley, James Demmel, Jack Dongarra, Iain Duﬀ, Sven Hammarling, Greg 
Henry, Michael Heroux, Linda Kaufman, and Andrew Lumsdaine. 2002. An 
updated set of basic linear algebra subprograms (BLAS). ACM Transactions 
on Mathematical Software 28: 135–151. 
Buttari, Alfredo, Julien Langou, Jakub Kurzak, and Jack Dongarra. 2009. A 
class of parallel tiled linear algebra algorithms for multicore architectures. 
Parallel Computing 35: 38–53. 
Campbell, S.L., and C.D. Meyer, Jr. 1991. Generalized Inverses of Linear 
Transformations. Dover Publications, Inc. 
Chambers, John M. 2008. Software ﬁr Data Analysis: Programming with R. 
Springer. 
Chambers, John M. 2016. Extending R. Chapman and Hall/CRC Press. 
Chan, T.F. 1982a. An improved algorithm for computing the singular value 
decomposition. ACM Transactions on Mathematical Software 8: 72–83. 
Chan, T.F. 1982b. Algorithm 581: An improved algorithm for computing the 
singular value decomposition. ACM Transactions on Mathematical Software 
8: 84–88. 
Chan, T.F., G.H. Golub, and R.J. LeVeque. 1982. Updating formulae and 
a pairwise algorithm for computing sample variances. In Compstat 1982: 
Proceedings in Computational Statistics, ed. H. Caussinus, P. Ettinger, and 
R. Tomassone, 30–41. Physica-Verlag. 
Chan, Tony F., Gene H. Golub, and Randall J. LeVeque. 1983. Algorithms 
for computing the sample variance: Analysis and recommendations. The 
American Statistician 37: 242–247. 
Chapman, Barbara, Gabriele Jost, and Ruud van der Pas. 2007. Using 
OpenMP: Portable Shared Memory Parallel Programming. The MIT Press. 
Cheng, John, Max Grossman, and Ty McKercher. 2014. Professional CUDA 
C Programming. Wrox Press, an imprint of John Wiley and Sons. 
Clerman, Norman, and Walter Spector. 2012. Modern Fortran. Cambridge 
University Press. 
Cline, Alan K., Andrew R. Conn, and Charles F. Van Loan. 1982. Generalizing 
the LINPACK condition estimator. In Numerical Analysis, Mexico, 1981, 
ed. J.P. Hennart, 73–83. Springer.

670
Bibliography
Cline, A.K., C.B. Moler, G.W. Stewart, and J.H. Wilkinson. 1979. An estimate 
for the condition number of a matrix. SIAM Journal of Numerical Analysis 
16: 368–375. 
Cody, W.J. 1988. Algorithm 665: MACHAR: A subroutine to dynamically de-
termine machine parameters. ACM Transactions on Mathematical Software 
14: 303–329. 
Cody, W.J., and Jerome T. Coonen. 1993. Algorithm 722: Functions to sup-
port the IEEE standard for binary ﬂoating-point arithmetic. ACM Trans-
actions on Mathematical Software 19: 443–451. 
Coleman, Thomas F., and Charles Van Loan. 1988. Handbook for Matrix 
Computations. Society for Industrial and Applied Mathematics. 
Cragg, John G., and Stephen G. Donald. 1996. On the asymptotic proper-
ties of LDU-based tests of the rank of a matrix. Journal of the American 
Statistical Association 91: 1301–1309. 
Cullen, M.R. 1985. Linear Models in Biology. Halsted Press. 
Dauger, Dean E., and Viktor K. Decyk. 2005. Plug-and-play cluster com-
puting: High-performance computing for the mainstream. Computing in 
Science and Engineering 07(2): 27–33. 
Davies, Philip I., and Nicholas J. Higham. 2000. Numerically stable generation 
of correlation matrices and their factors. BIT 40: 640–651. 
Devlin, Susan J., R. Gnanadesikan, and J.R. Kettenring. 1975. Robust esti-
mation and outlier detection with correlation coeﬃcients. Biometrika 62: 
531–546. 
Dodson, David S., Roger G. Grimes, and John G. Lewis. 1991. Sparse exten-
sions to the FORTRAN basic linear algebra subprograms. ACM Transac-
tions on Mathematical Software 17: 253–263. 
Dongarra, J.J., J.R. Bunch, C.B. Moler, and G.W. Stewart. 1979. LINPACK 
Users’ Guide. Society for Industrial and Applied Mathematics. 
Dongarra, J.J., J. DuCroz, S. Hammarling, and I. Duﬀ. 1990. A set of level 3 
basic linear algebra subprograms. ACM Transactions on Mathematical Soft-
ware 16: 1–17. 
Dongarra, J.J., J. DuCroz, S. Hammarling, and R.J. Hanson. 1988. An ex-
tended set of Fortran basic linear algebra subprograms. ACM Transactions 
on Mathematical Software 14: 1–17. 
Dongarra, Jack J., and Victor Eijkhout. 2000. Numerical linear algebra algo-
rithms and software. Journal of Computational and Applied Mathematics 
123: 489–514. 
Duﬀ, Iain S., Michael A. Heroux, and Roldan Pozo. 2002. An overview of the 
sparse basic linear algebra subprograms: The new standard from the BLAS 
technical forum. ACM Transactions on Mathematical Software 28: 239–267. 
Duﬀ, Iain S., Michele Marrone, Guideppe Radicati, and Carlo Vittoli. 1997. 
Level 3 basic linear algebra subprograms for sparse matrices: A user-level 
interface. ACM Transactions on Mathematical Software 23: 379–401.

Bibliography
671
Duﬀ, Iain S., and Christof V¨omel. 2002. Algorithm 818: A reference model 
implementation of the sparse BLAS in Fortran 95. ACM Transactions on 
Mathematical Software 28: 268–283. 
Eckart, Carl, and Gale Young. 1936. The approximation of one matrix by 
another of lower rank. Psychometrika 1: 211–218. 
Eddelbuettel, Dirk. 2013. Seamless R and C++ Integration with Rcpp. 
Springer. 
Ericksen, Wilhelm S. 1985. Inverse pairs of test matrices. ACM Transactions 
on Mathematical Software 11: 302–304. 
Efron, Bradley, Trevor Hastie, Iain Johnstone, and Robert Tibshirani. 2004. 
Least angle regression. The Annals of Statistics 32: 407–499. 
Escobar, Luis A., and E. Barry Moser. 1993. A note on the updating of re-
gression estimates. The American Statistician 47: 192–194. 
Eskow, Elizabeth, and Robert B. Schnabel. 1991. Algorithm 695: Software for 
a new modiﬁed Cholesky factorization. ACM Transactions on Mathematical 
Software 17: 306–312. 
Eubank, Randall L., and Ana Kupresanin. 2012. Statistical Computing in 
C++ and R. Chapman and Hall/CRC Press. 
Filippone, Salvatore, and Michele Colajanni. 2000. PSBLAS: A library for 
parallel linear algebra computation on sparse matrices. ACM Transactions 
on Mathematical Software 26: 527–550. 
Fuller, Wayne A. 1995. Introduction to Statistical Time Series. second edition. 
Wiley. 
Galassi, Mark, Jim Davies, James Theiler, Brian Gough, Gerard Jungman, 
Patrick Alken, Michael Booth, Fabrice Rossi, and Rhys Ulerich. 2021. GNU 
Scientiﬁc Library Reference Manual, 3rd ed. Network Theory Limited. 
Gandrud, Christopher. 2015. Reproducible Research with R and R Studio, 
2nd ed. Chapman and Hall/CRC Press. 
Gantmacher, F.R. 1959. The Theory of Matrices, Volumes I and II. Trans. 
K.A. Hirsch. Chelsea. 
Gentle, James E. 2003. Random Number Generation and Monte Carlo Meth-
ods, 2nd ed. Springer. 
Gentle, James E. 2009. Computational Statistics. Springer. 
Gentle, James E. 2023. Theory of Statistics. Available freely at https://mason. 
gmu.edu/∼jgentle/books/MathStat.pdf 
Gill, Len, and Arthur Lewbel. 1992. Testing the rank and deﬁniteness of esti-
mated matrices with applications to factor, state-space and ARMA models. 
Journal of the American Statistical Association 87: 766–776. 
Golub, G., and W. Kahan. 1965. Calculating the singular values and pseudo-
inverse of a matrix. SIAM Journal of Numerical Analysis, Series B 2: 205– 
224. 
Golub, G.H., and C. Reinsch. 1970. Singular value decomposition and least 
squares solutions. Numerische Mathematik 14: 403–420. 
Golub, G.H., and C.F. Van Loan. 1980. An analysis of the total least squares 
problem. SIAM Journal of Numerical Analysis 17: 883–893.

672
Bibliography
Golub, Gene H., and Charles F. Van Loan. 1996. Matrix Computations, 3rd  
ed. The Johns Hopkins Press. 
Graybill, Franklin A. 1983. Introduction to Matrices with Applications in 
Statistics, 2nd ed. Wadsworth Publishing Company. 
Gregory, Robert T., and David L. Karney. 1969. A Collection of Matrices for 
Testing Computational Algorithms. Wiley. 
Griﬃths, P., and I.D. Hill, eds. 1985. Applied Statistics Algorithms. Ellis 
Horwood Limited. 
Griva, Igor, Stephen G. Nash, and Ariela Sofer. 2009. Linear and Nonlinear 
Optimization, 2nd ed. Society for Industrial and Applied Mathematics. 
Gropp, William, Ewing Lusk, and Anthony Skjellum. 2014. Using MPI: 
Portable Parallel Programming with the Message-Passing Interface, 3rd  ed.  
The MIT Press. 
Gropp, William, Ewing Lusk, and Thomas Sterling, eds. 2003. Beowulf Cluster 
Computing with Linux, 2nd ed. The MIT Press. 
Haag, J.B., and D.S. Watkins. 1993. QR-like algorithms for the nonsymmetric 
eigenvalue problem. ACM Transactions on Mathematical Software 19: 407– 
418. 
Hager, W.W. 1984. Condition estimates. SIAM Journal on Scientiﬁc and Sta-
tistical Computing 5: 311–316. 
Hansen, Per Christian. 1998. Rank-Deﬁcient and Discrete Ill-Posed Problems: 
Numerical Aspects of Linear Inversion. Society for Industrial and Applied 
Mathematics. 
Hanson, Richard J., and Tim Hopkins. 2013. Numerical Computing with Mod-
ern Fortran. Society for Industrial and Applied Mathematics. 
Harville, David A. 1997. Matrix Algebra from a Statistician’s Point of View. 
Springer. 
Heiberger, Richard M. 1978. Algorithm AS127: Generation of random orthog-
onal matrices. Applied Statistics 27: 199–205. 
Heroux, Michael A. 2015. Editorial: ACM TOMS replicated computational 
results initiative. ACM Transactions on Mathematical Software 41: Article 
No. 13. 
Higham, Nicholas J. 1988. FORTRAN codes for estimating the one-norm of 
a real or complex matrix, with applications to condition estimation. ACM 
Transactions on Mathematical Software 14: 381–386. 
Higham, Nicholas J. 1990. Experience with a matrix norm estimator. SIAM 
Journal on Scientiﬁc and Statistical Computing 11: 804–809. 
Higham, Nicholas J. 1991. Algorithm 694: A collection of test matrices in 
Matlab. ACM Transactions on Mathematical Software 17: 289–305. 
Higham, Nicholas J. 2002. Accuracy and Stability of Numerical Algorithms, 
2nd ed. Society for Industrial and Applied Mathematics. 
Hoﬀman, A.J., and H.W. Wielandt. 1953. The variation of the spectrum of a 
normal matrix. Duke Mathematical Journal 20: 37–39. 
Hong, H.P., and C.T. Pan. 1992. Rank-revealing QR factorization and SVD. 
Mathematics of Computation 58: 213–232.

Bibliography
673
Horn, Roger A., and Charles R. Johnson. 1991. Topics in Matrix Analysis. 
Cambridge University Press. 
IEEE. 2008. IEEE Standard for Floating-Point Arithmetic, Std 754-2008. 
IEEE, Inc. 
Jansen, Paul, and Peter Weidner. 1986. High-accuracy arithmetic software— 
Some tests of the ACRITH problem-solving routines. ACM Transactions 
on Mathematical Software 12: 62–70. 
Jolliﬀe, I.T. 2002. Principal Component Analysis, 2nd ed. New York: Springer. 
Karau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. 
Learning Spark. O’Reilly Media, Inc. 
Kearfott, R. Baker. 1996. Interval arithmetic: A Fortran 90 module for 
an interval data type. ACM Transactions on Mathematical Software 22: 
385–392. 
Kearfott, R. Baker, and Vladik Kreinovich, eds. 1996. Applications of Interval 
Computations. Kluwer.  
Kearfott, R.B., M. Dawande, K. Du, and C. Hu. 1994. Algorithm 737: INTLIB: 
A portable Fortran 77 interval standard-function library. ACM Transactions 
on Mathematical Software 20: 447–459. 
Kendall, M.G. 1961. A Course in the Geometry of n Dimensions. Charles  
Griﬃn and Company Limited. 
Kennedy, William J., and James E. Gentle. 1980. Statistical Computing. Mar-
cel Dekker, Inc. 
Kleibergen, Frank, and Richard Paap. 2006. Generalized reduced rank tests 
using the singular value decomposition. Journal of Econometrics 133: 97– 
126. 
Kollo, T˜onu, and Dietrich von Rosen. 2005. Advanced Multivariate Statistics 
with Matrices. Springer. 
Kulisch, Ulrich. 2011. Very fast and exact accumulation of products. Com-
puting 91: 397–405. 
Lawson, C.L., and R.J. Hanson. 1974. Solving Least Squares Problems. Pren-
tice Hall. (Reprinted in 1995 in the Classics in Applied Mathematics Series 
by Society for Industrial and Applied Mathematics.) 
Lawson, C.L., R.J. Hanson, D.R. Kincaid, and F.T. Krogh. 1979. Basic linear 
algebra subprograms for Fortran usage. ACM Transactions on Mathemati-
cal Software 5: 308–323. 
Lemmon, David R., and Joseph L. Schafer. 2005. Developing Statistical Soft-
ware in Fortran 95. Springer. 
Leskovec, Jure, Anand Rajaraman, and Jeﬀrey David Ullman. 2014. Mining 
of Massive Datasets, 2nd ed. Cambridge University Press. 
Linnainmaa, Seppo. 1975. Towards accurate statistical estimation of rounding 
errors in ﬂoating-point computations. BIT 15: 165–173. 
Liu, Shuangzhe, and Heinz Neudecker. 1996. Several matrix Kantorovich-type 
inequalities. Journal of Mathematical Analysis and Applications 197: 23–26.

674
Bibliography
Longley, James W. 1967. An appraisal of least squares problems for the elec-
tronic computer from the point of view of the user. Journal of the American 
Statistical Association 62: 819–841. 
Luk, F.T., and H. Park. 1989. On parallel Jacobi orderings. SIAM Journal on 
Scientiﬁc and Statistical Computing 10: 18–26. 
Magnus, Jan R., and Heinz Neudecker. 1999. Matrix Diﬀerential Calculus with 
Applications in Statistics and Econometrics, revised edition. Wiley. 
Markus, Arjen. 2012. Modern Fortran in Practice. Cambridge University 
Press. 
Metcalf, Michael, John Reid, and Malcolm Cohen. 2018. Modern Fortran Ex-
plained; Incorporating Fortran 2018. Oxford University Press. 
Meyn, Sean, and Richard L. Tweedie. 2009. Markov Chains and Stochastic 
Stability, 2nd ed. Cambridge University Press. 
Mizuta, Masahiro. 2012. Dimension reduction methods. In Handbook of Com-
putational Statistics: Concepts and Methods, second revised and updated 
edition, ed. James E. Gentle, Wolfgang H¨ardle, and Yuichi Mori, 619–644. 
Springer. 
Moore, E.H. 1920. On the reciprocal of the general algebraic matrix. Bulletin 
of the American Mathematical Society. 26: 394–395. 
Mosteller, Frederick, and David L. Wallace. 1963. Inference in an authorship 
problem. Journal of the American Statistical Association 58: 275–309. 
Muirhead, Robb J. 1982. Aspects of Multivariate Statistical Theory. Wiley. 
Nachbin, Leopoldo. 1965. The Haar Integral. Trans. Lulu Bechtolsheim. D. 
Van Nostrand Co Inc. 
Nash, John C. 2022. Function minimization and nonlinear least squares in R. 
WIREs Computational Statistics 14(6). 
Olshevsky, Vadim, ed. 2003. Fast Algorithms for Structured Matrices: Theory 
and Applications. American Mathematical Society. 
Olver, Frank W.J., Daniel W. Lozier, Ronald F. Boisvert, and Charles W. 
Clark. 2010. NIST Handbook of Mathematical Functions. Cambridge Uni-
versity Press. 
Overton, Michael L. 2001. Numerical Computing with IEEE Floating Point 
Arithmetic. Society for Industrial and Applied Mathematics. 
Penrose, R. 1955. A generalized inverse for matrices. Proceedings of the Cam-
bridge Philosophical Society 51: 406–413. 
Rice, John R. 1993. Numerical Methods, Software, and Analysis, 2nd  ed.  
McGraw-Hill Book Company. 
Robin, J.M., and R.J. Smith. 2000. Tests of rank. Econometric Theory 16: 
151–175. 
Rousseeuw, Peter J., and Geert Molenberghs. 1993. Transformation of non-
positive semideﬁnite correlation matrices. Communications in Statistics— 
Theory and Methods 22: 965–984. 
Saad, Y., and M.H. Schultz. 1986. GMRES: A generalized minimal residual 
algorithm for solving nonsymmetric linear systems. SIAM Journal on Sci-
entiﬁc and Statistical Computing 7: 856–869.

Bibliography
675
Schott, James R. 2004. Matrix Analysis for Statistics, 2nd ed. Wiley. 
Searle, Shayle R. 1982. Matrix Algebra Useful for Statistics. Wiley. 
Shao, Jun. 2003. Mathematical Statistics, 2nd ed. Springer. 
Sherman, J., and W.J. Morrison. 1950. Adjustment of an inverse matrix cor-
responding to a change in one element of a given matrix. Annals of Math-
ematical Statistics 21: 124–127. 
Smith, B.T., J.M. Boyle, J.J. Dongarra, B.S. Garbow, Y. Ikebe, V.C. Klema, 
and C.B. Moler. 1976. Matrix Eigensystem Routines—EISPACK Guide. 
Springer. 
Stallings, W.T., and T.L. Boullion. 1972. Computation of pseudo-inverse using 
residue arithmetic. SIAM Review 14: 152–163. 
Stewart, G.W. 1980. The eﬃcient generation of random orthogonal matrices 
with an application to condition estimators. SIAM Journal of Numerical 
Analysis 17: 403–409. 
Stodden, Victoria, Friedrich Leisch, and Roger D. Peng. 2014. Implementing 
Reproducible Research. Chapman and Hall/CRC Press. 
Strassen, V. 1969. Gaussian elimination is not optimal. Numerische Mathe-
matik 13: 354–356. 
Tanner, M.A., and R.A. Thisted. 1982. A remark on AS127. Generation of 
random orthogonal matrices. Applied Statistics 31: 190–192. 
Trosset, Michael W. 2002. Extensions of classical multidimensional scaling via 
variable reduction. Computational Statistics 17: 147–163. 
Unicode Consortium. 1990. The Unicode Standard, Worldwide Character En-
coding, Version 1.0, Volume 1. Addison-Wesley Publishing Company. 
Unicode Consortium. 1992. The Unicode Standard, Worldwide Character En-
coding, Version 1.0, Volume 2. Addison-Wesley Publishing Company. 
Walster, G. William. 1996. Stimulating hardware and software support for 
interval arithmetic. In Applications of Interval Computations, ed. R. Baker 
Kearfott and Vladik Kreinovich, 405–416. Kluwer. 
Walster, G. William. 2005. The use and implementation of interval data types. 
In Accuracy and Reliability in Scientiﬁc Computing, ed. Bo Einarsson, 173– 
194. 
Watkins, David S. 2002. Fundamentals of Matrix Computations, 2nd  ed. Wi-
ley. 
White, Halbert. 1980. A heteroskedasticity-consistent covariance matrix esti-
mator and a direct test for heteroskedasticity. Econometrica 48: 817–838. 
White, Tom. 2015. Hadoop: The Deﬁnitive Guide, 4th ed. O’Reilly Media, 
Inc. 
Wickham, Hadley. 2016. ggplot2. Elegant Graphics for Data Analysis, 2nd  ed.  
New York: Springer. 
Wickham, Hadley. 2019. Advanced R, 2nd ed. Chapman and Hall/CRC Press. 
Wilkinson, J.H. 1959. The evaluation of the zeros of ill-conditioned polyno-
mials. Numerische Mathematik 1: 150–180. 
Wilkinson, J.H. 1963. Rounding Errors in Algebraic Processes. Prentice-Hall 
(Reprinted by Dover Publications, Inc., 1994).

676
Bibliography
Wilkinson, J.H. 1965. The Algebraic Eigenvalue Problem. Oxford University 
Press. 
Woodbury, M.A. 1950. Inverting Modiﬁed Matrices, Memorandum Report 42, 
Statistical Research Group. Princeton University. 
Wynn, P. 1962. Acceleration techniques for iterated vector and matrix prob-
lems. Mathematics of Computation 16: 301–322. 
Xie, Yihui. 2015. Dynamic Documents with R and knitr, 2nd ed. Chapman 
and Hall/CRC Press. 
Zhou, Bing Bing, and Richard P. Brent. 2003. An eﬃcient method for comput-
ing eigenvalues of a real normal matrix. Journal of Parallel and Distributed 
Computing 63: 638–648.

Index 
A 
absolute error, 551, 562, 596 
ACM Transactions on Mathematical 
Software, 624 
normal a normal d normal j left parenthesis dot right parenthesisadj(·), 91 
adjacency matrix, 371–374, 429 
Exercise 8.20:, 434 
augmented, 430 
adjoint (see also conjugate transpose), 
78 
adjoint, classical (see also adjugate), 
91 
adjugate, 91, 111 
adjugate and inverse, 139 
aﬃne group, 134, 210 
aﬃne space, 53 
aﬃne transformation, 220 
Aitken’s integral, 347 
script upper A upper L left parenthesis dot right parenthesisAL(·) (aﬃne group), 134 
algebraic multiplicity, 167 
algorithm, 260, 566–581 
batch, 578 
deﬁnition, 576 
direct, 260 
divide and conquer, 573 
greedy, 574 
iterative, 260, 576–578, 636 
reverse communication, 636 
online, 578 
out-of-core, 578 
real-time, 578 
algorithmically singular, 143 
Anaconda, 624, 628, 641 
normal a normal n normal g normal l normal e left parenthesis dot comma dot right parenthesisangle(·, ·), 47 
angle between matrices, 192 
angle between vectors, 47, 57, 221, 397 
directional, 57 
ANSI (standards), 529, 543, 634, 635 
antisymmetric matrix, 74 
Apache Software Foundation, 528, 532 
approximation and estimation, 479 
approximation of a matrix, 198, 249, 
379, 485, 602 
approximation of a vector, 51–53 
arithmetic mean, 45, 47 
Arnoldi method, 316 
artiﬁcial ill-conditioning, 265 
ASCII code, 529 
association matrix, 368–378, 397, 404, 
405, 408–411 
adjacency matrix, 371–372, 429 
connectivity matrix, 371–372, 429 
dissimilarity matrix, 408 
distance matrix, 408 
incidence matrix, 371–372, 429 
similarity matrix, 408 
ATLAS (Automatically Tuned Linear 
Algebra Software), 626 
© The Editor(s) (if applicable) and The Author(s), under exclusive 
license to Springer Nature Switzerland AG 2024 
J. E. Gentle, Matrix Algebra, Springer Texts in Statistics, 
https://doi.org/10.1007/978-3-031-42144-0 
677

678
Index
augmented adjacency matrix, 430 
augmented connectivity matrix, 430 
Automatically Tuned Linear Algebra 
Software (ATLAS), 626 
autoregressive process, 491–510 
axpy, 20, 61, 105, 625, 627 
axpy elementary operator matrix, 105 
B 
back substitution, 270, 447 
backward error analysis, 562, 568 
Banachiewicz factorization, 246 
Banach space, 43 
banded matrix, 76 
inverse, 142 
Bartlett decomposition, 386 
base, 536 
base point, 535 
basis, 31–32 
Exercise 2.19:, 68 
orthonormal, 50–51 
batch algorithm, 578 
Bauer-Fike theorem, 305 
Beowulf (cluster computing), 631 
bias, in exponent of ﬂoating-point 
number, 536 
big endian, 548 
big integer, 534, 559 
big O (order), 564, 570 
big omega (order), 565 
bilinear form, 112, 156 
bit, 529 
bitmap, 530 
BLACS (software), 628, 629 
BLAS (software), 624–627 
CUDA, 631 
PBLAS, 629 
PLASMA, 630 
PSBLAS, 629 
block diagonal matrix, 81 
determinant of, 87 
inverse of, 142 
multiplication, 100 
BLUE (regression estimators), 465 
BMvN distribution, 349, 620 
Bolzano–Weierstrass theorem for 
orthogonal matrices, 156 
Boolean matrix, 429 
Box M statistic, 408 
bradot·ket notation, 34 
byte, 529 
C 
C (programming language), 542, 558, 
640–641 
C++ (programming language), 543, 
640–641 
cancellation error, 555, 568 
canonical form, equivalent, 135 
canonical form, similar, 172 
canonical singular value factorization, 
183 
Cartesian geometry, 45, 94 
catastrophic cancellation, 554 
Cauchy matrix, 428 
Cauchy–Schwarz inequality, 33, 120 
Cauchy–Schwarz inequality for matrices, 
120, 210 
Cayley-Hamilton theorem, 161 
Cayley multiplication, 96, 115 
CDF (Common Data Format), 531 
centered matrix, 284, 403 
centered vector, 60 
chaining of operations, 552 
change-of-variables method, 345 
character data, 530 
characteristic equation, 160 
characteristic function, 342 
characteristic polynomial, 160 
characteristic value (see also eigenvalue), 
157 
characteristic vector (see also eigenvec-
tor), 157 
character string, 530 
chasing, 315 
Chebyshev norm, 37 
chi-squared distribution, 352 
noncentral, 353 
PDF, equation (7.51), 353 
Cholesky decomposition, 245–248, 386, 
485 
computing, 629 
root-free, 246 
circulant matrix, 423 
classiﬁcation, 429 
cloud computing, 631 
cluster analysis, 429 
cluster computing, 631

Index
679
coarray (Fortran construct), 635 
Cochran’s theorem, 353, 393–396 
cofactor, 90 
collinearity, 261, 443, 478 
column-major, 592, 610, 616 
column rank, 121 
column space, 73, 101, 126, 127 
column-sum norm, 189 
Common Data Format (CDF), 531 
companion matrix, 162, 303 
compatible linear systems, 127 
compensated summation, 553 
complementary minor, 90 
complementary projection matrix, 396 
complementary vector spaces, 29 
complete graph, 369 
complete pivoting, 271 
complete space, 43 
completing the Gramian, 200 
complex numbers, vectors, and matrices, 
43, 155, 426 
compressed sparse column format 
(sparse matrices), 619 
computer algebra, 613, 645 
Conda, 624 
condition (problem or data), 566 
conditional inverse, 149 
condition number, 261, 267, 286, 474, 
475, 567, 569, 593, 603 
computing the number, 603 
inverse of matrix, 263, 267 
nonfull rank matrices, 286 
nonsquare matrices, 286 
sample standard deviation, 569 
cone, 53–56, 367 
convex cone, 54, 386 
Exercise 2.20:, 68 
Lorentz cone, 54 
of nonnegative deﬁnite matrices, 386 
of nonnegative matrices, 411 
of positive deﬁnite matrices, 389 
of positive matrices, 411 
conference matrix, 421 
conﬁguration matrix, 409 
conjugate gradient method, 275–279 
preconditioning, 278 
conjugate norm, 115 
conjugate transpose, 78, 155 
conjugate vectors, 115, 156 
connected vertices, 369, 374 
connectivity matrix, 371–374, 429 
augmented, 430 
Exercise 8.20:, 434 
consistency property of matrix norms, 
187 
consistency test, 596, 622 
consistent system of equations, 127, 268, 
273 
constrained least squares, equality 
constraints, 451 
Exercise 9.3d:, 518 
continuous function, 326 
contrast, 463 
convergence criterion, 576 
convergence of a sequence of matrices, 
156, 175, 195 
convergence of a sequence of vectors, 42 
convergence of powers of a matrix, 195, 
415 
convergence rate, 576 
convex combination, 20 
convex cone, 54–56, 386, 389, 411 
convex function, 35, 495 
convexity, 35 
convex optimization, 386 
convex set, 20 
coordinate, 3 
correlation, 62 
correlation matrix, 405, 470 
Exercise 7.13:, 363 
positive deﬁnite approximation, 484 
pseudo-correlation matrix, 485 
sample, 470 
Cor left parenthesis dot comma dot right parenthesis(·, ·), 62 
cost matrix, 409 
covariance matrix, see variance-
covariance matrix 404 
Covleft parenthesis dot comma dot right parenthesis(·, ·), 61 
CRAN (Comprehensive R Archive 
Network), 10, 623 
covariance, 61 
cross product of vectors, 57 
Exercise 2.21:, 69 
cross products, computing sum of 
Exercise 10.17c:, 587 
cross products matrix, 248, 398 
Crout method, 237 
cuBLAS, 631

680
Index
CUDA, 631 
curl, 333 
curse of dimensionality, 578 
cuSPARSE, 631 
D 
data frame, 512 
daxpy, 20 
decomposable matrix, 413 
decomposition, see also factorization of 
a matrix 
additive, 394 
Bartlett decomposition, 386 
Cholesky, 245 
full rank decomposition, 131 
multiplicative, 131 
nonnegative matrix factorization, 
249, 376 
rank decomposition, 131 
singular value decomposition, 
182–185, 318, 376, 473, 602 
spectral decomposition, 177 
defective (deﬁcient) matrix, 172, 173 
deﬁcient (defective) matrix, 172, 173 
deﬂation, 306–308 
degrees of freedom, 400, 402, 450, 478 
del (nabla∇), 332 
derivative with respect to a matrix, 
335–336 
derivative with respect to a vector, 
330–335 
detleft parenthesis dot right parenthesis(·), 85 
determinant, 85–95 
of block diagonal matrix, 87 
of Cayley product, 110 
computing, 603 
derivative of, 336 
diagonal expansion, 92 
of diagonal matrix, 87 
of elementary operator matrix, 109 
of inverse, 139 
Jacobian, 347 
of Kronecker product, 118 
Laplace expansion, 90, 92 
of nonnegative deﬁnite matrix, 385 
of partitioned matrix, 87, 144 
of permutation matrix, 109 
relation to eigenvalues, 163 
relation to geometric volume, 94, 347 
of transpose, 87 
of triangular matrix, 87 
of positive deﬁnite matrix, 387 
diagleft parenthesis dot right parenthesis(·), 78, 79 
with matrix arguments, 82 
diagonal element, 74 
diagonal expansion of determinant, 94 
diagonal factorization, 171, 175 
diagonalizable matrix, 171–175, 304, 384 
orthogonally, 177 
singular value decomposition, 185 
unitarily, 169, 384, 426 
diagonally dominant matrix, 75, 81, 122, 
388 
diagonal matrix, 75 
determinant of, 87 
inverse of, 142 
multiplication, 99 
diﬀerential, 328 
diﬀerentiation of vectors and matrices, 
323–336 
digraph, 372 
of a matrix, 373 
normal d normal i normal m left parenthesis dot right parenthesisdim(·), 23 
dimension of matrix, 4 
dimension of vector space, 23 
dimension reduction, 30, 396, 473 
directed dissimilarity matrix, 409 
direction cosines, 48, 223 
direct method for solving linear systems, 
268–273 
direct product (of matrices), 116 
direct product (of sets), 3 
direct product (of vector spaces), 30 
basis for, 32 
direct sum decomposition of a vector 
space, 29 
direct sum of matrices, 82 
direct sum of vector spaces, 27–29, 83 
basis for, 32 
direct sum decomposition, 29 
discrete Fourier transform, 424 
discrete Legendre polynomials, 419 
discretization error, 566, 577 
dissimilarity matrix, 408, 409 
distance, 41 
between matrices, 199 
between vectors, 42 
distance matrix, 408, 409

Index
681
distributed computing, 532, 614, 631 
distributed linear algebra machine, 629 
distribution vector, 417 
divleft parenthesis dot right parenthesis(·), 332 
divergence, 332 
divide and conquer, 573 
document-term matrix, 376 
dominant eigenvalue, 165 
Doolittle method, 237 
D-optimality, 603 
dot product of matrices, 119 
dot product of vectors, 34, 112 
double cone, 53 
double precision, 541, 547 
doubly stochastic matrix, 417 
dplyr (R package), 513 
dual cone, 55 
E 
upper E Subscript p qEpq, upper E Subscript left parenthesis pi right parenthesisE(π), upper E Subscript p Baseline left parenthesis a right parenthesisEp(a), upper E Subscript p q Baseline left parenthesis a right parenthesisEpq(a) (elementary 
operator matrices), 107 
Eleft parenthesis dot right parenthesis(·) (expectation operator), 341 
echelon form, 135 
edge of a graph, 369 
EDP (exact dot product), 561 
eﬀective degrees of freedom, 402, 478 
eﬃciency, computational, 570–576 
eigenpair, 156 
eigenspace, 167 
eigenvalue, 156–185, 190, 303–320 
computing, 304–316, 629 
Jacobi method, 311–314 
Krylov methods, 316 
power method, 309–311 
QR method, 314–316 
imaginary values, 162 
of a graph, 431 
of a polynomial Exercise 3.26:, 212 
relation to singular value, 184 
upper bound on, 165, 167, 304 
eigenvector, 156–185, 303–320 
left eigenvector, 160, 180 
eigenvectors, linear independence of, 
166 
EISPACK, 627 
elastic net regression, 479 
elementary operation, 102 
elementary operator matrix, 102–110, 
123, 234, 269 
eigenvalues, 159 
elliptic metric, 115 
elliptic norm, 115 
endian, 548 
equivalence of norms, 39, 42, 193 
equivalence relation, 488 
equivalent canonical factorization, 136 
equivalent canonical form, 135, 136 
equivalent matrices, 121, 134 
error bound, 564 
error-free computations, 561 
error in computations 
cancellation, 555, 568 
error-free computations, 561 
measures of, 551, 562–564, 596 
rounding, 555, 562, 563 
Exercise 10.9:, 586 
error, measures of, 268, 551, 562–564, 
596 
error of approximation, 565 
discretization, 566 
truncation, 566 
errors-in-variables, 443 
essentially disjoint vector spaces, 26, 83 
estimable combinations of parameters, 
462 
estimation and approximation, 479 
Euclidean distance, 42, 408 
Euclidean distance matrix, 408 
Euclidean matrix norm (see also 
Frobenius norm), 191 
Euclidean vector norm, 37 
Euler’s constant Exercise 10.2:, 585 
Euler’s rotation theorem, 223 
exact computations, 561 
exact dot product (EDP), 561 
exception, in computer operations, 550, 
555 
expectation, 341–350 
exponent, 536 
exponential, matrix, 176, 324 
exponential order, 570 
extended precision, 541 
extrapolation, 577 
F 
factorization of a matrix, 97, 131, 
136, 169, 171, 182, 185, 217–219, 
232–250, 268, 270

682
Index
Banachiewicz factorization, 246 
Bartlett decomposition, 386 
canonical singular value factorization, 
183 
Cholesky factorization, 245–248 
diagonal factorization, 171 
equivalent canonical factorization, 
136 
full rank factorization, 131 
Gaussian elimination, 268 
LQ factorization, 239 
LU or LDU factorization, 232–238 
nonnegative matrix factorization, 
249, 376 
orthogonally diagonal factorization, 
169, 185 
QL factorization, 239 
QR factorization, 238–243 
rank factorization, 131 
root-free Cholesky, 246 
RQ factorization, 239 
Schur factorization, 170 
singular value factorization, 182–185, 
318, 376, 473, 602 
square root factorization, 187 
unitarily diagonal factorization, 169, 
185 
fan-in algorithm, 553, 573 
fast Fourier transform (FFT), 426 
fast Givens rotation, 231, 595 
ﬁll-in, 250, 596 
Fisher information, 502 
ﬁxed-point representation, 534 
ﬂat, 53 
ﬂoating-point representation, 535 
FLOP, or ﬂop, 572 
Fortran, 544–546, 572, 616, 638–640 
Fourier coeﬃcient, 51, 52, 120, 180, 185, 
192 
Fourier expansion, 46, 51, 120, 180, 185, 
192 
Fourier matrix, 424 
Free Software Foundation, 528 
Frobenius norm, 191–192, 195, 199, 312, 
380, 409 
Frobenius p norm, 193 
full precision, 547 
full rank, 122, 123, 125, 132 
full rank factorization, 131 
full rank partitioning, 125, 143 
G 
g 1g1 inverse, 149 
g 4g4 inverse (see also Moore-Penrose 
inverse), 151 
g 12g12 inverse, 149, 445 
gamma function, 350 
GAMS (Guide to Available Mathemati-
cal Software), 610 
Gaussian elimination, 106, 268, 315 
Gaussian matrix, 106, 234 
Gaussian sweep operator, 448 
Gauss-Markov theorem, 465 
Gauss-Newton method, 500 
Gauss-Seidel method, 273 
gemm (general matrix-matrix), 627 
gemv (general matrix-vector), 627 
generalized eigenvalue, 181, 317 
generalized inverse, 146, 148–154, 241, 
287–288, 399, 445 
g 1g1 inverse, 149 
g 12g12 inverse, 149 
of a Gramian matrix, 150 
reﬂexive, 149 
relation to QR factorization, 241 
generalized least squares, 452 
generalized least squares with equality 
constraints Exercise 9.3d:, 518 
generalized linear model, 514 
generalized variance, 406 
general linear group, 134, 156 
generating set, 23, 31 
of a cone, 54 
generation of random numbers, 13, 
355–359 
geometric multiplicity, 167 
geometry, 45, 94, 219, 223 
Gershgorin disks, 167 
GitHub, 10, 609 
Exercise 12.3:, 647 
Givens transformation (rotation), 
228–231, 315 
QR factorization, 243 
script upper G upper L left parenthesis dot right parenthesisGL(·) (general linear group), 134 
GMP (software library), 534, 559 
Exercise 10.26:, 589 
Exercise 10.27:, 589 
GMRES, 278

Index
683
GNU Scientiﬁc Library (GSL), 627 
GPU (graphical processing unit), 631 
graceful underﬂow, 538 
gradient, 330 
projected gradient, 505 
reduced gradient, 505 
gradient descent, 495, 496 
gradient of a function, 330, 331 
gradual underﬂow, 538, 555 
Gramian matrix, 137–138, 162, 183, 184, 
186, 248, 285, 398–401, 404–406 
completing the Gramian, 200 
eigenvalues, 162, 184 
factorizations, 248 
generalized inverse, 150 
Gram-Schmidt transformation, 49, 50, 
594 
linear least squares, 285 
QR factorization, 243 
graphical processing unit (GPU), 631 
graph of a matrix, 372 
graph theory, 7, 369–375, 429 
greedy algorithm, 574 
group, 134, 156 
GSL (GNU Scientiﬁc Library), 627 
guard digit, 552 
H 
Haar distribution, 349, 620 
Exercise 7.12:, 362 
Exercise 7.13:, 363 
Haar invariant measure, 350 
Hadamard matrix, 420 
Hadamard multiplication, 115 
Hadamard’s inequality Exercise 4.4:, 
254 
Hadoop, 528, 532, 615 
Hadoop Distributed File System 
(HDFS), 532, 581 
half precision, 547 
Hankel matrix, 427 
Hankel norm, 428 
hat matrix, 400, 450 
HDF, HDF5 (Hierarchical Data 
Format), 531 
HDFS (Hadoop Distributed File 
System), 532, 581 
Helmert matrix, 418, 463 
Hemes formula, 282, 453 
Hermite form, 136 
Hermitian matrix, 74, 78 
Hermitian transpose, 78 
Hessenberg matrix, 77, 314 
Hessian matrix, 334 
projected Hessian, 505 
reduced Hessian, 505 
Hessian of a function, 334 
heteroscedasticity, 467, 516 
hidden bit, 536 
Hierarchical Data Format (HDF), 531 
high-performance computing, 575, 628 
R, 630 
Hilbert matrix, 619 
Hilbert–Schmidt norm (see also 
Frobenius norm), 191 
Hilbert space, 43, 191 
Hoﬀman-Wielandt theorem, 380 
H¨older norm, 37 
H¨older’s inequality Exercise 2.12a:, 67 
hollow matrix, 75, 409 
homogeneous coordinates, 224 
in graphics applications, Exer-
cise 4.10:, 255 
homogeneous system of equations, 53, 
145 
Horner’s method, 580 
Householder transformation (reﬂection), 
225–228, 242, 315 
hyperplane, 53 
hypothesis testing, 458 
I 
idempotent matrix, 98, 134, 391–397 
rank, 391 
trace, 391 
identity matrix, 97 
additive identity, 83 
IDL (software), 4 
IEC standards, 533 
IEEE standards, 533, 554 
Standard 754, 541, 547, 554, 561 
Standard P1788, 560 
IFIP Working Group 2.5, 528, 561 
ill-conditioned (problem or data), 260, 
474, 566, 567, 593 
artiﬁcial, 265 
stiﬀ data, 569 
ill-posed problem, 143

684
Index
image data, 530 
IMSL Libraries, 627, 632–634 
incidence matrix, 371–374, 429 
incomplete data, 483–486 
incomplete factorization, 250, 596 
independence, linear, see linear 
independence 
independent vertices, 429 
index-index-value (sparse matrices), 619 
index of a matrix, 134 
index of idempotent matrix, 98 
index of nilpotent matrix, 98 
induced matrix norm, 188 
Inf, 14 
inﬁnity, ﬂoating-point representation, 
541, 555 
inﬁx operator, 558 
inner product, 34, 237 
inner product of matrices, 118–120 
inner product space, 33 
inner pseudoinverse, 149 
integer representation, 534 
integration, 337–340 
integration of vectors and matrices, 338 
Intel Math Kernel Library (MKL), 626, 
646 
intersection graph, 374 
intersection of vector spaces, 27 
interval arithmetic, 559, 560 
invariance property, 219 
invariant distribution, 489 
invariant vector (eigenvector), 157 
inverse CDF method, 345 
inverse of a matrix, 129 
determinant of, 139 
generalized inverse, 146, 148–154 
Moore–Penrose inverse, 151–152 
pseudoinverse, 151 
Kronecker product, 140 
left inverse, 130 
Moore–Penrose inverse, 151–152 
partitioned matrix, 144 
products or sums of matrices, 140 
pseudoinverse, 151 
right inverse, 130 
transpose, 130 
triangular matrix, 142 
inverse of a vector, 41 
IRLS (iteratively reweighted least 
squares), 293 
irreducible Markov chain, 488 
irreducible matrix, 309, 375, 412–416, 
489 
is.na, 542 
isnan, is.nan, 542 
ISO (standards), 529, 543, 634, 635 
isometric matrix, 190 
isometric transformation, 220 
isotropic transformation, 220 
iteratively reweighted least squares 
(IRLS), 293 
iterative method, 273, 279, 303, 576–578, 
595 
iterative reﬁnement, 279 
for solving linear systems, 273–279 
J 
Jacobian, 332, 347 
Jacobi method for eigenvalues, 311–314 
Jacobi transformation (rotation), 228 
Jordan decomposition, 174 
Jordan form, 136 
K 
Kalman ﬁlter, 569 
Kantorovich inequality, 390 
Karush-Kuhn-Tucker conditions, 508 
kind (for data types), 545 
Kronecker multiplication, 116–118 
inverse, 140 
properties, 117 
symmetric matrices, 118, 178 
diagonalization, 178 
Kronecker structure, 118, 349, 468 
Krylov method, 277, 316 
Krylov space, 277 
Kuhn-Tucker conditions, 508 
Kulisch accumulator, 561 
Kullback-Leibler divergence, 199 
L 
normal upper L 1L1, normal upper L 2L2, and  normal upper L Subscript normal infinityL∞norms 
of a matrix, 189 
relations among, 194 
of a symmetric matrix, 191 
of a vector, 37

Index
685
. L2 norm of a matrix (see also spectral 
norm), 190 
Lagrange multiplier, 451, 502, 505–507 
Exercise 9.3a:, 518 
Lagrangian function, 506 
Lanczos method, 316 
LAPACK, 272, 604, 627, 629 
LAPACK95, 627 
Laplace expansion, 90 
Laplace operator (nabla squared∇2), 333 
Laplacian matrix, 431 
lasso regression, 478 
latent root (see also eigenvalue), 157 
LAV (least absolute values), 291 
LDU factorization, 232–238 
leading principal submatrix, 81, 389 
least absolute values, 291 
least squares, 247, 282–291, 497–502 
constrained, 504–508 
maximum likelihood, 461 
nonlinear, 499–502 
nonnegative, 295, 507 
least squares regression, 498 
left eigenvector, 160, 180 
left inverse, 130 
length of a vector, 2, 37, 38 
Leslie matrix, 417, 490 
Exercise 8.10:, 434 
Exercise 9.15:, 520 
Levenberg-Marquardt method, 502 
leverage, 451 
Exercise 9.19:, 521 
life table, 490 
likelihood function, 502 
line, 53 
linear convergence, 576 
linear estimator, 462 
linear independence, 21, 121 
linear independence of eigenvectors, 166 
linear programming, 387 
linear regression, 438–469, 474–479 
variable selection, 475 
LINPACK, 272, 603, 627 
Linux Foundation, 528 
little endian, 548 
little o (order), 565 
little omega (order), 565 
log-likelihood function, 502 
log order, 570 
Longley data Exercise 9.20:, 522 
loop unrolling, 637 
Lorentz cone, 54 
lower triangular matrix, 75 
normal upper L Subscript pLp norm 
of a matrix, 189 
of a vector, 37–38, 326 
LQ factorization, 239 
LR method, 304 
LU factorization, 232–238 
computing, 629 
M 
MACHAR, 546 
Exercise 10.3(d)i:, 585 
machine epsilon, 14, 538 
Mahalanobis distance, 115, 405 
Manhattan norm, 37 
manifold of a matrix, 73 
Maple (software), 634 
MapReduce, 532, 581, 601, 615 
Markov chain, 487–489 
Markov chain Monte Carlo (MCMC), 
489 
Mathematica (software), 634 
Matlab (software), 616, 642–643 
matlib (R package), 201 
matrix, 4 
Matrix (R package), 201 
matrix derivative, 323–336 
matrix exponential, 176, 324 
matrix factorization, 97, 131, 136, 169, 
171, 182, 185, 217–219, 232–250, 
268, 270 
matrix function, 175 
matrix gradient, 331 
matrix inverse, 129 
matrix multiplication, 95–120, 597 
Cayley, 96, 115 
CUDA, 631 
Hadamard, 115 
inner product, 118–120 
Kronecker, 116–118 
MapReduce, 601 
Strassen algorithm, 599–600 
matrix norm, 187–195 
orthogonally invariant, 188 
matrix normal distribution, 349 
matrix of type 2, 76, 422

686
Index
matrix pencil, 182 
matrix polynomial, 99 
Exercise 3.26:, 211 
matrix random variable, 343–344, 
349–350 
matrix storage mode, 617–619 
Matrix Template Library, 641 
maximal linearly independent subset, 
21 
maximum likelihood, 461, 502–503, 514 
least squares, 461 
max norm, 37 
MCMC (Markov chain Monte Carlo), 
489 
mean, 45, 47 
mean vector, 44 
message passing, 628 
Message Passing Library, 628 
metric, 41, 199 
metric space, 42 
Microsoft R Open, 646 
MIL-STD-1753 standard, 546 
Minkowski inequality, 37 
Exercise 2.12b:, 68 
Minkowski norm, 37 
minor, 90 
missing data, 483–486, 510 
casewise deletion, 484, 511 
pairwise deletion, 484, 511 
representation of, 530, 542 
MKL (Intel Math Kernel Library), 626, 
646 
M-matrix, 432 
mobile Jacobi scheme, 313 
modiﬁed Cholesky decomposition, 485 
“modiﬁed” Gauss-Newton, 500 
“modiﬁed” Gram–Schmidt (see also 
Gram–Schmidt transformation), 
50 
moment-generating function (MGF), 
341 
Moore–Penrose inverse, 151–152, 240, 
241, 288, 445 
relation to QR factorization, 241 
MPI (message passing interface), 628, 
631 
MPL (Message Passing Library), 628 
multicollinearity, 261, 443 
multigrid method, 279 
multiple precision, 534, 559 
R, 583 
multiplicity of an eigenvalue, 167 
multivariate gamma function, 350 
multivariate linear regression, 456–457, 
468–469 
multivariate normal distribution, 
346–349, 355 
singular, 346, 481 
multivariate random variable, 340–350 
N 
script upper N left parenthesis dot right parenthesisN(·), 147 
NA (“Not Applicable”), 14, 510, 531, 
542 
nabla (nabla∇), 330, 332 
Nag Libraries, 627 
NaN (“Not-a-Number”), 14, 541, 555 
NetCDF, 531 
netlib, XVIII 
network, 369–375, 431 
Newton’s method, 495 
nilpotent matrix, 98, 197 
unipotent matrix, 100 
NMF (nonnegative matrix factoriza-
tion), 249, 376 
noncentral chi-squared distribution, 353 
PDF, equation (7.51), 353 
noncentral Wishart distribution 
Exercise 7.9:, 361 
nonlinear regression, 498 
nonnegative deﬁnite matrix, 113, 185, 
245, 384–390 
summary of properties, 384–385 
nonnegative least squares, 295, 507 
nonnegative matrix, 249, 409 
nonnegative matrix factorization, 249, 
376 
nonsingular matrix, 122, 132 
norm, 35–40 
convexity, 35 
equivalence of norms, 39, 42, 193 
of a matrix, 187–195 
orthogonally invariant, 188 
of a vector, 37–41 
weighted, 38, 115 
normal distribution, 346–350 
matrix, 349 
multivariate, 346–349

Index
687
normal equations, 247, 285, 441, 457 
normalized ﬂoating-point numbers, 
536 
normalized generalized inverse (see also 
Moore–Penrose inverse), 151 
normalized vector, 41 
normal matrix, 383 
Exercise 8.1:, 433 
circulant matrix, 423 
normal vector, 44 
normed space, 35 
not-a-number (“NaN”), 541 
NP-complete problem, 571 
nuclear norm, 193 
nullity, 147 
null space, 147, 148, 166 
NumPy, 628, 641, 646 
Nvidia, 631 
O 
Oleft parenthesis dot right parenthesis(·), 564, 570 
oleft parenthesis dot right parenthesis(·), 565 
oblique projection, 396 
Octave (software), 642–643 
OLS (ordinary least squares), 284 
omitted variable bias, 475 
one vector, 24, 44 
online algorithm, 578 
online processing, 578 
OpenMP, 628, 630 
open-source, 609 
Open Source Initiative, 528 
operator matrix, 102, 269 
operator norm, 188 
optimization of vector/matrix functions, 
494–510 
constrained, 503–508 
least squares, 497–502, 504–508 
order of a graph, 369 
order of a vector, 2 
order of a vector space, 23 
order of computations, 570 
order of convergence, 564 
order of error, 564 
order of matrix, 4 
ordinal relations among matrices, 113, 
388 
ordinal relations among vectors, 25 
orthogonal array, 420 
orthogonal basis, 50–51 
orthogonal complement, 44, 147, 154 
orthogonal distance regression, 296–299, 
443 
orthogonal group, 155, 350 
orthogonalization (Gram–Schmidt 
transformations), 48, 243, 594 
orthogonally diagonal factorization, 
169, 185 
orthogonally diagonalizable, 169, 177, 
185, 379, 384, 471 
orthogonally invariant norm, 188, 190, 
192 
orthogonally similar, 169, 177, 188, 192, 
265, 384 
orthogonal matrices, binary relation-
ship, 120 
orthogonal matrix, 154–156, 221 
unitary matrix, 155 
orthogonal residuals, 296–299, 443 
orthogonal transformation, 221 
orthogonal vectors, 43 
Exercise 2.19:, 68 
orthogonal vector spaces, 44, 154 
orthonormal vectors, 43 
outer/inner products matrix, 397 
outer product, 112, 236 
Exercise 3.16:, 211 
outer product for matrix multiplication, 
599 
outer pseudoinverse, 149 
out-of-core algorithm, 578 
overdetermined linear system, 146, 247, 
282 
overﬁtting, 294, 477 
overﬂow, in computer operations, 551, 
555 
Overleaf, 609 
overloading, 20, 82, 189, 547, 558 
P 
p-inverse (see also Moore–Penrose 
inverse), 151 
packleft parenthesis dot right parenthesis(·), 80 
paging, 637 
parallelogram equality, 36 
parallelotope, 95 
parallel processing, 575, 598, 614, 628 
forks, 615

688
Index
R, 630 
sockets, 615 
Parseval’s identity, 51, 192 
partial ordering, 25, 113, 388 
Exercise 8.2a:, 433 
partial pivoting, 271 
partitioned matrix, 80, 101, 153 
determinant, 87, 144 
generalized inverse, 153 
inverse, 144 
multiplication, 101 
sum of squares, 401, 457 
partitioned matrix, inverse, 144 
partitioning sum of squares, 401, 457 
PBLAS (parallel BLAS), 629 
PDF (probability density function), 341 
pencil, 182 
permutation, 85 
permutation matrix, 103, 109, 269, 417 
Perron-Frobenius theorem, 414 
Perron root, 411, 414 
Perron theorem, 411 
Perron vector, 411, 414, 489 
pivoting, 107, 236, 241, 271 
PLASMA, 630 
polar cone, 55 
polynomial, evaluation of, 580 
polynomial in a matrix, 99 
Exercise 3.26:, 211 
polynomial order, 570 
polynomial regression, 419 
pooled variance-covariance matrix, 
407 
population model, 489 
portability, 549, 562, 610 
Posit, 8 
positive deﬁnite matrix, 113, 122, 
185–187, 245, 387–390, 470 
summary of properties, 387–388 
positive matrix, 249, 409 
positive semideﬁnite matrix, 113 
positive stable, 186, 432 
power method for eigenvalues, 309–311 
precision, 540–547, 559 
arbitrary, 534 
double, 541, 547 
extended, 541 
half precision, 547 
inﬁnite, 534 
multiple, 534, 559 
single, 541, 547 
preconditioning, 278, 308, 595 
in the conjugate gradient method, 
278 
for eigenvalue computations, 308 
primitive Markov chain, 489 
primitive matrix, 415, 489 
principal axis, 45 
principal components, 470–474 
principal components regression, 476 
principal diagonal, 74 
principal minor, 93, 126 
principal submatrix, 81, 126, 233, 384, 
387 
leading, 81, 389 
probabilistic error bound, 564 
probability density function (PDF), 341 
programming model, 532, 614 
projected gradient, 505 
projected Hessian, 505 
projection (of a vector), 30, 46 
projection matrix, 396–397, 450 
rank, 396 
trace, 396 
projective transformation, 220 
proper value (see also eigenvalue), 157 
PSBLAS (parallel sparse BLAS), 629 
pseudo-correlation matrix, 485 
pseudoinverse (see also Moore–Penrose 
inverse), 151 
PV-Wave (software), 4 
Pythagorean theorem, 36 
Python, 616, 641 
Q 
Q-convergence, 577 
QL factorization, 239 
QR factorization, 238–243 
computing, 629 
and a generalized inverse, 241 
matrix rank, 241 
skinny, 238 
QR method for eigenvalues, 314–316 
quadratic convergence, 577 
quadratic form, 112, 115 
quasi-Newton method, 496 
quotient space, 134

Index
689
R 
normal fraktur upper RIR, 1 
normal fraktur upper R Superscript nIRn, 3 
normal fraktur upper R Superscript n times mIRn×m, 5 
R (software), 8–16, 62–66, 201–209, 251– 
253, 299–300, 357–359, 510–517, 
582–583, 616, 644–646 
apply functions, 208 
classes, 16 
Exercise 12.16:, 650 
complex numbers, 14 
computer algebra, 645 
data frame, 512 
merging, 513 
tibble, 513 
exercises, 70–71, 213–215, 255–258, 
362–364, 521–524, 589, 649–650 
$ extractor, 251 
graphics, 253 
Exercise 4.10:, 255 
heteroscedasticity-robust procedures, 
516 
high-performance computing, 630 
lapply and sapply, 66 
lists, 65, 251 
$ extractor, 251 
indexing, 65 
machine constants, 15, 582 
matrices, 201–209, 251–252, 644–646 
colnames, rownames, 202 
merging datasets, 513 
Microsoft R Open, 646 
missing data, 510 
multiple precision, 583 
NA, 14, 510 
Overleaf, 609 
parallel processing, 630 
random number generation, 357 
Rcpp, 646 
RcppArmadillo, 646 
roxygen, 11 
RPy, 646 
rseek, 15, 609 
RStudio, 8, 646 
S4 classes 
Exercise 12.16:, 650 
slots, 650 
sparse matrices, 644 
sweave, knitr, markdown, 609 
symbolic computation, 645 
tibble, 513 
vectors, 62–66 
views, 15, 610 
vignette, 16 
radix, 536 
random graph, 377 
random matrix, 343–344, 349–350 
BMvN distribution, 349 
computer generation Exercise 7.12:, 
362 
correlation matrix, 356 
Haar distribution, 349 
Exercise 7.12:, 362 
normal, 349 
orthogonal Exercise 7.12:, 362 
rank Exercise 7.7:, 360 
Wishart, 353, 386, 469, 480 
Exercise 7.9:, 361 
random number generation, 13, 355–359 
random matrices Exercise 7.12:, 362 
random variable, 340 
range of a matrix, 73 
rankleft parenthesis dot right parenthesis(·), 121 
rank decomposition (factorization), 131 
rank-revealing QR, 241, 479, 602 
rank deﬁciency, 122, 167 
rank determination, 602 
rank, linear independence, 121, 602 
rank, number of dimensions, 3 
rank-one decomposition, 185 
rank-one update, 226, 281 
rank of a matrix, 121–143, 241, 479, 602 
of idempotent matrix, 391 
rank-revealing QR, 241 
statistical tests, 479–483 
rank of an array, 3 
rank reduction, 602 
rank-revealing QR, 241, 479 
rate constant, 577 
rate of convergence, 577 
rational fraction, 559 
Rayleigh quotient, 101, 179, 432, 506 
Rcpp, 646 
RcppBlaze, 630 
RCR (Replicated Computational 
Results), 624 
real numbers, 535 
real-time algorithm, 578

690
Index
recursion, 579 
reduced gradient, 505 
reduced Hessian, 505 
reduced rank regression problem, 481 
reducibility, 309, 374–375, 412 
Markov chains, 488 
reﬂection, 223–225 
reﬂector, 225 
reﬂexive generalized inverse, 149 
register, in computer processor, 552 
regression, 438–469, 474–479 
nonlinear, 498 
variable selection, 475 
regular graph, 369 
regularization, 294, 442, 477 
regular matrix (see also diagonalizable 
matrix), 172 
relative error, 551, 562, 596 
relative spacing, 538 
Reliable Computing, 560 
Replicated Computational Results 
(RCR), 624 
reproducible research, 622–624, 646 
Reproducible R Toolkit, 646 
residue arithmetic, 561 
restarting, 595 
reverse communication, 636 
R Foundation, 528 
rho left parenthesis dot right parenthesisρ(·) (spectral radius), 164 
Richardson extrapolation, 578 
ridge regression, 266, 401, 443, 456, 477 
Exercise 9.7a:, 519 
right direct product, 116 
right inverse, 130 
robustness (algorithm or software), 567 
root-free Cholesky, 246 
root of a function, 554 
Rosser test matrix, 621 
rotation, 221–224, 228 
rounding, 541 
rounding error, 555, 562 
row echelon form, 135 
row-major, 592, 610, 616 
row rank, 121 
row space, 74 
row-sum norm, 189 
roxygen, 11 
RPy, 646 
RQ factorization, 239 
RStudio, 8, 646 
S 
Samelson inverse, 41 
sample variance, computing, 568 
sandwich estimator, 343, 468, 516 
saxpy, 20 
scalability, 574 
ScaLAPACK, 629 
scalar, 19 
scalar product, 34 
scaled matrix, 405 
scaled vector, 60 
scaling of an algorithm, 570, 574 
scaling of a vector or matrix, 265 
Schatten p norm, 193 
Schur complement, 143, 448, 469 
Schur factorization, 170–171 
Schur norm (see also Frobenius norm), 
191 
SDP (semideﬁnite programming), 386 
Seidel adjacency matrix, 372 
self-adjoint matrix (see also Hermitian 
matrix), 74 
semideﬁnite programming (SDP), 386 
seminorm, 35 
semisimple eigenvalue, 167, 172 
sequences of matrices, 195 
sequences of vectors, 42 
shape of matrix, 4 
shearing transformation, 220 
Sherman-Morrison formula, 281, 453 
shifting eigenvalues, 308 
shrinkage, 443 
side eﬀect, 625 
signleft parenthesis dot right parenthesis(·), 24 
sigma left parenthesis dot right parenthesisσ(·) (sign of permutation), 85, 109 
sigma left parenthesis dot right parenthesisσ(·) (spectrum of matrix), 164 
sign bit, 533 
signiﬁcand, 536 
similar canonical form, 172 
similarity matrix, 408 
similarity transformation, 168–171, 311, 
315 
similar matrices, 168 
simple eigenvalue, 167 
simple graph, 369 
simple matrix (see also diagonalizable 
matrix), 172

Index
691
single precision, 541, 547 
singular matrix, 122 
singular multivariate normal distribu-
tion, 346, 481 
singular value, 183, 473, 602 
relation to eigenvalue, 184 
singular value decomposition, 182–185, 
318, 376, 473, 602 
computing, 318 
uniqueness, 184 
size of matrix, 4 
skew diagonal element, 79 
skew diagonal matrix, 75 
skew symmetric matrix, 74, 78 
skew upper triangular matrix, 76, 428 
skinny QR factorization, 238 
smoothing matrix, 401, 456 
software testing, 619–622 
SOR (method), 275 
spanleft parenthesis dot right parenthesis(·), 23, 31, 73 
spanning set, 23, 31 
of a cone, 54 
Spark, 528, 532 
Spark (software system), 615 
sparse matrix, 77, 250, 273, 377, 592, 
596, 627, 628 
compressed sparse column format, 
619 
index-index-value, 619 
R, 644 
software, 628, 644 
CUDA, 631 
storage mode, 619 
triplet format method, 619 
spectral circle, 165 
spectral condition number, 264, 266, 286 
spectral decomposition, 177, 185 
spectral norm, 190, 193 
spectral projector, 178 
spectral radius, 164, 190, 194, 274 
spectrum of a graph, 431 
spectrum of a matrix, 163–168 
splitting extrapolation, 578 
square root matrix, 187, 244, 245, 386 
stability, 272, 567 
standard deviation, 60, 404 
computing the standard deviation, 
568 
standards (see also speciﬁc standard), 
528 
Standard Template Library, 641 
stationary point of vector/matrix 
functions, 495 
statistical reference datasets (StRD), 
622 
steepest descent, 495, 496 
Stiefel manifold, 155 
stiﬀ data, 569 
stochastic matrix, 417 
stochastic process, 486–510 
stopping criterion, 576 
storage mode, for matrices, 617–619 
storage unit, 532, 536, 548 
Strassen algorithm, 599–600 
StRD (statistical reference datasets), 
622 
stride, 592, 610, 625 
string, character, 530 
strongly connected graph, 374 
submatrix, 80, 101 
subspace of vector space, 26 
successive overrelaxation, 275 
summation, 553 
summing vector, 44 
Sun ONE Studio Fortran 95, 560 
superlinear convergence, 576 
SVD (singular value decomposition), 
182–185, 318, 376, 473, 602 
computing, 318 
uniqueness, 184 
sweep operator (Gaussian), 448 
Sylvester’s law of nullity, 139 
symbolic computations, 613, 645 
symmetric matrix, 74, 78, 136, 176–178, 
186–187, 378–384 
eingenvalues/vectors, 176–178, 
186–187 
equivalent forms, 136 
Hermitian matrix, 74 
inverse of, 142 
self-adjoint matrix (see also Hermitian 
matrix), 74 
summary of properties, 378 
symmetric pair, 317 
symmetric storage mode, 80, 617

692
Index
T 
Taylor series, 328, 496 
Template Numerical Toolkit, 641 
tensor, 3 
term-document matrix, 376 
testable hypothesis, 466 
testing software, 619–622 
test problems for algorithms or software, 
596, 619–622 
consistency test, 596, 622 
Ericksen matrix, 620 
Exercise 12.8:, 648 
Exercise 3.46:, 214 
Hilbert matrix, 619 
Matrix Market, 621 
randomly generated data, 620 
Exercise 11.5:, 606 
Rosser matrix, 621 
StRD (statistical reference datasets), 
622 
Wilkinson’s polynomial, 567 
Wilkinson matrix, 621 
Exercise 12.9:, 648 
thread, 614 
Tikhonov regularization, 295, 477 
time series, 491–510 
variance-covariance matrix, 422, 493 
timing computations, 573 
R, 583 
Toeplitz matrix, 421, 493 
circulant matrix, 423 
inverse of, 422 
Exercise 8.12:, 434 
Exercise 12.12:, 649 
total least squares, 297, 443 
normal t normal r left parenthesis dot right parenthesistr(·), 84 
trace, 84 
of Cayley product, 110 
derivative of, 336 
of idempotent matrix, 391 
of inner product, 114 
of Kronecker product, 118 
of matrix inner product, 119 
of outer product, 114 
relation to eigenvalues, 163 
trace norm, 193 
transition matrix, 487 
translation transformation, 224 
transpose, 77 
of Cayley product of matrices, 96 
determinant of, 87 
generalized inverse of, 149 
inverse of, 130 
of Kronecker product, 117 
norm of, 188 
of partitioned matrices, 82 
of sum of matrices, 83 
trace of, 84 
of a vector, 100 
trapezoidal matrix, 75, 233, 238 
triangle inequality, 35, 188 
Exercise 2.12b:, 68 
triangular matrix, 75, 106, 233 
determinant of, 87 
inverse of, 142 
multiplication, 100 
tridiagonal matrix, 76 
triple scalar product Exercise 2.21c:, 69 
triplet format method (sparse matrices), 
619 
triple vector product Exercise 2.21d:, 69 
truncation error, 52, 120, 565, 566 
twos-complement representation, 533, 
550 
type 2 matrix, 76, 422 
U 
ulp (“unit in the last place”), 540 
underdetermined linear system, 145 
underﬂow, in computer operations, 538, 
555 
Unicode, 529 
union of vector spaces, 27 
unipotent matrix, 100 
unitarily diagonalizable, 169, 384, 426 
unitarily similar, 169 
unitary matrix, 155 
unit in the last place (ulp), 540 
unit roundoﬀ, 539 
unit triangular (unipotent) matrix, 100 
unit vector, 24, 31, 45, 97 
unrolling do-loop, 637 
updating a solution, 280, 289, 453–455 
regression computations, 453–455 
upper Hessenberg form, 77, 314 
upper triangular matrix, 75 
usual norm (see also Frobenius norm), 
191

Index
693
V 
Vleft parenthesis dot right parenthesis(·) (variance operator), 60, 343 
script upper V left parenthesis dot right parenthesisV(·) (vector space), 31, 73 
Vandermonde matrix, 419 
Fourier matrix, 424 
variable metric method, 497 
variable selection, 475 
variance, computing, 568 
variance–covariance matrix, 343, 349 
Kronecker structure, 118, 349, 468 
positive deﬁnite approximation, 484 
sample, 404, 470 
normal v normal e normal c left parenthesis dot right parenthesisvec(·), 80 
vecdiagleft parenthesis dot right parenthesis(·), 78 
vechleft parenthesis dot right parenthesis(·), 80 
vec-permutation matrix, 104 
vector, 2 
centered vector, 59 
mean vector, 44 
normalized vector, 40 
normal vector, 44 
null vector, 24 
one vector, 24, 44 
“row vector”, 100 
scaled vector, 60 
sign vector, 24 
summing vector, 24, 44 
transpose, 100 
unit vector, 24 
zero vector, 24 
vector derivative, 323–336 
vectorized processor, 575 
vector processing, 628 
vector space, 22–32, 73, 83–84, 147, 148 
basis, 31–32 
deﬁnition, 22 
dimension, 23 
direct product, 30 
direct sum, 27–29 
direct sum decomposition, 29 
essentially disjoint, 26 
generated by a matrix, 74 
generated by a set of vectors, 23 
intersection, 27 
null vector space, 22 
of matrices, 83–84 
order, 23 
set operations, 25 
subspace, 26 
union, 27 
vector subspace, 26 
vecleft parenthesis dot right parenthesis(·), 80 
vertex of a graph, 369 
volume as a determinant, 94, 347 
W 
weighted graph, 369 
weighted least squares, 452 
with equality constraints Exer-
cise 9.3d:, 518 
weighted norm, 38, 115 
Wilkinson matrix, 621 
Wilk’s normal upper LamdaΛ, 469 
Wishart distribution, 386 
Exercise 7.9:, 361 
Woodbury formula, 281, 453 
word, computer, 532, 536, 548 
X 
XDR (external data representation), 
549 
Y 
Yule-Walker equations, 493 
Z 
zero matrix, 97, 121 
zero of a function, 554 
zero vector, 24 
Z-matrix, 432


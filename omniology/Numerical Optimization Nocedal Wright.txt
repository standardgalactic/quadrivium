Numerical Optimization
Jorge Nocedal 
Stephen J. Wright
Springer

Springer Series in Operations Research
Editors:
Peter Glynn
Stephen M. Robinson
Springer
New York
Berlin
Heidelberg
Barcelona
Hong Kong
London
Milan
Paris
Singapore
Tokyo


Jorge Nocedal
Stephen J. Wright
Numerical Optimization
With 85 Illustrations
1 3

Jorge Nocedal
Stephen J. Wright
ECE Department
Mathematics and Computer
Northwestern University
Science Division
Evanston, IL 60208-3118
Argonne National Laboratory
USA
9700 South Cass Avenue
Argonne, IL 60439-4844
USA
Series Editors:
Peter Glynn
Stephen M. Robinson
Department of Operations Research
Department of Industrial Engineering
Stanford University
University of Wisconsin–Madison
Stanford, CA 94305
1513 University Avenue
USA
Madison, WI 53706-1572
USA
Cover illustration is from Pre-Hispanic Mexican Stamp Designs by Frederick V. Field, courtesy of Dover Publi-
cations, Inc.
Library of Congress Cataloging-in-Publication Data
Nocedal, Jorge.
Numerical optimization / Jorge Nocedal, Stephen J. Wright.
p.
cm. — (Springer series in operations research)
Includes bibliographical references and index.
ISBN 0-387-98793-2 (hardcover)
1. Mathematical optimization.
I. Wright, Stephen J., 1960–
.
II. Title.
III. Series.
QA402.5.N62
1999
519.3—dc21
99–13263
© 1999 Springer-Verlag New York, Inc.
All rights reserved. This work may not be translated or copied in whole or in part without the written permission
of the publisher (Springer-Verlag New York, Inc., 175 Fifth Avenue, New York, NY 10010, USA), except for brief
excerpts in connection with reviews or scholarly analysis. Use in connection with any form of information storage
and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or
hereafter developed is forbidden.
The use of general descriptive names, trade names, trademarks, etc., in this publication, even if the former are
not especially identiﬁed, is not to be taken as a sign that such names, as understood by the Trade Marks and
Merchandise Marks Act, may accordingly be used freely by anyone.
ISBN 0-387-98793-2 Springer-Verlag New York Berlin Heidelberg
SPIN 10764949

To Our Parents:
Ra´ul and Concepci´on
Peter and Berenice

Preface
This is a book for people interested in solving optimization problems. Because of the wide
(and growing) use of optimization in science, engineering, economics, and industry, it is
essential for students and practitioners alike to develop an understanding of optimization
algorithms. Knowledge of the capabilities and limitations of these algorithms leads to a better
understanding of their impact on various applications, and points the way to future research
on improving and extending optimization algorithms and software. Our goal in this book
is to give a comprehensive description of the most powerful, state-of-the-art, techniques
for solving continuous optimization problems. By presenting the motivating ideas for each
algorithm, we try to stimulate the reader’s intuition and make the technical details easier to
follow. Formal mathematical requirements are kept to a minimum.
Becauseofourfocusoncontinuousproblems,wehaveomitteddiscussionofimportant
optimization topics such as discrete and stochastic optimization. However, there are a great
manyapplicationsthatcanbeformulatedascontinuousoptimizationproblems;forinstance,
ﬁnding the optimal trajectory for an aircraft or a robot arm;
identifying the seismic properties of a piece of the earth’s crust by ﬁtting a model of
the region under study to a set of readings from a network of recording stations;

viii
P r e f a c e
designing a portfolio of investments to maximize expected return while maintaining
an acceptable level of risk;
controlling a chemical process or a mechanical device to optimize performance or
meet standards of robustness;
computing the optimal shape of an automobile or aircraft component.
Every year optimization algorithms are being called on to handle problems that are
much larger and complex than in the past. Accordingly, the book emphasizes large-scale
optimization techniques, such as interior-point methods, inexact Newton methods, limited-
memorymethods,andtheroleofpartiallyseparablefunctionsandautomaticdifferentiation.
It treats important topics such as trust-region methods and sequential quadratic program-
ming more thoroughly than existing texts, and includes comprehensive discussion of such
“core curriculum” topics as constrained optimization theory, Newton and quasi-Newton
methods, nonlinear least squares and nonlinear equations, the simplex method, and penalty
and barrier methods for nonlinear programming.
THE AUDIENCE
We intend that this book will be used in graduate-level courses in optimization, as of-
fered in engineering, operations research, computer science, and mathematics departments.
There is enough material here for a two-semester (or three-quarter) sequence of courses.
We hope, too, that this book will be used by practitioners in engineering, basic science, and
industry, and our presentation style is intended to facilitate self-study. Since the book treats
a number of new algorithms and ideas that have not been described in earlier textbooks, we
hope that this book will also be a useful reference for optimization researchers.
Prerequisites for this book include some knowledge of linear algebra (including nu-
merical linear algebra) and the standard sequence of calculus courses. To make the book as
self-contained as possible, we have summarized much of the relevant material from these ar-
eas in the Appendix. Our experience in teaching engineering students has shown us that the
material is best assimilated when combined with computer programming projects in which
thestudentgainsagoodfeelingforthealgorithms—theircomplexity,memorydemands,and
elegance—and for the applications. In most chapters we provide simple computer exercises
that require only minimal programming proﬁciency.
EMPHASIS AND WRITING STYLE
We have used a conversational style to motivate the ideas and present the numerical
algorithms. Rather than being as concise as possible, our aim is to make the discussion ﬂow
in a natural way. As a result, the book is comparatively long, but we believe that it can be
read relatively rapidly. The instructor can assign substantial reading assignments from the
text and focus in class only on the main ideas.

P r e f a c e
ix
A typical chapter begins with a nonrigorous discussion of the topic at hand, including
ﬁguresanddiagramsandexcludingtechnicaldetailsasfaraspossible.Insubsequentsections,
the algorithms are motivated and discussed, and then stated explicitly. The major theoretical
results are stated, and in many cases proved, in a rigorous fashion. These proofs can be
skipped by readers who wish to avoid technical details.
The practice of optimization depends not only on efﬁcient and robust algorithms,
but also on good modeling techniques, careful interpretation of results, and user-friendly
software. In this book we discuss the various aspects of the optimization process—modeling,
optimality conditions, algorithms, implementation, and interpretation of results—but not
with equal weight. Examples throughout the book show how practical problems are formu-
lated as optimization problems, but our treatment of modeling is light and serves mainly
to set the stage for algorithmic developments. We refer the reader to Dantzig [63] and
Fourer, Gay, and Kernighan [92] for more comprehensive discussion of this issue. Our treat-
ment of optimality conditions is thorough but not exhaustive; some concepts are discussed
more extensively in Mangasarian [154] and Clarke [42]. As mentioned above, we are quite
comprehensive in discussing optimization algorithms.
TOPICS NOT COVERED
We omit some important topics, such as network optimization, integer programming,
stochastic programming, nonsmooth optimization, and global optimization. Network and
integeroptimizationaredescribedinsomeexcellenttexts:forinstance,Ahuja,Magnanti,and
Orlin [1] in the case of network optimization and Nemhauser and Wolsey [179], Papadim-
itriou and Steiglitz [190], and Wolsey [249] in the case of integer programming. Books on
stochastic optimization are only now appearing; we mention those of Kall and Wallace [139],
Birge and Louveaux [11]. Nonsmooth optimization comes in many ﬂavors. The relatively
simple structures that arise in robust data ﬁtting (which is sometimes based on the ℓ1 norm)
are treated by Osborne [187] and Fletcher [83]. The latter book also discusses algorithms
for nonsmooth penalty functions that arise in constrained optimization; we discuss these
brieﬂy, too, in Chapter 18. A more analytical treatment of nonsmooth optimization is given
by Hiriart-Urruty and Lemar´echal [137]. We omit detailed treatment of some important
topics that are the focus of intense current research, including interior-point methods for
nonlinear programming and algorithms for complementarity problems.
ADDITIONAL RESOURCE
The material in the book is complemented by an online resource called the NEOS
Guide, which can be found on the World-Wide Web at
http://www.mcs.anl.gov/otc/Guide/
The Guide contains information about most areas of optimization, and presents a number of
casestudiesthatdescribeapplicationsofvariousoptimizationalgorithmstoreal-worldprob-

x
P r e f a c e
lems such as portfolio optimization and optimal dieting. Some of this material is interactive
in nature and has been used extensively for class exercises.
For the most part, we have omitted detailed discussions of speciﬁc software packages,
and refer the reader to Mor´e and Wright [173] or to the Software Guide section of the NEOS
Guide, which can be found at
http://www.mcs.anl.gov/otc/Guide/SoftwareGuide/
Users of optimization software refer in great numbers to this web site, which is being
constantly updated to reﬂect new packages and changes to existing software.
ACKNOWLEDGMENTS
Wearemostgratefultothefollowingcolleaguesfortheirinputandfeedbackonvarious
sections of this work: Chris Bischof, Richard Byrd, George Corliss, Bob Fourer, David Gay,
Jean-Charles Gilbert, Phillip Gill, Jean-Pierre Goux, Don Goldfarb, Nick Gould, Andreas
Griewank, Matthias Heinkenschloss, Marcelo Marazzi, Hans Mittelmann, Jorge Mor´e, Will
Naylor, Michael Overton, Bob Plemmons, Hugo Scolnik, David Stewart, Philippe Toint,
Luis Vicente, Andreas Waechter, and Ya-xiang Yuan. We thank Guanghui Liu, who provided
help with many of the exercises, and Jill Lavelle who assisted us in preparing the ﬁgures. We
also express our gratitude to our sponsors at the Department of Energy and the National
Science Foundation, who have strongly supported our research efforts in optimization over
the years.
Oneofus(JN)wouldliketoexpresshisdeepgratitudetoRichardByrd,whohastaught
him so much about optimization and who has helped him in very many ways throughout
the course of his career.
FINAL REMARK
In the preface to his 1987 book [83], Roger Fletcher described the ﬁeld of optimization
as a “fascinating blend of theory and computation, heuristics and rigor.” The ever-growing
realm of applications and the explosion in computing power is driving optimization research
in new and exciting directions, and the ingredients identiﬁed by Fletcher will continue to
play important roles for many years to come.
Jorge Nocedal
Stephen J. Wright
Evanston, IL
Argonne, IL

Contents
Preface
vii
1
Introduction
xxi
Mathematical Formulation
. . . . . . . . . . . . . . . . . . . . . . . .
2
Example: A Transportation Problem
. . . . . . . . . . . . . . . . . . .
4
Continuous versus Discrete Optimization . . . . . . . . . . . . . . . . .
4
Constrained and Unconstrained Optimization . . . . . . . . . . . . . .
6
Global and Local Optimization . . . . . . . . . . . . . . . . . . . . . .
6
Stochastic and Deterministic Optimization . . . . . . . . . . . . . . . .
7
Optimization Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
7
Convexity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2
Fundamentals of Unconstrained Optimization
10
2.1
What Is a Solution?
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
Recognizing a Local Minimum
. . . . . . . . . . . . . . . . . . . . . .
15
Nonsmooth Problems . . . . . . . . . . . . . . . . . . . . . . . . . . .
18

xii
C o n t e n t s
2.2
Overview of Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Two Strategies: Line Search and Trust Region . . . . . . . . . . . . . . .
19
Search Directions for Line Search Methods . . . . . . . . . . . . . . . .
21
Models for Trust-Region Methods . . . . . . . . . . . . . . . . . . . . .
26
Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
Rates of Convergence
. . . . . . . . . . . . . . . . . . . . . . . . . . .
28
R-Rates of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . .
29
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
3
Line Search Methods
34
3.1
Step Length
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
The Wolfe Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
The Goldstein Conditions . . . . . . . . . . . . . . . . . . . . . . . . .
41
Sufﬁcient Decrease and Backtracking . . . . . . . . . . . . . . . . . . .
41
3.2
Convergence of Line Search Methods . . . . . . . . . . . . . . . . . . .
43
3.3
Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
Convergence Rate of Steepest Descent . . . . . . . . . . . . . . . . . . .
47
Quasi-Newton Methods . . . . . . . . . . . . . . . . . . . . . . . . . .
49
Newton’s Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
Coordinate Descent Methods . . . . . . . . . . . . . . . . . . . . . . .
53
3.4
Step-Length Selection Algorithms . . . . . . . . . . . . . . . . . . . . .
55
Interpolation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
The Initial Step Length . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
A Line Search Algorithm for the Wolfe Conditions . . . . . . . . . . . .
58
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4
Trust-Region Methods
64
Outline of the Algorithm
. . . . . . . . . . . . . . . . . . . . . . . . .
67
4.1
The Cauchy Point and Related Algorithms
. . . . . . . . . . . . . . . .
69
The Cauchy Point . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
Improving on the Cauchy Point . . . . . . . . . . . . . . . . . . . . . .
70
The Dogleg Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
Two-Dimensional Subspace Minimization
. . . . . . . . . . . . . . . .
74
Steihaug’s Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.2
Using Nearly Exact Solutions to the Subproblem . . . . . . . . . . . . .
77
Characterizing Exact Solutions
. . . . . . . . . . . . . . . . . . . . . .
77
Calculating Nearly Exact Solutions
. . . . . . . . . . . . . . . . . . . .
78
The Hard Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
Proof of Theorem 4.3
. . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.3
Global Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87

C o n t e n t s
xiii
Reduction Obtained by the Cauchy Point . . . . . . . . . . . . . . . . .
87
Convergence to Stationary Points . . . . . . . . . . . . . . . . . . . . .
89
Convergence of Algorithms Based on Nearly Exact Solutions . . . . . . .
93
4.4
Other Enhancements
. . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
94
Non-Euclidean Trust Regions . . . . . . . . . . . . . . . . . . . . . . .
96
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
5
Conjugate Gradient Methods
100
5.1
The Linear Conjugate Gradient Method . . . . . . . . . . . . . . . . . .
102
Conjugate Direction Methods . . . . . . . . . . . . . . . . . . . . . . .
102
Basic Properties of the Conjugate Gradient Method
. . . . . . . . . . .
107
A Practical Form of the Conjugate Gradient Method . . . . . . . . . . .
111
Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112
Preconditioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
Practical Preconditioners
. . . . . . . . . . . . . . . . . . . . . . . . .
119
5.2
Nonlinear Conjugate Gradient Methods
. . . . . . . . . . . . . . . . .
120
The Fletcher–Reeves Method
. . . . . . . . . . . . . . . . . . . . . . .
120
The Polak–Ribi`ere Method
. . . . . . . . . . . . . . . . . . . . . . . .
121
Quadratic Termination and Restarts . . . . . . . . . . . . . . . . . . . .
122
Numerical Performance . . . . . . . . . . . . . . . . . . . . . . . . . .
124
Behavior of the Fletcher–Reeves Method
. . . . . . . . . . . . . . . . .
124
Global Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
6
Practical Newton Methods
134
6.1
Inexact Newton Steps
. . . . . . . . . . . . . . . . . . . . . . . . . . .
136
6.2
Line Search Newton Methods . . . . . . . . . . . . . . . . . . . . . . .
139
Line Search Newton–CG Method . . . . . . . . . . . . . . . . . . . . .
139
Modiﬁed Newton’s Method . . . . . . . . . . . . . . . . . . . . . . . .
141
6.3
Hessian Modiﬁcations . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
Eigenvalue Modiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . .
143
Adding a Multiple of the Identity . . . . . . . . . . . . . . . . . . . . .
144
Modiﬁed Cholesky Factorization
. . . . . . . . . . . . . . . . . . . . .
145
Gershgorin Modiﬁcation
. . . . . . . . . . . . . . . . . . . . . . . . .
150
Modiﬁed Symmetric Indeﬁnite Factorization . . . . . . . . . . . . . . .
151
6.4
Trust-Region Newton Methods . . . . . . . . . . . . . . . . . . . . . .
154
Newton–Dogleg and Subspace-Minimization Methods . . . . . . . . . .
154
Accurate Solution of the Trust-Region Problem . . . . . . . . . . . . . .
155
Trust-Region Newton–CG Method . . . . . . . . . . . . . . . . . . . .
156

xiv
C o n t e n t s
Preconditioning the Newton–CG Method . . . . . . . . . . . . . . . . .
157
Local Convergence of Trust-Region Newton Methods
. . . . . . . . . .
159
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
7
Calculating Derivatives
164
7.1
Finite-Difference Derivative Approximations . . . . . . . . . . . . . . .
166
Approximating the Gradient . . . . . . . . . . . . . . . . . . . . . . . .
166
Approximating a Sparse Jacobian . . . . . . . . . . . . . . . . . . . . .
169
Approximating the Hessian . . . . . . . . . . . . . . . . . . . . . . . .
173
Approximating a Sparse Hessian . . . . . . . . . . . . . . . . . . . . . .
174
7.2
Automatic Differentiation . . . . . . . . . . . . . . . . . . . . . . . . .
176
An Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
The Forward Mode
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
The Reverse Mode . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
Vector Functions and Partial Separability . . . . . . . . . . . . . . . . .
183
Calculating Jacobians of Vector Functions . . . . . . . . . . . . . . . . .
184
Calculating Hessians: Forward Mode . . . . . . . . . . . . . . . . . . .
185
Calculating Hessians: Reverse Mode . . . . . . . . . . . . . . . . . . . .
187
Current Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
8
Quasi-Newton Methods
192
8.1
The BFGS Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
Properties of the BFGS Method . . . . . . . . . . . . . . . . . . . . . .
199
Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
200
8.2
The SR1 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Properties of SR1 Updating . . . . . . . . . . . . . . . . . . . . . . . .
205
8.3
The Broyden Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
Properties of the Broyden Class . . . . . . . . . . . . . . . . . . . . . .
209
8.4
Convergence Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
Global Convergence of the BFGS Method . . . . . . . . . . . . . . . . .
211
Superlinear Convergence of BFGS . . . . . . . . . . . . . . . . . . . . .
214
Convergence Analysis of the SR1 Method . . . . . . . . . . . . . . . . .
218
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
9
Large-Scale Quasi-Newton and Partially Separable Optimization
222
9.1
Limited-Memory BFGS . . . . . . . . . . . . . . . . . . . . . . . . . .
224
Relationship with Conjugate Gradient Methods
. . . . . . . . . . . . .
227
9.2
General Limited-Memory Updating . . . . . . . . . . . . . . . . . . . .
229

C o n t e n t s
xv
Compact Representation of BFGS Updating
. . . . . . . . . . . . . . .
230
SR1 Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
Unrolling the Update
. . . . . . . . . . . . . . . . . . . . . . . . . . .
232
9.3
Sparse Quasi-Newton Updates
. . . . . . . . . . . . . . . . . . . . . .
233
9.4
Partially Separable Functions
. . . . . . . . . . . . . . . . . . . . . . .
235
A Simple Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
Internal Variables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
9.5
Invariant Subspaces and Partial Separability
. . . . . . . . . . . . . . .
240
Sparsity vs. Partial Separability
. . . . . . . . . . . . . . . . . . . . . .
242
Group Partial Separability . . . . . . . . . . . . . . . . . . . . . . . . .
243
9.6
Algorithms for Partially Separable Functions . . . . . . . . . . . . . . .
244
Exploiting Partial Separability in Newton’s Method . . . . . . . . . . . .
244
Quasi-Newton Methods for Partially Separable Functions . . . . . . . .
245
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248
10 Nonlinear Least-Squares Problems
250
10.1
Background
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
Modeling, Regression, Statistics . . . . . . . . . . . . . . . . . . . . . .
253
Linear Least-Squares Problems
. . . . . . . . . . . . . . . . . . . . . .
256
10.2
Algorithms for Nonlinear Least-Squares Problems . . . . . . . . . . . .
259
The Gauss–Newton Method . . . . . . . . . . . . . . . . . . . . . . . .
259
The Levenberg–Marquardt Method . . . . . . . . . . . . . . . . . . . .
262
Implementation of the Levenberg–Marquardt Method . . . . . . . . . .
264
Large-Residual Problems
. . . . . . . . . . . . . . . . . . . . . . . . .
266
Large-Scale Problems
. . . . . . . . . . . . . . . . . . . . . . . . . . .
269
10.3
Orthogonal Distance Regression . . . . . . . . . . . . . . . . . . . . . .
271
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
11 Nonlinear Equations
276
11.1
Local Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
Newton’s Method for Nonlinear Equations . . . . . . . . . . . . . . . .
281
Inexact Newton Methods
. . . . . . . . . . . . . . . . . . . . . . . . .
284
Broyden’s Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
286
Tensor Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
11.2
Practical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
Merit Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
Line Search Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . .
294
Trust-Region Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
11.3
Continuation/Homotopy Methods
. . . . . . . . . . . . . . . . . . . .
304
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
304

xvi
C o n t e n t s
Practical Continuation Methods . . . . . . . . . . . . . . . . . . . . . .
306
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
310
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
311
12 Theory of Constrained Optimization
314
Local and Global Solutions
. . . . . . . . . . . . . . . . . . . . . . . .
316
Smoothness
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
12.1
Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
319
A Single Equality Constraint . . . . . . . . . . . . . . . . . . . . . . . .
319
A Single Inequality Constraint . . . . . . . . . . . . . . . . . . . . . . .
321
Two Inequality Constraints
. . . . . . . . . . . . . . . . . . . . . . . .
324
12.2
First-Order Optimality Conditions . . . . . . . . . . . . . . . . . . . .
327
Statement of First-Order Necessary Conditions . . . . . . . . . . . . . .
327
Sensitivity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
12.3
Derivation of the First-Order Conditions . . . . . . . . . . . . . . . . .
331
Feasible Sequences . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
Characterizing Limiting Directions: Constraint Qualiﬁcations . . . . . .
336
Introducing Lagrange Multipliers . . . . . . . . . . . . . . . . . . . . .
339
Proof of Theorem 12.1 . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
12.4
Second-Order Conditions . . . . . . . . . . . . . . . . . . . . . . . . .
342
Second-Order Conditions and Projected Hessians
. . . . . . . . . . . .
348
Convex Programs
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
12.5
Other Constraint Qualiﬁcations . . . . . . . . . . . . . . . . . . . . . .
350
12.6
A Geometric Viewpoint . . . . . . . . . . . . . . . . . . . . . . . . . .
353
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
356
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
13 Linear Programming: The Simplex Method
360
Linear Programming . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
13.1
Optimality and Duality
. . . . . . . . . . . . . . . . . . . . . . . . . .
364
Optimality Conditions . . . . . . . . . . . . . . . . . . . . . . . . . . .
364
The Dual Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
365
13.2
Geometry of the Feasible Set . . . . . . . . . . . . . . . . . . . . . . . .
368
Basic Feasible Points . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
Vertices of the Feasible Polytope . . . . . . . . . . . . . . . . . . . . . .
370
13.3
The Simplex Method . . . . . . . . . . . . . . . . . . . . . . . . . . . .
372
Outline of the Method . . . . . . . . . . . . . . . . . . . . . . . . . . .
372
Finite Termination of the Simplex Method . . . . . . . . . . . . . . . .
374
A Single Step of the Method . . . . . . . . . . . . . . . . . . . . . . . .
376
13.4
Linear Algebra in the Simplex Method
. . . . . . . . . . . . . . . . . .
377
13.5
Other (Important) Details . . . . . . . . . . . . . . . . . . . . . . . . .
381
Pricing and Selection of the Entering Index . . . . . . . . . . . . . . . .
381

C o n t e n t s
xvii
Starting the Simplex Method
. . . . . . . . . . . . . . . . . . . . . . .
384
Degenerate Steps and Cycling . . . . . . . . . . . . . . . . . . . . . . .
387
13.6
Where Does the Simplex Method Fit? . . . . . . . . . . . . . . . . . . .
389
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
14 Linear Programming: Interior-Point Methods
392
14.1
Primal–Dual Methods . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
The Central Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
397
A Primal–Dual Framework
. . . . . . . . . . . . . . . . . . . . . . . .
399
Path-Following Methods . . . . . . . . . . . . . . . . . . . . . . . . . .
400
14.2
A Practical Primal–Dual Algorithm . . . . . . . . . . . . . . . . . . . .
402
Solving the Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . .
406
14.3
Other Primal–Dual Algorithms and Extensions . . . . . . . . . . . . . .
407
Other Path-Following Methods . . . . . . . . . . . . . . . . . . . . . .
407
Potential-Reduction Methods . . . . . . . . . . . . . . . . . . . . . . .
407
Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
408
14.4
Analysis of Algorithm 14.2 . . . . . . . . . . . . . . . . . . . . . . . . .
409
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
414
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415
15 Fundamentals of Algorithms for Nonlinear Constrained Optimization
418
Initial Study of a Problem . . . . . . . . . . . . . . . . . . . . . . . . .
420
15.1
Categorizing Optimization Algorithms . . . . . . . . . . . . . . . . . .
422
15.2
Elimination of Variables . . . . . . . . . . . . . . . . . . . . . . . . . .
424
Simple Elimination for Linear Constraints
. . . . . . . . . . . . . . . .
426
General Reduction Strategies for Linear Constraints . . . . . . . . . . .
429
The Effect of Inequality Constraints . . . . . . . . . . . . . . . . . . . .
431
15.3
Measuring Progress: Merit Functions . . . . . . . . . . . . . . . . . . .
432
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
16 Quadratic Programming
438
An Example: Portfolio Optimization
. . . . . . . . . . . . . . . . . . .
440
16.1
Equality–Constrained Quadratic Programs . . . . . . . . . . . . . . . .
441
Properties of Equality-Constrained QPs . . . . . . . . . . . . . . . . . .
442
16.2
Solving the KKT System . . . . . . . . . . . . . . . . . . . . . . . . . .
445
Direct Solution of the KKT System
. . . . . . . . . . . . . . . . . . . .
446
Range-Space Method
. . . . . . . . . . . . . . . . . . . . . . . . . . .
447
Null-Space Method
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
448
A Method Based on Conjugacy
. . . . . . . . . . . . . . . . . . . . . .
450

xviii
C o n t e n t s
16.3
Inequality-Constrained Problems . . . . . . . . . . . . . . . . . . . . .
451
Optimality Conditions for Inequality-Constrained Problems . . . . . . .
452
Degeneracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
453
16.4
Active-Set Methods for Convex QP . . . . . . . . . . . . . . . . . . . .
455
Speciﬁcation of the Active-Set Method for Convex QP . . . . . . . . . .
460
An Example
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
461
Further Remarks on the Active-Set Method . . . . . . . . . . . . . . . .
463
Finite Termination of the Convex QP Algorithm . . . . . . . . . . . . .
464
Updating Factorizations . . . . . . . . . . . . . . . . . . . . . . . . . .
465
16.5
Active-Set Methods for Indeﬁnite QP . . . . . . . . . . . . . . . . . . .
468
Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
470
Choice of Starting Point . . . . . . . . . . . . . . . . . . . . . . . . . .
472
Failure of the Active-Set Method
. . . . . . . . . . . . . . . . . . . . .
473
Detecting Indeﬁniteness Using the LBLT Factorization . . . . . . . . .
473
16.6
The Gradient–Projection Method . . . . . . . . . . . . . . . . . . . . .
474
Cauchy Point Computation . . . . . . . . . . . . . . . . . . . . . . . .
475
Subspace Minimization . . . . . . . . . . . . . . . . . . . . . . . . . .
478
16.7
Interior-Point Methods
. . . . . . . . . . . . . . . . . . . . . . . . . .
479
Extensions and Comparison with Active-Set Methods
. . . . . . . . . .
482
16.8
Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
482
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
483
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
17 Penalty, Barrier, and Augmented Lagrangian Methods
488
17.1
The Quadratic Penalty Method . . . . . . . . . . . . . . . . . . . . . .
490
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
490
Algorithmic Framework . . . . . . . . . . . . . . . . . . . . . . . . . .
492
Convergence of the Quadratic Penalty Function
. . . . . . . . . . . . .
493
17.2
The Logarithmic Barrier Method . . . . . . . . . . . . . . . . . . . . .
498
Properties of Logarithmic Barrier Functions
. . . . . . . . . . . . . . .
498
Algorithms Based on the Log-Barrier Function . . . . . . . . . . . . . .
503
Properties of the Log-Barrier Function and Framework 17.2 . . . . . . .
505
Handling Equality Constraints
. . . . . . . . . . . . . . . . . . . . . .
507
Relationship to Primal–Dual Methods
. . . . . . . . . . . . . . . . . .
508
17.3
Exact Penalty Functions . . . . . . . . . . . . . . . . . . . . . . . . . .
510
17.4
Augmented Lagrangian Method . . . . . . . . . . . . . . . . . . . . . .
511
Motivation and Algorithm Framework . . . . . . . . . . . . . . . . . .
512
Extension to Inequality Constraints . . . . . . . . . . . . . . . . . . . .
514
Properties of the Augmented Lagrangian . . . . . . . . . . . . . . . . .
517
Practical Implementation . . . . . . . . . . . . . . . . . . . . . . . . .
520
17.5
Sequential Linearly Constrained Methods . . . . . . . . . . . . . . . . .
522
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
523

C o n t e n t s
xix
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
524
18 Sequential Quadratic Programming
526
18.1
Local SQP Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
528
SQP Framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
529
Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
IQP vs. EQP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
18.2
Preview of Practical SQP Methods . . . . . . . . . . . . . . . . . . . . .
532
18.3
Step Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
534
Equality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . . .
534
Inequality Constraints . . . . . . . . . . . . . . . . . . . . . . . . . . .
536
18.4
The Hessian of the Quadratic Model
. . . . . . . . . . . . . . . . . . .
537
Full Quasi-Newton Approximations . . . . . . . . . . . . . . . . . . . .
538
Hessian of Augmented Lagrangian
. . . . . . . . . . . . . . . . . . . .
539
Reduced-Hessian Approximations . . . . . . . . . . . . . . . . . . . . .
540
18.5
Merit Functions and Descent
. . . . . . . . . . . . . . . . . . . . . . .
542
18.6
A Line Search SQP Method . . . . . . . . . . . . . . . . . . . . . . . .
545
18.7
Reduced-Hessian SQP Methods . . . . . . . . . . . . . . . . . . . . . .
546
Some Properties of Reduced-Hessian Methods . . . . . . . . . . . . . .
547
Update Criteria for Reduced-Hessian Updating . . . . . . . . . . . . . .
548
Changes of Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
549
A Practical Reduced-Hessian Method . . . . . . . . . . . . . . . . . . .
550
18.8
Trust-Region SQP Methods . . . . . . . . . . . . . . . . . . . . . . . .
551
Approach I: Shifting the Constraints
. . . . . . . . . . . . . . . . . . .
553
Approach II: Two Elliptical Constraints . . . . . . . . . . . . . . . . . .
554
Approach III: Sℓ1QP (Sequential ℓ1 Quadratic Programming) . . . . . .
555
18.9
A Practical Trust-Region SQP Algorithm . . . . . . . . . . . . . . . . .
558
18.10 Rate of Convergence . . . . . . . . . . . . . . . . . . . . . . . . . . . .
561
Convergence Rate of Reduced-Hessian Methods
. . . . . . . . . . . . .
563
18.11 The Maratos Effect . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
565
Second-Order Correction . . . . . . . . . . . . . . . . . . . . . . . . .
568
Watchdog (Nonmonotone) Strategy . . . . . . . . . . . . . . . . . . . .
569
Notes and References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
571
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
572
A
Background Material
574
A.1
Elements of Analysis, Geometry, Topology
. . . . . . . . . . . . . . . .
575
Topology of the Euclidean Space IRn . . . . . . . . . . . . . . . . . . . .
575
Continuity and Limits . . . . . . . . . . . . . . . . . . . . . . . . . . .
578
Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
579
Directional Derivatives
. . . . . . . . . . . . . . . . . . . . . . . . . .
581
Mean Value Theorem
. . . . . . . . . . . . . . . . . . . . . . . . . . .
582

xx
C o n t e n t s
Implicit Function Theorem . . . . . . . . . . . . . . . . . . . . . . . .
583
Geometry of Feasible Sets . . . . . . . . . . . . . . . . . . . . . . . . .
584
Order Notation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
589
Root-Finding for Scalar Equations
. . . . . . . . . . . . . . . . . . . .
590
A.2
Elements of Linear Algebra
. . . . . . . . . . . . . . . . . . . . . . . .
591
Vectors and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . .
591
Norms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
592
Subspaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
595
Eigenvalues, Eigenvectors, and the Singular-Value Decomposition . . . .
596
Determinant and Trace
. . . . . . . . . . . . . . . . . . . . . . . . . .
597
Matrix Factorizations: Cholesky, LU, QR . . . . . . . . . . . . . . . . .
598
Sherman–Morrison–Woodbury Formula . . . . . . . . . . . . . . . . .
603
Interlacing Eigenvalue Theorem . . . . . . . . . . . . . . . . . . . . . .
603
Error Analysis and Floating-Point Arithmetic . . . . . . . . . . . . . . .
604
Conditioning and Stability . . . . . . . . . . . . . . . . . . . . . . . . .
606
References
609
Index
623

Chapter1

Introduction
People optimize. Airline companies schedule crews and aircraft to minimize cost. Investors
seek to create portfolios that avoid excessive risks while achieving a high rate of return.
Manufacturers aim for maximum efﬁciency in the design and operation of their production
processes.
Nature optimizes. Physical systems tend to a state of minimum energy. The molecules
in an isolated chemical system react with each other until the total potential energy of their
electrons is minimized. Rays of light follow paths that minimize their travel time.
Optimization is an important tool in decision science and in the analysis of physical
systems. To use it, we must ﬁrst identify some objective, a quantitative measure of the per-
formance of the system under study. This objective could be proﬁt, time, potential energy,
or any quantity or combination of quantities that can be represented by a single number.
The objective depends on certain characteristics of the system, called variables or unknowns.
Our goal is to ﬁnd values of the variables that optimize the objective. Often the variables are
restricted, or constrained, in some way. For instance, quantities such as electron density in a
molecule and the interest rate on a loan cannot be negative.
The process of identifying objective, variables, and constraints for a given problem is
known as modeling. Construction of an appropriate model is the ﬁrst step—sometimes the

2
C h a p t e r
1 .
I n t r o d u c t i o n
most important step—in the optimization process. If the model is too simplistic, it will not
give useful insights into the practical problem, but if it is too complex, it may become too
difﬁcult to solve.
Once the model has been formulated, an optimization algorithm can be used to ﬁnd
its solution. Usually, the algorithm and model are complicated enough that a computer is
needed to implement this process. There is no universal optimization algorithm. Rather,
there are numerous algorithms, each of which is tailored to a particular type of optimization
problem. It is often the user’s responsibility to choose an algorithm that is appropriate for
their speciﬁc application. This choice is an important one; it may determine whether the
problem is solved rapidly or slowly and, indeed, whether the solution is found at all.
After an optimization algorithm has been applied to the model, we must be able to
recognize whether it has succeeded in its task of ﬁnding a solution. In many cases, there
are elegant mathematical expressions known as optimality conditions for checking that the
current set of variables is indeed the solution of the problem. If the optimality conditions are
notsatisﬁed,theymaygiveusefulinformationonhowthecurrentestimateofthesolutioncan
be improved. Finally, the model may be improved by applying techniques such as sensitivity
analysis, which reveals the sensitivity of the solution to changes in the model and data.
MATHEMATICAL FORMULATION
Mathematically speaking, optimization is the minimization or maximization of a
function subject to constraints on its variables. We use the following notation:
x is the vector of variables, also called unknowns or parameters;
f is the objective function, a function of x that we want to maximize or minimize;
c is the vector of constraints that the unknowns must satisfy. This is a vector function of
thevariablesx.Thenumberofcomponentsinc isthenumberofindividualrestrictions
that we place on the variables.
The optimization problem can then be written as
min
x∈IRn f (x)
subject to

ci(x)  0,
i ∈E,
ci(x) ≥0,
i ∈I.
(1.1)
Here f and each ci are scalar-valued functions of the variables x, and I, E are sets of indices.
As a simple example, consider the problem
min (x1 −2)2 + (x2 −1)2
subject to

x2
1 −x2
≤0,
x1 + x2
≤2.
(1.2)

C h a p t e r
1 .
I n t r o d u c t i o n
3
*
x
f
c1
c2
contours of 
x
x1
feasible
region
2
Figure 1.1
Geometrical representation of an optimization problem.
We can write this problem in the form (1.1) by deﬁning
f (x)  (x1 −2)2 + (x2 −1)2,
x 

x1
x2

,
c(x) 

c1(x)
c2(x)



−x2
1 + x2
−x1 −x2 + 2

,
I  {1, 2},
E  ∅.
Figure 1.1 shows the contours of the objective function, i.e., the set of points for which f (x)
has a constant value. It also illustrates the feasible region, which is the set of points satisfying
all the constraints, and the optimal point x∗, the solution of the problem. Note that the
“infeasible side” of the inequality constraints is shaded.
The example above illustrates, too, that transformations are often necessary to express
an optimization problem in the form (1.1). Often it is more natural or convenient to label
the unknowns with two or three subscripts, or to refer to different variables by completely
different names, so that relabeling is necessary to achieve the standard form. Another com-
mon difference is that we are required to maximize rather than minimize f , but we can
accommodate this change easily by minimizing −f in the formulation (1.1). Good software
systems perform the conversion between the natural formulation and the standard form
(1.1) transparently to the user.

4
C h a p t e r
1 .
I n t r o d u c t i o n
EXAMPLE: A TRANSPORTATION PROBLEM
A chemical company has 2 factories F1 and F2 and a dozen retail outlets R1, . . . , R12.
Each factory Fi can produce ai tons of a certain chemical product each week; ai is called
the capacity of the plant. Each retail outlet Rj has a known weekly demand of bj tons of the
product. The cost of shipping one ton of the product from factory Fi to retail outlet Rj is
cij.
The problem is to determine how much of the product to ship from each factory
to each outlet so as to satisfy all the requirements and minimize cost. The variables of the
problem are xij, i  1, 2, j  1, . . . , 12, where xij is the number of tons of the product
shipped from factory Fi to retail outlet Rj; see Figure 1.2. We can write the problem as
min

ij
cijxij
(1.3)
subject to
12

j1
xij ≤ai,
i  1, 2,
(1.4a)
2

i1
xij ≥bj,
j  1, . . . , 12,
(1.4b)
xij ≥0,
i  1, 2, j  1, . . . , 12.
(1.4c)
In a practical model for this problem, we would also include costs associated with manu-
facturing and storing the product. This type of problem is known as a linear programming
problem, since the objective function and the constraints are all linear functions.
CONTINUOUS VERSUS DISCRETE OPTIMIZATION
In some optimization problems the variables make sense only if they take on integer
values. Suppose that in the transportation problem just mentioned, the factories produce
tractors rather than chemicals. In this case, the xij would represent integers (that is, the
number of tractors shipped) rather than real numbers. (It would not make much sense to
advise the company to ship 5.4 tractors from factory 1 to outlet 12.) The obvious strategy
of ignoring the integrality requirement, solving the problem with real variables, and then
rounding all the components to the nearest integer is by no means guaranteed to give
solutions that are close to optimal. Problems of this type should be handled using the tools
of discrete optimization. The mathematical formulation is changed by adding the constraint
xij ∈Z,
for all i and j,

C h a p t e r
1 .
I n t r o d u c t i o n
5
f
f1
x
r1
r2
r
2
3
12
Figure 1.2
A transportation problem.
to the existing constraints (1.4), where Z is the set of all integers. The problem is then known
as an integer programming problem.
The generic term discrete optimization usually refers to problems in which the solution
we seek is one of a number of objects in a ﬁnite set. By contrast, continuous optimization
problems—the class of problems studied in this book—ﬁnd a solution from an uncountably
inﬁnite set—typically a set of vectors with real components. Continuous optimization prob-
lems are normally easier to solve, because the smoothness of the functions makes it possible
to use objective and constraint information at a particular point x to deduce information
about the function’s behavior at all points close to x. The same statement cannot be made
about discrete problems, where points that are “close” in some sense may have markedly
different function values. Moreover, the set of possible solutions is too large to make an
exhaustive search for the best value in this ﬁnite set.
Some models contain variables that are allowed to vary continuously and others that
can attain only integer values; we refer to these as mixed integer programming problems.
Discrete optimization problems are not addressed directly in this book; we refer the
reader to the texts by Papadimitriou and Steiglitz [190], Nemhauser and Wolsey [179],
Cook et al. [56], and Wolsey [249] for comprehensive treatments of this subject. We point
out, however, that the continuous optimization algorithms described here are important in
discrete optimization, where a sequence of continuous subproblems are often solved. For
instance, the branch-and-bound method for integer linear programming problems spends
muchofitstimesolvinglinearprogram“relaxations,”inwhichallthevariablesarereal.These
subproblems are usually solved by the simplex method, which is discussed in Chapter 13 of
this book.

6
C h a p t e r
1 .
I n t r o d u c t i o n
CONSTRAINED AND UNCONSTRAINED OPTIMIZATION
Problems with the general form (1.1) can be classiﬁed according to the nature of
the objective function and constraints (linear, nonlinear, convex), the number of variables
(large or small), the smoothness of the functions (differentiable or nondifferentiable), and
so on. Possibly the most important distinction is between problems that have constraints
on the variables and those that do not. This book is divided into two parts according to this
classiﬁcation.
Unconstrained optimization problems arise directly in many practical applications. If
there are natural constraints on the variables, it is sometimes safe to disregard them and
to assume that they have no effect on the optimal solution. Unconstrained problems arise
also as reformulations of constrained optimization problems, in which the constraints are
replaced by penalization terms in the objective function that have the effect of discouraging
constraint violations.
Constrained optimization problems arise from models that include explicit constraints
on the variables. These constraints may be simple bounds such as 0 ≤x1 ≤100, more
generallinearconstraintssuchas
i xi ≤1,ornonlinearinequalitiesthatrepresentcomplex
relationships among the variables.
When both the objective function and all the constraints are linear functions of x, the
problem is a linear programming problem. Management sciences and operations research
make extensive use of linear models. Nonlinear programming problems, in which at least
some of the constraints or the objective are nonlinear functions, tend to arise naturally in
the physical sciences and engineering, and are becoming more widely used in management
and economic sciences.
GLOBAL AND LOCAL OPTIMIZATION
The fastest optimization algorithms seek only a local solution, a point at which the
objective function is smaller than at all other feasible points in its vicinity. They do not
always ﬁnd the best of all such minima, that is, the global solution. Global solutions are
necessary (or at least highly desirable) in some applications, but they are usually difﬁcult
to identify and even more difﬁcult to locate. An important special case is convex program-
ming (see below), in which all local solutions are also global solutions. Linear programming
problems fall in the category of convex programming. However, general nonlinear prob-
lems, both constrained and unconstrained, may possess local solutions that are not global
solutions.
In this book we treat global optimization only in passing, focusing instead on the
computation and characterization of local solutions, issues that are central to the ﬁeld of op-
timization. We note, however, that many successful global optimization algorithms proceed
by solving a sequence of local optimization problems, to which the algorithms described in
this book can be applied. A collection of recent research papers on global optimization can
be found in Floudas and Pardalos [90].

C h a p t e r
1 .
I n t r o d u c t i o n
7
STOCHASTIC AND DETERMINISTIC OPTIMIZATION
Insomeoptimizationproblems,themodelcannotbefullyspeciﬁedbecauseitdepends
on quantities that are unknown at the time of formulation. In the transportation problem
described above, for instance, the customer demands bj at the retail outlets cannot be
speciﬁed precisely in practice. This characteristic is shared by many economic and ﬁnancial
planning models, which often depend on the future movement of interest rates and the
future behavior of the economy.
Frequently, however, modelers can predict or estimate the unknown quantities with
some degree of conﬁdence. They may, for instance, come up with a number of possible
scenarios for the values of the unknown quantities and even assign a probability to each sce-
nario. In the transportation problem, the manager of the retail outlet may be able to estimate
demand patterns based on prior customer behavior, and there may be different scenarios for
the demand that correspond to different seasonal factors or economic conditions. Stochastic
optimization algorithms use these quantiﬁcations of the uncertainty to produce solutions
that optimize the expected performance of the model.
We do not consider stochastic optimization problems further in this book, focus-
ing instead on deterministic optimization problems, in which the model is fully speciﬁed.
Many algorithms for stochastic optimization do, however, proceed by formulating one or
more deterministic subproblems, each of which can be solved by the techniques outlined
here. For further information on stochastic optimization, consult the books by Birge and
Louveaux [11] and Kall and Wallace [139].
OPTIMIZATION ALGORITHMS
Optimization algorithms are iterative. They begin with an initial guess of the optimal
values of the variables and generate a sequence of improved estimates until they reach a solu-
tion. The strategy used to move from one iterate to the next distinguishes one algorithm from
another. Most strategies make use of the values of the objective function f , the constraints c,
and possibly the ﬁrst and second derivatives of these functions. Some algorithms accumulate
information gathered at previous iterations, while others use only local information from
the current point. Regardless of these speciﬁcs (which will receive plenty of attention in the
rest of the book), all good algorithms should possess the following properties:
• Robustness. They should perform well on a wide variety of problems in their class, for
all reasonable choices of the initial variables.
• Efﬁciency. They should not require too much computer time or storage.
• Accuracy. They should be able to identify a solution with precision, without being
overly sensitive to errors in the data or to the arithmetic rounding errors that occur
when the algorithm is implemented on a computer.

8
C h a p t e r
1 .
I n t r o d u c t i o n
These goals may conﬂict. For example, a rapidly convergent method for nonlinear program-
ming may require too much computer storage on large problems. On the other hand, a
robust method may also be the slowest. Tradeoffs between convergence rate and storage
requirements, and between robustness and speed, and so on, are central issues in numerical
optimization. They receive careful consideration in this book.
The mathematical theory of optimization is used both to characterize optimal points
and to provide the basis for most algorithms. It is not possible to have a good understanding
of numerical optimization without a ﬁrm grasp of the supporting theory. Accordingly, this
book gives a solid (though not comprehensive) treatment of optimality conditions, as well as
convergenceanalysisthatrevealsthestrengthsandweaknessesofsomeofthemostimportant
algorithms.
CONVEXITY
The concept of convexity is fundamental in optimization; it implies that the problem
is benign in several respects. The term convex can be applied both to sets and to functions.
S ∈IRn is a convex set if the straight line segment connecting any two points in
S lies entirely inside S. Formally, for any two points x ∈S and y ∈S, we have
αx + (1 −α)y ∈S for all α ∈[0, 1].
f is a convex function if its domain is a convex set and if for any two points x and
y in this domain, the graph of f lies below the straight line connecting (x, f (x)) to
(y, f (y)) in the space IRn+1. That is, we have
f (αx + (1 −α)y) ≤αf (x) + (1 −α)f (y),
for all α ∈[0, 1].
When f is smooth as well as convex and the dimension n is 1 or 2, the graph of f is
bowl-shaped (See Figure 1.3), and its contours deﬁne convex sets. A function f is said to be
concave if −f is convex.
As we will see in subsequent chapters, algorithms for unconstrained optimization are
usually guaranteed to converge to a stationary point (maximizer, minimizer, or inﬂection
point) of the objective function f . If we know that f is convex, then we can be sure that
the algorithm has converged to a global minimizer. The term convex programming is used to
describe a special case of the constrained optimization problem (1.1) in which
• the objective function is convex;
• the equality constraint functions ci(·), i ∈E, are linear;
• the inequality constraint functions ci(·), i ∈I, are concave.
As in the unconstrained case, convexity allows us to make stronger claims about the
convergence of optimization algorithms than we can make for nonconvex problems.

C h a p t e r
1 .
I n t r o d u c t i o n
9
Figure 1.3
The convex
function f (x) 
(x1−6)2+ 1
25(x2−4.5)4.
NOTES AND REFERENCES
Optimization traces its roots to the calculus of variations and the work of Euler and
Lagrange. The development of linear programming in the 1940s broadened the ﬁeld and
stimulated much of the progress in modern optimization theory and practice during the last
50 years.
Optimization is often called mathematical programming, a term that is somewhat con-
fusingbecauseitsuggeststhewritingofcomputerprogramswithamathematicalorientation.
This term was coined in the 1940s, before the word “programming” became inextricably
linked with computer software. The original meaning of this word (and the intended one in
this context) was more inclusive, with connotations of problem formulation and algorithm
design and analysis.
Modeling will not be treated extensively in the book. Information about modeling
techniques for various application areas can be found in Dantzig [63], Ahuja, Magnanti,
and Orlin [1], Fourer, Gay, and Kernighan [92], and Winston [246].

Chapter2

Fundamentals of
Unconstrained
Optimization
In unconstrained optimization, we minimize an objective function that depends on real
variables, with no restrictions at all on the values of these variables. The mathematical
formulation is
min
x
f (x),
(2.1)
where x ∈IRn is a real vector with n ≥1 components and f : IRn →IR is a smooth function.
Usually, we lack a global perspective on the function f . All we know are the values of f
and maybe some of its derivatives at a set of points x0, x1, x2, . . . . Fortunately, our algorithms
get to choose these points, and they try to do so in a way that identiﬁes a solution reliably
and without using too much computer time or storage. Often, the information about f
does not come cheaply, so we usually prefer algorithms that do not call for this information
unnecessarily.

12
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
.
.
.
.
.
t 1
t 2
t 3
t m
y1
y3
y2
y
t
Figure 2.1
Least squares data ﬁtting problem.
❏Example 2.1
Suppose that we are trying to ﬁnd a curve that ﬁts some experimental data. Figure 2.1
plots measurements y1, y2, . . . , ym of a signal taken at times t1, t2, . . . , tm. From the data and
our knowledge of the application, we deduce that the signal has exponential and oscillatory
behavior of certain types, and we choose to model it by the function
φ(t; x)  x1 + x2e−(x3−t)2/x4 + x5 cos(x6t).
The real numbers xi, i  1, 2, . . . , 6, are the parameters of the model. We would like to
choose them to make the model values φ(tj; x) ﬁt the observed data yj as closely as possible.
To state our objective as an optimization problem, we group the parameters xi into a vector
of unknowns x  (x1, x2, . . . , x6)T , and deﬁne the residuals
rj(x)  yj −φ(tj; x),
j  1, . . . , m,
(2.2)
which measure the discrepancy between the model and the observed data. Our estimate of
x will be obtained by solving the problem
min
x∈IR6 f (x)  r2
1(x) + · · · + r2
m(x).
(2.3)

2 . 1 .
W h a t I s a S o l u t i o n ?
13
This is a nonlinear least-squares problem, a special case of unconstrained optimization.
Itillustratesthatsomeobjectivefunctionscanbeexpensivetoevaluateevenwhenthenumber
of variables is small. Here we have n  6, but if the number of measurements m is large (105,
say), evaluation of f (x) for a given parameter vector x is a signiﬁcant computation.
❐
Suppose that for the data given in Figure 2.1 the optimal solution of (2.3) is ap-
proximately x∗ (1.1, 0.01, 1.2, 1.5, 2.0, 1.5) and the corresponding function value is
f (x∗)  0.34. Because the optimal objective is nonzero, there must be discrepancies be-
tween the observed measurements yj and the model predictions φ(tj, x∗) for some (usually
most) values of j—the model has not reproduced all the data points exactly. How, then, can
we verify that x∗is indeed a minimizer of f ? To answer this question, we need to deﬁne the
term “solution” and explain how to recognize solutions. Only then can we discuss algorithms
for unconstrained optimization problems.
2.1
WHAT IS A SOLUTION?
Generally, we would be happiest if we found a global minimizer of f , a point where the
function attains its least value. A formal deﬁnition is
A point x∗is a global minimizer if f (x∗) ≤f (x) for all x,
where x ranges over all of IRn (or at least over the domain of interest to the modeler). The
global minimizer can be difﬁcult to ﬁnd, because our knowledge of f is usually only local.
Since our algorithm does not visit many points (we hope!), we usually do not have a good
picture of the overall shape of f , and we can never be sure that the function does not take a
sharp dip in some region that has not been sampled by the algorithm. Most algorithms are
able to ﬁnd only a local minimizer, which is a point that achieves the smallest value of f in
its neighborhood. Formally, we say:
A point x∗is a local minimizer if there is a neighborhood N of x∗such that f (x∗) ≤
f (x) for x ∈N.
(Recallthataneighborhoodof x∗issimplyanopensetthatcontainsx∗.)Apointthatsatisﬁes
this deﬁnition is sometimes called a weak local minimizer. This terminology distinguishes it
from a strict local minimizer, which is the outright winner in its neighborhood. Formally,
A point x∗is a strict local minimizer (also called a strong local minimizer) if there is a
neighborhood N of x∗such that f (x∗) < f (x) for all x ∈N with x ̸ x∗.

14
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
For the constant function f (x)  2, every point x is a weak local minimizer, while the
function f (x)  (x −2)4 has a strict local minimizer at x  2.
A slightly more exotic type of local minimizer is deﬁned as follows.
A point x∗is an isolated local minimizer if there is a neighborhood N of x∗such that
x∗is the only local minimizer in N.
Some strict local minimizers are not isolated, as illustrated by the function
f (x)  x4 cos(1/x) + 2x4,
f (0)  0,
which is twice continuously differentiable and has a strict local minimizer at x∗ 0. How-
ever, there are strict local minimizers at many nearby points xn, and we can label these points
so that xn →0 as n →∞.
While strict local minimizers are not always isolated, it is true that all isolated local
minimizers are strict.
Figure 2.2 illustrates a function with many local minimizers. It is usually difﬁcult to
ﬁnd the global minimizer for such functions, because algorithms tend to be “trapped” at the
local minimizers. This example is by no means pathological. In optimization problems
associated with the determination of molecular conformation, the potential function to be
minimized may have millions of local minima.
Sometimeswehaveadditional“global”knowledgeaboutf thatmayhelpinidentifying
global minima. An important special case is that of convex functions, for which every local
minimizer is also a global minimizer.
f
x
Figure 2.2
A difﬁcult case for global minimization.

2 . 1 .
W h a t I s a S o l u t i o n ?
15
RECOGNIZING A LOCAL MINIMUM
From the deﬁnitions given above, it might seem that the only way to ﬁnd out whether
a point x∗is a local minimum is to examine all the points in its immediate vicinity, to make
sure that none of them has a smaller function value. When the function f is smooth, however,
there are much more efﬁcient and practical ways to identify local minima. In particular, if f
is twice continuously differentiable, we may be able to tell that x∗is a local minimizer (and
possibly a strict local minimizer) by examining just the gradient ∇f (x∗) and the Hessian
∇2f (x∗).
The mathematical tool used to study minimizers of smooth functions is Taylor’s the-
orem. Because this theorem is central to our analysis throughout the book, we state it now.
Its proof can be found in any calculus textbook.
Theorem 2.1 (Taylor’s Theorem).
Suppose that f : IRn →IR is continuously differentiable and that p ∈IRn. Then we have
that
f (x + p)  f (x) + ∇f (x + tp)T p,
(2.4)
for some t ∈(0, 1). Moreover, if f is twice continuously differentiable, we have that
∇f (x + p)  ∇f (x) +
 1
0
∇2f (x + tp)p dt,
(2.5)
and that
f (x + p)  f (x) + ∇f (x)T p + 1
2pT ∇2f (x + tp)p,
(2.6)
for some t ∈(0, 1).
Necessary conditions for optimality are derived by assuming that x∗is a local minimizer
and then proving facts about ∇f (x∗) and ∇2f (x∗).
Theorem 2.2 (First-Order Necessary Conditions).
If x∗is a local minimizer and f is continuously differentiable in an open neighborhood
of x∗, then ∇f (x∗)  0.
Proof.
Suppose for contradiction that ∇f (x∗) ̸ 0. Deﬁne the vector p  −∇f (x∗) and
note that pT ∇f (x∗)  −∥∇f (x∗)∥2 < 0. Because ∇f is continuous near x∗, there is a
scalar T > 0 such that
pT ∇f (x∗+ tp) < 0,
for all t ∈[0, T ].

16
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
For any ¯t ∈(0, T ], we have by Taylor’s theorem that
f (x∗+ ¯tp)  f (x∗) + ¯tpT ∇f (x∗+ tp),
for some t ∈(0, ¯t).
Therefore, f (x∗+ ¯tp) < f (x∗) for all ¯t ∈(0, T ]. We have found a direction leading
away from x∗along which f decreases, so x∗is not a local minimizer, and we have a
contradiction.
□
We call x∗a stationary point if ∇f (x∗)  0. According to Theorem 2.2, any local
minimizer must be a stationary point.
For the next result we recall that a matrix B is positive deﬁnite if pT Bp > 0 for all
p ̸ 0, and positive semideﬁnite if pT Bp ≥0 for all p (see the Appendix).
Theorem 2.3 (Second-Order Necessary Conditions).
If x∗is a local minimizer of f and ∇2f is continuous in an open neighborhood of x∗,
then ∇f (x∗)  0 and ∇2f (x∗) is positive semideﬁnite.
Proof.
We know from Theorem 2.2 that ∇f (x∗)  0. For contradiction, assume
that ∇2f (x∗) is not positive semideﬁnite. Then we can choose a vector p such that
pT ∇2f (x∗)p < 0, and because ∇2f is continuous near x∗, there is a scalar T > 0 such that
pT ∇2f (x∗+ tp)p < 0 for all t ∈[0, T ].
By doing a Taylor series expansion around x∗, we have for all ¯t ∈(0, T ] and some
t ∈(0, ¯t) that
f (x∗+ ¯tp)  f (x∗) + ¯tpT ∇f (x∗) + 1
2 ¯t2pT ∇2f (x∗+ tp)p < f (x∗).
As in Theorem 2.2, we have found a direction from x∗along which f is decreasing, and so
again, x∗is not a local minimizer.
□
We now describe sufﬁcient conditions, which are conditions on the derivatives of f at
the point z∗that guarantee that x∗is a local minimizer.
Theorem 2.4 (Second-Order Sufﬁcient Conditions).
Suppose that ∇2f is continuous in an open neighborhood of x∗and that ∇f (x∗)  0
and ∇2f (x∗) is positive deﬁnite. Then x∗is a strict local minimizer of f .
Proof.
Because the Hessian is continuous and positive deﬁnite at x∗, we can choose a radius
r > 0 so that ∇2f (x) remains positive deﬁnite for all x in the open ball D  {z | ∥z−x∗∥<
r}. Taking any nonzero vector p with ∥p∥< r, we have x∗+ p ∈D and so
f (x∗+ p)  f (x∗) + pT ∇f (x∗) + 1
2pT ∇2f (z)p
 f (x∗) + 1
2pT ∇2f (z)p,

2 . 1 .
W h a t I s a S o l u t i o n ?
17
where z  x∗+tp for some t ∈(0, 1). Since z ∈D, we have pT ∇2f (z)p > 0, and therefore
f (x∗+ p) > f (x∗), giving the result.
□
Note that the second-order sufﬁcient conditions of Theorem 2.4 guarantee something
stronger than the necessary conditions discussed earlier; namely, that the minimizer is astrict
local minimizer. Note too that the second-order sufﬁcient conditions are not necessary: A
point x∗may be a strict local minimizer, and yet may fail to satisfy the sufﬁcient conditions.
A simple example is given by the function f (x)  x4, for which the point x∗ 0 is a strict
local minimizer at which the Hessian matrix vanishes (and is therefore not positive deﬁnite).
When the objective function is convex, local and global minimizers are simple to
characterize.
Theorem 2.5.
When f is convex, any local minimizer x∗is a global minimizer of f . If in addition f is
differentiable, then any stationary point x∗is a global minimizer of f .
Proof.
Suppose that x∗is a local but not a global minimizer. Then we can ﬁnd a point
z ∈IRn with f (z) < f (x∗). Consider the line segment that joins x∗to z, that is,
x  λz + (1 −λ)x∗,
for some λ ∈(0, 1].
(2.7)
By the convexity property for f , we have
f (x) ≤λf (z) + (1 −λ)f (x∗) < f (x∗).
(2.8)
Any neighborhood N of x∗contains a piece of the line segment (2.7), so there will always
be points x ∈N at which (2.8) is satisﬁed. Hence, x∗is not a local minimizer.
For the second part of the theorem, suppose that x∗is not a global minimizer and
choose z as above. Then, from convexity, we have
∇f (x∗)T (z −x∗)  d
dλf (x∗+ λ(z −x∗)) |λ0
(see the Appendix)
 lim
λ↓0
f (x∗+ λ(z −x∗)) −f (x∗)
λ
≤lim
λ↓0
λf (z) + (1 −λ)f (x∗) −f (x∗)
λ
 f (z) −f (x∗) < 0.
Therefore, ∇f (x∗) ̸ 0, and so x∗is not a stationary point.
□
These results, which are based on elementary calculus, provide the foundations for
unconstrained optimization algorithms. In one way or another, all algorithms seek a point
where ∇f (·) vanishes.

18
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
NONSMOOTH PROBLEMS
This book focuses on smooth functions, by which we generally mean functions whose
second derivatives exist and are continuous. We note, however, that there are interesting
problems in which the functions involved may be nonsmooth and even discontinuous.
It is not possible in general to identify a minimizer of a general discontinuous function.
If, however, the function consists of a few smooth pieces, with discontinuities between the
pieces,itmaybepossibletoﬁndtheminimizerbyminimizingeachsmoothpieceindividually.
If the function is continuous everywhere but nondifferentiable at certain points, as in
Figure 2.3, we can identify a solution by examing the subgradient, or generalized gradient,
which is a generalization of the concept of gradient to the nonsmooth case. Nonsmooth
optimization is beyond the scope of this book; we refer instead to Hiriart-Urruty and
Lemar´echal [137] for an extensive discussion of theory. Here, we mention only that the
minimization of a function such as the one illustrated in Figure 2.3 (which contains a jump
discontinuity in the ﬁrst derivative f ′(x) at the minimum) is difﬁcult because the behav-
ior of f is not predictable near the point of nonsmoothness. That is, we cannot be sure
that information about f obtained at one point can be used to infer anything about f at
neighboring points, because points of nondifferentiability may intervene. However, certain
special nondifferentiable functions, such as functions of the form
f (x)  ∥r(x)∥1,
f (x)  ∥r(x)∥∞
(where r(x) is the residual vector reﬁned in (2.2)), can be solved with the help of special-
purpose algorithms; see, for example, Fletcher [83, Chapter 14].
x
f
x*
Figure 2.3
Nonsmooth function with minimum at a kink.

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
19
2.2
OVERVIEW OF ALGORITHMS
The last thirty years has seen the development of a powerful collection of algorithms for
unconstrained optimization of smooth functions. We now give a broad description of their
main properties, and we describe them in more detail in Chapters 3, 4, 5, 6, 8, and 9. All
algorithmsforunconstrainedminimizationrequiretheusertosupplyastartingpoint,which
we usually denote by x0. The user with knowledge about the application and the data set may
be in a good position to choose x0 to be a reasonable estimate of the solution. Otherwise,
the starting point must be chosen in some arbitrary manner.
Beginning at x0, optimization algorithms generate a sequence of iterates {xk}∞
k0 that
terminate when either no more progress can be made or when it seems that a solution
point has been approximated with sufﬁcient accuracy. In deciding how to move from one
iterate xk to the next, the algorithms use information about the function f at xk, and
possibly also information from earlier iterates x0, x1, . . . , xk−1. They use this information
to ﬁnd a new iterate xk+1 with a lower function value than xk. (There exist nonmonotone
algorithms that do not insist on a decrease in f at every step, but even these algorithms
require f to be decreased after some prescribed number m of iterations. That is, they enforce
f (xk) < f (xk−m).)
There are two fundamental strategies for moving from the current point xk to a new
iterate xk+1. Most of the algorithms described in this book follow one of these approaches.
TWO STRATEGIES: LINE SEARCH AND TRUST REGION
In the line search strategy, the algorithm chooses a direction pk and searches along this
directionfromthecurrentiteratexk foranewiteratewithalowerfunctionvalue.Thedistance
to move along pk can be found by approximately solving the following one-dimensional
minimization problem to ﬁnd a step length α:
min
α>0 f (xk + αpk).
(2.9)
By solving (2.9) exactly, we would derive the maximum beneﬁt from the direction pk, but
an exact minimization is expensive and unnecessary. Instead, the line search algorithm
generates a limited number of trial step lengths until it ﬁnds one that loosely approximates
the minimum of (2.9). At the new point a new search direction and step length are computed,
and the process is repeated.
In the second algorithmic strategy, known as trust region, the information gathered
about f is used to construct a model function mk whose behavior near the current point
xk is similar to that of the actual objective function f . Because the model mk may not be a
good approximation of f when x is far from xk, we restrict the search for a minimizer of mk
to some region around xk. In other words, we ﬁnd the candidate step p by approximately

20
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
solving the following subproblem:
min
p
mk(xk + p),
where xk + p lies inside the trust region.
(2.10)
If the candidate solution does not produce a sufﬁcient decrease in f , we conclude that the
trust region is too large, and we shrink it and re-solve (2.10). Usually, the trust region is a
ball deﬁned by ∥p∥2 ≤, where the scalar  > 0 is called the trust-region radius. Elliptical
and box-shaped trust regions may also be used.
The model mk in (2.10) is usually deﬁned to be a quadratic function of the form
mk(xk + p)  fk + pT ∇fk + 1
2pT Bkp,
(2.11)
where fk, ∇fk, and Bk are a scalar, vector, and matrix, respectively. As the notation indicates,
fk and ∇fk are chosen to be the function and gradient values at the point xk, so that mk
and f are in agreement to ﬁrst order at the current iterate xk. The matrix Bk is either the
Hessian ∇2fk or some approximation to it.
Suppose that the objective function is given by f (x)  10(x2 −x2
1)2 + (1 −x1)2. At
the point xk  (0, 1) its gradient and Hessian are
∇fk 

−2
20

,
∇2fk 

−38
0
0
20

.
The contour lines of the quadratic model (2.11) with Bk  ∇2fk are depicted in Figure 2.4,
which also illustrates the contours of the objective function f and the trust region. We have
indicated contour lines where the model mk has values 1 and 12. Note from Figure 2.4 that
each time we decrease the size of the trust region after failure of a candidate iterate, the step
from xk to the new candidate will be shorter, and it usually points in a different direction
from the previous candidate. The trust-region strategy differs in this respect from line search,
which stays with a single search direction.
In a sense, the line search and trust-region approaches differ in the order in which they
choose the direction and distance of the move to the next iterate. Line search starts by ﬁxing
the direction pk and then identifying an appropriate distance, namely the step length αk. In
trust region, we ﬁrst choose a maximum distance—the trust-region radius k—and then
seek a direction and step that attain the best improvement possible subject to this distance
constraint. If this step proves to be unsatisfactory, we reduce the distance measure k and
try again.
The line search approach is discussed in more detail in Chapter 3. Chapter 4 discusses
the trust-region strategy, including techniques for choosing and adjusting the size of the
region and for computing approximate solutions to the trust-region problems (2.10). We
now preview two major issues: choice of the search direction pk in line search methods, and
choice of the Hessian Bk in trust-region methods. These issues are closely related, as we now
observe.

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
21
contours
of model
of f
*
k
unconstrained
minimizer
contours
pk
kp
x
1
m =
m =12
Figure2.4
Two possible trust regions (circles) and their corresponding steps pk. The
solid lines are contours of the model function mk.
SEARCH DIRECTIONS FOR LINE SEARCH METHODS
The steepest-descent direction −∇fk is the most obvious choice for search direction
for a line search method. It is intuitive; among all the directions we could move from xk,
it is the one along which f decreases most rapidly. To verify this claim, we appeal again to
Taylor’stheorem(Theorem2.1),whichtellsusthatforanysearchdirectionp andstep-length
parameter α, we have
f (xk + αp)  f (xk) + αpT ∇fk + 1
2α2pT ∇2f (xk + tp)p,
for some t ∈(0, α)
(see (2.6)). The rate of change in f along the direction p at xk is simply the coefﬁcient of
α, namely, pT ∇fk. Hence, the unit direction p of most rapid decrease is the solution to the
problem
min
p
pT ∇fk,
subject to ∥p∥ 1.
(2.12)
Since pT ∇fk  ∥p∥∥∇fk∥cos θ, where θ is the angle between p and ∇fk, we have from
∥p∥ 1 that pT ∇fk  ∥∇fk∥cos θ, so the objective in (2.12) is minimized when cos θ

22
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
k
x
k
p
*
x.
Figure 2.5
Steepest descent direction for a function of two variables.
takes on its minimum value of −1 at θ  π radians. In other words, the solution to (2.12) is
p  −∇fk/∥∇fk∥,
as claimed. As we show in Figure 2.5, this direction is orthogonal to the contours of the
function.
The steepest descent method is a line search method that moves along pk  −∇fk at
everystep.Itcanchoosethesteplength αk inavarietyofways,aswediscussinChapter3.One
advantage of the steepest descent direction is that it requires calculation of the gradient ∇fk
but not of second derivatives. However, it can be excruciatingly slow on difﬁcult problems.
Line search methods may use search directions other than the steepest descent direc-
tion. In general, any descent direction—one that makes an angle of strictly less than π/2
radians with −∇fk—is guaranteed to produce a decrease in f , provided that the step length
is sufﬁciently small (see Figure 2.6). We can verify this claim by using Taylor’s theorem. From
(2.6), we have that
f (xk + ϵpk)  f (xk) + ϵpT
k ∇fk + O(ϵ2).
When pk is a downhill direction, the angle θk between pk and ∇fk has cos θk < 0, so that
pT
k ∇fk  ∥pk∥∥∇fk∥cos θk < 0.
It follows that f (xk + ϵpk) < f (xk) for all positive but sufﬁciently small values of ϵ.
Another important search direction—perhaps the most important one of all—
is the Newton direction. This direction is derived from the second-order Taylor series

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
23
_
k
f
pk
Figure 2.6
A downhill direction pk
approximation to f (xk + p), which is
f (xk + p) ≈fk + pT ∇fk + 1
2pT ∇2fkp
def mk(p).
(2.13)
Assuming for the moment that ∇2fk is positive deﬁnite, we obtain the Newton direction
by ﬁnding the vector p that minimizes mk(p). By simply setting the derivative of mk(p) to
zero, we obtain the following explicit formula:
pN
k  −∇2f −1
k ∇fk.
(2.14)
The Newton direction is reliable when the difference between the true function f (xk +
p) and its quadratic model mk(p) is not too large. By comparing (2.13) with (2.6), we see
that the only difference between these functions is that the matrix ∇2f (xk + tp) in the
third term of the expansion has been replaced by ∇2fk  ∇2f (xk). If ∇2f (·) is sufﬁciently
smooth, this difference introduces a perturbation of only O(∥p∥3) into the expansion, so
that when ∥p∥is small, the approximation f (xk + p) ≈mk(p) is very accurate indeed.
The Newton direction can be used in a line search method when ∇2fk is positive
deﬁnite, for in this case we have
∇f T
k pN
k  −pN
k
T ∇2fkpN
k ≤−σk∥pN
k∥2
for some σk > 0. Unless the gradient ∇fk (and therefore the step pN
k) is zero, we have that
∇f T
k pN
k < 0, so the Newton direction is a descent direction. Unlike the steepest descent
direction, there is a “natural” step length of 1 associated with the Newton direction. Most

24
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
line search implementations of Newton’s method use the unit step α  1 where possible and
adjust this step length only when it does not produce a satisfactory reduction in the value
of f .
When ∇2fk is not positive deﬁnite, the Newton direction may not even be deﬁned,
since ∇2f −1
k
may not exist. Even when it is deﬁned, it may not satisfy the descent property
∇f T
k pN
k < 0, in which case it is unsuitable as a search direction. In these situations, line
search methods modify the deﬁnition of pk to make it satisfy the downhill condition while
retaining the beneﬁt of the second-order information contained in ∇2fk. We will describe
these modiﬁcations in Chapter 6.
Methods that use the Newton direction have a fast rate of local convergence, typically
quadratic. When a neighborhood of the solution is reached, convergence to high accuracy
often occurs in just a few iterations. The main drawback of the Newton direction is the
need for the Hessian ∇2f (x). Explicit computation of this matrix of second derivatives is
sometimes, though not always, a cumbersome, error-prone, and expensive process.
Quasi-Newton search directions provide an attractive alternative in that they do not
require computation of the Hessian and yet still attain a superlinear rate of convergence.
In place of the true Hessian ∇2fk, they use an approximation Bk, which is updated after
each step to take account of the additional knowledge gained during the step. The updates
make use of the fact that changes in the gradient g provide information about the second
derivative of f along the search direction. By using the expression (2.5) from our statement
of Taylor’s theorem, we have by adding and subtracting the term ∇2f (x)p that
∇f (x + p)  ∇f (x) + ∇2f (x)p +
 1
0

∇2f (x + tp) −∇2f (x)

p dt.
Because ∇f (·) is continuous, the size of the ﬁnal integral term is o(∥p∥). By setting x  xk
and p  xk+1 −xk, we obtain
∇fk+1  ∇fk + ∇2fk+1(xk+1 −xk) + o(∥xk+1 −xk∥).
When xk and xk+1 lie in a region near the solution x∗, within which ∇f is positive deﬁnite,
the ﬁnal term in this expansion is eventually dominated by the ∇2fk(xk+1 −xk) term, and
we can write
∇2fk+1(xk+1 −xk) ≈∇fk+1 −∇fk.
(2.15)
We choose the new Hessian approximation Bk+1 so that it mimics this property (2.15) of
the true Hessian, that is, we require it to satisfy the following condition, known as the secant
equation:
Bk+1sk  yk,
(2.16)

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
25
where
sk  xk+1 −xk,
yk  ∇fk+1 −∇fk.
Typically, we impose additional requirements on Bk+1, such as symmetry (motivated by
symmetry of the exact Hessian), and a restriction that the difference between successive
approximation Bk to Bk+1 have low rank. The initial approximation B0 must be chosen by
the user.
Two of the most popular formulae for updating the Hessian approximation Bk are the
symmetric-rank-one (SR1) formula, deﬁned by
Bk+1  Bk + (yk −Bksk)(yk −Bksk)T
(yk −Bksk)T sk
,
(2.17)
and the BFGS formula, named after its inventors, Broyden, Fletcher, Goldfarb, and Shanno,
which is deﬁned by
Bk+1  Bk −BksksT
k Bk
sT
k Bksk
+ ykyT
k
yT
k sk
.
(2.18)
Note that the difference between the matrices Bk and Bk+1 is a rank-one matrix in the
case of (2.17), and a rank-two matrix in the case of (2.18). Both updates satisfy the secant
equation and both maintain symmetry. One can show that BFGS update (2.18) generates
positive deﬁnite approximations whenever the initial approximation B0 is positive deﬁnite
and sT
k yk > 0. We discuss these issues further in Chapter 8.
The quasi-Newton search direction is given by using Bk in place of the exact Hessian
in the formula (2.14), that is,
pk  −B−1
k ∇fk.
(2.19)
Some practical implementations of quasi-Newton methods avoid the need to factorize Bk
at each iteration by updating the inverse of Bk, instead of Bk itself. In fact, the equivalent
formula for (2.17) and (2.18), applied to the inverse approximation Hk
def B−1
k , is
Hk+1 
	
I −ρkskyT
k

Hk
	
I −ρkyksT
k

+ ρksksT
k ,
ρk 
1
yT
k sk
.
(2.20)
Calculation of pk can then be performed by using the formula pk  −Hk∇fk. This can
be implemented as a matrix–vector multiplication, which is typically simpler than the
factorization/back-substitution procedure that is needed to implement the formula (2.19).
Two variants of quasi-Newton methods designed to solve large problems—partially
separable and limited-memory updating—are described in Chapter 9.

26
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
The last class of search directions we preview here is that generated by nonlinear
conjugate gradient methods. They have the form
pk  −∇f (xk) + βkpk−1,
where βk is a scalar that ensures that pk and pk−1 are conjugate—an important concept in the
minimization of quadratic functions that will be deﬁned in Chapter 5. Conjugate gradient
methods were originally designed to solve systems of linear equations Ax  b, where the
coefﬁcient matrix A is symmetric and positive deﬁnite. The problem of solving this linear
system is equivalent to the problem of minimizing the convex quadratic function deﬁned by
φ(x)  1
2xT Ax + bT x,
so it was natural to investigate extensions of these algorithms to more general types of
unconstrained minimization problems. In general, nonlinear conjugate gradient directions
are much more effective than the steepest descent direction and are almost as simple to
compute. These methods do not attain the fast convergence rates of Newton or quasi-
Newton methods, but they have the advantage of not requiring storage of matrices. An
extensive discussion of nonlinear conjugate gradient methods is given in Chapter 5.
All of the search directions discussed so far can be used directly in a line search
framework. They give rise to the steepest descent, Newton, quasi-Newton, and conjugate
gradient line search methods. All except conjugate gradients have an analogue in the trust-
region framework, as we now discuss.
MODELS FOR TRUST-REGION METHODS
If we set Bk  0 in (2.11) and deﬁne the trust region using the Euclidean norm, the
trust-region subproblem (2.10) becomes
min
p
fk + pT ∇fk
subject to ∥p∥2 ≤k.
We can write the solution to this problem in closed form as
pk  −k∇fk
∥∇fk∥.
This is simply a steepest descent step in which the step length is determined by the trust-
region radius; the trust-region and line search approaches are essentially the same in this
case.
A more interesting trust-region algorithm is obtained by choosing Bk to be the exact
Hessian ∇2fk in the quadratic model (2.11). Because of the trust-region restriction ∥p∥2 ≤
k, there is no need to do anything special when ∇2fk is not positive deﬁnite, since the

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
27
subproblem(2.10)isguaranteedtohaveasolutionpk,asweseeinFigure2.4.Thetrust-region
Newton method has proved to be highly effective in practice, as we discuss in Chapter 6.
If the matrix Bk in the quadratic model function mk of (2.11) is deﬁned by means of
a quasi-Newton approximation, we obtain a trust-region quasi-Newton method.
SCALING
The performance of an algorithm may depend crucially on how the problem is formu-
lated.Oneimportantissueinproblemformulationisscaling.Inunconstrainedoptimization,
a problem is said to be poorly scaled if changes to x in a certain direction produce much larger
variations in the value of f than do changes to x in another direction. A simple example is
provided by the function f (x)  109x2
1 + x2
2, which is very sensitive to small changes in x1
but not so sensitive to perturbations in x2.
Poorly scaled functions arise, for example, in simulations of physical and chemical
systems where different processes are taking place at very different rates. To be more speciﬁc,
consider a chemical system in which four reactions occur. Associated with each reaction is
a rate constant that describes the speed at which the reaction takes place. The optimization
problem is to ﬁnd values for these rate constants by observing the concentrations of each
chemical in the system at different times. The four constants differ greatly in magnitude,
since the reactions take place at vastly different speeds. Suppose we have the following
rough estimates for the ﬁnal values of the constants, each correct to within, say, an order of
magnitude:
x1 ≈10−10,
x2 ≈x3 ≈1,
x4 ≈105.
Before solving this problem we could introduce a new variable z deﬁned by


x1
x2
x3
x4





10−10
0
0
0
0
1
0
0
0
0
1
0
0
0
0
105




z1
z2
z3
z4


,
and then deﬁne and solve the optimization problem in terms of the new variable z. The
optimal values of z will be within about an order of magnitude of 1, making the solution
more balanced. This kind of scaling of the variables is known as diagonal scaling.
Scaling is performed (sometimes unintentionally) when the units used to represent
variables are changed. During the modeling process, we may decide to change the units of
some variables, say from meters to millimeters. If we do, the range of those variables and
their size relative to the other variables will both change.
Some optimization algorithms, such as steepest descent, are sensitive to poor scaling,
while others, such as Newton’s method, are unaffected by it. Figure 2.7 shows the contours

28
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
fk
fk
-
-
Figure 2.7
Poorly scaled and well-scaled problems, and performance of the steepest
descent direction.
of two convex nearly quadratic functions, the ﬁrst of which is poorly scaled, while the second
is well scaled. For the poorly scaled problem, the one with highly elongated contours, the
steepest descent direction (also shown on the graph) does not yield much reduction in the
function, while for the well-scaled problem it performs much better. In both cases, Newton’s
method will produce a much better step, since the second-order quadratic model (mk in
(2.13)) happens to be a good approximation of f .
Algorithms that are not sensitive to scaling are preferable to those that are not, because
they can handle poor problem formulations in a more robust fashion. In designing complete
algorithms, we try to incorporate scale invariance into all aspects of the algorithm, including
the line search or trust-region strategies and convergence tests. Generally speaking, it is easier
to preserve scale invariance for line search algorithms than for trust-region algorithms.
RATES OF CONVERGENCE
One of the key measures of performance of an algorithm is its rate of convergence. We
now deﬁne the terminology associated with different types of convergence, for reference in
later chapters.
Let {xk} be a sequence in IRn that converges to x∗. We say that the convergence is
Q-linear if there is a constant r ∈(0, 1) such that
∥xk+1 −x∗∥
∥xk −x∗∥
≤r,
for all k sufﬁciently large.
(2.21)

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
29
This means that the distance to the solution x∗decreases at each iteration by at least a
constant factor. For example, the sequence 1 + (0.5)k converges Q-linearly to 1. The preﬁx
“Q” stands for “quotient,” because this type of convergence is deﬁned in terms of the quotient
of successive errors.
The convergence is said to be Q-superlinear if
lim
k→∞
∥xk+1 −x∗∥
∥xk −x∗∥
 0.
For example, the sequence 1 + k−k converges superlinearly to 1. (Prove this statement!)
Q-quadratic convergence, an even more rapid convergence rate, is obtained if
∥xk+1 −x∗∥
∥xk −x∗∥2 ≤M,
for all k sufﬁciently large,
where M is a positive constant, not necessarily less than 1. An example is the sequence
1 + (0.5)2k.
Thespeedofconvergencedependsonr and(moreweakly)onM,whosevaluesdepend
not only on the algorithm but also on the properties of the particular problem. Regardless of
these values, however, a quadratically convergent sequence will always eventually converge
faster than a linearly convergent sequence.
Obviously, any sequence that converges Q-quadratically also converges Q-super-
linearly, and any sequence that converges Q-superlinearly also converges Q-linearly. We
can also deﬁne higher rates of convergence (cubic, quartic, and so on), but these are less
interesting in practical terms. In general, we say that the Q-order of convergence is p (with
p > 1) if there is a positive constant M such that
∥xk+1 −x∗∥
∥xk −x∗∥p ≤M,
for all k sufﬁciently large.
Quasi-NewtonmethodstypicallyconvergeQ-superlinearly,whereasNewton’smethod
converges Q-quadratically. In contrast, steepest descent algorithms converge only at a Q-
linear rate, and when the problem is ill-conditioned the convergence constant r in (2.21) is
close to 1.
Throughout the book we will normally omit the letter Q and simply talk about
superlinear convergence, quadratic convergence, etc.
R-RATES OF CONVERGENCE
A slightly weaker form of convergence, characterized by the preﬁx “R” (for “root”),
is concerned with the overall rate of decrease in the error, rather than the decrease over a
single step of the algorithm. We say that convergence is R-linear if there is a sequence of

30
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
nonnegative scalars {νk} such that
∥xk −x∗∥≤νk for all k, and {νk} converges Q-linearly to zero.
The sequence {∥xk −x∗∥} is said to be dominated by {νk}. For instance, the sequence
xk 

1 + (0.5)k,
k even,
1,
k odd,
(2.22)
(the ﬁrst few iterates are 2, 1, 1.25, 1, 1.03125, 1, . . .) converges R-linearly to 1, because it is
dominated by the sequence 1+(0.5)k, which converges Q-linearly. Likewise, we say that {xk}
converges R-superlinearly to x∗if {∥xk −x∗∥} is dominated by a Q-superlinear sequence, and
{xk} converges R-quadratically to x∗if {∥xk −x∗∥} is dominated by a Q-quadratic sequence.
Note that in the R-linear sequence (2.22), the error actually increases at every second
iteration! Such behavior occurs even in sequences whose R-rate of convergence is arbitrarily
high, but it cannot occur for Q-linear sequences, which insist on a decrease at every step k,
for k sufﬁciently large.
Most convergence analyses of optimization algorithms are concerned with Q-
convergence.
NOTES AND REFERENCES
For an extensive discussion of convergence rates see Ortega and Rheinboldt [185].
✐
E x e r c i s e s
✐
2.1 Compute the gradient ∇f (x) and Hessian ∇2f (x) of the Rosenbrock function
f (x)  100(x2 −x2
1)2 + (1 −x1)2.
(2.23)
Show that x∗ (1, 1)T is the only local minimizer of this function, and that the Hessian
matrix at that point is positive deﬁnite.
✐
2.2 Show that the function f (x)  8x1 + 12x2 + x2
1 −2x2
2 has only one stationary
point, and that it is neither a maximum or minimum, but a saddle point. Sketch the contour
lines of f .
✐
2.3 Let a be a given n-vector, and A be a given n × n symmetric matrix. Compute the
gradient and Hessian of f1(x)  aT x and f2(x)  xT Ax.
✐
2.4 Write the second-order Taylor expansion (2.6) for the function cos(1/x) around
a nonzero point x, and the third-order Taylor expansion of cos(x) around any point x.
Evaluate the second expansion for the speciﬁc case of x  1.

2 . 2 .
O v e r v i e w o f A l g o r i t h m s
31
✐
2.5 Consider the function f : IR2 →IR deﬁned by f (x)  ∥x∥2. Show that the
sequence of iterates {xk} deﬁned by
xk 

1 + 1
2k
 
cos k
sin k

satisﬁes f (xk+1) < f (xk) for k  0, 1, 2, . . . . Show that every point on the unit circle
{x | ∥x∥2  1} is a limit point for {xk}. Hint: Every value θ ∈[0, 2π] is a limit point of the
subsequence {ξk} deﬁned by
ξk  k(mod 2π)  k −2π
 k
2π

,
where the operator ⌊·⌋denotes rounding down to the next integer.
✐
2.6 Prove that all isolated local minimizers are strict. (Hint: Take an isolated local
minimizer x∗and a neighborhood N. Show that for any x ∈N, x ̸ x∗we must have
f (x) > f (x∗).)
✐
2.7 Suppose that f is a convex function. Show that the set of global minimizers of f
is a convex set.
✐
2.8 Consider the function f (x1, x2) 
	
x1 + x2
2

2. At the point xT  (1, 0) we
consider the search direction pT  (−1, 1). Show that p is a descent direction and ﬁnd all
minimizers of the problem (2.9).
✐
2.9 Suppose that ˜f (z)  f (x), where x  Sz + s for some S ∈IRn×n and s ∈IRn.
Show that
∇˜f (z)  ST ∇f (x),
∇2 ˜f (z)  ST ∇2f (x)S.
(Hint: Use the chain rule to express d ˜f /dzj in terms of df /dxi and dxi/dzj for all i, j 
1, 2, . . . , n.)
✐
2.10 Show that the symmetric rank-one update (2.17) and the BFGS update (2.18)
are scale-invariant if the initial Hessian approximations B0 are chosen appropriately. That is,
using the notation of the previous exercise, show that if these methods are applied to f (x)
starting from x0  Sz0 + s with initial Hessian B0, and to ˜f (z) starting from z0 with initial
Hessian ST B0S, then all iterates are related by xk  Szk + s. (Assume for simplicity that the
methods take unit step lengths.)
✐
2.11 Suppose that a function f of two variables is poorly scaled at the solution x∗.
Write two Taylor expansions of f around x∗—one along each coordinate direction—and
use them to show that the Hessian ∇2f (x∗) is ill-conditioned.

32
C h a p t e r
2 .
F u n d a m e n t a l s o f U n c o n s t r a i n e d O p t i m i z a t i o n
✐
2.12 Show that the sequence xk  1/k is not Q-linearly convergent, though it does
converge to zero. (This is called sublinear convergence.)
✐
2.13 Show that the sequence xk  1 + (0.5)2k is Q–quadratically convergent to 1.
✐
2.14 Does the sequence 1/(k!) converge Q-superlinearly? Q-quadratically?
✐∗2.15 Consider the sequence {xk} deﬁned by
xk 
 	 1
4

2k
,
k even,
(xk−1)/k,
k odd.
Is this sequence Q-superlinearly convergent? Q-quadratically convergent? R-quadratically
convergent?

Chapter3

Line Search
Methods
Each iteration of a line search method computes a search direction pk and then decides how
far to move along that direction. The iteration is given by
xk+1  xk + αkpk,
(3.1)
where the positive scalar αk is called the step length. The success of a line search method
depends on effective choices of both the direction pk and the step length αk.
Most line search algorithms require pk to be a descent direction—one for which
pT
k ∇fk < 0—because this property guarantees that the function f can be reduced along
this direction, as discussed in the previous chapter. Moreover, the search direction often has
the form
pk  −B−1
k ∇fk,
(3.2)
where Bk is a symmetric and nonsingular matrix. In the steepest descent method Bk is
simply the identity matrix I, while in Newton’s method Bk is the exact Hessian ∇2f (xk).
In quasi-Newton methods, Bk is an approximation to the Hessian that is updated at every

36
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
(
φ α)
point
stationary
first
minimizer
local
first
global minimizer
α
Figure 3.1
The ideal step length is the global minimizer.
iteration by means of a low-rank formula. When pk is deﬁned by (3.2) and Bk is positive
deﬁnite, we have
pT
k ∇fk  −∇f T
k B−1
k ∇fk < 0,
and therefore pk is a descent direction.
In the next chapters we study how to choose the matrix Bk, or more generally, how
to compute the search direction. We now give careful consideration to the choice of the
step-length parameter αk.
3.1
STEP LENGTH
In computing the step length αk, we face a tradeoff. We would like to choose αk to
give a substantial reduction of f , but at the same time, we do not want to spend too much
time making the choice. The ideal choice would be the global minimizer of the univariate
function φ(·) deﬁned by
φ(α)  f (xk + αpk),
α > 0,
(3.3)
but in general, it is too expensive to identify this value (see Figure 3.1). To ﬁnd even a local
minimizer of φ to moderate precision generally requires too many evaluations of the objec-

3 . 1 .
S t e p L e n g t h
37
2
x
0x
1
x
x
x
f( )
Figure 3.2
Insufﬁcient reduction in f .
tive function f and possibly the gradient ∇f . More practical strategies perform an inexact
line search to identify a step length that achieves adequate reductions in f at minimal cost.
Typical line search algorithms try out a sequence of candidate values for α, stopping to
accept one of these values when certain conditions are satisﬁed. The line search is done in two
stages: A bracketing phase ﬁnds an interval containing desirable step lengths, and a bisection
or interpolation phase computes a good step length within this interval. Sophisticated line
search algorithms can be quite complicated, so we defer a full description until the end of
this chapter. We now discuss various termination conditions for the line search algorithm
and show that effective step lengths need not lie near minimizers of the univariate function
φ(α) deﬁned in (3.3).
A simple condition we could impose on αk is that it provide a reduction in f , i.e.,
f (xk + αkpk) < f (xk). That this is not appropriate is illustrated in Figure 3.2, where the
minimum is f ∗ −1, but the sequence of function values {5/k}, k  0, 1, . . ., converges
to zero. The difﬁculty is that we do not have sufﬁcient reduction in f , a concept we discuss
next.
THE WOLFE CONDITIONS
Apopularinexactlinesearchconditionstipulatesthatαk shouldﬁrstofallgivesufﬁcient
decrease in the objective function f , as measured by the following inequality:
f (xk + αpk) ≤f (xk) + c1α∇f T
k pk,
(3.4)

38
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
α
l( )
φ(α
f(x +
) =
k
α
k
p )
acceptable
acceptable
α
Figure 3.3
Sufﬁcient decrease condition.
for some constant c1 ∈(0, 1). In other words, the reduction in f should be proportional to
both the step length αk and the directional derivative ∇f T
k pk. Inequality (3.4) is sometimes
called the Armijo condition.
The sufﬁcient decrease condition is illustrated in Figure 3.3. The right-hand-side of
(3.4), which is a linear function, can be denoted by l(α). The function l(·) has negative slope
c1∇f T
k pk, but because c1 ∈(0, 1), it lies above the graph of φ for small positive values of
α. The sufﬁcient decrease condition states that α is acceptable only if φ(α) ≤l(α). The
intervals on which this condition is satisﬁed are shown in Figure 3.3. In practice, c1 is chosen
to be quite small, say c1  10−4.
The sufﬁcient decrease condition is not enough by itself to ensure that the algorithm
makes reasonable progress, because as we see from Figure 3.3, it is satisﬁed for all sufﬁciently
small values of α. To rule out unacceptably short steps we introduce a second requirement,
called the curvature condition, which requires αk to satisfy
∇f (xk + αkpk)T pk ≥c2∇f T
k pk,
(3.5)
for some constant c2 ∈(c1, 1), where c1 is the constant from (3.4). Note that the left-hand-
side is simply the derivative φ′(αk), so the curvature condition ensures that the slope of
φ(αk) is greater than c2 times the gradient φ′(0). This makes sense because if the slope φ′(α)
is strongly negative, we have an indication that we can reduce f signiﬁcantly by moving
further along the chosen direction. On the other hand, if the slope is only slightly negative
or even positive, it is a sign that we cannot expect much more decrease in f in this direction,

3 . 1 .
S t e p L e n g t h
39
desired
slope
k )
φ(α) =f(xk+α p
tangent
α
acceptable
acceptable
Figure 3.4
The curvature condition.
so it might make sense to terminate the line search. The curvature condition is illustrated in
Figure 3.4. Typical values of c2 are 0.9 when the search direction pk is chosen by a Newton
or quasi-Newton method, and 0.1 when pk is obtained from a nonlinear conjugate gradient
method.
The sufﬁcient decrease and curvature conditions are known collectively as the Wolfe
conditions. We illustrate them in Figure 3.5 and restate them here for future reference:
f (xk + αkpk) ≤f (xk) + c1αk∇f T
k pk,
(3.6a)
∇f (xk + αkpk)T pk ≥c2∇f T
k pk,
(3.6b)
with 0 < c1 < c2 < 1.
A step length may satisfy the Wolfe conditions without being particularly close to a
minimizer of φ, as we show in Figure 3.5. We can, however, modify the curvature condition
to force αk to lie in at least a broad neighborhood of a local minimizer or stationary point
of φ. The strong Wolfe conditions require αk to satisfy
f (xk + αkpk) ≤f (xk) + c1αk∇f T
k pk,
(3.7a)
|∇f (xk + αkpk)T pk| ≤c2|∇f T
k pk|,
(3.7b)

40
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
slope
desired
line of sufficient
decrease
l(α )
acceptable
α
φ (α ) =
αp
f(x +
k
k
)
acceptable
Figure 3.5
Step lengths satisfying the Wolfe conditions.
with 0 < c1 < c2 < 1. The only difference with the Wolfe conditions is that we no longer
allow the derivative φ′(αk) to be too positive. Hence, we exclude points that are far from
stationary points of φ.
It is not difﬁcult to prove that there exist step lengths that satisfy the Wolfe conditions
for every function f that is smooth and bounded below.
Lemma 3.1.
Suppose that f : IRn →IR is continuously differentiable. Let pk be a descent direction at
xk, and assume that f is bounded below along the ray {xk + αpk|α > 0}. Then if 0 < c1 <
c2 < 1, there exist intervals of step lengths satisfying the Wolfe conditions (3.6) and the strong
Wolfe conditions (3.7).
Proof.
Since φ(α)  f (xk + αpk) is bounded below for all α > 0 and since 0 < c1 < 1,
the line l(α)  f (xk) + αc1∇f T
k pk must intersect the graph of φ at least once. Let α′ > 0
be the smallest intersecting value of α, that is,
f (xk + α′pk)  f (xk) + α′c1∇f T
k pk.
(3.8)
The sufﬁcient decrease condition (3.6a) clearly holds for all step lengths less than α′.
By the mean value theorem, there exists α′′ ∈(0, α′) such that
f (xk + α′pk) −f (xk)  α′∇f (xk + α′′pk)T pk.
(3.9)

3 . 1 .
S t e p L e n g t h
41
By combining (3.8) and (3.9), we obtain
∇f (xk + α′′pk)T pk  c1∇f T
k pk > c2∇f T
k pk,
(3.10)
since c1 < c2 and ∇f T
k pk < 0. Therefore, α′′ satisﬁes the Wolfe conditions (3.6), and the
inequalities hold strictly in both (3.6a) and (3.6b). Hence, by our smoothness assumption
on f , there is an interval around α′′ for which the Wolfe conditions hold. Moreover, since
the term in the left-hand side of (3.10) is negative, the strong Wolfe conditions (3.7) hold in
the same interval.
□
The Wolfe conditions are scale-invariant in a broad sense: Multiplying the objective
function by a constant or making an afﬁne change of variables does not alter them. They can
be used in most line search methods, and are particularly important in the implementation
of quasi-Newton methods, as we see in Chapter 8.
THE GOLDSTEIN CONDITIONS
Like the Wolfe conditions, the Goldstein conditions also ensure that the step length
α achieves sufﬁcient decrease while preventing α from being too small. The Goldstein
conditions can also be stated as a pair of inequalities, in the following way:
f (xk) + (1 −c)αk∇f T
k pk ≤f (xk + αkpk) ≤f (xk) + cαk∇f T
k pk,
(3.11)
with 0 < c < 1
2. The second inequality is the sufﬁcient decrease condition (3.4), whereas
the ﬁrst inequality is introduced to control the step length from below; see Figure 3.6
A disadvantage of the Goldstein conditions vis-`a-vis the Wolfe conditions is that the
ﬁrst inequality in (3.11) may exclude all minimizers of φ. However, the Goldstein and Wolfe
conditions have much in common, and their convergence theories are quite similar. The
Goldstein conditions are often used in Newton-type methods but are not well suited for
quasi-Newton methods that maintain a positive deﬁnite Hessian approximation.
SUFFICIENT DECREASE AND BACKTRACKING
We have mentioned that the sufﬁcient decrease condition (3.6a) alone is not sufﬁcient
to ensure that the algorithm makes reasonable progress along the given search direction.
However, if the line search algorithm chooses its candidate step lengths appropriately, by
using a so-called backtracking approach, we can dispense with the extra condition (3.6b) and
use just the sufﬁcient decrease condition to terminate the line search procedure. In its most
basic form, backtracking proceeds as follows.
Procedure 3.1 (Backtracking Line Search).
Choose ¯α > 0, ρ, c ∈(0, 1); set α ←¯α;
repeat until f (xk + αpk) ≤f (xk) + cα∇f T
k pk

42
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
fk
Tpk
c
α
Tpk
k
α (1_ c)
f
φ (
=f(x k+α p
α)
k )
acceptable steplengths
α
Figure 3.6
The Goldstein conditions.
α ←ρα;
end (repeat)
Terminate with αk  α.
In this procedure, the initial step length ¯α is chosen to be 1 in Newton and quasi-Newton
methods, but can have different values in other algorithms such as steepest descent or conju-
gate gradient. An acceptable step length will be found after a ﬁnite number of trials because
αk will eventually become small enough that the sufﬁcient decrease condition holds (see Fig-
ure 3.3). In practice, the contraction factor ρ is often allowed to vary at each iteration of the
line search. For example, it can be chosen by safeguarded interpolation, as we describe later.
We need ensure only that at each iteration we have ρ ∈[ρlo, ρhi], for some ﬁxed constants
0 < ρlo < ρhi < 1.
The backtracking approach ensures either that the selected step length αk is some
ﬁxed value (the initial choice ¯α), or else that it is short enough to satisfy the sufﬁcient
decrease condition but not too short. The latter claim holds because the accepted value αk
is within striking distance of the previous trial value, αk/ρ, which was rejected for violating
the sufﬁcient decrease condition, that is, for being too long.

3 . 2 .
C o n v e r g e n c e o f L i n e S e a r c h M e t h o d s
43
This simple and popular strategy for terminating a line search is well suited for Newton
methods (see Chapter 6) but is less appropriate for quasi-Newton and conjugate gradient
methods.
3.2
CONVERGENCE OF LINE SEARCH METHODS
To obtain global convergence, we must not only have well-chosen step lengths but also well-
chosen search directions pk. We discuss requirements on the search direction in this section,
focusing on one key property: the angle θk between pk and the steepest descent direction
−∇fk, deﬁned by
cos θk 
−∇f T
k pk
∥∇fk∥∥pk∥.
(3.12)
The following theorem, due to Zoutendijk, has far-reaching consequences. It shows,
for example, that the steepest descent method is globally convergent. For other algorithms
it describes how far pk can deviate from the steepest descent direction and still give rise to
a globally convergent iteration. Various line search termination conditions can be used to
establish this result, but for concreteness we will consider only the Wolfe conditions (3.6).
Though Zoutendijk’s result appears, at ﬁrst, to be technical and obscure, its power will soon
become evident.
Theorem 3.2.
Consider any iteration of the form (3.1), where pk is a descent direction and αk satisﬁes
the Wolfe conditions (3.6). Suppose that f is bounded below in IRn and that f is continuously
differentiable in an open set N containing the level set L
def {x : f (x) ≤f (x0)}, where x0 is
the starting point of the iteration. Assume also that the gradient ∇f is Lipschitz continuous on
N, that is, there exists a constant L > 0 such that
∥∇f (x) −∇f (˜x)∥≤L∥x −˜x∥,
for all x, ˜x ∈N.
(3.13)
Then

k≥0
cos2 θk ∥∇fk∥2 < ∞.
(3.14)
Proof.
From (3.6b) and (3.1) we have that
(∇fk+1 −∇fk)T pk ≥(c2 −1)∇f T
k pk,

44
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
while the Lipschitz condition (3.13) implies that
(∇fk+1 −∇fk)T pk ≤αkL∥pk∥2.
By combining these two relations, we obtain
αk ≥c2 −1
L
∇f T
k pk
∥pk∥2 .
By substituting this inequality into the ﬁrst Wolfe condition (3.6a), we obtain
fk+1 ≤fk −c1
1 −c2
L
(∇f T
k pk)2
∥pk∥2
.
From the deﬁnition (3.12), we can write this relation as
fk+1 ≤fk −c cos2 θk∥∇fk∥2,
where c  c1(1 −c2)/L. By summing this expression over all indices less than or equal to
k, we obtain
fk+1 ≤f0 −c
k

j0
cos2 θj∥∇fj∥2.
(3.15)
Since f is bounded below, we have that f0 −fk+1 is less than some positive constant, for all
k. Hence by taking limits in (3.15), we obtain
∞

k0
cos2 θk∥∇fk∥2 < ∞,
which concludes the proof.
□
Similar results to this theorem hold when the Goldstein conditions (3.11) or strong
Wolfe conditions (3.7) are used in place of the Wolfe conditions.
NotethattheassumptionsofTheorem3.2arenottoorestrictive.Ifthefunction f were
not bounded below, the optimization problem would not be well-deﬁned. The smoothness
assumption—Lipschitz continuity of the gradient—is implied by many of the smoothness
conditions that are used in local convergence theorems (see Chapters 6 and 8) and are often
satisﬁed in practice.
Inequality (3.14), which we call the Zoutendijk condition, implies that
cos2 θk∥∇fk∥2 →0.
(3.16)

3 . 2 .
C o n v e r g e n c e o f L i n e S e a r c h M e t h o d s
45
This limit can be used in turn to derive global convergence results for line search algorithms.
If our method for choosing the search direction pk in the iteration (3.1) ensures that
the angle θk deﬁned by (3.12) is bounded away from 90◦, there is a positive constant δ such
that
cos θk ≥δ > 0,
for all k.
(3.17)
It follows immediately from (3.16) that
lim
k→∞∥∇fk∥ 0.
(3.18)
In other words, we can be sure that the gradient norms∥∇fk∥converge to zero, provided that
the search directions are never too close to orthogonality with the gradient. In particular, the
method of steepest descent (for which the search direction pk makes an angle of zero degrees
with the negative gradient) produces a gradient sequence that converges to zero, provided
that it uses a line search satisfying the Wolfe or Goldstein conditions.
We use the term globally convergent to refer to algorithms for which the property
(3.18) is satisﬁed, but note that this term is sometimes used in other contexts to mean
different things. For line search methods of the general form (3.1), the limit (3.18) is the
strongest global convergence result that can be obtained: We cannot guarantee that the
method converges to a minimizer, but only that it is attracted by stationary points. Only
by making additional requirements on the search direction pk—by introducing negative
curvature information from the Hessian ∇2f (xk), for example—can we strengthen these
results to include convergence to a local minimum. See the Notes and References at the end
of this chapter for further discussion of this point.
Consider now the Newton-like method (3.1), (3.2) and assume that the matrices Bk
are positive deﬁnite with a uniformly bounded condition number. That is, there is a constant
M such that
∥Bk∥∥B−1
k ∥≤M,
for all k.
It is easy to show from the deﬁnition (3.12) that
cos θk ≥1/M
(3.19)
(see Exercise 5). By combining this bound with (3.16) we ﬁnd that
lim
k→∞∥∇fk∥ 0.
(3.20)
Therefore, we have shown that Newton and quasi-Newton methods are globally convergent
if the matrices Bk have a bounded condition number and are positive deﬁnite (which is

46
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
needed to ensure that pk is a descent direction), and if the step lengths satisfy the Wolfe
conditions.
For some algorithms, such as conjugate gradient methods, we will not be able to prove
the limit (3.18), but only the weaker result
lim inf
k→∞∥∇fk∥ 0.
(3.21)
In other words, just a subsequence of the gradient norms ∥∇fkj ∥converges to zero, rather
than the whole sequence (see Appendix A). This result, too, can be proved by using
Zoutendijk’s condition (3.14), but instead of a constructive proof, we outline a proof by
contradiction. Suppose that (3.21) does not hold, so that the gradients remain bounded
away from zero, that is, there exists γ > 0 such that
∥∇fk∥≥γ,
for all k sufﬁciently large.
(3.22)
Then from (3.16) we conclude that
cos θk →0,
(3.23)
that is, the entire sequence {cos θk} converges to 0. To establish (3.21), therefore, it is enough
to show that a subsequence {cos θkj } is bounded away from zero. We will use this strategy in
Chapter 5 to study the convergence of nonlinear conjugate gradient methods.
By applying this proof technique, we can prove global convergence in the sense of
(3.20) or (3.21) for a general class of algorithms. Consider any algorithm for which (i) every
iteration produces a decrease in the objective function, and (ii) every mth iteration is a
steepest descent step, with step length chosen to satisfy the Wolfe or Goldstein conditions.
Then since cos θk  1 for the steepest descent steps, the result (3.20) holds. Of course, we
would design the algorithm so that it does something “better” than steepest descent at the
other m −1 iterates; the occasional steepest descent steps may not make much progress, but
they at least guarantee overall global convergence.
NotethatthroughoutthissectionwehaveusedonlythefactthatZoutendijk’scondition
implies the limit (3.16). In later chapters we will make use of the bounded sum condition
(3.14), which forces the sequence {cos2 θk∥∇fk∥2} to converge to zero at a sufﬁciently rapid
rate.
3.3
RATE OF CONVERGENCE
It would seem that designing optimization algorithms with good convergence properties is
easy, since all we need to ensure is that the search direction pk does not tend to become
orthogonal to the gradient ∇fk, or that steepest descent steps are taken regularly. We could

3 . 3 .
R a t e o f C o n v e r g e n c e
47
simply compute cos θk at every iteration and turn pk toward the steepest descent direction
if cos θk is smaller than some preselected constant δ > 0. Angle tests of this type ensure
global convergence, but they are undesirable for two reasons. First, they may impede a
fast rate of convergence, because for problems with an ill-conditioned Hessian, it may be
necessary to produce search directions that are almost orthogonal to the gradient, and an
inappropriate choice of the parameter δ may prevent this. Second, angle tests destroy the
invariance properties of quasi-Newton methods.
Algorithmic strategies that achieve rapid convergence can sometimes conﬂict with
the requirements of global convergence, and vice versa. For example, the steepest descent
method is the quintessential globally convergent algorithm, but it is quite slow in practice,
as we shall see below. On the other hand, the pure Newton iteration converges rapidly when
started close enough to a solution, but its steps may not even be descent directions away
from the solution. The challenge is to design algorithms that incorporate both properties:
good global convergence guarantees and a rapid rate of convergence.
We begin our study of convergence rates of line search methods by considering the
most basic approach of all: the steepest descent method.
CONVERGENCE RATE OF STEEPEST DESCENT
We can learn much about the steepest descent method by considering the ideal case,
in which the objective function is quadratic and the line searches are exact. Let us suppose
that
f (x)  1
2xT Qx −bT x,
(3.24)
where Q is symmetric and positive deﬁnite. The gradient is given by ∇f (x)  Qx −b, and
the minimizer x∗is the unique solution of the linear system Qx  b.
Let us compute the step length αk that minimizes f (xk −α∇fk). By differentiating
f (xk −αgk)  1
2(xk −αgk)T Q(xk −αgk) −bT (xk −αgk)
with respect to α, we obtain
αk 
∇f T
k ∇fk
∇f T
k Q∇fk
.
(3.25)
If we use this exact minimizer αk, the steepest descent iteration for (3.24) is given by
xk+1  xk −
 ∇f T
k ∇fk
∇f T
k Q∇fk

∇fk.
(3.26)

48
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
Figure 3.7
Steepest descent steps.
Since ∇fk  Qxk −b, this equation yields a closed-form expression for xk+1 in terms of xk.
In Figure 3.7 we plot a typical sequence of iterates generated by the steepest descent method
on a two-dimensional quadratic objective function. The contours of f are ellipsoids whose
axes lie along the orthogonal eigenvectors of Q. Note that the iterates zigzag toward the
solution.
To quantify the rate of convergence we introduce the weighted norm ∥x∥2
Q  xT Qx.
By using the relation Qx∗ b, we can show easily that
1
2∥x −x∗∥2
Q  f (x) −f (x∗),
(3.27)
sothatthisnormmeasuresthedifferencebetweenthecurrentobjectivevalueandtheoptimal
value. By using the equality (3.26) and noting that ∇fk  Q(xk −x∗), we can derive the
equality
∥xk+1 −x∗∥2
Q 

1 −
	
∇f T
k ∇fk

2
	
∇f T
k Q∇fk

 	
∇f T
k Q−1∇fk


∥xk −x∗∥2
Q
(3.28)
(see Exercise 7). This expression describes the exact decrease in f at each iteration, but since
the term inside the brackets is difﬁcult to interpret, it is more useful to bound it in terms of
the condition number of the problem.
Theorem 3.3.
When the steepest descent method with exact line searches (3.26) is applied to the strongly
convex quadratic function (3.24), the error norm (3.27) satisﬁes
∥xk+1 −x∗∥2
Q ≤
λn −λ1
λn + λ1
2
∥xk −x∗∥2
Q,
(3.29)

3 . 3 .
R a t e o f C o n v e r g e n c e
49
where 0 < λ1 ≤· · · ≤λn are the eigenvalues of Q.
The proof of this result is given by Luenberger [152]. The inequalities (3.29) and (3.27)
show that the function values fk converge to the minimum f∗at a linear rate. As a special
case of this result, we see that convergence is achieved in one iteration if all the eigenvalues
are equal. In this case, Q is a multiple of the identity matrix, so the contours in Figure 3.7
are circles and the steepest descent direction always points at the solution. In general, as the
condition number κ(Q)  λn/λ1 increases, the contours of the quadratic become more
elongated, the zigzagging in Figure 3.7 becomes more pronounced, and (3.29) implies that
the convergence degrades. Even though (3.29) is a worst-case bound, it gives an accurate
indication of the behavior of the algorithm when n > 2.
The rate-of-convergence behavior of the steepest descent method is essentially the
same on general nonlinear objective functions. In the following result we assume that the
step length is the global minimizer along the search direction.
Theorem 3.4.
Suppose that f : IRn →IR is twice continuously differentiable, and that the iterates
generated by the steepest descent method with exact line searches converge to a point x∗where
the Hessian matrix ∇2f (x∗) is positive deﬁnite. Then
f (xk+1) −f (x∗) ≤
λn −λ1
λn + λ1
2
[f (xk) −f (x∗)],
where λ1 ≤· · · ≤λn are the eigenvalues of ∇2f (x∗).
In general, we cannot expect the rate of convergence to improve if an inexact line
search is used. Therefore, Theorem 3.4 shows that the steepest descent method can have an
unacceptablyslowrateofconvergence,evenwhentheHessianisreasonablywellconditioned.
For example, if κ(Q)  800, f (x1)  1 and f (x∗)  0, Theorem 3.4 suggests that the
function value will still be about 0.08 after one thousand iterations of the steepest descent
method.
QUASI-NEWTON METHODS
Let us now suppose that the search direction has the form
pk  −B−1
k ∇fk,
(3.30)
where the symmetric and positive deﬁnite matrix Bk is updated at every iteration by a quasi-
Newton updating formula. We already encountered one quasi-Newton formula, the BFGS
formula, in Chapter 2; others will be discussed in Chapter 8. We assume here that the step
length αk will be computed by an inexact line search that satisﬁes the Wolfe or strong Wolfe

50
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
conditions, with one important proviso: The line search algorithm will always try the step
length α  1 ﬁrst, and will accept this value if it satisﬁes the Wolfe conditions. (We could
enforce this condition by setting ¯α  1 in Procedure 3.1, for example.) This implementation
detail turns out to be crucial in obtaining a fast rate of convergence.
The following result, due to Dennis and Mor´e, shows that if the search direction of a
quasi-Newton method approximates the Newton direction well enough, then the unit step
length will satisfy the Wolfe conditions as the iterates converge to the solution. It also speciﬁes
a condition that the search direction must satisfy in order to give rise to a superlinearly
convergent iteration. To bring out the full generality of this result, we state it ﬁrst in terms
of a general descent iteration, and then examine its consequences for quasi-Newton and
Newton methods.
Theorem 3.5.
Suppose that f : IRn →IR is three times continuously differentiable. Consider the
iteration xk+1  xk +αkpk, where pk is a descent direction and αk satisﬁes the Wolfe conditions
(3.6) with c1 ≤1
2. If the sequence {xk} converges to a point x∗such that ∇f (x∗)  0 and
∇2f (x∗) is positive deﬁnite, and if the search direction satisﬁes
lim
k→∞
∥∇fk + ∇2fkpk∥
∥pk∥
 0,
(3.31)
then
(i) the step length αk  1 is admissible for all k greater than a certain index k0; and
(ii) if αk  1 for all k > k0, {xk} converges to x∗superlinearly.
It is easy to see that if c1 > 1
2, then the line search would exclude the minimizer of a
quadratic, and unit step lengths may not be admissible.
If pk is a quasi-Newton search direction of the form (3.30), then (3.31) is equivalent
to
lim
k→∞
∥(Bk −∇2f (x∗))pk∥
∥pk∥
 0.
(3.32)
Hence, we have the surprising (and delightful) result that a superlinear convergence rate can
be attained even if the sequence of quasi-Newton matrices Bk does not converge to ∇2f (x∗);
it sufﬁces that the Bk become increasingly accurate approximations to ∇2f (x∗) along the
search directions pk.
An important remark is that condition (3.32) is both necessary and sufﬁcient for the
superlinear convergence of quasi-Newton methods.

3 . 3 .
R a t e o f C o n v e r g e n c e
51
Theorem 3.6.
Suppose that f : IRn →IR is three times continuously differentiable. Consider the
iteration xk+1  xk + pk (that is, the step length αk is uniformly 1) and that pk is given by
(3.30). Let us also assume that {xk} converges to a point x∗such that ∇f (x∗)  0 and ∇2f (x∗)
is positive deﬁnite. Then {xk} converges superlinearly if and only if (3.32) holds.
Proof.
We ﬁrst show that (3.32) is equivalent to
pk −pN
k  o(∥pk∥),
(3.33)
where pN
k  −∇2f −1
k ∇fk is the Newton step. Assuming that (3.32) holds, we have that
pk −pN
k  ∇2f −1
k (∇2fkpk + ∇fk)
 ∇2f −1
k (∇2fk −Bk)pk
 O(∥(∇2fk −Bk)pk∥)
 o(∥pk∥),
where we have used the fact that ∥∇2f −1
k ∥is bounded above for xk sufﬁciently close to x∗,
since the limiting Hessian ∇2f (x∗) is positive deﬁnite. The converse follows readily if we
multiply both sides of (3.33) by ∇2fk and recall (3.30).
For the remainder of the proof we need to look ahead to the proof of quadratic con-
vergence of Newton’s method and, in particular, the estimate (3.37). By using this inequality
together with (3.33), we obtain that
∥xk + pk −x∗∥≤∥xk + pN
k −x∗∥+ ∥pk −pN
k∥
 O(∥xk −x∗∥2) + o(∥pk∥).
A simple manipulation of this inequality reveals that ∥pk∥ O(∥xk −x∗∥), so we obtain
∥xk + pk −x∗∥≤o(∥xk −x∗∥),
giving the superlinear convergence result.
□
We will see in Chapter 8 that quasi-Newton methods normally satisfy condition (3.32)
and are superlinearly convergent.
NEWTON’S METHOD
Let us now consider the Newton iteration where the search direction is given by
pN
k  −∇2f −1
k ∇fk.
(3.34)

52
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
Since the Hessian matrix ∇2fk may not always be positive deﬁnite, pN
k may not always be
a descent direction, and many of the ideas discussed so far in this chapter no longer apply.
In Chapter 6 we will describe two approaches for obtaining a globally convergent iteration
based on the Newton step: a line search approach, in which the Hessian ∇2fk is modiﬁed, if
necessary, to make it positive deﬁnite and thereby yield descent, and a trust region approach,
in which ∇2fk is used to form a quadratic model that is minimized in a ball.
Here we discuss just the local rate-of-convergence properties of Newton’s method. We
know that for all x in the vicinity of a solution point x∗such that ∇2f (x∗) is positive deﬁnite,
the Hessian ∇2f (x) will also be positive deﬁnite. Newton’s method will be well-deﬁned in
this region and will converge quadratically, provided that the step lengths αk are eventually
always 1.
Theorem 3.7.
Supposethatf istwicedifferentiableandthattheHessian∇2f (x)isLipschitzcontinuous
(see (A.8)) in a neighborhood of a solution x∗at which the sufﬁcient conditions (Theorem 2.4)
are satisﬁed. Consider the iteration xk+1  xk + pk, where pk is given by (3.34). Then
1. if the starting point x0 is sufﬁciently close to x∗, the sequence of iterates converges to x∗;
2. the rate of convergence of {xk} is quadratic; and
3. the sequence of gradient norms {∥∇fk∥} converges quadratically to zero.
Proof.
From the deﬁnition of the Newton step and the optimality condition ∇f∗ 0 we
have that
xk + pN
k −x∗ xk −x∗−∇2f −1
k ∇fk
 ∇2f −1
k

∇2fk(xk −x∗) −(∇fk −∇f∗)

.
(3.35)
Since
∇fk −∇f∗
 1
0
∇2f (xk + t(x∗−xk))(xk −x∗) dt,
we have
∇2f (xk)(xk −x∗) −(∇fk −∇f (x∗))



 1
0

∇2f (xk) −∇2f (xk + t(x∗−xk))

(xk −x∗) dt

≤
 1
0
∇2f (xk) −∇2f (xk + t(x∗−xk))
 ∥xk −x∗∥dt
≤∥xk −x∗∥2
 1
0
Lt dt  1
2L∥xk −x∗∥2,
(3.36)

3 . 3 .
R a t e o f C o n v e r g e n c e
53
where L is the Lipschitz constant for ∇2f (x) for x near x∗. Since ∇2f (x∗) is nonsingular,
and since ∇2fk →∇2f (x∗), we have that ∥∇2f −1
k ∥≤2∥∇2f (x∗)−1∥for all k sufﬁciently
large. By substituting in (3.35) and (3.36), we obtain
∥xk + pN
k −x∗∥≤L∥∇2f (x∗)−1∥∥xk −x∗∥2  ˜L∥xk −x∗∥2,
(3.37)
where ˜L  L∥∇2f (x∗)−1∥. Using this inequality inductively we deduce that if the starting
point is sufﬁciently near x∗, then the sequence converges to x∗, and the rate of convergence
is quadratic.
By using the relations xk+1 −xk  pN
k and ∇fk + ∇2fkpN
k  0, we obtain that
∥∇f (xk+1)∥ ∥∇f (xk+1) −∇fk −∇2f (xk)pN
k∥


 1
0
∇2f (xk + tpN
k)(xk+1 −xk) dt −∇2f (xk)pN
k

≤
 1
0
∇2f (xk + tpN
k) −∇2f (xk)
 ∥pN
k∥dt
≤1
2L∥pN
k∥2
≤1
2L∥∇2f (xk)−1∥2∥∇fk∥2
≤2L∥∇2f (x∗)−1∥2∥∇fk∥2,
proving that the gradient norms converge to zero quadratically.
□
WhenthesearchdirectionisgivenbyNewton’smethod,thelimit(3.31)issatisﬁed(the
ratio is zero for all k!), and Theorem 3.5 shows that the Wolfe conditions will accept the step
length αk for all large k. The same is true of the Goldstein conditions. Thus implementations
of Newton’s method using these conditions, and in which the line search always tries the unit
step length ﬁrst, will set αk  1 for all large k and attain a local quadratic rate of convergence.
COORDINATE DESCENT METHODS
An approach that is frequently used in practice is to cycle through the n coordinate
directions e1, e2, . . . , en, using each in turn as a search direction. At the ﬁrst iteration, we ﬁx
all except the ﬁrst variable, and ﬁnd a new value of this variable that minimizes (or at least
reduces) the objective function. On the next iteration, we repeat the process with the second
variable, and so on. After n iterations, we return to the ﬁrst variable and repeat the cycle. The
method is referred to as the method of alternating variables or the coordinate descent method.
Though simple and somewhat intuitive, it can be quite inefﬁcient in practice, as we illustrate
in Figure 3.8 for a quadratic function in two variables. Note that after a few iterations, neither
the vertical nor the horizontal move makes much progress toward the solution.

54
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
0
x
1
x
x*
Figure 3.8
Coordinate descent.
In fact, the coordinate descent method with exact line searches can iterate inﬁnitely
without ever approaching a point where the gradient of the objective function vanishes. (By
contrast, the steepest descent method produces a sequence for which ∥∇fk∥→0, as we
showed earlier.) This observation can be generalized to show that a cyclic search along any
set of linearly independent directions does not guarantee global convergence (Powell [198]).
The difﬁculty that arises is that the gradient ∇fk may become more and more perpendicular
to the coordinate search direction, so that cos θk approaches zero sufﬁciently rapidly that the
Zoutendijk condition (3.14) is satisﬁed even when ∇fk does not approach zero.
If the coordinate descent method converges to a solution, then its rate of convergence is
often much slower than that of the steepest descent method, and the difference between them
increases with the number of variables. However, the method may still be useful because
it does not require calculation of the gradient ∇fk, and
the speed of convergence can be quite acceptable if the variables are loosely coupled.
Many variants of the coordinate descent method have been proposed, some of which
are globally convergent. One simple variant is a “back-and-forth” approach in which we

3 . 4 .
S t e p - L e n g t h S e l e c t i o n A l g o r i t h m s
55
search along the sequence of directions
e1, e2, . . . , en−1, en, en−1, . . . , e2, e1, e2, . . .
(repeats).
Another approach, suggested by Figure 3.8, is ﬁrst to perform a sequence of coordinate
descent steps and then search along the line joining the ﬁrst and last points in the cycle.
Several algorithms, such as that of Hooke and Jeeves, are based on these ideas; see [104, 83].
3.4
STEP-LENGTH SELECTION ALGORITHMS
We now consider techniques for ﬁnding a minimum of the one-dimensional function
φ(α)  f (xk + αpk),
(3.38)
or for simply ﬁnding a step length αk satisfying one of the termination conditions described
in Section 3.1. We assume that pk is a descent direction—that is, φ′(0) < 0—so that our
search can be conﬁned to positive values of α.
If f is a convex quadratic, f (x)  1
2xT Qx +bT x +c, its one-dimensional minimizer
along the ray xk + αpk can be computed analytically and is given by
αk  −∇f T
k pk
pT
k Qpk
.
(3.39)
For general nonlinear functions, it is necessary to use an iterative procedure. Much attention
mustbegiventothislinesearchbecauseithasamajorimpactontherobustnessandefﬁciency
of all nonlinear optimization methods.
Linesearchprocedurescanbeclassiﬁedaccordingtothetypeofderivativeinformation
they use. Algorithms that use only function values can be inefﬁcient, since to be theoretically
sound, they need to continue iterating until the search for the minimizer is narrowed down
to a small interval. In contrast, knowledge of gradient information allows us to determine
whether a suitable step length has been located, as stipulated, for example, by the Wolfe
conditions (3.6) or Goldstein conditions (3.11). Often, particularly when the iterates are
close to the solution, the very ﬁrst step satisﬁes these conditions, so the line search need not
be invoked at all. In the rest of this section we will discuss only algorithms that make use
of derivative information. More information on derivative-free procedures is given in the
notes at the end of this chapter.
All line search procedures require an initial estimate α0 and generate a sequence {αi}
that either terminates with a step length satisfying the conditions speciﬁed by the user (for
example, the Wolfe conditions) or determines that such a step length does not exist. Typical
procedures consist of two phases: a bracketing phase that ﬁnds an interval [a, b] containing

56
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
acceptable step lengths, and a selection phase that zooms in to locate the ﬁnal step length.
The selection phase usually reduces the bracketing interval during its search for the desired
step length and interpolates some of the function and derivative information gathered on
earlier steps to guess the location of the minimizer. We will ﬁrst discuss how to perform this
interpolation.
In the following discussion we let αk and αk−1 denote the step lengths used at iterations
k and k −1 of the optimization algorithm, respectively. On the other hand, we denote the
trial step lengths generated during the line search by αi and αi−1 and also αj. We use α0 to
denote the initial guess.
INTERPOLATION
We begin by describing a line search procedure based on interpolation of known func-
tionandderivativevaluesofthefunctionφ.Thisprocedurecanbeviewedasanenhancement
of Procedure 3.1. The aim is to ﬁnd a value of α that satisﬁes the sufﬁcient decrease condition
(3.6a), without being “too small.” Accordingly, the procedures here generate a decreasing
sequence of values αi such that each value αi is not too much smaller than its predecessor
αi−1.
Note that we can write the sufﬁcient decrease condition in the notation of (3.38) as
φ(αk) ≤φ(0) + c1αkφ′(0),
(3.40)
and that since the constant c1 is usually chosen to be small in practice (c1  10−4, say), this
condition asks for little more than descent in f . We design the procedure to be “efﬁcient”
in the sense that it computes the derivative ∇f (x) as few times as possible.
Suppose that the initial guess α0 is given. If we have
φ(α0) ≤φ(0) + c1α0φ′(0),
this step length satisﬁes the condition, and we terminate the search. Otherwise, we know that
the interval [0, α0] contains acceptable step lengths (see Figure 3.3). We form a quadratic
approximation φq(α) to φ by interpolating the three pieces of information available—φ(0),
φ′(0), and φ(α0)—to obtain
φq(α) 
φ(α0) −φ(0) −α0φ′(0)
α2
0

α2 + φ′(0)α + φ(0).
(3.41)
(Notethatthisfunctionisconstructedsothatitsatisﬁestheinterpolationconditionsφq(0) 
φ(0),φ′
q(0)  φ′(0),andφq(α0)  φ(α0).)Thenewtrialvalueα1 isdeﬁnedastheminimizer
of this quadratic, that is, we obtain
α1  −
φ′(0)α2
0
2 [φ(α0) −φ(0) −φ′(0)α0].
(3.42)

3 . 4 .
S t e p - L e n g t h S e l e c t i o n A l g o r i t h m s
57
Ifthesufﬁcientdecreasecondition(3.40)issatisﬁedatα1,weterminatethesearch.Otherwise,
we construct a cubic function that interpolates the four pieces of information φ(0), φ′(0),
φ(α0), and φ(α1), obtaining
φc(α)  aα3 + bα2 + αφ′(0) + φ(0),
where

a
b


1
α2
0α2
1(α1 −α0)

α2
0
−α2
1
−α3
0
α3
1
 
φ(α1) −φ(0) −φ′(0)α1
φ(α0) −φ(0) −φ′(0)α0

.
By differentiating φc(x), we see that the minimizer α2 of φc lies in the interval [0, α1] and is
given by
α2  −b +

b2 −3aφ′(0)
3a
.
If necessary, this process is repeated, using a cubic interpolant of φ(0), φ′(0) and the two
most recent values of φ, until an α that satisﬁes (3.40) is located. If any αi is either too close to
its predecessor αi−1 or else too much smaller than αi−1, we reset αi  αi−1/2. This safeguard
procedure ensures that we make reasonable progress on each iteration and that the ﬁnal α
is not too small.
Thestrategyjustdescribedassumesthatderivativevaluesaresigniﬁcantlymoreexpen-
sive to compute than function values. It is often possible, however, to compute the directional
derivative simultaneously with the function, at little additional cost; see Chapter 7. Accord-
ingly, we can design an alternative strategy based on cubic interpolation of the values of φ
and φ′ at the two most recent values of α.
Cubic interpolation provides a good model for functions with signiﬁcant changes of
curvature. Suppose we have an interval [a, b] known to contain desirable step lengths, and
two previous step length estimates αi−1 and αi in this interval. We use a cubic function to
interpolate φ(αi−1), φ′(αi−1), φ(αi), and φ′(αi). (This cubic function always exists and is
unique; see, for example, Bulirsch and Stoer [29, p. 52].) The minimizer of this cubic in
[a, b] is either at one of the endpoints or else in the interior, in which case it is given by
αi+1  αi −(αi −αi−1)

φ′(αi) + d2 −d1
φ′(αi) −φ′(αi−1) + 2d2

,
(3.43)
with
d1  φ′(αi−1) + φ′(αi) −3φ(αi−1) −φ(αi)
αi−1 −αi
,
d2 

d2
1 −φ′(αi−1)φ′(αi)
1/2 .

58
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
The interpolation process can be repeated by discarding the data at one of the step
lengths αi−1 or αi and replacing it by φ(αi+1) and φ′(αi+1). The decision on which of αi−1
and αi should be kept and which discarded depends on the speciﬁc conditions used to
terminate the line search; we discuss this issue further below in the context of the Wolfe
conditions. Cubic interpolation is a powerful strategy, since it can produce a quadratic rate
of convergence of the iteration (3.43) to the minimizing value of α.
THE INITIAL STEP LENGTH
For Newton and quasi-Newton methods the step α0  1 should always be used as the
initial trial step length. This choice ensures that unit step lengths are taken whenever they
satisfy the termination conditions and allows the rapid rate-of-convergence properties of
these methods to take effect.
For methods that do not produce well-scaled search directions, such as the steepest
descentandconjugategradientmethods,itisimportanttousecurrentinformationaboutthe
problem and the algorithm to make the initial guess. A popular strategy is to assume that the
ﬁrst-orderchangeinthefunctionatiteratexk willbethesameasthatobtainedattheprevious
step. In other words, we choose the initial guess α0 so that α0∇f T
k pk  αk−1∇f T
k−1pk−1. We
therefore have
α0  αk−1
∇f T
k−1pk−1
∇f T
k pk
.
Another useful strategy is to interpolate a quadratic to the data f (xk−1), f (xk), and
φ′(0)  ∇f T
k pk and to deﬁne α0 to be its minimizer. This strategy yields
α0  2(fk −fk−1)
φ′(0)
.
(3.44)
It can be shown that if xk →x∗superlinearly, then the ratio in this expression converges to
1. If we adjust the choice (3.44) by setting
α0 ←min(1, 1.01α0),
we ﬁnd that the unit step length α0  1 will eventually always be tried and accepted, and the
superlinear convergence properties of Newton and quasi-Newton methods will be observed.
A LINE SEARCH ALGORITHM FOR THE WOLFE CONDITIONS
The Wolfe (or strong Wolfe) conditions are among the most widely applicable and
useful termination conditions. We now describe in some detail a one-dimensional search
procedure that is guaranteed to ﬁnd a step length satisfying the strong Wolfe conditions (3.7)

3 . 4 .
S t e p - L e n g t h S e l e c t i o n A l g o r i t h m s
59
for any parameters c1 and c2 satisfying 0 < c1 < c2 < 1. As before, we assume that p is a
descent direction and that f is bounded below along the direction p.
The algorithm has two stages. This ﬁrst stage begins with a trial estimate α1, and keeps
increasing it until it ﬁnds either an acceptable step length or an interval that brackets the
desired step lengths. In the latter case, the second stage is invoked by calling a function called
zoom (Algorithm 3.3 below), which successively decreases the size of the interval until an
acceptable step length is identiﬁed.
A formal speciﬁcation of the line search algorithm follows. We refer to (3.7a) as the
sufﬁcient decrease condition and to (3.7b) as the curvature condition. The parameter αmax
is a user-supplied bound on the maximum step length allowed. The line search algorithm
terminates with α∗set to a step length that satisﬁes the strong Wolfe conditions.
Algorithm 3.2 (Line Search Algorithm).
Set α0 ←0, choose α1 > 0 and αmax;
i ←1;
repeat
Evaluate φ(αi);
if φ(αi) > φ(0) + c1αiφ′(0) or [φ(αi) ≥φ(αi−1) and i > 1]
α∗←zoom(αi−1, αi) and stop;
Evaluate φ′(αi);
if |φ′(αi)| ≤−c2φ′(0)
set α∗←αi and stop;
if φ′(αi) ≥0
set α∗←zoom(αi, αi−1) and stop;
Choose αi+1 ∈(αi, αmax)
i ←i + 1;
end (repeat)
Notethatthesequenceoftrialsteplengths{αi}ismonotonicallyincreasing,butthattheorder
of the arguments supplied to the zoom function may vary. The procedure uses the knowledge
that the interval (αi−1, αi) contains step lengths satisfying the strong Wolfe conditions if one
of the following three conditions is satisﬁed:
(i) αi violates the sufﬁcient decrease condition;
(ii) φ(αi) ≥φ(αi−1);
(iii) φ′(αi) ≥0.
The last step of the algorithm performs extrapolation to ﬁnd the next trial value αi+1. To
implementthisstepwecanuseapproachesliketheinterpolationproceduresabove,orwecan
simply set αi+1 to some constant multiple of αi. Whichever strategy we use, it is important

60
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
that the successive steps increase quickly enough to reach the upper limit αmax in a ﬁnite
number of iterations.
We now specify the function zoom, which requires a little explanation. The order of
its input arguments is such that each call has the form zoom(αlo, αhi), where
(a) the interval bounded by αlo and αhi contains step lengths that satisfy the strong Wolfe
conditions;
(b) αlo is, among all step lengths generated so far and satisfying the sufﬁcient decrease
condition, the one giving the smallest function value; and
(c) αhi is chosen so that φ′(αlo)(αhi −αlo) < 0.
Each iteration of zoom generates an iterate αj between αlo and αhi, and then replaces one of
these endpoints by αj in such a way that the properties (a), (b), and (c) continue to hold.
Algorithm 3.3 (zoom).
repeat
Interpolate (using quadratic, cubic, or bisection) to ﬁnd
a trial step length αj between αlo and αhi;
Evaluate φ(αj);
if φ(αj) > φ(0) + c1αjφ′(0) or φ(αj) ≥φ(αlo)
αhi ←αj;
else
Evaluate φ′(αj);
if |φ′(αj)| ≤−c2φ′(0)
Set α∗←αj and stop;
if φ′(αj)(αhi −αlo) ≥0
αhi ←αlo;
αlo ←αj;
end (repeat)
If the new estimate αj happens to satisfy the strong Wolfe conditions, then zoom has served
its purpose of identifying such a point, so it terminates with α∗ αj. Otherwise, if αj
satisﬁes the sufﬁcient decrease condition and has a lower function value than xlo, then we
set αlo ←αj to maintain condition (b). If this results in a violation of condition (c), we
remedy the situation by setting αhi to the old value of αlo. The reader should sketch some
graphs to illustrate the workings of zoom!
As mentioned earlier, the interpolation step that determines αj should be safeguarded
to ensure that the new step length is not too close to the endpoints of the interval. Practical
line search algorithms also make use of the properties of the interpolating polynomials to
make educated guesses of where the next step length should lie; see [27, 172]. A problem
that can arise in the implementation is that as the optimization algorithm approaches the

3 . 4 .
S t e p - L e n g t h S e l e c t i o n A l g o r i t h m s
61
solution, two consecutive function values f (xk) and f (xk−1) may be indistinguishable in
ﬁnite-precision arithmetic. Therefore, the line search must include a stopping test if it cannot
attain a lower function value after a certain number (typically, ten) of trial step lengths.
Some procedures also stop if the relative change in x is close to machine accuracy, or to some
user-speciﬁed threshold.
A line search algorithm that incorporates all these features is difﬁcult to code. We
advocate the use of one of the several good software implementations available in the public
domain. See Dennis and Schnabel [69], Lemar´echal [149], Fletcher [83], and in particular
Mor´e and Thuente [172].
One may ask how much more expensive it is to require the strong Wolfe conditions
instead of the regular Wolfe conditions. Our experience suggests that for a “loose” line search
(with parameters such as c1  10−4 and c2  0.9), both strategies require a similar amount
ofwork.ThestrongWolfeconditionshavetheadvantagethatbydecreasingc2 wecandirectly
control the quality of the search by forcing the accepted value of α to lie closer to a local
minimum. This feature is important in steepest descent or nonlinear conjugate gradient
methods, and therefore a step selection routine that enforces the strong Wolfe conditions is
of wider applicability.
NOTES AND REFERENCES
For an extensive discussion of line search termination conditions see Ortega and
Rheinboldt [185]. Akaike [2] presents a probabilistic analysis of the steepest descent method
with exact line searches on quadratic functions. He shows that when n > 2, the worst-case
bound (3.29) can be expected to hold for most starting points. The case where n  2 can be
studied in closed form; see Bazaraa, Sherali, and Shetty [7].
Somelinesearchmethods(seeGoldfarb[113]andMor´eandSorensen[169])compute
a direction of negative curvature, whenever it exists, to prevent the iteration from converging
to nonminimizing stationary points. A direction of negative curvature p−is one that satisﬁes
pT
−∇2f (xk)p−< 0. These algorithms generate a search direction by combining p−with the
steepest descent direction −∇f , and often perform a curvilinear backtracking line search.
It is difﬁcult to determine the relative contributions of the steepest descent and negative
curvature directions, and due to this, this approach fell out of favor after the introduction
of trust-region methods.
For a discussion on the rate of convergence of the coordinate descent method and for
more references about this method see Luenberger [152].
Derivative-free line search algorithms include golden section and Fibonacci search.
They share some of the features with the line search method given in this chapter. They
typically store three trial points that determine an interval containing a one-dimensional
minimizer. Golden section and Fibonacci differ in the way in which the trial step lengths are
generated; see, for example, [58, 27].
Our discussion of interpolation follows Dennis and Schnabel [69], and the algorithm
for ﬁnding a step length satisfying the strong Wolfe conditions can be found in Fletcher [83].

62
C h a p t e r
3 .
L i n e S e a r c h M e t h o d s
✐
E x e r c i s e s
✐
3.1 Program the steepest descent and Newton algorithms using the backtracking line
search, Procedure 3.1. Use them to minimize the Rosenbrock function (2.23). Set the initial
step length α0  1 and print the step length used by each method at each iteration. First try
the initial point x0  (1.2, 1.2) and then the more difﬁcult point x0  (−1.2, 1).
✐
3.2 Show that if 0 < c2 < c1 < 1, then there may be no step lengths that satisfy the
Wolfe conditions.
✐
3.3 Showthattheone-dimensionalminimizerofastronglyconvexquadraticfunction
is given by (3.39).
✐
3.4 Show that if c ≤1
2, then the one-dimensional minimizer of a strongly convex
quadratic function always satisﬁes the Goldstein conditions (3.11).
✐
3.5 Provethat∥Bx∥≥∥x∥/∥B−1∥foranynonsingularmatrixB.Usethistoestablish
(3.19).
✐
3.6 Consider the steepest descent method with exact line searches applied to the
convex quadratic function (3.24). Using the properties given in this chapter, show that if the
initial point is such that x0 −x∗is parallel to an eigenvector of Q, then the steepest descent
method will ﬁnd the solution in one step.
✐
3.7 Prove the result (3.28) by working through the following steps. First, use (3.26)
to show that
∥xk −x∗∥2
Q −∥xk+1 −x∗∥2
Q  2αk∇f T
k Q(xk −x∗) −α2
k∇f T
k Q∇fk,
where ∥· ∥Q is deﬁned by (3.27). Second, use the fact that ∇fk  Q(xk −x∗) to obtain
∥xk −x∗∥2
Q −∥xk+1 −x∗∥2
Q  2(gT
k gk)2
(gT
k Qgk) −(gT
k gk)2
(gT
k Qgk)
and
∥xk −x∗∥2
Q  ∇f T
k Q−1∇fk.
✐
3.8 Let Q be a positive deﬁnite symmetric matrix. Prove that for any vector x,
(xT x)2
(xT Qx)(xT Q−1x) ≥
4λnλ1
(λn + λ1)2 ,
where λn and λ1 are, respectively, the largest and smallest eigenvalues of Q. (This relation,
which is known as the Kantorovich inequality, can be used to deduce (3.29) from (3.28).

3 . 4 .
S t e p - L e n g t h S e l e c t i o n A l g o r i t h m s
63
✐
3.9 Program the BFGS algorithm using the line search algorithm described in this
chapter that implements the strong Wolfe conditions. Have the code verify that yT
k sk is
always positive. Use it to minimize the Rosenbrock function using the starting points given
in Exercise 1.
✐
3.10 Show that the quadratic function that interpolates φ(0), φ′(0), and φ(α0) is
given by (3.41). Then, make use of the fact that the sufﬁcient decrease condition (3.6a) is
not satisﬁed at α0 to show that this quadratic has positive curvature and that the minimizer
satisﬁes
α1 <
1
2(1 −c1).
Since c1 is chosen to be quite small in practice, this indicates that α1 cannot be much greater
than 1
2 (and may be smaller), which gives us an idea of the new step length.
✐
3.11 If φ(α0) is large, (3.42) shows that α1 can be quite small. Give an example of a
function and a step length α0 for which this situation arises. (Drastic changes to the estimate
of the step length are not desirable, since they indicate that the current interpolant does not
provide a good approximation to the function and that it should be modiﬁed before being
trusted to produce a good step length estimate. In practice, one imposes a lower bound—
typically, ρ  0.1—and deﬁnes the new step length as αi  max(ραi−1, ˆαi), where ˆαi is the
minimizer of the interpolant.)
✐
3.12 Suppose that the sufﬁcient decrease condition (3.6a) is not satisﬁed at the step
lengths α0, and α1, and consider the cubic interpolating φ(0), φ′(0), φ(α0) and φ(α1). By
drawing graphs illustrating the two situations that can arise, show that the minimizer of the
cubic lies in [0, α1]. Then show that if φ(0) < φ(α1), the minimizer is less than 2
3α1.

Chapter4

Trust-Region
Methods
Line search methods and trust-region methods both generate steps with the help of a
quadratic model of the objective function, but they use this model in different ways. Line
search methods use it to generate a search direction, and then focus their efforts on ﬁnding
a suitable step length α along this direction. Trust-region methods deﬁne a region around
the current iterate within which they trust the model to be an adequate representation of the
objective function, and then choose the step to be the approximate minimizer of the model
in this trust region. In effect, they choose the direction and length of the step simultaneously.
If a step is not acceptable, they reduce the size of the region and ﬁnd a new minimizer. In
general, the step direction changes whenever the size of the trust region is altered.
The size of the trust region is critical to the effectiveness of each step. If the region is
too small, the algorithm misses an opportunity to take a substantial step that will move it
much closer to the minimizer of the objective function. If too large, the minimizer of the
model may be far from the minimizer of the objective function in the region, so we may have
to reduce the size of the region and try again. In practical algorithms, we choose the size of
the region according to the performance of the algorithm during previous iterations. If the
model is generally reliable, producing good steps and accurately predicting the behavior of
the objective function along these steps, the size of the trust region is steadily increased to

66
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
contours of f
mk
contours of 
Trust region step
Trust region
Line search direction
Figure 4.1
Trust-region and line search steps.
allow longer, more ambitious, steps to be taken. On the other hand, a failed step indicates
that our model is an inadequate representation of the objective function over the current
trust region, so we reduce the size of the region and try again.
Figure 4.1 illustrates the trust-region approach on a function f of two variables in
which the current point lies at one end of a curved valley while the minimizer x∗lies at the
other end. The quadratic model function mk, whose elliptical contours are shown as dashed
lines, is based on function and derivative information at xk and possibly also on information
accumulated from previous iterations and steps. A line search method based on this model
searches along the step to the minimizer of mk (shown), but this direction allows only a
small reduction in f even if an optimal step is taken. A trust-region method, on the other
hand, steps to the minimizer of mk within the dotted circle, which yields a more signiﬁcant
reduction in f and a better step.
We will assume that the ﬁrst two terms of the quadratic model functions mk at each
iterate xk are identical to the ﬁrst two terms of the Taylor-series expansion of f around xk.
Speciﬁcally, we have
mk(p)  fk + ∇f T
k p + 1
2pT Bkp,
(4.1)

C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
67
where fk  f (xk), ∇fk  ∇f (xk), and Bk is some symmetric matrix. Since by (2.6) we
have
f (xk + p)  fk + ∇f T
k p + 1
2pT ∇2f (xk + tp)p,
(4.2)
for some scalar t ∈(0, 1), and since mk(p)  fk + ∇f T
k p + O
	
∥p∥2
, the difference
between mk(p) and f (xk + p) is O
	
∥p∥2
, so the approximation error is small when p is
small.
When Bk is equal to the true Hessian ∇2f (xk), the model function actually agrees
with the Taylor series to three terms. The approximation error is O
	
∥p∥3
in this case,
so this model is especially accurate when ∥p∥is small. The algorithm based on setting
Bk  ∇2f (xk) is called the trust-region Newton method, and will be discussed further in
Chapter 6. In the current chapter, we emphasize the generality of the trust-region approach
by assuming little about Bk except symmetry and uniform boundedness in the index k.
To obtain each step, we seek a solution of the subproblem
min
p∈IRn mk(p)  fk + ∇f T
k p + 1
2pT Bkp
s.t. ∥p∥≤k,
(4.3)
where k > 0 is the trust-region radius. For the moment, we deﬁne ∥·∥to be the Euclidean
norm, so that the solution p∗
k of (4.3) is the minimizer of mk in the ball of radius k. Thus,
the trust-region approach requires us to solve a sequence of subproblems (4.3) in which the
objective function and constraint (which can be written as pT p ≤2
k) are both quadratic.
When Bk is positive deﬁnite and ∥B−1
k ∇fk∥≤k, the solution of (4.3) is easy to identify—it
is simply the unconstrained minimum pB
k  −B−1
k ∇fk of the quadratic mk(p). In this case,
we call pB
k the full step. The solution of (4.3) is not so obvious in other cases, but it can usually
be found without too much expense. In any case, we need only an approximate solution to
obtain convergence and good practical behavior.
OUTLINE OF THE ALGORITHM
The ﬁrst issue to arise in deﬁning a trust-region method is the strategy for choosing
the trust-region radius k at each iteration. We base this choice on the agreement between
the model function mk and the objective function f at previous iterations. Given a step pk
we deﬁne the ratio
ρk  f (xk) −f (xk + pk)
mk(0) −mk(pk)
;
(4.4)
the numerator is called the actual reduction, and the denominator is the predicted reduction.
Note that since the step pk is obtained by minimizing the model mk over a region that
includes the step p  0, the predicted reduction will always be nonnegative. Thus if ρk is

68
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
negative, the new objective value f (xk + pk) is greater than the current value f (xk), so the
step must be rejected.
On the other hand, if ρk is close to 1, there is good agreement between the model mk
and the function f over this step, so it is safe to expand the trust region for the next iteration.
If ρk is positive but not close to 1, we do not alter the trust region, but if it is close to zero or
negative, we shrink the trust region. The following algorithm describes the process.
Algorithm 4.1 (Trust Region).
Given ¯ > 0, 0 ∈(0, ¯), and η ∈

0, 1
4

:
for k  0, 1, 2, . . .
Obtain pk by (approximately) solving (4.3);
Evaluate ρk from (4.4);
if ρk < 1
4
k+1  1
4∥pk∥
else
if ρk > 3
4 and ∥pk∥ k
k+1  min(2k, ¯)
else
k+1  k;
if ρk > η
xk+1  xk + pk
else
xk+1  xk;
end (for).
Here ¯ is an overall bound on the step lengths. Note that the radius is increased only if ∥pk∥
actually reaches the boundary of the trust region. If the step stays strictly inside the region,
we infer that the current value of k is not interfering with the progress of the algorithm,
so we leave its value unchanged for the next iteration.
To turn Algorithm 4.1 into a practical algorithm, we need to focus on solving (4.3).
We ﬁrst describe three strategies for ﬁnding approximate solutions, which achieve at least as
much reduction in mk as the reduction achieved by the so-called Cauchy point. This point is
simply the minimizer of mk along the steepest descent direction −∇fk, subject to the trust-
region bound. The ﬁrst approximate strategy is the dogleg method, which is appropriate when
the model Hessian Bk is positive deﬁnite. The second strategy, known as two-dimensional
subspace minimization, can be applied when Bk is indeﬁnite, though it requires an estimate
of the most negative eigenvalue of this matrix. The third strategy, due to Steihaug, is most
appropriate when Bk is the exact Hessian ∇2f (xk) and when this matrix is large and sparse.
We also describe a strategy due to Mor´e and Sorensen that ﬁnds a “nearly exact”
solution of (4.3). This strategy is based on the fact that the solution p satisﬁes (Bk +λI)p 
−∇fk for some positive value of λ > 0. This strategy seeks the value of λ that corresponds to

4 . 1 .
T h e C a u c h y P o i n t a n d R e l a t e d A l g o r i t h m s
69
the trust-region radius k and performs additional calculations in the special case in which
the resulting modiﬁed Hessian (Bk + λI) is nonsingular. Details are given below.
4.1
THE CAUCHY POINT AND RELATED ALGORITHMS
THE CAUCHY POINT
As we saw in the previous chapter, line search methods do not require optimal step
lengths to be globally convergent. In fact, only a crude approximation to the optimal step
length that satisﬁes certain loose criteria is needed. A similar situation applies in trust-region
methods. Although in principle we are seeking the optimal solution of the subproblem (4.3),
it is enough for global convergence purposes to ﬁnd an approximate solution pk that lies
within the trust region and gives a sufﬁcient reduction in the model. The sufﬁcient reduction
can be quantiﬁed in terms of the Cauchy point, which we denote by pC
k and deﬁne in terms
of the following simple procedure:
Algorithm 4.2 (Cauchy Point Calculation).
Find the vector pS
k that solves a linear version of (4.3), that is,
pS
k  arg min
p∈IRn fk + ∇f T
k p
s.t. ∥p∥≤k;
(4.5)
Calculate the scalar τk > 0 that minimizes mk(τpS
k) subject to
satisfying the trust-region bound, that is,
τk  arg min
τ>0 mk(τpS
k)
s.t. ∥τpS
k∥≤k;
(4.6)
Set pC
k  τkpS
k.
In fact, it is easy to write down a closed-form deﬁnition of the Cauchy point. The solution
of (4.5) is simply
pS
k  −
k
∥∇fk∥∇fk.
To obtain τk explicitly, we consider the cases of ∇f T
k Bk∇fk ≤0 and ∇f T
k Bk∇fk > 0 sepa-
rately. For the former case, the function mk(τpS
k) decreases monotonically with τ whenever
∇fk ̸ 0, so τk is simply the largest value that satisﬁes the trust-region bound, namely,
τk  1. For the case ∇f T
k Bk∇fk > 0, mk(τpS
k) is a convex quadratic in τ, so τk is either

70
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
pk
C
mk
contours of 
gk
-
Trust region
Figure 4.2
The Cauchy point.
the unconstrained minimizer of this quadratic, ∥∇fk∥3/(k∇f T
k Bk∇fk), or the boundary
value 1, whichever comes ﬁrst. In summary, we have
pC
k  −τk
k
∥∇fk∥∇fk,
(4.7)
where
τk 

1
if ∇f T
k Bk∇fk ≤0;
min
	
∥∇fk∥3/(k∇f T
k Bk∇fk), 1

otherwise.
(4.8)
Figure4.2illustratestheCauchypointforasubprobleminwhichBk ispositivedeﬁnite.
In this example, pC
k lies strictly inside the trust region.
The Cauchy step pC
k is inexpensive to calculate—no matrix factorizations are
required—and is of crucial importance in deciding if an approximate solution of the
trust-region subproblem is acceptable. Speciﬁcally, a trust-region method will be globally
convergent if its steps pk attain a sufﬁcient reduction in mk; that is, they give a reduction in
the model mk that is at least some ﬁxed multiple of the decrease attained by the Cauchy step
at each iteration.
IMPROVING ON THE CAUCHY POINT
Since the Cauchy point pC
k provides sufﬁcient reduction in the model function mk to
yield global convergence, and since the cost of calculating it is so small, why should we look

4 . 1 .
T h e C a u c h y P o i n t a n d R e l a t e d A l g o r i t h m s
71
any further for a better approximate solution of (4.3)? The reason is that by always taking
the Cauchy point as our step, we are simply implementing the steepest descent method with
a particular choice of step length. As we have seen in Chapter 3, steepest descent performs
poorly even if an optimal step length is used at each iteration.
The Cauchy point does not depend very strongly on the matrix Bk, which is used only
in the calculation of the step length. Rapid convergence (superlinear, for instance) can be
expected only if Bk plays a role in determining the direction of the step as well as its length.
A number of algorithms for generating approximate solutions pk to the trust-region
problem (4.3) start by computing the Cauchy point and then try to improve on it. The
improvement strategy is often designed so that the full step pB
k  −B−1
k ∇fk is chosen
whenever Bk is positive deﬁnite and ∥pB
k∥≤k. When Bk is the exact Hessian ∇2f (xk) or a
quasi-Newtonapproximation,thisstrategycanbeexpectedtoyieldsuperlinearconvergence.
We now consider three methods for ﬁnding approximate solutions to (4.3) that have
the features just described. Throughout this section we will be focusing on the internal
workings of a single iteration, so we drop the subscript “k” from the quantities k, pk, and
mk to simplify the notation. With this simpliﬁcation, we restate the trust-region subproblem
(4.3) as follows:
min
p∈IRn m(p)
def f + gT p + 1
2pT Bp
s.t. ∥p∥≤.
(4.9)
We denote the solution of (4.9) by p∗(), to emphasize the dependence on .
THE DOGLEG METHOD
We start by examining the effect of the trust-region radius  on the solution p∗()
of the subproblem (4.9). When B is positive deﬁnite, we have already noted that the uncon-
strained minimizer of m is the full step pB  −B−1g. When this point is feasible for (4.9),
it is obviously a solution, so we have
p∗()  pB,
when  ≥∥pB∥.
(4.10)
When  is tiny, the restriction ∥p∥≤ ensures that the quadratic term in m has little effect
on the solution of (4.9). The true solution p() is approximately the same as the solution
we would obtain by minimizing the linear function f + gT p over ∥p∥≤, that is,
p∗() ≈− g
∥g∥,
when  is small.
(4.11)
For intermediate values of , the solution p∗() typically follows a curved trajectory like
the one in Figure 4.3.
The dogleg method ﬁnds an approximate solution by replacing the curved trajectory
for p∗() with a path consisting of two line segments. The ﬁrst line segment runs from the

72
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
pU (
g )
unconstrained min along
g
-
full step
(
)
( )
∆
−
Trust region
Dogleg path
Optimal trajectory
pB
p
Figure 4.3
Exact trajectory and dogleg approximation.
origin to the unconstrained minimizer along the steepest descent direction deﬁned by
pU  −gT g
gT Bg g,
(4.12)
while the second line segment runs from pU to pB (see Figure 4.3). Formally, we denote this
trajectory by ˜p(τ) for τ ∈[0, 2], where
˜p(τ) 

τp
U,
0 ≤τ ≤1,
p
U + (τ −1)(p
B −p
U),
1 ≤τ ≤2.
(4.13)
The dogleg method chooses p to minimize the model m along this path, subject to
the trust-region bound. In fact, it is not even necessary to carry out a search, because the
dogleg path intersects the trust-region boundary at most once and the intersection point
can be computed analytically. We prove these claims in the following lemma.
Lemma 4.1.
Let B be positive deﬁnite. Then
(i) ∥˜p(τ)∥is an increasing function of τ, and
(ii) m( ˜p(τ)) is a decreasing function of τ.

4 . 1 .
T h e C a u c h y P o i n t a n d R e l a t e d A l g o r i t h m s
73
Proof.
It is easy to show that (i) and (ii) both hold forτ ∈[0, 1],sowerestrictourattention
to the case of τ ∈[1, 2].
For (i), deﬁne h(α) by
h(α)  1
2∥˜p(1 + α)∥2
 1
2∥pU + α(pB −pU)∥2
 1
2∥pU∥2 + αpUT (pB −pU) + 1
2α2∥pB −pU∥2.
Our result is proved if we can show that h′(α) ≥0 for α ∈(0, 1). Now,
h′(α)  −pUT (pU −pB) + α∥pU −pB∥2
≥−pUT (pU −pB)
 gT g
gT Bg gT

−gT g
gT Bg g + B−1g

 gT g gB−1g
gT Bg

1 −
(gT g)2
(gT Bg)(gT B−1g)

≥0,
where the ﬁnal inequality follows from Exercise 3.
For (ii), we deﬁne ˆh(α)  m( ˜p(1 + α)) and show that ˆh′(α) ≤0 for α ∈(0, 1).
Substitution of (4.13) into (4.9) and differentiation with respect to the argument leads to
ˆh′(α)  (pB −pU)T (g + BpU) + α(pB −pU)T B(pB −pU)
≤(pB −pU)T (g + BpU + B(pB −pU))
 (pB −pU)T (g + BpB)  0,
giving the result.
□
It follows from this lemma that the path ˜p(τ) intersects the trust-region boundary
∥p∥  at exactly one point if ∥pB∥≥, and nowhere otherwise. Since m is decreasing
along the path, the chosen value of p will be at pB if ∥pB∥≤, otherwise at the point of
intersection of the dogleg and the trust-region boundary. In the latter case, we compute the
appropriate value of τ by solving the following scalar quadratic equation:
∥pU + (τ −1)(pB −pU)∥2  2.
The dogleg strategy can be adapted to handle indeﬁnite B, but there is not much
point in doing so because the full step pB is not the unconstrained minimizer of m in this
case. Instead, we now describe another strategy, which aims to include directions of negative
curvature (that is, directions d for which dT Bd < 0) in the space of candidate trust-region
steps.

74
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
TWO-DIMENSIONAL SUBSPACE MINIMIZATION
When B is positive deﬁnite, the dogleg method strategy can be made slightly more
sophisticated by widening the search for p to the entire two-dimensional subspace spanned
by pU and pB (equivalently, g and −B−1g). The subproblem (4.9) is replaced by
min
p m(p)  f + gT p + 1
2pT Bp
s.t. ∥p∥≤, p ∈span[g, B−1g].
(4.14)
This is a problem in two variables that can be solved without much effort (see the exercises).
Clearly, the Cauchy point pC is feasible for (4.14), so the optimal solution of this subproblem
yields at least as much reduction in m as the Cauchy point, resulting in global convergence
of the algorithm. The two-dimensional subspace minimization strategy is obviously an
extension of the dogleg method as well, since the entire dogleg path lies in span[g, B−1g].
An advantage of this strategy is that it can be modiﬁed to handle the case of indeﬁnite
B in a way that is intuitive, practical, and theoretically sound. We mention just the salient
points of the handling of the indeﬁniteness here, and refer the reader to papers by Byrd,
Schnabel, and Schultz (see [39] and [226]) for details. When B has negative eigenvalues, the
two-dimensional subspace in (4.14) is changed to
span[g, (B + αI)−1g],
for some α ∈(−λ1, −2λ1],
(4.15)
where λ1 denotes the most negative eigenvalue of B. (This choice of α ensures that B +αI is
positive deﬁnite, and the ﬂexibility in this deﬁnition allows us to use a numerical procedure
suchastheLanczosmethodtocomputeanacceptablevalueofα.)When∥(B+αI)−1g∥≤,
we discard the subspace search of (4.14), (4.15) and instead deﬁne the step to be
p  −(B + αI)−1g + v,
(4.16)
where v is a vector that satisﬁes vT (B + αI)−1g ≤0. (This condition ensures that v does
not move p back toward zero, but instead continues to move roughly in the direction of
−(B + αI)−1g).
When B has zero eigenvalues but no negative eigenvalues, the Cauchy step p  pC is
used as the approximate solution of (4.9).
The reduction in model function m achieved by the two-dimensional minimization
strategy often is close to the reduction achieved by the exact solution of (4.9). Most of the
computational effort lies in a single factorization of B or B + αI (estimation of α and
solution of (4.14) are less signiﬁcant), while strategies that ﬁnd nearly exact solutions of
(4.9) typically require two or three such factorizations.

4 . 1 .
T h e C a u c h y P o i n t a n d R e l a t e d A l g o r i t h m s
75
STEIHAUG’S APPROACH
Both methods described above require the solution of a single linear system involving
B or (B + αI). When B is large, this operation may be quite costly, so we are motivated to
consider other techniques for ﬁnding an approximate solution of (4.9) that do not require
exact solution of a linear system but still produce an improvement on the Cauchy point.
Steihaug [231] proposed a technique with these properties. Steihaug’s implementation is
based on the conjugate gradient algorithm, an iterative algorithm for solving linear systems
withsymmetricpositivedeﬁnitecoefﬁcientmatrices.Theconjugategradient(CG)algorithm
is the subject of Chapter 5, and the interested reader should look ahead to that chapter for
further details. Our comments in this section focus on the differences between standard CG
and Steihaug’s approach, which are essentially that the algorithm terminates when it either
exits the trust region ∥p∥≤ or when it encounters a direction of negative curvature in B.
Steihaug’s approach can be stated formally as follows:
Algorithm 4.3 (CG–Steihaug).
Given ϵ > 0;
Set p0  0, r0  g, d0  −r0;
if ∥r0∥< ϵ
return p  p0;
for j  0, 1, 2, . . .
if dT
j Bdj ≤0
Find τ such that p  pj + τdj minimizes m(p) in (4.9)
and satisﬁes ∥p∥ ;
return p;
Set αj  rT
j rj/dT
j Bdj;
Set pj+1  pj + αjdj;
if ∥pj+1∥≥
Find τ ≥0 such that p  pj + τdj satisﬁes ∥p∥ ;
return p;
Set rj+1  rj + αjBdj;
if ∥rj+1∥< ϵ∥r0∥
return p  pj+1;
Set βj+1  rT
j+1rj+1/rT
j rj;
Set dj+1  rj+1 + βj+1dj;
end (for).
To connect this algorithm with Algorithm CG of Chapter 5, we note that m(·) takes
the place of φ(·), p takes the place of x, B takes the place of A, and −g takes the place of b.
The change of sign in the substitution b →−g propagates through the algorithm.
Algorithm4.3differsfromstandardCGinthattwoextrastoppingcriteriaarepresent—
the ﬁrst two if statements inside the for loop. The ﬁrst if statement stops the method if its

76
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
current search direction dj is a direction of zero curvature or negative curvature along B.
The second one causes termination if pj+1 violates the trust-region bound. In both cases,
a ﬁnal point p is found by intersecting the current search direction with the trust-region
boundary.
The initialization of p0 to zero is a crucial feature of the algorithm. After the ﬁrst
iteration (assuming ∥r0∥2 ≥ϵ), we have
p1  α0d0 
rT
0 r0
dT
0 Bd0
d0  −gT g
gT Bg g,
which is exactly the Cauchy point! Since each iteration of the conjugate gradient method
reduces m(·), this algorithm fulﬁlls the necessary condition for global convergence.
Another crucial property of the method is that each iterate pj is larger in norm than
its predecessor. This property is another consequence of the initialization p0  0. Its main
implication is that it is acceptable to stop iterating as soon as the trust-region boundary is
reached, because no further iterates giving a lower value of φ will be inside the trust region.
We state and prove this property formally in the following theorem. (The proof makes use
of the expanding subspace property of the CG algorithm, which we do not describe until
Chapter 5, so it can be skipped on the ﬁrst pass.)
Theorem 4.2.
The sequence of vectors generated by Algorithm 4.3 satisﬁes
0  ∥p0∥2 < · · · < ∥pj∥2 < ∥pj+1∥2 < · · · < ∥p∥2 ≤.
Proof.
We ﬁrst show that the sequences of vectors generated by Algorithm 4.3 satisfy
pT
j rj  0 for j ≥0 and pT
j dj > 0 for j ≥1.
Algorithm 4.3 computes pj+1 recursively in terms of pj, but when all the terms of this
recursion are written explicitly, we see that
pj  p0 +
j−1

i0
αidi 
j−1

i0
αidi,
since p0  0. Multiplying by rj and applying the expanding subspace property of CG gives
pT
j rj 
j−1

i0
αidT
i rj  0.
An induction proof establishes the relation pT
j dj > 0. By applying the expanding
subspace property again, we obtain
pT
1 d1  (α0d0)T (r1 + β1d0)  α0β1 dT
0 d0 > 0.
(4.17)

4 . 2 .
U s i n g N e a r l y E x a c t S o l u t i o n s t o t h e S u b p r o b l e m
77
WenowmaketheinductivehypothesisthatpT
j dj > 0andshowthatthisimpliespT
j+1dj+1 >
0. From (4.17), we have pT
j+1rj+1  0, and therefore we have
pT
j+1dj+1  pT
j+1(rj+1 + βj+1dj)
 βj+1pT
j+1dj
 βj+1(pj + αjdj)T dj
 βj+1pT
j dj + αjβj+1dT
j dj.
Because of the inductive hypothesis, the last expression is positive.
Wenowprovethetheorem.IfthealgorithmstopsbecausedT
j Bdj ≤0or∥pj+1∥2 ≥,
then the ﬁnal point p is chosen to make ∥p∥2  , which is the largest possible length
any point can have. To cover all other possibilities in the algorithm we must show that
∥pj∥2 < ∥pj+1∥2 when pj+1  pj + αjdj and j ≥1. Observe that
∥pj+1∥2
2  (pj + αjdj)T (pj + αjdj)  ∥pj∥2
2 + 2αjpT
j dj + α2
j∥dj∥2
2.
It follows from this expression and our intermediate result that ∥pj∥2 < ∥pj+1∥2, so our
proof is complete.
□
From this theorem we see that the iterates of Algorithm 4.3 sweep out points pj that
move on some interpolating path from p1 to p, a path in which every step increases its total
distance from the start point. When B is positive deﬁnite, this path may be compared to the
path of the dogleg method, because both methods move from the Cauchy step pC to the full
step pB, until the trust-region boundary intervenes.
A Newton trust-region method chooses B to be the exact Hessian ∇2f (x), which may
be indeﬁnite during the course of the iteration (hence our focus on the case of indeﬁnite B).
This method has excellent local and global convergence properties, as we see in Chapter 6.
4.2
USING NEARLY EXACT SOLUTIONS TO THE SUBPROBLEM
CHARACTERIZING EXACT SOLUTIONS
The methods discussed above make no serious attempt to seek the exact solution of
the subproblem (4.9). They do, however, make some use of the information in the Hessian
B, and they have advantages of low cost and global convergence, since they all generate a
point that is at least as good as the Cauchy point.
When the problem is relatively small (that is, n is not too large), it may be worthwhile
to exploit the model more fully by looking for a closer approximation to the solution of
the subproblem. In the next few pages we describe an approach for ﬁnding a good approxi-
mation at the cost of about three factorizations of the matrix B, as compared with a single

78
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
factorization for the dogleg and two-dimensional subspace minimization methods. This
approach is based on a convenient characterization of the exact solution of (4.9) (we need to
be able to recognize an exact solution when we see it, after all) and an ingenious application
of Newton’s method in one variable. Essentially, we see that a solution p of the trust-region
problem satisﬁes the formula
(B + λI)p∗ −g
for some λ ≥0, and our algorithm for ﬁnding p∗aims to identify the appropriate value of λ.
The following theorem gives the precise characterization of the solution of (4.9).
Theorem 4.3.
The vector p∗is a global solution of the trust-region problem
min
p∈IRn m(p)  f + gT p + 1
2pT Bp,
s.t. ∥p∥≤,
(4.18)
if and only if p∗is feasible and there is a scalar λ ≥0 such that the following conditions are
satisﬁed:
(B + λI)p∗ −g,
(4.19a)
λ( −||p∗||)  0,
(4.19b)
(B + λI)
is positive semideﬁnite.
(4.19c)
We delay the proof of this result until later in the chapter, and instead discuss just its
key features here with the help of Figure 4.4. The condition (4.19b) is a complementarity
condition that states that at least one of the nonnegative quantities λ and (−∥p∗∥) must be
zero. Hence, when the solution lies strictly inside the trust region (as it does when   1 in
Figure4.4),wemusthave λ  0andsoBp∗ −g withB positivesemideﬁnite,from(4.19a)
and (4.19c), respectively. In the other cases   2 and   3, we have ∥p∗∥ , and
so λ is allowed to take a positive value. Note from (4.19a) that
λp∗ −Bp∗−g  −∇m(p∗),
that is, the solution p∗is collinear with the negative gradient of m and normal to its contours.
These properties can be seen clearly in Figure 4.4.
CALCULATING NEARLY EXACT SOLUTIONS
The characterization of Theorem 4.3 suggests an algorithm for ﬁnding the solution p
of (4.18). Either λ  0 satisﬁes (4.19a) and (4.19c) with ∥p∥≤, or else we deﬁne
p(λ)  −(B + λI)−1g

4 . 2 .
U s i n g N e a r l y E x a c t S o l u t i o n s t o t h e S u b p r o b l e m
79
p1*
p3
*
p2
*
mk
1
3
∆
∆2
∆
contours of 
Figure 4.4
Solution of trust-region subproblem for different radii 1, 2, 3.
for λ sufﬁciently large that B + λI is positive deﬁnite (see the exercises), and seek a value
λ > 0 such that
∥p(λ)∥ .
(4.20)
This problem is a one-dimensional root-ﬁnding problem in the variable λ.
To see that a value of λ with all the desired properties exists, we appeal to the eigende-
composition of B and use it to study the properties of ∥p(λ)∥. Since B is symmetric, there
is an orthogonal matrix Q and a diagonal matrix  such that B  QQT , where
  diag(λ1, λ2, . . . , λn),
and λ1 ≤λ2 ≤· · · ≤λn are the eigenvalues of B; see (A.46). Clearly, B + λI  Q( +
λI)QT , and for λ ̸ λj, we have
p(λ)  −Q( + λI)−1QT g  −
n

j1
qT
j g
λj + λqj,
(4.21)

80
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
where qj denotes the jth column of Q. Therefore, by orthonormality of q1, q2, . . . , qn, we
have
∥p(λ)∥2 
n

j1

qT
j g
2
(λj + λ)2 .
(4.22)
This expression tells us a lot about ∥p(λ)∥. If λ > −λ1, we have λj + λ > 0 for all
j  1, 2, . . . , n, and so ∥p(λ)∥is a continuous, nonincreasing function of λ on the interval
(−λ1, ∞). In fact, we have that
lim
λ→∞∥p(λ)∥ 0.
(4.23)
Moreover, we have when qT
j g ̸ 0 that
lim
λ→−λj ∥p(λ)∥ ∞.
(4.24)
These features can be seen in Figure 4.5. It is clear that the graph of ∥p(λ)∥attains the
valueatexactlyonepointintheinterval(−λ1, ∞),whichisdenotedbyλ∗intheﬁgure.For
the case of B positive deﬁnite and ∥B−1g∥≤, the value λ  0 satisﬁes (4.19), so there
λ3
λ*
λ2
λ1
|| p||
∆
-
-
-
-
Figure 4.5
∥p(λ)∥as a function of λ.

4 . 2 .
U s i n g N e a r l y E x a c t S o l u t i o n s t o t h e S u b p r o b l e m
81
is no need to carry out a search. When B is positive deﬁnite but ∥B−1g∥> , there is a
strictly positive value of λ for which ∥p(λ)∥ , so we search for the solution to (4.20) in
the interval (0, ∞).
For the case of B indeﬁnite and qT
1 g ̸ 0, (4.23) and (4.24) guarantee that we can ﬁnd
a solution in the interval (−λ1, ∞). We could use the root-ﬁnding Newton’s method (see
the Appendix) to ﬁnd the value of λ > λ1 that solves
φ1(λ)  ∥p(λ)∥−  0.
(4.25)
The disadvantage of this approach can be seen by considering the form of ∥p(λ)∥when λ is
greater than, but close to, −λ1. We then have
φ1(λ) ≈
C1
λ + λ1
+ C2,
where C1 > 0 and C2 are constants. For these values of λ the function is highly nonlinear,
and therefore the root-ﬁnding Newton’s method will be unreliable or slow. Better results will
be obtained if we reformulate the problem (4.25) so that it is nearly linear near the optimal
λ. By deﬁning
φ2(λ)  1
 −
1
∥p(λ)∥,
we see that for λ slightly greater than −λ1 we have from (4.22) that
φ2(λ) ≈1
 −λ + λ1
C3
for some C3 > 0. Hence φ2 is nearly linear in the range we consider, and the root-ﬁnding
Newton’smethodwillperformwell,providedthatitmaintainsλ > −λ1 (seeFigure4.6).The
root-ﬁnding Newton’s method applied to φ2 generates a sequence of iterates λ(ℓ) by setting
λ(ℓ+1)  λ(ℓ) −φ2
	
λ(ℓ)
φ′
2
	
λ(ℓ)
.
(4.26)
After some elementary manipulation (see the exercises), this updating formula can be
implemented in the following practical way.
Algorithm 4.4 (Exact Trust Region).
Given λ(0),  > 0:
for ℓ 0, 1, 2, . . .
Factor B + λ(ℓ)I  RT R;

82
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
-1
-1
∆
p
||
||
λ
λ
λ
λ
3
2
1
*
-
-
-
-
Figure 4.6
1/∥p(λ)∥as a function of λ.
Solve RT Rpℓ −g, RT qℓ pℓ;
Set
λ(ℓ+1)  λ(ℓ) +
∥pℓ∥
∥qℓ∥
2 ∥pℓ∥−


;
(4.27)
end (for).
Safeguards must be added to this algorithm to make it practical; for instance, when λ(ℓ) <
−λ1, the Cholesky factorization B +λ(ℓ)I  RT R will not exist. A slightly enhanced version
of this algorithm does, however, converge to a solution of (4.20) in most cases.
Themainworkineachiterationofthismethodis,ofcourse,theCholeskyfactorization.
Practical versions of this algorithm do not iterate until convergence to the optimal λ is
obtained with high accuracy, but are content with an approximate solution that can be
obtained in two or three iterations.
THE HARD CASE
Recall that in the discussion above, we assumed that qT
1 g ̸ 0 in dealing with the case
of indeﬁnite B. In fact, the approach described above can be applied even when the most

4 . 2 .
U s i n g N e a r l y E x a c t S o l u t i o n s t o t h e S u b p r o b l e m
83
λ3
∆
λ
λ
2
1
|| p||
-
-
-
Figure 4.7
The hard case: ∥p(λ)∥<  for all λ ∈(−λ1, ∞).
negative eigenvalue is a multiple eigenvalue (that is, 0 > λ1  λ2  · · ·), provided that
qT
j g ̸ 0 for at least one of the indices j for which λj  λ1. When this condition does not
hold, the situation becomes a little complicated, because the limit (4.24) does not hold for
λj  λ1 and so there may not be a value λ ∈(−λ1, ∞) such that ∥p(λ)∥  (see Figure
4.7). Mor´e and Sorensen [170] refer to this case as the hard case. At ﬁrst glance, it is not
clear how p and λ can be chosen to satisfy (4.19) in the hard case. Clearly, our root-ﬁnding
technique will not work, since there is no solution for λ in the open interval (−λ1, ∞). But
Theorem 4.3 assures us that the right value of λ lies in the interval [−λ1, ∞), so there is only
one possibility: λ  −λ1. To ﬁnd p, it is not enough to delete the terms for which λj  λ1
from the formula (4.21) and set
p 

j:λj ̸λ1
qT
j g
λj + λqj.
Instead, we note that (B −λ1I) is singular, so there is a vector z such that ∥z∥ 1 and
(B −λ1I)z  0. In fact, z is an eigenvector of B corresponding to the eigenvalue λ1, so by
orthogonality of Q we have qT
j z  0 for λj ̸ λ1. It follows from this property that if we set
p 

j:λj̸λ1
qT
j g
λj + λqj + τz
(4.28)

84
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
for any scalar τ, we have
∥p∥2 

j:λj̸λ1

qT
j g
2
(λj + λ)2 + τ 2,
so it is always possible to choose τ to ensure that ∥p∥ . It is easy to check that (4.19)
holds for this choice of p and λ  −λ1.
PROOF OF THEOREM 4.3
We now give a formal proof of Theorem 4.3, the result that characterizes the exact
solution of (4.9). The proof relies on the following technical lemma, which deals with the
unconstrained minimizers of quadratics and is particularly interesting in the case where the
Hessian is positive semideﬁnite.
Lemma 4.4.
Let m be the quadratic function deﬁned by
m(p)  gT p + 1
2pT Bp,
(4.29)
where B is any symmetric matrix. Then
(i) m attains a minimum if and only if B is positive semideﬁnite and g is in the range of B;
(ii) m has a unique minimizer if and only if B is positive deﬁnite;
(iii) if B is positive semideﬁnite, then every p satisfying Bp  −g is a global minimizer of m.
Proof.
We prove each of the three claims in turn.
(i) We start by proving the “if” part. Since g is in the range of B, there is a p with Bp  −g.
For all w ∈Rn, we have
m(p + w)  gT (p + w) + 1
2(p + w)T B(p + w)
 (gT p + 1
2pT Bp) + gT w + (Bp)T w + 1
2wT Bw
 m(p) + 1
2wT Bw
≥m(p),
since B is positive semideﬁnite. Hence p is a minimum of m.
For the “only if” part, let p be a minimizer of m. Since ∇m(p)  Bp + g  0, we
have that g is in the range of B. Also, we have ∇2m(p)  B positive semideﬁnite, giving the
result.

4 . 2 .
U s i n g N e a r l y E x a c t S o l u t i o n s t o t h e S u b p r o b l e m
85
(ii) For the “if” part, the same argument as in (i) sufﬁces with the additional point that
wT Bw > 0 whenever w ̸ 0. For the “only if” part, we proceed as in (i) to deduce that B is
positive semideﬁnite. If B is not positive deﬁnite, there is a vector w ̸ 0 such that Bw  0.
Hence from the logic above we have m(p + w)  m(p), so the minimizer is not unique,
giving a contradiction.
(iii) Follows from the proof of (i).
□
To illustrate case (i), suppose that
B 


1
0
0
0
0
0
0
0
2

,
which has eigenvalues 0, 1, 2 and is therefore singular. If g is any vector whose second
component is zero, then g will be in the range of B, and the quadratic will attain a minimum.
But if the second element in g is nonzero, we can decrease m(·) indeﬁnitely by moving along
the direction α(0, −g2, 0)T as α ↑∞.
We are now in a position to take account of the trust-region bound ∥p∥≤ and
hence prove Theorem 4.3.
Proof.
(Theorem 4.3)
Assume ﬁrst that there is λ ≥0 such that the conditions (4.19) are satisﬁed.
Lemma 4.4(iii) implies that p∗is a global minimum of the quadratic function
ˆm(p)  gT p + 1
2pT (B + λI)p  m(p) + λ
2 pT p.
(4.30)
Since ˆm(p) ≥ˆm(p∗), we have
m(p) ≥m(p∗) + λ
2 ((p∗)T p∗−pT p).
(4.31)
Because λ( −∥p∗∥)  0 and therefore λ(2 −(p∗)T p∗)  0, we have
m(p) ≥m(p∗) + λ
2 (2 −pT p).
Hence, from λ ≥0, we have m(p) ≥m(p∗) for all p with ∥p∥≤. Therefore, p∗is a
global minimizer of (4.18).
For the converse, we assume that p∗is a global solution of (4.18) and show that there
is a λ ≥0 that satisﬁes (4.19).

86
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
In the case ∥p∗∥< , p∗is an unconstrained minimizer of m, and so
∇m(p∗)  Bp∗+ g  0,
∇2m(p∗)  B positive semideﬁnite,
and so the properties (4.19) hold for λ  0.
Assume for the remainder of the proof that ∥p∗∥ . Then (4.19b) is immediately
satisﬁed, and p∗also solves the constrained problem
min m(p)
subject to ∥p∥ .
Byapplyingoptimalityconditionsforconstrainedoptimizationtothisproblem(see(12.30)),
we ﬁnd that there is a λ such that the Lagrangian function deﬁned by
L(p, λ)  m(p) + λ
2 (pT p −2)
has a stationary point at p∗. By setting ∇pL(p∗, λ) to zero, we obtain
Bp∗+ g + λp∗ 0
⇒(B + λI)p∗ −g,
(4.32)
so that (4.19a) holds. Since m(p) ≥m(p∗) for any p with pT p  (p∗)T p∗ 2, we have
for such vectors p that
m(p) ≥m(p∗) + λ
2
	
(p∗)T p∗−pT p

.
If we substitute the expression for g from (4.32) into this expression, we obtain after some
rearrangement that
1
2(p −p∗)T (B + λI)(p −p∗) ≥0.
(4.33)
Since the set of directions

w : w  ± p −p∗
∥p −p∗∥, for some p with ∥p∥ 

is dense on the unit sphere, (4.33) sufﬁces to prove (4.19c).
It remains to show that λ ≥0. Because (4.19a) and (4.19c) are satisﬁed by p∗, we have
fromLemma4.4(i)thatp∗minimizes ˆm,so(4.31)holds.Supposethatthereareonlynegative
values of λ that satisfy (4.19a) and (4.19c). Then we have from (4.31) that m(p) ≥m(p∗)
whenever ∥p∥≥∥p∗∥ . Since we already know that p∗minimizes m for ∥p∥≤,
it follows that m is in fact a global, unconstrained minimizer of m. From Lemma 4.4(i) it
follows that Bp  −g and B is positive semideﬁnite. Therefore conditions (4.19a) and

4 . 3 .
G l o b a l C o n v e r g e n c e
87
(4.19c) are satisﬁed by λ  0, which contradicts our assumption that only negative values
of λ can satisfy the conditions. We conclude that λ ≥0, completing the proof.
□
4.3
GLOBAL CONVERGENCE
REDUCTION OBTAINED BY THE CAUCHY POINT
In the preceding discussion of algorithms for approximately solving the trust-region
subproblem,wehaverepeatedlyemphasizedthatglobalconvergencedependsontheapprox-
imate solution obtaining at least as much decrease in the model function m as the Cauchy
point. In fact, a ﬁxed fraction of the Cauchy decrease sufﬁces, as we show in the next few
pages. We start by obtaining an estimate of the decrease in m achieved by the Cauchy point,
and then use this estimate to prove that the sequence of gradients {∇fk} generated by Algo-
rithm 4.1 either has an accumulation point at zero or else converges to zero, depending on
whether we choose the parameter η to be zero or strictly positive in Algorithm 4.1. Finally,
we state a convergence result for the version of Algorithm 4.1 that uses the nearly exact
solutions calculated by Algorithm 4.4 above.
We start by proving that the dogleg and two-dimensional subspace minimization
algorithms and Algorithm 4.3 produce approximate solutions pk of the subproblem (4.3)
that satisfy the estimate
mk(0) −mk(pk) ≥c1∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

,
(4.34)
for some constant c1 ∈(0, 1]. The presence of an alternative given by the minimum in
(4.34) is typical of trust-region methods and arises because of the trust-region bound. The
usefulness of this estimate will become clear in the following two sections. For now, we note
that when k is the minimum value in (4.34), the condition is slightly reminiscent of the
ﬁrst Wolfe condition: The desired reduction in the model is proportional to the gradient
and the size of the step.
We show now that the Cauchy point pC
k satisﬁes (4.34), with c1  1
2.
Lemma 4.5.
The Cauchy point pC
k satisﬁes (4.34) with c1  1
2, that is,
mk(0) −mk(pC
k) ≥1
2∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

.
(4.35)
Proof.
We consider ﬁrst the case of ∇f T
k Bk∇fk ≤0. Here, we have
mk(pC
k) −mk(0)  mk(k∇fk/∥∇fk∥)

88
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
 −
k
∥∇fk∥∥∇fk∥2 + 1
2
2
k
∥∇fk∥2 ∇f T
k Bk∇fk
≤−k∥∇fk∥
≤−∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

,
and so (4.35) certainly holds.
For the next case, consider ∇f T
k Bk∇fk > 0 and
∥∇fk∥3
k∇f T
k Bk∇fk
≤1.
(4.36)
We then have τ  ∥∇fk∥3/
	
k∇f T
k Bk∇fk

, and so
mk(pC
k) −mk(0)  −
∥∇fk∥4
∇f T
k Bk∇fk
+ 1
2∇f T
k Bk∇fk
∥∇fk∥4
(∇f T
k Bk∇fk)2
 −1
2
∥∇fk∥4
∇f T
k Bk∇fk
≤−1
2
∥∇fk∥4
∥Bk∥∥∇fk∥2
 −1
2
∥∇fk∥2
∥Bk∥
≤−1
2∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

,
so (4.35) holds here too.
In the remaining case, (4.36) does not hold, and therefore
∇f T
k Bk∇fk < ∥∇fk∥3
k
.
(4.37)
From the deﬁnition of pC
k, we have τ  1, so using this fact together with (4.37), we obtain
mk(pC
k) −mk(0)  −
k
∥∇fk∥∥∇fk∥2 + 1
2
2
k
∥∇fk∥2 ∇f T
k Bk∇fk
≤−k∥∇fk∥+ 1
2
2
k
∥∇fk∥2
∥∇fk∥3
k
 −1
2k∥∇fk∥
≤−1
2∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

,
yielding the desired result (4.35) once more.
□

4 . 3 .
G l o b a l C o n v e r g e n c e
89
To satisfy (4.34), our approximate solution pk has only to achieve a reduction that is
at least some ﬁxed fraction c2 of the reduction achieved by the Cauchy point. We state the
observation formally as a theorem.
Theorem 4.6.
Let pk be any vector such that ∥pk∥≤k and mk(0)−mk(pk) ≥c2
	
mk(0) −mk(pC
k)

.
Then pk satisﬁes (4.34) with c1  c2/2. In particular, if pk is the exact solution p∗
k of (4.3),
then it satisﬁes (4.34) with c1  1
2.
Proof.
Since ∥pk∥≤k, we have from (4.35) that
mk(0) −mk(pk) ≥c2
	
mk(0) −mk(pC
k)

≥1
2c2∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

,
giving the result.
□
Note that the dogleg and two-dimensional subspace minimization algorithms and
Algorithm 4.3 all satisfy (4.34) with c1  1
2, because they all produce approximate solutions
pk for which mk(pk) ≤mk(pC
k).
CONVERGENCE TO STATIONARY POINTS
Global convergence results for trust-region methods come in two varieties, depending
on whether we set the parameter η in Algorithm 4.1 to zero or to some small positive value.
When η  0 (that is, the step is taken whenever it produces a lower value of f ), we can
show that the sequence of gradients {∇fk} has a limit point at zero. For the more stringent
acceptance test with η > 0, which requires the actual decrease in f to be at least some small
fraction of the predicted decrease, we have the stronger result that ∇fk →0.
In this section we prove the global convergence results for both cases. We assume
throughout that the approximate Hessians Bk are uniformly bounded in norm, and that the
level set
{x | f (x) ≤f (x0)}
(4.38)
is bounded. For generality, we also allow the length of the approximate solution pk of (4.3)
to exceed the trust-region bound, provided that it stays within some ﬁxed multiple of the
bound; that is,
∥pk∥≤γ k,
for some constant γ ≥1.
(4.39)
The ﬁrst result deals with the case of η  0.

90
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
Theorem 4.7.
Let η  0 in Algorithm 4.1. Suppose that ∥Bk∥≤β for some constant β, that f is
continuously differentiable and bounded below on the level set (4.38), and that all approximate
solutions of (4.3) satisfy the inequalities (4.34) and (4.39), for some positive constants c1 and
γ . We then have
lim inf
k→∞∥∇fk∥ 0.
(4.40)
Proof.
We ﬁrst perform some technical manipulation with the ratio ρk from (4.4). We have
|ρk −1| 

(f (xk) −f (xk + pk)) −(mk(0) −mk(pk))
mk(0) −mk(pk)



mk(pk) −f (xk + pk)
mk(0) −mk(pk)
 .
Since from Taylor’s theorem (Theorem 2.1) we have that
f (xk + pk)  f (xk) + ∇f (xk)T pk +
 1
0
[∇f (xk + tpk) −∇f (xk)]T pk dt,
it follows from the deﬁnition (4.1) of mk that
|mk(pk) −f (xk + pk)|  | 1
2pT
k Bkpk −
 1
0
[∇f (xk + tpk) −∇f (xk)]T pk dt|
≤(β/2)∥pk∥2 + C4(pk)∥pk∥,
(4.41)
where we can make the scalar C4(pk) arbitrarily small by restricting the size of pk.
Suppose for contradiction that there is ϵ > 0 and a positive index K such that
∥∇fk∥≥ϵ
for all k ≥K.
(4.42)
From (4.34), we have for k ≥K that
mk(0) −mk(pk) ≥c1∥∇fk∥min

k, ∥∇fk∥
∥Bk∥

≥c1ϵ min

k, ϵ
β

.
(4.43)
Using (4.43), (4.41), and the bound (4.39), we have
|ρk −1| ≤γ k(βγ k/2 + C4(pk))
2c1ϵ min(k, ϵ/β)
.
(4.44)
We now derive a bound on the right-hand-side that holds for all sufﬁciently small values of
k, that is, for all k ≤¯, where ¯ is to be determined. By choosing ¯ to be small enough

4 . 3 .
G l o b a l C o n v e r g e n c e
91
and noting that ∥pk∥≤γ k ≤γ ¯, we can ensure that the term in parentheses in the
numerator of (4.44) satisﬁes the bound
βγ k/2 + C4(pk) ≤c1ϵ
2γ .
(4.45)
By choosing ¯ even smaller, if necessary, to ensure that k ≤¯ ≤ϵ/β, we have from (4.44)
that
|ρk −1| ≤γ kc1ϵ/(2γ )
2c1ϵk
 1
4.
Therefore, ρk > 3
4, and so by the workings of Algorithm 4.1, we have k+1 ≥k whenever
k falls below the threshold ¯. It follows that reduction of k
	
by a factor of 1
4

can occur
in our algorithm only if
k ≥¯,
and therefore we conclude that
k ≥min
	
K, ¯/4

for all k ≥K.
(4.46)
Suppose now that there is an inﬁnite subsequence K such that ρk ≥1
4 for k ∈K. If
k ∈K and k ≥K, we have from (4.43) that
f (xk) −f (xk+1)  f (xk) −f (xk + pk)
≥1
4 [mk(0) −mk(pk)]
≥1
4c1ϵ min(k, ϵ/β).
Since f is bounded below, it follows from this inequality that
lim
k∈K, k→∞k  0,
contradicting (4.46). Hence no such inﬁnite subsequence K can exist, and we must have
ρk < 1
4 for all k sufﬁciently large. In this case, k will eventually be reduced by a factor of 1
4
at every iteration, and we have limk→∞k  0, which again contradicts (4.46). Hence, our
original assertion (4.42) must be false, giving (4.40).
□
Our second global convergence result, for the case η > 0, borrows much of the analysis
from the proof above. Our approach here follows that of Schultz, Schnabel, and Byrd [226].

92
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
Theorem 4.8.
Let η ∈
	
0, 1
4

in Algorithm 4.1 . Suppose that ∥Bk∥≤β for some constant β, that f
is Lipschitz continuously differentiable and bounded below on the level set (4.38), and that all
approximate solutions pk of (4.3) satisfy the inequalities (4.34) and (4.39) for some positive
constants c1 and γ . We then have
lim
k→∞∇fk  0.
(4.47)
Proof.
Consider any index m such that ∇fm ̸ 0. If we use β1 to denote the Lipschitz
constant for ∇f on the level set (4.38), we have
∥∇f (x) −∇fm∥≤β1∥x −xm∥,
for all x in the level set. Hence, by deﬁning the scalars
ϵ  1
2∥∇fm∥,
R  ∥∇fm∥
2β1
 ϵ
β1
,
and the ball
B(xm, R)  {x | ∥x −xm∥≤R},
we have
x ∈B(xm, R) ⇒∥∇f (x)∥≥∥∇fm∥−∥∇f (x) −∇fm∥≥1
2∥∇fm∥ ϵ.
If the entire sequence {xk}k≥m stays inside the ball B(xm, R), we would have ∥∇fk∥≥ϵ > 0
for all k ≥m. The reasoning in the proof of Theorem 4.7 can be used to show that this
scenario does not occur. Therefore, the sequence {xk}k≥m eventually leaves B(xm, R).
Let the index l ≥m be such that xl+1 is the ﬁrst iterate after xm outside B(xm, R).
Since ∥∇fk∥≥ϵ for k  m, m + 1, . . . , l, we can use (4.43) to write
f (xm) −f (xl+1) 
l
km
f (xk) −f (xk+1)
≥
l
km,xk̸xk+1η[mk(0) −mk(pk)]
≥
l
km,xk̸xk+1ηc1ϵ min

k, ϵ
β

,
wherewehavelimitedthesumtotheiterationsk forwhich xk ̸ xk+1,thatis,thoseiterations
on which a step was actually taken. If k ≤ϵ/β for all k  m, m + 1, . . . , l, we have
f (xm) −f (xl+1) ≥ηc1ϵ
l
km,xk̸xk+1
k ≥ηc1ϵR  ηc1ϵ2 1
β1
.
(4.48)

4 . 3 .
G l o b a l C o n v e r g e n c e
93
Otherwise, we have k > ϵ/β for some k  m, m + 1, . . . , l, and so
f (xm) −f (xl+1) ≥ηc1ϵ ϵ
β .
(4.49)
Since the sequence {f (xk)}∞
k0 is decreasing and bounded below, we have that
f (xk) ↓f ∗
(4.50)
for some f ∗> −∞. Therefore, using (4.48) and (4.49), we can write
f (xm) −f ∗≥f (xm) −f (xl+1)
≥ηc1ϵ2 min
 1
β , 1
β1

 1
4ηc1 min
 1
β , 1
β1

∥∇fm∥2.
By rearranging this expression, we obtain
∥∇fm∥2 ≤
1
4ηc1 min
 1
β , 1
β1
−1
(f (xm) −f ∗),
so from (4.50) we conclude that ∇fm →0, giving the result.
□
CONVERGENCE OF ALGORITHMS BASED ON NEARLY EXACT SOLUTIONS
As we noted in the discussion of Algorithm 4.4, the loop to determine the optimal
values of λ and p for the subproblem (4.9) does not iterate until high accuracy is achieved.
Instead, it is terminated after two or three iterations with a fairly loose approximation to
the true solution. The inexactness in this approximate solution is, however, measured in a
different way from the dogleg and subspace minimization algorithms and Algorithm 4.3,
and this difference affects the nature of the global convergence results that can be proved.
Mor´e and Sorensen [170] describe a safeguarded version of the root-ﬁnding Newton
method that adds features for handling the hard case. Its termination criteria ensure that
their approximate solution p satisﬁes the conditions
m(0) −m(p) ≥c1(m(0) −m(p∗)),
(4.51a)
∥p∥≤γ 
(4.51b)
(where p∗is the exact solution of (4.3)), for some constants c1 ∈(0, 1] and γ > 0. The
condition (4.51a) ensures that the approximate solution achieves a signiﬁcant fraction of
the maximum decrease possible in the model function m. Of course, it is not necessary to
know p∗to enforce this condition; it follows from practical termination criteria. One major
difference between (4.51) and the earlier criterion (4.34) is that (4.51) makes better use of
the second-order part of m(·), that is, the pT Bp term. This difference is illustrated by the

94
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
case in which g  0 while B has negative eigenvalues, indicating that the current iterate
xk is a saddle point. Here, the right-hand-side of (4.34) is zero (indeed, the algorithms we
described earlier would terminate at such a point). The right-hand-side of (4.51) is positive,
indicating that decrease in the model function is still possible, so it forces the algorithm to
move away from xk.
The close attention that near-exact algorithms pay to the second-order term is war-
ranted only if this term closely reﬂects the actual behavior of the function f —in fact, the
trust-region Newton method, for which B  ∇2f (x), is the only case that has been treated
in the literature. For purposes of global convergence analysis, the use of the exact Hessian
allows us to say more about the limit points of the algorithm than merely that they are
stationary points. In fact, second-order necessary conditions (Theorem 2.3) are satisﬁed at
the limit points. The following result establishes this claim.
Theorem 4.9.
Suppose Algorithm 4.1 is applied with Bk  ∇2f (xk), constant η in the open interval
	
0, 1
4

, and the approximate solution pk at each iteration satisfying (4.51) for some ﬁxed γ > 0.
Then limk→∞∥∇fk∥ 0.
If, in addition, the level set {x | f (x) ≤f (x0)} is compact, then either the algorithm
terminates at a point xk at which the second-order necessary conditions (Theorem 2.3) for a
local minimum hold, or else {xk} has a limit point x∗in the level set at which the necessary
conditions hold.
We omit the proof, which can be found in Mor´e and Sorensen [170, Section 4].
4.4
OTHER ENHANCEMENTS
SCALING
As we noted in Chapter 2, optimization problems are often posed with poor scaling—
the objective function f is highly sensitive to small changes in certain components of the
vector x and relatively insensitive to changes in other components. Topologically, a symptom
of poor scaling is that the minimizer x∗lies in a narrow valley, so that the contours of
the objective f (·) near x∗tend towards highly eccentric ellipses. Algorithms can perform
poorly unless they compensate for poor scaling; see Figure 2.7 for an illustration of the poor
performance of the steepest descent approach.
Recalling our deﬁnition of a trust region—a region around the current iterate within
which the model mk(·) is an adequate representation of the true objective f (·)—it is easy to
see that a spherical trust region is not appropriate to the case of poorly scaled functions. We
can trust our model mk to be reasonably accurate only for short distances along the highly
sensitive directions, while it is reliable for longer distances along the less sensitive directions.
Since the shape of our trust region should be such that our conﬁdence in the model is more
or less the same at all points on the boundary of the region, we are led naturally to consider

4 . 4 .
O t h e r E n h a n c e m e n t s
95
elliptical trust regions in which the axes are short in the sensitive directions and longer in
the less sensitive directions. Elliptical trust regions can be deﬁned by
∥Dp∥≤,
(4.52)
where D is a diagonal matrix with positive diagonal elements, yielding the following scaled
trust-region subproblem:
min
p∈IRn mk(p)
def fk + ∇f T
k p + 1
2pT Bkp
s.t. ∥Dp∥≤k.
(4.53)
When f (x) is highly sensitive to the value of the ith component xi, we set the corresponding
diagonal element dii of D to be large. The value of dii will be closer to zero for less-sensitive
components xi.
Information to construct the scaling matrix D can be derived reliably from the second
derivatives ∂2f/∂x2
i . We can allow D to change from iteration to iteration, as long as all
diagonal elements dii stay inside some predetermined range [dlo, dhi], where 0 < dlo ≤
dhi < ∞. Of course, we do not need D to be a precise reﬂection of the scaling of the problem,
so it is not necessary to devise elaborate heuristics or to spend a lot of computation to get it
just right.
All algorithms discussed in this chapter can be modiﬁed for the case of elliptical trust
regions, and the convergence theory continues to hold, with numerous superﬁcial modiﬁca-
tions. The Cauchy point calculation procedure (Algorithm 4.2), for instance, changes only in
the speciﬁcations of the trust region in (4.5) and (4.6). We obtain the following generalized
version.
Algorithm 4.5 (Generalized Cauchy Point Calculation).
Find the vector pS
k that solves
pS
k  arg min
p∈IRn fk + ∇f T
k p
s.t. ∥Dp∥≤k;
(4.54)
Calculate the scalar τk > 0 that minimizes mk(τpS
k) subject to satisfying the trust-region
bound, that is,
τk  arg min
τ>0 mk(τpS
k)
s.t. ∥τDpS
k∥≤k;
(4.55)
pC
k  τkpS
k.
For this scaled version, we ﬁnd that
pS
k  −
k
∥D−1∇fk∥D−2∇fk,
(4.56)

96
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
and that the step length τk is obtained from the following modiﬁcation of (4.8):
τk 



1
if ∇f T
k D−2BkD−2∇fk ≤0
min

∥D−1∇fk∥3
k∇f T
k D−2BkD−2∇fk
, 1

otherwise.
(4.57)
(The details are left as an exercise.)
A simpler alternative for adjusting the deﬁnition of the Cauchy point and the various
algorithms of this chapter to allow for the elliptical trust region is simply to rescale the
variables p in the subproblem (4.53) so that the trust region is spherical in the scaled
variables. By deﬁning
˜p
def Dp,
and by substituting into (4.53), we obtain
min
˜p∈IRn ˜mk( ˜p)
def fk + ∇f T
k D−1 ˜p + 1
2 ˜pT D−1BkD−1 ˜p
s.t. ∥˜p∥≤k.
The theory and algorithms can now be derived in the usual way by substituting ˜p for p,
D−1∇fk for ∇fk, D−1BkD−1 for Bk, and so on.
NON-EUCLIDEAN TRUST REGIONS
Trust regions may also be deﬁned in terms of norms other than the Euclidean norm.
For instance, we may have
∥p∥1 ≤k
or
∥p∥∞≤k,
or their scaled counterparts
∥Dp∥1 ≤k
or
∥Dp∥∞≤k,
where D is a positive diagonal matrix as before. Norms such as these offer no obvious
advantagesforunconstrainedoptimization,buttheymaybeusefulforconstrainedproblems.
For instance, for the bound-constrained problem
min
x∈IRn f (x),
subject to x ≥0,
the trust-region subproblem may take the form
min
p∈IRn mk(p)  fk + ∇f T
k p + 1
2pT Bkp
s.t. xk + p ≥0, ∥p∥≤k.
(4.58)

4 . 4 .
O t h e r E n h a n c e m e n t s
97
WhenthetrustregionisdeﬁnedbyaEuclideannorm,thefeasibleregionfor(4.58)consistsof
the intersection of a sphere and the nonnegative orthant—an awkward object, geometrically
speaking. When the ∞-norm is used, however, the feasible region is simply the rectangular
box deﬁned by
xk + p ≥0,
p ≥−ke,
p ≤ke,
where e  (1, 1, . . . , 1)T , so the solution of the subproblem is easily calculated by standard
techniques for quadratic programming.
NOTES AND REFERENCES
The inﬂuential paper of Powell [199] proves a result like Theorem 4.7 for the case of
η  0, where the algorithm takes a step whenever it decreases the function value. Powell uses
a weaker assumption than ours on the matrices ∥B∥, but his analysis is more complicated.
Mor´e [167] summarizes developments in algorithms and software before 1982, paying par-
ticular attention to the importance of using a scaled trust-region norm. Much of the material
in this chapter on methods that use nearly exact solutions to the subproblem (4.3) is drawn
from the paper of Mor´e and Sorensen [170].
Byrd, Schnabel, and Schultz [226], [39] provide a general theory for inexact trust-
region methods; they introduce the idea of two-dimensional subspace minimization and
also focus on proper handling of the case of indeﬁnite B to ensure stronger local convergence
results than Theorems 4.7 and 4.8. Dennis and Schnabel [70] survey trust-region methods as
part of their overview of unconstrained optimization, providing pointers to many important
developments in the literature.
✐
E x e r c i s e s
✐
4.1 Let f (x)  10(x2 −x2
1)2 + (1 −x1)2. At x  (0, −1) draw the contour lines of
the quadratic model (4.1) assuming that B is the Hessian of f . Draw the family of solutions
of (4.3) as the trust region radius varies from   0 to   2. Repeat this at x  (0, 0.5).
✐
4.2 Write a program that implements the dogleg method. Choose Bk to be the exact
Hessian. Apply it to solve Rosenbrock’s function (2.23). Experiment with the update rule for
the trust region by changing the constants in Algorithm 4.1, or by designing your own rules.
✐
4.3 Program the trust region method based on Algorithm 4.3. Choose Bk to be the
exact Hessian, and use it to minimize the function
min f (x) 
n

i1

(1 −x2i−1)2 + 10(x2i −x2
2i−1)2

98
C h a p t e r
4 .
T r u s t - R e g i o n M e t h o d s
with n  10. Experiment with the starting point and the stopping test for the CG iteration.
Repeat the computation with n  50.
Your program should indicate, at every iteration, whether Algorithm 4.3 encountered
negative curvature, reached the trust region boundary, or met the stopping test.
✐
4.4 Theorem 4.7 shows that the sequence {∥g∥} has an accumulation point at zero.
Show that if the iterates x stay in a bounded set B, then there is a limit point x∞of the
sequence {xk} such that ∇f (x∞)  0.
✐
4.5 Show that τk deﬁned by (4.8) does indeed identify the minimizer of mk along the
direction −∇fk.
✐
4.6 The Cauchy–Schwarz inequality states that for any vectors u and v, we have
|uT v|2 ≤(uT u)(vT v),
with equality only when u and v are parallel. When B is positive deﬁnite, use this inequality
to show that
γ
def
∥g∥4
(gT Bg)(gT B−1g) ≤1,
with equality only if g and Bg (and B−1g) are parallel.
✐
4.7 When B is positive deﬁnite, the double-dogleg method constructs a path with three
line segments from the origin to the full step. The four points that deﬁne the path are
• the origin;
• the unconstrained Cauchy step pC  −(gT g)/(gT Bg)g;
• a fraction of the full step ¯γ pB  −¯γ B−1g, for some ¯γ ∈(γ, 1], where γ is deﬁned in
the previous question; and
• the full step pB  −B−1g.
Show that ∥p∥increases monotonically along this path.
(Note: The double-dogleg method, as discussed in Dennis and Schnabel [69, Section
6.4.2], was for some time thought to be superior to the standard dogleg method, but later
testing has not shown much difference in performance.)
✐
4.8 Show that (4.26) and (4.27) are equivalent. Hints: Note that
d
dλ

1
∥p(λ)∥

 d
dλ
	
∥p(λ)∥2
−1/2  −1
2
	
∥p(λ)∥2
−3/2 d
dλ∥p(λ)∥2,
d
dλ ∥p(λ)∥2  −2
n

j1
(qT
j g)2
(λj + λ)3

4 . 4 .
O t h e r E n h a n c e m e n t s
99
(from (4.22)), and
∥q∥2  ∥R−T p∥2  pT (B + λI)−1p 
n

j1
(qT
j g)2
(λj + λ)3 .
✐
4.9 Derive the solution of the two-dimensional subspace minimization problem in
the case where B is positive deﬁnite.
✐
4.10 Show that if B is any symmetric matrix, then there exists λ ≥0 such that B +λI
is positive deﬁnite.
✐
4.11 Verify that the deﬁnitions (4.56) for pS
k and (4.57) for τk are valid for the Cauchy
point in the case of an elliptical trust region. (Hint: Using the theory of Chapter 12, we can
show that the solution of (4.54) satisﬁes ∇fk + αD2pS
k  0 for some scalar α ≥0.)
✐
4.12 The following example shows that the reduction in the model function m
achieved by the two-dimensional minimization strategy can be much smaller than that
achieved by the exact solution of (4.9).
In (4.9), set
g 

−1
ϵ , −1, −ϵ2
T
,
where ϵ is a small positive number. Set
B  diag
 1
ϵ3 , 1, ϵ3

,
  0.5.
Show that the solution of (4.9) has components
	
O(ϵ), 1
2 + O(ϵ), O(ϵ)

T and that the
reduction in the model m is 3
8 + O(ϵ). For the two-dimensional minimization strategy,
show that the solution is a multiple of B−1g and that the reduction in m is O(ϵ).

Chapter5

Conjugate
Gradient Methods
Our interest in the conjugate gradient method is twofold. It is one of the most useful tech-
niques for solving large linear systems of equations, and it can also be adapted to solve
nonlinearoptimizationproblems.Thesetwovariantsofthefundamentalapproach,whichwe
refertoasthelinear andnonlinear conjugategradientmethods,respectively,haveremarkable
properties that will be described in this chapter.
The linear conjugate gradient method was proposed by Hestenes and Stiefel in the
1950s as an iterative method for solving linear systems with positive deﬁnite coefﬁcient
matrices. It is an alternative to Gaussian elimination that is very well suited for solving large
problems.Theperformanceofthelinearconjugategradientmethodistiedtothedistribution
of the eigenvalues of the coefﬁcient matrix. By transforming, or preconditioning, the linear
system, we can make this distribution more favorable and improve the convergence of the
method signiﬁcantly. Preconditioning plays a crucial role in the design of practical conjugate
gradient strategies. Our treatment of the linear conjugate gradient method will highlight
those properties of the method that are important in optimization.
The ﬁrst nonlinear conjugate gradient method was introduced by Fletcher and Reeves
in the 1960s. It is one of the earliest known techniques for solving large-scale nonlinear
optimization problems. Over the years, many variants of this original scheme have been

102
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
proposed, and some are widely used in practice. The key features of these algorithms are
that they require no matrix storage and are faster than the steepest descent method.
5.1
THE LINEAR CONJUGATE GRADIENT METHOD
In this section we derive the linear conjugate gradient method and discuss its essential
convergence properties. For simplicity, we drop the qualiﬁer “linear” throughout.
The conjugate gradient method is an iterative method for solving a linear system of
equations
Ax  b,
(5.1)
where A is an n × n matrix that is symmetric and positive deﬁnite. The problem (5.1) can
be stated equivalently as the following minimization problem:
φ(x)  1
2xT Ax −bT x,
(5.2)
that is, both (5.1) and (5.2) have the same unique solution. This equivalence will allow us
to interpret the conjugate gradient method either as an algorithm for solving linear systems
or as a technique for minimization of convex quadratic functions. For future reference we
note that the gradient of φ equals the residual of the linear system,
∇φ(x)  Ax −b
def r(x).
(5.3)
CONJUGATE DIRECTION METHODS
One of the remarkable properties of the conjugate gradient method is its ability to
generate, in a very economical fashion, a set of vectors with a property known as conjugacy.
A set of nonzero vectors {p0, p1, . . . , pl} is said to be conjugate with respect to the symmetric
positive deﬁnite matrix A if
pT
i Apj  0,
for all i ̸ j.
(5.4)
It is easy to show that any set of vectors satisfying this property is also linearly independent.
The importance of conjugacy lies in the fact that we can minimize φ(·) in n steps
by successively minimizing it along the individual directions in a conjugate set. To verify
this claim, we consider the following conjugate direction method. (The distinction between
the conjugate gradient method and the conjugate direction method will become clear as we
proceed).Givenastartingpointx0 ∈IRn andasetofconjugatedirections{p0, p1, . . . , pn−1},

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
103
let us generate the sequence {xk} by setting
xk+1  xk + αkpk,
(5.5)
where αk is the one-dimensional minimizer of the quadratic function φ(·) along xk + αpk,
given explicitly by
αk  −rT
k pk
pT
k Apk
;
(5.6)
see (3.39). We have the following result.
Theorem 5.1.
For any x0 ∈IRn the sequence {xk} generated by the conjugate direction algorithm (5.5),
(5.6) converges to the solution x∗of the linear system (5.1) in at most n steps.
Proof.
Since the directions {pi} are linearly independent, they must span the whole space
IRn. Hence, we can write the difference between x0 and the solution x∗in the following way:
x∗−x0  σ0p0 + σ1p1 + · · · + σn−1pn−1,
for some choice of scalars σk. By premultiplying this expression by pT
k A and using the
conjugacy property (5.4), we obtain
σk  pT
k A(x∗−x0)
pT
k Apk
.
(5.7)
We now establish the result by showing that these coefﬁcients σk coincide with the step
lengths αk generated by the formula (5.6).
If xk is generated by algorithm (5.5), (5.6), then we have
xk  x0 + α0p0 + α1p1 + · · · + αk−1pk−1.
By premultiplying this expression by pT
k A and using the conjugacy property, we have that
pT
k A(xk −x0)  0,
and therefore
pT
k A(x∗−x0)  pT
k A(x∗−xk)  pT
k (b −Axk)  −pT
k rk.
By comparing this relation with (5.6) and (5.7), we ﬁnd that σk  αk, giving the result.
□

104
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
e1
e2
x*
x0
x1
.
.
.
Figure 5.1
Successive minimizations along the coordinate directions ﬁnd the
minimizer of a quadratic with a diagonal Hessian in n iterations.
There is a simple interpretation of the properties of conjugate directions. If the matrix
A in (5.2) is diagonal, the contours of the function φ(·) are ellipses whose axes are aligned
with the coordinate directions, as illustrated in Figure 5.1. We can ﬁnd the minimizer of this
function by performing one-dimensional minimizations along the coordinate directions
e1, e2, . . . , en in turn.
When A is not diagonal, its contours are still elliptical, but they are usually no longer
aligned with the coordinate directions. The strategy of successive minimization along these
directions in turn no longer leads to the solution in n iterations (or even in a ﬁnite number
of iterations). This phenomenon is illustrated in the two-dimensional example of Figure 5.2
We can recover the nice behavior of Figure 5.1 if we transform the problem to make
A diagonal and then minimize along the coordinate directions. Suppose we transform the
problem by deﬁning new variables ˆx as
ˆx  S−1x,
(5.8)
where S is the n × n matrix deﬁned by
S  [p0 p1 · · · pn−1],

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
105
x1
x3
x*
x2
x0
e1
e2
Figure5.2
Successive minimization along coordinate axes does not ﬁnd the solution
in n iterations, for a general convex quadratic.
where {p0, p2, . . . , pn−1} is the set of conjugate directions with respect to A. The quadratic
φ deﬁned by (5.2) now becomes
ˆφ(ˆx)
def φ(S ˆx)  1
2 ˆxT (ST AS)ˆx −(ST b)T ˆx.
By the conjugacy property (5.4), the matrix ST AS is diagonal, so we can ﬁnd the minimizing
value of ˆφ by performing n one-dimensional minimizations along the coordinate directions
of ˆx. Because of the relation (5.8), however, each coordinate direction in ˆx-spacecorresponds
to the direction pi inx-space.Hence,thecoordinatesearchstrategyappliedto ˆφ isequivalent
to the conjugate direction algorithm (5.5), (5.6). We conclude, as in Theorem 5.1 that the
conjugate direction algorithm terminates in at most n steps.
Returning to Figure 5.1, we note another interesting property: When the Hessian ma-
trix is diagonal, each coordinate minimization correctly determines one of the components
of the solution x∗. In other words, after k one-dimensional minimizations, the quadratic has
been minimized on the subspace spanned by e1, e2, . . . , ek. The following theorem proves
this important result for the general case in which the Hessian of the quadratic is not neces-
sarily diagonal. (Here and later, we use the notation span{p0, p1, . . . , pk} to denote the set
of all linear combinations of p0, p1, . . . , pk.) In proving the result we will make use of the

106
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
following expression, which is easily veriﬁed from the relations (5.3) and (5.5):
rk+1  rk + αkApk.
(5.9)
Theorem 5.2 (Expanding Subspace Minimization).
Let x0 ∈IRn be any starting point and suppose that the sequence {xk} is generated by the
conjugate direction algorithm (5.5), (5.6). Then
rT
k pi  0,
for i  0, . . . , k −1,
(5.10)
and xk is the minimizer of φ(x)  1
2xT Ax −bT x over the set
{x | x  x0 + span{p0, p1, . . . , pk−1}}.
(5.11)
Proof.
We begin by showing that a point ˜x minimizes φ over the set (5.11) if and only
if r(˜x)T pi  0, for each i  0, 1, . . . , k −1. Let us deﬁne h(σ)  φ(x0 + σ0p0 + · · · +
σk−1pk−1), where σ  (σ0, σ1, . . . , σk−1)T . Since h(σ) is a strictly convex quadratic, it has
a unique minimizer σ ∗that satisﬁes
∂h(σ ∗)
∂σi
 0,
i  0, 1, . . . , k −1.
By the chain rule, this implies that
∇φ(x0 + σ ∗
0 p0 + · · · + σ ∗
k−1pk−1)T pi  0,
i  0, 1, . . . , k −1.
By recalling the deﬁnition (5.3), we obtain the desired result.
We now use induction to show that xk satisﬁes (5.10). Since αk is always the one-
dimensional minimizer, we have immediately that rT
1 p0  0. Let us now make the induction
hypothesis, namely, that rT
k−1pi  0 for i  0, . . . , k −2. By (5.9),
rk  rk−1 + αk−1Apk−1,
we have
pT
k−1rk  pT
k−1rk−1 + αk−1pT
k−1Apk−1  0,
by the deﬁnition (5.6) of αk−1. Meanwhile, for the other vectors pi, i  0, 1, . . . , k −2, we
have
pT
i rk  pT
i rk−1 + αk−1pT
i Apk−1  0

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
107
by the induction hypothesis and the conjugacy of the pi. We conclude that rT
k pi  0, for
i  0, 1, . . . , k −1, so that the proof is complete.
□
The fact that the current residual rk is orthogonal to all previous search directions, as
expressed in (5.10), is a property that will be used extensively in this chapter.
The discussion so far has been general, in that it applies to a conjugate direction
method (5.5), (5.6) based on any choice of the conjugate direction set {p0, p1, . . . , pn−1}.
There are many ways to choose the set of conjugate directions. For instance, the eigenvectors
v1, v2, . . . , vn of A are mutually orthogonal as well as conjugate with respect to A, so these
could be used as the vectors {p0, p1, . . . , pn−1}. For large-scale applications, however, it is
not practical to compute the complete set of eigenvectors, for this requires a large amount
of computation. An alternative is to modify the Gram–Schmidt orthogonalization process
to produce a set of conjugate directions rather than a set of orthogonal directions. (This
modiﬁcation is easy to produce, since the properties of conjugacy and orthogonality are
closely related in spirit.) This approach is also expensive, since it requires us to store the
entire direction set.
BASIC PROPERTIES OF THE CONJUGATE GRADIENT METHOD
The conjugate gradient method is a conjugate direction method with a very special
property: In generating its set of conjugate vectors, it can compute a new vector pk by
using only the previous vector pk−1. It does not need to know all the previous elements
p0, p1, . . . , pk−2 of the conjugate set; pk is automatically conjugate to these vectors. This
remarkable property implies that the method requires little storage and computation.
Now for the details of the conjugate gradient method. Each direction pk is chosen to
be a linear combination of the steepest descent direction −∇φ(xk) (which is the same as the
negative residual −rk, by (5.3)) and the previous direction pk−1. We write
pk  −rk + βkpk−1,
(5.12)
where the scalar βk is to be determined by the requirement that pk−1 and pk must be
conjugate with respect to A. By premultiplying (5.12) by pT
k−1A and imposing the condition
pT
k−1Apk  0, we ﬁnd that
βk 
rT
k Apk−1
pT
k−1Apk−1
.
It makes intuitive sense to choose the ﬁrst search direction p0 to be the steepest descent
direction at the initial point x0. As in the general conjugate direction method, we perform
successive one-dimensional minimizations along each of the search directions. We have thus
speciﬁed a complete algorithm, which we express formally as follows:

108
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
Algorithm 5.1 (CG–Preliminary Version).
Given x0;
Set r0 ←Ax0 −b, p0 ←−r0, k ←0;
while rk ̸ 0
αk ←−rT
k pk
pT
k Apk
;
(5.13a)
xk+1 ←xk + αkpk;
(5.13b)
rk+1 ←Axk+1 −b;
(5.13c)
βk+1 ←rT
k+1Apk
pT
k Apk
;
(5.13d)
pk+1 ←−rk+1 + βk+1pk;
(5.13e)
k ←k + 1;
(5.13f)
end (while)
Later, we will present a more efﬁcient version of the conjugate gradient method; the
version above is useful for studying the essential properties of the method. We show ﬁrst
that the directions p0, p1, . . . , pn−1 are indeed conjugate, which by Theorem 5.1 implies
termination in n steps. The theorem below establishes this property and two other important
properties. First, the residuals ri are mutually orthogonal. Second, each search direction pk
and residual rk is contained in the Krylov subspace of degree k for r0, deﬁned as
K(r0; k)
def span{r0, Ar0, . . . , Akr0}.
(5.14)
Theorem 5.3.
Suppose that the kth iterate generated by the conjugate gradient method is not the solution
point x∗. The following four properties hold:
rT
k ri  0,
for i  0, . . . , k −1,
(5.15)
span {r0, r1, . . . , rk}  span {r0, Ar0, . . . , Akr0},
(5.16)
span {p0, p1, . . . , pk}  span {r0, Ar0, . . . , Akr0},
(5.17)
pT
k Api  0,
for i  0, 1, . . . , k −1.
(5.18)
Therefore, the sequence {xk} converges to x∗in at most n steps.
Proof.
The proof is by induction. The expressions (5.16) and (5.17) hold trivially for k  0,
while (5.18) holds by construction for k  1. Assuming now that these three expressions are
true for some k (the induction hypothesis), we show that they continue to hold for k + 1.

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
109
To prove (5.16), we show ﬁrst that the set on the left-hand side is contained in the set
on the right-hand side. Because of the induction hypothesis, we have from (5.16) and (5.17)
that
rk ∈span {r0, Ar0, . . . , Akr0},
pk ∈span {r0, Ar0, . . . , Akr0},
while by multiplying the second of these expressions by A, we obtain
Apk ∈span {Ar0, . . . , Ak+1r0}.
(5.19)
By applying (5.9), we ﬁnd that
rk+1 ∈span {r0, Ar0, . . . , Ak+1r0}.
By combining this expression with the induction hypothesis for (5.16), we conclude that
span {r0, r1, . . . , rk, rk+1} ∈span {r0, Ar0, . . . , Ak+1r0}.
To prove that the reverse inclusion holds as well, we use the induction hypothesis on (5.17)
to deduce that
Ak+1r0  A(Akr0) ∈span {Ap0, Ap1, . . . , Apk}.
Since by (5.9) we have Api  (ri+1 −ri)/αi for i  0, 1, . . . , k, it follows that
Ak+1r0 ∈span {r0, r1, . . . , rk+1}.
By combining this expression with the induction hypothesis for (5.16), we ﬁnd that
span {r0, Ar0, . . . , Ak+1r0} ⊂span {r0, r1, . . . , rk, rk+1}.
Therefore, the relation (5.16) continues to hold when k is replaced by k + 1, as claimed.
We show that (5.17) continues to hold when k is replaced by k + 1 by the following
argument:
span{p0, p1, . . . , pk, pk+1}
 span{p0, p1, . . . , pk, rk+1}
by (5.13e)
 span{r0, Ar0, . . . , Akr0, rk+1}
by induction hypothesis for (5.17)
 span{r0, r1, . . . , rk, rk+1}
by (5.16)
 span{r0, Ar0, . . . , Ak+1r0}
by (5.16) for k + 1.

110
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
Next, we prove the conjugacy condition (5.18) with k replaced by k+1. By multiplying
(5.13e) by Api, i  0, 1, . . . , k, we obtain
pT
k+1Api  −rT
k+1Api + βk+1pT
k Api.
(5.20)
By the deﬁnition (5.13d) of βk, the right-hand-side of (5.20) vanishes when i  k. For
i ≤k −1 we need to collect a number of observations. Note ﬁrst that our induction
hypothesis for (5.18) implies that the directions p0, p1, . . . , pk are conjugate, so we can
apply Theorem 5.2 to deduce that
rT
k+1pi  0,
for i  0, 1, . . . , k.
(5.21)
Second, by repeatedly applying (5.17), we ﬁnd that for i  0, 1, . . . , k −1, the following
inclusion holds:
Api ∈A span{r0, Ar0, . . . , Air0}  span{Ar0, A2r0, . . . , Ai+1r0}
⊂span{p0, p1, . . . , pi+1}.
(5.22)
By combining (5.21) and (5.22), we deduce that
rT
k+1Api  0,
for i  0, 1, . . . , k −1,
so the ﬁrst term in the right-hand-side of (5.20) vanishes for i  0, 1, . . . , k −1. Because of
the induction hypothesis for (5.18), the second term vanishes as well, and we conclude that
pT
k+1Api  0, i  0, 1, . . . , k. Hence, the induction argument holds for (5.18) also.
It follows that the direction set generated by the conjugate gradient method is indeed
a conjugate direction set, so Theorem 5.1 tells us that the algorithm terminates in at most n
iterations.
Finally, we prove (5.15) by a noninductive argument. Because the direction set is
conjugate, we have from (5.10) that rT
k pi  0 for all i  0, 1, . . . , k −1 and any k 
1, 2, . . . , n −1. By rearranging (5.13e), we ﬁnd that
pi  −ri + βipi−1,
so that ri ∈span{pi, pi−1} for all i  1, . . . , k −1. We conclude that rT
k ri  0 for all
i  1, . . . , k −1, as claimed.
□
The proof of this theorem relies on the fact that the ﬁrst direction p0 is the steepest
descent direction −r0; in fact, the result does not hold for other choices of p0. Since the
gradients rk are mutually orthogonal, the term “conjugate gradient method” is actually a
misnomer. It is the search directions, not the gradients, that are conjugate with respect to A.

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
111
A PRACTICAL FORM OF THE CONJUGATE GRADIENT METHOD
We can derive a slightly more economical form of the conjugate gradient method by
using the results of Theorems 5.2 and 5.3. First, we can use (5.13e) and (5.10) to replace the
formula (5.13a) for αk by
αk 
rT
k rk
pT
k Apk
.
Second, we have from (5.9) that αkApk  rk+1 −rk, so by applying (5.13e) and (5.10) once
again we can simplify the formula for βk+1 to
βk+1  rT
k+1rk+1
rT
k rk
.
By using these formulae together with (5.9), we obtain the following standard form of the
conjugate gradient method.
Algorithm 5.2 (CG).
Given x0;
Set r0 ←Ax0 −b, p0 ←−r0, k ←0;
while rk ̸ 0
αk ←
rT
k rk
pT
k Apk
;
(5.23a)
xk+1 ←xk + αkpk;
(5.23b)
rk+1 ←rk + αkApk;
(5.23c)
βk+1 ←rT
k+1rk+1
rT
k rk
;
(5.23d)
pk+1 ←−rk+1 + βk+1pk;
(5.23e)
k ←k + 1;
(5.23f)
end (while)
At any given point in Algorithm 5.2 we never need to know the vectors x, r, and
p for more than the last two iterations. Accordingly, implementations of this algorithm
overwrite old values of these vectors to save on storage. The major computational tasks to be
performed at each step are computation of the matrix–vector product Apk, calculation of
the inner products pT
k (Apk) and rT
k+1rk+1, and calculation of three vector sums. The inner
product and vector sum operations can be performed in a small multiple of n ﬂoating-point

112
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
operations, while the cost of the matrix–vector product is, of course, dependent on the
problem. The CG method is recommended only for large problems; otherwise, Gaussian
elimination or other factorization algorithms such as the singular value decomposition are
to be preferred, since they are less sensitive to rounding errors. For large problems, the CG
methodhastheadvantagethatitdoesnotalterthecoefﬁcientmatrix,andunlikefactorization
techniques, cannot produce ﬁll in the arrays holding the matrix. The other key property is
that the CG method sometimes approaches the solution very quickly, as we discuss next.
RATE OF CONVERGENCE
We have seen that in exact arithmetic the conjugate gradient method will terminate at
the solution in at most n iterations. What is more remarkable is that when the distribution
of the eigenvalues of A has certain favorable features, the algorithm will identify the solution
in many fewer than n iterations. To show this we begin by viewing the expanding subspace
minimization property proved in Theorem 5.2 in a slightly different way, using it to show
that Algorithm 5.2 is optimal in a certain important sense.
From (5.23b) and (5.17), we have that
xk+1  x0 + α0p0 + · · · + αkpk
 x0 + γ0r0 + γ1Ar0 + · · · + γkAkr0,
(5.24)
for some constants γi. We now deﬁne P ∗
k (·) to be a polynomial of degree k with coefﬁcients
γ0, γ1, . . . , γk. Like any polynomial, P ∗
k can take either a scalar or a square matrix as its
argument; for a matrix argument A, we have
P ∗
k (A)  γ0I + γ1A + · · · + γkAk.
We can now write (5.24) as
xk+1  x0 + P ∗
k (A)r0.
(5.25)
We will now see that among all possible methods whose ﬁrst k steps are restricted to
the Krylov subspace K(r0; k) given by (5.14), Algorithm 5.2 does the best job of minimizing
the distance to the solution after k steps, when this distance is measured by the weighted
norm measure ∥· ∥A deﬁned by
∥z∥2
A  zT Az.
(5.26)
(Recall that this norm was used in the analysis of the steepest descent method of Chapter 3.)
Using this norm and the deﬁnition of φ (5.2), it is easy to show that
1
2∥x −x∗∥2
A  1
2(x −x∗)T A(x −x∗)  φ(x) −φ(x∗).
(5.27)

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
113
Theorem 5.2 states that xk+1 minimizes φ, and hence ∥x −x∗∥2
A, over the set x0 +
span{p0, p1, . . . , pk}. It follows from (5.25) that the polynomial P ∗
k solves the following
problem in which the minimum is taken over the space of all possible polynomials of degree
k:
min
Pk ∥x0 + Pk(A)r0 −x∗∥A.
(5.28)
We exploit this optimality property repeatedly in the remainder of the section.
Since
r0  Ax0 −b  Ax0 −Ax∗ A(x0 −x∗),
we have that
xk+1 −x∗ x0 + P ∗
k (A)r0 −x∗ [I + P ∗
k (A)A](x0 −x∗).
(5.29)
Let 0 < λ1 ≤λ2 ≤· · · ≤λn be the eigenvalues of A, and let v1, v2, . . . , vn be the
corresponding orthonormal eigenvectors. Since these eigenvectors span the whole space IRn,
we can write
x0 −x∗
n

i1
ξivi,
(5.30)
for some coefﬁcients ξi. It is easy to show that any eigenvector of A is also an eigenvector
of Pk(A) for any polynomial Pk. For our particular matrix A and its eigenvalues λi and
eigenvectors vi, we have
Pk(A)vi  Pk(λi)vi,
i  1, 2, . . . , n.
By substituting (5.30) into (5.29) we have
xk+1 −x∗
n

i1
[1 + λiP ∗
k (λi)]ξivi,
and hence
∥xk+1 −x∗∥2
A 
n

i1
λi[1 + λiP ∗
k (λi)]2ξ 2
i .
(5.31)

114
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
Since the polynomial P ∗
k generated by the CG method is optimal with respect to this norm,
we have
∥xk+1 −x∗∥2
A  min
Pk
n

i1
λi[1 + λiP ∗
k (λi)]2ξ 2
i .
By extracting the largest of the terms [1 + λiPk(λi)]2 from this expression, we obtain that
∥xk+1 −x∗∥2
A ≤min
Pk
max
1≤i≤n[1 + λiPk(λi)]2
# n

j1
λjξ 2
j
$
 min
Pk
max
1≤i≤n[1 + λiPk(λi)]2∥x0 −x∗∥2
A,
(5.32)
where we have used the fact that ∥x0 −x∗∥2
A  n
j1 λjξ 2
j .
The expression (5.32) allows us to quantify the convergence rate of the CG method by
estimating the nonnegative scalar quantity
min
Pk
max
1≤i≤n[1 + λiPk(λi)]2.
(5.33)
In other words, we search for a polynomial Pk that makes this expression as small as possible.
In some practical cases, we can ﬁnd this polynomial explicitly and draw some interesting
conclusions about the properties of the CG method. The following result is an example.
Theorem 5.4.
If A has only r distinct eigenvalues, then the CG iteration will terminate at the solution
in at most r iterations.
Proof.
Suppose that the eigenvalues λ1, λ2, . . . , λn take on the r distinct values τ1 < τ2 <
· · · < τr. We deﬁne a polynomial Qr(λ) by
Qr(λ) 
(−1)r
τ1τ2 · · · τr
(λ −τ1)(λ −τ2) · · · (λ −τr),
and note that Qr(λi)  0 for i  1, 2, . . . , n and Qr(0)  1. From the latter observation,
we deduce that Qr(λ) −1 is a polynomial of degree r with a root at λ  0, so by polynomial
division, the function ¯Pr−1 deﬁned by
¯Pr−1(λ)  (Qr(λ) −1)/λ
is a polynomial of degree r −1. By setting k  r −1 in (5.33), we have
0 ≤min
Pr−1 max
1≤i≤n[1 + λiPr−1(λi)]2 ≤max
1≤i≤n[1 + λi ¯Pr−1(λi)]2  max
1≤i≤n Qr(λi)  0.

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
115
Hence the constant in (5.33) is zero for the value k  r −1, so we have by substituting into
(5.32) that ∥xr −x∗∥2
A  0, and therefore xr  x∗, as claimed.
□
By using similar reasoning, Luenberger [152] establishes the following estimate, which
gives a useful characterization of the behavior of the CG method.
Theorem 5.5.
If A has eigenvalues λ1 ≤λ2 ≤· · · ≤λn, we have that
∥xk+1 −x∗∥2
A ≤
λn−k −λ1
λn−k + λ1
2
∥x0 −x∗∥2
A.
(5.34)
Without giving details of the proof, we describe how this result is obtained from (5.32). One
selects a polynomial ¯Pk of degree k such that the polynomial Qk+1(λ)  1 + λ ¯Pk(λ) has
roots at the k largest eigenvalues λn, λn−1, . . . , λn−k+1, as well as at the midpoint between
λ1 and λn−k. It can be shown that the maximum value attained by Qk+1 on the remaining
eigenvalues λ1, λ2, . . . , λn−k is precisely (λn−k −λ1)/(λn−k + λ1).
We now illustrate how Theorem 5.5 can be used to predict the behavior of the CG
method on speciﬁc problems. Suppose we have the situation plotted in Figure 5.3, where
the eigenvalues of A consist of m large values, with the remaining n −m smaller eigenvalues
clustered around 1.
If we deﬁne ϵ  λn−m −λ1, Theorem 5.5 tells us that after m+1 steps of the conjugate
gradient algorithm, we have
∥xm+1 −x∗∥≈ϵ∥x0 −x∗∥A.
For a small value of ϵ, we conclude that the CG iterates will provide a good estimate of the
solution after only m + 1 steps.
Figure 5.4 shows the behavior of CG on a problem of this type, which has ﬁve large
eigenvalues with all the smaller eigenvalues clustered between 0.95 and 1.05, and compares
this behavior with that of CG on a problem in which the eigenvalues satisfy some random
distribution. In both cases, we plot the log of φ after each iteration.
λ1
λn-m
λn-m +1
λ  n
|
0
1
Figure 5.3
Two clusters of eigenvalues.

116
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
uniformly distributed
eigenvalues
log(||x-x*||  )
A
2
clustered eigenvalues
4
5
6
2
3
1
-5
-10
5
0
7
iteration
Figure5.4
Performance of the conjugate gradient method on (a) a problem in which
ﬁve of the eigenvalues are large and the remainder are clustered near 1, and (b) a matrix
with uniformly distributed eigenvalues.
For the problem with clustered eigenvalues, Theorem 5.5 predicts a sharp decrease in
the error measure at iteration 6. Note, however, that this decrease was achieved one iteration
earlier, illustrating the fact that Theorem 5.5 gives only an upper bound, and that the rate
of convergence can be faster. By contrast, we observe in Figure 5.4 that for the problem with
randomly distributed eigenvalues the convergence rate is slower and more uniform.
Figure 5.4 illustrates another interesting feature: After one more iteration (a total
of seven) on the problem with clustered eigenvalues, the error measure drops sharply. An
extension of the arguments leading to Theorem 5.4 explains this behavior. It is almost true to
say that the matrix A has just six distinct eigenvalues: the ﬁve large eigenvalues and 1. Then
we would expect the error measure to be zero after six iterations. Because the eigenvalues
near 1 are slightly spread out, however, the error does not become very small until the next
iteration, i.e. iteration 7.
To state this more precisely, it is generally true that if the eigenvalues occur in r distinct
clusters, the CG iterates will approximately solve the problem after r steps (see [115]).
This result can be proved by constructing a polynomial ¯Pr−1 such that (1 + λ ¯Pr−1(λ))
has zeros inside each of the clusters. This polynomial may not vanish at the eigenvalues
λi, i  1, 2, . . . , n, but its value will be small at these points, so the constant deﬁned in
(5.33) will be tiny for k ≥r −1. We illustrate this behavior in Figure 5.5, which shows the

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
117
log(||x-x*||  )
A
2
1
2
3
4
6
5
0
5
-5
-10
iteration
7
Figure 5.5
Performance of the conjugate gradient method on a matrix in which the
eigenvalues occur in four distinct clusters.
performance of CG on a matrix of dimension n  14 that has four clusters of eigenvalues:
single eigenvalues at 140 and 120, a cluster of 10 eigenvalues very close to 10, with the
remaining eigenvalues clustered between 0.95 and 1.05. After four iterations, the error norm
is quite small. After six iterations, the solution is identiﬁed to good accuracy.
Another, more approximate, convergence expression for CG is based on the Euclidean
condition number of A, which is deﬁned by
κ(A)  ∥A∥2∥A−1∥2  λ1/λn.
It can be shown that
∥xk −x∗∥A ≤
√κ(A) −1
√κ(A) + 1
2k
∥x0 −x∗∥A.
(5.35)
This bound often gives a large overestimate of the error, but it can be useful in those cases
where the only information we have about A is estimates of the extreme eigenvalues λ1 and
λn. This bound should be compared with that of the steepest descent method given by (3.29),
which is identical in form but which depends on the condition number κ(A), and not on
its square root √κ(A).

118
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
PRECONDITIONING
We can accelerate the conjugate gradient method by transforming the linear system
to improve the eigenvalue distribution of A. The key to this process, which is known as
preconditioning, is a change of variables from x to ˆx via a nonsingular matrix C, that is,
ˆx  Cx.
(5.36)
The quadratic φ deﬁned by (5.2) is transformed accordingly to
ˆφ(ˆx)  1
2 ˆxT (C−T AC−1)ˆx −(C−T b)T ˆx.
(5.37)
If we use Algorithm 5.2 to minimize ˆφ or, equivalently, to solve the linear system
(C−T AC−1)ˆx  C−T b,
then the convergence rate will depend on the eigenvalues of the matrix C−T AC−1 rather
than those of A. Therefore, we aim to choose C such that the eigenvalues of C−T AC−1 are
more favorable for the convergence theory discussed above. We can try to choose C such that
the condition number of C−T AC−1 is much smaller than the original condition number
of A, for instance, so that the constant in (5.35) is smaller. We could also try to choose C
such that the eigenvalues of C−T AC−1 are clustered, which by the discussion of the previous
section ensures that the number of iterates needed to ﬁnd a good approximate solution is
not much larger than the number of clusters.
It is not necessary to carry out the transformation (5.36) explicitly. Rather, we can
apply Algorithm 5.2 to the problem (5.37), in terms of the variables ˆx, and then invert the
transformations to reexpress all the equations in terms of x. This process of derivation results
in Algorithm 5.3 (preconditioned conjugate gradient), which we now deﬁne. It happens that
Algorithm 5.3 does not make use of C explicitly, but rather the matrix M  CT C, which is
symmetric and positive deﬁnite by construction.
Algorithm 5.3 (Preconditioned CG).
Given x0, preconditioner M;
Set r0 ←Ax0 −b;
Solve My0  r0 for y0;
Set p0  −r0, k ←0;
while rk ̸ 0
αk ←
rT
k yk
pT
k Apk
;
(5.38a)
xk+1 ←xk + αkpk;
(5.38b)

5 . 1 .
T h e L i n e a r C o n j u g a t e G r a d i e n t M e t h o d
119
rk+1 ←rk + αkApk;
(5.38c)
Myk+1 ←rk+1;
(5.38d)
βk+1 ←rT
k+1yk+1
rT
k yk
;
(5.38e)
pk+1 ←−yk+1 + βk+1pk;
(5.38f)
k ←k + 1;
(5.38g)
end (while)
If we set M  I in Algorithm 5.3, we recover the standard CG method, Algorithm 5.2.
The properties of Algorithm 5.2 generalize to this case in interesting ways. In particular, the
orthogonality property (5.15) of the successive residuals becomes
rT
i M−1rj  0
for all i ̸ j.
(5.39)
In terms of computational effort, the main difference between the preconditioned and
unpreconditioned CG methods is the need to solve systems of the form My  r.
PRACTICAL PRECONDITIONERS
No single preconditioning strategy is “best” for all conceivable types of matrices:
The tradeoff between various objectives—effectiveness of M, inexpensive computation and
storage of M, inexpensive solution of My  r—varies from problem to problem.
Good preconditioning strategies have been devised for speciﬁc types of matrices, in
particular, those arising from discretizations of partial differential equations (PDEs). Often,
the preconditioner is deﬁned in such a way that the system My  r amounts to a simpliﬁed
versionoftheoriginalsystem Ax  b.InthecaseofaPDE,My  r couldrepresentacoarser
discretization of the underlying continuous problem than Ax  b. As in many other areas of
optimization and numerical analysis, knowledge about the structure and origin of a problem
(in this case, knowledge that the system Ax  b is a ﬁnite-dimensional representation of a
PDE) is the key to devising effective techniques for solving the problem.
General-purpose preconditioners have also been proposed, but their success varies
greatly from problem to problem. The most important strategies of this type include sym-
metricsuccessiveoverrelaxation(SSOR),incompleteCholesky,andbandedpreconditioners.
(See [220], [115], and [53] for discussions of these techniques.) Incomplete Cholesky is prob-
ably the most effective in general; we discussed it brieﬂy in Chapter 6. The basic idea is
simple: We follow the Cholesky procedure, but instead of computing the exact Cholesky
factor L that satisﬁes A  LLT , we compute an approximate factor ˜L that is sparser than
L. (Usually, we require ˜L to be no denser, or not much denser, than the lower triangle of the
original matrix A.) We then have A ≈˜L ˜LT , and by choosing C  ˜LT , we obtain M  ˜L ˜LT

120
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
and
C−T AC−1  ˜L−1A ˜L−T ≈I,
so the eigenvalue distribution of C−T AC−1 is favorable. We do not compute M explicitly,
but rather store the factor ˜L and solve the system My  r by performing two triangular
substitutions with ˜L. Because the sparsity of ˜L is similar to that of A, the cost of solving
My  r is similar to the cost of computing the matrix–vector product Ap.
There are several possible pitfalls in the incomplete Cholesky approach. One is that
the resulting matrix may not be (sufﬁciently) positive deﬁnite, and in this case one may need
to increase the values of the diagonal elements to ensure that a value for ˜L can be found.
Numerical instability or breakdown can occur during the incomplete factorization because
of the sparsity conditions we impose on the factor ˜L. This difﬁculty can be remedied by
allowing additional ﬁll-in in ˜L, but the denser factor will be more expensive to compute and
to apply at each iteration.
5.2
NONLINEAR CONJUGATE GRADIENT METHODS
We have noted that the CG method, Algorithm 5.2, can be viewed as a minimization algo-
rithm for the convex quadratic function φ deﬁned by (5.2). It is natural to ask whether we
can adapt the approach to minimize general convex functions, or even general nonlinear
functions f .
THE FLETCHER–REEVES METHOD
Fletcher and Reeves [88] showed that an extension of this kind is possible by making
two simple changes in Algorithm 5.2. First, in place of the choice (5.23a) for the step length
αk (which minimizes φ along the search direction pk), we need to perform a line search
that identiﬁes an approximate minimum of the nonlinear function f along pk. Second,
the residual r, which is simply the gradient of φ in Algorithm 5.2, must be replaced by the
gradient of the nonlinear objective f . These changes give rise to the following algorithm for
nonlinear optimization.
Algorithm 5.4 (FR-CG).
Given x0;
Evaluate f0  f (x0), ∇f0  ∇f (x0);
Set p0  −∇f0, k ←0;
while ∇fk ̸ 0
Compute αk and set xk+1  xk + αkpk;
Evaluate ∇fk+1;

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
121
βFR
k+1 ←∇f T
k+1∇fk+1
∇f T
k ∇fk
;
(5.40a)
pk+1 ←−∇fk+1 + βFR
k+1pk;
(5.40b)
k ←k + 1;
(5.40c)
end (while)
If we choose f to be a strongly convex quadratic and αk to be the exact minimizer, this
algorithm reduces to the linear conjugate gradient method, Algorithm 5.2. Algorithm 5.4
is appealing for large nonlinear optimization problems because each iteration requires only
evaluation of the objective function and its gradient. No matrix operations are performed,
and just a few vectors of storage are required.
To make the speciﬁcation of Algorithm 5.4 complete, we need to be more precise about
the choice of line search parameter αk. Because of the second term in (5.40b), the search
direction pk may fail to be a descent direction unless αk satisﬁes certain conditions. By taking
the inner product of (5.40b) (with k replacing k+1) with the gradient vector ∇fk, we obtain
∇f T
k pk  −∥∇fk∥2 + βFR
k ∇f T
k pk−1.
(5.41)
If the line search is exact, so that αk−1 is a local minimizer of f along the direction pk−1,
we have that ∇f T
k pk−1  0. In this case we have from (5.41) that ∇f T
k pk < 0, so that pk
is indeed a descent direction. But if the line search is not exact, the second term in (5.41)
may dominate the ﬁrst term, and we may have ∇f T
k pk > 0, implying that pk is actually a
direction of ascent. Fortunately, we can avoid this situation by requiring the step length αk
to satisfy the strong Wolfe conditions, which we restate here:
f (xk + αkpk) ≤f (xk) + c1αk∇f T
k pk,
(5.42a)
|∇f (xk + αkpk)T pk| ≤c2|∇f T
k pk|,
(5.42b)
where 0 < c1 < c2 < 1
2. (Note that we impose c2 < 1
2 here, in place of the looser condition
c2 < 1 that was used in the earlier statement (3.7).) By applying Lemma 5.6 below, we can
show that condition (5.42b) implies that (5.41) is negative, and we conclude that any line
search procedure that yields an αk satisfying (5.42) will ensure that all directions pk are
descent directions for the function f .
THE POLAK–RIBI`ERE METHOD
There are many variants of the Fletcher–Reeves method that differ from each other
mainly in the choice of the parameter βk. The most important of these variants, proposed

122
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
by Polak and Ribi`ere, deﬁnes this parameter as follows:
βPR
k+1  ∇f T
k+1(∇fk+1 −∇fk)
∥∇fk∥2
.
(5.43)
We refer to the algorithm in which (5.43) replaces (5.40a) as Algorithm PR-CG, and refer to
Algorithm 5.4 as Algorithm FR-CG. They are identical when f is a strongly convex quadratic
function and the line search is exact, since by (5.15) the gradients are mutually orthogonal,
and so βPR
k+1  βFR
k+1. When applied to general nonlinear functions with inexact line searches,
however,thebehaviorofthetwoalgorithmsdiffersmarkedly.Numericalexperienceindicates
that Algorithm PR-CG tends to be the more robust and efﬁcient of the two.
A surprising fact about Algorithm PR-CG is that the strong Wolfe conditions (5.42)
do not guarantee that pk is always a descent direction. If we deﬁne the β parameter as
β+
k+1  max{βPR
k+1, 0},
(5.44)
giving rise to an algorithm we call Algorithm PR+, then a simple adaptation of the strong
Wolfe conditions ensures that the descent property holds.
There are many other choices for βk+1 that coincide with the Fletcher–Reeves formula
βFR
k+1 in the case where the objective is quadratic and the line search is exact. The Hestenes–
Stiefel formula, which deﬁnes
βHS
k+1  ∇f T
k+1(∇fk+1 −∇fk)
(∇fk+1 −∇fk)T pk
,
(5.45)
gives rise to an algorithm that is similar to Algorithm PR-CG, both in terms of its theoretical
convergence properties and in its practical performance. Formula (5.45) can be derived
by demanding that consecutive search directions be conjugate with respect to the average
Hessian over the line segment [xk, xk+1], which is deﬁned as
¯Gk ≡
 1
0
[∇2f (xk + ταkdk)]dτ.
Recalling from Taylor’s theorem (2.5) that ∇fk+1  ∇fk + αk ¯Gkpk, we see that for any
direction of the form pk+1  −∇fk+1 + βk+1pk, the condition pT
k+1 ¯Gkpk  0 requires
βk+1 to be given by (5.45).
None of the other proposed deﬁnitions of βk has proved to be signiﬁcantly more
efﬁcient than the Polak–Ribi`ere choice (5.43).
QUADRATIC TERMINATION AND RESTARTS
Implementations of nonlinear conjugate gradient methods usually preserve their close
connections with the linear conjugate gradient method. Usually, a quadratic (or cubic)

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
123
interpolation along the search direction pk is incorporated into the line search procedure; see
Chapter 3. This feature guarantees that when f is a strictly convex quadratic, the step length
αk is chosen to be the exact one-dimensional minimizer, so that the nonlinear conjugate
gradient method reduces to the linear method, Algorithm 5.2.
Another modiﬁcation that is often used in nonlinear conjugate gradient procedures
is to restart the iteration at every n steps by setting βk  0 in (5.40a), that is, by taking
a steepest descent step. Restarting serves to periodically refresh the algorithm, erasing old
information that may not be beneﬁcial. We can even prove a strong theoretical result about
restarting: It leads to n-step quadratic convergence, that is,
∥xk+n −x∥ O
	
∥xk −x∗∥2
.
(5.46)
With a little thought, we can see that this result is not so surprising. Consider a function
f that is strongly convex quadratic in a neighborhood of the solution, but is nonquadratic
everywhere else. Assuming that the algorithm is converging to the solution in question,
the iterates will eventually enter the quadratic region. At some point, the algorithm will be
restarted in that region, and from that point onward, its behavior will simply be that of
the linear conjugate gradient method, Algorithm 5.2. In particular, ﬁnite termination will
occur within n steps of the restart. The restart is important, because the ﬁnite-termination
property (and other appealing properties) of Algorithm 5.2 holds only when its initial search
direction p0 is equal to the negative gradient.
Even if the function f is not exactly quadratic in the region of a solution, Taylor’s
theorem (2.6) implies that it can still be approximated quite closely by a quadratic, provided
that it is smooth. Therefore, while we would not expect termination in n steps after the
restart, it is not surprising that substantial progress is made toward the solution, as indicated
by the expression (5.46).
Though the result (5.46) is interesting from a theoretical viewpoint, it may not be
relevant in a practical context, because nonlinear conjugate gradient methods can be rec-
ommended only for solving problems with large n. In such problems restarts may never
occur, since an approximate solution is often located in fewer than n steps. Hence, nonlinear
CG method are sometimes implemented without restarts, or else they include strategies for
restarting that are based on considerations other than iteration counts. The most popular
restart strategy makes use of the observation (5.15), which is that the gradients are mutually
orthogonal when f is a quadratic function. A restart is performed whenever two consecutive
gradients are far from orthogonal, as measured by the test
|∇f T
k ∇fk−1|
∥∇fk∥2
≥ν,
(5.47)
where a typical value for the parameter ν is 0.1.
Another modiﬁcation to the restart strategy is to use a direction other than steepest
descent as the restart direction. The Harwell subroutine VA14 [133], for instance, deﬁnes

124
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
pk+1 by using a three-term recurrence based on ∇fk+1, pk and a third direction that contains
earlier information about the behavior of the objective function. An algorithm that takes
this idea one step further is CONMIN, which is discussed in Chapter 9.
We could also think of formula (5.44) as a restarting strategy, because pk+1 will revert
to the steepest descent direction whenever βPR
k is negative. In contrast to (5.47), however,
these restarts are rather infrequent because βPR
k is positive most of the time.
NUMERICAL PERFORMANCE
Table 5.1 illustrates the performance of Algorithms FR-CG, PR-CG, and PR+ without
restarts. For these tests, the parameters in the strong Wolfe conditions (5.42) were chosen to
be c1  10−4 and c2  0.1. The iterations were terminated when
∥∇fk∥∞< 10−5(1 + |fk|),
or after 10,000 iterations (the latter is denoted by a ∗).
The ﬁnal column, headed “mod,” indicates the number of iterations of Algorithm
PR+ for which the adjustment (5.44) was needed to ensure that βPR
k ≥0. An examination
of the results of Algorithm FR-CG on problem GENROS shows that the method takes very
short steps far from the solution that lead to tiny improvements in the objective function.
The Polak–Ribi`ere algorithm, or its variation PR+, are not always more efﬁcient than
Algorithm FR-CG, and it has the slight disadvantage of requiring one more vector of storage.
Nevertheless,werecommendthatuserschooseAlgorithmPR-CGorPR+wheneverpossible.
BEHAVIOR OF THE FLETCHER–REEVES METHOD
We now investigate the Fletcher–Reeves algorithm, Algorithm 5.4, a little more closely,
proving that it is globally convergent and explaining some of its observed inefﬁciencies.
The following result gives conditions on the line search that guarantee that all search
directions are descent directions. It assumes that the level set L  {x : f (x) ≤f (x0)} is
Table 5.1
Iterations and function/gradient evaluations required by three
nonlinear conjugate gradient methods on a set of test problems.
Alg FR
Alg PR
Alg PR+
Problem
n
it/f-g
it/f-g
it/f-g
mod
CALCVAR3
200
2808/5617
2631/5263
2631/5263
0
GENROS
500
∗
1068/2151
1067/2149
1
XPOWSING
1000
533/1102
212/473
97/229
3
TRIDIA1
1000
264/531
262/527
262/527
0
MSQRT1
1000
422/849
113/231
113/231
0
XPOWELL
1000
568/1175
212/473
97/229
3
TRIGON
1000
231/467
40/92
40/92
0

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
125
bounded, and that f is twice continuously differentiable, so that we have from Lemma 3.1
that there exists a step length αk that satisﬁes the strong Wolfe conditions.
Lemma 5.6.
Suppose that Algorithm 5.4 is implemented with a step length αk that satisﬁes the strong
Wolfe conditions (5.42) with 0 < c2 < 1
2. Then the method generates descent directions pk that
satisfy the following inequalities:
−
1
1 −c2
≤∇f T
k pk
∥∇fk∥2 ≤2c2 −1
1 −c2
,
for all k  0, 1, . . . .
(5.48)
Proof.
Note ﬁrst that the function t(ξ)
def (2ξ −1)(1 −ξ) is monotonically increasing on
the interval [0, 1
2] and that t(0)  −1 and t( 1
2)  0. Hence, because of c2 ∈(0, 1
2), we have
−1 < 2c2 −1
1 −c2
< 0.
(5.49)
The descent condition ∇f T
k pk < 0 follows immediately once we establish (5.48).
The proof is by induction. For k  0, the middle term in (5.48) is −1, so by using
(5.49), we see that both inequalities in (5.48) are satisﬁed. Next, assume that (5.48) holds
for some k ≥1. From (5.40b) and (5.40a) we have
∇f T
k+1pk+1
∥∇fk+1∥2  −1 + βk+1
∇f T
k+1pk
∥∇fk+1∥2  −1 + ∇f T
k+1pk
∥∇fk∥2 .
(5.50)
By using the line search condition (5.42b), we have
|∇f T
k+1pk| ≤−c2∇f T
k pk,
so by combining with (5.50), we obtain
−1 + c2
∇f T
k pk
∥∇fk∥2 ≤∇f T
k+1pk+1
∥∇fk+1∥2 ≤−1 −c2
∇f T
k pk
∥∇fk∥2 .
Substituting for the term ∇f T
k pk/∥∇fk∥2 from the left-hand-side of the induction
hypothesis (5.48), we obtain
−1 −
c2
1 −c2
≤∇f T
k+1pk+1
∥∇fk+1∥2 ≤−1 +
c2
1 −c2
,
which shows that (5.48) holds for k + 1 as well.
□

126
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
This result used only the second strong Wolfe condition (5.42b); the ﬁrst Wolfe condi-
tion (5.42a) will be needed in the next section to establish global convergence. The bounds
on ∇f T
k pk in (5.48) impose a limit on how fast the norms of the steps ∥pk∥can grow, and
they will play a crucial role in the convergence analysis given below.
Lemma 5.6 can also be used to explain a weakness of the Fletcher–Reeves method. We
will argue that if the method generates a bad direction and a tiny step, then the next direction
and next step are also likely to be poor. As in Chapter 3, we let θk denote the angle between
pk and the steepest descent direction −∇fk, deﬁned by
cos θk 
−∇f T
k pk
∥∇fk∥∥pk∥.
(5.51)
Suppose that pk is a poor search direction, in the sense that it makes an angle of nearly 90◦
with −∇fk, that is, cos θk ≈0. By multiplying both sides of (5.48) by ∥∇fk∥/∥pk∥and
using (5.51), we obtain
χ1
∥∇fk∥
∥pk∥≤cos θk ≤χ2
∥∇fk∥
∥pk∥,
for all k  0, 1, . . .,
(5.52)
where χ1 and χ2 are two positive constants. From the right-hand inequality we can have
cos θk ≈0 if and only if
∥∇fk∥≪∥pk∥.
Since pk is almost orthogonal to the gradient, it is likely that the step from xk to xk+1 is tiny,
that is, xk+1 ≈xk. If so, we have ∇fk+1 ≈∇fk, and therefore
βFR
k+1 ≈1,
(5.53)
by the deﬁnition (5.40a). By using this approximation together with ∥∇fk+1∥≈∥∇fk∥≪
∥pk∥in (5.40b), we conclude that
pk+1 ≈pk,
so the new search direction will improve little (if at all) on the previous one. It follows that
if the condition cos θk ≈0 holds at some iteration k and if the subsequent step is small, a
long sequence of unproductive iterates will follow.
The Polak–Ribi`ere method behaves quite differently in these circumstances. If, as in
the previous paragraph, the search direction pk satisﬁes cos θk ≈0 for some k, and if the
subsequent step is small, it follows by substituting ∇fk ≈∇fk+1 into (5.43) that βPR
k+1 ≈0.
From the formula (5.40b), we ﬁnd that the new search direction pk+1 will be close to the
steepest descent direction −∇fk+1, and cos θk+1 will be close to 1. Therefore, Algorithm

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
127
PR-CG essentially performs a restart after it encounters a bad direction. The same argument
can be applied to Algorithms PR+ and HS-CG.
The inefﬁcient behavior of the Fletcher–Reeves method predicted by the arguments
given above can be observed in practice. For example, the paper [103] describes a problem
with n  100 in which cos θk is of order 10−2 for hundreds of iterations, and the steps
∥xk−xk−1∥are of order 10−2. Algorithm FR-CG requires thousands of iterations to solve this
problem, while Algorithm PR-CG requires just 37 iterations. In this example, the Fletcher–
Reeves method performs much better if it is periodically restarted along the steepest descent
direction, since each restart terminates the cycle of bad steps. In general, Algorithm FR-CG
should not be implemented without some kind of restart strategy.
GLOBAL CONVERGENCE
Unlike the linear conjugate gradient method, whose convergence properties are well
understood and which is known to be optimal as described above, nonlinear conjugate
gradient methods possess surprising, sometimes bizarre, convergence properties. The theory
developed in the literature offers fascinating glimpses into their behavior, but our knowledge
remainsfragmentary.WenowpresentafewofthemainresultsknownfortheFletcher–Reeves
and Polak–Ribi`ere methods using practical line searches.
For the purposes of this section, we make the following (nonrestrictive) assumptions
on the objective function.
Assumption 5.1.
(i) The level set L : {x : f (x) ≤f (x0)} is bounded.
(ii) In some neighborhood N of L, the objective function f is Lipschitz continuously
differentiable, that is, there exists a constant L > 0 such that
∥∇f (x) −∇f (˜x)∥≤L∥x −˜x∥,
for all x, ˜x ∈N.
(5.54)
This assumption implies that there is a constant ¯γ such that
∥∇f (x)∥≤¯γ , for all x ∈L.
(5.55)
Our main analytical tool in this section is Zoutendijk’s theorem—Theorem 3.2 in
Chapter 3—which we restate here for convenience.
Theorem 5.7.
Suppose that Assumptions 5.1 hold. Consider any line search iteration of the form xk+1 
xk + αkpk, where pk is a descent direction and αk satisﬁes the Wolfe conditions (5.42). Then
∞

k1
cos2 θk ∥∇fk∥2 < ∞.
(5.56)

128
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
We can use this theorem to prove global convergence for algorithms that are periodi-
cally restarted by setting βk  0. If k1, k2, and so on denote the iterations on which restarts
occur, we have from (5.56) that

kk1,k2,...
∥∇fk∥2 < ∞.
(5.57)
If we allow no more than ¯n iterations between restarts, the sequence {kj}∞
j1 will be inﬁnite,
and from (5.57) we have that limj→∞∥∇fkj ∥ 0. That is, a subsequence of gradients
approaches zero, or equivalently,
lim inf
k→∞∥∇fk∥ 0.
(5.58)
This result applies equally to restarted versions of all the algorithms discussed in this chapter.
Itismoreinteresting,however,tostudytheglobalconvergenceofunrestarted conjugate
gradient methods, because for large problems (say n ≥1000) we expect to ﬁnd a solution in
many fewer than n iterations—the ﬁrst point at which a regular restart would take place. Our
study of large sequences of unrestarted conjugate gradient iterations reveals some surprising
patterns in their behavior.
We can build on Lemma 5.6 and Theorem 5.7 to prove a global convergence result
for the Fletcher–Reeves method. While we cannot show that the limit of the sequence of
gradients {∇fk} is zero (as in the restarted method above), the following result shows that
it is at least not bounded away from zero.
Theorem 5.8.
Suppose that Assumptions 5.1 hold, and that Algorithm 5.4 is implemented with a line
search that satisﬁes the strong Wolfe conditions (5.42), with 0 < c1 < c2 < 1
2. Then
lim inf
k→∞∥∇fk∥ 0.
(5.59)
Proof.
The proof is by contradiction. It assumes that the opposite of (5.59) holds, that is,
there is a constant γ > 0 such that
∥∇fk∥≥γ,
for all k sufﬁciently large,
(5.60)
and uses Lemma 5.6 and Theorem 5.7 to derive the contradiction.
From Lemma 5.6, we have that
cos θk ≥
1
1 −c2
∥∇fk∥
∥pk∥,
k  1, 2, . . . ,
(5.61)

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
129
and by substituting this relation in Zoutendijk’s condition (5.56), we obtain
∞

k1
∥∇fk∥4
∥pk∥2 < ∞.
(5.62)
By using (5.42b) and Lemma 5.6 again, we obtain that
|∇f T
k pk−1| ≤−c2∇f T
k−1pk−1 ≤
c2
1 −c2
∥∇fk−1∥2.
(5.63)
Thus, from (5.40b) and recalling the deﬁnition (5.40a) of βFR
k we obtain
∥pk∥2 ≤∥∇fk∥2 + 2βFR
k |∇f T
k pk−1| + (βFR
k )2∥pk−1∥2
≤∥∇fk∥2 +
2c2
1 −c2
βFR
k ∥∇fk−1∥2 + (βFR
k )2∥pk−1∥2
≤
1 + c2
1 −c2

∥∇fk∥2 + (βFR
k )2∥pk−1∥2.
Applying this relation repeatedly, deﬁning c3
def (1 + c2)/(1 −c2) ≥1, and using the
deﬁnition (5.40a) of βFR
k , we have
∥pk∥2 ≤c3∥∇fk∥2 + (βFR
k )2[χ3∥∇fk−1∥2 + (β
FR
k−1)2∥pk−2∥2]
 c3∥∇fk∥4

1
∥∇fk∥2 +
1
∥∇fk−1∥2

+ ∥∇fk∥4
∥∇fk−2∥4 ∥pk−2∥2
≤c3∥∇fk∥4
k

j1
∥∇fj∥−2,
(5.64)
where we used the facts that
(βFR
k )2(β
FR
k−1)2 · · · (β
FR
k−i)2 
∥∇fk∥4
∥∇fk−i−1∥4
and p1  −∇f1. By using the bounds (5.55) and (5.60) in (5.64), we obtain
∥pk∥2 ≤χ3 ¯γ 4
γ 2
k,
(5.65)
which implies that
∞

k1
1
∥pk∥2 ≥γ4
∞

k1
1
k ,
(5.66)

130
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
for some positive constant γ4.
Suppose for contradiction that (5.60) holds. Then from (5.62), we have that
∞

k1
1
∥pk∥2 < ∞.
(5.67)
However, if we combine this inequality with (5.66), we obtain that ∞
k1 1/k < ∞, which
is not true. Hence, (5.60) does not hold, and the claim (5.59) is proved.
□
Note that this global convergence result applies to a practical implementation of the
Fletcher–Reevesmethodandisvalidforgeneralnonlinearobjectivefunctions.Inthissense,it
is more satisfactory than other convergence results that apply to speciﬁc types of objectives—
for example, convex functions.
In general, if we can show that there is a constant c4 > 0 such that
cos θk ≥c4
∥∇fk∥
∥pk∥,
k  1, 2, . . . ,
and another constant c5 such that
∥∇fk∥
∥pk∥≥c5 > 0,
k  1, 2, . . . ,
it follows from Theorem 5.7 that
lim
k→∞∥∇fk∥ 0.
This result can be established for the Polak–Ribi`ere method under the assumption that f is
strongly convex and that an exact line search is used.
For general (nonconvex) functions, is it not possible to prove a result like Theorem 5.8
for Algorithm PR-CG. This is unexpected, since the Polak–Ribi`ere method performs better
in practice than the Fletcher–Reeves method. In fact, the following surprising result shows
that the Polak–Ribi`ere method can cycle inﬁnitely without approaching a solution point,
even if an ideal line search is used. (By “ideal” we mean that line search returns a value αk
that is the ﬁrst stationary point for the function t(α)  f (xk + αpk).)
Theorem 5.9.
Consider the Polak–Ribi`ere method method (5.43), with an ideal line search. There exists
a twice continuously differentiable objective function f : IR3 →IR and a starting point x0 ∈IR3
such that the sequence of gradients {∥∇fk∥} is bounded away from zero.

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
131
The proof of this result is given in [207], and is quite complex. It demonstrates the
existence of the desired objective function without actually constructing this function explic-
itly. The result is interesting, since the step length assumed in the proof—the ﬁrst stationary
point—may be accepted by any of the practical line search algorithms currently in use.
The proof of Theorem 5.9 requires that some consecutive search directions become
almost negatives of each other. In the case of ideal line searches, this can be achieved only if
βk < 0, so the analysis suggests a modiﬁcation of the Polak–Ribi`ere method in which we set
β+
k  max{βPR
k , 0}.
(5.68)
This method is exactly Algorithm PR+ discussed above. We mentioned earlier that a line
search strategy based on a slight modiﬁcation of the Wolfe conditions guarantees that all
search directions generated by Algorithm PR+ are descent directions. Using these facts, it is
possible to prove global convergence of Algorithm PR+ for general functions.
NOTES AND REFERENCES
The conjugate gradient method was developed in the 1950s by Hestenes and
Stiefel [135] as an alternative to factorization methods for ﬁnding exact solutions of symmet-
ric positive deﬁnite systems. It was not until some years later, in one of the most important
developments in sparse linear algebra, that this method came to be viewed as an iterative
method that could give good approximate solutions to systems in many fewer than n steps.
Our presentation of the linear conjugate gradient method follows that of Luenberger [152].
Interestingly enough, the nonlinear conjugate gradient method of Fletcher and
Reeves [88] was proposed after the linear conjugate gradient method had fallen out of favor,
butseveralyearsbeforeitwasrediscoveredasaniterativemethod.ThePolak–Ribi`eremethod
was proposed in [194], and the example showing that it may fail to converge on nonconvex
problems is given by Powell [207]. Restart procedures are discussed in Powell [203].
Analysis due to Powell [200] provides further evidence of the inefﬁciency of the
Fletcher–Reeves method using exact line searches. He shows that if the iterates enter a region
in which the function is the two-dimensional quadratic
f (x)  1
2xT x,
thentheanglebetweenthegradient∇fk andthesearchdirectionpk staysconstant.Therefore,
if this angle is close to 90◦, the method will converge very slowly. Indeed, since this angle can
bearbitrarilycloseto90◦,theFletcher–Reevesmethodcanbeslowerthanthesteepestdescent
method.ThePolak–Ribi`eremethodbehavesquitedifferentlyinthesecircumstances:Ifavery
small step is generated, the next search direction tends to the steepest descent direction, as
argued above. This feature prevents a sequence of tiny steps.
The global convergence of nonlinear conjugate gradient methods is studied also in
Al-Baali [3] and Gilbert and Nocedal [103].

132
C h a p t e r
5 .
C o n j u g a t e G r a d i e n t M e t h o d s
Most of the theory on the rate of convergence of conjugate gradient methods assumes
thatthelinesearchisexact.CrowderandWolfe[61]showthattherateofconvergenceislinear,
and show by constructing an example that Q-superlinear convergence is not achievable.
Powell [200] studies the case in which the conjugate gradient method enters a region where
the objective function is quadratic, and shows that either ﬁnite termination occurs or the rate
of convergence is linear. Cohen [43] and Burmeister [33] prove n-step quadratic convergence
for general objective functions, that is,
∥xk+n −x∗∥ O(∥xk −x∗∥2).
Ritter [214] shows that in fact, the rate is superquadratic, that is,
∥xk+n −x∗∥ o(∥xk −x∗∥2).
Powell [206] gives a slightly better result and performs numerical tests on small problems
to measure the rate observed in practice. He also summarizes rate-of-convergence results
for asymptotically exact line searches, such as those obtained by Baptist and Stoer [6] and
Stoer [232].
Even faster rates of convergence can be established (see Schuller [225], Ritter [214]),
under the assumption that the search directions are uniformly linearly independent, but this
assumption is hard to verify and does not often occur in practice.
Nemirovsky and Yudin [180] devote some attention to the global efﬁciency of the
Fletcher–Reeves and Polak–Ribi`ere methods with exact line searches. For this purpose they
deﬁne a measure of “laboriousness” and an “optimal bound” for it among a certain class
of iterations. They show that on strongly convex problems not only do the Fletcher–Reeves
and Polak–Ribi`ere methods fail to attain the optimal bound, but they may also be slower
than the steepest descent method. Subsequently, Nesterov [180] presented an algorithm that
attains this optimal bound. It is related to PARTAN, the method of parallel tangents (see, for
example, Luenberger [152]). We feel that this approach is unlikely to be effective in practice,
but no conclusive investigation has been carried out, to the best of our knowledge.
Special line search strategies that ensure global convergence of the Polak–Ribi`ere
method have been proposed, but they are not without disadvantages.
The results in Table 5.1 are taken from Gilbert and Nocedal [103]. This paper also de-
scribes a line search that guarantees that Algorithm PR+ always generates descent directions,
and proves global convergence.
✐
E x e r c i s e s
✐
5.1 ImplementAlgorithm5.2andusetoitsolvelinearsystemsinwhichAistheHilbert
matrix,whoseelementsareAi,j  1/(i+j−1).Settheright-hand-sidetob  (1, 1, . . . , 1)T

5 . 2 .
N o n l i n e a r C o n j u g a t e G r a d i e n t M e t h o d s
133
and the initial point to x0  0. Try dimensions n  5, 8, 12, 20 and report the number of
iterations required to reduce the residual below 10−6.
✐
5.2 Show that if the nonzero vectors p0, p1, . . . , pl satisfy (5.4), where A is symmetric
and positive deﬁnite, then these vectors are linearly independent. (This result implies that
A has at most n conjugate directions.)
✐
5.3 Verify (5.6).
✐
5.4 Show that if f (x) is a strictly convex quadratic, then the function h(σ)
def f (x0 +
σ0p0 + · · · + σk−1pk−1) also is a strictly convex quadratic in σ  (σ0, . . . , σk−1)T .
✐
5.5 Using the form of the CG iteration prove directly that (5.16) and (5.17) hold for
k  1.
✐
5.6 Show that (5.23d) is equivalent to (5.13d).
✐
5.7 Let {λi, vi} i  1, . . . , n be the eigenpairs of A. Show that the eigenvalues and
eigenvectors of [I + Pk(A)A]T A[I + Pk(A)A] are λi[1 + λiPk(λi)]2 and vi, respectively.
✐
5.8 Construct matrices with various eigenvalue distributions and apply the CG
method to them. Then observe whether the behavior can be explained from Theorem 5.5.
✐
5.9 Derive Algorithm 5.3 by applying the standard CG method in the variables ˆx and
then transforming back into the original variables.
✐
5.10 Verify the modiﬁed conjugacy condition (5.39).
✐
5.11 Show that when applied to a quadratic function, and with exact line searches,
the Polak–Ribi`ere formula given by (5.43) and the Hestenes–Stiefel formula given by (5.45)
reduce to the Fletcher–Reeves formula (5.40a).
✐
5.12 Prove that Lemma 5.6 holds for any choice of βk satisfying |βk| ≤βFR
k .

Chapter6

Practical Newton
Methods
We saw in Chapter 3 that the pure Newton method with unit steps converges rapidly once
it approaches a minimizer x∗. This simple algorithm is inadequate for general use, however,
sinceitmayfailtoconvergetoasolutionfromremotestartingpoints.Evenifitdoesconverge,
its behavior may be erratic in regions where the function is not convex. Our ﬁrst goal in
designing a practical Newton-based algorithm is to make it robust and efﬁcient in all cases.
Recall that the basic Newton step pN
k is obtained by solving the symmetric n×n linear
system
∇2f (xk)pN
k  −∇f (xk).
(6.1)
To obtain global convergence we require the search direction pN
k to be a descent direction,
which will be true here if the Hessian ∇2f (xk) is positive deﬁnite. However, if the Hessian
is not positive deﬁnite, or is close to being singular, pN
k may be an ascent direction or may
be excessively long. In this chapter, we describe two strategies for ensuring that the step is
always of good quality. In the ﬁrst strategy we solve (6.1) using the conjugate gradient (CG)
method (see Chapter 5), terminating if negative curvature is encountered. There are both
line search and trust-region implementations of this strategy, which we call the Newton–CG

136
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
method. The second approach consists in modifying the Hessian matrix ∇2f (xk) before or
during the solution of the system (6.1) so that it becomes sufﬁciently positive deﬁnite. We
call this the modiﬁed Newton method.
Another concern in designing practical Newton methods is to keep the computational
cost as small as possible. In the Newton–CG method, we accomplish this goal by terminating
the CG iteration before an exact solution to (6.1) is found. This inexact Newton approach
thus computes an approximation pk to the pure Newton step pN
k. When using a direct
method to solve the Newton equations we can take advantage of any sparsity structure in the
Hessian by using sparse Gaussian elimination techniques to factor ∇2f (xk) and then using
the factors to obtain an exact solution of (6.1).
The computation of the Hessian matrix ∇2f (xk) usually represents a major task in
the implementation of Newton’s method. If the Hessian is not available in analytic form,
we can use automatic differentiation techniques to compute it, or we can use ﬁnite differ-
ences to estimate it. A detailed treatment of these topics is deferred to Chapter 7. With all
these ingredients—modiﬁcation of the pure Newton iteration, exploitation of sparsity, and
differentiation techniques—the Newton methods described in this chapter represent some
of the most reliable and powerful methods for solving both small or large unconstrained
optimization problems.
Since inexact Newton steps are useful in both line search and trust-region
implementations, we discuss them ﬁrst.
6.1
INEXACT NEWTON STEPS
We have noted that a drawback of the pure Newton method is the need to solve the equations
(6.1) exactly. Techniques based on Gaussian elimination or another type of factorization of
the coefﬁcient matrix can be expensive when the number of variables is large. An accurate
solution to (6.1) may not be warranted in any case, since the quadratic model used to
derive the Newton equations may not provide a good prediction of the behavior of the
function f , especially when the current iterate xk is remote from the solution x∗. It is
therefore appealing to apply an iterative method to (6.1), terminating the iterations at some
approximate (inexact) solution of this system.
Most rules for terminating the iterative solver for (6.1) are based on the residual
rk  ∇2f (xk)pk + ∇f (xk),
(6.2)
where pk is the inexact Newton step. Since the size of the residual changes if f is multiplied
by a constant (i.e., r is not invariant to scaling of the objective function), we consider its
size relative to that of the right-hand-side vector in (6.1), namely ∇f (xk). We therefore
terminate the iterative solver when
∥rk∥≤ηk∥∇f (xk)∥,
(6.3)

6 . 1 .
I n e x a c t N e w t o n S t e p s
137
where the sequence {ηk} (with 0 < ηk < 1 for all k) is called the forcing sequence.
We now study how the rate of convergence of inexact Newton methods based on
(6.1)–(6.3) is affected by the choice of the forcing sequence. Our ﬁrst result says that local
convergence is obtained simply by ensuring that ηk is bounded away from 1.
Theorem 6.1.
Suppose that ∇f (x) is continuously differentiable in a neighborhood of a minimizer x∗,
and assume that ∇2f (x∗) is positive deﬁnite. Consider the iteration xk+1  xk + pk where pk
satisﬁes (6.3), and assume that ηk ≤η for some constant η ∈[0, 1). Then, if the starting point
x0 is sufﬁciently near x∗, the sequence {xk} converges to x∗linearly. That is, for all k sufﬁciently
large, we have
∥xk+1 −x∗∥≤c∥xk −x∗∥,
for some constant 0 < c < 1.
Note that the condition on the forcing sequence {ηk} is not very restrictive, in the sense
that if we relaxed it just a little, it would yield an algorithm that obviously does not converge.
Speciﬁcally, if we allowed ηk ≥1, the step pk  0 would satisfy (6.3) at every iteration, but
the resulting method would set xk  x0 for all k and would not converge to the solution.
Rather than giving a rigorous proof of Theorem 6.1, we now present an informal
derivation that contains the essence of the argument and motivates the results that follow.
Since the Hessian matrix ∇2f (x∗) is assumed to be positive deﬁnite, there is a positive
constant L such that ∥∇2f (xk)−1∥≤L for all xk sufﬁciently close to x∗. We therefore have
from (6.2) that the inexact Newton step satisﬁes
∥pk∥≤L(∥∇f (xk)∥+ ∥rk∥) ≤2L∥∇f (xk)∥,
where the second inequality follows from (6.3) and ηk < 1. By using this expression together
with Taylor’s theorem we obtain
∇f (xk+1)  ∇f (xk) + ∇2f (xk)pk + O
	
∥pk∥2
 ∇f (xk) −(∇f (xk) −rk) + O
	
L2∥∇f (xk)∥2
 rk + O
	
∥∇f (xk)∥2
.
(6.4)
By taking norms and recalling (6.3), we have that
∥∇f (xk+1)∥≤ηk∥∇f (xk)∥+ O
	
∥∇f (xk)∥2
.
(6.5)

138
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
Therefore, if xk is close to x∗, we can expect ∥∇f (x)∥to decrease by a factor of approximately
ηk < 1 at every iteration. More precisely, we have that
lim sup
k→∞
∥∇f (xk+1)∥
∥∇f (xk)∥
≤η < 1.
Relation (6.4) also suggests that if rk  o(∥∇f (xk)∥), then the rate of convergence in the
gradient will be superlinear, for in this case the limit satisﬁes
lim sup
k→∞
∥∇f (xk+1)∥
∥∇f (xk)∥
 0.
Similarly, if rk  O(∥∇f (xk)∥2), we have
lim sup
k→∞
∥∇f (xk+1)∥
∥∇f (xk)∥2  c,
for some constant c, suggesting that the quadratic rate of convergence of the exact Newton
method has been recaptured.
It is not difﬁcult to show that the iterates {xk} converge to x∗at the same rate as the
sequence of gradients {∇f (xk)} converges to zero.
Theorem 6.2.
Suppose that the conditions of Theorem 6.1 hold and assume that the iterates {xk} gener-
ated by the inexact Newton method converge to x∗. Then the rate of convergence is superlinear
if ηk →0 and quadratic if ηk  O(∥∇f (xk)∥).
The proof makes use of Taylor’s theorem and the relation (6.4). We leave the details as an
exercise.
To obtain superlinear convergence we can set, for example, ηk

min
	
0.5,

∥∇f (xk)∥

, while the choice ηk

min(0.5, ∥∇f (xk)∥) would yield quadratic
convergence.
All the results presented in this section, which are proved by Dembo, Eisenstat, and
Steihaug [66], are local in nature: They assume that the sequence {xk} eventually enters
the near vicinity of the solution x∗. They also assume that the unit step length αk  1 is
taken, and hence that globalization strategies (line search, trust-region) do not get in the way
of rapid convergence. In the next sections we show that inexact Newton strategies can, in
fact, be incorporated in practical line search and trust-region implementations of Newton’s
method, yielding algorithms with good local and global convergence properties.

6 . 2 .
L i n e S e a r c h N e w t o n M e t h o d s
139
6.2
LINE SEARCH NEWTON METHODS
We now describe line search implementations of Newton’s method that are practical for
small and large problems. Each iteration has the form xk+1  xk + αkpk, where αk is the
step length and pk is either the pure Newton direction pN
k or an approximation to it. We have
dealt with the issue of selecting the step length αk in Chapter 3. As stated there, αk can be
chosen to satisfy the Wolfe conditions (3.6) or the Goldstein conditions (3.11), or it can be
obtained by an Armijo backtracking line search described in Section 3.1. We stress again that
the line search should always try the unit step length αk  1 ﬁrst, so that this step length is
used when acceptable. We now consider two different techniques for computing the search
direction pk.
LINE SEARCH NEWTON–CG METHOD
In the line search Newton-CG method, also known as the truncated Newton method,
we compute the search direction by applying the CG method to the Newton equations
(6.1), and attempt to satisfy a termination test of the form (6.3). Note, however, that the
CG method is designed to solve positive deﬁnite systems, and that the Hessian could have
negative eigenvalues away from the solution. Therefore, we terminate the CG iteration as
soon as a direction of negative curvature is generated. This adaptation of the CG method
ensures that the direction pk is a descent direction, and that the fast convergence rate of the
pure Newton method is preserved.
We discuss now the details of this inner CG iteration, which is a slight modiﬁcation of
Algorithm CG described in Chapter 5. The linear system to be solved is (6.1), and thus in the
notation of Chapter 5, the coefﬁcient matrix is A  ∇2fk and the right-hand-side vector is
b  −∇fk. We will use superscripts to denote the iterates {x(i)} and search directions {p(i)}
generated by the CG iteration, so as to distinguish them from the Newton iterates xk and
search directions pk. We impose the following three requirements on the CG iteration for
solving (6.1).
(a) The starting point for the CG iteration is x(0)  0.
(b) Negative curvature test. If a search direction p(i) generated by the CG iteration satisﬁes
(p(i))T Ap(i) ≤0,
(6.6)
then we check whether this is the ﬁrst CG iteration. If so (i  0), we complete the ﬁrst
CG iteration, compute the new iterate x(1), and stop. If (6.6) holds and i > 0, then
we stop the CG iteration immediately and return the most recently generated solution
point, x(i).
(c) The Newton step pk is deﬁned as the ﬁnal CG iterate x(f ).

140
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
We could also choose x(0) to be the optimal solution of the previous linear system, which
is the same as the most recently generated Newton–CG step. Both choices seem to perform
equally well in this line search context.
If the condition (6.6) is not satisﬁed, then the ith CG iteration is exactly as described in
Algorithm CG of Chapter 5. A vector p(i) satisfying (6.6) with a strict inequality is said to be
a direction of negative curvature for A. Note that if negative curvature is encountered during
the ﬁrst CG iteration, then the Newton–CG direction pk is the steepest descent direction
−∇fk. This is the reason for choosing the initial estimate in the CG iteration as x(0)  0.
On the other hand, if the CG method performs more than one iteration, the Newton–CG
direction will also be a descent direction; see Exercise 2. Preconditioning can be used in the
CG iteration.
The Newton–CG method does not require explicit knowledge of the Hessian ∇2f (xk).
Rather, it requires only that we can supply matrix–vector products of the form ∇2f (xk)p for
any given vector p. This fact is useful for cases in which the user cannot easily supply code to
calculate second derivatives, or where the Hessian requires too much storage. In these cases,
the techniques of Chapter 7, which include automatic differentiation and ﬁnite differences,
can be used to calculate the Hessian–vector products.
To illustrate the ﬁnite-differencing technique brieﬂy, we note that we can use the
approximation
∇2f (xk)p ≈∇f (xk + hp) −∇f (xk)
h
,
(6.7)
for some small differencing interval h. It is easy to prove that the accuracy of this approxi-
mation is O(h); appropriate choices of h are discussed in Chapter 7. The price we pay for
bypassing the computation of the Hessian is one new gradient evaluation per CG iteration.
An alternative to ﬁnite differencing is automatic differentiation, which can in principle be
used to compute ∇2f (xk)p exactly. (Again, details are given in Chapter 7.) Methods of this
type are known as Hessian-free Newton methods.
We can now give a general description of the Newton–CG method. For concreteness,
we choose the forcing sequence as ηk  min

0.5,

∥∇2fk∥

, so as to obtain a superlinear
convergence rate, but other choices are possible.
Algorithm 6.1 (Line Search Newton–CG).
Given initial point x0;
for
k  0, 1, 2, . . .
Compute a search direction pk by applying the CG method to
∇2f (xk)p  −∇fk, starting from x(0)  0. Terminate when
∥rk∥≤min(0.5, √∥∇fk∥)∥∇f (xk)∥, or if negative curvature is
encountered, as described in (b);
Set xk+1  xk + αkpk, where αk satisﬁes the Wolfe, Goldstein, or
Armijo backtracking conditions;
end

6 . 2 .
L i n e S e a r c h N e w t o n M e t h o d s
141
This method is well suited for large problems, but it has a minor weakness, especially
in the case where no preconditioning is used in the CG iteration. When the Hessian ∇2fk is
nearly singular, the Newton–CG direction can be excessively long, requiring many function
evaluations in the line search. In addition, the reduction in the function may be very small
in this case. To alleviate this difﬁculty we can try to normalize the Newton step, but good
rules for doing so are difﬁcult to determine. (They run the risk of undermining the rapid
convergence of Newton’s method in the case where the pure Newton step is well scaled.)
It is preferable to introduce a threshold value in (6.6), but once again, good choices of
this value are difﬁcult to determine. The trust-region implementation of the Newton–CG
method described in Section 6.4 deals more effectively with this problematic situation, and
is therefore slightly preferable, in our opinion.
MODIFIED NEWTON’S METHOD
In many applications it is desirable to use a direct linear algebra technique, such as
Gaussian elimination, to solve the Newton equations (6.1). If the Hessian is not positive
deﬁnite, or is close to being singular, then we can modify this matrix before or during the
solution process to ensure that the computed direction pk satisﬁes a linear system identical
to (6.1) except that the coefﬁcient matrix is replaced with a positive deﬁnite approximation.
The modiﬁed Hessian is obtained by adding either a positive diagonal matrix or a full matrix
to the true Hessian ∇2f (xk). Following is a general description of this method.
Algorithm 6.2 (Line Search Newton with Modiﬁcation).
given initial point x0;
for
k  0, 1, 2, . . .
Factorize the matrix Bk  ∇2f (xk) + Ek, where Ek  0 if ∇2f (xk)
is sufﬁciently positive deﬁnite; otherwise, Ek is chosen to
ensure that Bk is sufﬁciently positive deﬁnite;
Solve Bkpk  −∇f (xk);
Set xk+1  xk + αkpk, where αk satisﬁes the Wolfe, Goldstein, or
Armijo backtracking conditions;
end
The choice of Hessian modiﬁcation Ek is crucial to the effectiveness of the method.
Some approaches do not compute Ek explicitly, but rather introduce extra steps and tests
into standard factorization procedures, modifying these procedures “on the ﬂy” so that the
computed factors are in fact the factors of a positive deﬁnite matrix. Strategies based on
modifying a Cholesky factorization and on modifying a symmetric indeﬁnite factorization
of the Hessian are described in the next section.
We can establish fairly satisfactory global convergence results for Algorithm 6.2, pro-
vided that the strategy for choosing Ek (and hence Bk) satisﬁes the bounded modiﬁed
factorization property. This property is that the matrices in the sequence {Bk} have bounded

142
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
condition number whenever the sequence of Hessians {∇2f (xk)} is bounded; that is,
cond(Bk)  ∥Bk∥∥B−1
k ∥≤C,
for some C > 0 and all k  0, 1, 2, . . ..
(6.8)
Ifthispropertyholds,globalconvergenceofthemodiﬁedlinesearchNewtonmethodfollows
directly from the results of Chapter 3, as we show in the following result.
Theorem 6.3.
Let f be twice continuously differentiable on an open set D, and assume that the starting
point x0 of Algorithm 6.2 is such that the level set L  {x ∈D : f (x) ≤f (x0)} is compact.
Then if the bounded modiﬁed factorization property holds, we have that
lim
k→∞∇f (xk)  0.
Proof.
The line search ensures that all iterates xk remain in the level set D. Since ∇2f (x)
is assumed to be continuous on D, and since D is compact, we have that the sequence of
Hessians {∇2f (xk)} is bounded, and therefore (6.8) holds. Since Algorithm 6.2 uses one of
the line searches for which Zoutendijk’s theorem (Theorem 3.2) holds, the result follows
from (3.16).
□
We now consider the convergence rate of Algorithm 6.2. Suppose that the sequence
of iterates xk converges to a point x∗where ∇2f (x∗) is sufﬁciently positive deﬁnite in the
sense that the modiﬁcation strategies described in the next section return the modiﬁcation
Ek  0 for all sufﬁciently large k. By Theorem 3.5, we have that αk  1 for all sufﬁciently
large k, so that Algorithm 6.2 reduces to a pure Newton method, and the rate of convergence
is quadratic.
For problems in which ∇f ∗is close to singular, there is no guarantee that the
modiﬁcation Ek will eventually vanish, and the convergence rate may be only linear.
6.3
HESSIAN MODIFICATIONS
We now discuss procedures for modifying the Hessian matrices ∇2f (xk) that implicitly
or explicitly choose the modiﬁcation Ek so that the matrix Bk  ∇2f (xk) + Ek in Algo-
rithm 6.2 is sufﬁciently positive deﬁnite. Besides requiring the modiﬁed matrix Bk to be well
conditioned (so that Theorem 6.3 holds), we would like the modiﬁcation to be as small as
possible, so that the second-order information in the Hessian is preserved as far as possible.
Naturally, we would also like the modiﬁed factorization to be computable at moderate cost.
TosetthestageforthematrixfactorizationtechniquesthatwillbeusedinAlgorithm6.2
we will begin by describing an “ideal” strategy based on the eigenvalue decomposition of
∇2f (xk).

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
143
EIGENVALUE MODIFICATION
Consider a problem in which, at the current iterate xk, ∇f (xk)  (1, −3, 2) and
∇2f (xk)  diag(10, 3, −1), which is clearly indeﬁnite. By the spectral decomposition
theorem (see the Appendix) we can deﬁne Q  I and   diag(λ1, λ2, λ3), and write
∇2f (xk)  QQT 
n

i1
λiqiqT
i .
(6.9)
The pure Newton step—the solution of (6.1)—is pN
k  (−0.1, 1, 2), which is not a descent
direction, since ∇f (xk)T pN
k > 0. One might suggest a modiﬁed strategy in which we re-
place ∇2f (xk) by a positive deﬁnite approximation Bk, in which all negative eigenvalues in
∇2f (xk) are replaced by a small positive number δ that is somewhat larger than machine
accuracy u; say δ  √u. Assuming that machine accuracy is 10−16, the resulting matrix in
our example is
Bk 
2

i1
λiqiqT
i + δq3qT
3  diag
	
10, 3, 10−8
,
(6.10)
which is numerically positive deﬁnite and whose curvature along the eigenvectors q1 and q2
has been preserved. Note, however, that the search direction based on this modiﬁed Hessian
is
pk  −B−1
k ∇fk  −
2

i1
1
λi
qi
	
qT
i ∇fk

−1
δ q3
	
qT
3 ∇f (xk)

≈−
	
2 × 108
q3.
(6.11)
For small δ, this step is nearly parallel to q3 (with relatively small contributions from q1 and
q2) and very long. Although f decreases along the direction pk, its extreme length violates
the spirit of Newton’s method, which relies on a quadratic approximation of the objective
function that is valid in a neighborhood of the current iterate xk. It is therefore questionable
that this search direction is effective.
A different type of search direction would be obtained by ﬂipping the signs of the
negative eigenvalues in (6.9), which amounts to setting δ  1 in our example. Again, there
is no consensus as to whether this constitutes a desirable modiﬁcation to Newton’s method.
Various other strategies can be considered: We could set the last term in (6.11) to zero, so
that the search direction has no components along the negative curvature directions, or we
could adapt the choice of δ to ensure that the length of the step is not excessive. (This last
strategy has the ﬂavor of trust-region methods.) We see that there is a great deal of freedom
in devising modiﬁcation strategies, and there is currently no agreement on which is the ideal
strategy.

144
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
Setting the issue of the choice of δ aside for the moment, let us look more closely at
the process of modifying a matrix so that it becomes positive deﬁnite. The modiﬁcation
(6.10) to the example matrix (6.9) can be shown to be optimal in the following sense. If A is
a symmetric matrix with spectral decomposition A  QQT , then the correction matrix
A of minimum Frobenius norm that ensures that λmin(A + A) ≥δ is given by
A  Q diag (τi)QT ,
with
τi 

0,
λi ≥δ,
δ −λi,
λi < δ.
(6.12)
Here λmin(A) denotes the smallest eigenvalue of A, and the Frobenius norm of a matrix is
deﬁned as ∥A∥2
F  n
i,j1 a2
ij (see (A.40)). Note that A is not diagonal in general, and
that the modiﬁed matrix is given by
A + A  Q( + diag(τi))QT .
By using a different norm we can obtain a diagonal modiﬁcation. Suppose again that
A is a symmetric matrix with spectral decomposition A  QQT . A correction matrix A
with minimum Euclidean norm that satisﬁes λmin(A + A) ≥δ is given by
A  τI,
with
τ  max(0, δ −λmin(A)).
(6.13)
The modiﬁed matrix now has the form
A + τI,
(6.14)
which happens to have the same form as the matrix occurring in (unscaled) trust–region
methods (see Chapter 4). All the eigenvalues of (6.14) have thus been shifted, and all are
greater than δ.
These results suggest that both diagonal and nondiagonal modiﬁcations can be con-
sidered. Even though we have not answered the question of what constitutes a good
modiﬁcation, various practical diagonal and nondiagonal modiﬁcations have been pro-
posed and implemented in software. They do not make use of the spectral decomposition
of the Hessian, since this is generally too expensive to compute. Instead, they use Gaussian
elimination, choosing the modiﬁcations indirectly and hoping that somehow they will pro-
duce good steps. Numerical experience indicates that the strategies described next often (but
not always) produce good search directions.
ADDING A MULTIPLE OF THE IDENTITY
Perhapsthesimplestideaistoﬁndascalarτ > 0suchthat∇2f (xk) + τI issufﬁciently
positive deﬁnite. From the previous discussion we know that τ must satisfy (6.13), but we

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
145
normally don’t have a good estimate of the smallest eigenvalue of the Hessian. A simple
idea is to recall that the largest eigenvalue (in absolute value) of a matrix A is bounded by
the Frobenius norm ∥A∥F. This suggests the following strategy; here aii denotes a diagonal
element of A.
Algorithm 6.3 (Cholesky with Added Multiple of the Identity).
set β ←∥A∥F;
if
mini aii > 0
τ0 ←0
else
τ0 ←β/2;
end (if)
for
k  0, 1, 2, . . .
Attempt to apply the incomplete Cholesky algorithm to obtain
LLT  A + τkI;
if
factorization is completed successfully
stop and return L
else
τk+1 ←max(2τk, β/2);
end (if)
end (for)
This strategy is quite simple and may be preferable to the modiﬁed factorization
techniques described next, but it suffers from two drawbacks. The value of τ generated by
this procedure may be unnecessarily large, which would bias the modiﬁed Newton direction
too much toward the steepest descent direction. In addition, every value of τk requires a new
factorization of A + τkI, which can be quite expensive if several trial values are generated.
(The symbolic factorization of A is performed only once, but the numerical factorization
must be performed from scratch every time.)
MODIFIED CHOLESKY FACTORIZATION
A popular approach for modifying a Hessian matrix that is not positive deﬁnite is
to perform a Cholesky factorization of ∇2f (xk), but to increase the diagonal elements
encountered during the factorization (where necessary) to ensure that they are sufﬁciently
positive. This modiﬁed Cholesky approach is designed to accomplish two goals: It guarantees
that the modiﬁed Cholesky factors exist and are bounded relative to the norm of the actual
Hessian, and it does not modify the Hessian if it is sufﬁciently positive deﬁnite.

146
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
We begin our description of this approach by brieﬂy reviewing the Cholesky
factorization. Every symmetric and positive deﬁnite matrix A can be written as
A  LDLT ,
(6.15)
where L is a lower triangular matrix with unit diagonal elements and D is a diagonal matrix
with positive elements on the diagonal. By equating the elements in (6.15), column by
column, it is easy to derive formulas for computing L and D.
❏Example 6.1
Consider the case n  3. The equation A  LDLT is given by


a11
a21
a31
a21
a22
a32
a31
a32
a33 




1
0
0
l21
1
0
l31
l32
1




d1
0
0
0
d2
0
0
0
d3




1
l21
l31
0
1
l32
0
0
1

.
(The notation indicates that A is symmetric.) By equating the elements of the ﬁrst column,
we have
a11

d1,
a21

d1l21
⇒
l21  a21/d1,
a31

d1l31
⇒
l31  a31/d1.
Proceeding with the next two columns, we obtain
a22

d1l2
21 + d2
⇒
d2  a22 −d1l2
21,
a32

d1l31l21 + d2l32
⇒
l32  (a32 −d1l31l21) /d2,
a33

d1l2
31 + d2l2
32 + d3
⇒
d3  a33 −d1l2
31 −d2l2
32.
❐
This procedure is generalized in the following algorithm.
Algorithm 6.4 (Cholesky Factorization, LDLT Form).
for
j  1, 2, . . . , n
cjj ←ajj −j−1
s1 dsl2
js;
dj ←cjj;
for
i  j + 1, . . . , n
cij ←aij −j−1
s1 dslisljs;

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
147
lij ←cij/dj;
end
end
One can show (see, for example, Golub and Van Loan [115, Section 4.2.3]) that the diag-
onal elements djj are all positive whenever A is positive deﬁnite. The scalars cij have been
introduced only to facilitate the description of the modiﬁed factorization discussed below.
We should note that Algorithm 6.4 differs a little from the standard form of the Cholesky
factorization, which produces a lower triangular matrix M such that
A  MMT .
(6.16)
In fact, we can make the identiﬁcation M  LD1/2 to relate M to the factors L and D
computed in Algorithm 6.4. The technique for computing M appears as Algorithm A.2 in
the Appendix.
If A is indeﬁnite, the factorization A  LDLT may not exist. Even if it does exist,
Algorithm 6.4 is numerically unstable when applied to such matrices, in the sense that the
elements of L and D can become arbitrarily large. It follows that a strategy of computing
the LDLT factorization and then modifying the diagonal after the fact to force its elements
to be positive may break down, or may result in a matrix that is drastically different from A.
Instead, we will modify the matrix A during the course of the factorization in such a
waythatallelementsinD aresufﬁcientlypositive,andsothattheelementsofD andLarenot
too large. To control the quality of the modiﬁcation, we choose two positive parameters δ and
β and require that during the computation of the jth columns of L and D in Algorithm 6.4
(that is, for each j in the outer loop of the algorithm) the following bounds be satisﬁed:
dj ≥δ,
|mij| ≤β,
i  j + 1, . . . , n.
(6.17)
To satisfy these bounds we only need to change one step in Algorithm 6.4: The formula for
computing the diagonal element dj in Algorithm 6.4 is replaced by
dj  max
#
|cjj|,
θj
β
2
, δ
$
,
with θj  max
j<i≤n |cij|.
(6.18)
To verify that (6.17) holds, we note from Algorithm 6.4 that cij  lijdj, and therefore
|mij|  |lij

dj|  |cij|
dj
≤|cij|β
θj
≤β,
for all i > j.
We note that θj can be computed prior to dj because the elements cij in the second
for loop of Algorithm 6.4 do not involve dj. In fact, this is the reason for introducing
the quantities cij into the algorithm. This observation also suggests that the computations

148
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
should be reordered so that the cij are computed before the diagonal element dj. We use this
procedure in Algorithm 6.5, the modiﬁed Cholesky factorization algorithm described below.
To try to reduce the size of the modiﬁcation, symmetric interchanges of rows and columns
are introduced, so that at the jth step of the factorization, the jth row and column are those
that yield the largest diagonal element. We also note that the computation of the elements
cjj can be carried out recursively after every column of the factorization is calculated.
Algorithm 6.5 (Modiﬁed Cholesky [104]).
given δ > 0, β > 0:
for
k  1, 2, . . . , n
ckk  akk;
(initialize the diagonal elements)
end
Find index q such that |cqq| ≥|cii|, i  j, . . . , n;
Interchange row and column j and q;
for
j  1, 2, . . . , n
(compute the jth column of L)
for
s  1, 2, . . . , j −1
ljs ←cjs/ds;
end
for
i  j + 1, . . . , n
cij ←aij −j−1
s1 ljscis;
end
θj ←0;
if
j ≤n
θj ←maxj<i≤n |cij|;
end
dj ←max{|cjj|, (θj/β)2, δ};
if
j < n
for
i  j + 1, . . . , n
cii ←cii −c2
ij/dj;
end
end
end.
Thealgorithmrequiresapproximately n3/6arithmeticoperations,whichisroughlythesame
as the standard Cholesky factorization. However, the row and column interchanges require
movement of data in the computer, and the cost of this operation may be signiﬁcant on
large problems. No additional storage is needed beyond the amount required to store A; the
triangular factors L and D, as well as the intermediate scalars cij, can overwrite the elements
of A.
Suppose that we use P to denote the permutation matrix associated with the row
and column interchanges that occur during Algorithm 6.5. It is not difﬁcult to see that the
algorithmproducestheCholeskyfactorizationofthepermuted,modiﬁedmatrixPAP T +E,

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
149
that is,
PAP T + E  LDLT  MMT ,
(6.19)
where E is a nonnegative diagonal matrix that is zero if A is sufﬁciently positive deﬁnite.
From an examination of the formulae for cjj and dj in Algorithm 6.5, it is clear that the
diagonal entries of E are ej  dj −cjj. It is also clear that incrementing cjj by ej in the
factorization is equivalent to incrementing ajj by ej in the original data.
It remains only to specify the choice of the parameters δ and β in Algorithm 6.5. The
constant δ is normally chosen to be close to machine accuracy u; a typical choice is
δ  u max(γ (A) + ξ(A), 1),
where γ (A) and ξ(A) are, respectively, the largest-magnitude diagonal and off-diagonal
elements of the matrix A, that is,
γ  max
1≤i≤n |aii|,
ξ  max
i̸j |aij|.
Gill, Murray, and Wright [104] suggest the following choice of β:
β  max

γ (A),
ξ(A)
√
n2 −1
, u
1/2
,
whose intent is to minimize the norm of the modiﬁcation ∥E∥∞.
❏Example 6.2
Consider the matrix
A 


4.0
2.0
1.0
2.0
6.0
3.0
1.0
3.0
−0.004

,
whose eigenvalues are, to three digits, −1.25, 2.87, 8.38. Algorithm 6.5 gives the following
Cholesky factor M and diagonal modiﬁcation E:
M 


0.8165
1.8257
0
2.4495
0
0
1.2247
−1.2 × 10−16
1.2264

,
E  diag(0, 0, 3.008).

150
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
The modiﬁed matrix is
A′  MMT 


4.00
2.00
1.00
2.00
6.00
3.00
1.00
3.00
3.004

,
whose eigenvalues are 1.13, 3.00, 8.87, and whose condition number of 7.8 is quite
moderate.
❐
One can show (Mor´e and Sorensen [171]) that the matrices Bk obtained by applying
Algorithm 6.5 to the exact Hessians ∇2f (xk) have bounded condition numbers, that is, the
bound (6.8) holds for some value of C.
GERSHGORIN MODIFICATION
We now give a brief outline of an alternative modiﬁed Cholesky algorithm that makes
use of Gershgorin circle estimates to increase the diagonal elements as necessary. The ﬁrst
step of this strategy is to apply Algorithm 6.5 to the matrix A, terminating with the usual
factorization (6.19). If the perturbation matrix E is zero, we are ﬁnished, since in this case A
is already sufﬁciently positive deﬁnite. Otherwise, we compute two upper bounds b1 and b2
onthesmallesteigenvalueλmin(A)ofA.Theﬁrstestimateb1 isobtainedfromtheGershgorin
circle theorem; it guarantees that A+b1I is strictly diagonally dominant. The second upper
bound b2 is simply
b2  max
1≤i≤n eii.
We now deﬁne
µ  min(b1, b2),
and conclude the algorithm by computing the factorization of A + µI, taking the Cholesky
factor of this matrix as our modiﬁed Cholesky factor. The use of b2 gives a much needed
control on the process, since the estimate b1 obtained from the Gershgorin circle theorem
can be quite loose.
It is not known whether this alternative is preferable to Algorithm 6.5 alone. Neither
strategy will modify a sufﬁciently positive deﬁnite matrix A, but it is difﬁcult to quantify the
meaning of the term “sufﬁcient” in terms of the smallest eigenvalue λmin(A). Both strategies
may in fact modify a matrix A whose minimum eigenvalue is greater than the parameter δ
of Algorithm 6.5.

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
151
MODIFIED SYMMETRIC INDEFINITE FACTORIZATION
Another strategy for modifying an indeﬁnite Hessian is to use a procedure based on
a symmetric indeﬁnite factorization. Any symmetric matrix A, whether positive deﬁnite or
not, can be written as
PAP T  LBLT ,
(6.20)
where L is unit lower triangular, B is a block diagonal matrix with blocks of dimension 1 or 2,
and P is a permutation matrix (see Golub and Van Loan [115, Section 4.4]). We mentioned
earlier that attempting to compute the LDLT factorization of an indeﬁnite matrix (where D
is a diagonal matrix) is inadvisable because even if the factors L and D are well-deﬁned, they
may contain entries that are larger than the original elements of A, thus amplifying rounding
errors that arise during the computation. However, by using the block diagonal matrix B,
which allows 2 × 2 blocks as well as 1 × 1 blocks on the diagonal, we can guarantee that the
factorization (6.20) always exists and can be computed by a numerically stable process.
❏Example 6.3
The matrix
A 


0
1
2
3
1
2
2
2
2
2
3
3
3
2
3
4


can be written in the form (6.20) with P  [e1, e4, e3, e2],
L 


1
0
0
0
0
1
0
0
1
9
2
3
1
0
2
9
1
3
0
1


,
B 


0
3
0
0
3
4
0
0
0
0
7
9
5
9
0
0
5
9
10
9


.
(6.21)
Note that both diagonal blocks in B are 2 × 2.
❐
The symmetric indeﬁnite factorization allows us to determine the inertia of a matrix,
that is, the number of positive, zero, and negative eigenvalues. One can show that the inertia
of B equals the inertia of A. Moreover, the 2 × 2 blocks in B are always constructed to

152
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
have one positive and one negative eigenvalue. Thus the number of positive eigenvalues in
A equals the number of positive 1 × 1 blocks plus the number of 2 × 2 blocks.
The ﬁrst step of the symmetric indeﬁnite factorization proceeds as follows. We identify
a submatrix E of A that is suitable to be used as a pivot block. The precise criteria that can
be used to choose E are described below, but we note here that E is either a single diagonal
element of A (giving rise to a 1 × 1 pivot block), or else it consists of two diagonal elements
of A (say, aii and ajj) together with the corresponding off-diagonal element (that is, aji and
aij). In either case, E is chosen to be nonsingular. We then ﬁnd a permutation matrix  that
makes E a leading principal submatrix of A, that is,
AT 
 E
CT
C
H

,
(6.22)
and then perform a block factorization on this rearranged matrix, using E as the pivot block,
to obtain
AT 

I
0
CE−1
I
 
E
0
0
H −CE−1CT
 
I
E−1CT
0
I

.
The next step of the factorization consists in applying exactly the same process to H −
CE−1CT , known as the remaining matrix or the Schur complement, which has dimension
either (n −1) × (n −1) or (n −2) × (n −2). The same procedure is applied recursively,
until we terminate with the factorization (6.20).
The symmetric indeﬁnite factorization requires approximately n3/3 ﬂoating-point
operations—the same as the cost of the Cholesky factorization of a positive deﬁnite matrix—
but to this we must add the cost of identifying the pivot blocks E and performing the
permutations , which can be considerable. There are various strategies for determining
the pivot blocks, which have an important effect on both the cost of the factorization and its
numerical properties. Ideally, our strategy for choosing E at each step of the factorization
procedure should be inexpensive, should lead to at most modest growth in the elements of
the remaining matrix at each step of the factorization, and should not lead to excessive ﬁll-in
(that is, L should not be too much more dense than A).
A well-known strategy, due to Bunch and Parlett [31], searches the whole working
matrix and identiﬁes the largest-magnitude diagonal and largest-magnitude off-diagonal
elements, denoting their respective magnitudes by ξdia and ξoff. If the diagonal element
whose magnitude is ξdia is selected to be a 1 × 1 pivot block, the element growth in the
remaining matrix is bounded by the ratio ξdia/ξoff. If this growth rate is acceptable, we
choose this diagonal element to be the pivot block. Otherwise, we select the off-diagonal
element whose magnitude is ξoff (aij, say), and choose E to be the 2 × 2 submatrix that

6 . 3 .
H e s s i a n M o d i f i c a t i o n s
153
includes this element, that is,
E 

aii
aij
aij
ajj

.
This pivoting strategy of Bunch and Parlett is numerically stable and guarantees to yield a
matrix L whose maximum element is bounded by 2.781. Its drawback is that the evaluation
of ξdia and ξoff at each iteration requires many comparisons between ﬂoating-point numbers
to be performed: O(n3) in total during the overall factorization. Since each comparison costs
roughly the same as an arithmetic operation, this overhead is not insigniﬁcant, and the total
run time may be roughly twice as long as that of Algorithm 6.5.
The more economical pivoting strategy of Bunch and Kaufman [30] searches at most
two columns of the working matrix at each stage and requires just O(n2) comparisons in
total. Its details are somewhat tricky, and we refer the interested reader to the original paper
or to Golub and Van Loan [115, Section 4.4] for details. Unfortunately, this algorithm can
give rise to arbitrarily large elements in the lower triangular factor L, making it unsuitable
for use with a modiﬁed Cholesky strategy.
The bounded Bunch–Kaufman strategy is essentially a compromise between the
Bunch–Parlett and Bunch–Kaufman strategies. It monitors the sizes of elements in L, ac-
cepting the (inexpensive) Bunch–Kaufman choice of pivot block when it yields only modest
element growth, but searching further for an acceptable pivot when this growth is exces-
sive. Its total cost is usually similar to that of Bunch–Kaufman, but in the worst case it can
approach the cost of Bunch–Parlett.
So far, we have ignored the effect of the choice of pivot block E on the sparsity of the
ﬁnal L factor. This consideration is an important one when the matrix to be factored is large
and sparse, since it greatly affects both the CPU time and the amount of storage required by
the algorithm. Algorithms that modify the strategies above to take account of sparsity have
been proposed by Duff et al. [76], Duff and Reid [74], and Fourer and Mehrotra [93].
As for the Cholesky factorization, the efﬁcient and numerically stable indeﬁnite sym-
metric factorization algorithms discussed above can be modiﬁed to ensure that the modiﬁed
factors are the factors of a positive deﬁnite matrix. The strategy is ﬁrst to compute the factor-
ization (6.20), as well as the spectral decomposition B  QQT , which is very inexpensive
to compute because B is block diagonal (Exercise 6). Then we construct a modiﬁcation
matrix F such that
L(B + F)LT
is sufﬁciently positive deﬁnite. Motivated by the modiﬁed spectral decomposition (6.12) we
will choose a parameter δ > 0 and deﬁne F to be
F  Q diag(τi) QT ,
τi 

0,
λi ≥δ,
δ −λi,
λi < δ,
i  1, 2, . . . , n,
(6.23)

154
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
where λi are the eigenvalues of B. The matrix F is thus the modiﬁcation of minimum
Frobenius norm that ensures that all eigenvalues of the modiﬁed matrix B + F are no less
than δ. This strategy therefore modiﬁes the factorization (6.20) as follows:
P (A + E)P T  L(B + F)LT ,
where E  P T LFLT P T ;
note that E will not be diagonal, in general. Hence, in contrast to the modiﬁed Cholesky
approach, this modiﬁcation strategy changes the entire matrix A and not just its diagonal.
The aim of strategy (6.23) is that the modiﬁed matrix satisﬁes λmin(A + E) ≈δ whenever
the original matrix A has λmin(A) < δ. It is not clear, however, whether it always comes
close to attaining this goal.
6.4
TRUST-REGION NEWTON METHODS
Unlikelinesearchmethods,trust-regionmethodsdonotrequiretheHessianofthequadratic
model to be positive deﬁnite. Therefore, we can use the exact Hessian Bk  ∇2f (xk) directly
in this model and obtain steps pk by solving
min
p∈IRn mk(p)
def fk + ∇fk
T p + 1
2pT Bkp
s.t. ∥p∥≤k.
(6.24)
In Chapter 4 we described a variety of techniques for ﬁnding an approximate or accurate
solution of this subproblem. Most of these techniques apply when Bk is any symmetric
matrix, and we do not need to say much about the speciﬁc case in which Bk  ∇2f (xk). We
will, however, pay attention to the implementation of these trust-region Newton methods
when the number of variables is large. Four techniques for solving (6.24) will be discussed:
(i) the dogleg method; (ii) two-dimensional subspace minimization; (iii) accurate solution
using iteration; (iv) the conjugate gradient (CG) method. For the CG-based method, we
study the choice of norm deﬁning the trust region, which can be viewed as a means of
preconditioning the subproblem.
NEWTON–DOGLEG AND SUBSPACE-MINIMIZATION METHODS
If Bk is positive deﬁnite, the dogleg method described in Chapter 4 provides an ap-
proximate solution of (6.24) that is relatively inexpensive to compute and good enough to
allow the method to be robust and rapidly convergent. However, since the Hessian matrix
may not always be positive deﬁnite, the dogleg method is not directly applicable. To adapt it
for the minimization of nonconvex functions, we can use one of the Hessian modiﬁcations
described in Section 6.3 as Bk, in place of the true Hessian ∇2f (xk), thus guaranteeing that
we are working with a convex quadratic function in (6.24). This strategy for choosing Bk

6 . 4 .
T r u s t - R e g i o n N e w t o n M e t h o d s
155
and for ﬁnding an approximation Newton step allows us to implement the dogleg method
exactly as in Chapter 4. That is, we choose the approximate solution of (6.24) to be the
minimizer of the modiﬁed model function mk in (6.24) along the dogleg path deﬁned by
˜p(τ) 

τp
U,
0 ≤τ ≤1,
p
U + (τ −1)(p
B −p
U),
1 ≤τ ≤2,
,
(6.25)
where pU is deﬁned as in (4.12) and pB is the unconstrained minimizer of (6.24); see
Figure 4.3.
Similarly, the two-dimensional subspace minimization algorithm of Chapter 4 can
also be applied, when Bk is the exact Hessian or a modiﬁcation that ensures its positive
deﬁniteness, to ﬁnd an approximate solution of (6.24), as
min
p
mk(p)  fk + ∇fk
T p + 1
2pT Bkp
s.t. ∥p∥≤k, p ∈span {∇fk, pB}.
(6.26)
The dogleg and two-dimensional subspace minimization approaches have the advan-
tage that all the linear algebra computations can be performed with direct linear solvers, a
feature that can be important in some applications. They are also globally convergent, as
shown in Chapter 4. We hope, too, that the modiﬁed factorization strategy used to obtain
Bk (and hence pB) does not modify the exact Hessian ∇2f (xk) when it is sufﬁciently posi-
tive deﬁnite, allowing the rapid local convergence that characterizes Newton’s method to be
observed.
The use of a modiﬁed factorization in the dogleg method is not completely satisfying
from an intuitive viewpoint, however. A modiﬁed factorization perturbs ∇2f (xk) in a some-
what random manner, giving more weight to certain directions than others, and the beneﬁts
of the trust-region approach may not be realized. In fact, the modiﬁcation introduced during
the factorization of the Hessian is redundant in some sense because the trust-region strategy
introduces its own modiﬁcation: The solution of the trust-region problem results in the
factorization of the positive (semideﬁnite) matrix ∇2f (xk) + λI, where λ depends on the
size of the trust-region radius k.
We conclude that the dogleg method is most appropriate when the objective function
is convex (that is, ∇2f (xk) is always positive semideﬁnite). The techniques described next
may be more suitable for the general case.
ACCURATE SOLUTION OF THE TRUST-REGION PROBLEM
We can also ﬁnd a solution of (6.24) using Algorithm 4.4 described in Chapter 4. This
algorithm requires the repeated factorization of matrices of the form Bk + λI, for different
valuesofλ.Practicalexperienceindicatesthatbetween1and3suchsystemsneedtobesolved,
onaverage,ateachiteration,sothecostofthisapproachmaynotbeprohibitive.Theresulting

156
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
trust-region algorithm is quite robust—it can be expected to converge to a minimizer, not
just to a stationary point. In some large-scale situations, however, the requirement of solving
more than one linear system per iteration may become onerous.
TRUST-REGION NEWTON–CG METHOD
We can approximately solve the trust-region problem (6.24) by means of the conjugate
gradient method (CG) with the termination tests proposed by Steihaug; see the CG–Steihaug
algorithm, Algorithm 4.3 in Chapter 4. The step computation of this Newton–CG algorithm
is obtained by setting Bk  ∇2f (xk) at every iteration in Algorithm 4.3. This procedure
amounts to applying the CG method to the system
Bkpk  −∇fk
(6.27)
and stopping if (i) the size of the approximate solution exceeds the trust-region radius, (ii)
the system (6.27) has been solved to a required accuracy, or (iii) if negative curvature is
encountered. In the latter case we follow the direction of negative curvature to the boundary
of the trust region ∥p∥≤k. This is therefore the trust-region analogue of Algorithm 6.1.
Careful control of the accuracy in the inner CG iteration is crucial to keeping the cost
of the Newton–CG method as low as possible. Near a well-behaved solution x∗, the trust-
region constraint becomes inactive, and the Newton–CG method reduces to the inexact
Newton method analyzed in Section 6.1. The results of that section, which relate the choice
of forcing sequence {ηk} to the rate of convergence, become relevant during the later stages
of the Newton–CG method.
As discussed in the context of the line search Newton–CG method, it is not necessary
to have explicit knowledge of the Hessian matrix, but we can compute products of the form
∇2f (xk)v by automatic differentiation or using the ﬁnite difference approximation (6.7).
Once again, the result is a Hessian-free method.
The trust-region Newton–CG method has a number of attractive computational
and theoretical properties. First, it is globally convergent. Its ﬁrst step along the direc-
tion −∇f (xk) identiﬁes the Cauchy point for the subproblem (6.24) (see (4.5)), and any
subsequent CG iterates only serve to improve the model value. Second, it requires no ma-
trix factorizations, so we can exploit the sparsity structure of the Hessian ∇2f (xk) without
worrying about ﬁll-in during a direct factorization. Moreover, the CG iteration—the most
computationally intensive part of the algorithm—may be executed in parallel, since the
key operation is a matrix–vector product. When the Hessian matrix is positive deﬁnite, the
Newton–CG method approximates the pure Newton step more and more closely as the
solution x∗is approached, so rapid convergence is also possible, as discussed above.
Two advantages, compared with the line search Newton–CG method, are that the
lengthsofthestepsarecontrolledbythetrustregionandthatdirectionsofnegativecurvature
are explored. Our computational experience shows that the latter is beneﬁcial in practice, as
it sometimes allows the iterates to move away from nonminimizing stationary points.

6 . 4 .
T r u s t - R e g i o n N e w t o n M e t h o d s
157
A limitation of the trust-region Newton–CG method is that it accepts any direction of
negative curvature, even when this direction gives an insigniﬁcant reduction in the model.
Consider, for example, the case where the subproblem (6.24) is
m(p)  10−3p1 + 10−4p2
1 −p2
2
subject to ∥p∥≤1,
where subscripts indicate elements of the vector p. The steepest descent direction at p  0
is (10−3, 0)T , which is a direction of negative curvature for the model. Algorithm 4.3 would
follow this direction to the boundary of the trust region, yielding a reduction in model
function m of about 10−3. A step along e2—also a direction of negative curvature—would
yield a much greater reduction of 1.
Several remedies have been proposed. We have seen in Chapter 4 that when the Hessian
∇2f (xk) contains negative eigenvalues, the search direction should have a signiﬁcant com-
ponent along the eigenvector corresponding to the most negative eigenvalue of ∇2f (xk).
This feature would allow the algorithm to move away rapidly from stationary points that
are not minimizers. A variation of the Newton–CG method that overcomes this drawback
uses the Lanczos method, rather than CG, to solve the linear system (6.27). This approach
does not terminate after encountering the ﬁrst direction of negative curvature, but continues
in search of a direction of sufﬁciently negative curvature; see [177], [121]. The additional
robustness in the trust-region algorithm comes at the cost of a more expensive solution of
the subproblem.
PRECONDITIONING THE NEWTON–CG METHOD
The unpreconditioned CG method can be inefﬁcient when the Hessian is ill-
conditioned, and may even fail to reach the desired accuracy. Hence, it is important to
introduce preconditioning techniques into the CG iteration. Such techniques are based on
ﬁnding a nonsingular matrix D such that the eigenvalues of D−T BkD have a more favorable
distribution, as discussed in Chapter 5.
The use of preconditioning in the Newton–CG method requires care because the
preconditioned CG iteration no longer generates iterates of increasingℓ2 norm; this property
holds only for the unpreconditioned CG iteration (see Theorem 4.2). Thus we cannot simply
terminate the preconditioned CG method as soon as the iterates reach the boundary of the
trust region ∥p∥≤k, since later CG iterates could return to the trust region.
However, there exists a weighted ℓ2 norm in which the magnitudes of the precon-
ditioned CG iterates grow monotonically. This will allow us to develop an extension of
Algorithm 4.3 of Chapter 4. (Not surprisingly, the weighting of the norm depends on the
preconditioner D!) Consider the subproblem
min
p∈IRn mk(p)
def fk + ∇fk
T p + 1
2pT Bkp
s.t. ∥Dp∥≤k.
(6.28)

158
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
By making the change of variables ˆp  Dp and deﬁning
ˆgk  D−T ∇fk,
ˆBk  D−T BkD−1,
we can write (6.28) as
min
ˆp∈IRn fk + ˆgT
k ˆp + 1
2 ˆpT ˆBk ˆp
s.t. ∥ˆp∥≤k,
which has exactly the form of (6.24). We can now apply the CG algorithm without any
modiﬁcation to this subproblem. Algorithm 4.3 now monitors the norm ∥ˆp∥ ∥Dp∥,
which is the quantity that grows monotonically during the CG iteration, and terminates
when ∥Dp∥exceeds the bound k applied in (6.28).
Many preconditioners can be used within this framework; we discuss some of them in
Chapter 5. Of particular interest is incomplete Cholesky factorization, which has proved to
be useful in a wide range of optimization problems. The incomplete Cholesky factorization
of a positive deﬁnite matrix B ﬁnds a lower triangular matrix L such that
B  LLT −R,
where the amount of ﬁll-in in L is restricted in some way. (For instance, it is constrained
to have the same sparsity structure as the lower triangular part of B, or is allowed to have a
number of nonzero entries similar to that in B.) The matrix R accounts for the “inexactness”
in the approximate factorization. The situation is complicated a little further by the possible
indeﬁniteness of the Hessian ∇2f (xk); we must be able to handle this indeﬁniteness as well
as maintain the sparsity. The following algorithm combines incomplete Cholesky and a form
of modiﬁed Cholesky to deﬁne a preconditioner for Newton–CG methods.
Algorithm 6.6 (Inexact Modiﬁed Cholesky).
(scale B)
Compute T  diag(∥Bei∥), where ei is the ith coordinate vector;
¯B ←T −1/2BT −1/2; β ←∥¯B∥;
(compute a shift to ensure positive deﬁniteness)
if
mini bii > 0
α0 ←0
else
α0 ←β/2;
end
for
k  0, 1, 2, . . .
Attempt to apply incomplete Cholesky algorithm to obtain
LLT  ¯B + αkI;

6 . 4 .
T r u s t - R e g i o n N e w t o n M e t h o d s
159
if
factorization is completed successfully
stop and return L
else
αk+1 ←max(2αk, β/2);
end
end
We can then set the preconditioner to be D  L, where L is the lower triangular matrix
output from Algorithm 6.6. A Newton–CG trust region method using this preconditioner
is implemented in the forthcoming MINPACK-2 package under the name NMTR. The
LANCELOT package also contains a Newton–CG method that makes use of slightly different
preconditioning techniques.
LOCAL CONVERGENCE OF TRUST-REGION NEWTON METHODS
Since global convergence of trust-region methods that incorporate (possibly inexact)
Newton steps is established above, we turn our attention now to local convergence. The key
to attaining the fast rate of convergence associated with Newton’s method is to show that
the trust-region bound eventually does not interfere with the convergence. That is, when we
reach the vicinity of a solution, the (approximate) solution of the subproblem is well inside
the region deﬁned by the trust-region constraint ∥p∥≤k and becomes closer and closer
to the pure Newton step. Sequences of steps that satisfy the latter property are said to be
asymptotically exact.
WeﬁrstproveageneralresultthatappliestoanyalgorithmoftheformofAlgorithm4.1
(see Chapter 4) that generates asymptotically exact steps whenever the true Newton step is
well inside the trust region. It shows that the trust-region constraint eventually becomes
inactive in algorithms with this property. The result assumes that the exact Hessian Bk 
∇2f (xk) is used in (6.24) when xk is close to a solution x∗that satisﬁes second-order
conditions. Moreover, it assumes that the algorithm uses an approximate solution pk of
(6.24) that achieves at least the same decrease in the model function mk as the Cauchy step.
(All the methods discussed above satisfy this condition.)
Theorem 6.4.
Let f be twice Lipschitz continuously differentiable. Suppose the sequence {xk} converges
to a point x∗that satisﬁes the second-order sufﬁcient conditions (Theorem 2.4) and that for all k
sufﬁciently large, the trust-region algorithm based on (6.24) with Bk  ∇2f (xk) chooses steps
pk that achieve at least the same reduction as the Cauchy point (that is, mk(pk) ≤mk(pC
k))
and are asymptotically exact whenever ∥pN
k∥≤1
2k, that is,
∥pk −pN
k∥ o(∥pN
k∥).
(6.29)
Then the trust-region bound k becomes inactive for all k sufﬁciently large.

160
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
Proof.
We show that ∥pN
k∥≤
1
2k and ∥pk∥≤k, for all sufﬁciently large k, so the
near-optimal step pk in (6.29) will eventually always be taken.
First, we seek a lower bound on the predicted reduction mk(0) −mk(pk) for all sufﬁ-
ciently large k. We assume that k is large enough that the o(∥pN
k∥) term in (6.29) is less than
∥pN
k∥. When ∥pN
k∥≤1
2k, we then have that ∥pk∥≤∥pN
k∥+ o(∥pN
k∥) ≤2∥pN
k∥, while if
∥pN
k∥> 1
2k, we have ∥pk∥≤k < 2∥pN
k∥. In both cases, then, we have
∥pk∥≤2∥pN
k∥≤2∥∇2f (xk)
−1∥∥∇f (xk)∥,
and so ∥∇f (xk)∥≥1
2∥pk∥/∥∇2f −1
k ∥.
Because we assume that the step pk achieves at least the same decrease as the Cauchy
point, we have from the relation (4.34) that there is a constant c1 > 0 such that
mk(0) −mk(pk) ≥c1∥∇f (xk)∥min

k, ∥∇f (xk)∥
∥Bk∥

≥c1
∥pk∥
2∥∇2f (xk)−1∥
min

∥pk∥,
∥pk∥
2∥∇2f (xk)∥∥∇2f (xk)−1∥

 c1
∥pk∥2
4∥∇2f (xk)−1∥2∥∇2f (xk)∥
.
Because xk →x∗, then by continuity of ∇2f (x) and positive deﬁniteness of ∇2f (x∗), we
have for k sufﬁciently large that
c1
4∥∇2f (xk)−1∥2∥∇2f (xk)∥
≥
c1
8∥∇2f (x∗)−1∥2∥∇2f (x∗)∥
def c3,
and therefore
mk(0) −mk(pk) ≥c3∥pk∥2
(6.30)
for all sufﬁciently large k. By Lipschitz continuity of ∇2f (x), we have by the argument
leading to (4.41) that
|(f (xk) −f (xk + pk)) −(mk(0) −mk(pk))| ≤L
2 ∥pk∥3,
where L > 0 is the Lipschitz constant for ∇2f (·) in the neighborhood of x∗. Hence, by
deﬁnition of ρk (see (4.4)), we have for sufﬁciently large k that
|ρk −1| ≤∥pk∥3(L/2)
c3∥pk∥2
 L
2c3
∥pk∥≤L
2c3
k.
(6.31)

6 . 4 .
T r u s t - R e g i o n N e w t o n M e t h o d s
161
Now, the trust-region radius can be reduced only if ρk < 1
4 (or some other ﬁxed number less
than 1), so it is clear from (6.31) that there is a threshold ˜ such that k cannot be reduced
further once it falls below ˜. Hence, the sequence {k} is bounded away from zero. Since
xk →x∗, we have ∥pN
k∥→0 and therefore ∥pk∥→0 from (6.29). Hence the trust-region
bound is inactive for all k sufﬁciently large, and our proof is complete.
□
The conditions of Theorem 6.4 are satisﬁed trivially if pk  pN
k. In addition, all three
algorithms discussed above also satisfy the assumptions of this theorem. We state this result
formally as follows.
Lemma 6.5.
Suppose that xk →x∗, where x∗satisﬁes the second-order sufﬁcient conditions of The-
orem 2.4. Consider versions of Algorithm TR in which the inexact dogleg method (6.25) or the
inexact two-dimensional subspace minimization method (6.26) with Bk  ∇2f (xk) is used to
obtain an approximate step pk. Then for all sufﬁciently large k, the unconstrained minimum of
the models (6.25) and (6.26) is simply pN
k, so the conditions of Theorem 6.4 are satisﬁed.
When the termination criterion (6.3) is used with ηk →0 in the Newton–CG method
(along with termination when the trust-region bound is exceeded or when negative curvature
is detected), then this algorithm also satisﬁes the conditions of Theorem 6.4.
Proof.
For the case of the dogleg and two-dimensional subspace minimization methods,
the exact step pN
k is one of the candidates for pk—it lies inside the trust region, along the
dogleg path, and inside the two-dimensional subspace. Since in fact, pN
k is the minimizer of
mk inside the trust region for k sufﬁciently large (since Bk  ∇2f (xk) is positive deﬁnite),
it is certainly the minimizer in these more restricted domains, so we have pk  pN
k.
For the Newton–CG method, the method does not terminate by ﬁnding a negative
curvature direction, because ∇2f (xk) is positive deﬁnite for all k sufﬁciently large. Also,
since the norms of the CG iterates increase with iteration number, they do not exceed pN
k,
and hence stay inside the trust region. Hence, the CG iterations can terminate only because
condition (6.3) is satisﬁed. Hence from (6.1), (6.2), and (6.3), we have that
∥pk −pN
k∥≤∥∇2f (xk)
−1∥∥∇2f (xk)pk −∇2f (xk)pN
k∥
 ∥∇2f (xk)
−1∥∥rk∥
≤ηk∥∇2f (xk)
−1∥∥∇f (xk)∥
≤ηk∥∇2f (xk)
−1∥∥∇2f (xk)∥∥pN
k∥.
Hence, the condition (6.29) is satisﬁed.
□
The nearly exact algorithm of Chapter 4—Algorithm 4.4—also satisﬁes the conditions
of Theorem 6.4, since if the true Newton step pN
k lies inside the trust region, the initial guesses

162
C h a p t e r
6 .
P r a c t i c a l N e w t o n M e t h o d s
λ  0andpk  pN
k willeventuallysatisfyanyreasonableterminationcriteriaforthismethod
of determining the step.
Rapidconvergenceofallthesealgorithmsnowfollowsimmediatelyfromtheireventual
similaritytoNewton’smethod.ThemethodsthateventuallytakeexactNewtonstepspk  pN
k
converge quadratically. The asymptotic convergence rate of the Newton–CG algorithm with
termination test (6.3) depends on the rate of convergence of the forcing sequence {ηk} to
zero, as described in Theorem 6.2.
NOTES AND REFERENCES
Newton methods in which the step is computed by an iterative algorithm have received
much attention; see Sherman [229] and Ortega and Rheinboldt [185]. Our discussion of
inexact Newton methods is based on Dembo, Eisenstat, and Steihaug [66].
ForamorethoroughtreatmentofthemodiﬁedCholeskyfactorizationseeGill,Murray,
and Wright [104] or Dennis and Schnabel [138]. The modiﬁed Cholesky factorization based
on Gershgorin disk estimates is described in Schnabel and Eskow [223]. The modiﬁed
indeﬁnite factorization is from Cheng and Higham [41].
Another strategy for implementing a line search Newton method when the Hessian
contains negative eigenvalues is to compute a direction of negative curvature and use it to
deﬁne the search direction (see Mor´e and Sorensen [169] and Goldfarb [113]).
✐
E x e r c i s e s
✐
6.1 ProgramapureNewtoniterationwithoutlinesearches,wherethesearchdirection
is computed by the CG method. Select stopping criteria such that the rate of convergence
is linear, superlinear, and quadratic. Try your program on the following convex quartic
function:
f (x)  1
2xT x + 0.25σ(xT Ax)2,
(6.32)
where σ is a parameter and
A 


5
1
0
0.5
1
4
0.5
0
0
0.5
3
0
0.5
0
0
2


.

6 . 4 .
T r u s t - R e g i o n N e w t o n M e t h o d s
163
This is a strictly convex function that allows us to control the deviation from a quadratic by
means of the parameter σ. The starting point is
x1  (cos 70◦, sin 70◦, cos 70◦, sin 70◦)T .
Try σ  1 or larger values and observe the rate of convergence of the iteration.
✐
6.2 Consider the line search Newton–CG method described in Section 6.2. Use the
propertiesoftheCGmethoddescribedinChapter5toprovethatthesearchNewtondirection
pk is always a descent direction for f .
✐
6.3 Compute the eigenvalues of the 2 diagonal blocks of (6.21), and verify that each
block has a positive and diagonal eigenvalue. Then compute the eigenvalues of A and verify
that its inertia is the same as that of B.
✐
6.4 Describe the effect that the modiﬁed Cholesky factorization of Algorithm 6.5
would have on the Hessian ∇2f (xk)  diag(−2, 12, 4).
✐
6.5 Program a trust-region Newton–CG method in which the step is computed by
the CG–Steihaug method (Algorithm 4.3), without preconditioning. Select the constants so
that the rate of convergence is superlinear. Try it on (6.32). Then deﬁne an objective function
that has negative curvature in a neighborhood of the starting point x0  0, and observe the
effect of the negative curvature steps.
✐
6.6 Prove Theorem 6.2.
✐
6.7 Consider a block diagonal matrix B with 1 × 1 and 2 × 2 blocks. Show that the
eigenvalues and eigenvectors of B can be obtained by computing the spectral decomposition
of each diagonal block separately.

Chapter7

Calculating
Derivatives
Most algorithms for nonlinear optimization and nonlinear equations require knowledge of
derivatives. Sometimes the derivatives are easy to calculate by hand, and it is reasonable
to expect the user to provide code to compute them. In other cases, the functions are too
complicated, so we look for ways to calculate or approximate the derivatives automatically.
A number of interesting approaches are available, of which the most important are probably
the following.
Finite Differencing. This technique has its roots in Taylor’s theorem (see Chapter 2). By
observing the change in function values in response to small perturbations of the unknowns
near a given point x, we can estimate the response to inﬁntesimal perturbations, that is, the
derivatives. For instance, the partial derivative of a smooth function f : IRn →IR with
respect to the ith variable xi can be approximated by the central-difference formula
∂f
∂xi
≈f (x + ϵei) −f (x −ϵei)
2ϵ
,
where ϵ is a small positive scalar and ei is the ith unit vector.

166
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
AutomaticDifferentiation.Thistechniquetakestheviewthatthecomputercodeforevaluat-
ingthefunctioncanbebrokendownintoacompositionofelementaryarithmeticoperations,
to which the chain rule (one of the basic rules of calculus) can be applied. Some software
tools for automatic differentiation (such as ADIFOR [13]) produce new code that calculates
both function and derivative values. Other tools (such as ADOL-C [126]) keep a record
of the elementary computations that take place while the function evaluation code for a
given point x is executing on the computer. This information is processed to produce the
derivatives at the same point x.
Symbolic Differentiation. In this technique, the algebraic speciﬁcation for the function f is
manipulated by symbolic manipulation tools to produce new algebraic expressions for each
component of the gradient. Commonly used symbolic manipulation tools can be found in
the packages Mathematica [248], Maple [243], and Macsyma [153].
In this chapter we discuss the ﬁrst two approaches: ﬁnite differencing and automatic
differentiation.
The usefulness of derivatives is not restricted to algorithms for optimization. Modelers
in areas such as design optimization and economics are often interested in performing post-
optimal sensitivity analysis, in which they determine the sensitivity of the optimum to small
perturbations in the parameter or constraint values. Derivatives are also important in other
areas of numerical analysis such as nonlinear differential equations.
7.1
FINITE-DIFFERENCE DERIVATIVE APPROXIMATIONS
Finite differencing is an approach to the calculation of approximate derivatives whose moti-
vation (like that of so many other algorithms in optimization) comes from Taylor’s theorem.
Many software packages perform automatic calculation of ﬁnite differences whenever the
user is unable or unwilling to supply code to calculate exact derivatives. Although they yield
only approximate values for the derivatives, the results are adequate in many situations.
Bydeﬁnition,derivativesareameasureofthesensitivityofthefunctiontoinﬁnitesimal
changes in the values of the variables. Our approach in this section is to make small but ﬁnite
perturbations in the values of x and examine the resulting differences in the function values.
By taking ratios of the function difference to variable difference, we obtain approximations
to the derivatives.
APPROXIMATING THE GRADIENT
An approximation to the gradient vector ∇f (x) can be obtained by evaluating the
function f at (n + 1) points and performing some elementary arithmetic. We describe this
technique, along with a more accurate variant that requires additional function evaluations.

7 . 1 .
F i n i t e - D i f f e r e n c e D e r i v a t i v e A p p r o x i m a t i o n s
167
A popular formula for approximating the partial derivative ∂f/∂xi at a given point x
is the forward-difference, or one-sided-difference, approximation, deﬁned as
∂f
∂xi
(x) ≈f (x + ϵei) −f (x)
ϵ
.
(7.1)
The gradient can be built up by simply applying this formula for i  1, 2, . . . , n. This
process requires evaluation of f at the point x as well as the n perturbed points x + ϵei,
i  1, 2, . . . , n: a total of (n + 1) points.
The basis for the formula (7.1) is Taylor’s theorem, which we stated as Theorem 2.1 in
Chapter 2. When f is twice continuously differentiable, we have
f (x + p)  f (x) + ∇f (x)T p + 1
2pT ∇2f (x + tp)p,
some t ∈(0, 1)
(7.2)
(see (2.6)). If we choose L to be a bound on the size of ∥∇2f (·)∥in the region of interest,
it follows directly from this formula that the last term in this expression is bounded by
(L/2)∥p∥2, so that
f (x + p) −f (x) −∇f (x)T p
 ≤(L/2)∥p∥2.
(7.3)
We now choose the vector p to be ϵei, so that it represents a small change in the value
of a single component of x (the ith component). For this p, we have that ∇f (x)T p 
∇f (x)T ei  ∂f/∂xi, so by rearranging (7.3), we conclude that
∂f
∂xi
(x)  f (x + ϵei) −f (x)
ϵ
+ δϵ,
where |δϵ| ≤(L/2)ϵ.
(7.4)
We derive the forward-difference formula (7.1) by simply ignoring the error term δϵ in this
expression, which becomes smaller and smaller as ϵ approaches zero.
An important issue in implementing the formula (7.1) is the choice of the parameter
ϵ. The error expression (7.4) suggests that we should choose ϵ as small as possible. Unfor-
tunately, this expression ignores the roundoff errors that are introduced when the function
f is evaluated on a real computer, in ﬂoating-point arithmetic. From our discussion in the
Appendix (see (A.58) and (A.59)), we know that the quantity u known as unit roundoff is cru-
cial: It is a bound on the relative error that is introduced whenever an arithmetic operation is
performed on two ﬂoating-point numbers. (Typically, u is about 10−16 in double-precision
arithmetic.) The effect of these errors on the ﬁnal computed value of f depends on the way
in which f is computed. It could come from an arithmetic formula, or from a differential
equation solver, with or without reﬁnement.
As a rough estimate, let us assume simply that the relative error in the computed f is
bounded by u, so that the computed values of f (x) and f (x + ϵei) are related to the exact

168
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
values in the following way:
|comp(f (x)) −f (x)| ≤uLf ,
|comp(f (x + ϵei)) −f (x + ϵei)| ≤uLf ,
where comp(·) denotes the computed value, and Lf is a bound on the value of |f (·)| in the
region of interest. If we use these computed values of f in place of the exact values in (7.4)
and (7.1), we obtain an error that is bounded by
(L/2)ϵ + 2uLf /ϵ.
(7.5)
Naturally, we would like to choose ϵ to make this error as small as possible; it is easy to see
that the minimizing value is
ϵ2  4Lf u
L
.
If we assume that the problem is well scaled, then the ratio Lf /L (the ratio of function
values to second derivative values) does not exceed a modest size. We can conclude that the
following choice of ϵ is fairly close to optimal:
ϵ  √u.
(7.6)
(In fact, this value is used in many of the optimization software packages that use ﬁnite
differencing as an option for estimating derivatives.) For this value of ϵ, we have from (7.5)
that the total error in the forward-difference approximation is fairly close to √u.
A more accurate approximation to the derivative can be obtained by using the central-
difference formula, deﬁned as
∂f
∂xi
(x) ≈f (x + ϵei) −f (x −ϵei)
2ϵ
.
(7.7)
As we show below, this approximation is more accurate than the forward-difference approx-
imation (7.1). It is also about twice as expensive, since we need to evaluate f at the points
x + ϵei, i  1, 2, . . . , n, and x −ϵei, i  1, 2, . . . , n: a total of 2n points.
The basis for the central difference approximation is again Taylor’s theorem. When
the second derivatives of f exist and are Lipschitz continuous, we have from (7.2) that
f (x + p)  f (x) + ∇f (x)T p + 1
2pT ∇2f (x + tp)p
for some t ∈(0, 1)
 f (x) + ∇f (x)T p + 1
2pT ∇2f (x)p + O
	
∥p∥3
.
(7.8)

7 . 1 .
F i n i t e - D i f f e r e n c e D e r i v a t i v e A p p r o x i m a t i o n s
169
By setting p  ϵei and p  −ϵei, respectively, we obtain
f (x + ϵei)  f (x) + ϵ ∂f
∂xi
+ 1
2ϵ2 ∂2f
∂x2
i
+ O
	
ϵ3
,
f (x −ϵei)  f (x) −ϵ ∂f
∂xi
+ 1
2ϵ2 ∂2f
∂x2
i
+ O
	
ϵ3
.
(Note that the ﬁnal error terms in these two expressions are generally not the same, but they
are both bounded by some multiple of ϵ3.) By subtracting the second equation from the ﬁrst
and dividing by 2ϵ, we obtain the expression
∂f
∂xi
(x)  f (x + ϵei) −f (x −ϵei)
2ϵ
+ O
	
ϵ2
.
We see from this expression that the error is O
	
ϵ2
, as compared to the O(ϵ) error in
the forward-difference formula (7.1). However, when we take evaluation error in f into
account,theaccuracythatcanbeachievedinpracticeislessimpressive;thesameassumptions
that were used to derive (7.6) lead to the estimate ϵ  u2/3 (see the exercises). In some
applications, however, the extra few digits of accuracy may improve the performance of the
algorithm enough to make the extra expense worthwhile.
APPROXIMATING A SPARSE JACOBIAN
Consider now the case of a vector function r : IRn →IRm, such as the residual vector
that we consider in Chapter 10 or the system of nonlinear equations from Chapter 11. The
matrix J(x) of ﬁrst derivatives for this function is shown in (7.34). The techniques described
in the previous section can be used to evaluate the full Jacobian J(x) one column at a time.
When r is twice continuously differentiable, we can use Taylor’s theorem to deduce that
∥r(x + p) −r(x) −J(x)p∥≤(L/2)∥p∥2,
(7.9)
where L is a Lipschitz constant for J in the region of interest. If we require an approximation
to the Jacobian–vector product J(x)p for a given vector p (as is the case with inexact Newton
methods for nonlinear systems of equations; see Section 11.1), this expression immediately
suggests choosing a small nonzero ϵ and setting
J(x)p ≈r(x + ϵp) −r(x)
ϵ
,
(7.10)
an approximation that is accurate to O(ϵ). (A two-sided approximation can be derived from
the formula (7.7).)

170
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
If an approximation to the full Jacobian J(x) is required, we can compute it a column
at a time, analogously to (7.1), by setting set p  ϵei in (7.9) to derive the following estimate
of the ith column:
∂r
∂xi
(x) ≈r(x + ϵei) −r(x)
ϵ
.
(7.11)
A full Jacobian estimate can be obtained at a cost of n + 1 evaluations of the function r.
When the Jacobian is sparse, however, we can often obtain the estimate at a much lower cost,
sometimes just three or four evaluations of r. The key is to estimate a number of different
columns of the Jacobian simultaneously, by judicious choices of the perturbation vector p
in (7.9).
Weillustratethetechniquewithasimpleexample.Considerthefunctionr : IRn →IRn
deﬁned by
r(x) 


2(x3
2 −x2
1)
3(x3
2 −x2
1) + 2(x3
3 −x2
2)
3(x3
3 −x2
2) + 2(x3
4 −x2
3)
...
3(x3
n −x2
n−1)


.
(7.12)
Each component of r depends on just two or three components of x, so that each row of the
Jacobian contains just two or three nonzero elements. For the case of n  6, the Jacobian
has the following structure:


×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×


,
(7.13)
where each cross represents a nonzero element, and zeros are represented by blank space.
Staying for the moment with the case n  6, suppose that we wish to compute a ﬁnite-
difference approximation to the Jacobian. (Of course, it is easy to calculate this particular
Jacobian by hand, but there are complicated functions with similar structure for which hand
calculation is more difﬁcult.) A perturbation p  ϵe1 to the ﬁrst component of x will affect
only the ﬁrst and second components of r. The remaining components will be unchanged, so
that the right-hand-side of formula (7.11) will correctly evaluate to zero in the components
3, 4, 5, 6. It is wasteful, however, to reevaluate these components of r when we know in

7 . 1 .
F i n i t e - D i f f e r e n c e D e r i v a t i v e A p p r o x i m a t i o n s
171
advance that their values are not affected by the perturbation. Instead, we look for a way to
modify the perturbation vector so that it does not have any further effect on components 1
and 2, but does produce a change in some of the components 3, 4, 5, 6, which we can then
use as the basis of a ﬁnite-difference estimate for some other column of the Jacobian. It is not
hard to see that the additional perturbation ϵe4 has the desired property: It alters the 3rd,
4th, and 5th elements of r, but leaves the 1st and 2nd elements unchanged. The changes in
r as a result of the perturbations ϵe1 and ϵe4 do not interfere with each other.
To express this discussion in mathematical terms, we set
p  ϵ(e1 + e4),
and note that
r(x + p)1,2  r(x + ϵ(e1 + e4))1,2  r(x + ϵe1)1,2
(7.14)
(where the notation [·]1,2 denotes the subvector consisting of the ﬁrst and second elements),
while
r(x + p)3,4,5  r(x + ϵ(e1 + e4))3,4,5  r(x + ϵe4)3,4,5.
(7.15)
By substituting (7.14) into (7.9), we obtain
r(x + p)1,2  r(x)1,2 + ϵ[J(x)e1]1,2 + O(ϵ2).
By rearranging this expression, we obtain the following difference formula for estimating
the (1, 1) and (2, 1) elements of the Jacobian matrix:


∂r1
∂x1
(x)
∂r2
∂x1
(x)

 [J(x)e1]1,2 ≈r(x + p)1,2 −r(x)1,2
ϵ
.
(7.16)
A similar argument shows that the nonzero elements of the fourth column of the Hessian
can be estimated by substituting (7.15) into (7.9); we obtain eventually that


∂r4
∂x3
(x)
∂r4
∂x4
(x)
∂r4
∂x5
(x)


 [J(x)e4]3,4,5 ≈r(x + p)3,4,5 −r(x)3,4,5
ϵ
.
(7.17)
To summarize: We have been able to estimate two columns of the Jacobian J(x) by evaluating
the function r at the single extra point x + ϵ(e1 + e4).

172
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
We can approximate the remainder of J(x) in an economical manner as well. Columns
2 and 5 can be approximated by choosing p  ϵ(e2 + e5), while we can use p  ϵ(e3 + e6)
to approximate columns 3 and 6. In total, we need 3 extra gradient evaluations to estimate
the entire matrix.
In fact, for any choice of n in (7.12) (no matter how large), three extra gradient
evaluations are sufﬁcient for the entire Hessian. The corresponding choices of perturbation
vectors p are
p  ϵ(e1 + e4 + e7 + e10 + · · ·),
p  ϵ(e2 + e5 + e8 + e11 + · · ·),
p  ϵ(e3 + e6 + e9 + e12 + · · ·).
Intheﬁrstofthesevectors,thenonzerocomponentsarechosensothatnotwoofthecolumns
1, 4, 7, . . . have a nonzero element in the same row. The same property holds for the other
two vectors and, in fact, points the way to the criterion that we can apply to general problems
to decide on a valid set of perturbation vectors.
Algorithms for choosing the perturbation vectors can be expressed conveniently in the
language of graphs and graph coloring. For any function r : IRn →IRm, we can construct
a column incidence graph G with n nodes by drawing an arc between nodes i and k if there
is some component of r that depends on both xi and xk. (In other words, the ith and
kth columns of the Jacobian J(x) each have a nonzero element in some row j, for some
j  1, 2, . . . , m and some value of x.) The intersection graph for the function deﬁned in
(7.12) (with n  6) is shown in Figure 7.1. We now assign each node a “color” according
to the following rule: Two nodes can have the same color if there is no arc that connects
them. Finally, we choose one perturbation vector corresponding to each color: If nodes
i1, i2, . . . , iℓhave the same color, the corresponding p is ϵ(ei1 + ei2 + · · · + eiℓ).
1
3
2
4
5
6
Figure 7.1
Column incidence graph for r(x) deﬁned in
(7.12).

7 . 1 .
F i n i t e - D i f f e r e n c e D e r i v a t i v e A p p r o x i m a t i o n s
173
Usually, there are many ways to assign colors to the n nodes in the graph in a way that
satisﬁes the required condition. The simplest way is just to assign each node a different color,
but since that scheme produces n perturbation vectors, it is usually not the most efﬁcient
approach. It is generally very difﬁcult to ﬁnd the coloring scheme that uses the fewest possible
colors, but there are simple algorithms that do a good job of ﬁnding a near-optimal coloring
at low cost. Curtis, Powell, and Reid [62] and Coleman and Mor´e [48] provide descriptions
of some methods and performance comparisons. Newsam and Ramsdell [182] show that by
considering a more general class of perturbation vectors p, it is possible to evaluate the full
Jacobian using no more than nz evaluations of r (in addition to the evaluation at the point
x), where nz is the maximum number of nonzeros in each row of J(x).
Forsomefunctionsr withwell-studiedstructures(thosethatarisefromdiscretizations
of differential operators, or those that give rise to banded Jacobians, as in the example above),
optimal coloring schemes are known. For the tridiagonal Jacobian of (7.13) and its associated
graph in Figure 7.1, the scheme with three colors is optimal.
APPROXIMATING THE HESSIAN
In some situations, the user may be able to provide a routine to calculate the gradient
∇f (x) but not the Hessian ∇2f (x). We can obtain the Hessian by applying the forward-
difference or central-difference formula derived above to each element of the gradient vector
in turn. When the second derivatives exist and are Lipschitz continuous, Taylor’s theorem
implies that
∇f (x + p)  ∇f (x) + ∇2f (x)p + O(∥p∥2).
(7.18)
By substituting p  ϵei in this expression and rearranging, we obtain
∇2f (x)ei  ∂(∇f )
∂xi
(x) ≈∇f (x + ϵei) −∇f (x)
ϵ
,
(7.19)
wheretheapproximationerrorisO(ϵ).AnapproximationtothefullHessiancanbeobtained
by setting i  1, 2, . . . , n in turn, a process that requires a total of n + 1 evaluations of
the gradient ∇f . Note that this column-at-a-time process does not necessarily lead to a
symmetric result; however, we can recover symmetry by adding the approximation to its
transpose and dividing the result by 2.
As in the case of sparse Jacobians discussed above, it usually is possible to approximate
the entire Hessian using considerably fewer than n perturbation vectors. We discuss some
approaches in the next section.
Some important algorithms—most notably the Newton–CG methods described in
Chapter 6—do not require knowledge of the full Hessian. Instead, each iteration requires
us to supply the Hessian–vector product ∇2f (x)p, for a given vector p. An approximation

174
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
to this product can be obtained by simply replacing p by ϵp in the expression (7.18), to
obtain
∇2f (x)p ≈∇f (x + ϵp) −∇f (x)
ϵ
(7.20)
(see also (6.7)). The approximation error is again O(ϵ), and the cost of obtaining the approx-
imation is a single gradient evaluation at the point x + ϵp. The formula (7.20) corresponds
to the forward-difference approximation (7.1). A central difference formula (7.7) can be
derived by evaluating ∇f (x −ϵp) as well.
For the case in which even gradients are not available, we can use Taylor’s theorem
once again to derive formulae for approximating the Hessian that use only function values.
The main tool is the formula (7.8): By substituting the vectors p  ϵei, p  ϵej, and
p  ϵ(ei + ej) into this formula and combining the results appropriately, we obtain
∂2f
∂xi∂xj
(x)  f (x + ϵei + ϵej) −f (x + ϵei) −f (x + ϵej) + f (x)
ϵ2
+ O(ϵ).
(7.21)
If we wished to approximate every element of the Hessian with this formula, then we would
need to evaluate f at x+ϵ(ei +ej) for all possible i and j (a total of n(n+1)/2 points) as well
as at the n points x + ϵei, i  1, 2, . . . , n. If the Hessian is sparse, we can, of course, reduce
this operation count by skipping the evaluation whenever we know the element ∂2f/∂xi∂xj
to be zero.
APPROXIMATING A SPARSE HESSIAN
The Hessian ∇2f (x) can be thought of as the Jacobian of the vector function
∇f : IRn →IRn, so we could apply the sparse Jacobian estimation techniques described
above to ﬁnd an approximation, often at a cost of many fewer than n + 1 evaluations of the
gradient vector ∇f . Such an approach, however, ignores the fact that the Hessian is sym-
metric. Because of symmetry, any estimate of the element [∇2f (x)]i,j  ∂2f (x)/∂xi∂xj is
also an estimate of its symmetric counterpart [∇2f (x)]j,i. Additional savings—sometimes
very signiﬁcant—can be had if we exploit the symmetry of the Hessian in choosing our
perturbation vectors p.
We illustrate the point with the simple function f : IRn →IR deﬁned by
f (x)  x1
n

i1
i2x2
i .
(7.22)

7 . 1 .
F i n i t e - D i f f e r e n c e D e r i v a t i v e A p p r o x i m a t i o n s
175
It is easy to show that the Hessian ∇2f has the “arrowhead” structure depicted below (for
the case of n  6):


×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×


(7.23)
If we were to construct the intersection graph for the function ∇f (analogous to Figure 7.1),
we would ﬁnd that every node is connected to every other node, for the simple reason that
row 1 has a nonzero in every column. According to the rule for coloring the graph, then, we
would have to assign a different color to every node, which implies that we would need to
evaluate ∇f at the n + 1 points x and x + ϵei for i  1, 2, . . . , n.
We can construct a much more efﬁcient scheme by taking the symmetry into account.
Suppose we ﬁrst use the perturbation vector p  ϵe1 to estimate the ﬁrst column of ∇2f (x).
Because of symmetry, the same estimates apply to the ﬁrst row of ∇2f . From (7.23), we see
that all that remains is to ﬁnd the diagonal elements ∇2f (x)22, ∇2f (x)33, . . . , ∇2f (x)66.
The intersection graph for these remaining elements is completely disconnected, so we can
assign them all the same color and choose the corresponding perturbation vector to be
p  ϵ(e2 + e3 + · · · + e6)  ϵ(0, 1, 1, 1, 1, 1)T .
(7.24)
Note that the second component of ∇f is not affected by the perturbations in components
3, 4, 5, 6 of the unknown vector, while the third component of ∇f is not affected by pertur-
bations in components 2, 4, 5, 6 of x, and so on. As in (7.14) and (7.15), we have for each
component i that
∇f (x + p)i  ∇f (x + ϵ(e2 + e3 + · · · + e6))i  ∇f (x + ϵei)i.
By applying the forward-difference formula (7.1) to each of these individual components,
we then obtain
∂2f
∂x2
i
(x) ≈∇f (x + ϵei)i −∇f (x)i
ϵ
 ∇f (x + ϵp)i −∇f (x)i
ϵ
,
i  2, 3, . . . , 6.
By exploiting symmetry, we have been able to estimate the entire Hessian by evaluating ∇f
only at x and two other points.
Again, graph-coloring techniques can be used as the basis of methods for choosing the
perturbation vectors p economically. We use the adjacency graph in place of the intersection

176
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
graph described earlier. The adjacency graph has n nodes, with arcs connecting nodes i
and k whenever i ̸ k and ∂2f (x)/(∂xi∂xk) ̸ 0 for some x. The requirements on the
coloring scheme are a little more complicated than before, however. We require not only
that connected nodes have different colors, but also that any path of length 3 through the
graph contain at least three colors. In other words, if there exist nodes i1, i2, i3, i4 in the graph
that are connected by arcs (i1, i2), (i2, i3), and (i3, i4), then at least three different colors must
be used in coloring these four nodes. See Coleman and Mor´e [49] for an explanation of this
rule and for algorithms to compute valid colorings. The perturbation vectors are constructed
as before: Whenever the nodes i1, i2, . . . , iℓhave the same color, we set the corresponding
perturbation vector to be p  ϵ(ei1 + ei2 + · · · + eiℓ).
7.2
AUTOMATIC DIFFERENTIATION
Automatic differentiation is the generic name for techniques that use the computational
representation of a function to produce analytic values for the derivatives. Some techniques
produce code for the derivatives at a general point x by manipulating the function code
directly. Other techniques keep a record of the computations made during the evaluation
of the function at a speciﬁc point x and then review this information to produce a set of
derivatives at x.
Automatic differentiation techniques are founded on the observation that any func-
tion,nomatterhowcomplicated,isevaluatedbyperformingasequenceofsimpleelementary
operations involving just one or two arguments at a time. Two-argument operations include
addition, multiplication, division, and the power operation ab. Examples of single-argument
operationsincludethetrigonometric,exponential,andlogarithmicfunctions.Anothercom-
mon ingredient of the various automatic differentiation tools is their use of the chain rule.
This is the well-known rule from elementary calculus that says that if h is a function of the
vector y ∈IRm, which is in turn a function of the vector x ∈IRn, we can write the derivative
of h with respect to x as follows:
∇xh(y(x)) 
m

i1
∂h
∂yi
∇yi(x).
(7.25)
See Appendix A for further details.
There are two basic modes of automatic differentiation: the forward and reverse modes.
The difference between them can be illustrated by a simple example. We work through such
an example below, and indicate how the techniques can be extended to general functions,
including vector functions.

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
177
AN EXAMPLE
Consider the following function of 3 variables:
f (x)  (x1x2 sin x3 + ex1x2)/x3.
(7.26)
The graph in Figure 7.2 shows how the evaluation of this function can be broken down into
its elementary operations and also indicates the partial ordering associated with these oper-
ations. For instance, the multiplication x1 ∗x2 must take place prior to the exponentiation
ex1x2, or else we would obtain the incorrect result (ex1)x2. This graph introduces the inter-
mediate variables x4, x5, . . . that contain the results of intermediate computations; they are
distinguished from the independent variables x1, x2, x3 that appear at the left of the graph.
We can express the evaluation of f in arithmetic terms as follows:
x4

x1 ∗x2,
x5

sin x3,
x6

ex4,
x7

x4 ∗x5,
x8

x6 + x7,
x9

x8/x3.
(7.27)
The ﬁnal node x9 in Figure 7.2 contains the function value f (x). In the terminology
of graph theory, node i is the parent of node j, and node j the child of node i, whenever
there is a directed arc from i to j. Any node can be evaluated when the values of all its
parents are known, so computation ﬂows through the graph from left to right. Flow of
computation in this direction is known as a forward sweep. It is important to emphasize
x8
x3
5
x
x7
4
x
x6
/
exp
*
+
*
x2
9
x
x1
sin
Figure 7.2
Computational graph for f (x) deﬁned in (7.26).

178
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
that software tools for automatic differentiation do not require the user to break down the
code for evaluating the function into its elements, as in (7.27). Identiﬁcation of intermediate
quantitiesandconstructionofthecomputationalgraphiscarriedout,explicitlyorimplicitly,
by the software tool itself.
We now describe how the forward mode can be used to obtain the derivatives of (7.26).
THE FORWARD MODE
In the forward mode of automatic differentiation, we evaluate and carry forward a
directional derivative of each intermediate variable xi in a given direction p ∈IRm, si-
multaneously with the evaluation of xi itself. We introduce the following notation for the
directional derivative for p associated with each variable:
Dpxi
def (∇xi)T p 
3

j1
∂xi
∂xj
pj,
i  1, 2, . . . , 9.
(7.28)
Our goal is to evaluate Dpx9, which is the same as the directional derivative ∇f (x)T p. We
note immediately that initial values Dpxi for the independent variables xi, i  1, 2, 3, are
simply the components p1, p2, p3 of p. The direction p is referred to as the seed vector.
As soon as the value of xi at any node is known, we can ﬁnd the corresponding value
of Dpxi from the chain rule. For instance, suppose we know the values of x4, Dpx4, x5, and
Dpx5, and we are about to calculate x7 in Figure 7.2. We have that x7  x4x5; that is, x7 is a
function of the two variables x4 and x5, which in turn are functions of x1, x2, x3. By applying
the rule (7.25), we have that
∇x7  ∂x7
∂x4
∇x4 + ∂x7
∂x5
∇x5  x5∇x4 + x4∇x5.
Bytakingtheinnerproductofbothsidesofthisexpressionwith p andapplyingthedeﬁnition
(7.28), we obtain
Dpx7  ∂x7
∂x4
Dpx4 + ∂x7
∂x5
Dpx5  x5Dpx4 + x4Dpx5.
(7.29)
The directional derivatives Dpxi are therefore evaluated side by side with the intermediate
results xi, and at the end of the process we obtain Dpx9  Dpf  ∇f (x)T p.
The principle of the forward mode is straightforward enough, but what of its practical
implementation and computational requirements? First, we repeat that the user does not
need to construct the computational graph, break the computation down into elementary
operations as in (7.27), or identify intermediate variables. The automatic differentiation
software should perform these tasks implicitly and automatically. Nor is it necessary to store
the information xi and Dpxi for every node of the computation graph at once (which is just

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
179
as well, since this graph can be very large for complicated functions). Once all the children
of any node have been evaluated, its associated values xi and Dpxi are not needed further
and may be overwritten in storage.
The key to practical implementation is the side-by-side evaluation of xi and Dpxi. The
automatic differentiation software associates a scalar Dpw with any scalar w that appears
in the evaluation code. Whenever w is used in an arithmetic computation, the software
performs an associated operation (based on the chain rule) with the gradient vector Dpw.
For instance, if w is combined in a division operation with another value y to produce a new
value z, that is,
z ←w
y ,
we use w, z, Dpw, and Dpy to evaluate the directional derivative Dpz as follows:
Dpz ←1
y Dpw −w
y2 Dpy.
(7.30)
Toobtainthecompletegradientvector,wecancarryoutthisproceduresimultaneously
for the n seed vectors p  e1, e2, . . . , en. By the deﬁnition (7.28), we see that p  ej implies
that Dpf  ∂f/∂xj, j  1, 2, . . . , n. We note from the example (7.30) that the additional
cost of evaluating f and ∇f (over the cost of evaluating f alone) may be signiﬁcant. In
this example, the single division operation on w and y needed to calculate z gives rise
to approximately 2n multiplications and n additions in the computation of the gradient
elements Dej z, j  1, 2, . . . , n. It is difﬁcult to obtain an exact bound on the increase in
computation, since the costs of retrieving and storing the data should also be taken into
account. The storage requirements may also increase by a factor as large as n, since we
now have to store n additional scalars Dej xi, j  1, 2, . . . , n, alongside each intermediate
variable xi. It is usually possible to make savings by observing that many of these quantities
are zero, particularly in the early stages of the computation (that is, toward the left of the
computational graph), so sparse data structures can be used to store the vectors Dej xi,
j  1, 2, . . . , n (see [16]).
The forward mode of automatic differentiation can be implemented by means of a
precompiler, which transforms function evaluation code into extended code that evaluates
the derivative vectors as well. An alternative approach is to use the operator-overloading
facilities available in languages such as C++ to transparently extend the data structures and
operations in the manner described above.
THE REVERSE MODE
The reverse mode of automatic differentiation does not perform function and gradient
evaluations concurrently. Instead, after the evaluation of f is complete, it recovers the partial
derivatives of f with respect to each variable xi—independent and intermediate variables

180
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
alike—by performing a reverse sweep of the computational graph. At the conclusion of this
process, the gradient vector ∇f can be assembled from the partial derivatives ∂f/∂xi with
respect to the independent variables xi, i  1, 2, . . . , n.
Instead of the gradient vectors Dpxi used in the forward mode, the reverse mode
associates a scalar variable ¯xi with each node in the graph; information about the partial
derivative ∂f/∂xi is accumulated in ¯xi during the reverse sweep. The ¯xi are sometimes called
the adjoint variables, and we initialize their values to zero, with the exception of the rightmost
node in the graph (node N, say), for which we set ¯xN  1. This choice makes sense because
xN contains the ﬁnal function value f , so we have ∂f/∂xN  1.
The reverse sweep makes use of the following observation, which is again based on
the chain rule (7.25): For any node i, the partial derivative ∂f/∂xi can be built up from
the partial derivatives ∂f/∂xj corresponding to its child nodes j according to the following
formula:
∂f
∂xi


j a child of i
∂f
∂xj
∂xj
∂xi
.
(7.31)
For each node i, we add the right-hand-side term in (7.31) to ¯xi as soon as it becomes known;
that is, we perform the operation
¯xi +
∂f
∂xj
∂xj
∂xi
.
(7.32)
(In this expression and the ones below, we use the arithmetic notation of the programming
language C, in which x+a means x ←x + a.) Once contributions have been received
from all the child nodes of i, we have ¯xi  ∂f/∂xi, so we declare node i to be “ﬁnalized.”
At this point, node i is ready to contribute a term to the summation for each of its parent
nodes according to the formula (7.31). The process continues in this fashion until all nodes
are ﬁnalized. Note that the ﬂow of computation in the graph is from children to parents.
This is the opposite direction to the process of evaluating the function f , which proceeds
from parents to children.
During the reverse sweep, we work with numerical values, not with formulae or com-
puter code involving the variables xi or the partial derivatives ∂f/∂xi. During the forward
sweep—the evaluation of f —we not only calculate the values of each variable xi, but we
also calculate and store the numerical values of each partial derivative ∂xj/∂xi. Each of these
partial derivatives is associated with a particular arc of the computational graph. The nu-
merical values of ∂xj/∂xi computed during the forward sweep are then used in the formula
(7.32) during the reverse sweep.
We illustrate the reverse mode for the example function (7.26). In Figure 7.3 we ﬁll
in the graph of Figure 7.2 for a speciﬁc evaluation point x  (1, 2, π/2)T , indicating the
numerical values of the intermediate variables x4, x5, . . . , x9 associated with each node and
the partial derivatives ∂xj/∂xi associated with each arc.

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
181
e 2
2
2+e
p(9,8)=2/ π
4+2e /2 π
/2
π
π 2
/
p(5,3)=0
p(7,4)=1
p(6,4)=e
p(4,2)=1
p(8,6)=1
*
+
*
1
2
p(7,5)=2
p(8,7)=1
p(4,1)=2
sin
exp
2
1
2
2
(-8-4e  )/
Figure 7.3
Computational graph for f (x) deﬁned in (7.26) showing numerical val-
ues of intermediate values and partial derivatives for the point x  (1, 2, π/2)T .
Notation: p(j, i)  ∂xj/∂xi.
As mentioned above, we initialize the reverse sweep by setting all the adjoint variables
¯xi to zero, except for the rightmost node, for which we have ¯x9  1. Since f (x)  x9 and
since node 9 has no children, we have ¯x9  ∂f/∂x9, and so we can immediately declare node
9 to be ﬁnalized.
Node 9 is the child of nodes 3 and 8, so we use formula (7.32) to update the values of
¯x3 and ¯x8 as follows:
¯x3+ ∂f
∂x9
∂x9
∂x3
 −2 + e2
(π/2)2  −8 −4e2
π2
,
(7.33a)
¯x8+ ∂f
∂x9
∂x9
∂x8

1
π/2  2
π .
(7.33b)
Node 3 is not ﬁnalized after this operation; it still awaits a contribution from its other child,
node 5. On the other hand, node 9 is the only child of node 8, so we can declare node 8 to
be ﬁnalized with the value ∂f
∂x8  2/π. We can now update the values of ¯xi at the two parent
nodes of node 8 by applying the formula (7.32) once again; that is,
¯x6+ ∂f
∂x8
∂x8
∂x6
 2
π ;
¯x7+ ∂f
∂x8
∂x8
∂x7
 2
π .

182
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
At this point, nodes 6 and 7 are ﬁnalized, so we can use them to update nodes 4 and 5. At
the end of this process, when all nodes are ﬁnalized, nodes 1, 2, and 3 contain


¯x1
¯x2
¯x3

 ∇f (x) 


(4 + 4e2)/π
(2 + 2e2)/π
(−8 −4e2)/π2

,
and the derivative computation is complete.
The main appeal of the reverse mode is that its computational complexity is low for
the scalar functions f : IRn →IR discussed here. The extra arithmetic associated with
the gradient computation is at most four or ﬁve times the arithmetic needed to evaluate
the function alone. Taking the division operation in (7.33) as an example, we see that two
multiplications, a division, and an addition are required for (7.33a), while a division and an
addition are required for (7.33b). This is about ﬁve times as much work as the single division
involving these nodes that was performed during the forward sweep.
As we noted above, the forward mode may require up to n times more arithmetic
to compute the gradient ∇f than to compute the function f alone, making it appear
uncompetitive with the reverse mode. When we consider vector functions r : IRn →IRm,
the relative costs of the forward and reverse modes become more similar as m increases, as
we describe in the next section.
Anapparentdrawbackofthereversemodeistheneedtostoretheentirecomputational
graph, which is needed for the reverse sweep. In principle, storage of this graph is not too
difﬁcult to implement. Whenever an elementary operation is performed, we can form and
store a new node containing the intermediate result, pointers to the (one or two) parent
nodes, and the partial derivatives associated with these arcs. During the reverse sweep, the
nodes can be read in the reverse order to that in which they were written, giving a particularly
simple access pattern. The process of forming and writing the graph can be implemented
as a straightforward extension to the elementary operations via operator overloading (as in
ADOL-C [126]). The reverse sweep/gradient evaluation can be invoked as a simple function
call.
Unfortunately, the computational graph may require a huge amount of storage. If
each node can be stored in 20 bytes, then a function that requires one second of evaluation
time on a 100 megaﬂop computer may produce a graph of up to 2 gigabytes in size. The
storage requirements can be reduced, at the cost of some extra arithmetic, by performing
partial forward and reverse sweeps on pieces of the computational graph, reevaluating por-
tions of the graph as needed rather than storing the whole structure. Descriptions of this
approach, sometimes known as checkpointing, can be found in Griewank [122] and Grimm,
Pottier, and Rostaing-Schmidt [130], and an implementation can be found in the Odyssee
software tool [219]. An implementation of checkpointing in the context of variational data
assimilation can be found in Restrepo, Leaf, and Griewank [213].

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
183
VECTOR FUNCTIONS AND PARTIAL SEPARABILITY
So far, we have looked at automatic differentiation of general scalar-valued functions
f : IRn →IR. In nonlinear least-squares problems (Chapter 10) and nonlinear equations
(Chapter 11), we have to deal with vector functions r : IRn →IRm with m components
rj, j  1, 2, . . . , m. The rightmost column of the computational graph then consists of
m nodes, none of which has any children, in place of the single node described above. The
forward and reverse modes can be adapted in straightforward ways to ﬁnd the Jacobian of
r, that is, the m × n matrix J(x) deﬁned by
J(x) 
∂rj
∂xi

j1,2,...,m
i1,2,...,n
.
(7.34)
Besides their applications to least-squares and nonlinear-equa‘tions problems, au-
tomatic differentiation of vector functions is a useful technique for dealing with partially
separable functions. We recall that partial separability is commonly observed in large-scale
optimization, and we saw in Chapter 9 that there exist efﬁcient quasi-Newton procedures for
the minimization of objective functions with this property. Since an automatic procedure for
detecting the decomposition of a given function f into its partially separable representation
was developed recently by Gay [98], it has become possible to exploit the efﬁciencies that
accrue from this property without asking much information from the user.
In the simplest sense, a function f is partially separable if we can express it in the form
f (x) 
ne

i1
fi(x),
(7.35)
where each element function fi(·) depends on just a few components of x. If we construct
the vector function r from the partially separable components, that is,
r(x) 


f1(x)
f2(x)
...
fne(x)


,
it follows from (7.35) that
∇f (x)  J(x)T e,
(7.36)
where, as usual, e  (1, 1, . . . , 1)T . Because of the partial separability property, most
columns of J(x) contain just a few nonzeros. This structure makes it possible to calculate

184
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
J(x) efﬁciently by applying graph-coloring techniques, as we discuss below. The gradient
∇f (x) can then be recovered from the formula (7.36).
In constrained optimization, it is often beneﬁcial to evaluate the objective function f
andtheconstraintfunctions ci,i ∈I∪E,simultaneously.Bydoingso,wecantakeadvantage
of common expressions (which show up as shared intermediate nodes in the computation
graph) and hence can reduce the total workload. In Figure 7.2, for instance, the intermediate
variable x4 is shared during the computation of x6 and x7. In this case, the vector function
r can be deﬁned as
r(x) 

f (x)

cj(x)

j∈I∪E

.
CALCULATING JACOBIANS OF VECTOR FUNCTIONS
The forward mode is the same for vector functions as for scalar functions. Given a
seed vector p, we continue to associate quantities Dpxi with the node that calculates each
intermediate variable xi. At each of the rightmost nodes (containing rj, j  1, 2, . . . , m),
this variable contains the quantity Dprj  (∇rj)T p, j  1, 2, . . . , m. By assembling
these m quantities, we obtain J(x)p, the product of the Jacobian and our chosen vector
p. As in the case of scalar functions (m  1), we can evaluate the complete Jacobian
by setting p  e1, e2, . . . , en and evaluating the n quantities Dej xi simultaneously. For
sparse Jacobians, we can use the coloring techniques outlined above in the context of ﬁnite-
difference methods to make more intelligent and economical choices of the seed vectors p.
The factor of increase in cost of arithmetic, when compared to a single evaluation of r, is
about equal to the number of seed vectors used.
The key to applying the reverse mode to a vector function r(x) is to choose seed vectors
q ∈IRm and apply the reverse mode to the scalar functions r(x)T q. The result of this process
is the vector
∇[r(x)T q]  ∇
 m

j1
qjrj(x)

 J(x)T q.
Instead of the Jacobian–vector product that we obtain with the forward mode, the reverse
mode yields a Jacobian-transpose–vector product. The technique can be implemented by
seeding the variables ¯xi in the m dependent nodes that contain r1, r2, . . . , rm, with the
components q1, q2, . . . , qm of the vector q. At the end of the reverse sweep, the node for
independent variables x1, x2, . . . , xn will contain
d
dxi

r(x)T q

,
i  1, 2, . . . , n,
which are simply the components of J(x)T q.

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
185
As usual, we can obtain the full Jacobian by carrying out the process above for the m
unit vectors q  e1, e2, . . . , em. Alternatively, for sparse Jacobians, we can apply the usual
coloring techniques to ﬁnd a smaller number of seed vectors q—the only difference being
that the graphs and coloring strategies are deﬁned with reference to the transpose J(x)T
rather than to J(x) itself. The factor of increase in the number of arithmetic operations
required, in comparison to an evaluation of r alone, is no more than 5 times the number
of seed vectors. (The factor of 5 is the usual overhead from the reverse mode for a scalar
function.) The space required for storage of the computational graph is no greater than in
the scalar case. As before, we need only store the graph topology information together with
the partial derivative associated with each arc.
The forward- and reverse-mode techniques can be combined to cumulatively reveal
all the elements of J(x). We can choose a set of seed vectors p for the forward mode to reveal
some columns of J, then perform the reverse mode with another set of seed vectors q to
reveal the rows that contain the remaining elements.
Finally,wenotethatforsomealgorithms,wedonotneedfullknowledgeoftheJacobian
J(x). For instance, iterative methods such as the inexact Newton method for nonlinear
equations (see Section 11.1) require repeated calculation of J(x)p for a succession of vectors
p. Each such matrix–vector product can be computed using the forward mode by using a
single forward sweep, at a similar cost to evaluation of the function alone.
CALCULATING HESSIANS: FORWARD MODE
So far, we have described how the forward and reverse modes can be applied to obtain
ﬁrst derivatives of scalar and vector functions. We now outline extensions of these techniques
to the computation of the Hessian ∇2f of a scalar function f , and evaluation of the Hessian–
vector product ∇2f (x)p for a given vector p.
Recall that the forward mode makes use of the quantities Dpxi, each of which stores
(∇xi)T p for each node i in the computational graph and a given vector p. For a given pair
of seed vectors p and q (both in IRn) we now deﬁne another scalar quantity by
Dpqxi  pT (∇2xi)q,
(7.37)
for each node i in the computational graph. We can evaluate these quantities during the
forward sweep through the graph, alongside the function values xi and the ﬁrst-derivative
values Dpxi. The initial values of Dpq at the independent variable nodes xi, i  1, 2 . . . , n,
will be 0, since the second derivatives of xi are zero at each of these nodes. When the forward
sweep is complete, the value of Dpqxi in the rightmost node of the graph will be pT ∇2f (x)q.
The formulae for transformation of the Dpqxi variables during the forward sweep can
once again be derived from the chain rule. For instance, if xi is obtained by adding the values
at its two parent nodes, xi  xj + xk, the corresponding accumulation operations on Dpxi

186
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
and Dpqxi are as follows:
Dpxi  Dpxj + Dpxk,
Dpqxi  Dpqxj + Dpqxk.
(7.38)
The other binary operations −, ×, / are handled similarly. If xi is obtained by applying the
unitary transformation L to xj, we have
xi  L(xj),
(7.39a)
Dpxi  L′(xj)(Dpxj),
(7.39b)
Dpqxi  L′′(xj)(Dpxj)(Dqxj) + L′(xj)Dpqxj.
(7.39c)
We see in (7.39c) that computation of Dpqxi can rely on the ﬁrst-derivative quantities Dpxi
and Dqxi, so both these quantities must be accumulated during the forward sweep as well.
We could compute a general dense Hessian by choosing the pairs (p, q) to be all
possible pairs of unit vectors (ej, ek), for j  1, 2, . . . , n and k  1, 2, . . . , j, a total of
n(n + 1)/2 vector pairs. (Note that we need only evaluate the lower triangle of ∇2f (x),
because of symmetry.) When we know the sparsity structure of ∇2f (x), we need evaluate
Dej ekxi only for the pairs (ej, ek) for which the (j, k) component of ∇2f (x) is possibly
nonzero.
The total increase factor for the number of arithmetic operations, compared with the
amount of arithmetic to evaluate f alone, is a small multiple of 1 + n + Nz(∇2f ), where
Nz(∇2f ) is the number of elements of ∇2f that we choose to evaluate. This number reﬂects
the evaluation of the quantities xi, Dej xi (j  1, 2, . . . , n), and Dejekxi for the Nz(∇2f )
vector pairs (ej, ek). The “small multiple” results from the fact that the update operations
for Dpxi and Dpqxi may require a few times more operations than the update operation for
xi alone; see, for example, (7.39). One storage location per node of the graph is required for
each of the 1+n+Nz(∇2f ) quantities that are accumulated, but recall that storage of node
i can be overwritten once all its children have been evaluated.
When we do not need the complete Hessian, but only a matrix–vector product involv-
ing the Hessian (as in the Newton–CG algorithm of Chapter 6), the amount of arithmetic is,
of course, smaller. Given a vector q ∈IRn, we use the techniques above to compute the ﬁrst-
derivative quantities De1xi, . . . Denxi and Dqxi, as well as the second-derivative quantities
De1qxi, . . . , Denqxi, during the forward sweep. The ﬁnal node will contain the quantities
eT
j
	
∇2f (x)

q 

∇2f (x)q

j ,
j  1, 2, . . . , n,
which are the components of the vector ∇2f (x)q. Since 2n + 1 quantities in addition to
xi are being accumulated during the forward sweep, the increase factor in the number of
arithmetic operations increases by a small multiple of 2n.
An alternative technique for evaluating sparse Hessians is based on the forward-mode
propagationofﬁrstandsecondderivativesofunivariatefunctions.Tomotivatethisapproach,

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
187
note that the (i, j) element of the Hessian can be expressed as follows:
[∇2f (x)]ij  eT
i ∇2f (x)ej
 1
2

(ei + ej)T ∇2f (x)(ei + ej) −eT
i ∇2f (x)ei −eT
j ∇2f (x)ej

.(7.40)
We can use this interpolation formula to evaluate [∇2f (x)]ij, provided that the second
derivatives Dppxk, for p  ei, p  ej, p  ei + ej, and all nodes xk, have been evaluated
during the forward sweep through the computational graph. In fact, we can evaluate all the
nonzero elements of the Hessian, provided that we use the forward mode to evaluate Dpxk
and Dppxk for a selection of vectors p of the form ei + ej, where i and j are both indices in
{1, 2, . . . , n}, possibly with i  j.
One advantage of this approach is that it is no longer necessary to propagate “cross
terms” of the form Dpqxk for p ̸ q (see, for example, (7.38) and (7.39c)). The propagation
formulae therefore simplify somewhat. Each Dppxk is a function of xℓ, Dpxℓ, and Dppxℓfor
all parent nodes ℓof node k.
Note, too, that if we deﬁne the univariate function ψ by
ψ(t)  f (x + tp),
(7.41)
then the values of Dpf and Dppf (which emerge at the completion of the forward sweep)
are simply the ﬁrst two derivatives of ψ evaluated at t  0; that is,
Dpf  pT ∇f (x)  ψ′(t)|t0,
Dppf  pT ∇2f (x)p  ψ′′(t)|t0.
Extension of this technique to third, fourth, and higher derivatives is possible. Inter-
polation formulae analogous to (7.40) can be used in conjunction with higher derivatives
of the univariate functions ψ deﬁned in (7.41), again for a suitably chosen set of vectors p,
where each p is made up of a sum of unit vectors ei. For details, see Bischof, Corliss, and
Griewank [14].
CALCULATING HESSIANS: REVERSE MODE
We can also devise schemes based on the reverse mode for calculating Hessian–vector
products ∇2f (x)q, or the full Hessian ∇2f (x). A scheme for obtaining ∇2f (x)q proceeds
as follows. We start by using the forward mode to evaluate both f and ∇f (x)T q, by accu-
mulating the two variables xi and Dqxi during the forward sweep in the manner described
above. We then apply the reverse mode in the normal fashion to the computed function
∇f (x)T q. At the end of the reverse sweep, the nodes i  1, 2, . . . , n of the computational
graph that correspond to the independent variables will contain
∂
∂xi
(∇f (x)T q) 

∇2f (x)q

i ,
i  1, 2, . . . , n.

188
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
The number of arithmetic operations required to obtain ∇2f (x)q by this procedure
increases by only a modest factor, independent of n, over the evaluation of f alone. By
the usual analysis for the forward mode, we see that the computation of f and ∇f (x)T q
jointly requires a small multiple of the operation count for f alone, while the reverse sweep
introduces a further factor of at most 5. The total increase factor is approximately 12 over the
evaluation of f alone. If the entire Hessian ∇2f (x) is required, we could apply the procedure
just described with q  e1, e2, . . . , en. This approach would introduce an additional factor
of n into the operation count, leading to an increase of at most 12n over the cost of f alone.
Once again, when the Hessian is sparse with known structure, we may be able to
use graph-coloring techniques to evaluate this entire matrix using many fewer than n seed
vectors. The choices of q are similar to those used for ﬁnite-difference evaluation of the
Hessian, described above. The increase in operation count over evaluating f alone is a
multiple of up to 12Nc(∇2f ), where Nc is the number of seed vectors q used in calculating
∇2f .
CURRENT LIMITATIONS
The current generation of automatic differentiation tools has proved its worth through
successful application to some large and difﬁcult design optimization problems. However,
these tools can run into difﬁculties with some commonly used programming constructs and
some implementations of computer arithmetic. As an example, if the evaluation of f (x)
depends on the solution of a partial differential equation (PDE), then the computed value
of f may contain truncation error arising from the ﬁnite-difference or the ﬁnite-element
technique that is used to solve the PDE numerically. That is, we have ˆf (x)  f (x) + τ(x),
where ˆf (·) is the computed value of f (·) and τ(·) is the truncation error. Though |τ(x)| is
usually small, its derivative τ ′(x) may not be, so the error in the computed derivative ˆf ′(x)
is potentially large. (The ﬁnite-difference approximation techniques discussed in Section 7.1
experience the same difﬁculty.) Similar problems arise when the computer uses piecewise
rational functions to approximate trigonometric functions.
Another source of potential difﬁculty is the presence of branching in the code to
improve the speed or accuracy of function evaluation in certain domains. A pathological
example is provided by the linear function f (x)  x −1. If we used the following (perverse,
but valid) piece of code to evaluate this function,
if (x  1.0) then f  0.0 else f  x −1.0,
then by applying automatic differentiation to this procedure we would obtain the derivative
value f ′(1)  0. For a discussion of such issues and an approach to dealing with them, see
Griewank [123].
In conclusion, automatic differentiation should be regarded as a set of increasingly
sophisticated techniques that enhances optimization algorithms, allowing them to be ap-
plied successfully to many practical problems involving complicated functions. It facilitates
interpretations of the computed optimal solutions, allowing the modeler to extract more

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
189
information from the results of the computation. Automatic differentiation should not be
regarded as a panacea that absolves the user altogether from the responsibility of thinking
about derivative calculations.
NOTES AND REFERENCES
The ﬁeld of automatic differentiation has grown considerably in recent years, and a
number of good software tools are now available. We mention in particular ODYSSEE [219],
ADIFOR [13] and ADIC [17], and ADOL-C [126]. Current software tool development in
automaticdifferentiationismovingawayfromtheforward-reverse-modedichotomyandto-
ward “mixed modes” that combine the two approaches at different steps in the computation.
For a summary of recent work in this area see Bischof and Haghighat [15].
A number of good survey papers and volumes on automatic differentiation are avail-
able. The volume edited by Griewank and Corliss [125] covers much of the state of the art up
to 1991, while the update to this volume, edited by Berz, Bischof, Corliss, and Griewank [10],
covers the developments of the following ﬁve years. (Particularly notable contributions in
this volume are the paper of Corliss and Rall [57] and the extensive bibliography.) A survey
of the applications of automatic differentiation in optimization is given by Griewank [124].
The technique for calculating the gradient of a partially separable function was de-
scribed by Bischof et al. [12], whereas the computation of the Hessian matrix has been
considered by several authors; see, for example, Gay [98].
The work of Coleman and Mor´e [49] on efﬁcient estimation of Hessians was predated
by Powell and Toint [210], who did not use the language of graph coloring but nevertheless
devised highly effective schemes. Software for estimating sparse Hessians and Jacobians is
described by Coleman, Garbow, and Mor´e [46, 47].
✐
E x e r c i s e s
✐
7.1 Show that a suitable value for the perturbation ϵ in the central-difference formula
is ϵ  u1/3, and that the accuracy achievable by this formula when the values of f contain
roundoff errors of size u is approximately u2/3. (Use similar assumptions to the ones used
to derive the estimate (7.6) for the forward-difference formula.)
✐
7.2 Derive a central-difference analogue of the Hessian–vector approximation
formula (7.20).
✐
7.3 Verify the formula (7.21) for approximating an element of the Hessian using only
function values.
✐
7.4 Verify that if the Hessian of a function f has nonzero diagonal elements, then its
adjacency graph is a subgraph of the intersection graph for ∇f . In other words, show that
any arc in the adjacency graph also belongs to the intersection graph.

190
C h a p t e r
7 .
C a l c u l a t i n g D e r i v a t i v e s
✐
7.5 Draw the adjacency graph for the function f deﬁned by (7.22). Show that the
coloring scheme in which node 1 has one color while nodes 2, 3, . . . , n have another color
is valid. Draw the intersection graph for ∇f .
✐
7.6 Construct the adjacency graph for the function whose Hessian has the nonzero
structure


×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×
×


,
and ﬁnd a valid coloring scheme with just three colors.
✐
7.7 Trace the computations performed in the forward mode for the function f (x)
in (7.26), expressing the intermediate derivatives ∇xi, i  6, . . . , 12, in terms of quantities
available at their parent nodes and then in terms of the independent variables x1, . . . , x5.
✐
7.8 Formula (7.30) showed the gradient operations associated with scalar division.
Derive similar formulae for the following operations:
(s, t) →s + t
addition;
t →et
exponentiation;
t →tan(t)
tangent;
(s, t) →st.
✐
7.9 By calculating the partial derivatives ∂xj/∂xi for the function (7.26) from the
expressions (7.27), verify the numerical values for the arcs in Figure 7.3 for the evaluation
point x  (1, 2, π/2, −1, 3)T . Work through the remaining details of the reverse sweep
process, indicating the order in which the nodes become ﬁnalized.
✐
7.10 Using (7.33) as a guide, describe the reverse sweep operations corresponding to
the following elementary operations in the forward sweep:
xk ←xixj
multiplication;
xk ←cos(xi)
cosine.
In each case, compare the arithmetic workload in the reverse sweep to the workload required
for the forward sweep.

7 . 2 .
A u t o m a t i c D i f f e r e n t i a t i o n
191
✐
7.11 Deﬁne formulae similar to (7.38) for accumulating the ﬁrst derivatives Dpxi and
the second derivatives Dpqxi when xi is obtained from the following three binary operations:
xi  xj −xk, xi  xjxk, and xi  xj/xk.
✐
7.12 By using the deﬁnitions (7.28) of Dpxi and (7.37) of Dpqxi, verify the
differentiation formulae (7.39) for the unitary operation xi  L(xj).
✐
7.13 Let a ∈IRn be a ﬁxed vector and deﬁne f as f (x)  1
2

xT x +
	
aT x

2
. Count
the number of operations needed to evaluate f , ∇f , ∇2f , and the Hessian–vector product
∇2f (x)p for an arbitrary vector p.

Chapter8

Quasi-Newton
Methods
In the mid 1950s, W.C. Davidon, a physicist working at Argonne National Laboratory,
was using the coordinate descent method (see Section 3.3) to perform a long optimization
calculation. At that time computers were not very stable, and to Davidon’s frustration,
the computer system would always crash before the calculation was ﬁnished. So Davidon
decided to ﬁnd a way of accelerating the iteration. The algorithm he developed—the ﬁrst
quasi-Newton algorithm—turned out to be one of the most revolutionary ideas in nonlinear
optimization. It was soon demonstrated by Fletcher and Powell that the new algorithm
was much faster and more reliable than the other existing methods, and this dramatic
advance transformed nonlinear optimization overnight. During the following twenty years,
numerous variants were proposed and hundreds of papers were devoted to their study. An
interesting historical irony is that Davidon’s paper [64] was not accepted for publication; it
remained as a technical report for more than thirty years until it appeared in the ﬁrst issue
of the SIAM Journal on Optimization in 1991 [65].
Quasi-Newton methods, like steepest descent, require only the gradient of the ob-
jective function to be supplied at each iterate. By measuring the changes in gradients, they
construct a model of the objective function that is good enough to produce superlinear
convergence. The improvement over steepest descent is dramatic, especially on difﬁcult

194
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
problems. Moreover, since second derivatives are not required, quasi-Newton methods are
sometimes more efﬁcient than Newton’s method. Today, optimization software libraries
contain a variety of quasi-Newton algorithms for solving unconstrained, constrained, and
large-scale optimization problems. In this chapter we discuss quasi-Newton methods for
small and medium-sized problems, and in Chapter 9 we consider their extension to the
large-scale setting.
The development of automatic differentiation techniques has diminished the appeal
of quasi-Newton methods, but only to a limited extent. Automatic differentiation eliminates
the tedium of computing second derivatives by hand, as well as the risk of introducing errors
in the calculation. More details are given in Chapter 7. Nevertheless, quasi-Newton methods
remain competitive on many types of problems.
8.1
THE BFGS METHOD
The most popular quasi-Newton algorithm is the BFGS method, named for its discoverers
Broyden, Fletcher, Goldfarb, and Shanno. In this section we derive this algorithm (and
its close relative, the DFP algorithm) and describe its theoretical properties and practical
implementation.
We begin the derivation by forming the following quadratic model of the objective
function at the current iterate xk:
mk(p)  fk + ∇f T
k p + 1
2pT Bkp.
(8.1)
Here Bk is an n × n symmetric positive deﬁnite matrix that will be revised or updated at
every iteration. Note that the value and gradient of this model at p  0 match fk and ∇fk,
respectively. The minimizer pk of this convex quadratic model, which we can write explicitly
as
pk  −B−1
k ∇fk,
(8.2)
is used as the search direction, and the new iterate is
xk+1  xk + αkpk,
(8.3)
where the step length αk is chosen to satisfy the Wolfe conditions (3.6). This iteration is quite
similar to the line search Newton method; the key difference is that the approximate Hessian
Bk is used in place of the true Hessian.
Instead of computing Bk afresh at every iteration, Davidon proposed to update it in a
simple manner to account for the curvature measured during the most recent step. Suppose
that we have generated a new iterate xk+1 and wish to construct a new quadratic model, of

8 . 1 .
T h e B F G S M e t h o d
195
the form
mk+1(p)  fk+1 + ∇f T
k+1p + 1
2pT Bk+1p.
What requirements should we impose on Bk+1, based on the knowledge we have gained
during the latest step? One reasonable requirement is that the gradient of mk+1 should
match the gradient of the objective function f at the latest two iterates xk and xk+1. Since
∇mk+1(0) is precisely ∇fk+1, the second of these conditions is satisﬁed automatically. The
ﬁrst condition can be written mathematically as
∇mk+1(−αkpk)  ∇fk+1 −αkBk+1pk  ∇fk.
By rearranging, we obtain
Bk+1αkpk  ∇fk+1 −∇fk.
(8.4)
To simplify the notation it is useful to deﬁne the vectors
sk  xk+1 −xk,
yk  ∇fk+1 −∇fk,
(8.5)
so that (8.4) becomes
Bk+1sk  yk.
(8.6)
We refer to this formula as the secant equation.
Given the displacement sk and the change of gradients yk, the secant equation requires
that the symmetric positive deﬁnite matrix Bk+1 map sk into yk. This will be possible only
if sk and yk satisfy the curvature condition
sT
k yk > 0,
(8.7)
as is easily seen by premultiplying (8.6) by sT
k . When f is strongly convex, the inequality (8.7)
willbesatisﬁedforanytwopointsxk andxk+1 (seetheexercises).However,thisconditionwill
not always hold for nonconvex functions, and in this case we need to enforce (8.7) explicitly,
by imposing restrictions on the line search procedure that chooses α. In fact, the condition
(8.7) is guaranteed to hold if we impose the Wolfe (3.6) or strong Wolfe conditions (3.7) on
the line search. To verify this claim, we note from (8.5) and (3.6b) that ∇f T
k+1sk ≥c2∇f T
k sk,
and therefore
yT
k sk ≥(c2 −1)αk∇f T
k pk.
(8.8)
Since c2 < 1 and since pk is a descent direction, the term on the right will be positive, and
the curvature condition (8.7) holds.

196
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
When the curvature condition is satisﬁed, the secant equation (8.6) always has a
solution Bk+1. In fact, it admits an inﬁnite number of solutions, since there are n(n +
1)/2 degrees of freedom in a symmetric matrix, and the secant equation represents only
n conditions. The requirement of positive deﬁniteness imposes n additional inequalities—
all principal minors must be positive—but these conditions do not absorb the remaining
degrees of freedom.
To determine Bk+1 uniquely, then, we impose the additional condition that among all
symmetric matrices satisfying the secant equation, Bk+1 is, in some sense, closest to the current
matrix Bk. In other words, we solve the problem
min
B
∥B −Bk∥
(8.9a)
subject to
B  BT ,
Bsk  yk,
(8.9b)
where sk and yk satisfy (8.7) and Bk is symmetric and positive deﬁnite. Many matrix norms
can be used in (8.9a), and each norm gives rise to a different quasi-Newton method. A
norm that allows easy solution of the minimization problem (8.9), and that gives rise to a
scale-invariant optimization method, is the weighted Frobenius norm
∥A∥W ≡
W 1/2AW 1/2
F ,
(8.10)
where∥·∥F isdeﬁnedby∥C∥2
F  n
i1
n
j1 c2
ij.TheweightW canbechosenasanymatrix
satisfying the relation Wyk  sk. For concreteness, the reader can assume that W  ¯G−1
k
where ¯Gk is the average Hessian deﬁned by
¯Gk 
 1
0
∇2f (xk + ταkpk)dτ

.
(8.11)
The property
yk  ¯Gkαkpk  ¯Gksk
(8.12)
follows from Taylor’s theorem, Theorem 2.1. With this choice of weighting matrix W, the
norm (8.10) is adimensional, which is a desirable property, since we do not wish the solution
of (8.9) to depend on the units of the problem.
With this weighting matrix and this norm, the unique solution of (8.9) is
(DFP)
Bk+1 
	
I −γkyksT
k

Bk
	
I −γkskyT
k

+ γkykyT
k ,
(8.13)
with
γk 
1
yT
k sk
.

8 . 1 .
T h e B F G S M e t h o d
197
This formula is called the DFP updating formula, since it is the one originally proposed by
Davidon in 1959, and subsequently studied, implemented, and popularized by Fletcher and
Powell.
The inverse of Bk, which we denote by
Hk  B−1
k ,
is useful in the implementation of the method, since it allows the search direction (8.2)
to be calculated by means of a simple matrix–vector multiplication. Using the Sherman–
Morrison–Woodbury formula (A.56), we can derive the following expression for the update
of the inverse Hessian approximation Hk that corresponds to the DFP update of Bk in (8.13):
(DFP)
Hk+1  Hk −HkykyT
k Hk
yT
k Hkyk
+ sksT
k
yT
k sk
.
(8.14)
Note that the last two terms in the right-hand-side of (8.14) are rank-one matrices, so
that Hk undergoes a rank-two modiﬁcation. It is easy to see that (8.13) is also a rank-
two modiﬁcation of Bk. This is the fundamental idea of quasi-Newton updating: Instead
of recomputing the iteration matrices from scratch at every iteration, we apply a simple
modiﬁcation that combines the most recently observed information about the objective
function with the existing knowledge embedded in our current Hessian approximation.
The DFP updating formula is quite effective, but it was soon superseded by the BFGS
formula, which is presently considered to be the most effective of all quasi-Newton updating
formulae. BFGS updating can be derived by making a simple change in the argument that
led to (8.13). Instead of imposing conditions on the Hessian approximations Bk, we impose
similarconditionsontheirinversesHk.TheupdatedapproximationHk+1 mustbesymmetric
and positive deﬁnite, and must satisfy the secant equation (8.6), now written as
Hk+1yk  sk.
The condition of closeness to Hk is now speciﬁed by the following analogue of (8.9):
min
H ∥H −Hk∥
(8.15a)
subject to
H  H T ,
Hyk  sk.
(8.15b)
The norm is again the weighted Frobenius norm described above, where the weight matrix
W is now any matrix satisfying Wsk  yk. (For concreteness, we assume again that W is
given by the average Hessian ¯Gk deﬁned in (8.11).) The unique solution Hk+1 to (8.15) is
given by
(BFGS)
Hk+1  (I −ρkskyT
k )Hk(I −ρkyksT
k ) + ρksksT
k ,
(8.16)

198
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
where
ρk 
1
yT
k sk
.
(8.17)
Just one issue has to be resolved before we can deﬁne a complete BFGS algorithm: How
should we choose the initial approximation H0? Unfortunately, there is no magic formula
that works well in all cases. We can use speciﬁc information about the problem, for instance
by setting it to the inverse of an approximate Hessian calculated by ﬁnite differences at x0.
Otherwise, we can simply set it to be the identity matrix, or a multiple of the identity matrix,
where the multiple is chosen to reﬂect the scaling of the variables.
Algorithm 8.1 (BFGS Method).
Given starting point x0, convergence tolerance ϵ > 0,
inverse Hessian approximation H0;
k ←0;
while ∥∇fk∥> ϵ;
Compute search direction
pk  −Hk∇fk;
(8.18)
Set xk+1  xk + αkpk where αk is computed from a line search
procedure to satisfy the Wolfe conditions (3.6);
Deﬁne sk  xk+1 −xk and yk  ∇fk+1 −∇fk;
Compute Hk+1 by means of (8.16);
k ←k + 1;
end (while)
Each iteration can be performed at a cost of O(n2) arithmetic operations (plus the cost of
function and gradient evaluations); there are no O(n3) operations such as linear system
solves or matrix–matrix operations. The algorithm is robust, and its rate of convergence is
superlinear, which is fast enough for most practical purposes. Even though Newton’s method
converges more rapidly (that is, quadratically), its cost per iteration is higher because it
requires the solution of a linear system. A more important advantage for BFGS is, of course,
that it does not require calculation of second derivatives.
We can derive a version of the BFGS algorithm that works with the Hessian approx-
imation Bk rather than Hk. The update formula for Bk is obtained by simply applying the
Sherman–Morrison–Woodbury formula (A.56) to (8.16) to obtain
(BFGS)
Bk+1  Bk −BksksT
k Bk
sT
k Bksk
+ ykyT
k
yT
k sk
.
(8.19)

8 . 1 .
T h e B F G S M e t h o d
199
A naive implementation of this variant is not efﬁcient for unconstrained minimization,
because it requires the system Bkpk  −∇fk to be solved for the step pk, thereby increasing
the cost of the step computation to O(n3). We discuss later, however, that less expensive
implementations of this variant are possible by updating Cholesky factors of Bk.
PROPERTIES OF THE BFGS METHOD
It is usually easy to observe the superlinear rate of convergence of the BFGS method
on practical problems. Below, we report the last few iterations of the steepest descent, BFGS,
and an inexact Newton method on Rosenbrock’s function (2.23). The table gives the value of
∥xk −x∗∥. The Wolfe conditions were imposed on the step length in all three methods. From
the starting point (−1.2, 1), the steepest descent method required 5264 iterations, whereas
BFGS and Newton took only 34 and 21 iterations, respectively to reduce the gradient norm
to 10−5.
steep. desc.
BFGS
Newton
1.827e-04
1.70e-03
3.48e-02
1.826e-04
1.17e-03
1.44e-02
1.824e-04
1.34e-04
1.82e-04
1.823e-04
1.01e-06
1.17e-08
A few points in the derivation of the BFGS and DFP methods merit further discussion.
Note that the minimization problem (8.15) that gives rise to the BFGS update formula does
not explicitly require the updated Hessian approximation to be positive deﬁnite. It is easy to
show, however, that Hk+1 will be positive deﬁnite whenever Hk is positive deﬁnite, by using
the following argument. First, note from (8.8) that yT
k sk is positive, so that the updating
formula (8.16), (8.17) is well-deﬁned. For any nonzero vector z, we have
zT Hk+1z  wT Hkw + ρk(zT sk)2 ≥0,
where we have deﬁned w  z −ρkyk(sT
k z). The right hand side can be zero only if sT
k z  0,
but in this case w  z ̸ 0, which implies that the ﬁrst term is greater than zero. Therefore,
Hk+1 is positive deﬁnite.
In order to obtain quasi-Newton updating formulae that are invariant to changes, in
thevariables,itisnecessarythattheobjectives(8.9a)and(8.15a)bealsoinvariant.Thechoice
of the weighting matrices W used to deﬁne the norms in (8.9a) and (8.15a) ensures that this
condition holds. Many other choices of the weighting matrix W are possible, each one of
them giving a different update formula. However, despite intensive searches, no formula has
been found that is signiﬁcantly more effective than BFGS.
The BFGS method has many interesting properties when applied to quadratic func-
tions. We will discuss these properties later on, in the more general context of the Broyden
family of updating formulae, of which BFGS is a special case.

200
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
It is reasonable to ask whether there are situations in which the updating formula such
as (8.16) can produce bad results. If at some iteration the matrix Hk becomes a very poor
approximation, is there any hope of correcting it? If, for example, the inner product yT
k sk
is tiny (but positive), then it follows from (8.16)–(8.17) that Hk+1 becomes huge. Is this
behavior reasonable? A related question concerns the rounding errors that occur in ﬁnite-
precision implementation of these methods. Can these errors grow to the point of erasing
all useful information in the quasi-Newton approximate matrix?
Thesequestionshavebeenstudiedanalyticallyandexperimentally,anditisnowknown
that the BFGS formula has very effective self-correcting properties. If the matrix Hk incorrectly
estimates the curvature in the objective function, and if this bad estimate slows down the
iteration, then the Hessian approximation will tend to correct itself within a few steps. It is
also known that the DFP method is less effective in correcting bad Hessian approximations;
this property is believed to be the reason for its poorer practical performance. The self-
correcting properties of BFGS hold only when an adequate line search is performed. In
particular, the Wolfe line search conditions ensure that the gradients are sampled at points
that allow the model (8.1) to capture appropriate curvature information.
It is interesting to note that the DFP and BFGS updating formulae are duals of each
other, in the sense that one can be obtained from the other by the interchanges s ↔y,
B ↔H. This symmetry is not surprising, given the manner in which we derived these
methods above.
IMPLEMENTATION
A few details and enhancements need to be added to Algorithm 8.1 to produce an
efﬁcient implementation. The line search, which should satisfy either the Wolfe conditions
(3.6) or the strong Wolfe conditions (3.7), should always try the step length αk  1 ﬁrst,
becausethissteplengthwilleventuallyalwaysbeaccepted(undercertainconditions),thereby
producing superlinear convergence of the overall algorithm. Computational observations
strongly suggest that it is more economical, in terms of function evaluations, to perform a
fairly inaccurate line search. The values c1  10−4 and c2  0.9 are commonly used in (3.6).
As mentioned earlier, the initial matrix H0 often is set to some multiple βI of the
identity, but there is no good general strategy for choosing β. If β is “too large,” so that
the ﬁrst step p0  −βg0 is too long, many function evaluations may be required to ﬁnd a
suitable value for the step length α0. Some software asks the user to prescribe a value δ for
the norm of the ﬁrst step, and then set H0  δ∥g0∥−1I to achieve this norm.
A heuristic that is often quite effective is to scale the starting matrix after the ﬁrst step
has been computed but before the ﬁrst BFGS update is performed. We change the provisional
value H0  I by setting
H0 ←yT
k sk
yT
k yk
I,
(8.20)

8 . 1 .
T h e B F G S M e t h o d
201
before applying the update (8.16), (8.17) to obtain H1. This formula attempts to make
the size of H0 similar to that of [∇2f (x0)]−1, in the following sense. Assuming that the
average Hessian deﬁned in (8.11) is positive deﬁnite, there exists a square root ¯G1/2
k
satisfying
¯Gk  ¯G1/2
k
¯G1/2
k
(see Exercise 5). Therefore, by deﬁning zk  ¯G1/2
k sk and using the relation
(8.12), we have
yT
k sk
yT
k yk

( ¯G1/2
k sk)T ¯G1/2
k sk
( ¯G1/2
k sk)T ¯Gk ¯G1/2
k sk

zT
k zk
zT
k ¯Gkzk
.
(8.21)
The reciprocal of (8.21) is an approximation to one of the eigenvalues of ¯Gk, which in
turn is close to an eigenvalue of ∇2f (xk). Hence, the quotient (8.21) itself approximates an
eigenvalue of [∇2f (xk)]−1. Other scaling factors can be used in (8.20), but the one presented
here appears to be the most successful in practice.
In (8.19) we gave an update formula for a BFGS method that works with the Hes-
sian approximation Bk instead of the the inverse Hessian approximation Hk. An efﬁcient
implementation of this approach does not store Bk explicitly, but rather the Cholesky fac-
torization LkDkLT
k of this matrix. A formula that updates the factors Lk and Dk directly in
O(n2) operations can be derived from (8.19). Since the linear system Bkpk  −∇fk also
can be solved in O(n2) operations (by performing triangular substitutions with Lk and LT
k
and a diagonal substitution with Dk), the total cost is quite similar to the variant described
in Algorithm 8.1. A potential advantage of this alternative strategy is that it gives us the
option of modifying diagonal elements in the Dk factor if they are not sufﬁciently large, to
prevent instability when we divide by these elements during the calculation of pk. However,
computational experience suggests no real advantages for this variant, and we prefer the
simpler strategy of Algorithm 8.1.
The performance of the BFGS method can degrade if the line search is not based
on the Wolfe conditions. For example, some software implements an Armijo backtracking
line search (see Section 3.1): The unit step length αk  1 is tried ﬁrst and is successively
decreased until the sufﬁcient decrease condition (3.6a) is satisﬁed. For this strategy, there is
no guarantee that the curvature condition yT
k sk > 0 (8.7) will be satisﬁed by the chosen step,
since a step length greater than 1 may be required to satisfy this condition. To cope with this
shortcoming, some implementations simply skip the BFGS update by setting Hk+1  Hk
when yT
k sk is negative or too close to zero. This approach is not recommended, because
the updates may be skipped much too often to allow Hk to capture important curvature
information for the objective function f . In Chapter 18 we discuss a damped BFGS update
that is a more effective strategy for coping with the case where the curvature condition (8.7)
is not satisﬁed.

202
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
8.2
THE SR1 METHOD
In the BFGS and DFP updating formulae, the updated matrix Bk+1 (or Hk+1) differs from
its predecessor Bk (or Hk) by a rank-2 matrix. In fact, as we now show, there is a simpler
rank-1 update that maintains symmetry of the matrix and allows it to satisfy the secant
equation. Unlike the rank-two update formulae, this symmetric-rank-1, or SR1, update does
not guarantee that the updated matrix maintains positive deﬁniteness. Good numerical
results have been obtained with algorithms based on SR1, so we derive it here and investigate
its properties.
The symmetric rank-1 update has the general form
Bk+1  Bk + σvvT ,
where σ is either +1 or −1, and σ and v are chosen so that Bk+1 satisﬁes the secant equation
(8.6), that is, yk  Bk+1sk. By substituting into this equation, we obtain
yk  Bksk +

σvT sk

v.
(8.22)
Since the term in brackets is a scalar, we deduce that v must be a multiple of yk −Bksk, that
is, v  δ(yk −Bksk) for some scalar δ. By substituting this form of v into (8.22), we obtain
(yk −Bksk)  σδ2 
sT
k (yk −Bksk)

(yk −Bksk),
(8.23)
and it is clear that this equation is satisﬁed if (and only if) we choose the parameters δ and
σ to be
σ  sign

sT
k (yk −Bksk)

,
δ  ±
sT
k (yk −Bksk)
−1/2 .
Hence, we have shown that the only symmetric rank-1 updating formula that satisﬁes the
secant equation is given by
(SR1)
Bk+1  Bk + (yk −Bksk)(yk −Bksk)T
(yk −Bksk)T sk
.
(8.24)
By applying the Sherman–Morrison formula (A.55), we obtain the corresponding update
formula for the inverse Hessian approximation Hk:
(SR1)
Hk+1  Hk + (sk −Hkyk)(sk −Hkyk)T
(sk −Hkyk)T yk
.
(8.25)
This derivation is so simple that the SR1 formula has been rediscovered a number of times.
It is easy to see that even if Bk is positive deﬁnite, Bk+1 may not have this property;
the same is, of course, true of Hk. This observation was considered a major drawback in the

8 . 2 .
T h e S R 1 M e t h o d
203
early days of nonlinear optimization when only line search iterations were used. However,
with the advent of trust-region methods, the SR1 updating formula has proved to be quite
useful, and its ability to generate indeﬁnite Hessian approximations can actually be regarded
as one of its chief advantages.
The main drawback of SR1 updating is that the denominator in (8.24) or (8.25) can
vanish. In fact, even when the objective function is a convex quadratic, there may be steps
on which there is no symmetric rank-1 update that satisﬁes the secant equation. It pays to
reexamine the derivation above in the light of this observation.
By reasoning in terms of Bk (similar arguments can be applied to Hk), we see that
there are three cases:
1. If (yk −Bksk)T sk ̸ 0, then the arguments above show that there is a unique rank-one
updating formula satisfying the secant equation (8.6), and that it is given by (8.24).
2. If yk  Bksk, then the only updating formula satisfying the secant equation is simply
Bk+1  Bk.
3. If yk ̸ Bksk and (yk −Bksk)T sk  0, then (8.23) shows that there is no symmetric
rank-one updating formula satisfying the secant equation.
The last case clouds an otherwise simple and elegant derivation, and suggests that numerical
instabilitiesandevenbreakdownofthemethodcanoccur.Itsuggeststhatrank-oneupdating
does not provide enough freedom to develop a matrix with all the desired characteristics, and
that a rank-two correction is required. This reasoning leads us back to the BFGS method,
in which positive deﬁniteness (and thus nonsingularity) of all Hessian approximations is
guaranteed.
Nevertheless, we are interested in the SR1 formula for the following reasons.
(i) A simple safeguard seems to adequately prevent the breakdown of the method and the
occurrence of numerical instabilities.
(ii) The matrices generated by the SR1 formula tend to be very good approximations of
the Hessian matrix—often better than the BFGS approximations.
(iii) In quasi-Newton methods for constrained problems, or in methods for partially sepa-
rable functions (see Chapters 18 and 9), it may not be possible to impose the curvature
condition yT
k sk > 0, and thus BFGS updating is not recommended. Indeed, in these
two settings, indeﬁnite Hessian approximations are desirable insofar as they reﬂect
indeﬁniteness in the true Hessian.
We now introduce a strategy to prevent the SR1 method from breaking down. It
has been observed in practice that SR1 performs well simply by skipping the update if the
denominator is small. More speciﬁcally, the update (8.24) is applied only if
sT
k (yk −Bksk)
 ≥r∥sk∥∥yk −Bksk∥,
(8.26)

204
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
where r ∈(0, 1) is a small number, say r  10−8. If (8.26) does not hold, we set Bk+1  Bk.
Most implementations of the SR1 method use a skipping rule of this kind.
Why do we advocate skipping of updates for the SR1 method, when in the previous
section we discouraged this strategy in the case of BFGS? The two cases are quite different.
The condition sT
k (yk −Bksk) ≈0 occurs infrequently, since it requires certain vectors to
be aligned in a speciﬁc way. When it does occur, skipping the update appears to have no
negative effects on the iteration. This is not surprising, since the skipping condition implies
that sT
k ¯Gsk ≈sT
k Bksk, where ¯G is the average Hessian over the last step—meaning that the
curvature of Bk along sk is already correct. In contrast, the curvature condition sT
k yk ≥0
required for BFGS updating may easily fail if the line search does not impose the Wolfe
conditions (e.g., if the step is not long enough), and therefore skipping the BFGS update can
occur often and can degrade the quality of the Hessian approximation.
We now give a formal description of an SR1 method using a trust-region framework.
We prefer it over a line search framework because it does not require us to modify the Hessian
approximations to make them sufﬁciently positive deﬁnite.
Algorithm 8.2 (SR1 Trust-Region Method).
Given starting point x0, initial Hessian approximation B0,
trust-region radius 0, convergence tolerance ϵ > 0,
parameters η ∈(0, 10−3) and r ∈(0, 1);
k ←0;
while ∥∇fk∥> ϵ;
Compute sk by solving the subproblem
min
s
∇f T
k s + 1
2sT Bks
subject to ∥s∥≤k.
(8.27)
Compute
yk  ∇f (xk + sk) −∇fk,
ared  fk −f (xk + sk)
(actual reduction)
pred  −

∇f T
k sk + 1
2sT
k Bksk

(predicted reduction);
if ared/pred > η
xk+1  xk + sk
else
xk+1  xk;
end (if)
if ared/pred > 0.75

8 . 2 .
T h e S R 1 M e t h o d
205
if ∥sk∥≤0.8k
k+1  k
else
k+1  2k;
end (if)
elseif 0.1 ≤ared/pred ≤0.75
k+1  k
else
k+1  0.5k;
end (if)
if (8.26) holds
Use (8.24) to compute Bk+1 (even if xk+1  xk)
else
Bk+1 ←Bk;
end (if)
k ←k + 1;
end (while)
This algorithm has the typical form of a trust region method (cf. Algorithm 4.1). For
concreteness we have speciﬁed a particular strategy for updating the trust region radius, but
other heuristics can be used instead.
To obtain a fast rate of convergence, it is important for the matrix Bk to be updated
even along a failed direction sk. The fact that the step was poor indicates that Bk is a poor
approximationofthetrueHessianinthisdirection.Unlessthequalityoftheapproximationis
improved, steps along similar directions could be generated on later iterations, and repeated
rejection of such steps could prevent superlinear convergence.
PROPERTIES OF SR1 UPDATING
One of the main advantages of SR1 updating is its ability to generate very good Hessian
approximations. We demonstrate this property by ﬁrst examining a quadratic function. For
functions of this type, the choice of step length does not affect the update, so to examine the
effect of the updates, we can assume for simplicity a uniform step length of 1, that is,
pk  −Hk∇fk,
xk+1  xk + pk.
(8.28)
It follows that pk  sk.
Theorem 8.1.
Suppose that f : IRn →IR is the strongly convex quadratic function f (x)  bT x +
1
2xT Ax,whereAissymmetricpositivedeﬁnite.Thenforanystartingpointx0 andanysymmetric
starting matrix H0, the iterates {xk} generated by the SR1 method (8.25), (8.28) converge to the

206
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
minimizer in at most n steps, provided that (sk −Hkyk)T yk ̸ 0 for all k. Moreover, if n steps
are performed, and if the search directions pi are linearly independent, then Hn  A−1.
Proof.
Because of our assumption (sk −Hkyk)T yk ̸ 0, the SR1 update is always well-
deﬁned. We start by showing inductively that
Hkyj  sj
for
j  0, . . . , k −1.
(8.29)
In other words, we claim that the secant equation is satisﬁed not only along the most recent
search direction, but along all previous directions.
By deﬁnition, the SR1 update satisﬁes the secant equation, so we have H1y0  s0. Let
us now assume that (8.29) holds for some value k > 1 and show that it holds also for k + 1.
From this assumption, we have from (8.29) that
(sk −Hkyk)T yj  sT
k yj −yT
k (Hkyj)  sT
k yj −yT
k sj  0,
for all j < k,
(8.30)
wherethelastequalityfollowsbecauseyi  Asi forthequadraticfunctionweareconsidering
here. By using (8.30) and the induction hypothesis (8.29) in (8.25), we have
Hk+1yj  Hkyj  sj,
for all j < k.
Since Hk+1yk  sk by the secant equation, we have shown that (8.29) holds when k is
replaced by k + 1. By induction, then, this relation holds for all k.
If the algorithm performs n steps and if these steps {sj} are linearly independent, we
have
sj  Hnyj  HnAsj,
for j  0, . . . , n −1.
It follows that HnA  I, that is, Hn  A−1. Therefore, the step taken at xn is the Newton
step, and so the next iterate xn+1 will be the solution, and the algorithm terminates.
Consider now the case in which the steps become linearly dependent. Suppose that sk
is a linear combination of the previous steps, that is,
sk  ξ0s0 + · · · + ξk−1sk−1,
(8.31)
for some scalars ξi. From (8.31) and (8.29) we have that
Hkyk  HkAsk
 ξ0HkAs0 + · · · + ξk−1HkAsk−1
 ξ0Hky0 + · · · + ξk−1Hkyk−1
 ξ0s0 + · · · + ξk−1sk−1
 sk.

8 . 3 .
T h e B r o y d e n C l a s s
207
Since yk  ∇fk+1 −∇fk and since sk  pk  −Hk∇fk from (8.28), we have that
Hk(∇fk+1 −∇fk)  −Hk∇fk,
which, by the nonsingularity of Hk, implies that ∇fk+1  0. Therefore, xk+1 is the solution
point.
□
The relation (8.29) shows that when f is quadratic, the secant equation is satisﬁed
along all previous search directions, regardless of how the line search is performed. A result
like this can be established for BFGS updating only under the restrictive assumption that the
line search is exact, as we show in the next section.
For general nonlinear functions, the SR1 update continues to generate good Hessian
approximations under certain conditions.
Theorem 8.2.
Suppose that f is twice continuously differentiable, and that its Hessian is bounded and
Lipschitz continuous in a neighborhood of a point x∗. Let {xk} be any sequence of iterates such
that xk →x∗for some x∗∈IRn. Suppose in addition that the inequality (8.26) holds for all k,
for some r ∈(0, 1), and that the steps sk are uniformly linearly independent. Then the matrices
Bk generated by the SR1 updating formula satisfy
lim
k→∞∥Bk −∇2f (x∗)∥ 0.
The term “uniformly linearly independent steps” means, roughly speaking, that the
steps do not tend to fall in a subspace of dimension less than n. This assumption is usually,
but not always, satisﬁed in practice. (See the Notes and Remarks at the end of this chapter.)
8.3
THE BROYDEN CLASS
So far, we have described the BFGS, DFP, and SR1 quasi-Newton updating formulae, but
there are many others. Of particular interest is the Broyden class, a family of updates speciﬁed
by the following general formula:
Bk+1  Bk −BksksT
k Bk
sT
k Bksk
+ ykyT
k
yT
k sk
+ φk(sT
k Bksk)vkvT
k ,
(8.32)
where φk is a scalar parameter and
vk 
 yk
yT
k sk
−
Bksk
sT
k Bksk

.
(8.33)

208
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
The BFGS and DFP methods are members of the Broyden class—we recover BFGS by setting
φk  0 and DFP by setting φk  1 in (8.32). We can therefore rewrite (8.32) as a “linear
combination” of these two methods, that is,
Bk+1  (1 −φk)B
BFGS
k+1 + φkB
DFP
k+1.
This relationship indicates that all members of the Broyden class satisfy the secant equation
(8.6),sincetheBGFSandDFPmatricesthemselvessatisfythisequation.Also,sinceBFGSand
DFP updating preserve positive deﬁniteness of the Hessian approximations when sT
k yk > 0,
this relation implies that the same property will hold for the Broyden family if 0 ≤φk ≤1.
Much attention has been given to the so-called restricted Broyden class, which is ob-
tained by restricting φk to the interval [0, 1]. It enjoys the following property when applied
to quadratic functions. Since the analysis is independent of the step length, we assume for
simplicity that each iteration has the form
pk  −B−1
k ∇fk,
xk+1  xk + pk.
(8.34)
Theorem 8.3.
Suppose that f : IRn →IR is the strongly convex quadratic function f (x)  bT x +
1
2xT Ax, where A is symmetric and positive deﬁnite. Let x0 be any starting point for the iteration
(8.34) and B0 be any symmetric positive deﬁnite starting matrix, and suppose that the matrices
Bk are updated by the Broyden formula (8.32) with φk ∈[0, 1]. Deﬁne λk
1 ≤λk
2 ≤· · · ≤λk
n
to be the eigenvalues of the matrix
A
1
2 B−1
k A
1
2 .
(8.35)
Then for all k, we have
min{λk
i , 1} ≤λk+1
i
≤max{λk
i , 1},
i  1, . . . , n.
(8.36)
Moreover, the property (8.36) does not hold if the Broyden parameter φk is chosen outside the
interval [0, 1].
Let us discuss the signiﬁcance of this result. If the eigenvalues λk
i of the matrix (8.35)
are all 1, then the quasi-Newton approximation Bk is identical to the Hessian A of the
quadratic objective function. This situation is the ideal one, so we should be hoping for
these eigenvalues to be as close to 1 as possible. In fact, relation (8.36) tells us that the
eigenvalues {λk
i } converge monotonically (but not strictly monotonically) to 1. Suppose, for
example, that at iteration k the smallest eigenvalue is λk
1  0.7. Then (8.36) tells us that
at the next iteration λk+1
1
∈[0.7, 1]. We cannot be sure that this eigenvalue has actually
gotten closer to 1, but it is reasonable to expect that it has. In contrast, the ﬁrst eigenvalue

8 . 3 .
T h e B r o y d e n C l a s s
209
can become smaller than 0.7 if we allow φk to be outside [0, 1]. Signiﬁcantly, the result of
Theorem 8.3 holds even if the linear searches are not exact.
Although Theorem 8.3 seems to suggest that the best update formulas belong to the
restricted Broyden class, the situation is not at all clear. Some analysis and computational
testing suggest that algorithms that allow φk to be negative (in a strictly controlled manner)
may in fact be superior to the BFGS method. The SR1 formula is a case in point: It is a
member of the Broyden class, obtained by setting
φk 
sT
k yk
sT
k yk −sT
k Bksk
,
but it does not belong to the restricted Broyden class, because this value of φk may fall outside
the interval [0, 1].
We complete our discussion of the Broyden class by informally stating some of its
main properties.
PROPERTIES OF THE BROYDEN CLASS
We have noted already that if Bk is positive deﬁnite, yT
k sk > 0, and φk ≥0, then
Bk+1 is also positive deﬁnite if a restricted Broyden class update, with φk ∈[0, 1], is used.
We would like to determine more precisely the range of values of φk that preserve positive
deﬁniteness.
The last term in (8.32) is a rank-one correction, which by the interlacing eigenvalue
theorem (Theorem A.2) decreases the eigenvalues of the matrix when φk is negative. As we
decreaseφk,thismatrixeventuallybecomessingularandthenindeﬁnite.Alittlecomputation
shows that Bk+1 is singular when φk has the value
φc
k 
1
1 −µk
,
(8.37)
where
µk  (yT
k B−1
k yk)(sT
k Bksk)
(yT
k sk)2
.
(8.38)
By applying the Cauchy–Schwarz inequality to (8.38) we see that µk ≥1 and therefore
φc
k ≤0. Hence, if the initial Hessian approximation B0 is symmetric and positive deﬁnite,
and if sT
k yk > 0 and φk > φc
k for each k, then all the matrices Bk generated by Broyden’s
formula (8.32) remain symmetric and positive deﬁnite.
When the line search is exact, all methods in the Broyden class with φk ≥φc
k generate
the same sequence of iterates. This result applies to general nonlinear functions and is
based on the observation that when all the line searches are exact, the directions generated
by Broyden-class methods differ only in their lengths. The line searches identify the same

210
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
minima along the chosen search direction, though the values of the line search parameter
may differ because of the different scaling.
The Broyden class has several remarkable properties when applied with exact line
searches to quadratic functions. We state some of these properties in the next theorem,
whose proof is omitted.
Theorem 8.4.
Suppose that a method in the Broyden class is applied to a strongly convex quadratic
function f : IRn →IR, where x0 is the starting point and B0 is any symmetric and positive
deﬁnite matrix. Assume that αk is the exact step length and that φk ≥φc
k for all k. Then the
following statements are true.
(i) The iterates converge to the solution in at most n iterations.
(ii) The secant equation is satisﬁed for all previous search directions, that is,
Bksj  yj,
j  k −1, . . . , 1.
(iii) If the starting matrix is B0  I, then the iterates are identical to those generated by
the conjugate gradient method (see Chapter 5). In particular, the search directions are
conjugate, that is,
sT
i Asj  0
for i ̸ j,
where A is the Hessian of the quadratic function.
(iv) If n iterations are performed, we have Bn+1  A.
Note that parts (i), (ii), and (iv) of this result echo the statement and proof of Theorem 8.1,
where similar results were derived for the SR1 update formula.
In fact, we can generalize Theorem 8.4 slightly: It continues to hold if the Hessian
approximations remain nonsingular but not necessarily positive deﬁnite. (Hence, we could
allow φk to be smaller than φc
k, provided that the chosen value did not produce a singular
updated matrix.) We also can generalize point (iii) as follows: If the starting matrix B0 is
not the identity matrix, then the Broyden-class method is identical to the preconditioned
conjugate gradient method that uses B0 as preconditioner.
We conclude by commenting that results like Theorem 8.4 would appear to be mainly
of theoretical interest, since the inexact line searches used in practical implementations
of Broyden-class methods (and all other quasi-Newton methods) cause their performance
to differ markedly. Nevertheless, this type of analysis guided most of the development of
quasi-Newton methods.

8 . 4 .
C o n v e r g e n c e A n a l y s i s
211
8.4
CONVERGENCE ANALYSIS
In this section we present global and local convergence results for practical implementations
of the BFGS and SR1 methods. We give many more details for BFGS because its analysis is
more general and illuminating than that of SR1. The fact that the Hessian approximations
evolve by means of updating formulas makes the analysis of quasi-Newton methods much
more complex than that of steepest descent and Newton’s method.
Although the BFGS and SR1 methods are known to be remarkably robust in practice,
wewillnotbeabletoestablishtrulyglobalconvergenceresultsforgeneralnonlinearobjective
functions.Thatis,wecannotprovethattheiteratesofthesequasi-Newtonmethodsapproach
a stationary point of the problem from any starting point and any (suitable) initial Hessian
approximation. In fact, it is not yet known if the algorithms enjoy such properties. In our
analysis we will either assume that the objective function is convex or that the iterates satisfy
certain properties. On the other hand, there are well known local, superlinear convergence
results that are true under reasonable assumptions.
Throughout this section we use ∥· ∥to denote the Euclidean vector or matrix norm,
and denote the Hessian matrix ∇2f (x) by G(x).
GLOBAL CONVERGENCE OF THE BFGS METHOD
We study the global convergence of BFGS, with a practical line search, when applied to
a smooth convex function from an arbitrary starting point x0 and from any initial Hessian
approximation B0 that is symmetric and positive deﬁnite. We state our precise assumptions
about the objective function formally:
Assumption 8.1.
(1) The objective function f is twice continuously differentiable.
(2) The level set   {x ∈IRn : f (x) ≤f (x0)} is convex, and there exist positive constants
m and M such that
m∥z∥2 ≤zT G(x)z ≤M∥z∥2
(8.39)
for all z ∈IRn and x ∈.
The second part of this assumption implies that G(x) is positive deﬁnite on  and that f
has a unique minimizer x∗in .
By using (8.12) and (8.39) we obtain
yT
k sk
sT
k sk
 sT
k ¯Gksk
sT
k sk
≥m,
(8.40)

212
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
where ¯Gk is the average Hessian deﬁned in (8.11). Assumption 8.1 implies that ¯Gk is positive
deﬁnite, so its square root is well-deﬁned. Therefore, as in (8.21), we have by deﬁning
zk  ¯G1/2
k sk that
yT
k yk
yT
k sk
 zT
k ¯Gkzk
zT
k zk
≤M.
(8.41)
We are now ready to present the global convergence result for the BFGS method. As
in Section 3.2, we could try to establish a bound on the condition number of the Hessian
approximationsBk,butthisapproachdoesnotseemtobepossible.Instead,wewillintroduce
two new tools in the analysis, the trace and determinant, to estimate the size of the largest
and smallest eigenvalues of the Hessian approximations. The trace of a matrix (denoted
by trace(·)) is the sum of its eigenvalues, while the determinant (denoted by det(·)) is the
product of the eigenvalues; see the Appendix for a brief discussion of their properties.
Theorem 8.5.
Let B0 be any symmetric positive deﬁnite initial matrix, and let x0 be a starting point for
which Assumption 8.1 is satisﬁed. Then the sequence {xk} generated by Algorithm 8.1 converges
to the minimizer x∗of f .
Proof.
Let us deﬁne
mk  yT
k sk
sT
k sk
,
Mk  yT
k yk
yT
k sk
,
(8.42)
and note from (8.40) and (8.41) that
mk ≥m,
Mk ≤M.
(8.43)
By computing the trace of the BFGS approximation (8.19), we obtain that
trace(Bk+1)  trace(Bk) −∥Bksk∥2
sT
k Bksk
+ ∥yk∥2
yT
k sk
(8.44)
(see Exercise 10). We can also show (Exercise 9) that
det(Bk+1)  det(Bk) yT
k sk
sT
k Bksk
.
(8.45)
Let us also deﬁne
cos θk 
sT
k Bksk
∥sk∥∥Bksk∥,
qk  sT
k Bksk
sT
k sk
,
(8.46)

8 . 4 .
C o n v e r g e n c e A n a l y s i s
213
so that θk is the angle between sk and Bksk. We then obtain that
∥Bksk∥2
sT
k Bksk
 ∥Bksk∥2∥sk∥2
(sT
k Bksk)2
sT
k Bksk
∥sk∥2

qk
cos2 θk
.
(8.47)
In addition, we have from (8.42) that
det(Bk+1)  det(Bk)yT
k sk
sT
k sk
sT
k sk
sT
k Bksk
 det(Bk)mk
qk
.
(8.48)
We now combine the trace and determinant by introducing the following function of
a positive deﬁnite matrix B:
ψ(B)  trace(B) −ln(det(B)),
(8.49)
where ln(·) denotes the natural logarithm. It is not difﬁcult to show that ψ(B) > 0; see
Exercise 8. By using (8.42) and (8.44)–(8.49), we have that
ψ(Bk+1)  ψ(Bk) + Mk −
qk
cos2 θk
−ln(det(Bk)) −ln mk + ln qk
 ψ(Bk) + (Mk −ln mk −1)
+

1 −
qk
cos2 θk
+ ln
qk
cos2 θk

+ ln cos2 θk.
(8.50)
Now, since the function h(t)  1 −t + ln t ≤0 is nonpositive for all t > 0 (see Exercise 7),
the term inside the square brackets is nonpositive, and thus from (8.43) and (8.50) we have
0 < ψ(Bk+1) ≤ψ(B1) + ck +
k

j1
ln cos2 θj,
(8.51)
where we can assume the constant c  M −ln m−1 to be positive, without loss of generality.
We now relate these expressions to the results given in Section 3.2. Note from the form
sk  −αkB−1
k ∇fk of the quasi-Newton iteration that cos θk deﬁned by (8.46) is the angle
between the steepest descent direction and the search direction, which plays a crucial role in
the global convergence theory of Chapter 3. From (3.22), (3.23) we know that the sequence
∥∇fk∥generated by the line search algorithm is bounded away from zero only if cos θj →0.
Let us then proceed by contradiction and assume that cos θj →0. Then there exists
k1 > 0 such that for all j > k1, we have
ln cos2 θj < −2c,

214
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
where c is the constant deﬁned above. Using this inequality in (8.51) we ﬁnd the following
relations to be true for all k > k1:
0 < ψ(B1) + ck +
k1

j1
ln cos2 θj +
k

jk1+1
(−2c)
 ψ(B1) +
k1

j1
ln cos2 θj + 2ck1 −ck.
However, the right-hand-side is negative for large k, giving a contradiction. Therefore, there
exists a subsequence of indices {jk} such that {cos θjk} ≥δ > 0. By Zoutendijk’s result (3.14)
this limit implies that lim inf ∥∇fk∥→0. Since the problem is strongly convex, the latter
limit is enough to prove that xk →x∗.
□
Theorem 8.5 has been generalized to the entire restricted Broyden class, except for
the DFP method. In other words, Theorem 8.5 can be shown to hold for all φk ∈[0, 1)
in (8.32), but the argument seems to break down as φk approaches 1 because some of the
self-correcting properties of the update are weakened considerably.
An extension of the analysis just given shows that the rate of convergence of the iterates
is linear. In particular, we can show that the sequence ∥xk −x∗∥converges to zero rapidly
enough that
∞

k1
∥xk −x∗∥< ∞.
(8.52)
We will not prove this claim, but rather establish that if (8.52) holds, then the rate of
convergence is actually superlinear.
SUPERLINEAR CONVERGENCE OF BFGS
The analysis of this section makes use of the Dennis and Mor´e characterization (3.32)
of superlinear convergence. It applies to general nonlinear—not just convex—objective
functions. For the results that follow we need to make an additional assumption.
Assumption 8.2.
The Hessian matrix G is Lipschitz continuous at x∗, that is,
∥G(x) −G(x∗)∥≤L∥x −x∗∥,
for all x near x∗, where L is a positive constant.

8 . 4 .
C o n v e r g e n c e A n a l y s i s
215
We start by introducing the quantities
˜sk  G1/2
∗sk,
˜yk  G−1/2
∗
yk,
˜Bk  G−1/2
∗
BkG−1/2
∗
,
where G∗ G(x∗) and x∗is a minimizer of f . As in (8.46), we deﬁne
cos ˜θk 
˜sT
k ˜Bk ˜sk
∥˜sk∥∥˜Bk ˜sk∥
,
˜qk  ˜sT
k ˜Bk ˜sk
∥˜sk∥2 ,
while we echo (8.42) and (8.43) in deﬁning
˜Mk  ∥˜yk∥2
˜yT
k ˜sk
,
˜mk  ˜yT
k ˜sk
˜sT
k ˜sk
.
By pre- and postmultiplying the BFGS update formula (8.19) by G−1/2
∗
and grouping
terms appropriately, we obtain
˜Bk+1  ˜Bk −
˜Bk ˜sk ˜sT
k ˜Bk
˜sT
k ˜Bk ˜sk
+ ˜yk ˜yT
k
˜yT
k ˜sk
.
Since this expression has precisely the same form as the BFGS formula (8.19), it follows from
the argument leading to (8.50) that
ψ( ˜Bk+1)  ψ( ˜Bk) + ( ˜Mk −ln ˜mk −1)
+

1 −
˜qk
cos2 ˜θk
+ ln
˜qk
cos2 ˜θk

(8.53)
+ ln cos2 ˜θk.
Recalling (8.12), we have that
yk −G∗sk  ( ¯Gk −G∗)sk,
and thus
˜yk −˜sk  G−1/2
∗
( ¯Gk −G∗)G−1/2
∗
˜sk.
By Assumption 8.2, and recalling the deﬁnition (8.11), we have
∥˜yk −˜sk∥≤∥G−1/2
∗
∥2∥˜sk∥∥¯Gk −G∗∥≤∥G−1/2
∗
∥2∥˜sk∥Lϵk,

216
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
where ϵk is deﬁned by
ϵk  max{∥xk+1 −x∗∥, ∥xk −x∗∥}.
We have thus shown that
∥˜yk −˜sk∥
∥˜sk∥
≤¯cϵk,
(8.54)
for some positive constant ¯c. This inequality and (8.52) play an important role in superlinear
convergence, as we now show.
Theorem 8.6.
Suppose that f is twice continuously differentiable and that the iterates generated by the
BFGS algorithm converge to a minimizer x∗at which Assumption 8.2 holds. Suppose also that
(8.52) holds. Then xk converges to x∗at a superlinear rate.
Proof.
From (8.54), we have from the triangle inequality (A.36a) that
∥˜yk∥−∥˜sk∥≤¯cϵk∥˜sk∥,
∥˜sk∥−∥˜yk∥≤¯cϵk∥˜sk∥,
so that
(1 −¯cϵk)∥˜sk∥≤∥˜yk∥≤(1 + ¯cϵk)∥˜sk∥.
(8.55)
By squaring (8.54) and using (8.55), we obtain
(1 −¯cϵk)2∥˜sk∥2 −2 ˜yT
k ˜sk + ∥˜sk∥2 ≤∥˜yk∥2 −2 ˜yT
k ˜sk + ∥˜sk∥2 ≤¯c2ϵ2
k∥˜sk∥2,
and therefore
2 ˜yT
k ˜sk ≥(1 −2¯cϵk + ¯c2ϵ2
k + 1 −¯c2ϵ2
k)∥˜sk∥2  2(1 −¯cϵk)∥˜sk∥2.
It follows from the deﬁnition of ˜mk that
˜mk  ˜yT
k ˜sk
∥˜sk∥2 ≥1 −¯cϵk.
(8.56)
By combining (8.55) and (8.56), we obtain also that
˜Mk  ∥˜yk∥2
˜yT
k ˜sk
≤1 + ¯cϵk
1 −¯cϵk
.
(8.57)

8 . 4 .
C o n v e r g e n c e A n a l y s i s
217
Since xk →x∗, we have that ϵk →0, and thus by (8.57) there exists a positive constant
c > ¯c such that the following inequalities hold for all sufﬁciently large k:
˜Mk ≤1 +
2¯c
1 −¯cϵk
ϵk ≤1 + cϵk.
(8.58)
We again make use of the nonpositiveness of the function h(t)  1 −t + ln t. Therefore, we
have
−x
1 −x −ln(1 −x)  h

1
1 −x

≤0.
Now, for k large enough we can assume that ¯cϵk < 1
2, and therefore
ln(1 −¯cϵk) ≥
−¯cϵk
1 −¯cϵk
≥−2¯cϵk.
This relation and (8.56) imply that for sufﬁciently large k, we have
ln ˜mk ≥ln(1 −¯cϵk) ≥−2¯cϵk > −2cϵk.
(8.59)
We can now deduce from (8.53), (8.58), and (8.59) that
0 < ψ( ˜Bk+1) ≤ψ( ˜Bk) + 3cϵk + ln cos2 ˜θk +

1 −
˜qk
cos2 ˜θk
+ ln
˜qk
cos2 ˜θk

.
(8.60)
By summing this expression and making use of (8.52) we have that
∞

j0
#
ln
1
cos2 ˜θj
−

1 −
˜qj
cos2 ˜θj
+ ln
˜qj
cos2 ˜θj
$
≤ψ( ˜B0) + 3c
∞

j0
ϵj < +∞.
Since the term in the square brackets is nonpositive, and since ln

1/ cos2 ˜θj

≥0 for all j,
we obtain the two limits
lim
j→∞ln
1
cos2 ˜θj
 0,
lim
j→∞
#
1 −
˜qj
cos2 ˜θj
+ ln
˜qj
cos2 ˜θj
$
 0,
which imply that
lim
j→∞cos ˜θj  1,
lim
j→∞˜qj  1.
(8.61)

218
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
The essence of the result has now been proven; we need only to interpret these limits
in terms of the Dennis–Mor´e characterization of superlinear convergence.
Recalling (8.47), we have
∥G−1/2
∗
(Bk −G∗)sk∥2
∥G1/2
∗sk∥2
 ∥( ˜Bk −I)˜sk∥2
∥˜sk∥2
 ∥˜Bk ˜sk∥2 −2˜sT
k ˜Bk ˜sk + ˜sT
k ˜sk
˜sT
k ˜sk

˜q2
k
cos ˜θ2
k
−2˜qk + 1.
Since by (8.61) the right-hand-side converges to 0, we conclude that
lim
k→∞
∥(Bk −G∗)sk∥
∥sk∥
 0.
The limit (3.32) and Theorem 3.5 imply that the unit step length αk  1 will satisfy the Wolfe
conditions near the solution, and hence that the rate of convergence is superlinear.
□
CONVERGENCE ANALYSIS OF THE SR1 METHOD
TheconvergencepropertiesoftheSR1methodarenotaswellunderstoodasthoseofthe
BFGSmethod.NoglobalresultslikeTheorem8.5orlocalsuperlinearresultslikeTheorem8.6
have been established, except the results for quadratic functions discussed earlier. There is,
however, an interesting result for the trust-region SR1 algorithm, Algorithm 8.2. It states that
when the objective function has a unique stationary point and the condition (8.26) holds
at every step (so that the SR1 update is never skipped) and the Hessian approximations Bk
are bounded above, then the iterates converge to x∗at an (n + 1)-step superlinear rate. The
result does not require exact solution of the trust-region subproblem (8.27).
We state the result formally as follows.
Theorem 8.7.
Supposethattheiteratesxk aregeneratedbyAlgorithm8.2.Supposealsothatthefollowing
conditions hold:
(c1) The sequence of iterates does not terminate, but remains in a closed, bounded, convex set
D, on which the function f is twice continuously differentiable, and in which f has a
unique stationary point x∗;
(c2) the Hessian ∇2f (x∗) is positive deﬁnite, and ∇2f (x) is Lipschitz continuous in a
neighborhood of x∗;
(c3) the sequence of matrices {Bk} is bounded in norm;

8 . 4 .
C o n v e r g e n c e A n a l y s i s
219
(c4) condition (8.26) holds at every iteration, where r is some constant in (0, 1).
Then limk→∞xk  x∗, and we have that
lim
k→∞
∥xk+n+1 −x∗∥
∥xk −x∗∥
 0.
Note that the BFGS method does not require the boundedness assumption (c3) to
hold. As we have mentioned already, the SR1 update does not necessarily maintain positive
deﬁniteness of the Hessian approximations Bk. In practice, Bk may be indeﬁnite at any
iteration, which means that the trust region bound may continue to be active for arbitrarily
large k. Interestingly, however, it can be shown that the SR1 Hessian approximations tend
to be positive deﬁnite most of the time. The precise result is that
lim
k→∞
number of indices j  1, 2, . . . , k for which Bj is positive semideﬁnite
k
 1,
under the assumptions of Theorem 8.7. This result holds regardless of whether the initial
Hessian approximation is positive deﬁnite or not.
NOTES AND REFERENCES
For a comprehensive treatment of quasi-Newton methods see Dennis and Schn-
abel [69], Dennis and Mor´e [68], and Fletcher [83]. A formula for updating the Cholesky
factors of the BFGS matrices is given in Dennis and Schnabel [69].
Several safeguards and modiﬁcations of the SR1 method have been proposed, but
the condition (8.26) is favored in the light of the analysis of Conn, Gould, and Toint [52].
Computational experiments by Conn, Gould, and Toint [51, 54] and Khalfan, Byrd, and
Schnabel [143], using both line search and trust-region approaches, indicate that the SR1
method appears to be competitive with the BFGS method. The proof of Theorem 8.7 is given
in Byrd, Khalfan, and Schnabel [35].
A study of the convergence of BFGS matrices for nonlinear problems can be found in
Ge and Powell [100] and Boggs and Tolle [22]; however, the results are not as satisfactory as
for SR1 updating.
The global convergence of the BFGS method was established by Powell [201]. This
result was extended to the restricted Broyden class, except for DFP, by Byrd, Nocedal, and
Yuan [38]. For a discussion of the self-correcting properties of quasi-Newton methods see
Nocedal[184].Mostoftheearlyanalysisofquasi-Newtonmethodswasbasedonthebounded
deterioration principle. This is a tool for the local analysis that quantiﬁes the worst-case
behavior of quasi-Newton updating. Assuming that the starting point is sufﬁciently close to
the solution x∗and that the initial Hessian approximation is sufﬁciently close to ∇2f (x∗),
one can use the bounded deterioration bounds to prove that the iteration cannot stray away

220
C h a p t e r
8 .
Q u a s i - N e w t o n M e t h o d s
from the solution. This property can then be used to show that the quality of the quasi-
Newton approximations is good enough to yield superlinear convergence. For details, see
Dennis and Mor´e [68] or Dennis and Schnabel [69].
✐
E x e r c i s e s
✐
8.1
(a) Show that if f is strongly convex, then (8.7) holds for any vectors xk and xk+1.
(b) Give an example of a function of one variable satisfying g(0)  −1 and g(1)  −1
4
and show that (8.7) does not hold in this case.
✐
8.2 Show that the second strong Wolfe condition (3.7b) implies the curvature
condition (8.7).
✐
8.3 Verify that (8.19) and (8.16) are inverses of each other.
✐
8.4 Use the Sherman–Morrison formula (A.55) to show that (8.24) is the inverse of
(8.25).
✐
8.5 Prove the statements (ii) and (iii) given in the paragraph following (8.25).
✐
8.6 The square root of a matrix A is a matrix A1/2 such that A1/2A1/2  A. Show that
any symmetric positive deﬁnite matrix A has a square root, and that this square root is itself
symmetric and positive deﬁnite. (Hint: Use the factorization A  UDU T (A.46), where U
is orthogonal and D is diagonal with positive diagonal elements.)
✐
8.7 Deﬁne h(t)  1 −t + ln t, and note that h′(t)  −1 + 1/t, h′′(t)  −1/t2 < 0,
h(1)  0, and h′(1)  0. Show that h(t) ≤0 for all t > 0.
✐
8.8 Denote the eigenvalues of the positive deﬁnite matrix B by λ1, λ2, . . . , λn, where
0 < λ1 ≤λ2 ≤· · · ≤λn. Show that the ψ function deﬁned in (8.49) can be written as
ψ(B) 
n

i1
(λi −ln λi).
Use this form to show that ψ(B) > 0.
✐
8.9 The object of this exercise is to prove (8.43).
(a) First show that det(I + xyT )  1 + yT x, where x and y are n-vectors. Hint: Assuming
that x ̸ 0, we can ﬁnd vectors w1, w2, . . . , wn−1 such that the matrix Q deﬁned by
Q  [x, w1, . . . , wn−1]

8 . 4 .
C o n v e r g e n c e A n a l y s i s
221
is nonsingular and x  Qe1, where e1  (1, 0, . . . , 0)T . If we deﬁne
yT Q  (z1, . . . , zn),
then
z1  yT Qe1  yT Q(Q−1x)  yT x,
and
det(I + xyT )  det(Q−1(I + xyT )Q)  det(I + e1yT Q).
(b) Use a similar technique to prove that
det(I + xyT + uvT )  (1 + yT x)(1 + vT u) −(xT v)(yT u).
(c) Now use this relation to establish (8.45).
✐
8.10
Use the properties of the trace of a symmetric matrix and the formula (8.19) to prove
(8.44).
✐
8.11 Show that if f satisﬁes Assumption 8.1 and if the sequence of gradients satisﬁes
lim inf ∥∇fk∥ 0, then the whole sequence of iterates x converges to the solution x∗.

Chapter9

Large-Scale
Quasi-Newton and
Partially Separable
Optimization
The quasi-Newton methods of Chapter 8 are not directly applicable to large optimization
problems because their approximations to the Hessian or its inverse are usually dense. The
storage and computational requirements grow in proportion to n2, and become excessive
for large n. We can, however, modify and extend quasi-Newton methods in several ways to
make them suitable for large problems.
The ﬁrst such approach—limited-memory quasi-Newton methods—modiﬁes the
techniques described in Chapter 8 to obtain Hessian approximations that can be stored
compactly in just a few vectors of length n, where n is the number of unknowns in the prob-
lem. These methods are fairly robust, inexpensive, and easy to implement, but they do not
converge rapidly. A second approach is to deﬁne quasi-Newton approximations that pre-
servesparsity,forexamplebymimickingthesparsitypatternofthetrueHessian.Wemention
but do not dwell on these sparse quasi-Newton methods, since they have not proved to be
particularly effective.
A third approach is based on the observation that many (perhaps most) large-scale
objective functions possess a structural property known as partial separability. Effective
Newton and quasi-Newton methods that exploit this property have been developed. Such

224
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
methods usually converge rapidly and are robust, but they require detailed information
about the objective function, which can be difﬁcult to obtain in some applications.
9.1
LIMITED-MEMORY BFGS
Limited-memory quasi-Newton methods are useful for solving large problems whose Hes-
sian matrices cannot be computed at a reasonable cost or are too dense to be manipulated
easily. These methods maintain simple and compact approximations of Hessian matrices:
Instead of storing fully dense n × n approximations, they save just a few vectors of length
n that represent the approximations implicitly. Despite these modest storage requirements,
they often yield an acceptable (albeit linear) rate of convergence. Various limited-memory
methods have been proposed; we will focus mainly on an algorithm known as L-BFGS,
which as its name suggests, is based on the BFGS updating formula. The main idea of this
method is to use curvature information from only the most recent iterations to construct the
Hessian approximation. Curvature information from earlier iterations, which is less likely
to be relevant to the actual behavior of the Hessian at the current iteration, is discarded in
the interests of saving storage.
We begin our description of the L-BFGS method by recalling its parent, the BFGS
method, which was described in Algorithm 8.1. Each step of the BFGS method has the form
xk+1  xk −αkHk∇fk,
k  0, 1, 2, . . . ,
(9.1)
where αk is the step length, and Hk is updated at every iteration by means of the formula
Hk+1  V T
k HkVk + ρksksT
k
(9.2)
(see (8.16)), where
ρk 
1
yT
k sk
,
Vk  I −ρkyksT
k ,
(9.3)
and
sk  xk+1 −xk,
yk  ∇fk+1 −∇fk.
(9.4)
We say that the matrix Hk+1 is obtained by updating Hk using the pair {sk, yk}.
The inverse Hessian approximation Hk will generally be dense, so that the cost of stor-
ing and manipulating it is prohibitive when the number of variables is large. To circumvent
this problem, we store a modiﬁed version of Hk implicitly, by storing a certain number (say m)
of the vector pairs {si, yi} that are used in the formulae (9.2)–(9.4). The product Hk∇fk can
be obtained by performing a sequence of inner products and vector summations involving

9 . 1 .
L i m i t e d - M e m o r y B F G S
225
∇fk and the pairs {si, yi}. After the new iterate is computed, the oldest vector pair in the set
of pairs {si, yi} is deleted and replaced by the new pair {sk, yk} obtained from the current
step (9.4). In this way, the set of vector pairs includes curvature information from the m most
recent iterations. Practical experience has shown that modest values of m (between 3 and 20,
say) often produce satisfactory results. Apart from the modiﬁed matrix updating strategy
and a modiﬁed technique for handling the initial Hessian approximation (described below),
the implementation of L-BFGS is identical to that of the standard BFGS method given in
Algorithm 8.1. In particular, the same line search strategy can be used.
We now describe the updating process in a little more detail. At iteration k, the current
iterate is xk and the set of vector pairs contains {si, yi} for i  k −m, . . . , k −1. We ﬁrst
choose some initial Hessian approximation H 0
k (in contrast to the standard BFGS iteration,
this initial approximation is allowed to vary from iteration to iteration), and ﬁnd by repeated
application of the formula (9.2) that the L-BFGS approximation Hk satisﬁes the following
formula:
Hk 
	
V T
k−1 · · · V T
k−m

H 0
k (Vk−m · · · Vk−1)
+ ρk−m
	
V T
k−1 · · · V T
k−m+1

sk−msT
k−m (Vk−m+1 · · · Vk−1)
+ ρk−m+1
	
V T
k−1 · · · V T
k−m+2

sk−m+1sT
k−m+1 (Vk−m+2 · · · Vk−1)
+ · · ·
+ ρk−1sk−1sT
k−1.
(9.5)
From this expression we can derive a recursive procedure to compute the product Hk∇fk
efﬁciently.
Algorithm 9.1 (L-BFGS two-loop recursion).
q ←∇fk;
for i  k −1, k −2, . . . , k −m
αi ←ρisT
i q;
q ←q −αiyi;
end (for)
r ←H 0
k q;
for i  k −m, k −m + 1, . . . , k −1
β ←ρiyT
i r;
r ←r + si(αi −β)
end (for)
stop with result Hk∇fk  r.
Without considering the multiplication H 0
k q, the two-loop recursion scheme requires
4mn multiplications; if H 0
k is diagonal, then n additional multiplications are needed. Apart
from being inexpensive, this recursion has the advantage that the multiplication by the initial
matrix H 0
k is isolated from the rest of the computations, allowing this matrix to be chosen

226
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
freely and to vary between iterations. We may even use an implicit choice of H 0
k by deﬁning
some initial approximation B0
k to the Hessian (not its inverse), and obtaining r by solving
the system B0
kr  q.
AmethodforchoosingH 0
k thathasprovedtobeeffectiveinpracticeistoset H 0
k  γkI,
where
γk  sT
k−1yk−1
yT
k−1yk−1
.
(9.6)
As discussed in Chapter 8, γk is the scaling factor that attempts to estimate the size of the
true Hessian matrix along the most recent search direction (see (8.21)). This choice helps
to ensure that the search direction pk is well-scaled, and as a result the step length αk  1 is
accepted in most iterations. As discussed in Chapter 8, it is important that the line search be
based on the Wolfe conditions (3.6) or strong Wolfe conditions (3.7), so that BFGS updating
is stable.
The limited-memory BFGS algorithm can be stated formally as follows.
Algorithm 9.2 (L-BFGS).
Choose starting point x0, integer m > 0;
k ←0;
repeat
Choose H 0
k (for example, by using (9.6));
Compute pk ←−Hk∇fk from Algorithm 9.1;
Compute xk+1 ←xk + αkpk, where αk is chosen to
satisfy the Wolfe conditions;
if k > m
Discard the vector pair {sk−m, yk−m} from storage;
Compute and save sk ←xk+1 −xk, yk  ∇fk+1 −∇fk;
k ←k + 1;
until convergence.
During its ﬁrst m −1 iterations, Algorithm 9.2 is equivalent to the BFGS algorithm of
Chapter 8 if the initial matrix H0 is the same in both methods, and if L-BFGS chooses
H 0
k  H0 at each iteration. In fact, we could reimplement the standard BFGS method by
setting m to some large value in Algorithm 9.2 (larger than the number of iterations required
to ﬁnd the solution). However, as m approaches n (speciﬁcally, m > n/2), this approach
would be more costly in terms of computer time and storage than the approach of Algorithm
BFGS.
The strategy of keeping the m most recent correction pairs {si, yi} works well in
practice, and no other strategy has yet proved to be consistently better. Other criteria may
be considered, however—for example, one in which we maintain the set of correction pairs
for which the matrix formed by the si components has the best conditioning, thus tending
to avoid sets of vector pairs in which some of the s′
is are nearly collinear.

9 . 1 .
L i m i t e d - M e m o r y B F G S
227
Table 9.1
Performance of Algorithm 9.2.
L-BFGS
L-BFGS
L-BFGS
L-BFGS
Problem
n
m  3
m  5
m  17
m  29
nfg
time
nfg
time
nfg
time
nfg
time
DIXMAANL
1500
146
16.5
134
17.4
120
28.2
125
44.4
EIGENALS
110
821
21.5
569
15.7
363
16.2
168
12.5
FREUROTH
1000
>999
—
>999
—
69
8.1
38
6.3
TRIDIA
1000
876
46.6
611
41.4
531
84.6
462
127.1
Table 9.1 presents a few results illustrating the behavior of Algorithm 9.2 for various
levels of memory m. It gives the number of function and gradient evaluations (nfg) and the
total CPU time. The test problems are taken from the CUTE collection [25], the number of
variables is indicated by n, and the termination criterion ∥∇fk∥≤10−5 is used. The table
shows that the algorithm tends to be less robust when m is small. As the amount of storage
increases, the number of function evaluations tends to decrease, but since the cost of each
iteration increases with the amount of storage, the best CPU time is often obtained for small
values of m. Clearly, the optimal choice of m is problem-dependent.
Algorithm 9.2 is often the approach of choice for large problems in which the true
Hessian is not sparse, because some rival algorithms become inefﬁcient. In particular, a
Newton method, in which the true Hessian is computed and factorized, is not practical
in such circumstances. The L-BFGS approach may even outperform Hessian-free Newton
methods such as Algorithm Newton–CG discussed in Chapter 6, in which Hessian–vector
products are calculated by ﬁnite differences or automatic differentiation. Computational
experience to date also indicates that L-BFGS is more rapid and robust than nonlinear
conjugate gradient methods.
When the Hessian is dense but the objective function has partially separable structure
(see Section 9.4), the methods that exploit this structure normally outperform L-BFGS by a
wide margin in terms of function evaluations. In terms of computing time, however, L-BFGS
can be more efﬁcient due to the low cost of its iteration.
The main weaknesses of the L-BFGS method are that it often converges slowly, which
usually leads to a relatively large number of function evaluations, and that it is inefﬁcient
on highly ill-conditioned problems—speciﬁcally, on problems where the Hessian matrix
contains a wide distribution of eigenvalues.
RELATIONSHIP WITH CONJUGATE GRADIENT METHODS
Limited-memory methods evolved gradually as an attempt to improve nonlinear con-
jugate gradient methods, and early implementations resembled conjugate gradient methods
more than quasi-Newton methods. The relationship between the two classes is the basis of
a memoryless BFGS iteration, which we now outline.

228
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
We start by considering the Hestenes–Stiefel form of the nonlinear conjugate gradient
method (5.45). Recalling that sk  αkpk, we have that the search direction for this method
is given by
pk+1  −∇fk+1 + ∇f T
k+1yk
yT
k pk
pk  −

I −skyT
k
yT
k sk

∇fk+1 ≡−ˆHk+1∇fk+1.
(9.7)
This formula resembles a quasi-Newton iteration, but the matrix ˆHk+1 is neither symmetric
nor positive deﬁnite. We could symmetrize it as ˆH T
k+1 ˆHk+1, but this matrix does not satisfy
the secant equation ˆHk+1yk  sk, and is, in any case, singular. An iteration matrix that is
symmetric, positive deﬁnite, and satisﬁes the secant equation is given by
Hk+1 

I −skyT
k
yT
k sk
 
I −yksT
k
yT
k sk

+ sksT
k
yT
k sk
.
(9.8)
Interestingly enough, this matrix is exactly the one obtained by applying a single BFGS
update (9.2) to the identity matrix. By using the notation of (9.3) and (9.4), we can rewrite
(9.8) as
Hk+1  V T
k Vk + ρksksT
k .
Hence, an algorithm whose search direction is given by pk+1  −Hk+1∇fk+1, with Hk+1
deﬁned by (9.8), can be thought of as a “memoryless” BFGS method in which the previous
Hessian approximation is always reset to the identity matrix before updating, and where only
the most recent correction pair (sk, yk) is kept at every iteration. Alternatively, we can view
the method as a variant of Algorithm 9.2 in which m  1 and H 0
k  I at each iteration. In
this sense, L-BFGS is a natural extension of the memoryless method that uses extra memory
to store additional curvature information.
This discussion suggests that for any quasi-Newton method (in particular, for every
formulaintheBroydenclass)thereisamemorylesscounterpartinwhichthecurrentHessian
approximation is always reset to the identity in the update formula. The form and storage
requirements of these methods are similar to those of the nonlinear conjugate gradient
methods discussed in Chapter 5. One connection can be seen if we consider the memoryless
BFGS formula (9.8) in conjunction with an exact line search, for which ∇f T
k+1pk  0 for all
k. We then obtain
pk+1  −Hk+1∇fk+1  −∇fk+1 + ∇f T
k+1yk
yT
k pk
pk,
(9.9)
which is none other than the Hestenes–Stiefel conjugate gradient method. Moreover, it is
easy to verify that when ∇f T
k+1pk  0, the Hestenes–Stiefel formula reduces to the Polak–
Ribi`ere formula (5.43). Even though the assumption of exact line searches is unrealistic, it is

9 . 2 .
G e n e r a l L i m i t e d - M e m o r y U p d a t i n g
229
intriguing that the BFGS formula, which is considered to be the most effective quasi-Newton
update, is related in this way to the Polak–Ribi´ere and Hestenes–Stiefel methods, which are
the most efﬁcient nonlinear conjugate gradient methods.
We summarize the storage requirements of various conjugate gradient, memoryless
quasi-Newton, and limited-memory methods in the following table.
Method
Storage
Fletcher–Reeves
3n
Polak–Ribi`ere
4n
Harwell VA14
6n
CONMIN
7n
L-BFGS
2mn + 4n
The program CONMIN [228] is an extension of the memoryless BFGS method described
above. It outperforms nonlinear conjugate gradient methods because of a few important
enhancements, such as automatic restarts along a carefully chosen direction. Restarts are
also used in the Harwell routine VA14, which implements an extension of the Polak–
Ribi`ere method. The efﬁciency of these methods, in terms of function evaluations, and
their robustness tends to increase with storage as we proceed down the table.
9.2
GENERAL LIMITED-MEMORY UPDATING
Limited-memory quasi-Newton approximations are useful in a variety of optimization
methods. L-BFGS, Algorithm 9.2, is a line search method for unconstrained optimization
that (implicitly) updates an approximation Hk to the inverse of the Hessian matrix. Trust-
region methods, on the other hand, require an approximation Bk to the Hessian matrix,
not to its inverse. We would also like to develop limited-memory methods based on the SR1
formula, which is an attractive alternative to BFGS; see Chapter 8. In this section we consider
limited-memory updating in a general setting and show that by representing quasi-Newton
matrices in a compact (or outer product) form, we can derive efﬁcient implementations of
all popular quasi-Newton update formulae, and their inverses. These compact representa-
tions will also be useful in designing limited-memory methods for constrained optimization,
where approximations to the Hessian or reduced Hessian of the Lagrangian are needed; see
Chapter 18.
We will only consider limited-memory methods (like L-BFGS) that continuously re-
fresh the correction pairs by removing and adding information at each stage. A different
approach proceeds by saving correction pairs until the available storage is exhausted, and
then discarding all correction pairs (except perhaps one) and starting the process anew.
Computational experience suggests that this second approach is less effective in practice.
Throughout this chapter we let Bk denote an approximation to a Hessian matrix and
Hk the approximation to the inverse. In particular, we always have that B−1
k
 Hk.

230
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
COMPACT REPRESENTATION OF BFGS UPDATING
We now describe an approach to limited-memory updating that is based on repre-
senting quasi-Newton matrices in outer-product form. We illustrate it for the case of a BFGS
approximation Bk to the Hessian.
Theorem 9.1.
Let B0 be symmetric and positive deﬁnite and assume that the k vector pairs {si, yi}k−1
i0
satisfy sT
i yi > 0. Let Bk be obtained by applying k BFGS updates with these vector pairs to B0,
using the formula (8.19). We then have that
Bk  B0 −

B0Sk
Yk


ST
k B0Sk
Lk
LT
k
−Dk
−1 
ST
k B0
Y T
k

,
(9.10)
where Sk and Yk are the n × k matrices deﬁned by
Sk  [s0, . . . , sk−1] ,
Yk  [y0, . . . , yk−1] ,
(9.11)
while Lk and Dk are the k × k matrices
(Lk)i,j 

sT
i−1yj−1
if i > j,
0
otherwise,
(9.12)
Dk  diag

sT
0 y0, . . . , sT
k−1yk−1

.
(9.13)
This result can be proved by induction. We should note that the conditions sT
i yi > 0 i 
0, . . . , k −1 ensure that the middle matrix in (9.10) is nonsingular, so that this expression is
well-deﬁned. The utility of this representation becomes apparent when we consider limited-
memory updating.
AsintheL-BFGSalgorithm,wewillkeepthemmostrecentcorrectionpairs{si, yi}and
refresh this set at every iteration by removing the oldest pair and adding a newly generated
pair. During the ﬁrst m iterations, the update procedure described in Theorem 9.1 can be
used without modiﬁcation, except that usually we make the speciﬁc choice B0
k  δkI for the
basic matrix, where δk  1/γk and γk is deﬁned by (9.6).
At subsequent iterations k > m, the update procedure needs to be modiﬁed slightly
to reﬂect the changing nature of the set of vector pairs {si, yi} for i  k −m, . . . , k −1.
Deﬁning the n × m matrices Sk and Yk by
Sk  [sk−m, . . . , sk−1] ,
Yk  [yk−m, . . . , yk−1] ,
(9.14)

9 . 2 .
G e n e r a l L i m i t e d - M e m o r y U p d a t i n g
231
we ﬁnd that the matrix Bk resulting from m updates to the basic matrix B(k)
0
 δkI is given
by
Bk  δkI −

δkSk
Yk


δkST
k Sk
Lk
LT
k
−Dk
−1 
δkST
k
Y T
k

,
(9.15)
where Lk and Dk are now the m × m matrices deﬁned by
(Lk)i,j 

(sk−m−1+i)T (yk−m−1+j)
if i > j,
0
otherwise,
Dk  diag

sT
k−myk−m, . . . , sT
k−1yk−1

.
After the new iterate xk+1 is generated, we obtain Sk+1 by deleting sk−m from Sk and adding
the new displacement sk, and we update Yk+1 in a similar fashion. The new matrices Lk+1
and Dk+1 are obtained in an analogous way.
Since the middle matrix in (9.15) is small—of dimension 2m—its factorization re-
quires a negligible amount of computation. The key idea behind the compact representation
(9.15) is that the corrections to the basic matrix can be expressed as an outer product of two
long and narrow matrices—[δkSk Yk] and its transpose—with an intervening multiplication
by a small 2m × 2m matrix. See Figure 9.1 for a graphical illustration.
The limited-memory updating procedure of Bk requires approximately 2mn+O(m3)
operations, and matrix–vector products of the form Bkv can be performed at a cost of
(4m + 1)n + O(m2) multiplications. These operation counts indicate that updating and
manipulating the direct limited-memory BFGS matrix Bk is quite economical when m is
small.
This approximation Bk can be used in a trust-region method for unconstrained opti-
mization or, more signiﬁcantly, in methods for bound-constrained and general-constrained
optimization. The program L-BFGS-B [259] makes extensive use of compact limited-
memory approximations to solve large nonlinear optimization problems with bound
constraints. In this situation, projections of Bk into subspaces deﬁned by the constraint gra-
Bk= k I+
δ
Figure 9.1
Compact (or outer
product) representation of
Bk in (9.15).

232
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
dients must be calculated repeatedly. Many algorithms for general-constrained optimization
(for instance the SQP methods of Chapter 18) can make use of the limited-memory matrix
Bk just described to approximate the Hessian (or reduced Hessian) of the Lagrangian.
We can derive a formula, similar to (9.10), that provides a compact representation of
the inverse BFGS approximation Hk; see [37] for details. An implementation of the L-BFGS
algorithm based on this expression requires essentially the same amount of computation as
the algorithm described in the previous section. However, when the compact form is used,
the product Hk∇fk can be performed more rapidly on advanced computer systems with
hierarchical memory, since it can be implemented with BLAS-2 matrix–vector operations.
SR1 MATRICES
Compact representations can also be derived for matrices generated by the symmetric
rank-one (SR1) formula. We recall that the SR1 update (8.24) is well-deﬁned only if the
denominator (yk −Bksk)T sk is nonzero. We can assume that this condition always holds,
since if not, we can simply skip the update. The following result uses this assumption to
derive the form of the approximation Bk.
Theorem 9.2.
Suppose that k updates are applied to the symmetric matrix B0 using the vector pairs
{si, yi}k−1
i0 and the SR1 formula (8.24), and assume that each update is well-deﬁned in the sense
that (yi −Bisi)T si ̸ 0, i  0, . . . , k −1. The resulting matrix Bk can be expressed as
Bk  B0 + (Yk −B0Sk)(Dk + Lk + LT
k −ST
k B0Sk)−1(Yk −B0Sk)T ,
(9.16)
where Sk, Yk, Dk, and Lk are as deﬁned in (9.11), (9.12), and (9.13).
The conditions of the theorem imply that the matrix (Dk + Lk + LT
k −ST
k B0Sk)
is nonsingular. Since the SR1 method is self-dual, the inverse formula Hk can be obtained
simply by replacing B, s, and y by H, y, and s, respectively. Limited-memory SR1 methods
can be derived in the same way as for the BFGS method. We replace B0 with the basic matrix
B0
k at the kth iteration, and we redeﬁne Sk and Yk to contain the m most recent corrections,
as in (9.14).
UNROLLING THE UPDATE
The reader may wonder whether limited-memory updating can be implemented
in simpler ways. In fact, as we show here, the most obvious implementation of limited-
memoryBFGSupdatingisconsiderablymoreexpensivethantheapproachbasedoncompact
representations discussed in the previous section.
The direct BFGS formula (8.19) can be written as
Bk+1  Bk −akaT
k + bkbT
k ,
(9.17)

9 . 3 .
S p a r s e Q u a s i - N e w t o n U p d a t e s
233
where the vectors ak and bk are deﬁned by
ak 
Bksk
(sT
k BksT
k )
1
2 ,
bk 
yk
(yT
k sk)
1
2 .
(9.18)
We could continue to save the vector pairs {si, yi} but use the formula (9.17) to compute
matrix–vector products. A limited-memory BFGS method that uses this approach would
proceed by deﬁning the basic matrix B0
k at each iteration and then updating according to
the formula
Bk  B0
k +
k−1

ik−m

bibT
i −aiaT
i

.
(9.19)
The vector pairs {ai, bi}, i  k −m, . . . , k −1, would then be recovered from the stored
vector pairs {si, yi}, i  k −m, . . . , k −1, by the following procedure:
Procedure 9.3 (Unrolling the BFGS formula).
for i  k −m, . . . , k −1
bi ←yi/(yT
i si)1/2;
ai ←B0
ksi + i−1
jk−m
%
(bT
j si)bj −(aT
j si)aj
&
;
ai ←ai/(sT
i ai)1/2;
end (for)
Note that the vectors ai must be recomputed at each iteration, since they all depend on the
vector pair {sk−m, yk−m}, which is removed at the end of iteration k. On the other hand, the
vectors bi and the inner products bT
j si can be saved from the previous iteration, so only the
new values bk−1 and bT
j sk−1 need to be computed at the current iteration.
By taking all these computations into account, and assuming that B0
k  I, we ﬁnd
that approximately 3
2m2n operations are needed to determine the limited-memory matrix.
The actual computation of the inner product Bmv (for arbitrary v ∈IRn) requires 4mn
multiplications. Overall, therefore, this approach is less efﬁcient than the one based on the
compact matrix representation described above. Indeed, while the product Bkv costs the
same in both cases, updating the representation of the limited-memory matrix using the
compact form requires only 2mn multiplications, compared to 3
2m2n multiplications needed
when the BFGS formula is unrolled.
9.3
SPARSE QUASI-NEWTON UPDATES
We now discuss a quasi-Newton approach to large-scale problems that has intuitive appeal:
We demand that the quasi-Newton approximations Bk have the same (or similar) sparsity

234
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
pattern as the true Hessian. This would reduce the storage requirements of the algorithm
and perhaps give rise to more accurate Hessian approximations.
Suppose that we know which components of the Hessian may be nonzero at some
point in the domain of interest. That is, we know the contents of the set  deﬁned by

def {(i, j) | [∇2f (x)]ij ̸ 0 for some x in the domain of f }.
Suppose also that the current Hessian approximation Bk mirrors the nonzero structure of
the exact Hessian, i.e., (Bk)ij  0 for (i, j) /∈. In updating Bk to Bk+1, then, we could try
to ﬁnd the matrix Bk+1 that satisﬁes the secant condition, has the same sparsity pattern, and
is as close as possible to Bk. Speciﬁcally, we deﬁne Bk+1 to be the solution of the following
quadratic program:
min
B
∥B −Bk∥2
F 

(i,j)∈
[Bij −(Bk)ij]2,
(9.20a)
subject to Bsk  yk, B  BT , and Bij  0 for (i, j) /∈.
(9.20b)
It can be shown that the solution Bk+1 of this problem can be obtained by solving an n × n
linear system whose sparsity pattern is , the same as the sparsity of the true Hessian. Once
Bk+1 has been computed we can use it, within a trust region method, to obtain the new
iterate xk+1. We note that Bk+1 is not guaranteed to be positive deﬁnite.
We omit further details of this approach because it has several major drawbacks:
The updating process does not possess scale invariance under linear transformations of
the variables, and, more signiﬁcantly, its practical performance has been disappointing. A
standard implementation of this sparse quasi-Newton method typically requires at least as
many function evaluations as L-BFGS, but its cost per iteration is higher. The fundamental
weakness of this approach is that (9.20a) is an inadequate model and can produce poor
Hessian approximations.
An alternative approach is to relax the secant equation, making sure that it is approx-
imately satisﬁed along the last few steps rather than requiring it to hold strictly on the latest
step. To do so, we deﬁne Sk and Yk by (9.14) so that they contain the m most recent difference
pairs. We can then deﬁne the new Hessian approximation Bk+1 to be the solution of
min
B
∥BSk −Yk∥2
F
subject to B  BT and Bij  0 for (i, j) /∈.
Thisconvexoptimizationproblemhasasolution,butitisnoteasytocompute.Moreover,this
approach can produce singular or poorly conditioned Hessian approximations. Even though
it frequently outperforms methods based on (9.20a), its performance on large problems has
not been impressive.

9 . 4 .
P a r t i a l l y S e p a r a b l e F u n c t i o n s
235
A much more promising approach to exploiting structure in large-scale optimization
is outlined in the next section.
9.4
PARTIALLY SEPARABLE FUNCTIONS
In a separable unconstrained optimization problem, the objective function can be decom-
posed into a sum of simpler functions that can be optimized independently. For example, if
we have
f (x)  f1(x1, x3) + f2(x2, x4, x6) + f3(x5),
we can ﬁnd the optimal value of x by minimizing each function fi, i  1, 2, 3, inde-
pendently, since no variable appears in more than one function. The cost of performing
n lower-dimensional optimizations is much less in general than the cost of optimizing an
n-dimensional function.
In many large problems the objective function f : IRn →IR is not separable, but it can
still be written as the sum of simpler functions, known as element functions. Each element
function has the property that it is unaffected when we move along a large number of linearly
independent directions. If this property holds, we say that f is partially separable. We will
see that all functions whose Hessians ∇2f are sparse are partially separable, but so are many
functions whose Hessian is not sparse. Partial separability allows for economical problem
representation, efﬁcient automatic differentiation, and effective quasi-Newton updating.
The simplest form of partial separability arises when the objective function can be
written as
f (x) 
ne

i1
fi(x),
(9.21)
whereeachoftheelementfunctions fi dependsononlyafewcomponentsofx.Itfollowsthat
the gradients ∇fi and Hessians ∇2fi of each element function contain just a few nonzeros.
By differentiating (9.21), we obtain
∇f (x) 
ne

i1
∇fi(x),
∇2f (x) 
ne

i1
∇2fi(x),
and it is natural to ask whether it is more effective to maintain quasi-Newton approximations
to each of the element Hessians ∇2fi(x) separately, rather than approximating the entire
Hessian∇2f (x).Wewillshowthattheanswerisafﬁrmative,providedthatthequasi-Newton
approximation fully exploits the structure of each element Hessian.

236
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
A SIMPLE EXAMPLE
Consider the objective function
f (x)  (x1 −x2
3)2 + (x2 −x2
4)2 + (x3 −x2
2)2 + (x4 −x2
1)2
(9.22)
≡f1(x) + f2(x) + f3(x) + f4(x),
where the element functions fi have been deﬁned in the obvious way. The Hessians of these
element functions are 4 × 4 sparse, singular matrices with 4 nonzero entries.
Letusfocuson f1;allotherelementfunctionshaveexactlythesameform.Eventhough
f1 is formally a function of all components of x, it depends only on x1 and x3, that we call
the element variables for f1. We assemble the element variables into a vector that we call x[1],
that is,
x[1] 

x1
x3

,
and note that
x[1]  U1x
with
U1 

1
0
0
0
0
0
1
0

.
If we deﬁne the function φ1 by
φ1(z1, z2)  (z1 −z2
2)2,
then we can write f1(x)  φ1(U1x). By applying the chain rule to this representation, we
obtain
∇f1(x)  U T
1 ∇φ1(U1x),
∇2f1(x)  U T
1 ∇2φ1(U1x)U1.
(9.23)
In our case, we have
∇2φ(U1x) 

2
−4x3
−4x3
12x2
3

,
∇2f1(x) 


2
0
−4x3
0
0
0
0
0
−4x3
0
12x2
3
0
0
0
0
0


.
It is easy to see that U1 performs a “compactiﬁcation” of the variable vector; it allows us
to map the derivative information for the low-dimensional function φ1 into the derivative
information for the element function f1.

9 . 4 .
P a r t i a l l y S e p a r a b l e F u n c t i o n s
237
Now comes the key idea: Instead of maintaining a quasi-Newton approximation to
∇2f1, we maintain a 2 × 2 quasi-Newton approximation B[1] of ∇2φ1 and use the relation
(9.23) to transform it into a quasi-Newton approximation to ∇2f1. To update B[1] after a
typical step from x to x+, we record the information
s[1]  x+
[1] −x[1],
y[1]  ∇φ1(x+
[1]) −∇φ1(x[1]),
(9.24)
and use BFGS or SR1 updating to obtain the new approximation B+
[1]. We therefore update
small, dense quasi-Newton approximations with the property
B[1] ≈∇2φ1(U1x)  ∇2φ1(x[1]).
(9.25)
To obtain an approximation of the element Hessian ∇2f1, we use the transformation
suggested by the relationship (9.23); that is,
∇2f1(x) ≈U T
1 B[1]U1.
This operation has the effect of mapping the elements of B[1] to the correct positions in the
full n × n Hessian approximation.
The previous discussion concerned only the ﬁrst element function f1. We do the same
for all other element functions, maintaining a quasi-Newton approximation B[i] for each
one of them. To obtain a complete approximation to the full Hessian ∇2f , we simply sum
the element Hessian approximations as follows:
B 
ne

i1
U T
i B[i]Ui.
We argue later that this strategy will produce a very good quasi-Newton approximation,
much better than one that would be obtained if we ignored the partially separable structure
of the objective function f .
Before discussing these quasi-Newton methods further, we examine the concept of
partialseparabilityindetail.Weﬁrstdescribehowtodeterminewhetherafunctionispartially
separable and how to obtain its best representation. Next, we explain the concept of an
invariant subspace, and we explore the relationship between sparsity and partial separability.
INTERNAL VARIABLES
In the simple example (9.22), the best strategy for decomposing the objective function
(into the four element functions that we chose) was obvious. In many applications, however,
there are many alternative ways to choose the partially separable decomposition, and it is
importanttotheefﬁciencyofquasi-Newtonmethodsthatweselecttheﬁnestdecomposition.

238
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
We illustrate this issue with the following example, which arises in the solution of the
minimum surface area problem (see Exercise 13).
The objective function is given by (9.21), where each of the ne element functions fi
has the form
fi(x)  1
q2

1 + q2
2 [(xj −xj+q+1)2 + (xj+1 −xj+q)2]
 1
2
.
(9.26)
Here q is a parameter that determines the size of the discretization, ne  n  q2, and the
integer j, which determines which components of x inﬂuence fi, is a function of i and q.
(Details of the formulation are given in Exercise 13.)
Each element function fi depends on only four components of x. The gradient of fi
(with respect to the full vector x) contains at most four nonzeros and has the form
∇fi(x) 
1
2q2 f −1
i
(x)


0
...
xj −xj+q+1
xj+1 −xj+q
...
−xj+1 + xj+q
−xj + xj+q+1
...
0


.
Note that two of these four nonzero components are negatives of the other two. The Hessian
of the element function fi has the sparsity pattern


∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗
∗


,
(9.27)

9 . 4 .
P a r t i a l l y S e p a r a b l e F u n c t i o n s
239
and a close examination shows that some of the nonzero entries differ only in sign. In fact,
only three different magnitudes are represented by the 16 nonzero elements, and the 4 × 4
matrix that can be assembled from these nonzero elements happens to be singular.
The obvious way to deﬁne the element variables x[i] for fi is simply to take the four
components of x that appear in (9.26). However, the fact that repeated information is
contained in the derivatives suggests that there is a more economical representation that
avoids these redundancies. The key observation is that fi is invariant in the subspace
Ni  {w ∈IRn | wj  wj+q+1
and
wj+1  wj+q},
(9.28)
which means that for any x and for any w ∈Ni we have that fi(x + w)  fi(x). In other
words, any move along a direction in Ni (a subspace of IRn whose dimension is n −2) does
not change the value of fi, so it is not useful to try to gather curvature information about
fi along Ni. Put another way, we can say that there is an orthogonal basis for IRn for which
fi changes along just two of the vectors in this basis. It therefore makes sense to look for a
compact representation of ∇2fi that is based on a 2 × 2 matrix.
We can ﬁnd such a representation almost immediately by examining the deﬁnition
(9.28). We deﬁne the internal variables uj and uj+1 by
uj  xj −xj+q+1,
uj+1  xj+1 −xj+q,
(9.29)
and the internal function φi by
φi(uj, uj+1)  1
q2

1 + q2
2 (u2
j + u2
j+1)
 1
2
.
(9.30)
In fact, this representation of fi is minimal, since it allows for just two distinct nonzero values
in ∇fi and three distinct nonzero values in the Hessian ∇2fi, which is exactly what we ob-
served earlier. Hence, we call this representation the ﬁnest partially separable representation
of f .
Formally, we can represent (9.29) in matrix form,
u[i]  Uix,
(9.31)
where the internal variable vector u[i] has two components and Ui is a 2 × n matrix that has
zero elements except for
U1,j  1,
U1,j+q+1  −1,
U2,j+1  1,
U2,j+q  −1.

240
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
Note that the null space of Ui is exactly the invariant subspace Ni. The full objective function
for the minimum surface problem can now be written as
f (x) 
ne

i1
φi(Uix).
(9.32)
The gradient and Hessian of f are given by
∇f (x) 
ne

i1
U T
i ∇φi(Uix),
∇2f (x) 
ne

i1
U T
i ∇2φi(Uix)Ui,
(9.33)
which clearly exhibit the underlying structure. All the information is now contained in
the transformations Ui and in the internal gradient and Hessians—∇φi(·) and ∇2φi(·)—
which contain just two and four nonzeros, respectively. We will see that effective algorithms
result when we assemble quasi-Newton approximations of ∇2f from quasi-Newton
approximations of the internal Hessians ∇2φi.
Returning to the example (9.22), we see that the internal variables for f1 should be
deﬁned as u[1]  x[1], and that the internal function is given by the function φ described
there.
9.5
INVARIANT SUBSPACES AND PARTIAL SEPARABILITY
We now generalize the previous examples and formally describe the key concepts that
underlie partial separability.
Deﬁnition 9.1 (Invariant Subspace).
The invariant subspace Ni of a function f (x) : IRn →IR is the largest subspace in IRn
such that for all w ∈Ni, we have f (x + w)  f (x) whenever x and x + w are in the domain
of f .
We have seen one example of an invariant subspace in (9.28). Another simple example can
be derived from the element function fi deﬁned by
fi(x1, . . . , xn)  x2
50
(n > 50),
(9.34)
for which Ni is the set of all vectors w ∈IRn for which w50  0. This subspace has dimension
n−1, so that all the nontrivial behavior of the function is contained in the one-dimensional
subspace orthogonal to Ni. In this case, the obvious compact representation of fi is the
simple function φi(z)  z2, where the compactifying matrix Ui is the 1 × n matrix deﬁned
by
Ui 

0
. . .
0
1
0
· · ·
0

,

9 . 5 .
I n v a r i a n t S u b s p a c e s a n d P a r t i a l S e p a r a b i l i t y
241
where the nonzero element occurs in the 50th position.
It is interesting to compare (9.34) with the element function fi deﬁned by
fi(x1, . . . , xn)  (x1 + . . . + xn)2.
(9.35)
At ﬁrst glance, the two functions appear to be quite different. The ﬁrst one depends on only
one variable, and as a result has a very sparse gradient and Hessian, while (9.35) depends
on all variables, and its gradient and Hessian are completely dense. However, the invariant
subspace Ni of (9.35) is
Ni  {w ∈IRn | eT w  0},
where e  (1, 1, . . . , 1)T .
(9.36)
Again, the dimension of this subspace is n−1, so that as in (9.34), all the nontrivial behavior
is conﬁned to a one-dimensional subspace. We can derive a compact representation of the
function in (9.35) by deﬁning
Ui 

1
1
· · ·
1

,
φi(z)  z2.
Note that the compact representation function φi(z) is the same for both (9.34) and (9.35);
the two functions differ only in the makeup of the matrix Ui.
We now use the concept of an invariant subspace to deﬁne partial separability.
Deﬁnition 9.2 (Partial Separability).
A function f is said to be partially separable if it is the sum of element functions, f (x) 
ne
i1 fi(x), where each fi has a large invariant subspace. In other words, f can be written in
the form (9.32), where the matrices Ui (whose null spaces coincide with the invariant subspaces
Ni) have dimension ni × n, with ni ≪n.
This deﬁnition is a little vague in that the term “large invariant subspace” is not precisely
deﬁned. However, such vagueness is also present in the deﬁnition of sparsity, for instance,
which depends not just on the proportion of nonzeros in a matrix but also on its structure,
size, and the underlying application. For practical purposes, it makes sense to exploit partial
separability only if the dimensions of all the invariant subspaces are close to n.
Note that in all our examples above, the matrices Ui whose null spaces spanned each
Ni could have been chosen in a number of different ways; a basis for a subspace is not unique,
after all. In all cases, however, a deﬁnition of Ui was fairly clear from the analytic deﬁnition of
fi. The situation may be different when the function is deﬁned by a complicated computer
program. In this case we would prefer to circumvent the task of analyzing the function in
order to identify the matrix Ui. An automatic procedure for detection of partial separability
has been proposed by Gay [98] and has been implemented in the AMPL modeling language
[92] in conjunction with its automatic differentiation software. We show in Chapter 7 that
the decomposition of partially separable functions also improves the efﬁciency of automatic
differentiation.

242
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
SPARSITY VS. PARTIAL SEPARABILITY
We have already seen in (9.35) that functions can be partially separable even if their
Hessians are not sparse. The converse is also true: Functions with sparse Hessians are always
partially separable. We prove this result in the following theorem. In this sense, the concept
of partial separability is more general than sparsity, and it yields more information from the
optimization viewpoint.
Theorem 9.3.
Every twice continuously differentiable function with a sparse Hessian is partially sepa-
rable. More speciﬁcally, suppose that f : D →IRn is twice continuously differentiable in an
open subset D of IRn, and that
∂2
∂xi∂xj
f (x1, . . . , xn)  0
for all x ∈D. Then
f (x1, . . . , xn)  f (x1, . . . , xj−1, 0, xj+1, . . . , xn)
+ f (x1, . . . , xi−1, 0, xi+1, . . . , xn)
−f (x1, . . . , xi−1, 0, xi+1, . . . , xj−1, 0, xj+1, . . . , xn)
for all (x1, . . . , xn).
Proof.
Without loss of generality, consider the case n  2 so that (x1, . . . , xn)  (x, y).
The assumption and the integral mean value theorem imply that for all (x, y),
0 
 x
0
 z
0
∂2f (ξ, ζ)
∂x∂y
dξ dζ

 x
0
∂f (ξ, ζ)
∂x
dξ
y
0

 x
0
∂f (ξ, y)
∂x
dξ −
 x
0
∂f (ξ, 0)
∂x
dξ
 [f (ξ, y)]x
0 −[f (ξ, 0)]x
0
 f (x, y) −f (0, y) −f (x, 0) + f (0, 0).
□
Theorem 9.3 states that even if only one of the partials vanishes, then f is the sum of
element functions, each of which has a nontrivial invariant subspace.

9 . 5 .
I n v a r i a n t S u b s p a c e s a n d P a r t i a l S e p a r a b i l i t y
243
GROUP PARTIAL SEPARABILITY
Partial separability is an important concept, but it is not quite as general as we would
like. Consider a nonlinear least-squares problem of the form
f (x) 
l
k1
(fk(x) + fk+1(x) + ck)2,
(9.37)
where the functions fj are partially separable and each ck is a constant. The deﬁnition of
partial separability given above forces us either to expand the quadratic function and end
up with products of the fk, or to regard the whole kth term in the summation as the kth
element function. A more intuitive approach would avoid this somewhat artiﬁcial expansion
and recognize that each term contains two element functions, grouped together and squared.
By extending slightly the concept of partial separability, we can make better use of this type
of structure.
We say that a function f : IRn →IR is group partially separable if it can be written in
the form
f (x) 
l
k1
ψk(hk(x))
(9.38)
where each hk is a partially separable function from IRn to IR, and each ψk is a scalar twice
continuously differentiable function deﬁned on the range of hk. We refer to ψk as the group
function. Group partial separability is a useful concept in many important areas of optimiza-
tion, such as nonlinear least-squares problems and penalty and merit functions arising in
constrained optimization.
Compact representations of the derivatives of f can be built up from the representa-
tions of the hk functions (and their derivatives), together with the values of ψk, ψ′
k, and ψ′′
k ,
for each k. By using the chain rule, we obtain
∇[ψk(hk(x))]  ψ′
k(hk(x))∇hk(x),
∇2[ψk(hk(x))]  ψ′′
k (hk(x))∇hk(x)∇hk(x)T + ψ′
k(hk(x))∇2hk(x).
These formulae, together with the compact representations of the quantities ∇hk and ∇2hk
derivedin(9.33),canbeusedtorepresentthederivativesofthefunctionf of(9.38)efﬁciently.
Hence, all the ideas we have discussed so far in the context of partially separable functions
generalize easily to group partially separable objectives.

244
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
❏Example 9.1
Consider the function
1
2 ((x5 −x25)x7 + x11)2 .
Thegroupfunctionisψ(z)  1
2z2.Thepartiallyseparablefunctionh(x)  (x5−x25)x7+x11
that serves as the argument of ψ(·) can be partitioned into two element functions deﬁned
by f1  (x5 −x25)x7 and f2  x11. Note that the function f1 has three element variables,
but these can be transformed into the two internal variables u1  x5 −x25 and u2  x7.
❐
9.6
ALGORITHMS FOR PARTIALLY SEPARABLE FUNCTIONS
The concept of partial separability has most to offer when the number of variables is large.
Savings can be obtained by representing the Hessian matrix economically in a Newton
method or by developing special quasi-Newton updating techniques.
EXPLOITING PARTIAL SEPARABILITY IN NEWTON’S METHOD
We consider ﬁrst an inexact (truncated) Newton method in which the search direction
pk is an approximate solution of the Newton equations
∇2f (xk)p  −∇f (xk).
(9.40)
We solve this system by the conjugate gradient method. To obtain a complete algorithm,
we embed this step computation into either a line search or a trust-region framework, as
discussed in Chapter 6.
Suppose that we know how to express the objective function f in partially separable
form,thatis,wehaveidentiﬁedthenumberofelementfunctions ne,theinternalvariablesui,
and the transformations Ui for each element function. Recalling that the conjugate gradient
method requires matrix–vector products to be formed with the Hessian ∇2f (x), we see from
the representation (9.33) that such products can be obtained by means of matrix–vector
operations involving the matrices Ui and ∇2φi, for i  1, 2, . . . , ne. By using this structure,
we often ﬁnd that the Hessian–vector product can be computed more economically than if
we formed ∇2f (x) and performed the multiplication explicitly. The representation (9.33)
can therefore yield faster computations as well as a reduction in storage.
As an example, consider the minimum surface problem where the element functions
areoftheform(9.26).Inthisfunction,therearene  nHessians∇2φi,eachofwhichisa2×2
symmetric matrix that can be stored in three ﬂoating-point numbers. The matrices Ui all

9 . 6 .
A l g o r i t h m s f o r P a r t i a l l y S e p a r a b l e F u n c t i o n s
245
have similar structure; each contains four nonzeros, two 1’s and two −1’s, in matrix locations
that are determined by the index i. It is not necessary to store these matrices explicitly at all,
since all we need to know to perform matrix–vector products with them is the value of i.
Hence, the Hessian can be stored in approximately 3n ﬂoating-point memory locations. By
contrast, it can be shown that storage of the lower triangle of the full Hessian ∇2f (x) in the
standard format requires about 5n locations. Hence, in this example, the partially separable
representation has yielded a 40% savings in storage. In other applications, the savings can
be even more signiﬁcant.
We should note, however, that the storage reduction that we achieve by the use of inter-
nal variables comes at a price: The mapping performed by the transformations Ui requires
memory access and computation. The efﬁciency loss due to these operations depends on the
problem structure and on the computer architecture, but sometimes it can be signiﬁcant.
Consequently, it may not be worth our while to seek the ﬁnest possible partially separable
representation for a given problem, but to settle instead for an easily identiﬁed representation
that captures most of the structure, or even to ignore the partial separability altogether. For
example, in the minimum surface problem, we could represent each element function as a
function of 4 variables, and not as a function of two variables, as in (9.30).
Another implementation of Newton’s method that is suitable for partially separable
functions is based on solving (9.40) by using a direct method instead of conjugate gradients.
The multifrontal factorization technique described by Duff and Reid [74], which was de-
veloped initially in connection with ﬁnite element systems, is well suited for the Hessians of
partially separable functions. In this approach, partial assembly of the Hessian matrix is used
in conjunction with dense factorization of submatrices to obtain an efﬁcient factorization.
The LANCELOT software package [53] includes an option to allow the user to choose be-
tween conjugate gradients and the multifrontal approach in its implementation of Newton’s
method.
In conclusion, the effectiveness of Newton’s method for partially separable
functions—asmeasuredbytotalcomputingtime—variesaccordingtotheproblemstructure
and computer architecture. In some cases it may be proﬁtable to exploit partial separability
and in other cases it may not. The situation is less ambiguous in the case of quasi-Newton
methods, where exploitation of the partially separable structure often yields a dramatic
reduction in the number of function and gradient evaluations.
QUASI-NEWTON METHODS FOR PARTIALLY SEPARABLE FUNCTIONS
As indicated in (9.25), (9.24), we can construct quasi-Newton Hessian approximations
forpartiallyseparablefunctionsbystoringandupdatingapproximationsB[i] toeachinternal
function ∇2φi and assembling these into an approximation B to the full Hessian ∇2f (x)
using the formula
B 
ne

i1
U T
i B[i]Ui.
(9.41)

246
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
We may use this approximate Hessian in a trust-region algorithm, obtaining an approximate
solution pk of the system
Bkpk  −∇f (xk).
(9.42)
As in the case of Newton’s method, we need not assemble Bk explicitly, but rather use the
conjugate gradient approach to solve (9.42), computing matrix–vector products of the form
Bkv by performing operations with the matrices Ui and B[i].
As for any quasi-Newton method, we update the approximations B[i] by requiring
them to satisfy the secant equation for each element function, namely,
B[i]s[i]  y[i],
(9.43)
where
s[i]  u+
[i] −u[i]
(9.44)
is the change in the internal variables corresponding to the ith element function, and
y[i]  ∇φi(u+
[i]) −∇φ(u[i])
(9.45)
is the corresponding change in gradients. Here u+ and u indicate the vectors of internal
variables at the current and previous iterations, respectively. Thus the formulae (9.44) and
(9.45) are obvious generalizations of (9.24) to the case of internal variables.
We return again to the minimum surface problem (9.26) to illustrate the usefulness of
this element-by-element updating technique. In this case, the functions φi depend on only
two internal variables, so that each Hessian approximation B[i] is a 2 × 2 matrix. After just
a few iterations, we will have sampled enough directions s[i] to make each B[i] an accurate
approximation to ∇2φi. Hence the full quasi-Newton approximation (9.41) will tend to be
a very good approximation to ∇2f (x).
By contrast, a quasi-Newton method that ignores the partially separable structure of
the objective function will attempt to estimate the total average curvature—the sum of the
individual curvatures of the element functions—by constructing a dense n×n matrix. When
the number of variables n is large, many iterations will be required before this quasi-Newton
approximation is of good quality. Hence an algorithm of this type (e.g., standard BFGS or
L-BFGS) will require many more iterations than a method based on the partially separable
approximate Hessian.
It is not always possible to use the BFGS formula in conjunction with the formulae
(9.44) and (9.45) to update the partial Hessian B[i], because there is no guarantee that the

9 . 6 .
A l g o r i t h m s f o r P a r t i a l l y S e p a r a b l e F u n c t i o n s
247
curvature condition sT
[i]y[i] > 0 will be satisﬁed. (Recall from Chapter 8 that this curvature
condition is needed to ensure that the BFGS approximate Hessian remains positive deﬁnite.)
Thatis,eventhoughthefullHessian∇2f (x)isatleastpositivesemideﬁniteatthesolutionx∗,
someoftheindividualHessians∇2φi(·)maybeindeﬁnite.Onewaytoovercomethisobstacle
istoapplytheSR1updatetoeachoftheelementHessians,usingsimpleprecautionstoensure
thatitiswell-deﬁned.SR1updatinghasprovedtobeveryeffectiveintheLANCELOTpackage
[53], which is designed to take full advantage of partial separability. The performance of the
SR1-based quasi-Newton method is often comparable to that of Newton’s method, provided
that we ﬁnd the ﬁnest partially separable decomposition of the problem; otherwise, there
can be a substantial loss of efﬁciency. For the problems listed in Table 9.1, the Newton and
quasi-Newton versions of LANCELOT required a similar number of function evaluations
and similar computing time.
NOTES AND REFERENCES
For further discussion on the L-BFGS method see Nocedal [183], Liu and Nocedal
[151], and Gilbert and Lemar´echal [102]. The last paper also discusses various ways in
which the scaling parameter can be chosen.
Algorithm 9.1, the two-loop L-BFGS recursion, constitutes an economical procedure
for computing the product Hk∇fk. It is based, however, on the speciﬁc form of the BFGS
update formula (9.2), and recursions of this type have not yet been developed (and may not
exist) for other members of the Broyden class (for instance, the SR1 and DFP methods).
Limited-memorymethodsusingroughlyhalfofthestorageemployedbyAlgorithm9.2
are discussed by Siegel [230], Deuﬂhard et al. [71], and Gill and Leonard [106]. It is not yet
known, however, whether such approaches outperform L-BFGS. A limited-memory method
that combines cycles of quasi-Newton and CG iterations and that is often competitive with
L-BFGS is described by Buckley and Le Nir [28]. Our discussion of compact representations
of limited-memory matrices is based on Byrd, Nocedal, and Schnabel [37].
Limited-memory methods for solving systems of nonlinear equations can also be
developed; see Chapter 11. In particular, the paper [37] shows that compact representations
of Broyden matrices for these problems can be derived.
Sparse quasi-Newton updates have been studied by Toint [237, 238] and Fletcher et
al. [84, 85], among others.
The concept of partial separability was introduced by Griewank and Toint [129, 127].
For an extensive treatment of the subject see Conn, Gould, and Toint [53]. Theorem 9.3 was
proved by Griewank and Toint [128].
If f is partially separable, a general afﬁne transformation will not in general preserve
the partially separable structure. The quasi-Newton method for partially separable functions
described here is not invariant to afﬁne transformations of the variables, but this is not a
drawback because it is invariant to transformations that preserve separability.

248
C h a p t e r
9 .
L a r g e S c a l e M e t h o d s
✐
E x e r c i s e s
✐
9.1 Code Algorithm 9.2 and test it on the extended Rosenbrock function
f (x) 
n/2

i1

α(x2i −x2
2i−1)2 + (1 −x2i−1)2
,
where α is a parameter that you can vary (e.g., 1, 100). The solution is x∗

(1, 1, . . . , 1), f ∗ 0.
Choose the starting point as (−1, −1, . . . , −1)T . Observe the behavior of your
program for various values of the memory parameter m.
✐
9.2 Show that the matrix ˆHk+1 in (9.7) is singular.
✐
9.3 Derive the formula (9.9) under the assumption that line searches are exact.
✐
9.4 Describe how to take advantage of parallelism when computing the matrix–vector
product Bkv, when Bk is given by (9.10). Compare this with the level of parallelism that can
be achieved in the two-loop recursion for computing Hk∇fk.
✐
9.5 Consider limited-memory SR1 updating based on (9.16). Explain how the storage
can be cut in half if the basic matrix B0
k is kept ﬁxed for all k. (Hint: Consider the matrix
Qk  [q0, . . . , qk−1]  Yk −B0Sk.)
✐
9.6 Compute the elements in the Hessian (9.27) and verify that if differences in the
signs are ignored, only three of the elements are different.
✐
9.7 Write the function deﬁned by
f (x)  x2x3ex1+x3−x4 + (x2x3)2 + (x3 −x4)
in the form (9.32). In particular, give the deﬁnition of each of the compactifying
transformations Ui.
✐
9.8 Show that the dimension of (9.28) is n −2 and that the dimension of (9.36) is
n −1.
✐
9.9 For the following two functions, ﬁnd the invariant subspace, and deﬁne the
internal variables. Make sure that the number of internal variables is n −dim(N).
1. f (x)  (x9 + x3 −2x7) exp(x9 −x7).
2. f (x) 
100
i1 ixi
2
+
100
i1 i2xn−1
2
.
✐
9.10 Give an example of a partially separable function that is strictly convex but that
contains an element function fi that is concave.

9 . 6 .
A l g o r i t h m s f o r P a r t i a l l y S e p a r a b l e F u n c t i o n s
249
✐
9.11 Does the approximation B obtained by the partially separable quasi-Newton
updating (9.41), (9.45) satisfy the secant equation Bs  y?
✐
9.12 (Griewank and Toint [128]) Let t be a nonsingular afﬁne transformation t(x) 
Ax + b. Show that f and f ◦t have the same separability structure if the linear part of t is
block-diagonal, where the ith and jth variables belong to the same block only if the ith and
jth components of e are identical for all e ∈E (see the proof of Theorem 9.3 for a deﬁnition
of e and E). In other words, t is only allowed to mix variables that occur either together or
not at all in the element functions fi.
✐
9.13 The minimum surface problem is a classical application of the calculus of vari-
ations and can be found in many textbooks. We wish to ﬁnd the surface of minimum area,
deﬁned on the unit square, that interpolates a prescribed continuous function on the bound-
ary of the square. In the standard discretization of this problem, the unknowns are the values
of the sought-after function z(x, y) on a q × q rectangular mesh of points over the unit
square.
More speciﬁcally, we divide each edge of the square into q intervals of equal length,
yielding (q + 1)2 grid points. We label the grid points as
x(i−1)(q+1)+1, . . . , xi(q+1)
for i  1, 2, q + 1,
so that each value of i generates a line. With each point we associate a variable zi that
represents the height of the surface at this point. For the 4q grid points on the boundary
of the unit square, the values of these variables are determined by the given function. The
optimization problem is to determine the other (q + 1)2 −4q variables zi so that the total
surface area is minimized.
A typical subsquare in this partition looks as follows:
xj+q+1
xj+q+2
xj
xj+1
We denote this square by Aj and note that its area is q2. The desired function is z(x, y), and
we wish to compute its surface over Aj. It is shown in calculus books that the area of the
surface is given by
fj(x) ≡
 
(x,y)∈Aj
'
1 +
 ∂z
∂x
2
+
 ∂z
∂y
2
dx dy.
Approximate the derivatives by ﬁnite differences and show that fj has the form (9.26).

Chapter10

Nonlinear
Least-Squares
Problems
In least-squares problems, the objective function f has the following special form:
f (x)  1
2
m

j1
r2
j (x),
(10.1)
where each rj is a smooth function from IRn to IR. We refer to each rj as a residual, and we
assume throughout this chapter that m ≥n.
These problems have been a fruitful area of study for over 30 years, mainly be-
cause of their applicability to many practical problems. Almost anyone who formulates
a parametrized model for a chemical, physical, ﬁnancial, or economic application uses a
function of the form (10.1) to measure the discrepancy between the model and the output
of the system at various observation points. By minimizing this function, they select values
for the parameters that best match the model to the data. Researchers in numerical analy-
sis and optimization have been able to devise efﬁcient, robust minimization algorithms by
exploiting the special structure in f and its derivatives.
Toseewhythespecialformoff oftenmakesleast-squaresproblemseasiertosolvethan
general unconstrained minimization problems, we ﬁrst assemble the individual components

252
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
rj from (10.1) into a residual vector r : IRn →IRm deﬁned by
r(x)  (r1(x), r2(x), . . . , rm(x))T .
(10.2)
Using this notation, we can rewrite f as f (x)  1
2∥r(x)∥2
2. The derivatives of f (x) can be
expressed in terms of the Jacobian of r, which is the m × n matrix of ﬁrst partial derivatives
deﬁned by
J(x) 
∂rj
∂xi

j1,2,...,m
i1,2,...,n
.
(10.3)
We have
∇f (x) 
m

j1
rj(x)∇rj(x)  J(x)T r(x),
(10.4)
∇2f (x) 
m

j1
∇rj(x)∇rj(x)T +
m

j1
rj(x)∇2rj(x)
 J(x)T J(x) +
m

j1
rj(x)∇2rj(x).
(10.5)
In many applications, it is possible to calculate the ﬁrst partial derivatives that make up
the Jacobian matrix J(x) explicitly. These could be used to calculate the gradient ∇f (x)
as written in formula (10.4). However, the distinctive feature of least-squares problems is
that by knowing the Jacobian we can compute the ﬁrst part of the Hessian ∇2f (x) for free.
Moreover, this term J(x)T J(x) is often more important than the second summation term
in (10.5), either because of near-linearity of the model near the solution (that is, ∇2rj small)
or because of small residuals (that is, rj small). Most algorithms for nonlinear least-squares
exploit these structural properties of the Hessian.
The major algorithms for minimizing (10.1) ﬁt nicely into the line search and trust-
region frameworks we have already described, with a few simpliﬁcations. They are based on
the Newton and quasi-Newton approaches described in earlier sections, with modiﬁcations
that exploit the particular structure of f .
In some large-scale applications, it is more practical to compute the gradient ∇f (x)
directly using computational differentiation techniques (see Chapter 7) than to compute the
ﬁrst partial derivatives (10.3) separately and forming J(x)T r(x) from (10.4). Since we do not
haveaccesstoJ(x)fortheseproblems,wecannotexploitthespecialstructureassociatedwith
the least-squares objective (10.1), and therefore the algorithms of this chapter do not apply.
Instead, we can apply the algorithms for general large-scale unconstrained optimization
problems described in Chapters 6 and 9. Many large-scale problems do, however, allow ready
computation of the ﬁrst partial derivatives and can therefore make use of the algorithms in
this chapter.

1 0 . 1 .
B a c k g r o u n d
253
We return to our discussion of algorithms in a moment, but ﬁrst we take a little time
to explain the widespread interest in nonlinear least-squares problems.
10.1
BACKGROUND
MODELING, REGRESSION, STATISTICS
We start with an example of process modeling and show why least-squares functions
are often used to ﬁt this model to a set of observed data.
❏Example 10.1
We would like to study the effect of a certain medication on a patient. We draw blood
samples at certain times after the patient takes a dose, and measure the concentration of the
medication in each sample, tabulating the time tj and concentration yj for each sample.
We wish to construct a model that indicates the concentration of the medication as
a function of time, choosing the parameters of this model so that its predictions agree as
closely as possible with the observations we made by drawing the blood samples. Based on
our previous experience with projects of this type, we choose the following model function:
φ(x; t)  x1 + tx2 + e−x3t.
(10.6)
Here, x1, x2, x3, and t are real numbers; the ordinate t indicates time, while the xi’s are
the parameters of the model. The predicted concentration at time t is given by φ(x; t). The
differences between model predictions and observed values are combined in the following
least-squares function:
1
2
m

j1
[yj −φ(x; tj)]2.
(10.7)
This function has precisely the form (10.1) if we deﬁne rj(x)  yj −φ(x; tj). Graphically,
each term in (10.7) represents the square of the vertical distance between the curve φ(x; t)
(plotted as a function of t) and the point (tj, yj); see Figure 10.1. We choose the minimizer
x∗of the least-squares problem as the best estimate of the parameters, and use φ(x∗; t) to
estimate the concentration remaining in the patient’s bloodstream at any time t.
❐
This model is an example of what statisticians call a ﬁxed-regressor model. It assumes
that the times tj at which the blood samples are drawn are known to high accuracy, while

254
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
t 7
t 6
t 5
t 4
t 3
t 2
t 1
y
t
Figure 10.1
Deviation between the model (10.7) (smooth curve) and the observed
measurements are indicated by the vertical bars.
the observations yj may contain more or less random errors due to the limitations of the
equipment (or the lab technician!)
In general data-ﬁtting problems of the type just described, the ordinate t in the model
φ(x; t) could be a vector instead of a scalar. (In the example above, for instance, t could
contain one dimension for each of a large number of patients in a study of this particular
medication.)
The sum-of-squares function (10.7) is not the only way of measuring the discrepancy
between the model and the observations. Instead, we could use a sum of sixth-power terms
m

j1
[yj −φ(x; tj)]6,
or a sum of absolute values
m

j1
|yj −φ(x; tj)|,
(10.8)
or even some more complicated function. In fact, there are statistical motivations for choos-
ing one ﬁtting criterion over the alternatives. The least-squares criterion is solidly grounded

1 0 . 1 .
B a c k g r o u n d
255
in statistics, as we show below, though other criteria such as (10.8) are appropriate in some
circumstances.
Changing the notation slightly, let the discrepancies between model and observation
in a general data-ﬁtting problem be denoted by ϵj, that is,
ϵj  yj −φ(x; tj).
It is often reasonable to assume that the ϵj’s are independent and identically distributed with
a certain variance σ 2 and probability density function gσ(·). (This assumption will often be
true, for instance, when the model accurately reﬂects the actual process, and when the errors
made in obtaining the measurements yi do not contain a “systematic” component.) Under
this assumption, the likelihood of a particular set of observations yj, j  1, 2, . . . , m, given
that the actual parameter vector is x, is given by the function
p(y; x, σ) 
m
(
j1
gσ(ϵj) 
m
(
j1
gσ(yj −φ(x; tj)).
(10.9)
Since we know the value of the observations y1, y2, . . . , ym, the “most likely” value of x is
obtained by maximizing p(y; x, σ) with respect to x. The resulting value of x is called the
maximum likelihood estimate of the parameters.
When we assume that the discrepancies follow a normal distribution, we have
gσ(ϵ) 
1
√
2πσ 2 exp

−ϵ2
2σ 2

.
Substitution in (10.9) yields
p(y; x, σ)  (2πσ 2)−m/2 exp
#
−1
2
m

j1
[yi −φ(x; ti)]2
σ 2
$
.
For any ﬁxed value of the variance σ 2, it is obvious that p is maximized when the sum
of squares (10.7) is minimized. To summarize: When the discrepancies are assumed to be
independent, identically distributed with a normal distribution function, the maximum
likelihood estimate is obtained by minimizing the sum of squares.
The assumptions on ϵj in the previous paragraph are common, but they do not
describe the only situation for which the minimizer of the sum of squares makes statistical
sense. Seber and Wild [227] describe many instances in which minimization of functions
like (10.7), or generalizations of this function such as
rT V r, where V ∈IRm×m is symmetric and rj(x)  yj −φ(x; tj),
is the crucial step in obtaining estimates of the parameters from observed data.

256
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
Before discussing algorithms for nonlinear least-squares problems, we consider ﬁrst
the case in which the function r(x) is linear. Linear least-squares problems have a wide
range of applications of their own, and they also arise as subproblems in algorithms for the
nonlinear problem. The algorithms that we discuss in the following section are therefore an
essential component of most algorithms for nonlinear least-squares.
LINEAR LEAST-SQUARES PROBLEMS
In the special case in which each function ri is linear, the Jacobian J is constant, and
we can write
f (x)  1
2∥Jx + r∥2
2,
(10.10)
where r  r(0). We also have
∇f (x)  J T (Jx + r),
∇2f (x)  J T J.
(Note that the second term in ∇2f (x) (see (10.5)) disappears, because ∇2ri  0 for all i.)
The function f (x) in (10.10) is convex—a property that does not necessarily hold for the
nonlinear problem (10.1). By setting ∇f (x∗)  0, we see that any solution x∗of (10.10)
must satisfy
J T Jx∗ −J T r.
(10.11)
The equations (10.11) are the normal equations for (10.10).
Thelinearproblem(10.10)isinterestinginitsownright,sincemanymodelsφ(x; t)are
linear. (Our example (10.6) would be linear but for the existence of the x3 term.) Algorithms
for linear least-squares problems fall under the aegis of numerical linear algebra rather than
optimization, and there is a great deal of literature on the topic. We give additional details
in the Notes and References.
We outline brieﬂy three major algorithms for the unconstrained linear least-squares
problem. We assume for the present that m ≥n and that J has full column rank.
The ﬁrst and most obvious algorithm is simply to form and solve the system (10.11)
by the following three-step procedure:
• compute the coefﬁcient matrix J T J and the right-hand-side −J T r;
• compute the Cholesky factorization of the symmetric matrix J T J;
• perform two triangular substitutions with the Cholesky factors to recover the solution
x∗.
The Cholesky factorization
J T J  ¯RT ¯R
(10.12)

1 0 . 1 .
B a c k g r o u n d
257
(where ¯R is n × n upper triangular) is guaranteed to exist when m ≥n and J has rank m
(see the exercises). This method is frequently used in practice and is often effective, but it has
one signiﬁcant disadvantage, namely, that the condition number of J T J is the square of the
condition number of J. Since the relative error in the computed solution of any problem is
usually proportional to the condition number, the solution obtained by the Cholesky-based
method may be much less accurate than solutions obtained from algorithms that work
directly with the formulation (10.10). When J is ill conditioned, the Cholesky factorization
process may even break down, since roundoff errors may cause small negative elements to
appear on the diagonal.
A second approach is based on a QR factorization of the matrix J. Since the Euclidean
norm of any vector is not affected by orthogonal transformations, we have
∥Jx + r∥2  ∥QT (Jx + r)∥2
(10.13)
for any m × m orthogonal matrix Q. Suppose we perform a QR factorization with column
pivoting on the matrix J (see (A.54)) to obtain
J  Q

R
0



Q1
Q2


R
0

 Q1R,
(10.14)
where
 is an n × n permutation matrix (hence, orthogonal);
Q is m × m orthogonal;
Q1 is the ﬁrst n columns of Q, while Q2 contains the last m −n columns;
R is n × n upper triangular.
By combining (10.13) and (10.14), we obtain
∥Jx + r∥2
2 


QT
1
QT
2

(J T x + r)

2
2



R
0

(T x) +

QT
1 r
QT
2 r

2
2
 ∥R(T x) + QT
1 r∥2
2 + ∥QT
2 r∥2
2.
(10.15)
No choice of x has any effect on the second term of this last expression, but we can minimize
∥Jx + r∥by driving the ﬁrst term to zero, that is, by setting
x∗ −R−1QT
1 r.
(In practice, we perform a triangular substitution to solve Rz  −QT
1 r, then permute the
components of z to get x∗ z.)

258
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
This QR-based approach does not degrade the conditioning of the problem unnec-
essarily. The relative error in the ﬁnal computed solution x∗is usually proportional to the
condition number of J, not its square, and this method is usually reliable. Some situations,
however, call for greater robustness or more information about the sensitivity of the solu-
tion to perturbations in the data (J or r). A third approach, based on the singular-value
decomposition (SVD) of J, can be used in these circumstances. Recall from (A.45) that the
SVD of J is given by
J  U

S
0

V T 

U1
U2


S
0

V T  U1SV T ,
(10.16)
where
U is m × m orthogonal;
U1 contains the ﬁrst n columns of U, U2 the last m −n columns;
V is n × n orthogonal;
S is n × n diagonal, with diagonal elements σ1 ≥σ2 ≥· · · ≥σn > 0.
(Note that J T J  V S2V T , so that the columns of V are eigenvectors of J T with eigenvalues
σ 2
j , j  1, 2, . . . , n.) By following the same logic that led to (10.15), we obtain
∥Jx + r∥2
2 


S
0

(V T x) +

U T
1
U T
2

r

2
2
 ∥S(V T x) + U T
1 r∥2
2 + ∥U T
2 r∥2
2.
(10.17)
Again, the optimum is found by choosing x to make the ﬁrst term equal to zero; that is,
x∗ V S−1U T
1 r.
If we denote the ith columns of U and V by ui ∈IRm and vi ∈IRn, respectively, we can
write
x∗
n

i1
uT
i r
σi
vi.
(10.18)
This formula yields useful information about the sensitivity of x∗. When σi is small, x∗is
particularly sensitive to perturbations in r that affect uT
i r, and also to perturbations in J
that affect this same quantity. Information such as this is particularly useful when J is nearly
rank-deﬁcient, that is, when σn/σ1 ≪1. This information is not generally available when
the QR algorithm is used, and it is sometimes worth the extra cost of the SVD algorithm to
obtain it.

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
259
All three approaches above have their place. The Cholesky-based algorithm is partic-
ularly useful when m ≫n and it is practical to store J T J but not J itself. However, it must
be modiﬁed when J is rank-deﬁcient or ill conditioned to allow pivoting of the diagonal
elements of J T J. In the QR approach, ill conditioning usually (but not always) causes the
elements in the lower right-hand corner of R to be much smaller than the other elements
in this matrix. In this situation, the strategy above can be modiﬁed to produce a solution to
a nearby problem in which J is slightly perturbed. The SVD approach is the most robust
and reliable one for ill-conditioned problems. When J is actually rank-deﬁcient, some of
the singular values σi are exactly zero, and any vector x∗of the form
x∗

σi̸0
uT
i r
σi
vi +

σi0
τivi
(10.19)
(for arbitrary coefﬁcients τi) is a minimizer of (10.17). Frequently, the solution with smallest
normismostdesirable,andweobtainthisbysettingeachτi  0in(10.19)(seetheexercises).
When J has full rank but is ill conditioned, the last few singular values σn, σn−1, . . . are
small. The coefﬁcients uT
i r/σi in (10.19) are then particularly sensitive to perturbations in
uT
i r when σi is small, so a “robust” approximate solution is sometimes obtained by omitting
these terms from the summation.
10.2
ALGORITHMS FOR NONLINEAR LEAST-SQUARES
PROBLEMS
THE GAUSS–NEWTON METHOD
We now describe methods for minimizing the nonlinear objective function (10.1) that
exploit the structure in the gradient ∇f (10.4) and Hessian ∇2f (10.5). The simplest of
these methods—the Gauss–Newton method—can be viewed as a modiﬁcation of Newton’s
methodwithlinesearch.Insteadofgeneratingthesearchdirection pk bysolvingthestandard
Newtonequations∇2f (xk)p  −∇f (xk),weexcludethesecond-ordertermfrom ∇2f (xk)
and obtain pGN
k by solving
J T
k JkpGN
k  −J T
k rk.
(10.20)
This simple modiﬁcation gives a surprising number of advantages over the plain
Newton’s method. First, our use of the approximation
∇2fk ≈J T
k Jk
(10.21)
saves us the trouble of computing the individual Hessians ∇2ri, i  1, 2, . . . , m, of the
residuals, which are needed in the second term in (10.5). In fact, if we calculated the Jacobian

260
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
J(xk) in the course of evaluating the gradient ∇fk  J T
k fk, the approximation (10.21) is
essentially free, and the savings in function and derivative evaluation time can be very
signiﬁcant. Second, there are many interesting situations in which the ﬁrst term J T J in
(10.5) is much more signiﬁcant than the second term, so that the Gauss–Newton method
gives performance quite similar to that of Newton’s method even when the second term
m
j1 rj∇2rj is omitted. A sufﬁcient condition for the ﬁrst term in (10.5) to dominate
the second term is that the size of each second-order term (that is, |rj(x)|∥∇2rj(x)∥) be
signiﬁcantly smaller than the eigenvalues of J T J. This happens, for instance, when the
residuals rj are small (the so-called small-residual case), or when each rj is nearly a linear
function (so that ∥∇2rj∥is small). In practice, many least-squares problems have small
residuals at the solution, and rapid local convergence of Gauss–Newton often is observed on
these problems.
A third advantage of Gauss–Newton is that whenever J(xk) has full rank and the
gradient ∇f is nonzero, the direction pGN is a descent direction for f (·), and therefore a
suitable direction for a line search. From (10.4) and (10.20) we have
(pGN
k )T ∇fk  (pGN
k )T J T
k rk  −(pGN
k )T J T
k JkpGN
k  −∥JkpGN
k ∥2
2 ≤0.
The ﬁnal inequality is strict unless we have JkpGN
k
 0, which by (10.20) is equivalent to
J T
k rk  ∇fk  0. Finally, the fourth advantage of this method arises from the similarity
between the Gauss–Newton equations (10.20) and the normal equations (10.11) for the
linear least-squares problem. This connection tells us that pGN
k is the solution of the linear
least-squares problem
min
p ∥Jkp + fk∥2
2.
(10.22)
Hence, we can ﬁnd the search direction by applying the algorithms of the previous section to
the subproblem (10.22). If the QR or SVD algorithms are used, there is no need to calculate
the Hessian approximation J T
k Jk explicitly.
The subproblem (10.22) suggests another motivation for the Gauss–Newton step.
Rather than forming a quadratic model of the function f (x), we are forming a linear model
of the vector function r(x + p) ≈r(x) + J(x)p. The step p is obtained by substituting this
linear model into the expression f (x)  1
2∥r(x)∥2
2 and minimizing over p.
As mentioned above, we usually perform a line search in the Gauss–Newton direction,
ﬁnding a step length αk that satisﬁes the Wolfe conditions (3.6). The theory of Chapter 3
can then be applied to ensure global convergence of this method. Assume for the moment
that the Jacobians J(x) have their singular values uniformly bounded away from zero in the
region of interest; that is, there is a constant γ > 0 such that
∥J(x)z∥2 ≥γ ∥z∥2
(10.23)

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
261
for all x in a neighborhood N of the level set
L  {x | f (x) ≤f (x0)},
(10.24)
where x0 is the starting point. This assumption is the key to proving the following result,
which is a consequence of Theorem 3.2.
Theorem 10.1.
Suppose that each residual function rj is Lipschitz continuously differentiable in a neigh-
borhood N of the level set (10.24), and that the Jacobians J(x) satisfy the uniform full-rank
condition (10.23). Then if the iterates xk are generated by the Gauss–Newton method with step
lengths αk that satisfy (3.6), we have
lim
k→∞J T
k rk  0.
Proof.
First, we check that the angle θk between the search direction pGN
k and the negative
gradient −∇fk is uniformly bounded away from π/2. Note that Lipschitz continuity of each
rj(x) implies continuity of J(x); hence there is a constant β > 0 such that ∥J(x)T ∥2 
∥J(x)∥2 ≤β for all x ∈L. From (3.12), we have for x  xk ∈L and pGN  pGN
k that
cos θk  −(∇f )T pGN
∥pGN∥∥∇f ∥
 −
rT JpGN
∥pGN∥∥J T r∥
∥JpGN∥2
∥pGN∥∥J T JpGN∥≥γ 2∥pGN∥2
β2∥pGN∥2  γ 2
β2 > 0.
Lipschitzcontinuityof∇rj alsoimpliesLipschitzcontinuityof∇f (·)overtheneighborhood
N, so we can apply Theorem 3.2 to deduce that ∇f (xk) →0, giving the result.
□
If J(xk) is rank-deﬁcient for some k (that is, a condition like (10.23) is not satisﬁed),
the coefﬁcient matrix in (10.20) is singular. The system (10.20) still has a solution, however,
becauseoftheequivalencebetweenthislinearsystemandtheminimizationproblem(10.22).
In fact, there are inﬁnitely many solutions for pGN
k in this case; each of them has the form
shown in (10.19). However, there is no longer an assurance that cos θk is uniformly bounded
away from zero, so we cannot prove a result like Theorem 10.1 in general.
The speed of convergence of Gauss–Newton near a solution x∗depends on how much
the leading term J T J dominates the second-order term in the Hessian (10.5). Suppose that
xk is close to x∗and that an assumption like (10.23) is satisﬁed. Then, applying an argument
like the Newton’s method analysis (3.35), (3.36), (3.37) in Chapter 3, we have
xk + pGN
k −x∗ xk −x∗−[J T J(xk)]−1∇f (xk)
 [J T J(xk)]−1 
J T J(xk)(xk −x∗) + ∇f (x∗) −∇f (xk)

,

262
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
where J T J(x) is shorthand notation for J(x)T J(x). Using H(x) to denote the second-order
term in (10.5), we can show that
∇f (xk) −∇f (x∗) 
 1
0
J T J(x∗+ t(xk −x∗))(xk −x∗) dt
+
 1
0
H(x∗+ t(xk −x∗))(xk −x∗) dt.
The same argument as in (3.36), (3.37) shows that
∥xk + pGN
k −x∗∥
≤
 1
0
∥[J T J(xk)]−1H(x∗+ t(xk −x∗))∥∥xk −x∗∥dt + O(∥xk −x∗∥2)
≈∥[J T J(x∗)]−1H(x∗)∥∥xk −x∗∥+ O(∥xk −x∗∥2).
(10.25)
Therefore, we cannot expect a unit step of Gauss–Newton to move us closer to the solution
unless ∥[J T J(x∗)]−1H(x∗)∥< 1. In the small residual case discussed above, we often have
∥[J T J(x∗)]−1H(x∗)∥≪1, so (10.25) implies that the convergence of xk to x∗is rapid.
When H(x∗)  0, the convergence is quadratic.
THE LEVENBERG–MARQUARDT METHOD
Recall that the Gauss–Newton method is like Newton’s method with line search, except
that we use the convenient and often effective approximation (10.21) for the Hessian. The
Levenberg–Marquardt method can be derived by replacing the line search strategy with
a trust-region strategy. The use of a trust region avoids one of the weaknesses of Gauss–
Newton, namely, its behavior when the Jacobian J(x) is rank-deﬁcient, or nearly so. The
second-order Hessian component in (10.5) is still ignored, however, so the local convergence
properties of the two methods are similar.
The Levenberg–Marquardt method can be described and analyzed using the trust-
region framework of Chapter 4. (In fact, the Levenberg–Marquardt method is generally
considered to be the progenitor of the trust-region approach for general unconstrained
optimization discussed in Chapter 4.) For a spherical trust region, the subproblem to be
solved at each iteration is
min
p
1
2∥Jkp + rk∥2
2,
subject to ∥p∥≤k,
(10.26)
where k > 0 is the trust-region radius. In effect, we are choosing the model function mk(·)
in (4.3) to be
mk(p)  1
2∥rk∥2 + pT J T
k rk + 1
2pT J T
k Jkp.
(10.27)

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
263
The following convergence result can be obtained as a direct consequence of Theorem 4.8.
Theorem 10.2.
Let η ∈
	
0, 1
4

in Algorithm 4.1 of Chapter 4, and suppose that the functions ri(·) are
twice continuously differentiable in a neighborhood of the level set L  {x | f (x) ≤f (x0)},
and that for each k, the approximate solution pk of (10.26) satisﬁes the inequality
mk(0) −mk(pk) ≥c1∥J T
k rk∥min

k, ∥J T
k rk∥
∥J T
k Jk∥

,
(10.28)
for some constant c1 > 0. We then have that
lim
k→∞∇fk  lim
k→∞J T
k rk  0.
Proof.
The smoothness assumption on ri(·) implies that ∥J T J∥and ∥∇2f ∥are bounded
over a neighborhood of L, so the assumptions of Theorem 4.8 are satisﬁed, and the result
follows.
□
As in Chapter 4, there is no need to calculate the right-hand-side in the inequality
(10.28) or to check it explicitly. Instead, we can simply require the decrease given by our
approximate solution pk of (10.26) to at least match the decrease given by the Cauchy point,
which can be calculated inexpensively in the same way as in Chapter 4.
Let us drop the iteration counter k during the rest of this section and concern ourselves
with the subproblem (10.26). It happens that we can characterize the solution of (10.26) in
thefollowingway:Whenthesolution pGN oftheGauss–Newtonequations(10.20)liesstrictly
inside the trust region (that is, ∥pGN∥< ), then this step pGN also solves the subproblem
(10.26). Otherwise, there is a λ > 0 such that the solution p  pLM of (10.26) satisﬁes
∥p∥  and
	
J T J + λI

p  −J T r.
(10.29)
This claim is veriﬁed in the following lemma, which is a straightforward consequence of
Theorem 4.3 from Chapter 4.
Lemma 10.3.
The vector pLM is a solution of the trust-region subproblem
min
p
∥Jp + r∥2
2,
subject to ∥p∥≤,
for some  > 0 if and only if there is a scalar λ ≥0 such that
(J T J + λI)pLM  −J T r,
(10.30a)
λ( −∥pLM∥)  0.
(10.30b)

264
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
Proof.
In Theorem 4.3, the semideﬁniteness condition (4.19c) is satisﬁed automatically,
since J T J is positive semideﬁnite and λ ≥0. The two conditions (10.30a) and (10.30b)
follow from (4.19a) and (4.19b), respectively.
□
Note that the equations (10.29) are just the normal equations for the linear least-
squares problem
min
p
1
2


J
√
λI

p +

r
0

2
.
(10.31)
Just as in the Gauss–Newton case, the equivalence between (10.29) and (10.31) gives us a
way of solving the subproblem without computing the matrix–matrix product J T J and its
Cholesky factorization.
The original description of the Levenberg–Marquardt algorithm [150, 160] did not
make the connection with the trust-region concept. Rather, it adjusted the value of λ in
(10.29) directly, increasing or decreasing it by a certain factor according to whether or not
the previous trial step was effective in decreasing f (·). (The heuristics for adjusting λ were
similar to those used for adjusting the trust-region radius k in Algorithm 4.1.) Similar
convergence results to Theorem 10.2 can be proved for algorithms that use this approach
(see, for instance, Osborne [186]), independently of trust-region analysis. The connection
withtrustregionswasﬁrmlyestablishedinapaperbyMor´e[166].Thetrust-regionviewpoint
has proved interesting from both a theoretical and practical perspective, since it has given
rise to efﬁcient, robust software, as we note below.
IMPLEMENTATION OF THE LEVENBERG–MARQUARDT METHOD
To ﬁnd a value of λ that approximately matches the given  in Lemma 10.3, we can
use the root-ﬁnding algorithm described in Chapter 4. It is easy to safeguard this procedure:
The Cholesky factor R is guaranteed to exist whenever the current estimate λ(ℓ) is positive,
since the approximate Hessian B  J T J is already positive semideﬁnite. Because of the
special structure of B, we do not need to compute the Cholesky factorization naively as in
Chapter 4. Rather, it is easy to show that the following QR factorization of the coefﬁcient
matrix in (10.31),

Rλ
0

 QT
λ

J
√
λI

(10.32)
(Qλ orthogonal, Rλ upper triangular), yields an Rλ that satisﬁes RT
λ Rλ  (J T J + λI).
Wecansavealittlecomputertimeinthecalculationofthefactorization(10.32)byusing
a combination of Householder and Givens transformations. Suppose we use Householder

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
265
transformations to calculate the QR factorization of J alone as
J  Q

R
0

.
(10.33)
We then have


R
0
√
λI



QT
I
 
J
√
λI

.
(10.34)
The leftmost matrix in this formula is upper triangular except for the n nonzero terms of the
matrix λI. These can be eliminated by a sequence of n(n + 1)/2 Givens rotations, in which
the diagonal elements of the upper triangular part are used to eliminate the nonzeros of λI
and the ﬁll-in terms that arise in the process. The ﬁrst few steps of this process are as follows:
rotate row n of R with row n of
√
λI, to eliminate the (n, n) element of
√
λI;
rotate row n −1 of R with row n −1 of
√
λI to eliminate the (n −1, n −1) element
of the latter matrix. This step introduces ﬁll-in in position (n −1, n) of
√
λI, which
is eliminated by rotating row n of R with row n −1 of
√
λI, to eliminate the ﬁll-in
element at position (n −1, n);
rotate row n −2 of R with row n −2 of
√
λI, to eliminate the (n −2) diagonal in the
latter matrix. This step introduces ﬁll-in in the (n−2, n−1) and (n−2, n) positions,
which we eliminate by . . .
and so on. If we gather all the Givens rotations into a matrix ¯Qλ, we obtain from (10.34)
that
¯QT
λ


R
0
√
λI




Rλ
0
0

,
and hence (10.32) holds with
Qλ 

Q
I

¯Qλ.
The advantage of this combined approach is that when the value of λ is changed in the
root-ﬁnding algorithm, we need only recalculate ¯Qλ and not the Householder part of the
factorization (10.34). This feature can save a lot of computation in the case of m ≫n, since

266
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
just O
	
n3
operations are required to recalculate ¯Qλ and Rλ for each value of λ, after the
initial cost of O
	
mn2
operations needed to calculate Q in (10.33).
Least-squares problems are often poorly scaled. Some of the variables could have
values of about 104, while other variables could be of order 10−6. If such wide variations are
ignored, the algorithms above may encounter numerical difﬁculties or produce solutions of
poor quality. One way to reduce the effects of poor scaling is to use an ellipsoidal trust region
in place of the spherical trust region deﬁned above. The step is conﬁned to an ellipse in
which the lengths of the principal axes are related to the typical values of the corresponding
variables. Analytically, the trust-region subproblem becomes
min
p
1
2∥Jkp + rk∥2
2,
subject to ∥Dkp∥≤k,
(10.35)
where Dk is a diagonal matrix with positive diagonal entries. Instead of (10.29), the solution
of (10.35) satisﬁes an equation of the form
	
J T
k Jk + λD2
k

pLM
k  −J T
k rk,
(10.36)
and, equivalently, solves the linear least-squares problem
min
p


Jk
√
λDk

p +

rk
0
 .
(10.37)
The diagonals of the scaling matrix Dk can change from iteration to iteration, as we gather
information about the typical range of values for each component of x. If the variation in
these elements is kept within certain bounds, then the convergence theory for the spherical
case continues to hold. Moreover, the technique described above for calculating Rλ needs
no modiﬁcation. Seber and Wild [227] suggest choosing the diagonals of D2
k to match those
of J T
k Jk, to make the algorithm invariant under diagonal scaling of the components of x.
This is analogous to the technique of scaling by diagonal elements of the Hessian, which
was described in Chapter 4 in the context of trust-region algorithms for unconstrained
optimization.
The local convergence behavior of Levenberg–Marquardt is similar to the Gauss–
Newton method. Near a solution with small residuals, the model function in (10.26) will
giveanaccuratepictureofthebehavioroff .Thetrustregionwilleventuallybecomeinactive,
and the algorithm will take unit Gauss–Newton steps, giving the rapid linear convergence
rate that we derived in our analysis of the Gauss–Newton method.
LARGE-RESIDUAL PROBLEMS
In large-residual problems, the quadratic model in (10.26) is an inadequate repre-
sentation of the function f , because the second-order part of the Hessian ∇2f (x) is too
signiﬁcant to be ignored. Some statisticians downplay the importance of algorithms for such

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
267
problems. They argue that if the residuals are large at the solution, then the model is not
up to the task of matching the observations. Instead of solving the least-squares problem,
they would prefer to construct a new and better model. Often, however, large residuals are
caused by “outliers” in the observations due to human error: An operator may read a dial
incorrectly, or a geologist may assign a particular seismic reading to the wrong earthquake.
In these cases, we may still want to solve the least-squares problem because the solution
may be useful in identifying the outliers. These can then be removed from the data set or
deemphasized by assigning them a lower weight.
Performance of the Gauss–Newton and Levenberg–Marquardt algorithms is usually
poor in the large-residual case. Asymptotic convergence is only linear—slower than the
superlinear convergence rate attained by algorithms for general unconstrained problems,
such as Newton or quasi-Newton. Newton’s method with trust region or line search is an
option, particularly if the individual Hessians ∇2rj(·) are easy to calculate. Quasi-Newton
methods will also converge at a faster asymptotic rate than Gauss–Newton or Levenberg–
Marquardt in the large-residual case. However, the behavior of both Newton and quasi-
Newton on early iterations (before the iterations reach a neighborhood of the solution)
may be inferior to Gauss–Newton and Levenberg–Marquardt, and in the case of Newton’s
method, we have to pay the additional cost of computing second derivatives.
Of course, we often do not know beforehand whether a problem will turn out to
have small or large residuals at the solution. It seems reasonable, therefore, to consider
hybrid algorithms, which would behave like Gauss–Newton or Levenberg–Marquardt if the
residuals turn out to be small (and hence take advantage of the cost savings associated with
these methods) but switch to Newton or quasi-Newton steps if the residuals at the solution
appear to be large.
There are a couple of ways to construct hybrid algorithms. One approach, due to
Fletcher and Xu (see Fletcher [83]), maintains a sequence of positive deﬁnite Hessian ap-
proximations Bk. If the Gauss–Newton step from xk reduces the function f by a certain ﬁxed
amount (say, a factor of 5), then this step is taken and Bk is overwritten by J T
k Jk. Otherwise,
a direction is computed using Bk, and the new point xk+1 is obtained by doing a line search.
In either case, a BFGS-like update is applied to Bk to obtain a new approximation Bk+1.
In the zero-residual case, the method eventually always takes Gauss–Newton steps (giving
quadratic convergence), while it eventually reduces to BFGS in the nonzero-residual case
(giving superlinear convergence). Numerical results in Fletcher [83, Tables 6.1.2, 6.1.3] show
good results for this approach on small-, large-, and zero-residual problems.
A second way to combine Gauss–Newton and quasi-Newton ideas is to maintain
approximations to just the second-order part of the Hessian. That is, we maintain a sequence
of matrices Sk that approximate the summation term m
j1 rj(xk)∇2rj(xk) in (10.5), and
then use the overall Hessian approximation
Bk  J T
k Jk + Sk
in a trust-region or line search model for calculating the step pk. Updates to Sk are devised
so that the approximate Hessian Bk, or its constituent parts, mimics the behavior of the

268
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
corresponding exact quantities over the step just taken. Here is another instance of the secant
equation, which also arose in the context of unconstrained minimization (8.6) and nonlinear
equations (11.26). In the present case, there are a number of different ways to deﬁne the
secant equation and to specify the other conditions needed for a complete update formula
for Sk. We describe the algorithm of Dennis, Gay, and Welsch [67], which is probably the
best-knownalgorithminthisclassbecauseofitsimplementationinthewell-knownNL2SOL
package.
Dennis, Gay, and Welsch motivate their form of the secant equation in the following
way.Ideally,Sk+1 shouldbeacloseapproximationtotheexactsecond-ordertermatx  xk+1;
that is,
Sk+1 ≈
m

j1
rj(xk+1)∇2rj(xk+1).
Since we do not want to calculate the individual Hessians ∇2rj in this formula, we could
replace each of them with an approximation (Bj)k+1 and impose the condition that (Bj)k+1
should mimic the behavior of its exact counterpart ∇2rj over the step just taken; that is,
(Bj)k+1(xk+1 −xk)  ∇rj(xk+1) −∇rj(xk)
 (row j of J(xk+1))T −(row j of J(xk))T .
This condition leads to a secant equation on Sk+1, namely,
Sk+1(xk+1 −xk) 
m

j1
rj(xk+1)(Bj)k+1(xk+1 −xk)

m

j1
rj(xk+1)

(row j of J(xk+1))T −(row j of J(xk))T 
 J T
k+1rk+1 −J T
k rk.
As usual, this condition does not completely specify the new approximation Sk+1. Dennis,
Gay, and Welsch add requirements that Sk+1 be symmetric and that the difference Sk+1 −Sk
from the previous estimate Sk be minimized in a certain sense, and derive the following
update formula:
Sk+1  Sk + (y♯−Sks)yT + y(y♯−Sks)T
yT s
−(y♯−Sks)T s
(yT s)2
yyT ,
(10.38)
where
s  xk+1 −xk,

1 0 . 2 .
A l g o r i t h m s f o r N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
269
y  J T
k+1rk+1 −J T
k rk,
y♯ J T
k+1rk+1 −J T
k rk+1.
Note that (10.38) is a slight variant on the DFP update for unconstrained minimization. It
would be identical if y♯and y were the same.
Dennis, Gay, and Welsch use their approximate Hessian J T
k Jk + Sk in conjunction
with a trust-region strategy, but a few more features are needed to enhance its performance.
One deﬁciency of its basic update strategy for Sk is that this matrix is not guaranteed to
vanish as the iterates approach a zero-residual solution, so it can interfere with superlinear
convergence. This problem is avoided by scaling Sk prior to its update; we replace Sk by τkSk
on the right-hand-side of (10.38), where
τk  min

1, |sT y♯|
|sT Sks|

.
A ﬁnal modiﬁcation in the overall algorithm is that the Sk term is omitted from the Hessian
approximation when the resulting Gauss–Newton model produces a sufﬁciently good step.
LARGE-SCALE PROBLEMS
Theproblem(10.1)canbeclassiﬁedaslargeifthenumberofparameters nandnumber
of residuals m are both large. If n is small while m is large, then the algorithms described
in earlier sections can be applied directly, possibly with some modiﬁcations to account for
peculiaritiesoftheproblem.If,forinstance,misoforder106 whilenisabout100,theJacobian
J(x) may be too large to store conveniently. However, it is easy to calculate the matrix J T J
and gradient vector J T r by evaluating rj and ∇rj successively for j  1, 2, . . . , m and
performing the accumulations
J T J 
m

i1
(∇rj)(∇rj)T ,
J T r 
m

i1
rj(∇rj).
The Gauss–Newton and Levenberg–Marquardt steps can then be computed by solving the
systems (10.20) and (10.29) of normal equations directly. Note that there is no need to
recalculate J T J in (10.29) for each new value of λ in the Levenberg–Marquardt step; we
need only add the new value of λI to the previously computed J T J and then recompute
the factorization.
When n and m are both large, however, and the Jacobian J(x) is sparse, the cost of
computing steps exactly via the formulae (10.20) and (10.29) may become quite expen-
sive relative to the cost of function and gradient evaluations. In this case, we can design
inexact variants of the Gauss–Newton and Levenberg–Marquardt algorithms that are anal-
ogous to the inexact Newton algorithms discussed in Chapters 6. We simply replace the

270
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
Hessian ∇2f (xk) in these methods by its approximation J T
k Jk. The positive semideﬁnite-
ness of this approximation simpliﬁes the resulting algorithms in several places. Naturally,
we cannot expect the superlinear convergence results to hold, except in the case in which the
approximation is exact at the solution, that is, ∇2f (x∗)  J(x∗)T J(x∗).
We mention two other approaches for large-scale problems that take advantage of the
specialstructureofleastsquares.Toint[239]combinestheGauss–NewtonandDennis–Gay–
WelschapproachesdescribedearlierwiththepartiallyseparableideasdiscussedinChapter9.
A compact representation of each second-order term ∇2rj, j  1, 2, . . . , m, of the Hessian
is maintained, and at each iteration a choice is made as to whether this approximation should
be used in the step calculation or simply ignored. (In the latter case, the Gauss–Newton step
will be calculated.) The step equations are solved by a conjugate gradient procedure.
The inexact Levenberg–Marquardt algorithm described by Wright and Holt [257]
takes steps ¯p that are inexact solutions of the system (10.29). Analogously to (6.2) and (6.3)
in Chapter 6, they satisfy
	
J T J + λI

¯p + J T r
 ≤η∥J T r∥,
for some η ∈(0, 1).
(10.39)
Instead of using a trust-region radius to dictate the choice of λ, however, this method reverts
to the original Levenberg–Marquardt strategy of manipulating λ directly. Speciﬁcally, λ is
chosen sufﬁciently large at each iteration to force a “sufﬁcient decrease” in the sum of squares
at the new point x + ¯p, according to the following criterion:
∥r(x)∥2 −∥r(x + ¯p)∥2
∥r(x)∥2 −∥r(x) + J(x) ¯p∥2 −λ2∥¯p∥2 ≥γ1,
(10.40)
for some chosen constant γ1 ∈(0, 1). By choosing λ to be not too much larger than the
smallest value for which this bound in satisﬁed, we are able to prove a global convergence
result.
Rather than recomputing approximate solutions of the system (10.29) for a number
of different λ values in turn, until one such value yields a step ¯p for which (10.39) holds,
the algorithm in [257] applies Algorithm LSQR of Paige and Saunders [188] to the least-
squares formulation (10.31), solving this system simultaneously for a number of different
values of λ. This approach takes advantage of the fact that each additional value of λ entails
just a small marginal cost for LSQR: to be precise, storage of just two additional vectors
and some additional vector arithmetic (but no extra matrix–vector multiplications). We can
therefore identify a value of λ and corresponding step ¯p that satisfy the conditions of the
global convergence result at a cost not too much higher than that of the approximate solution
of a single linear least-squares problem of the form (10.31) by the LSQR algorithm.

1 0 . 3 .
O r t h o g o n a l D i s t a n c e R e g r e s s i o n
271
10.3
ORTHOGONAL DISTANCE REGRESSION
In Example 10.1 we assumed that no errors were made in noting the time at which the blood
samples were drawn, so that the differences between the model φ(x; tj) and the observation
yj were due to inadequacy in the model or in errors in making the measurement of yj. We
assumed that any errors in the ordinates—the times tj—are tiny by comparison with the
errors in the observations. This assumption often is reasonable, but there are cases where
the answer can be seriously distorted if we fail to take possible errors in the ordinates into
account. Models that take these errors into account are known in the statistics literature as
errors-in-variables models [227, Chapter 10], and the resulting optimization problems are
referred to as total least squares in the case of a linear model (see Golub and Van Loan [115,
Chapter 5]) or as orthogonal distance regression in the nonlinear case (see Boggs, Byrd, and
Schnabel [20]).
We formulate this problem mathematically by introducing perturbations δj for the
ordinates tj, as well as perturbations ϵj for yj, and seeking the values of these 2m perturba-
tions that minimize the discrepancy between the model and the observations, as measured
by a weighted least-squares objective function. To be precise, we relate the quantities tj, yj,
δj, and ϵj by
yj  φ(x; tj + δj) + ϵj,
j  1, 2, . . . , m,
(10.41)
and deﬁne the minimization problem as
min
x,δj,ϵj
1
2
m

j1
w2
jϵ2
j + d2
j δ2
j,
subject to (10.41).
(10.42)
The quantities wi and di are weights, selected either by the modeler or by some automatic
estimate of the relative signiﬁcance of the error terms.
It is easy to see how the term “orthogonal distance regression” originates when we
graph this problem; see Figure 10.2. If all the weights wi and di are equal, then each term
in the summation (10.42) is simply the shortest distance between the point (tj, yj) and the
curve φ(x; t) (plotted as a function of t). The shortest path between the point and the curve
will be normal (orthogonal) to the curve at the point of intersection.
It is easy to use the constraints (10.41) to eliminate the variables ϵj from (10.42) and
obtain the unconstrained least-squares problem
min
x,δj F(x, δ)  1
2
m

j1
w2
i [yj −φ(x; tj + δj)]2 + d2
j δ2
j  1
2
2m

j1
rj(x, δ),
(10.43)

272
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
t 7
t 6
t 5
t 4
t 3
t 2
t 1
y
t
Figure 10.2
Orthogonal distance regression minimizes the sum of squares of the
distance from each point to the curve.
where we have deﬁned
rj(x, δ) 

wj[φ(x; tj + δj) −yj],
j  1, 2, . . . , m,
dj−mδj−m,
j  m + 1, . . . , 2m.
(10.44)
Note that (10.43) is now a standard least-squares problem with 2m terms and m + n
unknowns, which we can solve by using any of the techniques in this chapter. A naive
implementation of this strategy may, however, be quite expensive, since the number of pa-
rameters (2n) and the number of observations (m + n) may both be much larger than for
the original problem.
Fortunately,theJacobianmatrixfor(10.43)hasaspecialstructurethatcanbeexploited
in implementing methods such as Gauss–Newton or Levenberg–Marquardt. Many of its
components are zero; for instance, we have
∂rj
∂δi
 ∂[φ(tj + δj; x) −yj]
∂δi
 0,
i, j  1, 2, . . . , m, i ̸ j,
whereas
∂rj
∂xi
 0,
j  m + 1, . . . , 2n, i  1, 2, . . . , n.

1 0 . 3 .
O r t h o g o n a l D i s t a n c e R e g r e s s i o n
273
Additionally, we have for j  1, 2, . . . , m and i  1, 2, . . . , m that
∂rm+j
∂δi


dj
if i  j,
0
otherwise.
Hence, we can partition the Jacobian of the residual function r deﬁned by (10.44) into blocks
and write
J(x, δ) 

ˆJ
V
0
D

,
(10.45)
where V and D are m×m diagonal matrices and ˆJ is the m×n matrix of partial derivatives
of the functions wjφ(tj + δj; x) with respect to x. Boggs, Byrd, and Schnabel apply the
Levenberg–Marquardt algorithm to (10.43) and note that block elimination can be used to
solve the subproblems (10.29), (10.31) efﬁciently. Given the partitioning (10.45), we can
partition the step vector p and the residual vector r accordingly as
p 

px
pδ

,
r 

ˆr1
ˆr2

,
and write the normal equations (10.29) in the partitioned form

ˆJ T ˆJ + λI
ˆJ T V
V ˆJ
V 2 + D2 + λI
 
px
pδ

 −

ˆJ T ˆr1
V ˆr1 + Dˆr2

.
(10.46)
Since the lower right submatrix V 2 + D2 + λI is diagonal, it is easy to eliminate pδ from
this system and obtain a smaller n × n system to be solved for px alone. The total cost of
ﬁnding a step is only marginally greater than for the m×n problem arising from the standard
least-squares model.
NOTES AND REFERENCES
Interesting examples of large-scale linear least-squares problems arise in structural
engineering [192, 18] and in numerical geodesy [191]. Algorithms for linear least squares
are discussed comprehensively by Lawson and Hanson [148], who include detailed error
analyses of the different algorithms and software listings. They consider not just the basic
problem (10.10) but also the situation in which there are bounds (for example, x ≥0)
or linear constraints (for example, Ax ≥b) on the variables. Golub and Van Loan [115,
Chapter 5] survey the state of the art, including discussion of the suitability of the different
approaches (e.g., normal equations vs. QR factorization) for different problem types. The
recent book of Bj¨orck [19] gives a comprehensive survey of the whole ﬁeld.

274
C h a p t e r
1 0 .
N o n l i n e a r L e a s t - S q u a r e s P r o b l e m s
Problems in which m is of order 106 while n is less than 100 occur in a Laue
crystallography application; see Ren and Moffatt [212].
Software for nonlinear least squares is fairly prevalent because of the high demand
for it. An overview of available programs with details on how to obtain them is given in
the NEOS Guide on the World-Wide Web (see the Preface) and by Mor´e and Wright [173,
Chapter 3]. Seber and Wild [227, Chapter 15] describe some of the important practical issues
in selecting software for statistical applications. For completeness, we mention a few of the
more popular programs here.
The pure Gauss–Newton method is apparently not available in production software
because it lacks robustness. However, as we discuss above, many algorithms do take Gauss–
Newton steps when these steps are effective in reducing the objective function; consider,
for instance, the NL2SOL program, which implements the method of Dennis, Gay, and
Welsch [67]. The Levenberg–Marquardt algorithm is also available in a high-quality im-
plementation in the MINPACK package. The orthogonal distance regression algorithm is
implemented by ODRPACK [21]. All these routines give the user the option of either sup-
plying Jacobians explicitly or else allowing the code to compute them by ﬁnite differencing.
(In the latter case, the user need only write code to compute the residual vector r(x); see
Chapter 7.) All routines mentioned are freely available.
Major numerical software libraries such as NAG and Harwell [133] also contain robust
nonlinear least-squares implementations.
✐
E x e r c i s e s
✐
10.1 When J is an m × n matrix with m ≥n, show that J has full column rank if
and only if J T J is nonsingular.
✐
10.2 Show that ¯R in (10.12) and R in (10.14) are identical if   I and if both have
nonnegative diagonal elements.
✐
10.3 Prove that when J is rank-deﬁcient, the minimizer of (10.10) with smallest
Euclidean norm is obtained by setting each τi  0 in (10.19).
✐
10.4 Suppose that each residual function rj and its gradient are Lipschitz continuous
with Lipschitz constant L, that is,
∥rj(x1) −rj(x2)∥≤L∥x1 −x2∥,
∥∇rj(x1) −∇rj(x2)∥≤L∥x1 −x2∥
for all j  1, 2, . . . , m and all x1, x2 ∈D, where D is a compact subset of IRn. Find Lipschitz
constants for the Jacobian J(x) (10.3) and the gradient ∇f (x) (10.4) over D.
✐
10.5 For the Gauss–Newton step pGN, use the singular-value decomposition (10.16)
of Jk and the formula (10.19) to express (∇fk)T pGN
k in terms of rk, ui, vi, and σi. Express

1 0 . 3 .
O r t h o g o n a l D i s t a n c e R e g r e s s i o n
275
∥pGN
k ∥and ∥∇fk∥, and therefore cos θk deﬁned by (3.12), in these same terms, and explain
why there is no guarantee that cos θk > 0 when Jk is rank-deﬁcient.
✐
10.6 Express the solution p of (10.29) in terms of the singular-value decomposition
of J(x) and the scalar λ. Express its squared-norm ∥p∥2 in these same terms, and show that
lim
λ→0 p 

σi̸0
uT
i r
σi
vi.
✐
10.7 Eliminatepδ from(10.46)toobtainann×nlinearsysteminpx alone.Thissystem
happens to be the normal equations for a certain (somewhat messy) linear least-squares
problem in px. What is this problem?

Chapter11

Nonlinear
Equations
In many applications we do not need to optimize an objective function explicitly, but rather
to ﬁnd values of the variables in a model that satisfy a number of given relationships. When
these relationships take the form of n equalities—the same number of equality conditions
as variables in the model—the problem is one of solving a system of nonlinear equations. We
write this problem mathematically as
r(x)  0,
(11.1)
where r : IRn →IRn is a vector function, that is,
r(x) 


r1(x)
r2(x)
...
rn(x)


,

278
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
where each ri : IRn →IR, i  1, 2, . . . , n, is smooth. A vector x∗for which (11.1) is satisﬁed
is called a solution or root of the nonlinear equations. A simple example is the system
r(x) 

x2
2 −1
sin x1 −x2

 0,
which is a system of n  2 equations with inﬁnitely many solutions, two of which are
x∗ (3π/2, −1)T and x∗ (π/2, 1)T . In general, the system (11.1) may have no solutions,
a unique solution, or many solutions.
The techniques for solving nonlinear equations overlap in their motivation, analysis,
and implementation with optimization techniques discussed in earlier chapters. In both
optimization and nonlinear equations, Newton’s method lies at the heart of many important
algorithms. Features such as line searches, trust regions, and inexact solution of the linear
algebra subproblems at each iteration are important in both areas, as are other issues such
as derivative evaluation and global convergence.
Because some important algorithms for nonlinear equations proceed by minimizing
a sum of squares of the equations, that is,
min
x
n

i1
r2
i (x),
there are particularly close connections with the nonlinear least-squares problem discussed
inChapter10.Thedifferencesarethatinnonlinearequations,thenumberofequationsequals
the number of variables (instead of exceeding the number of variables as in Chapter 10),
and that we expect all equations to be satisﬁed at the solution, rather than just minimizing
the sum of squares. This point is important because the nonlinear equations may represent
physical or economic constraints such as conservation laws or consistency principles, which
must hold exactly in order for the solution to be meaningful.
Many applications require us to solve a sequence of closely related nonlinear systems,
as in the following example.
❏Example 11.1
(Rheinboldt; see [168])
An interesting problem in control is to analyze the stability of an aircraft in response
to the commands of the pilot. The following is a simpliﬁed model based on force-balance
equations, in which gravity terms have been neglected.
The equilibrium equations for a particular aircraft are given by a system of 5 equations
in 8 unknowns of the form
F(x) ≡Ax + φ(x)  0,
(11.2)

C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
279
where F : IR8 →IR5, the matrix A is given by
A 


−3.933
0.107
0.126
0
−9.99
0
−45.83
−7.64
0
−0.987
0
−22.95
0
−28.37
0
0
0.002
0
−0.235
0
5.67
0
−0.921
−6.51
0
1.0
0
−1.0
0
−0.168
0
0
0
0
−1.0
0
−0.196
0
−0.0071
0


,
and the nonlinear part is deﬁned by
φ(x) 


−0.727x2x3 + 8.39x3x4 −684.4x4x5 + 63.5x4x2
0.949x1x3 + 0.173x1x5
−0.716x1x2 −1.578x1x4 + 1.132x4x2
−x1x5
x1x4


.
The ﬁrst three variables x1, x2, x3, represent the rates of roll, pitch, and yaw, respec-
tively, while x4 is the incremental angle of attack and x5 the sideslip angle. The last three
variables x6, x7, x8 are the controls; they represent the deﬂections of the elevator, aileron,
and rudder, respectively.
For a given choice of the control variables x6, x7, x8 we obtain a system of 5 equations
and 5 unknowns. If we wish to study the behavior of the aircraft as the controls are changed,
we need to solve a system of nonlinear equations with unknowns x1, . . . , x5 for each setting
of the controls.
❐
Despite the many similarities between nonlinear equations and unconstrained and
least-squares optimization algorithms, there are also some important differences. To ob-
tain quadratic convergence in optimization we require second derivatives of the objective
function, whereas knowledge of the ﬁrst derivatives is sufﬁcient in nonlinear equations.
Quasi-Newton methods are perhaps less useful in nonlinear equations than in optimiza-
tion. In unconstrained optimization, the objective function is the natural choice of merit
function that gauges progress towards the solution, but in nonlinear equations various merit
functions can be used, all of which have some drawbacks. Line search and trust-region tech-
niques play an equally important role in optimization, but one can argue that trust-region
algorithms have certain theoretical advantages in solving nonlinear equations.
Some of the difﬁculties that arise in trying to solve nonlinear equation problems can
be illustrated by a simple scalar example (n  1). Suppose we have
r(x)  sin(5x) −x,
(11.3)

280
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
−3
−2
−1
0
1
2
3
−4
−3
−2
−1
0
1
2
3
4
Figure 11.1
The function r(x)  sin(5x) −x has three roots.
as plotted in Figure 11.1. From this ﬁgure we see that there are three solutions of the problem
r(x)  0, also known as roots of r, located at zero and approximately ±0.519148. This
situation of multiple solutions is similar to optimization problems where, for example, a
function may have more than one local minimum. It is not quite the same, however: In the
case of optimization, one of the local minima may have a lower function value than the
others (making it a “better” solution), while in nonlinear equations all solutions are equally
good from a mathematical viewpoint. (If the modeler decides that the solution found by the
algorithm makes no sense on physical grounds, their model may need to be reformulated.)
In this chapter we start by outlining algorithms related to Newton’s method and
examining their local convergence properties. Besides Newton’s method itself, these include
Broyden’s quasi-Newton method, inexact Newton methods, and tensor methods. We then
address global convergence, which is the issue of trying to force convergence to a solution
from a remote starting point. Finally, we discuss a class of methods in which an “easy”
problem—one to which the solution is well known—is gradually transformed into the
problem F(x)  0. In these so-called continuation (or homotopy) methods, we track the
solution as the problem changes, with the aim of ﬁnishing up at a solution of F(x)  0.
Throughout this chapter we make the assumption that the vector function r is contin-
uously differentiable in the region D containing the values of x we are interested in. In other
words, the Jacobian J(x)  ∇r(x) exists and is continuous. For some local convergence

1 1 . 1 .
L o c a l A l g o r i t h m s
281
results, we will use the stronger assumption of Lipschitz continuity of the Jacobian, that is,
existence of a constant βL > 0 such that
∥J(x0) −J(x1)∥≤βL∥x0 −x1∥,
(11.4)
for all x0, x1 ∈D. We say that x∗satisfying r(x∗)  0 is a degenerate solution if J(x∗) is
singular, and a nondegenerate solution otherwise.
11.1
LOCAL ALGORITHMS
NEWTON’S METHOD FOR NONLINEAR EQUATIONS
Recall from Theorem 2.1 that Newton’s method for minimizing f : IRn →IR forms a
quadratic model function by taking the ﬁrst three terms of the Taylor series approximation
of f around the current iterate xk. Unless trust-region constraints or step lengths interfere,
the Newton step is the vector that minimizes this model. In the case of nonlinear equations,
Newton’s method is derived in a similar way, by taking a Taylor series approximation to r
around the current iterate xk. The relevant variant of Taylor’s theorem is as follows.
Theorem 11.1.
Suppose that r : IRn →IRn is continuously differentiable in some convex open set D and
that x and x + p are vectors in D. Then we have that
r(x + p)  r(x) +
 1
0
J(x + tp)p dt.
(11.5)
We can deﬁne a linear model Mk(p) of r(xk + p) by approximating the second term on the
right-hand-side of (11.5) by J(x)p, and writing
Mk(p)
def r(xk) + J(xk)p.
(11.6)
The difference between Mk(p) and r(xk + p) is
r(xk + p)  Mk(p) +
 1
0
[J(xk + tp) −J(xk)]p dt.
We can quantify this difference by noting that J is continuous, so that
∥J(x + tp) −J(x)∥→0 as p →0, for all t ∈[0, 1],

282
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
and therefore

 1
0
[J(x + tp) −J(x)]p dt
 ≤
 1
0
∥J(x + tp) −J(x)∥∥p∥dt  o(∥p∥).
(11.7)
When we make the stronger assumption that J is Lipschitz continuous (11.4), we have the
stronger estimate

 1
0
[J(x + tp) −J(x)]p dt
  O(∥p∥2).
(11.8)
Newton’s method, in its pure form, chooses the step pk to be the vector for which
Mk(pk)  0, that is, pk  −J(xk)−1r(xk). We deﬁne it formally as follows.
Algorithm 11.1 (Newton’s Method for Nonlinear Equations).
Choose x0;
for k  0, 1, 2, . . .
Calculate a solution pk to the Newton equations
J(xk)pk  −r(xk);
(11.9)
xk+1 ←xk + pk;
end (for)
We use a linear model to derive the Newton step—rather than a quadratic model
as in unconstrained optimization—because the linear model normally has a solution and
yields an algorithm with rapid convergence properties. In fact, Newton’s method for un-
constrained optimization (see (2.14)) can be derived by applying Algorithm 11.1 to the
nonlinear equations ∇f (x)  0. We see also in Chapter 18 that sequential quadratic pro-
gramming for equality-constrained optimization can be derived by applying Algorithm 11.1
to the nonlinear equations formed by the ﬁrst-order optimality conditions (18.3) for this
problem. Another connection is with the Gauss–Newton method for nonlinear least squares;
the formula (11.9) is equivalent to (10.20) in the usual case in which J(xk) is nonsingular.
When the iterate xk is close to a nondegenerate root x∗, Newton’s method has local su-
perlinear convergence when the Jacobian J is a continuous function of x, and local quadratic
convergence when J is Lipschitz continuous. (We outline these convergence properties
below.) Potential shortcomings of the method include the following.
• When the starting point is remote from a solution, Algorithm 11.1 can behave
erratically. When J(xk) is singular, the Newton step may not even be deﬁned.
• First-derivative information (required for the Jacobian matrix J) may be difﬁcult to
obtain.

1 1 . 1 .
L o c a l A l g o r i t h m s
283
• It may be too expensive to ﬁnd and calculate the Newton step pk exactly when n is
large.
• The root x∗in question may be degenerate, that is, J(x∗) may be singular.
An example of a degenerate problem is the scalar function r(x)  x2, which has a single
degenerate root at x∗ 0. Algorithm 11.1, when started from any nonzero x0, generates the
sequence of iterates
xk  1
2k x0,
which converges to the solution 0, but at only a linear rate.
As we show later in this chapter, Newton’s method can be modiﬁed and enhanced in
various ways to get around most of these problems. The variants we describe form the basis
of much of the available software for solving nonlinear equations.
We summarize the local convergence properties of Algorithm 11.1 in the following
theorem.
Theorem 11.2.
Suppose that r is continuously differentiable in a convex open set D ⊂IRn. Let x∗∈D
be a nondegenerate solution of r(x)  0, and let {xk} be the sequence of iterates generated by
Algorithm 11.1. Then when xk ∈D is sufﬁciently close to x∗, we have
xk+1 −x∗ o(∥xk −x∗∥),
(11.10)
indicatinglocalQ-superlinearconvergence.Whenr isLipschitzcontinuouslydifferentiablenear
x∗, we have for all xk sufﬁciently close to x∗that
xk+1 −x∗ O(∥xk −x∗∥2),
(11.11)
indicating local Q-quadratic convergence.
Proof.
Since r(x∗)  0, an estimate similar to (11.7) yields that
r(xk)
 r(xk) −r(x∗)
 J(xk)(xk −x∗) +
 1
0

J(xk + t(x∗−xk)) −J(xk)

(xk −x∗) dt
 J(xk)(xk −x∗) + o(∥xk −x∗∥).
(11.12)

284
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
Since J(x∗) is nonsingular, there is a radius δ > 0 and a positive constant β∗such that for
all x in the ball B(x∗, δ) deﬁned by
B(x∗, δ)  {x | ∥x −x∗∥≤δ},
(11.13)
we have that
∥J(x)−1∥≤β∗
and
x ∈D.
(11.14)
Assuming that xk ∈B(x∗, δ), and recalling the deﬁnition (11.9), we multiply both sides of
(11.12) by J(xk)−1 to obtain
−pk  (xk −x∗) + ∥J(xk)−1∥o(∥xk −x∗∥),
⇒
xk + pk −x∗ o(∥xk −x∗∥),
⇒
xk+1 −x∗ o(∥xk −x∗∥),
(11.15)
which yields (11.10).
When the Lipschitz continuity assumption (11.4) is satisﬁed, we can write
r(xk)  r(xk) −r(x∗)  J(xk)(xk −x∗) + w(xk, x∗),
(11.16)
where the remainder term w(xk, x∗) is deﬁned by
w(xk, x∗) 
 1
0

J(xk + t(x∗−xk)) −J(xk)

(xk −x∗) dt.
We can use the same argument that led to the bound (11.8) to estimate w(xk, x∗) as follows:
∥w(xk, x∗)∥ O(∥xk −x∗∥2).
(11.17)
By multiplying (11.16) by J(xk)−1 as above and using (11.9), we obtain
−pk + (xk −x∗)  J(xk)−1w(xk, x∗),
so the estimate (11.11) follows by taking norms of both sides and using the bound
(11.17).
□
INEXACT NEWTON METHODS
Instead of solving (11.9) exactly, inexact Newton methods use search directions pk
that satisfy the condition
∥rk + Jkpk∥≤ηk∥rk∥,
for some ηk ∈[0, η],
(11.18)

1 1 . 1 .
L o c a l A l g o r i t h m s
285
where η ∈[0, 1) is a constant. We refer to ηk as the forcing parameter. Different methods
make different choices of the sequence {ηk}, and they use different algorithms for ﬁnding
the approximate solutions pk. The general framework for this class of methods can be stated
as follows.
Framework 11.2 (Inexact Newton for Nonlinear Equations).
Given η;
Choose x0;
for k  0, 1, 2, . . .
Choose forcing parameter ηk ∈[0, η];
Find a vector pk that satisﬁes (11.18);
xk+1 ←xk + pk;
end (for)
The convergence theory for these methods depends only on the condition (11.18) and
not on the particular technique used to calculate pk. The most important methods in this
class, however, make use of iterative techniques for solving linear systems of the form Jp 
−r, such as GMRES (Saad and Schultz [221], Walker [242]) or other Krylov-space methods.
Like the conjugate-gradient algorithm of Chapter 5 (which is not directly applicable here,
since the coefﬁcient matrix J is not symmetric positive deﬁnite), these methods typically
require us to perform a matrix–vector multiplication of the form Jd for some d at each
iteration, and to store a number of work vectors of length n. GMRES itself requires an
additional vector to be stored at each iteration, and so must be restarted periodically (often
after every 10 or 20 iterations) to keep memory requirements at a reasonable level.
The matrix–vector products Jd can be computed without explicit knowledge of the
Jacobian J. A ﬁnite difference approximation to Jd that requires one evaluation of r(·)
is given by the formula (7.10). Calculation of Jd exactly (at least, to within the limits of
ﬁnite-precision arithmetic) can be performed by using the forward mode of automatic
differentiation, at a cost of at most a small multiple of an evaluation of r(·). Details of this
procedure are given in Section 7.2.
We do not discuss the iterative methods for sparse linear systems here, but refer the
interested reader to Kelley [141] for comprehensive descriptions and implementations of the
most interesting techniques. We prove a local convergence theorem for the method, similar
to Theorem 11.2.
Theorem 11.3.
Suppose that r is continuously differentiable in a convex open set D ⊂IRn. Let x∗∈D
be a nondegenerate solution of r(x)  0, and let {xk} be the sequence of iterates generated by
the Framework 11.2. Then when xk ∈D is sufﬁciently close to x∗, the following are true:
(i) If η in (11.18) is sufﬁciently small, then the convergence of {xk} to x∗is Q-linear.
(ii) If ηk →0, the convergence is Q-superlinear.

286
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
(iii) If, in addition, J(·) is Lipschitz continuous on D and ηk  O(∥rk∥), then the convergence
is Q-quadratic.
Proof.
We ﬁrst rewrite (11.18) as
J(xk)pk  −r(xk) + vk,
where ∥vk∥≤ηk∥r(xk)∥.
(11.19)
Since x∗is a nondegenerate root, we have as in (11.14) that there is a radius δ > 0 such that
∥J(x)−1∥≤β∗for some constant β∗and all x ∈B(x∗, δ). By multiplying both sides of
(11.19) by J(xk)−1 and rearranging, we ﬁnd that
pk + J(xk)−1r(xk)  J(xk)−1vk  β∗ηk∥r(xk)∥.
(11.20)
As in (11.12), we have that
r(x)  J(x)(x −x∗) + w(x),
(11.21)
where ρ(x)
def ∥w(x)∥/∥x −x∗∥→0 as x →x∗. By reducing δ if necessary, we have from
this expression that the following bound holds for all x ∈B(x∗, δ):
∥r(x)∥≤2∥J(x∗)∥∥x −x∗∥+ o(∥x −x∗∥) ≤4∥J(x∗)∥∥x −x∗∥.
(11.22)
We now set x  xk in (11.21), and substitute into (11.22) to obtain
∥xk + pk −x∗∥≤4∥J(x∗)∥β∗ηk∥xk −x∗∥+ ∥J(xk)−1∥∥w(xk)∥
≤(4∥J(x∗)∥β∗ηk + β∗ρ(xk)) ∥xk −x∗∥.
(11.23)
By choosing xk close enough to x∗such that ρ(xk) ≤1/(4β∗), and choosing η 
1/(8∥J(x∗)∥β∗), we have that the term in parentheses in (11.23) is at most 1
2. Hence, since
xk+1  xk + pk, this formula indicates Q-linear convergence of {xk} to x∗, proving part (i).
Part (ii) follows immediately from the fact that the term in brackets in (11.23) goes
to zero as xk →x∗. For part (iii), we combine the techniques above with the logic of the
second part of the proof of Theorem 11.2. Details are left as an exercise.
□
BROYDEN’S METHOD
Secant methods, also known as quasi-Newton methods, do not require calculation of
the Jacobian J(x). Instead, they construct their own approximation to this matrix, updating
it at each iteration so that it mimics the behavior of the true Jacobian J over the step just
taken. The approximate Jacobian is then used in place of J(xk) in a formula like (11.9) to
compute the new search direction.

1 1 . 1 .
L o c a l A l g o r i t h m s
287
To formalize these ideas, let the Jacobian approximation at iteration k be denoted by
Bk. Assuming that Bk is nonsingular, we deﬁne the step to the next iterate by
pk  −B−1
k r(xk),
xk+1  xk + pk.
(11.24)
Let sk and yk denote the differences between successive iterates and successive functions,
respectively:
sk  xk+1 −xk,
yk  r(xk+1) −r(xk).
(Note that pk  sk, but we change the notation because we will soon consider replacing pk
by αpk for some positive step length α > 0.) From Theorem 11.1, we have that sk and yk are
related by the expression
yk 
 1
0
J(xk + tsk)sk dt ≈J(xk+1)sk + o(∥sk∥).
(11.25)
We require the updated Jacobian approximation Bk+1 to satisfy the following equation,
which is known as the secant equation,
yk  Bk+1sk,
(11.26)
which ensures that Bk+1 and J(xk+1) have similar behavior along the direction sk. (Note
the similarity with the secant equation (8.6) in quasi-Newton methods for unconstrained
optimization; the motivation is the same in both cases.) The secant equation does not say
anything about how Bk+1 should behave along directions orthogonal to sk. In fact, we can
view (11.26) as a system of n linear equations in n2 unknowns, where the unknowns are
the components of Bk+1, so for n > 1 the equation (11.26) does not determine all the
components of Bk+1 uniquely. (The scalar case of n  1 gives rise to the scalar secant
method; see (A.32).)
The most successful practical algorithm is Broyden’s method, for which the update
formula is
Bk+1  Bk + (yk −Bksk)sT
k
sT
k sk
.
(11.27)
The Broyden update makes the smallest possible change to the Jacobian (as measured by the
Euclidean norm ∥Bk −Bk+1∥) that is consistent with (11.26), as we show in the following
Lemma.
Lemma 11.4 (Dennis and Schnabel [69, Lemma 8.1.1]).
Among all matrices B satisfying Bsk  yk, the matrix Bk+1 deﬁned by (11.27) minimizes
the difference ∥B −Bk∥.

288
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
Proof.
Let B be any matrix that satisﬁes Bsk  yk. By the properties of the Euclidean norm
(see (A.41)) and the fact that ∥ssT /sT s∥ 1 for any vector s (see the exercises), we have
∥Bk+1 −Bk∥

(yk −Bksk)sT
k
sT
k sk



(B −Bk)sksT
k
sT
k sk
 ≤∥B −Bk∥

sksT
k
sT
k sk
  ∥B −Bk∥.
Hence, we have that
Bk+1 ∈arg
min
B : ykBsk ∥B −Bk∥,
and the result is proved.
□
We summarize the algorithm as follows.
Algorithm 11.3 (Broyden).
Choose x0 and B0;
for k  0, 1, 2, . . .
Calculate a solution pk to the linear equations
Bkpk  −r(xk);
(11.28)
Choose αk by performing a line search along pk;
xk+1 ←xk + αkpk;
sk ←xk+1 −xk;
yk ←r(xk+1) −r(xk);
Obtain Bk+1 from the formula (11.27);
end (for)
Under certain assumptions, Broyden’s method converges superlinearly, that is,
∥xk+1 −x∗∥ o(∥xk −x∗∥).
(11.29)
This local convergence rate is fast enough for most practical purposes, though not as fast as
the Q-quadratic convergence of Newton’s method.
We illustrate the difference between the convergence rates of Newton’s and Broyden’s
method with a small example. The function r : IR2 →IR2 deﬁned by
r(x) 

(x1 + 3)(x3
2 −7) + 18
sin(x2ex1 −1)

(11.30)

1 1 . 1 .
L o c a l A l g o r i t h m s
289
Table 11.1
Convergence of Iterates in Broyden
and Newton Methods
∥xk −x∗∥2
Iteration k
Broyden
Newton
0
0.64 × 100
0.64 × 100
1
0.62 × 10−1
0.62 × 10−1
2
0.52 × 10−3
0.21 × 10−3
3
0.25 × 10−3
0.18 × 10−7
4
0.43 × 10−4
0.12 × 10−15
5
0.14 × 10−6
6
0.57 × 10−9
7
0.18 × 10−11
8
0.87 × 10−15
has a nondegenerate root at x∗ (0, 1)T . We start both methods from the point
x0  (−0.5, 1.4)T , and use the exact Jacobian J(x0) at this point as the initial Jacobian
approximation B0. Results are shown in Table 11.1.
Newton’s method clearly exhibits Q-quadratic convergence, which is characterized by
doubling of the exponent of the error at each iteration. Broyden’s method takes twice as many
iterations as Newton’s, and reduces the error at a rate that accelerates slightly towards the
end. Table 11.2 shows that the ratio of successive errors ∥xk+1 −x∗∥/∥xk −x∗∥in Broyden’s
method does indeed approach zero, despite some dithering on iterations 5 and 6.
The function norms ∥r(xk)∥approach zero at a similar rate to the iteration errors
∥xk −x∗∥. As in (11.16), we have that
r(xk)  r(xk) −r(x∗) ≈J(x∗)(xk −x∗),
so by nonsingularity of J(x∗), the norms of r(xk) and (xk −x∗) are bounded above and
below by multiples of each other. For our example problem (11.30), convergence of the
sequence of function norms in the two methods is shown in Table 11.3.
The convergence analysis of Broyden’s method is more complicated than that of
Newton’s method. We state the following result without proof.
Theorem 11.5.
Suppose the assumptions of Theorem 11.2 hold. Then there are positive constants ϵ and
δ such that if the starting point x0 and the starting approximate Jacobian B0 satisfy
∥x0 −x∗∥≤δ,
∥B0 −J(x∗)∥≤ϵ,
(11.31)
the sequence {xk} generated by Broyden’s method (11.24), (11.27) is well-deﬁned and converges
Q-superlinearly to x∗.
The second condition in (11.31)—that the initial Jacobian approximation B0 must
be close to the true Jacobian at the solution J(x∗)—is difﬁcult to guarantee in practice. In

290
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
Table 11.2
Ratio of Successive Errors in
Broyden’s Method
Iteration k
∥xk+1 −x∗∥2/∥xk −x∗∥2
0
0.097
1
0.0085
2
0.47
3
0.17
4
0.0033
5
0.0041
6
0.0030
7
0.00049
Table 11.3
Convergence of Function Norms in
Broyden and Newton Methods
∥r(xk)∥2
Iteration k
Broyden
Newton
0
0.74 × 101
0.74 × 101
1
0.59 × 100
0.59 × 100
2
0.20 × 10−2
0.23 × 10−2
3
0.21 × 10−2
0.16 × 10−6
4
0.37 × 10−3
0.22 × 10−15
5
0.12 × 10−5
6
0.49 × 10−8
7
0.15 × 10−10
8
0.11 × 10−18
contradistinction to the case of unconstrained minimization, a good choice of B0 can be
critical to the performance of the algorithm. Some implementations of Broyden’s method
recommend choosing B0 to be J(x0), or some ﬁnite difference approximation to this matrix.
The Broyden matrix Bk will be dense in general, even if the true Jacobian J is sparse.
Therefore, when n is large, an implementation of Broyden’s method that stores Bk as a full
n × n matrix may be inefﬁcient. Instead, we can use limited-memory methods in which
Bk is stored implicitly in the form of a number of vectors of length n, while the system
(11.28) is solved by a technique based on application of the Sherman–Morrison–Woodbury
formula (A.56). These methods are similar to the ones described in Chapter 9 for large-scale
unconstrained optimization.
TENSOR METHODS
In tensor methods, the linear model Mk(p) used by Newton’s method is augmented
with an extra term that aims to capture some of the nonlinear, higher-order, behavior of r. By
doing so, it achieves more rapid and reliable convergence to degenerate roots—in particular,
to roots x∗for which the Jacobian J(x∗) has rank n −1 or n −2.

1 1 . 1 .
L o c a l A l g o r i t h m s
291
We give a broad outline of the method here, and refer to the paper of Schnabel and
Frank [224] for details.
We use ˆMk(p) to denote the model function on which tensor methods are based; this
function has the form
ˆMk(p)  r(xk) + J(xk)p + 1
2Tkpp,
(11.32)
where Tk is a tensor deﬁned by n3 elements (Tk)ijl whose action on a pair of arbitrary vectors
u and v in IRn is deﬁned by
(Tkuv)i 
n

j1
n

l1
(Tk)ijlujvl.
If we followed the reasoning behind Newton’s method, we could consider building Tk from
the second derivatives of r at the point xk, that is,
(Tk)ijl  [∇2ri(xk)]jl.
For instance, in the example (11.30), we have that
(T (x)uv)1  uT ∇2r1(x)v  uT

0
3x2
2
3x2
2
6x2(x1 + 3)

v
 3x2
2(u1v2 + u2v1) + 6x2(x1 + 3)u2v2.
However, use of the exact second derivatives is not practical in most instances. If we were to
store this information explicitly, about n3/2 memory locations would be needed: about n
times the requirements of Newton’s method. Moreover, there may be no vector p for which
ˆMk(p)  0, so the step may not even be deﬁned.
Instead, the approach described in [224] deﬁnes Tk in a way that requires little addi-
tional storage, but which gives ˆMk some potentially appealing properties. Speciﬁcally, Tk is
chosen so that ˆMk(p) interpolates the function r(xk + p) at some previous iterates visited
by the algorithm. That is, we require that
ˆMk(xk−j −xk)  r(xk−j),
for j  1, 2, . . . , q,
(11.33)
for some integer q > 0. By substituting from (11.32), we see that Tk must satisfy the
condition
1
2Tksjksjk  r(xk−j) −r(xk) −J(xk)sjk,

292
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
where
sjk
def xk−j −xk and j  1, 2, . . . , q.
In [224] it is shown that this condition can be ensured by choosing Tk so that its action on
arbitrary vectors u and v is
Tkuv 
q

j1
aj(sT
jku)(sT
jkv),
where aj, j  1, 2, . . . , q, are vectors of length n. The number of interpolating points q
is typically chosen to be quite modest, usually less than √n. This Tk can be stored in 2nq
locations, which contain the vectors aj and sjk for j  1, 2, . . . , q. Note the connection
betweenthisideaandBroyden’smethod,whichalsochoosesinformationinthemodel(albeit
in the ﬁrst-order part of the model) to interpolate the function value at the previous iterate.
This technique can be reﬁned in various ways. The points of interpolation can be
chosen to make the collection of directions sjk more linearly independent. There may still
not be a vector p for which ˆMk(p)  0, but we can instead take the step to be the vector that
minimizes ∥ˆMk(p)∥2
2, which can be found by using a specialized least-squares technique.
There is no assurance that the step obtained in this way is a descent direction for the merit
function 1
2∥r(x)∥2 (which is discussed in the next section), and in this case it can be replaced
by the standard Newton direction −J −1
k rk.
Numerical testing by Schnabel and Frank [224] shows that the method generally
requires fewer iterations on standard test problems, while on degenerate problems it
outperforms Newton’s method (which is known to behave poorly in these cases).
11.2
PRACTICAL METHODS
MERIT FUNCTIONS
As mentioned above, neither Newton’s method (11.9) nor Broyden’s method (11.24),
(11.27) with unit step lengths can be guaranteed to converge to a solution of r(x)  0 unless
they are started close to that solution. Sometimes, components of the unknown or function
vector or the Jacobian will blow up. Another, more exotic, kind of behavior is cycling, where
the iterates move between distinct regions of the parameter space without approaching a
root. An example is the scalar function
r(x)  −x5 + x3 + 4x,

1 1 . 2 .
P r a c t i c a l M e t h o d s
293
−3
−2
−1
0
1
2
3
0
1
2
3
4
5
6
Figure 11.2
Plot of 1
2[sin(5x) −x]2, showing its many local minima.
which has ﬁve nondegenerate roots. When started from the point x0  1, Newton’s method
produces a sequence of iterates that oscillates between 1 and −1 (see the exercises) without
converging to any of the roots.
The Newton and Broyden methods can, however, be made more robust by using line
search and trust-region techniques similar to those described in Chapters 3 and 4. Before
doing so, however, we need to deﬁne a merit function, which is a scalar-valued function of
x whose value indicates whether a new candidate iterate is better or worse than the current
iterate, in the sense of making progress toward a root of r. In unconstrained optimization,
the objective function f is itself a natural merit function; algorithms for minimizing f
typically require a decrease in f at each iteration. In nonlinear equations, the merit function
is obtained by combining the n components of the vector r in some way.
The most widely used merit function is the sum of squares, deﬁned by
f (x)  1
2∥r(x)∥2  1
2
n

i1
r2
i (x).
(11.34)
(The factor 1
2 is introduced for convenience.) Any root x∗of r obviously has f (x∗)  0,
and since f (x) ≥0 for all x, each root is at least a local minimizer of f . The converse is not
true—local minimizers of f are not necessarily roots of r—but the merit function (11.34)
has been used successfully in many applications and is implemented in a number of software
packages.

294
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
The merit function for the example (11.3) is plotted in Figure 11.2. It shows three
local minima corresponding to the three roots, but there are many other local minima (for
example, those at around ±1.53053). Local minima like these that are not roots of f satisfy
an interesting property. Since
∇f (x∗)  J(x∗)T r(x∗)  0,
(11.35)
we can have r(x∗) ̸ 0 only if J(x∗) is singular.
Since local minima for the sum-of-squares merit function may be points of attraction
for the algorithms described in this section, global convergence results for the algorithms
discussed here are less satisfactory than for similar algorithms applied to unconstrained
optimization.
Othermeritfunctionshavealsobeenproposed.Onesuchistheℓ1 normmeritfunction
deﬁned by
f1(x)  ∥r(x)∥1 
m

i1
|ri(x)|.
There is a connection here with ℓ1 merit functions for nonlinear programming, which
contain a sum of absolute values of the constraint violations; see Chapter 15.
LINE SEARCH METHODS
We can obtain algorithms with global convergence properties by applying the line
search approach of Chapter 3 to the sum-of-squares merit function f (x) 
1
2∥r(x)∥2.
From any point xk, the search direction pk must be a descent direction for r; that is,
cos θk 
−pT
k ∇f (xk)
∥pk∥∥∇f (xk)∥> 0.
(11.36)
Step lengths αk are chosen by one of the procedures of Chapter 3, and the iterates are deﬁned
by the formula
xk+1  xk + αkpk,
k  0, 1, 2, . . . .
(11.37)
The line search procedures in Section 3.1 can be used to calculate the step αk. For the case
of line searches that choose αk that satisfy the Wolfe conditions (3.6), we have the following
convergence result, which follows directly from Theorem 3.2.
Theorem 11.6.
Suppose that J(x) is Lipschitz continuous in a neighborhood D of the level set L  {x :
f (x) ≤f (x0)}. Suppose that a line search algorithm (11.37) is applied to r, where the search

1 1 . 2 .
P r a c t i c a l M e t h o d s
295
directions pk satisfy (11.36) while the step lengths αk satisfy the Wolfe conditions (3.6). Then
for cos θk deﬁned as in (11.36), we have that the Zoutendijk condition holds, that is,

k≥0
cos2 θk∥J T
k rk∥2 < ∞.
Proof.
Since J(x) is Lipschitz continuous in D (with constant βL deﬁned in (11.4)), we
can deﬁne a constant βR by
βR
def max

sup
x∈D
f (x), sup
x∈D
∥J(x)∥

.
(11.38)
Then for any y and z in D, we have
∥∇f (y) −∇f (z)∥ ∥J(y)T r(y) −J(z)T r(z)∥
≤∥J(y) −J(z)∥∥r(y)∥+ ∥J(z)∥∥r(y) −r(z)∥
≤(βLβR + β2
R)∥y −z∥,
showing that ∇r is Lipschitz continuous on D. Since in addition, f (x) is bounded below
by 0 on D, the conditions of Theorem 3.2 are satisﬁed. The result follows from ∇f (xk) 
J T
k rk.
□
Provided that we ensure that
cos θk ≥δ,
for some δ ∈(0, 1) and all k sufﬁciently large,
(11.39)
Theorem 11.6 guarantees that J T
k rk →0. Moreover, if we know that ∥J(x)−1∥is bounded
on the set D deﬁned in the theorem, then we must have rk →0, so that the iterates xk must
approach a limit point x∗that solves the nonlinear equations problem r(x)  0.
We now investigate the values of cos θk for the directions generated by the Newton
and inexact Newton methods, and describe how the Newton direction can be modiﬁed to
ensure that the lower bound (11.39) is satisﬁed.
Whenitiswell-deﬁned,theNewtonstep(11.9)isadescentdirectionfor f (·)whenever
rk ̸ 0, since
pT
k ∇f (xk)  −pT
k J T
k rk  −∥rk∥2 < 0.
From (11.36), we have
cos θk  −
pT
k ∇f (xk)
∥pk∥∥∇f (xk)∥
∥rk∥2
∥J −1
k rk∥∥J T
k rk∥≥
1
∥J T
k ∥∥J −1
k ∥
1
κ(Jk).
(11.40)

296
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
If the condition number κ(Jk) is uniformly bounded for all k, then cos θk is bounded below
in the style of (11.39). When κ(Jk) is large, however, this lower bound is close to zero, and
use of the Newton direction may cause poor performance of the algorithm.
The importance of the condition (11.39) is illustrated by an example of Powell [196].
In this example, a line search Newton approach with exact line searches converges to a point
that is not a solution and not even a stationary point for the merit function (11.34). Powell’s
function is
r(x) 


x1
10x1
x1 + 0.1 + 2x2
2

,
(11.41)
with unique solution x∗ 0, whose Jacobian is
J(x) 


1
0
1
(x1 + 0.1)2
4x2

.
Note that the Jacobian is singular at all x for which x2  0, and that for points of this type,
we have
∇f (x) 

x1 +
10x1
(x1 + 0.1)3
0

,
so that the gradient points in the direction of the positive x1 axis whenever x1 > 0. Powell
shows, however, that the Newton step generated from an iterate that is close to (but not
quite on) the x1 axis tends to be parallel to the x2 axis, making it nearly orthogonal to the
gradient ∇f (x). That is, cos θk for the Newton direction may be arbitrarily close to zero.
Powell uses the starting point (3, 1)T and shows that convergence to the point (1.8016, 0)T
(to four digits of accuracy) is attained in four iterations. However, this point is not a solution
of (11.41)—in fact, it is easy to see that a step from this point in the direction −x1 will
produce a decrease in both components of r.
To ensure that (11.39) holds, we may have to modify the Newton direction. One
possibility is to add some multiple τkI of the identity to J T
k Jk, and deﬁne the step pk to be
pk  −(J T
k Jk + τkI)−1J T
k rk.
(11.42)
For each iterate k, we choose τk such that the condition (11.39) is satisﬁed, for some given
value of δ ∈(0, 1). Note that we can always choose τk large enough to ensure that this
condition holds, since pk approaches a multiple of −J T
k rk as τk ↑∞. Note that instead of
forming J T
k Jk explicitly and then performing trial Cholesky factorizations of matrices of the
form (J T
k Jk + τI), we can use the technique (10.32), illustrated earlier for the least-squares

1 1 . 2 .
P r a c t i c a l M e t h o d s
297
case, which uses the fact that the Cholesky factor of (J T
k Jk + τI) is identical to RT , where
R is the upper triangular factor from the QR factorization of the matrix

Jk
τ 1/2I

.
(11.43)
A combination of Householder and Givens transformations can be used, as for (10.32), and
the savings noted in the discussion following (10.32) continue to hold if we need to perform
this calculation for several candidate values of τk.
When pk is an inexact Newton direction—that is, one that satisﬁes the condition
(11.18)—it is not hard to derive a lower bound for cos θk in the style of (11.39). Discarding
the subscripts k, we have by squaring (11.18) that
∥r + Jp∥2 ≤η2
k∥r∥2 ⇒2pT J T r + ∥r∥2 + ∥Jp∥2 ≤η2∥r∥2
⇒pT ∇r  pT J T r ≤[(η2 −1)/2]∥r∥2.
Meanwhile,
∥p∥≤∥J −1∥[∥r + Jp∥+ ∥r∥] ≤∥J −1∥(η + 1)∥r∥,
and
∥∇r∥ ∥J T r∥≤∥J∥∥r∥.
By combining these estimates, we obtain
cos θk  −
pT ∇r
∥p∥∥∇r∥≥
1 −η2
2∥J∥∥J −1∥(1 + η) ≥1 −η
2κ(J),
and we conclude that a bound of the form (11.39) is satisﬁed, provided that κ(J) is bounded
over D. In other words, the inexact Newton method satisﬁes the bound (11.39) whenever
Newton’s method does (though for a different value of the constant δ), so by allowing
inexactness we do not compromise global convergence behavior.
A line search method that includes the enhancements mentioned above and that uses
the Newton direction (11.9) and unit step length αk  1 whenever these are acceptable can
be stated formally as follows.
Algorithm 11.4 (Line Search Newton for Nonlinear Equations).
Given δ ∈(0, 1) and c1, c2 with 0 < c2 < c1 < 1
2;
Choose x0;
for k  0, 1, 2, . . .
if Newton step (11.9) satisﬁes (11.39)

298
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
Set pk to the Newton step;
else
Obtain pk from (11.42), choosing τk to ensure that (11.39) holds;
end (if)
if α  1 satisﬁes the Wolfe conditions (3.6)
Set αk  1;
else
Perform a line search to ﬁnd αk > 0 that satisﬁes (3.6);
end (if)
xk+1 ←xk + αkpk;
end (for)
Rapid local convergence of line search methods such as Algorithm 11.4 follows from
a result similar to Theorem 6.2. For generality, we state this result so that it applies to any
algorithm that eventually uses the Newton search direction (11.9) and that accepts the unit
step αk  1 whenever this step satisﬁes the Wolfe conditions.
Theorem 11.7.
Suppose that a line search algorithm that uses Newton search directions pk from (11.9)
yields a sequence {xk} that converges to x∗, where r(x∗)  0 and J(x∗) is nonsingular. Suppose
also that there is an open neighborhood D of x∗such that the components ri, i  1, 2, . . . , n,
are twice differentiable, with ∥∇2ri(x)∥, i  1, 2, . . . , n, bounded for x ∈D. If the unit step
length αk  1 is accepted whenever it satisﬁes the Wolfe conditions (3.6), with c1 < 1
2, then the
convergence is Q-quadratic; that is, ∥xk+1 −x∗∥ O(∥xk −x∗∥2).
We omit the proof of this result, which is similar in spirit to the corresponding result
for trust-region methods, Theorem 11.10. The body of the proof shows that the step length
αk  1 is eventually always acceptable. Hence the method eventually reduces to a pure
Newton method, and the rapid convergence rate follows from Theorem 11.2.
TRUST-REGION METHODS
Themostwidelyusedtrust-regionmethodsfornonlinearequationssimplyapplyAlgo-
rithm 4.1 from Chapter 4 to the merit function f (x)  1
2∥r(x)∥2
2, using Bk  J(xk)T J(xk)
as the approximate Hessian in the model function mk(p). Global convergence properties
follow directly from Theorems 4.7 and 4.8. Rapid local convergence for an algorithm that
computes exact solutions of the trust-region subproblems can be proved under an assump-
tion of Lipschitz continuity of the Jacobian J(x). For background on the motivation and
analysis of the trust-region approach, see Chapter 4.
For the least-squares merit function f (x)  1
2∥r(x)∥2
2, the model function mk(p) is
deﬁned as
mk(p)  1
2∥rk + Jkp∥2
2  fk + pT J T
k rk + 1
2pT J T
k Jkpk;

1 1 . 2 .
P r a c t i c a l M e t h o d s
299
cf. the model function (4.1) for general objective functions. The step pk is generated by
ﬁnding an approximate solution of the subproblem
min
p
mk(p),
subject to ∥p∥≤k,
(11.44)
where k is the radius of the trust region. Given our merit function f (x)  ∥r(x)∥2 and
model function mk, the ratio ρk of actual to predicted reduction (see (4.4)) is deﬁned as
ρk 
∥r(xk)∥2 −∥r(xk + pk)∥2
∥r(xk)∥2 −∥r(xk) + J(xk)pk∥2 .
(11.45)
We can state the trust-region framework that results from this model as follows
Algorithm 11.5 (Trust Region for Nonlinear Equations).
Given ¯ > 0, 0 ∈(0, ¯), and η ∈

0, 1
4

:
for k  0, 1, 2, . . .
Calculate pk as an (approximate) solution of (11.44);
Evaluate ρk from (11.45);
if ρk < 1
4
k+1  1
4∥pk∥
else
if ρk > 3
4 and ∥pk∥ k
k+1  min(2k, ¯)
else
k+1  k;
end (if)
end (if)
if ρk > η
xk+1  xk + pk
else
xk+1  xk;
end (if)
end (for).
The dogleg method is a special case of the trust-region algorithm, Algorithm 4.1,
that constructs an approximate solution to (11.44) based on the Cauchy point pC
k and the
unconstrained minimizer p
J
k. The Cauchy point is
pC
k  −τk(k/∥J T
k rk∥)J T
k rk,
(11.46)
where
τk  min
)
1, ∥J T
k rk∥3/(krT
k Jk(J T
k Jk)J T
k rk)
*
;
(11.47)

300
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
Bycomparingwiththegeneraldeﬁnition(4.7),(4.8)weseethatitisnotnecessarytoconsider
the case of an indeﬁnite Hessian approximation in mk(p), since the model Hessian J T
k Jk
that we use is positive semideﬁnite. The unconstrained minimizer of mk(p) is unique when
Jk has full rank. In this case, we denote it by p
J
k and write
p
J
k  −(J T
k Jk)−1(J T
k rk)  −J −1
k rk.
The selection of pk proceeds as follows.
Procedure 11.6 (Dogleg).
Calculate pC
k;
if ∥pC
k∥ k
pk ←pC
k ;
else
Calculate p
J
k;
pk ←pC
k + τ(p
J
k −pC
k), where τ is the largest value in [0, 1]
such that ∥pk∥≤k
end (if).
Lemma 4.1 shows that when Jk is nonsingular, the vector pk chosen above is the
minimizer of mk along the piecewise linear path that leads from the origin to the Cauchy
point and then to the unconstrained minimizer p
J
k. Hence, the reduction in model function
at least matches the reduction obtained by the Cauchy point, which can be estimated by
specializing the bound (4.34) to the least-squares case by writing
mk(0) −mk(pk) ≥c1∥J T
k rk∥min

k, ∥J T
k rk∥
∥J T
k Jk∥

,
(11.48)
where c1 is some positive constant.
From Theorem 4.3, we know that the exact solution of (11.44) has the form
pk  −(J T
k Jk + λkI)−1J T
k rk,
(11.49)
forsomeλk ≥0,andthat λk  0iftheunconstrainedsolutionp
J
k satisﬁes∥p
J
k∥≤k.(Note
that (11.49) is identical to the formula (10.30a) from Chapter 10. In fact, the Levenberg–
Marquardt approach for nonlinear equations is in a sense a special case of the same algorithm
for nonlinear least-squares problems.) The Levenberg–Marquardt algorithm uses the tech-
niques of Section 4.2 to search for the value of λk that satisﬁes (11.49). The procedure
described in the “exact” trust-region algorithm, Algorithm 4.4, is based on Cholesky factor-
izations, but as in Chapter 10, we can replace these by specialized QR algorithms to compute
the factorization (10.34). Even if the exact λk corresponding to the solution of (11.44) is
not found, the pk calculated from (11.49) will still yield global convergence if it satisﬁes the

1 1 . 2 .
P r a c t i c a l M e t h o d s
301
condition (11.48) for some value of c1, together with
∥pk∥≤γ k,
for some constant γ ≥1.
(11.50)
The dogleg method has the advantage over methods that search for the exact solution
of (11.44) that just one linear system needs to be solved per iteration. As in Chapter 4, there
is a tradeoff to be made between the amount of effort to spend on each iteration and the
total number of function and derivative evaluations required.
We can also consider alternative trust-region approaches that are based on different
merit functions and different deﬁnitions of the trust region. An algorithm based on the ℓ1
merit function with an ℓ∞-norm trust region gives rise to subproblems of the form
min
p
∥Jkp + rk∥1
subject to ∥p∥∞≤,
(11.51)
which can be formulated and solved using linear programming techniques. This approach is
closely related to the Sℓ1QP approach for nonlinear programming discussed in Section 18.8,
for which the subproblem is (18.51).
Global convergence results of Algorithm 11.5 when the steps pk satisfy (11.48) and
(11.50) are given in the following two theorems. They can be proved by referring directly to
Theorems 4.7 and 4.8. The ﬁrst result is for the case of η  0, in which the algorithm accepts
all steps that produce a decrease in the merit function fk.
Theorem 11.8.
Let η  0 in Algorithm 11.5. Suppose that J(x) is continuous in a neighborhood D of
the level set L  {x : f (x) ≤f (x0)} and that ∥J(x)∥is bounded above on L. Suppose in
addition that all approximate solutions of (11.44) satisfy the bounds (11.48) and (11.50). We
then have that
lim inf
k→∞∥J T
k rk∥ 0.
The second result requires a strictly positive choice of η and Lipschitz continuity of
J, and produces a correspondingly stronger result: convergence of the sequence {J T
k rk} to
zero.
Theorem 11.9.
Let η ∈
	
0, 1
4

in Algorithm 11.5. Suppose that J(x) is Lipschitz continuous in a neigh-
borhood D of the level set L  {x : f (x) ≤f (x0)} and that ∥J(x)∥is bounded above on
L. Suppose in addition that all approximate solutions of (11.44) satisfy the bounds (11.48) and
(11.50). We then have that
lim
k→∞∥J T
k rk∥ 0.

302
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
We turn now to local convergence of the trust-region algorithm for the case in which
the subproblem (11.44) is solved exactly. We assume that the sequence {xk} converges to a
nondegeneratesolutionx∗ofthenonlinearequationsr(x)  0.Thesigniﬁcanceofthisresult
is that the algorithmic enhancements needed for global convergence do not, in well-designed
algorithms, interfere with the fast local convergence properties described in Section 11.1.
Theorem 11.10.
Suppose that the sequence {xk} generated by Algorithm 11.5 converges to a nondegenerate
solution x∗of the problem r(x)  0. Suppose also that J(x) is Lipschitz continuous in an open
neighborhood D of x∗and that the trust-region subproblem (11.44) is solved exactly for all
sufﬁciently large k. Then the sequence {xk} converges quadratically to x∗.
Proof.
We prove this result by showing that there is an index K such that the trust-region
radius is not reduced further after iteration K; that is, k ≥K for all k ≥K. We then
show that the algorithm eventually takes the pure Newton step at every iteration, so that
quadratic convergence follows from Theorem 11.2.
Let pk denote the exact solution of (11.44). Note ﬁrst that pk will simply be the
unconstrained Newton step −J −1
k rk whenever this step satisﬁes the trust-region bound.
Otherwise, we have ∥J −1
k rk∥> k, while the solution pk satisﬁes ∥pk∥≤k. In either case,
we have
∥pk∥≤∥J −1
k rk∥.
(11.52)
We consider the ratio ρk of actual to predicted reduction deﬁned by (11.45). We have
directly from the deﬁnition that
|1 −ρk| ≤
∥rk + Jkpk∥2 −∥r(xk + pk)∥2
∥r(xk)∥2 −∥r(xk) + J(xk)pk∥2 .
(11.53)
From Theorem 11.1, we have for the second term in the numerator that
∥r(xk + pk)∥2  ∥[r(xk) + J(xk)pk] + w(xk, pk)∥2 ,
where
w(xk, pk)
def
 1
0
[J(xk + tpk) −J(xk)]pk dt.
Because of Lipschitz continuity of J (with Lipschitz constant βL as in (11.4)), we have
∥w(xk, pk)∥≤
 1
0
∥J(xk + tpk) −J(xk)∥∥pk∥dt

1 1 . 2 .
P r a c t i c a l M e t h o d s
303
≤
 1
0
βL∥pk∥2 dt  (βL/2)∥pk∥2,
so that using the fact that ∥rk + Jkpk∥≤∥rk∥ f (xk)1/2 (since pk is the solution of
(11.44)), we can bound the numerator as follows:
∥rk + Jkpk∥2 −∥r(xk + pk)∥2 ≤2∥rk + Jkpk∥∥w(xk, pk)∥+ ∥w(xk, pk)∥2
≤f (xk)1/2βL∥pk∥2 + (βL/2)2∥pk∥4
≤ϵ(xk)∥pk∥2,
(11.54)
where we deﬁne
ϵ(xk)  f (xk)1/2βL + (βL/2)2∥pk∥2.
Since by assumption, we have xk →x∗, it follows that f (xk) →0 and ∥rk∥→0. Because
x∗is a nondegenerate root, we have as in (11.14) that ∥J(xk)−1∥≤β∗for all k sufﬁciently
large, so from (11.52), we have
∥pk∥≤∥J −1
k rk∥≤β∗∥rk∥→0.
(11.55)
Therefore, we have that
lim
k→∞ϵ(xk) →0.
(11.56)
Turning now to the denominator of (11.53), we deﬁne ¯pk to be a step of the same
length as the solution pk in the Newton direction −J −1
k rk, that is,
¯pk  −
∥pk∥
∥J −1
k rk∥J −1
k rk.
Since ¯pk is feasible for (11.44), and since pk is optimal for this subproblem, we have
∥rk∥2 −∥rk + Jkpk∥2 ≥∥rk∥2 −
rk −
∥pk∥
∥J −1
k rk∥rk

2
 2
∥pk∥
∥J −1
k rk∥∥rk∥2 −
∥pk∥2
∥J −1
k rk∥2 ∥rk∥2
≥
∥pk∥
∥J −1
k rk∥∥rk∥2,

304
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
where for the last inequality we have used (11.52). By using (11.55) again, we have from this
bound that
∥rk∥2 −∥rk + Jkpk∥2 ≥
∥pk∥
∥J −1
k rk∥∥rk∥2 ≥1
β∗∥pk∥∥rk∥.
(11.57)
By substituting (11.54) and (11.57) into (11.53), and then applying (11.55) again, we have
|1 −ρk| ≤β∗ϵ(xk)∥pk∥2
∥pk∥∥rk∥
≤(β∗)2ϵ(xk) →0.
(11.58)
Therefore, for all k sufﬁciently large, we have ρk > 1
4, and so the trust region radius k will
not be increased beyond this point. As claimed, there is an index K such that
k ≥K,
for all k ≥K.
Since ∥J −1
k rk∥≤β∗∥rk∥→0, the Newton step −J −1
k rk will eventually be smaller
than K (and hence k), so it will eventually always be accepted as the solution of (11.44).
The result now follows from Theorem 11.2.
□
We can replace the assumption that xk →x∗with an assumption that the nonde-
generate solution x∗is just one of the limit points of the sequence. (In fact, this condition
implies that xk →x∗; see the exercises.)
11.3
CONTINUATION/HOMOTOPY METHODS
MOTIVATION
We mentioned above that Newton-based methods all suffer from one shortcoming:
Unless J(x) is nonsingular in the region of interest—a condition that often cannot be
guaranteed—they are in danger of converging to a local minimum of the merit function
rather than to a solution of the nonlinear system. Continuation methods, which we outline
in this section, aim explicitly for a solution of r(x)  0 and are more likely to converge to
such a solution in difﬁcult cases. Their underlying motivation is simple to describe: Rather
than dealing with the original problem r(x)  0 directly, we set up an “easy” system of
equations for which the solution is obvious. We then gradually transform the “easy” system
of equations into the original system r(x), and follow the solution as it moves from the
solution of the easy problem to the solution of the original problem.
To be speciﬁc, we deﬁne the homotopy map H(x, λ) as follows:
H(x, λ)  λr(x) + (1 −λ)(x −a),
(11.59)

1 1 . 3 .
C o n t i n u a t i o n / H o m o t o p y M e t h o d s
305
0
λ
1
x
Figure11.3
Plot of a zero path: the trajectory of points (x, λ) for which H(x, λ)  0.
where λ is a scalar parameter and a ∈IRn is a ﬁxed vector. When λ  0, (11.59) deﬁnes the
artiﬁcial, easy problem H(x, 0)  x −a, whose solution is obviously x  a. When λ  1,
we have H(x, 1)  r(x), the original system of equations.
To solve r(x)  0, consider the following algorithm: First, set λ  0 in (11.59) and set
x  a. Then, increase λ from 0 to 1 in small increments, and for each value of λ, calculate
the solution of the system H(x, λ)  0. The ﬁnal value of x corresponding to λ  1 will
solve the original problem r(x)  0.
This naive approach sounds plausible, and Figure 11.3 illustrates a situation in which
it would be successful. In this ﬁgure, there is a unique solution x of the system H(x, λ)  0
for each value of λ in the range [0, 1]. The trajectory of points (x, λ) for which H(x, λ)  0
is called the zero path.
Unfortunately, however, the approach often fails, as illustrated in Figure 11.4. Here,
the algorithm follows the lower branch of the curve from λ  0 to λ  λT , but it then loses
the trail unless it is lucky enough to jump to the top branch of the path. The value λT is
known as a turning point, since at this point we can follow the path smoothly only if we no
longer insist on increasing λ at every step, but rather allow it to decrease where necessary. In
fact, practical continuation methods work by doing exactly as Figure 11.4 suggests, that is,
they follow the zero path explicitly, even if this means allowing λ to decrease from time to
time, and even to roam outside the interval [0, 1].

306
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
λ T
(x,   )λ
.
.
0
λ
1
x
Figure 11.4
A zero path with turning points. The path joins (a, 0) to (x∗, 1), but it
cannot be followed by merely increasing λ monotonically from 0 to 1.
PRACTICAL CONTINUATION METHODS
In one practical technique, we model the zero path by allowing both x and λ to be
functions of an independent variable s that represents arc length along the path. That is,
(x(s), λ(s)) is the point that we arrive at by traveling a distance s along the path from the
initial point (x(0), λ(0))  (a, 0). Because we have that
H(x(s), λ(s))  0,
for all s ≥0,
we can take the total derivative of this expression with respect to s to obtain
∂
∂x H(x, λ)˙x + ∂
∂λH(x, λ)˙λ  0,
where (˙x, ˙λ) 
dx
ds , dλ
ds

.
(11.60)
The vector (˙x(s), ˙λ(s)) is the tangent vector to the zero path, as we illustrate in Figure 11.4.
From (11.60), we see that it lies in the null space of the n × (n + 1) matrix

∂
∂x H(x, λ)
∂
∂λH(x, λ)

.
(11.61)
When this matrix has full rank, its null space has dimension 1, so to complete the deﬁnition
of (˙x, ˙λ) in this case, we need to assign it a length and direction. The length is ﬁxed by

1 1 . 3 .
C o n t i n u a t i o n / H o m o t o p y M e t h o d s
307
imposing the normalization condition
∥˙x(s)∥2 + |˙λ(s)|2  1,
for all s,
(11.62)
which ensures that s is the true arc length along the path from (0, a) to (x(s), λ(s)). We need
to choose the sign to ensure that we keep moving forward along the zero path. A heuristic
that works well is to choose the sign so that the tangent vector (˙x, ˙λ) at the current value of
s makes an angle of less than π/2 with the tangent point at the previous value of s.
We can outline the complete procedure for computing (˙x, ˙λ) as follows:
Procedure 11.7 (Tangent Vector Calculation).
Compute a vector in the null space of (11.61) by performing a QR
factorization with column pivoting,
QT

∂
∂x H(x, λ)
∂
∂λH(x, λ)

 

R
w

,
where Q is n × n orthogonal, R is n × n upper triangular,  is
an (n + 1) × (n + 1) permutation matrix, and w ∈IRn.
Set
v  

R−1w
−1

;
Set (˙x, ˙λ)  ±v/∥v∥2, where the sign is chosen to satisfy the angle
criterion mentioned above.
Details of the QR factorization procedure are given in the Appendix.
Since we can obtain the tangent at any given point (x, λ) and since we know the initial
point (x(0), λ(0))  (a, 0), we can trace the zero path by calling a standard initial-value
ﬁrst-order ordinary differential equation solver, terminating the algorithm when it ﬁnds a
value of s for which λ(s)  1.
A second approach for following the zero path is quite similar to the one just described,
exceptthatittakesanalgebraicviewpointinsteadofadifferential-equationsviewpoint.Given
a current point (x, λ), we compute the tangent vector (˙x, ˙λ) as above, and take a small step
(of length ϵ, say) along this direction to produce a “predictor” point (xP , λP ); that is,
(xP , λP )  (x, λ) + ϵ(˙x, ˙λ).
Usually, this new point will not lie exactly on the zero path, so we apply some “corrector”
iterations to bring it back to the path, thereby identifying a new iterate (x+, λ+) that satisﬁes

308
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
H(x+, λ+)  0. (This process is illustrated in Figure 11.5.) During the corrections, we
choose a component of the predictor step (xP , λP )—one of the components that has been
changing most rapidly during the past few steps—and hold this component ﬁxed during the
correction process. If the index of this component is i, and if we use a pure Newton corrector
process (often adequate, since (xP , λP ) is usually quite close to the target point (x+, λ+)),
the steps will have the form


∂H
∂x
∂H
∂λ
ei



δx
δλ



−H
0

,
where the quantities ∂H/∂x, ∂H/∂λ, and H are evaluated at the latest point of the corrector
process. The last row of this system serves to ﬁx the ith component of (δx, δλ) at zero; the
vector ei ∈IRn+1 is a vector of length n+1 containing all zeros, except for a 1 in the location i
that corresponds to the ﬁxed component. Note that in Figure 11.5 the λ component is chosen
to be ﬁxed on the current iteration. On the following iteration, it may be more appropriate
to choose x as the ﬁxed component, as we reach the turning point in λ.
The two variants on path-following described above are able to follow curves like
those depicted in Figure 11.4 to a solution of the nonlinear system. They rely, however, on
the n × (n + 1) matrix in (11.61) having full rank for all (x, λ) along the path, so that the
(x,   )λ
(x  ,    )
P λP
(x  ,    )
λ+
+
λ
x
Figure11.5
Thealgebraicpredictor–correctorprocedure,usingλastheﬁxedvariable
in the correction process.

1 1 . 3 .
C o n t i n u a t i o n / H o m o t o p y M e t h o d s
309
0
0.2
0.4
0.6
0.8
1
−12
−10
−8
−6
−4
−2
0
2
lambda
x
Figure 11.6
Continuation curves for the example in which H(x, λ)  λ(x2 −1) +
(1 −λ)(x + 2). There is no path from λ  0 to λ  1.
tangent vector is well-deﬁned. Fortunately, it can be shown that full rank is guaranteed under
certain assumptions, as in the following theorem.
Theorem 11.11 (Watson [244]).
Suppose that r is twice continuously differentiable. Then for almost all vectors a ∈IRn,
there is a zero path emanating from (0, a) along which the n × (n + 1) matrix (11.61) has full
rank. If this path is bounded for λ ∈[0, 1), then it has an accumulation point (¯x, 1) such that
r(¯x)  0. Furthermore, if the Jacobian J(¯x) is nonsingular, the zero path between (a, 0) and
(¯x, 1) has ﬁnite arc length.
The theorem assures us that unless we are unfortunate in the choice of a, the algorithms
described above can be applied to obtain a path that either diverges or else leads to a point
¯x that is a solution of the original nonlinear system if J(¯x) is nonsingular. More detailed
convergence results can be found in Watson [244] and the references therein.
Weconcludewithanexampletoshowthatdivergenceofthezeropath(thelessdesirable
outcome of Theorem 11.11) can happen even for innocent-looking problems.

310
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
❏Example 11.2
Consider the system r(x)  x2 −1, for which there are two nondegenerate solutions
+1 and −1. Suppose we choose a  −2 and attempt to apply a continuation method to the
function
H(x, λ)  λ(x2 −1) + (1 −λ)(x + 2)  λx2 + (1 −λ)x + (2 −3λ),
(11.63)
obtained by substituting into (11.59). The zero paths for this function are plotted in Fig-
ure 11.6. As can be seen from that diagram, there is no zero path that joins (−2, 0) to either
(1, 1) or (−1, 1), so the continuation methods fail on this example. We can ﬁnd the values
of λ for which no solution exists by ﬁxing λ in (11.63) and using the formula for a quadratic
root to obtain
x  −(1 −λ) ±

(1 −λ)2 −4λ(2 −3λ)
2λ
.
Now,whentheterminthesquarerootisnegative,thecorrespondingvaluesof x arecomplex,
that is, there are no real roots x. It is easy to verify that this is the case when
λ ∈
#
5 −2
√
3
13
, 5 + 2
√
3
13
$
≈(0.118, 0.651).
Note that the zero path starting from (−2, 0) becomes unbounded, which is one of the
possible outcomes of Theorem 11.11.
❐
This example indicates that continuation methods may fail to produce a solution even
to a fairly simple system of nonlinear equations. However, it is generally true that they are
more reliable than the merit-function methods described earlier in the chapter. The extra
robustness comes at a price, since continuation methods typically require signiﬁcantly more
function and derivative evaluations and linear algebra operations than the merit-function
methods.
NOTES AND REFERENCES
Nonlinear differential equations and integral equations are a rich source of nonlinear
equations. When these problems are discretized, we obtain a variable vector x ∈IRn whose
components can be used to construct an approximation to the function that solves the prob-
lem. In other applications, the vector x is intrinsically ﬁnite-dimensional—it may represent
the quantities of materials to be transported between pairs of cities in a distribution network,

1 1 . 3 .
C o n t i n u a t i o n / H o m o t o p y M e t h o d s
311
for instance. In all cases, the equations ri enforce consistency, conservation, and optimality
principles in the model. Mor´e [168] and Averick et al. [5] discuss a number of interesting
practical applications.
ForanalysisoftheconvergenceofBroyden’smethod,includingproofsofTheorem11.5,
see Dennis and Schnabel [69, Chapter 8] and Kelley [141, Chapter 6]. Details on a limited-
memory implementation of Broyden’s method are given by Kelley [141, Section 7.3].
The trust-region approach (11.51) was proposed by Duff, Nocedal, and Reid [73].
✐
E x e r c i s e s
✐
11.1 Show that for any vector s ∈IRn, we have

ssT
sT s
  1,
where ∥· ∥denotes the Euclidean matrix norm. (Hint: Use (A.37) and (A.38).)
✐
11.2 Consider the function r : IR →IR deﬁned by r(x)  xq, where q is an integer
greater than 2. Note that x∗ 0 is the sole root of this function and that it is degenerate.
Show that Newton’s method converges Q-linearly, and ﬁnd the value of the convergence
ratio r in (2.21).
✐
11.3 Show that Newton’s method applied to the function r(x)  −x5 + x3 + 4x
starting from x0  1 produces the cyclic behavior described in the text. Find the roots of
this function, and check that they are nondegenerate.
✐
11.4 For the scalar function r(x)  sin(5x) −x, show that the sum-of-squares merit
function has inﬁnitely many local minima, and ﬁnd a general formula for such points.
✐
11.5 When r : IRn →IRn, show that the function
φ(λ) 
(J T J + λI)−1J T r

is monotonically decreasing in λ unless J T r

0. (Hint: Use the singular-value
decomposition of J.)
✐
11.6 Show that ∇2f (x) is Lipschitz continuous under the assumptions of Theo-
rem 11.7.
✐
11.7 Prove part (iii) of Theorem 11.3.
✐
11.8 Suppose that Theorem 11.7 is modiﬁed to allow search directions that approach
theNewtondirectiononlyinthelimit,thatis,∥pk−(−J −1
k rk)∥ o(∥J −1
k rk∥).Bymodifying
the proof, show that xk converges superlinearly to x∗.

312
C h a p t e r
1 1 .
N o n l i n e a r E q u a t i o n s
✐
11.9 Consider a line search Newton method in which the step length αk is chosen to
be the exact minimizer of the merit function f (·); that is,
αk  arg min
α
f (xk −αJ −1
k rk).
Show that αk →1 as xk →x∗.
✐
11.10 Let J ∈IRn×m and r ∈IRn and suppose that JJ T r  0. Show that J T r  0.
(Hint: This doesn’t even take one line!)
✐∗11.11 Suppose we replace the assumption of xk →x∗in Theorem 11.10 by an as-
sumption that the nondegenerate solution x∗is a limit point of x∗. By adding some logic to
the proof of this result, show that in fact x∗is the only possible limit point of the sequence.
(Hint: Show that ∥J −1
k+1rk+1∥≤1
2∥J −1
k rk∥for all k sufﬁciently large, and hence that for any
constant ϵ > 0, the sequence {xk} satisﬁes ∥xk −x∗∥≤ϵ for all k sufﬁciently large. Since
ϵ can be made arbitrarily small, we can conclude that x∗is the only possible limit point of
{xk}.)
✐
11.12 Consider the following modiﬁcation of our example of failure of continuation
methods:
r(x)  x2 −1,
a  1
2.
Show that for this example there is a zero path for H(x, λ)  λ(x2 −1 + (1 −λ)(x −a)
that connects ( 1
2, 0) to (1, 0), so that continuation methods should work for this choice of
starting point.

Chapter12

Theory of
Constrained
Optimization
The second part of this book is about minimizing functions subject to constraints on the
variables. A general formulation for these problems is
min
x∈IRn f (x)
subject to

ci(x)  0,
i ∈E,
ci(x) ≥0,
i ∈I,
(12.1)
where f and the functions ci are all smooth, real-valued functions on a subset of IRn, and
I and E are two ﬁnite sets of indices. As before, we call f the objective function, while ci,
i ∈E are the equality constraints and ci, i ∈I are the inequality constraints. We deﬁne the
feasible set  to be the set of points x that satisfy the constraints; that is,
  {x | ci(x)  0, i ∈E; ci(x) ≥0, i ∈I},
(12.2)
so that we can rewrite (12.1) more compactly as
min
x∈ f (x).
(12.3)

316
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
In this chapter we derive mathematical characterizations of the solutions of (12.3).
Recall that for the unconstrained optimization problem of Chapter 2, we characterized
solution points x∗in the following way:
Necessary conditions: Local minima of unconstrained problems have ∇f (x∗)  0
and ∇2f (x∗) positive semideﬁnite.
Sufﬁcient conditions: Any point x∗at which ∇f (x∗)  0 and ∇2f (x∗) is positive
deﬁnite is a strong local minimizer of f .
Our aim in this chapter is to derive similar conditions to characterize the solutions of
constrained optimization problems.
LOCAL AND GLOBAL SOLUTIONS
We have seen already that global solutions are difﬁcult to ﬁnd even when there are no
constraints. The situation may be improved when we add constraints, since the feasible set
might exclude many of the local minima and it may be comparatively easy to pick the global
minimum from those that remain. However, constraints can also make things much more
difﬁcult. As an example, consider the problem
min
x∈IRn ∥x∥2
2,
subject to ∥x∥2
2 ≥1.
Without the constraint, this is a convex quadratic problem with unique minimizer x  0.
When the constraint is added, any vector x with ∥x∥2  1 solves the problem. There are
inﬁnitely many such vectors (hence, inﬁnitely many local minima) whenever n ≥2.
A second example shows how addition of a constraint produces a large number of
local solutions that do not form a connected set. Consider
min (x2 + 100)2 + 0.01x2
1,
subject to x2 −cos x1 ≥0,
(12.4)
illustrated in Figure 12.1. Without the constraint, the problem has the unique solution
(−100, 0). With the constraint there are local solutions near the points
(x1, x2)  (kπ, −1),
for
k  ±1, ±3, ±5, . . . .
Deﬁnitions of the different types of local solutions are simple extensions of the corre-
sponding deﬁnitions for the unconstrained case, except that now we restrict consideration
to the feasible points in the neighborhood of x∗. We have the following deﬁnition.
A vector x∗is a local solution of the problem (12.3) if x∗∈ and there is a
neighborhood N of x∗such that f (x) ≥f (x∗) for x ∈N ∩.
Similarly, we can make the following deﬁnitions:

C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
317
local solutions
constraint
contours of f
Figure 12.1
Constrained problem with many isolated local solutions.
A vector x∗is a strict local solution (also called a strong local solution) if x∗∈ and
there is a neighborhood N of x∗such that f (x) > f (x∗) for all x ∈N ∩ with
x ̸ x∗.
A point x∗is an isolated local solution if x∗∈ and there is a neighborhood N of x∗
such that x∗is the only local minimizer in N ∩.
At times, we replace the word “solution” by “minimizer” in our discussion. This alternative
is frequently used in the literature, but it is slightly less satisfying because it does not account
for the role of the constraints in deﬁning the point in question.
SMOOTHNESS
Smoothness of objective functions and constraints is an important issue in character-
izing solutions, just as in the unconstrained case. It ensures that the objective function and
the constraints all behave in a reasonably predictable way and therefore allows algorithms
to make good choices for search directions.
We saw in Chapter 2 that graphs of nonsmooth functions contain “kinks” or “jumps”
where the smoothness breaks down. If we plot the feasible region for any given constrained
optimization problem, we usually observe many kinks and sharp edges. Does this mean that
the constraint functions that describe these regions are nonsmooth? The answer is often
no, because the nonsmooth boundaries can often be described by a collection of smooth
constraint functions. Figure 12.2 shows a diamond-shaped feasible region in IR2 that could
be described by the single nonsmooth constraint
∥x∥1  |x1| + |x2| ≤1.
(12.5)

318
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Figure 12.2
A feasible region with a nonsmooth boundary can be described by
smooth constraints.
It can also be described by the following set of smooth (in fact, linear) constraints:
x1 + x2 ≤1,
x1 −x2 ≤1,
−x1 + x2 ≤1,
−x1 −x2 ≤1.
(12.6)
Each of the four constraints represents one edge of the feasible polytope. In general, the
constraint functions are chosen so that each one represents a smooth piece of the boundary
of .
Nonsmooth, unconstrained optimization problems can sometimes be reformulated
as smooth constrained problems. An example is given by the unconstrained scalar problem
of minimizing a nonsmooth function f (x) deﬁned by
f (x)  max(x2, x),
(12.7)
which has kinks at x  0 and x  1, and the solution at x∗ 0. We obtain a smooth,
constrained formulation of this problem by adding an artiﬁcial variable t and writing
min t
s.t.
t ≥x,
t ≥x2.
(12.8)
Reformulation techniques such as (12.6) and (12.8) are used often in cases where f is a
maximum of a collection of functions or when f is a 1-norm or ∞-norm of a vector
function.

1 2 . 1 .
E x a m p l e s
319
In the examples above we expressed inequality constraints in a slightly different way
from the form ci(x) ≥0 that appears in the deﬁnition (12.1). However, any collection of
inequality constraints with ≥and ≤and nonzero right-hand-sides can be expressed in the
form ci(x) ≥0 by simple rearrangement of the inequality. In general, it is good practice to
state the constraint in a way that is intuitive and easy to understand.
12.1
EXAMPLES
To introduce the basic principles behind the characterization of solutions of constrained
optimization problems, we work through three simple examples. The ideas discussed here
will be made rigorous in the sections that follow.
We start by noting one item of terminology that recurs throughout the rest of the
book: At a feasible point x, the inequality constraint i ∈I is said to be active if ci(x)  0
and inactive if the strict inequality ci(x) > 0 is satisﬁed.
A SINGLE EQUALITY CONSTRAINT
x1
x2
c1
f
f
f
c1
c1
*
x
f
Figure 12.3
Problem (12.9), showing constraint and function gradients at various
feasible points.

320
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
❏Example 12.1
Our ﬁrst example is a two-variable problem with a single equality constraint:
min x1 + x2
s.t.
x2
1 + x2
2 −2  0
(12.9)
(see Figure 12.3). In the language of (12.1), we have f (x)  x1 + x2, I  ∅, E  {1}, and
c1(x)  x2
1 + x2
2 −2. We can see by inspection that the feasible set for this problem is the
circle of radius
√
2 centered at the origin—just the boundary of this circle, not its interior.
The solution x∗is obviously (−1, −1)T . From any other point on the circle, it is easy to
ﬁnd a way to move that stays feasible (that is, remains on the circle) while decreasing f . For
instance, from the point x  (
√
2, 0)T any move in the clockwise direction around the circle
has the desired effect.
We also see from Figure 12.3 that at the solution x∗, the constraint normal ∇c1(x∗) is
parallel to ∇f (x∗). That is, there is a scalar λ∗
1 such that
∇f (x∗)  λ∗
1∇c1(x∗).
(12.10)
(In this particular case, we have λ∗
1  −1
2.)
❐
We can derive (12.10) by examining ﬁrst-order Taylor series approximations to the
objectiveandconstraintfunctions.Toretainfeasibilitywithrespecttothefunctionc1(x)  0,
we require that c1(x + d)  0; that is,
0  c1(x + d) ≈c1(x) + ∇c1(x)T d  ∇c1(x)T d.
(12.11)
Hence, the direction d retains feasibility with respect to c1, to ﬁrst order, when it satisﬁes
∇c1(x)T d  0.
(12.12)
Similarly, a direction of improvement must produce a decrease in f , so that
0 > f (x + d) −f (x) ≈∇f (x)T d,
or, to ﬁrst order,
∇f (x)T d < 0.
(12.13)
If there exists a direction d that satisﬁes both (12.12) and (12.13), we conclude that improve-
ment on our current point x is possible. It follows that a necessary condition for optimality
for the problem (12.9) is that there exist no direction d satisfying both (12.12) and (12.13).

1 2 . 1 .
E x a m p l e s
321
By drawing a picture, the reader can check that the only way that such a direction cannot
exist is if ∇f (x) and ∇c1(x) are parallel, that is, if the condition ∇f (x)  λ1∇c1(x) holds
at x, for some scalar λ1. If this condition is not satisﬁed, the direction deﬁned by
d  −

I −∇c1(x)∇c1(x)T
∥∇c1(x)∥2

∇f (x)
(12.14)
satisﬁes both conditions (12.12) and (12.13) (see the exercises).
By introducing the Lagrangian function
L(x, λ1)  f (x) −λ1c1(x),
(12.15)
and noting that ∇xL(x, λ1)  ∇f (x) −λ1∇c1(x), we can state the condition (12.10)
equivalently as follows: At the solution x∗, there is a scalar λ∗
1 such that
∇xL(x∗, λ∗
1)  0.
(12.16)
This observation suggests that we can search for solutions of the equality-constrained prob-
lem (12.9) by searching for stationary points of the Lagrangian function. The scalar quantity
λ1 in (12.15) is called a Lagrange multiplier for the constraint c1(x)  0.
Though the condition (12.10) (equivalently, (12.16)) appears to be necessary for an
optimal solution of the problem (12.9), it is clearly not sufﬁcient. For instance, in Exam-
ple 12.1, (12.10) is satisﬁed at the point x  (1, 1) (with λ1  1
2), but this point is obviously
not a solution—in fact, it maximizes the function f on the circle. Moreover, in the case of
equality-constrained problems, we cannot turn the condition (12.10) into a sufﬁcient con-
dition simply by placing some restriction on the sign of λ1. To see this, consider replacing
the constraint x2
1 +x2
2 −2  0 by its negative 2−x2
1 −x2
2  0 in Example 12.1. The solution
of the problem is not affected, but the value of λ∗
1 that satisﬁes the condition (12.10) changes
from λ∗
1  −1
2 to λ∗
1  1
2.
A SINGLE INEQUALITY CONSTRAINT
❏Example 12.2
This is a slight modiﬁcation of Example 12.1, in which the equality constraint is
replaced by an inequality. Consider
min x1 + x2
s.t.
2 −x2
1 −x2
2 ≥0,
(12.17)
for which the feasible region consists of the circle of problem (12.9) and its interior (see
Figure 12.4). Note that the constraint normal ∇c1 points toward the interior of the feasible

322
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
regionateachpointontheboundaryofthecircle.Byinspection,weseethatthesolutionisstill
(−1, −1)andthatthecondition(12.10)holdsforthevalueλ∗
1  1
2.However,thisinequality-
constrained problem differs from the equality-constrained problem (12.9) of Example 12.1
in that the sign of the Lagrange multiplier plays a signiﬁcant role, as we now argue.
❐
As before, we conjecture that a given feasible point x is not optimal if we can ﬁnd a
step d that both retains feasibility and decreases the objective function f to ﬁrst order. The
main difference between problems (12.9) and (12.17) comes in the handling of the feasibility
condition. As in (12.13), the direction d improves the objective function, to ﬁrst order, if
∇f (x)T d < 0. Meanwhile, the direction d retains feasibility if
0 ≤c1(x + d) ≈c1(x) + ∇c1(x)T d,
so, to ﬁrst order, feasibility is retained if
c1(x) + ∇c1(x)T d ≥0.
(12.18)
In determining whether a direction d exists that satisﬁes both (12.13) and (12.18), we
consider the following two cases, which are illustrated in Figure 12.4.
Case I: Consider ﬁrst the case in which x lies strictly inside the circle, so that the strict
inequality c1(x) > 0 holds. In this case, any vector d satisﬁes the condition (12.18), provided
only that its length is sufﬁciently small. In particular, whenever ∇f (x∗) ̸ 0, we can obtain
a direction d that satisﬁes both (12.13) and (12.18) by setting
d  −c1(x) ∇f (x)
∥∇f (x)∥.
The only situation in which such a direction fails to exist is when
∇f (x)  0.
(12.19)
CaseII:Considernowthecaseinwhichx liesontheboundaryofthecircle,sothatc1(x)  0.
The conditions (12.13) and (12.18) therefore become
∇f (x)T d < 0,
∇c1(x)T d ≥0.
The ﬁrst of these conditions deﬁnes an open half-space, while the second deﬁnes a closed
half-space, as illustrated in Figure 12.5. It is clear from this ﬁgure that the two regions fail to
intersect only when ∇f (x) and ∇c1(x) point in the same direction, that is, when
∇f (x)  λ1∇c1(x),
for some λ1 ≥0.
(12.20)

1 2 . 1 .
E x a m p l e s
323
c1
f
f
d
d
Figure 12.4
Improvement directions from two feasible points for the problem
(12.17) at which the constraint is active and inactive, respectively.
Note that the sign of the multiplier is signiﬁcant here. If (12.10) were satisﬁed with a negative
value of λ1, then ∇f (x) and ∇c1(x) would point in opposite directions, and we see from
Figure 12.5 that the set of directions that satisfy both (12.13) and (12.18) would make up an
entire open half-plane.
The optimality conditions for both cases I and II can again be summarized neatly with
reference to the Lagrangian function. When no ﬁrst-order feasible descent direction exists
at some point x∗, we have that
∇xL(x∗, λ∗
1)  0,
for some λ∗
1 ≥0,
(12.21)
where we also require that
λ∗
1c1(x∗)  0.
(12.22)
This condition is known as a complementarity condition; it implies that the Lagrange multi-
plier λ1 can be strictly positive only when the corresponding constraint c1 is active. Conditions
of this type play a central role in constrained optimization, as we see in the sections that
follow. In case I, we have that c1(x∗) > 0, so (12.22) requires that λ∗
1  0. Hence, (12.21)

324
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Any d
direction, to first order
f
c1
in this cone is a good search
Figure12.5
Adirectiond thatsatisﬁesboth(12.13)and(12.18)liesintheintersection
of a closed half-plane and an open half-plane.
reduces to ∇f (x∗)  0, as required by (12.19). In case II, (12.22) allows λ∗
1 to take on a
nonnegative value, so (12.21) becomes equivalent to (12.20).
TWO INEQUALITY CONSTRAINTS
❏Example 12.3
Suppose we add an extra constraint to the problem (12.17) to obtain
min x1 + x2
s.t.
2 −x2
1 −x2
2 ≥0, x2 ≥0,
(12.23)
for which the feasible region is the half-disk illustrated in Figure 12.6. It is easy to see that
the solution lies at (−
√
2, 0)T , a point at which both constraints are active. By repeating the
arguments for the previous examples, we conclude that a direction d is a feasible descent

1 2 . 1 .
E x a m p l e s
325
direction, to ﬁrst order, if it satisﬁes the following conditions:
∇ci(x)T d ≥0,
i ∈I  {1, 2},
∇f (x)T d < 0.
(12.24)
However, it is clear from Figure 12.6 that no such direction can exist when x  (−
√
2, 0)T .
The conditions ∇ci(x)T d ≥0, i  1, 2, are both satisﬁed only if d lies in the quadrant
deﬁned by ∇c1(x) and ∇c2(x), but it is clear by inspection that all vectors d in this quadrant
satisfy ∇f (x)T d ≥0.
Let us see how the Lagrangian and its derivatives behave for the problem (12.23) and
the solution point (−
√
2, 0)T . First, we include an additional term λici(x) in the Lagrangian
for each additional constraint, so we have
L(x, λ)  f (x) −λ1c1(x) −λ2c2(x),
whereλ  (λ1, λ2)T isthevectorofLagrangemultipliers.Theextensionofcondition(12.21)
to this case is
∇xL(x∗, λ∗)  0,
for some λ∗≥0,
(12.25)
where the inequality λ∗≥0 means that all components of λ∗are required to be nonnegative.
By applying the complementarity condition (12.22) to both inequality constraints, we obtain
λ∗
1c1(x∗)  0,
λ∗
2c2(x∗)  0.
(12.26)
When x∗ (−
√
2, 0)T , we have
∇f (x∗) 

1
1

,
∇c1(x∗) 

2
√
2
0

,
∇c2(x∗) 

0
1

,
so that it is easy to verify that ∇xL(x∗, λ∗)  0 when we select λ∗as follows:
λ∗

1/(2
√
2)
1

.
Note that both components of λ∗are positive.
We consider now some other feasible points that are not solutions of (12.23), and
examine the properties of the Lagrangian and its gradient at these points.
For the point x  (
√
2, 0)T , we again have that both constraints are active. How-
ever, the objective gradient ∇f (x) no longer lies in the quadrant deﬁned by the conditions
∇ci(x)T d ≥0, i  1, 2 (see Figure 12.7). One ﬁrst-order feasible descent direction from
this point—a vector d that satisﬁes (12.24)—is simply d  (−1, 0)T ; there are many others
(see the exercises). For this value of x it is easy to verify that the condition ∇xL(x, λ)  0

326
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
c2
c
f
1
Figure 12.6
Problem (12.23), illustrating the
gradients of the active constraints
and objective at the solution.
2
f
c1
c
Figure 12.7
Problem (12.23),
illustrating the gradients
of the active constraints
and objective at a
nonoptimal point.
is satisﬁed only when λ  (−1/(2
√
2), 1). Note that the ﬁrst component λ1 is negative, so
that the conditions (12.25) are not satisﬁed at this point.
Finally, let us consider the point x  (1, 0)T , at which only the second constraint
c2 is active. At this point, linearization of f and c as in Example 12.2 gives the following
conditions, which must be satisﬁed for d to be a feasible descent direction, to ﬁrst order:
1 + ∇c1(x)T d ≥0,
∇c2(x)T d ≥0,
∇f (x)T d < 0.
(12.27)

1 2 . 2 .
F i r s t - O r d e r O p t i m a l i t y C o n d i t i o n s
327
In fact, we need worry only about satisfying the second and third conditions, since we can
always satisfy the ﬁrst condition by multiplying d by a sufﬁciently small positive quantity.
By noting that
∇f (x) 

1
1

,
∇c2(x) 

0
1

,
it is easy to verify that the vector d 
	
−1
2, 1
4

satisﬁes (12.27) and is therefore a descent
direction.
To show that optimality conditions (12.25) and (12.26) fail, we note ﬁrst from (12.26)
that since c1(x) > 0, we must have λ1  0. Therefore, in trying to satisfy ∇xL(x, λ)  0,
we are left to search for a value λ2 such that ∇f (x) −λ2∇c2(x)  0. No such λ2 exists, and
thus this point fails to satisfy the optimality conditions.
❐
12.2
FIRST-ORDER OPTIMALITY CONDITIONS
STATEMENT OF FIRST-ORDER NECESSARY CONDITIONS
The three examples above suggest that a number of conditions are important in the
characterization of solutions for (12.1). These include the relation ∇xL(x, λ)  0, the
nonnegativity of λi for all inequality constraints ci(x), and the complementarity condition
λici(x)  0 that is required for all the inequality constraints. We now generalize the obser-
vations made in these examples and state the ﬁrst-order optimality conditions in a rigorous
fashion.
In general, the Lagrangian for the constrained optimization problem (12.1) is deﬁned
as
L(x, λ)  f (x) −

i∈E∪I
λici(x).
(12.28)
The active set A(x) at any feasible x is the union of the set E with the indices of the active
inequality constraints; that is,
A(x)  E ∪{i ∈I | ci(x)  0}.
(12.29)
Next, we need to give more attention to the properties of the constraint gradients.
The vector ∇ci(x) is often called the normal to the constraint ci at the point x, because it is
usually a vector that is perpendicular to the contours of the constraint ci at x, and in the case
of an inequality constraint, it points toward the feasible side of this constraint. It is possible,
however, that ∇ci(x) vanishes due to the algebraic representation of ci, so that the term

328
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
λi∇ci(x) vanishes for all values of λi and does not play a role in the Lagrangian gradient
∇xL. For instance, if we replaced the constraint in (12.9) by the equivalent condition
c1(x) 
	
x2
1 + x2
2 −2

2  0,
we would have that ∇c1(x)  0 for all feasible points x, and in particular that the condition
∇f (x)  λ1∇c1(x) no longer holds at the optimal point (−1, −1)T . We usually make an
assumption called a constraint qualiﬁcation to ensure that such degenerate behavior does
not occur at the value of x in question. One such constraint qualiﬁcation—probably the one
most often used in the design of algorithms—is the one deﬁned as follows:
Deﬁnition 12.1 (LICQ).
Given the point x∗and the active set A(x∗) deﬁned by (12.29), we say that the linear
independence constraint qualiﬁcation (LICQ) holds if the set of active constraint gradients
{∇ci(x∗), i ∈A(x∗)} is linearly independent.
Note that if this condition holds, none of the active constraint gradients can be zero.
This condition allows us to state the following optimality conditions for a general
nonlinear programming problem (12.1). These conditions provide the foundation for many
of the algorithms described in the remaining chapters of the book. They are called ﬁrst-order
conditions because they concern themselves with properties of the gradients (ﬁrst-derivative
vectors) of the objective and constraint functions.
Theorem 12.1 (First-Order Necessary Conditions).
Suppose that x∗is a local solution of (12.1) and that the LICQ holds at x∗. Then there
is a Lagrange multiplier vector λ∗, with components λ∗
i , i ∈E ∪I, such that the following
conditions are satisﬁed at (x∗, λ∗)
∇xL(x∗, λ∗)  0,
(12.30a)
ci(x∗)  0,
for all i ∈E,
(12.30b)
ci(x∗) ≥0,
for all i ∈I,
(12.30c)
λ∗
i ≥0,
for all i ∈I,
(12.30d)
λ∗
i ci(x∗)  0,
for all i ∈E ∪I.
(12.30e)
The conditions (12.30) are often known as the Karush–Kuhn–Tucker conditions, or
KKT conditions for short. Because the complementarity condition implies that the Lagrange
multipliers corresponding to inactive inequality constraints are zero, we can omit the terms
for indices i /∈A(x∗) from (12.30a) and rewrite this condition as
0  ∇xL(x∗, λ∗)  ∇f (x∗) −

i∈A(x∗)
λ∗
i ∇ci(x∗).
(12.31)
A special case of complementarity is important and deserves its own deﬁnition:

1 2 . 2 .
F i r s t - O r d e r O p t i m a l i t y C o n d i t i o n s
329
Deﬁnition 12.2 (Strict Complementarity).
Given a local solution x∗of (12.1) and a vector λ∗satisfying (12.30), we say that the strict
complementarity condition holds if exactly one of λ∗
i and ci(x∗) is zero for each index i ∈I.
In other words, we have that λ∗
i > 0 for each i ∈I ∩A(x∗).
For a given problem (12.1) and solution point x∗, there may be many vectors λ∗for which
the conditions (12.30) are satisﬁed. When the LICQ holds, however, the optimal λ∗is unique
(see the exercises).
The proof of Theorem 12.1 is quite complex, but it is important to our understanding
of constrained optimization, so we present it in the next section. First, we illustrate the KKT
conditions with another example.
❏Example 12.4
Consider the feasible region illustrated in Figure 12.2 and described by the four con-
straints (12.6). By restating the constraints in the standard form of (12.1) and including an
objective function, the problem becomes
min
x

x1 −3
2
2
+

x2 −1
8
4
s.t.


1 −x1 −x2
1 −x1 + x2
1 + x1 −x2
1 + x1 + x2


≥0.
(12.32)
x1
x2
Figure 12.8
Inequality-constrained problem (12.32) with solution at (1, 0)T .

330
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
It is fairly clear from Figure 12.8 that the solution is x∗ (1, 0). The ﬁrst and second
constraints in (12.32) are active at this point. Denoting them by c1 and c2 (and the inactive
constraints by c3 and c4), we have
∇f (x∗) 


−1
−1
2

,
∇c1(x∗) 

−1
−1

,
∇c2(x∗) 

−1
1

.
Therefore, the KKT conditions (12.30a)-(12.30e) are satisﬁed when we set
λ∗
	 3
4, 1
4, 0, 0

T .
❐
SENSITIVITY
The convenience of using Lagrange multipliers should now be clear, but what of their
intuitive signiﬁcance? The value of each Lagrange multiplier λ∗
i tells us something about
the sensitivity of the optimal objective value f (x∗) to the presence of constraint ci. To put
it another way, λ∗
i indicates how hard f is “pushing” or “pulling” against the particular
constraint ci. We illustrate this point with a little analysis. When we choose an inactive
constraint i /∈A(x∗) such that ci(x∗) > 0, the solution x∗and function value f (x∗) are
quiteindifferenttowhetherthisconstraintispresentornot.Ifweperturbci byatinyamount,
it will still be inactive and x∗will still be a local solution of the optimization problem. Since
λ∗
i  0 from (12.30e), the Lagrange multiplier indicates accurately that constraint i is not
signiﬁcant.
Supposeinsteadthatconstrainti isactive,andletusperturbtheright-hand-sideofthis
constraint a little, requiring, say, that ci(x) ≥−ϵ∥∇ci(x∗)∥instead of ci(x) ≥0. Suppose
that ϵ is sufﬁciently small that the perturbed solution x∗(ϵ) still has the same set of active
constraints, and that the Lagrange multipliers are not much affected by the perturbation.
(These conditions can be made more rigorous with the help of strict complementarity and
second-order conditions, as discussed later in the chapter.) We then ﬁnd that
−ϵ∥∇ci(x∗)∥ ci(x∗(ϵ)) −ci(x∗) ≈(x∗(ϵ) −x∗)T ∇ci(x∗),
0  cj(x∗(ϵ)) −cj(x∗) ≈(x∗(ϵ) −x∗)T ∇cj(x∗),
for all j ∈A(x∗) with j ̸ i.
The value of f (x∗(ϵ)), meanwhile, can be estimated with the help of (12.30a). We have
f (x∗(ϵ)) −f (x∗) ≈(x∗(ϵ) −x∗)T ∇f (x∗)


j∈A(x∗)
λ∗
j(x∗(ϵ) −x∗)T ∇cj(x∗)
≈−ϵ∥∇ci(x∗)∥λ∗
i .

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
331
By taking limits, we see that the family of solutions x∗(ϵ) satisﬁes
df (x∗(ϵ))
dϵ
 −λ∗
i ∥∇ci(x∗)∥.
(12.33)
A sensitivity analysis of this problem would conclude that if λ∗
i ∥∇ci(x∗)∥is large, then the
optimal value is sensitive to the placement of the ith constraint, while if this quantity is
small, the dependence is not too strong. If λ∗
i is exactly zero for some active constraint, small
perturbations to ci in some directions will hardly affect the optimal objective value at all;
the change is zero, to ﬁrst order.
This discussion motivates the deﬁnition below, which classiﬁes constraints according
to whether or not their corresponding Lagrange multiplier is zero.
Deﬁnition 12.3.
Let x∗be a solution of the problem (12.1), and suppose that the KKT conditions (12.30)
are satisﬁed. We say that an inequality constraint ci is strongly active or binding if i ∈A(x∗)
and λ∗
i > 0 for some Lagrange multiplier λ∗satisfying (12.30). We say that ci is weakly active
if i ∈A(x∗) and λ∗
i  0 for all λ∗satisfying (12.30).
Note that the analysis above is independent of scaling of the individual constraints. For
instance,wemightchangetheformulationoftheproblembyreplacingsomeactiveconstraint
ci by 10ci. The new problem will actually be equivalent (that is, it has the same feasible set
and same solution), but the optimal multiplier λ∗
i corresponding to ci will be replaced by
λ∗
i /10. However, since ∥∇ci(x∗)∥is replaced by 10∥∇ci(x∗)∥, the product λ∗
i ∥∇ci(x∗)∥does
not change. If, on the other hand, we replace the objective function f by 10f , the multipliers
λ∗
i in (12.30) all will need to be replaced by 10λ∗
i . Hence in (12.33) we see that the sensitivity
of f to perturbations has increased by a factor of 10, which is exactly what we would expect.
12.3
DERIVATION OF THE FIRST-ORDER CONDITIONS
Having studied some motivating examples, observed the characteristics of optimal and
nonoptimalpoints,andstatedtheKKTconditions,wenowdescribeacompleteproofofThe-
orem 12.1. This analysis is not just of esoteric interest, but is rather the key to understanding
all constrained optimization algorithms.
FEASIBLE SEQUENCES
The ﬁrst concept we introduce is that of a feasible sequence. Given a feasible point x∗,
a sequence {zk}∞
k0 with zk ∈IRn is a feasible sequence if the following properties hold:
(i) zk ̸ x∗for all k;
(ii) limk→∞zk  x∗;

332
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
(iii) zk is feasible for all sufﬁciently large values of k.
For later reference, we denote the set of all possible feasible sequences approaching x by
T (x).
We characterize a local solution of (12.1) as a point x at which all feasible sequences
have the property that f (zk) ≥f (x) for all k sufﬁciently large. We derive practical, veriﬁable
conditions under which this property holds. To do so we will make use of the concept of a
limiting direction of a feasible sequence.
Limiting directions of a feasible sequence are vectors d such that we have
lim
zk∈Sd
zk −x
∥zk −x∥→d,
(12.34)
where Sd is some subsequence of {zk}∞
k0. In general, a feasible sequence has at least one
limiting direction and may have more than one. To see this, note that the sequence of vectors
deﬁned by
dk 
zk −x
∥zk −x∥
lies on the surface of the unit sphere, which is a compact set, and thus there is at least one
limit point d. Moreover, all such points are limiting directions by the deﬁnition (12.34). If
we have some sequence {zk} with limiting direction d and corresponding subsequence Sd,
we can construct another feasible sequence {¯zk} such that
lim
k→∞
¯zk −x
∥¯zk −x∥ d
(that is, with a unique limit point) by simply deﬁning each ¯zk to be an element from the
subsequence Sd.
We illustrate these concepts by revisiting Example 12.1
❏Example 12.5
(Example 12.1, Revisited)
Figure 12.9 shows a closeup of the problem (12.9), the equality-constrained problem
in which the feasible set is a circle of radius
√
2, near the nonoptimal point x  (−
√
2, 0)T .
The ﬁgure also shows a feasible sequence approaching x. This sequence could be deﬁned
analytically by the formula
zk 

−

2 −1/k2
−1/k

.
(12.35)

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
333
zk
c1
f
_
limiting direction d
feasible approach sequence 
Figure 12.9
Constraint normal, objective gradient, and feasible sequence for
problem (12.9).
The vector d  (0, −1) plotted in Figure 12.9 is a limiting direction of this feasible sequence.
Note that d is tangent to the feasible sequence at x but points in the opposite direction. The
objective function f (x)  x1 + x2 increases as we move along the sequence (12.35); in fact,
wehavef (zk+1) > f (zk)forallk  2, 3, . . .(seetheexercises).Itfollowsthatf (zk) < f (x)
for k  2, 3, . . .. Hence, x cannot be a solution of (12.9).
Another feasible sequence is one that approaches x∗ (−
√
2, 0) from the opposite
direction. Its elements are deﬁned by
zk 

−

2 −1/k2
1/k

.
It is easy to show that f decreases along this sequence and that its limiting direction is
d  (0, 1)T . Other feasible sequences are obtained by combining elements from the two
sequences already discussed, for instance
zk 

(−

2 −1/k2, 1/k),
when k is a multiple of 3,
(−

2 −1/k2, −1/k),
otherwise.
In general, feasible sequences of points approaching (−
√
2, 0)T will have two limiting
directions, (0, 1)T and (0, −1)T .
❐

334
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
x2
1
x
Figure 12.10
Feasible sequences converging to a
particular feasible point for the region
deﬁned by x2
1 + x2
2 ≤2.
Wenowconsiderfeasiblesequencesandlimitingdirectionsforanexamplethatinvolves
inequality constraints.
❏Example 12.6
(Example 12.2, Revisited)
We now reconsider problem (12.17) in Example 12.2. The solution x∗ (−1, −1)T
is the same as in the equality-constrained case, but there is a much more extensive collection
of feasible sequences that converge to any given feasible point (see Figure 12.10). From
the point x  (−
√
2, 0)T , the various feasible sequences deﬁned above for the equality-
constrained problem are still feasible for (12.17). There are also inﬁnitely many feasible
sequences that converge to x  (−
√
2, 0)T along a straight line from the interior of the
circle. These are deﬁned by
zk  (−1, 0)T + (1/k)w,
(12.36)
where w is any vector whose ﬁrst component is positive (w1 > 0). Now, zk is feasible in
(12.36), provided that ∥zk∥≤1, that is,
(−1 + w1/k)2 + (w2/k)2 ≤1,
a condition that is satisﬁed, provided that k > (2w1)/(w2
1 + w2
2). In addition to these
straight-line feasible sequences, we can also deﬁne an inﬁnite variety of sequences that

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
335
approach (−
√
2, 0) along a curve from the interior of the circle or that make the approach
in a seemingly random fashion.
❐
Given a point x, if it is possible to choose a feasible sequence from T (x) such that the
ﬁrst-order approximation to the objective function actually increases monotonically along
the sequence, then x must not be optimal. This condition is the fundamental ﬁrst-order
necessary condition, and we state it formally in the following theorem.
Theorem 12.2.
If x∗is a local solution of (12.1), then all feasible sequences {zk} in T (x∗) must satisfy
∇f (x∗)T d ≥0,
(12.37)
where d is any limiting direction of the feasible sequence.
Proof.
Suppose that there is a feasible sequence {zk} with the property ∇f (x∗)T d < 0,
for some limiting direction d, and let Sd be the subsequence of {zk} that approaches x∗. By
Taylor’s theorem (Theorem 2.1), we have for any zk ∈Sd that
f (zk)  f (x∗) + (zk −x∗)T ∇f (x∗) + o(∥zk −x∗∥)
 f (x∗) + ∥zk −x∗∥dT ∇f (x∗) + o(∥zk −x∗∥).
Since dT ∇f (x∗) < 0, we have that the remainder term is eventually dominated by the
ﬁrst-order term, that is,
f (zk) < f (x∗) + 1
2∥zk −x∗∥dT ∇f (x∗),
for all k sufﬁciently large.
Hence, given any open neighborhood of x∗, we can choose k sufﬁciently large that zk lies
within this neighborhood and has a lower value of the objective f . Therefore, x∗is not a
local solution.
□
This theorem tells us why we can ignore constraints that are strictly inactive (that
is, constraints for which ci(x) > 0) in formulating optimality conditions. The theorem
does not use the whole range of properties of the feasible sequence, but rather one speciﬁc
property: the limiting directions of {zk}. Because of the way in which the limiting directions
are deﬁned, it is clear that only the asymptotic behavior of the sequence is relevant, that is,
its behavior for large values of the index k. If some constraint i ∈I is inactive at x, then
we have ci(zk) > 0 for all k sufﬁciently large, so that a constraint that is inactive at x is also
inactive at all sufﬁciently advanced elements of the feasible sequence {zk}.

336
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
CHARACTERIZING LIMITING DIRECTIONS: CONSTRAINT QUALIFICATIONS
Theorem 12.2 is quite general, but it is not very useful as stated, because it seems to
require knowledge of all possible limiting directions for all feasible sequences T (x∗). In this
section we show that constraint qualiﬁcations allow us to characterize the salient properties
of T (x∗), and therefore make the condition (12.37) easier to verify.
One frequently used constraint qualiﬁcation is the linear independence constrained
qualiﬁcation (LICQ) given in Deﬁnition 12.1. The following lemma shows that when LICQ
holds, there is a neat way to characterize the set of all possible limiting directions d in terms
of the gradients ∇ci(x∗) of the active constraints at x∗.
In subsequent results we introduce the notation A to represent the matrix whose rows
are the active constraint gradients at the optimal point, that is,
∇c∗
i  ∇ci(x∗),
AT  [∇c∗
i ]i∈A(x∗),
∇f ∗ ∇f (x∗),
(12.38)
where the active set A(x∗) is deﬁned as in (12.29).
Lemma 12.3.
The following two statements are true.
(i) If d ∈IRn is a limiting direction of a feasible sequence, then
dT ∇c∗
i  0,
for all i ∈E,
dT ∇c∗
i ≥0,
for all i ∈A(x∗) ∩I.
(12.39)
(ii) If (12.39) holds with ∥d∥ 1 and the LICQ condition is satisﬁed, then d ∈IRn is a
limiting direction of some feasible sequence.
Proof.
Without loss of generality, let us assume that all the constraints ci(·), i 
1, 2, . . . , m, are active. (We can arrive at this convenient ordering by simply dropping all
inactive constraints—which are irrelevant in some neighborhood of x∗—and renumbering
the active constraints that remain.)
To prove (i), let {zk} ∈T (x∗) be some feasible sequence for which d is a limiting
direction, and assume (by taking a subsequence if necessary) that
lim
k→∞
zk −x∗
∥zk −x∗∥ d.
From this deﬁnition, we have that
zk  x∗+ ∥zk −x∗∥d + o(∥zk −x∗∥).

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
337
By taking i ∈E and using Taylor’s theorem, we have that
0 
1
∥zk −x∗∥ci(zk)

1
∥zk −x∗∥

ci(x∗) + ∥zk −x∗∥∇cT
i d + o(∥zk −x∗∥)

 ∇cT
i d + o(∥zk −x∗∥)
∥zk −x∗∥.
By taking the limit as k →∞, the last term in this expression vanishes, and we have
∇cT
i d  0, as required. For the active inequality constraints i ∈A(x∗) ∩I, we have
similarly that
0 ≤
1
∥zk −x∗∥ci(zk)

1
∥zk −x∗∥

ci(x∗) + ∥zk −x∗∥∇cT
i d + o(∥zk −x∗∥)

 ∇cT
i d + o(∥zk −x∗∥)
∥zk −x∗∥.
Hence, by a similar limiting argument, we have that ∇cT
i d ≥0, as required.
For (ii), we use the implicit function theorem (see the Appendix or Lang [147, p. 131]
for a statement of this result). First, since the LICQ holds, we have from Deﬁnition 12.1 that
the m × n matrix A of active constraint gradients has full row rank m. Let Z be a matrix
whose columns are a basis for the null space of A; that is,
Z ∈IRn×(n−m),
Z has full column rank,
AZ  0.
(12.40)
Let d have the properties (12.39), and suppose that {tk}∞
k0 is any sequence of positive scalars
such limk→∞tk  0. Deﬁne the parametrized system of equations R : IRn × IR →IRn by
R(z, t) 

c(z) −tAd
ZT (z −x∗−td)



0
0

.
(12.41)
We claim that for each t  tk, the solutions z  zk of this system for small t > 0 give a
feasible sequence that approaches x∗.
Clearly, for t  0, the solution of (12.41) is z  x∗, and the Jacobian of R at this point
is
∇zR(x∗, 0) 

A
ZT

,
(12.42)

338
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
which is nonsingular by construction of Z. Hence, according to the implicit function the-
orem, the system (12.41) has a unique solution zk for all values of tk sufﬁciently small.
Moreover, we have from (12.41) and (12.39) that
i ∈E ⇒ci(zk)  tk∇cT
i d  0,
(12.43a)
i ∈A(x∗) ∩I ⇒ci(zk)  tk∇cT
i d ≥0,
(12.43b)
so that zk is indeed feasible. Also, for any positive value t  ¯t > 0, we cannot have z(t)  x∗,
since otherwise by substituting (z, t)  (x∗, ¯t) into (12.41), we obtain

c(x∗) −¯tAd
−ZT (¯td)



0
0

.
Since c(x∗)  0 (we have assumed that all constraints are active) and ¯t > 0, we have from
the full rank of the matrix in (12.42) that d  0, which contradicts ∥d∥ 1. It follows that
zk  z(tk) ̸ x∗for all k.
Itremainstoshowthatd isalimitingdirectionof {zk}.UsingthefactthatR(zk, tk)  0
for all k together with Taylor’s theorem, we ﬁnd that
0  R(zk, tk) 

c(zk) −tkAd
ZT (zk −x∗−tkd)



A(zk −x∗) + o(∥zk −x∗∥) −tkAd
ZT (zk −x∗−tkd)



A
ZT

(zk −x∗−tkd) + o(∥zk −x∗∥).
By dividing this expression by ∥zk −x∗∥and using nonsingularity of the coefﬁcient matrix
in the ﬁrst term, we obtain
lim
k→∞dk −
tk
∥zk −x∗∥d  0,
where dk 
zk −x∗
∥zk −x∗∥.
(12.44)
Since ∥dk∥ 1 for all k and since ∥d∥ 1, we must have
lim
k→∞
tk
∥zk −x∗∥ 1.
(12.45)
(We leave the simple proof by contradiction of this statement as an exercise.) Hence, from
(12.44), we have limk→∞dk  d, as required.
□
Thesetofdirectionsdeﬁnedby(12.39)playsacentralroleintheoptimalityconditions,
so for future reference we give this set a name and deﬁne it formally.

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
339
Deﬁnition 12.4.
Given a point x∗and the active constraint set A(x∗) deﬁned by (12.29), the set F1 is
deﬁned by
F1 

αd
 α > 0,
dT ∇c∗
i  0,
for all i ∈E,
dT ∇c∗
i ≥0,
for all i ∈A(x∗) ∩I

.
Note that F1 is a cone. In fact, when a constraint qualiﬁcation is satisﬁed, F1 is the tangent
cone to the feasible set at x∗. (See Section 12.6 and the deﬁnition (A.1) in the Appendix.)
INTRODUCING LAGRANGE MULTIPLIERS
Lemma 12.3 tells us that when the LICQ holds, the cone F1 is simply the set of all
positive multiples of all limiting directions of all possible feasible sequences. Therefore, the
condition (12.37) of Theorem 12.2 holds if ∇f (x∗)T d < 0 for all d ∈F1. This condition,
too, would appear to be impossible to check, since the set F1 contains inﬁnitely many vectors
in general. The next lemma gives an alternative, practical way to check this condition that
makes use of the Lagrange multipliers, the variables λi that were introduced in the deﬁnition
(12.28) of the Lagrangian L.
Lemma 12.4.
There is no direction d ∈F1 for which dT ∇f ∗< 0 if and only if there exists a vector
λ ∈IRm with
∇f ∗

i∈A(x∗)
λi∇c∗
i  A(x∗)T λ,
λi ≥0 for i ∈A(x∗) ∩I.
(12.46)
Proof.
If we deﬁne the cone N by
N 

s
 s 

i∈A(x∗)
λi∇c∗
i ,
λi ≥0 for i ∈A(x∗) ∩I

,
(12.47)
then the condition (12.46) is equivalent to ∇f ∗∈N. We note ﬁrst that the set N is closed—a
fact that is intuitively clear but nontrivial to prove rigorously. (An intriguing proof of this
claim appears in the Notes and References at the end of this chapter.)
We prove the forward implication by supposing that (12.46) holds and choosing d to
be any vector satisfying (12.39). We then have that
dT ∇f ∗

i∈E
λi(dT ∇c∗
i ) +

i∈A(x∗)∩I
λi(dT ∇c∗
i ).
The ﬁrst summation is zero because dT ∇c∗
i  0 for i ∈E, while the second term is
nonnegative because λi ≥0 and dT ∇c∗
i ≥0 for i ∈A(x∗) ∩I. Hence dT ∇f ∗≥0.

340
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
For the reverse implication, we show that if ∇f ∗does not satisfy (12.46) (that is,
∇f ∗/∈N), then we can ﬁnd a vector d for which dT ∇f ∗< 0 and (12.39) holds.
Let ˆs be the vector in N that is closest to ∇f ∗. Because N is closed, ˆs is well-deﬁned.
In fact, ˆs solves the constrained optimization problem
min ∥s −∇f ∗∥2
2
subject to s ∈N.
(12.48)
Since ˆs ∈N, we also have t ˆs ∈N for all scalars t ≥0. Since ∥t ˆs −∇f ∗∥2
2 is minimized at
t  1, we have
d
dt ∥t ˆs −∇f ∗∥2
2

t1
 0 ⇒
	
−2ˆsT ∇f ∗+ 2t ˆsT ˆs


t1  0
⇒ˆsT (ˆs −∇f ∗)  0.
(12.49)
Now, let s be any other vector in N. Since N is convex (check!), we have by the minimizing
property of ˆs that
∥ˆs + θ(s −ˆs) −∇f ∗∥2
2 ≥∥ˆs −∇f ∗∥2
2
for all θ ∈[0, 1],
and hence
2θ(s −ˆs)T (ˆs −∇f ∗) + θ2∥s −ˆs∥2
2 ≥0.
By dividing this expression by θ and taking the limit as θ ↓0, we have (s−ˆs)T (ˆs−∇f ∗) ≥0.
Therefore, because of (12.49),
sT (ˆs −∇f ∗) ≥0,
for all s ∈N.
(12.50)
We claim now that the vector
d  ˆs −∇f ∗
satisﬁes both (12.39) and dT ∇f ∗< 0. Note that d ̸ 0 because ∇f ∗does not belong to the
cone N. We have from (12.49) that
dT ∇f ∗ dT (ˆs −d)  (ˆs −∇f ∗)T ˆs −dT d  −∥d∥2
2 < 0,
so that d satisﬁes the descent property.
By making appropriate choices of coefﬁcients λi, i  1, 2, . . . , m, it is easy to see that
i ∈E ⇒∇c∗
i ∈N
and
−∇c∗
i ∈N;
i ∈A(x∗) ∩I ⇒∇c∗
i ∈N.

1 2 . 3 .
D e r i v a t i o n o f t h e F i r s t - O r d e r C o n d i t i o n s
341
Hence, from (12.50), we have by substituting d  ˆs −∇f ∗and the particular choices
s  ∇c∗
i and s  −∇c∗
i that
i ∈E ⇒dT ∇c∗
i ≥0
and
−dT ∇c∗
i ≥0 ⇒dT ∇c∗
i  0;
i ∈A(x∗) ∩I ⇒dT ∇c∗
i ≥0.
Therefore, d also satisﬁes (12.39), so the reverse implication is proved.
□
PROOF OF THEOREM 12.1
Lemmas 12.3 and 12.4 can be combined to give the KKT conditions described in
Theorem 12.1. Suppose that x∗∈IRn is a feasible point at which the LICQ holds. The
theorem claims that if x∗is a local solution for (12.1), then there is a vector λ∗∈IRm that
satisﬁes the conditions (12.30).
We show ﬁrst that there are multipliers λi, i ∈A(x∗), such that (12.46) is satisﬁed.
Theorem 12.2 tells us that dT ∇f ∗≥0 for all vectors d that are limiting directions of feasible
sequences. From Lemma 12.3, we know that when LICQ holds, the set of all possible limiting
directions is exactly the set of vectors that satisfy the conditions (12.39). By putting these
two statements together, we ﬁnd that all directions d that satisfy (12.39) must also have
dT ∇f ∗≥0. Hence, from Lemma 12.4, we have that there is a vector λ for which (12.46)
holds, as claimed.
We now deﬁne the vector λ∗by
λ∗
i 

λi,
i ∈A(x∗),
0,
otherwise,
(12.51)
and show that this choice of λ∗, together with our local solution x∗, satisﬁes the conditions
(12.30). We check these conditions in turn.
• The condition (12.30a) follows immediately from (12.46) and the deﬁnitions (12.28)
of the Lagrangian function and (12.51) of λ∗.
• Since x∗is feasible, the conditions (12.30b) and (12.30c) are satisﬁed.
• We have from (12.46) that λ∗
i ≥0 for i ∈A(x∗) ∩I, while from (12.51), λ∗
i  0 for
i ∈I\A(x∗). Hence, λ∗
i ≥0 for i ∈I, so that (12.30d) holds.
• We have for i ∈A(x∗) ∩I that ci(x∗)  0, while for i ∈I\A(x∗), we have λ∗
i  0.
Hence λ∗
i ci(x∗)  0 for i ∈I, so that (12.30e) is satisﬁed as well.
This completes the proof.

342
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
12.4
SECOND-ORDER CONDITIONS
So far, we have described the ﬁrst-order conditions—the KKT conditions—which tell us
how the ﬁrst derivatives of f and the active constraints ci are related at x∗. When these
conditions are satisﬁed, a move along any vector w from F1 either increases the ﬁrst-order
approximation to the objective function (that is, wT ∇f (x∗) > 0), or else keeps this value
the same (that is, wT ∇f (x∗)  0).
What implications does optimality have for the second derivatives of f and the con-
straints ci? We see in this section that these derivatives play a “tiebreaking” role. For the
directions w ∈F1 for which wT ∇f (x∗)  0, we cannot determine from ﬁrst derivative
information alone whether a move along this direction will increase or decrease the objec-
tive function f . Second-order conditions examine the second derivative terms in the Taylor
series expansions of f and ci, to see whether this extra information resolves the issue of
increase or decrease in f . Essentially, the second-order conditions concern the curvature of
the Lagrangian function in the “undecided” directions—the directions w ∈F1 for which
wT ∇f (x∗)  0.
Since we are discussing second derivatives, stronger smoothness assumptions are
needed here than in the previous sections. For the purpose of this section, f and ci, i ∈E ∪I,
are all assumed to be twice continuously differentiable.
Given F1 from Deﬁnition 12.4 and some Lagrange multiplier vector λ∗satisfying the
KKT conditions (12.30), we deﬁne a subset F2(λ∗) of F1 by
F2(λ∗)  {w ∈F1 | ∇ci(x∗)T w  0, all i ∈A(x∗) ∩I with λ∗
i > 0}.
Equivalently,
w ∈F2(λ∗) ⇔



∇ci(x∗)T w  0,
for all i ∈E,
∇ci(x∗)T w  0,
for all i ∈A(x∗) ∩I with λ∗
i > 0,
∇ci(x∗)T w ≥0,
for all i ∈A(x∗) ∩I with λ∗
i  0.
(12.52)
The subset F2(λ∗) contains the directions w that tend to “adhere” to the active inequality
constraints for which the Lagrange multiplier component λ∗
i is positive, as well as to the
equality constraints. From the deﬁnition (12.52) and the fact that λ∗
i  0 for all inactive
components i ∈I\A(x∗), it follows immediately that
w ∈F2(λ∗)
⇒
λ∗
i ∇ci(x∗)T w  0 for all i ∈E ∪I.
(12.53)
Hence, from the ﬁrst KKT condition (12.30a) and the deﬁnition (12.28) of the Lagrangian
function, we have that
w ∈F2(λ∗)
⇒
wT ∇f (x∗) 

i∈E∪I
λ∗
i wT ∇ci(x∗)  0.
(12.54)

1 2 . 4 .
S e c o n d - O r d e r C o n d i t i o n s
343
Hence the set F2(λ∗) contains directions from F1 for which it is not clear from ﬁrst derivative
information alone whether f will increase or decrease.
The ﬁrst theorem deﬁnes a necessary condition involving the second derivatives: If x∗
is a local solution, then the curvature of the Lagrangian along directions in F2(λ∗) must be
nonnegative.
Theorem 12.5 (Second-Order Necessary Conditions).
Suppose that x∗is a local solution of (12.1) and that the LICQ condition is satisﬁed. Let
λ∗be a Lagrange multiplier vector such that the KKT conditions (12.30) are satisﬁed, and let
F2(λ∗) be deﬁned as above. Then
wT ∇xxL(x∗, λ∗)w ≥0,
for all w ∈F2(λ∗).
(12.55)
Proof.
Since x∗is a local solution, all feasible sequences {zk} approaching x∗must have
f (zk) ≥f (x∗) for all k sufﬁciently large. Our approach in this proof is to construct a feasible
sequence whose limiting direction is w/∥w∥and show that the property f (zk) ≥f (x∗)
implies that (12.55) holds.
Since w ∈F2(λ∗) ⊂F1, we can use the technique in the proof of Lemma 12.3 to
construct a feasible sequence {zk} such that
lim
k→∞
zk −x∗
∥zk −x∗∥
w
∥w∥.
In particular, we have from formula (12.43) that
ci(zk) 
tk
∥w∥∇ci(x∗)T w,
for all i ∈A(x∗),
(12.56)
where {tk} is some sequence of positive scalars decreasing to zero. Moreover, we have from
(12.45) that
∥zk −x∗∥ tk + o(tk)
(12.57)
(this is an alternative way of stating the limit limk→∞tk/∥zk −x∗∥ 1), and so by
substitution into (12.44), we obtain
zk −x∗
tk
∥w∥w + o(tk).
(12.58)
From (12.28), (12.30), and (12.56), we have as in (12.31) that
L(zk, λ∗)  f (zk) −

i∈E∪I
λ∗
i ci(zk)

344
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
 f (zk) −
tk
∥w∥

i∈A(x∗)
λ∗
i ∇ci(x∗)T w
 f (zk),
(12.59)
where the last equality follows from the critical property (12.53). On the other hand, we can
perform a Taylor series expansion to obtain an estimate of L(zk, λ∗) near x∗. By using the
Taylor’s theorem expression (2.6) and continuity of the Hessians ∇2f and ∇2ci, i ∈E ∪I,
we obtain
L(zk, λ∗)  L(x∗, λ∗) + (zk −x∗)T ∇xL(x∗, λ∗)
(12.60)
+ 1
2(zk −x∗)T ∇xxL(x∗, λ∗)(zk −x∗) + o(∥zk −x∗∥2).
By the complementarity conditions (12.30e) the ﬁrst term on the right-hand-side of this
expression is equal to f (x∗). From (12.30a), the second term is zero. Hence we can rewrite
(12.60) as
L(zk, λ∗)  f (x∗) + 1
2(zk −x∗)T ∇xxL(x∗, λ∗)(zk −x∗) + o(∥zk −x∗∥2).
(12.61)
By using (12.57) and (12.58), we have for the second-order term and the remainder term
that
1
2(zk −x∗)T ∇xxL(x∗, λ∗)(zk −x∗) + o(∥zk −x∗∥2)
 1
2(tk/∥w∥)2wT ∇xxL(x∗, λ∗)w + o(t2
k ).
Hence, by substituting this expression together with (12.59) into (12.61), we obtain
f (zk)  f (x∗) + 1
2(tk/∥w∥)2wT ∇xxL(x∗, λ∗)w + o(t2
k ).
(12.62)
If wT ∇xxL(x∗, λ∗)w < 0, then (12.62) would imply that f (zk) < f (x∗) for all k sufﬁciently
large, contradicting the fact that x∗is a local solution. Hence, the condition (12.55) must
hold, as claimed.
□
Sufﬁcient conditions are conditions on f and ci, i ∈E ∪I, that ensure that x∗is a
local solution of the problem (12.1). (They take the opposite tack to necessary conditions,
which assume that x∗is a local solution and deduce properties of f and ci.) The second-
order sufﬁcient condition stated in the next theorem looks very much like the necessary
condition just discussed, but it differs in that the constraint qualiﬁcation is not required,
and the inequality in (12.55) is replaced by a strict inequality.

1 2 . 4 .
S e c o n d - O r d e r C o n d i t i o n s
345
Theorem 12.6 (Second-Order Sufﬁcient Conditions).
Suppose that for some feasible point x∗∈IRn there is a Lagrange multiplier vector λ∗
such that the KKT conditions (12.30) are satisﬁed. Suppose also that
wT ∇xxL(x∗, λ∗)w > 0,
for all w ∈F2(λ∗), w ̸ 0.
(12.63)
Then x∗is a strict local solution for (12.1).
Proof.
The result is proved if we can show that for any feasible sequence {zk} approaching
x∗, we have that f (zk) > f (x∗) for all k sufﬁciently large.
Given any feasible sequence, we have from Lemma 12.3(i) and Deﬁnition 12.4 that
all its limiting directions d satisfy d ∈F1. Choose a particular limiting direction d whose
associated subsequence Sd satisﬁes (12.34). In other words, we have for all k ∈Sd that
zk −x∗ ∥zk −x∗∥d + o(∥zk −x∗∥).
(12.64)
From (12.28), we have that
L(zk, λ∗)  f (zk) −

i∈A(x∗)
λ∗
i ci(zk) ≤f (zk),
(12.65)
while the Taylor series approximation (12.61) from the proof of Theorem 12.5 continues to
hold.
We know that d ∈F1, but suppose ﬁrst that it is not in F2(λ∗). We can then identify
some index j ∈A(x∗) ∩I such that the strict positivity condition
λ∗
j∇cj(x∗)T d > 0
(12.66)
is satisﬁed, while for the remaining indices i ∈A(x∗), we have
λ∗
i ∇ci(x∗)T d ≥0.
From Taylor’s theorem and (12.64), we have for all k ∈Sd and for this particular value of j
that
λ∗
jcj(zk)  λ∗
jcj(x∗) + λ∗
j∇cj(x∗)T (zk −x∗) + o(∥zk −x∗∥)
 ∥zk −x∗∥λ∗
j∇cj(x∗)T d + o(∥zk −x∗∥).
Hence, from (12.65), we have for k ∈Sd that
L(zk, λ∗)  f (zk) −

i∈A(x∗)
λ∗
i ci(zk)
≤f (zk) −∥zk −x∗∥λ∗
j∇cj(x∗)T d + o(∥zk −x∗∥).
(12.67)

346
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
From the Taylor series estimate (12.61), we have meanwhile that
L(zk, λ∗)  f (x∗) + O(∥zk −x∗∥2),
and by combining with (12.67), we obtain
f (zk) ≥f (x∗) + ∥zk −x∗∥λ∗
j∇cj(x∗)T d + o(∥zk −x∗∥).
Therefore, because of (12.66), we have f (zk) > f (x∗) for all k ∈Sd sufﬁciently large.
For the other case of d ∈F2(λ∗), we use (12.64), (12.65), and (12.61) to write
f (zk) ≥f (x∗) + 1
2(zk −x∗)T ∇xxL(x∗, λ∗)(zk −x∗) + o(∥zk −x∗∥2)
 1
2∥zk −x∗∥2dT ∇xxL(x∗, λ∗)d + o(∥zk −x∗∥2).
Because of (12.63), we again have f (zk) > f (x∗) for all k ∈Sd sufﬁciently large.
Since this reasoning applies to all limiting directions of {zk}, and since each element
zk of the sequence can be assigned to one of the subsequences Sd that converge to one of
these limiting directions, we conclude that f (zk) > f (x∗) for all k sufﬁciently large.
□
❏Example 12.7
(Example 12.2, One More Time)
We now return to Example 12.2 to check the second-order conditions for problem
(12.17). In this problem we have f (x)  x1 + x2, c1(x)  2 −x2
1 −x2
2, E  ∅, and I  {1}.
The Lagrangian is
L(x, λ)  (x1 + x2) −λ1(2 −x2
1 −x2
2),
and it is easy to show that the KKT conditions (12.30) are satisﬁed by x∗ (−1, −1)T , with
λ∗
1  1
2. The Lagrangian Hessian at this point is
∇xxL(x∗, λ∗) 

2λ∗
1
0
0
2λ∗
1



1
0
0
1

.
This matrix is positive deﬁnite, that is, it satisﬁes wT ∇xxL(x∗, λ∗)w > 0 for all w ̸ 0, so
it certainly satisﬁes the conditions of Theorem 12.6. We conclude that x∗ (−1, −1)T is a
strict local solution for (12.17). (In fact, it is the global solution of this problem, since, as we
note in a later section, this problem is a convex programming problem.)
❐

1 2 . 4 .
S e c o n d - O r d e r C o n d i t i o n s
347
❏Example 12.8
For an example in which the issues are more complex, consider the problem
min −0.1(x1 −4)2 + x2
2
s.t.
x2
1 + x2
2 −1 ≥0,
(12.68)
in which we seek to minimize a nonconvex function over the exterior of the unit circle.
Obviously, the objective function is not bounded below on the feasible region, since we can
take the feasible sequence

10
0

,

20
0

,

30
0

,

40
0

,
and note that f (x) approaches −∞along this sequence. Therefore, no global solution exists,
but it may still be possible to identify a strict local solution on the boundary of the constraint.
We search for such a solution by using the KKT conditions (12.30) and the second-order
conditions of Theorem 12.6.
By deﬁning the Lagrangian for (12.68) in the usual way, it is easy to verify that
∇xL(x, λ) 

−0.2(x1 −4) −2λx1
2x2 −2λx2

,
(12.69a)
∇xxL(x, λ) 

−0.2 −2λ
0
0
2 −2λ

.
(12.69b)
The point x∗ (1, 0)T satisﬁes the KKT conditions with λ∗
1  0.3 and the active set
A(x∗)  {1}. To check that the second-order sufﬁcient conditions are satisﬁed at this point,
we note that
∇c1(x∗) 

2
0

,
so that the space F2 deﬁned in (12.52) is simply
F2(λ∗)  {w | w1  0}  {(0, w2)T | w2 ∈IR}.
Now, by substituting x∗and λ∗into (12.69b), we have for any w ∈F2 with w ̸ 0 that
wT ∇xxL(x∗, λ∗)w 

0
w2
T 
−0.4
0
0
1.4
 
0
w2

 1.4w2
2 > 0.

348
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Hence, the second-order sufﬁcient conditions are satisﬁed, and we conclude from
Theorem 12.6 that (1, 0) is a strict local solution for (12.68).
❐
SECOND-ORDER CONDITIONS AND PROJECTED HESSIANS
The second-order conditions are sometimes stated in a form that is weaker but easier
to verify than (12.55) and (12.63). This form uses a two-sided projection of the Lagrangian
Hessian ∇xxL(x∗, λ∗) onto subspaces that are related to F2(λ∗).
The simplest case is obtained when the multiplier λ∗that satisﬁes the KKT conditions
(12.30) is unique (as happens, for example, when the LICQ condition holds) and strict
complementarity holds. In this case, the deﬁnition (12.52) of F2(λ∗) reduces to
F2(λ∗)  Null

∇ci(x∗)T 
i∈A(x∗)  Null A,
where A is deﬁned as in (12.38). In other words, F2(λ∗) is the null space of the matrix whose
rows are the active constraint gradients at x∗. As in (12.40), we can deﬁne the matrix Z
with full column rank whose columns span the space F2(λ∗). Any vector w ∈F2(λ∗) can
be written as w  Zu for some vector u, and conversely, we have that Zu ∈F2(λ∗) for all
u. Hence, the condition (12.55) in Theorem 12.5 can be restated as
uT ZT ∇xxL(x∗, λ∗)Zu ≥0 for all u,
or, more succinctly,
ZT ∇xxL(x∗, λ∗)Z is positive semideﬁnite.
Similarly, the condition (12.63) in Theorem 12.6 can be restated as
ZT ∇xxL(x∗, λ∗)Z is positive deﬁnite.
We see at the end of this section that Z can be computed numerically, so that the positive
(semi)deﬁniteness conditions can actually be checked by forming these matrices and ﬁnding
their eigenvalues.
When the optimal multiplier λ∗is unique but the strict complementarity condition is
not satisﬁed, F2(λ∗) is no longer a subspace. Instead, it is an intersection of planes (deﬁned
by the ﬁrst two conditions in (12.52)) and half-spaces (deﬁned by the third condition in
(12.52)). We can still, however, deﬁne two subspaces F 2 and F 2 that “bound” F2 above and
below, in the sense that F 2 is the smallest-dimensional subspace that contains F2(λ∗), while
F 2 is the largest-dimensional subspace contained in F2(λ∗). To be precise, we have
F 2  {d ∈F1 | ∇ci(x∗)T d  0, all i ∈A(x∗)},

1 2 . 4 .
S e c o n d - O r d e r C o n d i t i o n s
349
F 2  {d ∈F1 | ∇ci(x∗)T d  0, all i ∈A(x∗) with i ∈E or λ∗
i > 0},
so that
F 2 ⊂F2(λ∗) ⊂F 2.
(12.70)
As in the previous case, we can construct matrices Z and Z whose columns span the
subspaces F 2 and F 2, respectively. If the condition (12.55) of Theorem 12.5 holds, we can
be sure that
wT ∇xxL(x∗, λ∗)w ≥0,
for all w ∈F 2,
because F 2 ⊂F2(λ∗). Therefore, an immediate consequence of (12.55) is that the matrix
ZT ∇xxL(x∗, λ∗)Z is positive semideﬁnite.
Analogously, we have from F2(λ∗) ⊂F 2 that condition (12.63) is implied by the
condition
wT ∇xxL(x∗, λ∗)w > 0,
for all w ∈F 2.
Hence, given that the λ∗satisfying the KKT conditions is unique, a sufﬁcient condition
for (12.63) is that the matrix Z
T ∇xxL(x∗, λ∗)Z be positive deﬁnite. Again, this condition
provides a practical way to check the second-order sufﬁcient condition.
The matrices ZT ∇xxL(x∗, λ∗)Z and Z
T ∇xxL(x∗, λ∗)Z are sometimes called two-
sided projected Hessian matrices, or simply projected Hessians for short.
One way to compute the matrix Z (and its counterparts Z and Z) is to apply a QR
factorization to the matrix of active constraint gradients whose null space we seek. In the
simplest case above (in which the multiplier λ∗is unique and strictly complementary), we
deﬁne A as in (12.38) and write the QR factorization of AT as
AT  Q

R
0



Q1
Q2


R
0

 Q1R,
(12.71)
whereR isasquareuppertriangularmatrix,andQisn×northogonal.IfR isnonsingular,we
can set Z  Q2. If R is singular (indicating that the active constraint gradients are linearly
dependent), a slight enhancement of this procedure that makes use of column pivoting
during the QR procedure can be used to identify Z. For more details, see Section 15.2.
CONVEX PROGRAMS
Convex programming problems are constrained optimization problems in which the
objective function f is a convex function and the feasible set  deﬁned by (12.2) is a convex

350
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
set. As for unconstrained optimization (see Chapter 2), the convex case is special—all local
solutions are also global solutions, and the set of global minima is itself convex. (The proof
of these statements is similar to the proof of Theorem 2.5 and is left as an exercise.) Convex
programming problems often can be recognized by the algebraic properties of the constraint
functions ci, as we see in the following theorem.
Theorem 12.7.
A sufﬁcient condition for the feasible region  deﬁned by the constraints in (12.1) to be
convex is that the functions ci be linear for i ∈E and that −ci be convex functions for i ∈I.
Proof.
Let x0 and x1 be two feasible points, that is,
ci(x0)  ci(x1)  0,
i ∈E,
ci(x0) ≥0,
ci(x1) ≥0,
i ∈I,
and deﬁne xτ  (1 −τ)x0 + τx1 for any τ ∈[0, 1]. To prove the theorem, it is sufﬁcient to
show that xτ is feasible. By linearity of the equality constraints, we have
ci(xτ)  (1 −τ)ci(x0) + τci(x1)  0,
i ∈E,
so xτ satisﬁes the equality constraints. By convexity of −ci for i ∈I, we have
−ci(xτ) ≤(1 −τ)(−ci(x0)) + τ(−ci(x1)) ≤0,
i ∈I,
so xτ also satisﬁes the inequality constraints. Hence xτ ∈.
□
It follows from Theorem 12.7 that linear programming is a special case of convex
programming.
Of our examples in this chapter, Examples 12.2, 12.3, and 12.4 were all convex
programs, while Example 12.1 was not, because its feasible region was not convex.
12.5
OTHER CONSTRAINT QUALIFICATIONS
We now reconsider the constraint qualiﬁcation, the “extra” assumption that is needed to
make necessary conditions of the KKT conditions of Theorem 12.1. In general, constraint
qualiﬁcations allow us to show that the set F1 of Deﬁnition 12.4 is identical to the set of
multiples of limiting feasible directions. (The LICQ of Deﬁnition 12.1 was used in this way
in the proof of Lemma 12.3, for instance.) To put it another way, a constraint qualiﬁcation is
a condition that ensures that the linear approximation F1 to the feasible region at x∗captures
the essential geometric features of the true feasible set in some neighborhood of x∗.

1 2 . 5 .
O t h e r C o n s t r a i n t Q u a l i f i c a t i o n s
351
This point is perhaps best illustrated by an example in which F1 contains directions
that are obviously infeasible with respect to the actual constraint set. Consider the feasible
region deﬁned by
x2 ≤x3
1,
x2 ≥0,
(12.72)
as illustrated in Figure 12.11. Supposing that x∗ (0, 0), so that both constraints are active,
we have from Deﬁnition 12.4 that
F1  {d | −d2 ≥0,
d2 ≥0}  {d | d2  0}.
This set includes the direction (−1, 0), which points along the negative direction of the
horizontal axis and is clearly not a limiting direction for any feasible sequence. In fact, the
only possible limiting direction for this constraint set at x∗ (0, 0) is the vector (1, 0).
We conclude that the linear approximation to the feasible set at (0, 0) fails to capture the
geometry of the true feasible set, so constraint qualiﬁcations do not hold in this case.
One situation in which the linear approximation F1 is obviously an adequate repre-
sentation of the actual feasible set occurs when all the active constraints are already linear!
It is not difﬁcult to prove a version of Lemma 12.3 for this situation.
Lemma 12.8.
Suppose that all active constraints ci(·), i ∈A(x∗), are linear functions. Then the vector
w ∈IRn belongs to F1 if and only if w/∥w∥is a limiting direction of some feasible sequence.
2
x1
x
1
F
Figure 12.11
Failure of constraint qualiﬁcations at (0, 0) for the feasible region
deﬁned by (12.72).

352
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Proof.
The reverse implication holds, because Lemma 12.3 shows that all multiples of
limiting directions of feasible sequences belong to F1.
Toprovetheforwardimplication,wechoosew ∈F1 arbitrarilyandconstructafeasible
sequence for which w/∥w∥is a limiting direction. First note that we can choose a positive
scalar T such that
ci(x∗+ tw) > 0,
for all i ∈I\A(x∗) and all t ∈[0, T ],
that is, the inactive constraints stay inactive at least for short steps along w. Now deﬁne the
sequence zk by
zk  x∗+ (T/k)w,
k  1, 2, . . . .
Since aT
i w ≥0 for all i ∈I ∪A(x∗), we have
ci(zk)  ci(zk) −ci(x∗)  aT
i (zk −x∗)  T
k aT
i w ≥0,
for all i ∈I ∩A(x∗),
so that zk is feasible with respect to the active inequality constraints ci, i ∈I ∩A(x∗). By the
choice of T , we ﬁnd that zk is also feasible with respect to the inactive inequality constraints
i ∈I\A(x∗), and it is easy to show that ci(zk)  0 for the equality constraints i ∈E. Hence,
zk is feasible for each k  1, 2, . . .. In addition, we have that
zk −x∗
∥zk −x∗∥
(T/k)w
(T/k)∥w∥
w
∥w∥,
so that indeed the direction w/∥w∥is the limiting feasible direction of the sequence
{zk}.
□
We conclude from this result that the condition that all active constraints be linear is
another possible constraint qualiﬁcation. It is neither weaker nor stronger than the LICQ
condition, that is, there are situations in which one condition is satisﬁed but not the other
(see the exercises).
Another useful generalization of the LICQ is the Mangasarian–Fromovitz constraint
qualiﬁcation (MFCQ), which we deﬁne formally as follows.
Deﬁnition 12.5 (MFCQ).
Given the point x∗and the active set A(x∗) deﬁned by (12.29), we say that the
Mangasarian–Fromovitz constraint qualiﬁcation (MFCQ) holds if there exists a vector
w ∈IRn such that
∇ci(x∗)T w > 0,
for all i ∈A(x∗) ∩I,
∇ci(x∗)T w  0,
for all i ∈E,

1 2 . 6 .
A G e o m e t r i c V i e w p o i n t
353
and the set of equality constraint gradients {∇ci(x∗), i ∈E} is linearly independent.
Note the strict inequality involving the active inequality constraints.
The MFCQ is a weaker condition than LICQ. If LICQ is satisﬁed, then the system of
equalities deﬁned by
∇ci(x∗)T w  1,
for all i ∈A(x∗) ∩I,
∇ci(x∗)T w  0,
for all i ∈E,
has a solution w, by full rank of the active constraint gradients. Hence, we can choose the
w of Deﬁnition 12.5 to be precisely this vector. On the other hand, it is easy to construct
examples in which the MFCQ is satisﬁed but the LICQ is not; see the exercises.
It is possible to prove a version of the ﬁrst-order necessary condition result, Theo-
rem 12.1, in which MFCQ replaces LICQ in the assumptions. MFCQ has the particularly
nice property that it is equivalent to boundedness of the set of Lagrange multiplier vectors
λ∗for which the KKT conditions (12.30) are satisﬁed. (In the case of LICQ, this set consists
of a unique vector λ∗, and so is trivially bounded.)
Notethatconstraintqualiﬁcationsaresufﬁcientconditionsforthelinearapproximation
to be adequate, not necessary conditions. For instance, consider the set deﬁned by x2 ≥−x2
1
and x2 ≤x2
1 and the feasible point x∗ (0, 0). None of the constraint qualiﬁcations we have
discussed are satisﬁed, but the linear approximation F1  {w | w2  0} accurately reﬂects
the geometry of the feasible set near x∗.
12.6
A GEOMETRIC VIEWPOINT
Finally, we mention an alternative ﬁrst-order optimality condition that depends only on the
geometry of the feasible set  and not on its particular algebraic description in terms of the
functions ci, i ∈E ∪I. In geometric terms, our problem (12.1) can be stated as
min f (x)
subject to x ∈,
(12.73)
where  is the feasible set deﬁned by the conditions (12.2).
ThetoolsweneedtostatethisconditionarediscussedintheAppendix;seeinparticular
the Deﬁnition A.1 for the tangent cone T(x∗) and formula (A.22), which deﬁnes the normal
cone N(x∗).
The ﬁrst-order necessary condition for (12.73) is delightfully simple.
Theorem 12.9.
Suppose that x∗is a local minimizer of f in . Then
−∇f (x∗) ∈N(x∗).
(12.74)

354
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Proof.
Given any d ∈T(x∗), we have from Deﬁnition A.1 that for any decreasing positive
sequence tj ↓0, there is a sequence of vectors dj such that
x∗+ tjdj ∈,
dj →d.
(12.75)
(We simply take xj ≡x∗in Deﬁnition A.1.) Since x∗is a local solution, we must have
f (x∗+ tjdj) ≥f (x∗)
for allj sufﬁciently large. Hence, since f iscontinuouslydifferentiable,wehavefromTaylor’s
theorem (2.4) that
tj∇f (x∗)T dj + o(tj) ≥0.
By taking the limits of this inequality as j →∞, we have
∇f (x∗)T d ≥0.
Recall that d was an arbitrary member of T(x∗), so we have −∇f (x∗)T d ≤0 for all
d ∈T(x∗). We conclude from (A.22) that −∇f (x∗) ∈N(x∗).
□
This result, together with some observations we made earlier in this chapter, suggests
that there is a close relationship between N(x∗) and the convex combination of active
constraint gradients given by (12.47). When the linear independence constraint qualiﬁcation
holds, we can prove this result formally by using an argument based on the implicit function
theorem.
Lemma 12.10.
Suppose that the LICQ assumption (Deﬁnition 12.1) holds at x∗. Then the tangent cone
T(x∗) is identical to F1 from Deﬁnition 12.4, while the normal cone N(x∗) is simply −N,
where N is the set deﬁned in (12.47).
Proof.
Suppose that d ∈T(x∗). Then by Deﬁnition A.1, we have for any positive de-
creasing sequence tj ↓0 that there is a sequence {dj} with the property (12.75). In
particular,
ci(x∗+ tjdj)  0, i ∈E,
ci(x∗+ tjdj) ≥0, i ∈A(x∗) ∩I.
For i ∈E, we have by Taylor’s theorem that
0  ci(x∗) + tjaT
i dj + o(tj)  tjaT
i dj + o(tj).

1 2 . 6 .
A G e o m e t r i c V i e w p o i n t
355
By taking the limit as j →∞, we obtain aT
i d  0. Similarly, we obtain aT
i d ≥0 for
i ∈A(x∗) ∩I. Hence d ∈F1.
For the converse, choose d ∈F1 and let xj and tj be given sequences as in Deﬁni-
tion A.1. For simplicity, we assume as in Lemma 12.3 that all constraints ci, i  1, 2, . . . , m,
are active at the solution. Similarly to (12.41), and using the notation
A(x)T  [∇ci(x)]i1,2,...,m,
A∗ A(x∗),
Aj  A(xj),
we construct the following parametrized system of equations:
R(z, t; xj) 

c(z) −c(xj) −tAjd
ZT (z −xj −td)



0
0

.
(12.76)
Clearly, the solution for t  0 is z(0)  xj. Further, we have
∇zR(z, t; xj)

t0 

Aj
Z

,
which is nonsingular for j sufﬁciently large, since A(xj) →A(x∗), so the solution z(t) of
(12.76) is well-deﬁned for all values of t sufﬁciently small. Moreover, from Deﬁnition 12.4,
we have for such solutions z that
ci(z(t))  c(xj) + tAjd  0 for i ∈E;
ci(z(t))  c(xj) + tAjd ≥0 for i ∈A ∩I,
so that z(t) is feasible with respect to the constraints in (12.2).
We now deﬁne
dj  z(tj) −xj
tj
.
Sincexj+tjdj  z(tj) ∈,thesequence{dj}satisﬁesoneofthepropertiesofDeﬁnitionA.1.
To verify the other property, we rearrange (12.76) to obtain

Aj
ZT

d  1
tj

c(z(tj)) −c(xj)
z(tj) −xj

(12.77)


Aj
ZT

dj + 1
tj
O(∥c(z(tj)) −c(xj) −tjAjdj∥).
By using Taylor’s theorem, we have that
c(z(tj)) −c(xj)  Aj(z(tj) −xj) + o(∥z(tj) −xj∥)

356
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
 A∗(z(tj) −xj) + (Aj −A∗)(z(tj) −xj) + o(∥z(tj) −xj∥)
 A∗(z(tj) −xj) + o(∥z(tj) −xj∥),
and therefore from the deﬁnition of dj, we have
1
tj
[c(z(tj)) −c(xj) −tjA∗dj]  o(∥z(tj) −xj∥/tj)  o(∥dj∥).
By substituting into (12.77), we obtain

A∗
ZT

d 

A∗
ZT

dj + o(∥dj∥),
which implies, by nonsingularity of the matrix [(A∗)T | Z], that
d  dj + o(∥dj∥).
Hence, we have dj →d, and so d ∈T(x∗), completing the proof of equivalence between
F1 and T(x∗).
The proof of equivalence between −N and N(x∗) follows from Lemma 12.4 and the
deﬁnition (A.22) of N(x∗). From Lemma 12.4, we have that
g ∈N
⇒
gT d ≥0 for all d ∈F1.
Since we have F1  T(x∗), it follows by switching the sign of this expression that
g ∈−N
⇒
gT d ≤0 for all d ∈T(x∗).
By comparing with (A.22), we see that this is precisely the deﬁnition of N(x∗), so we
conclude that N(x∗)  −N, as claimed.
□
NOTES AND REFERENCES
We return to our claim that the set N deﬁned by (12.47) is a closed set. This fact is
needed in the proof of Lemma 12.4 to ensure that the solution of the projection subproblem
(12.48) is well-deﬁned. Note that N has the following form:
N  {s | s  Aλ, Cλ ≥0},
(12.78)
for appropriately deﬁned matrices A and C.
We outline a proof of closedness of N for the case in which LICQ holds (Deﬁni-
tion 12.1), that is, the matrix A has full column rank. It sufﬁces to show that whenever {sk} is

1 2 . 6 .
A G e o m e t r i c V i e w p o i n t
357
a sequence of vectors satisfying sk →s∗, then s∗∈N. Because A has full column rank, for
each sk there is a unique vector λk such that sk  Aλk; in fact, we have λk  (AT A)−1AT sk.
Because sk ∈N, we must have Cλk ≥0, and since sk →s∗, we also have that
lim
k λk  lim
k (AT A)−1AT sk  (AT A)−1AT s∗def λ∗.
The inequality Cλ∗≥0 is a consequence of Cλk ≥0 for all k, whereas s∗ Aλ∗follows
from the facts that sk −Aλk  0 for all k, and sk −Aλk →s∗−Aλ∗. Therefore we have
identiﬁed a vector λ∗such that s∗ Aλ∗and Cλ∗≥0, so s∗∈N, as claimed.
There is a more general proof, using an argument due to Mangasarian and Schu-
maker [155] and appealing to Theorem 13.2 (iii), that does not require LICQ. We omit the
details here.
The theory of constrained optimization is discussed in many books on numerical
optimization. The discussion in Fletcher [83, Chapter 9] is similar to ours, though a little
terser, and includes additional material on duality. Bertsekas [9, Chapter 3] emphasizes the
role of duality and discusses sensitivity of the solution with respect to the active constraints
in some detail. The classic treatment of Mangasarian [154] is particularly notable for its
thorough description of constraint qualiﬁcations.
The KKT conditions were described in a 1951 paper of Kuhn and Tucker [145], though
they were derived earlier (and independently) in an unpublished 1939 master’s thesis of W.
Karush. Lagrange multipliers and optimality conditions for general problems (including
nonsmooth problems) are described in the article of Rockafellar [217], which is both wide-
ranging and deep.
✐
E x e r c i s e s
✐
12.1 Does problem (12.4) have a ﬁnite or inﬁnite number of local solutions? Use the
ﬁrst-order optimality conditions (12.30) to justify your answer.
✐
12.2 Show by drawing a picture that the sufﬁcient conditions for  to be convex in
Theorem 12.7 are not necessary. That is, it is possible to have a convex feasible region  even
if the constraint functions do not satisfy the conditions of the theorem.
✐
12.3 If f is convex and the feasible region  is convex, show that local solutions of
the problem (12.3) are also global solutions. Show that the set of global solutions is convex.
✐
12.4 Let v : IRn →IRm be a smooth vector function and consider the unconstrained
optimization problems of minimizing f (x) where
f (x)  ∥v(x)∥∞,
f (x) 
max
i1,2,...,m vi(x).

358
C h a p t e r
1 2 .
T h e o r y o f C o n s t r a i n e d O p t i m i z a t i o n
Reformulate these (generally nonsmooth) problems as smooth constrained optimization
problems.
✐∗12.5 Can you perform a smooth reformulation as in the previous question when f is
deﬁned by
f (x) 
min
i1,2,...,m fi(x)?
(N.B. “min” not “max.”) Why or why not?
✐
12.6 Wedemonstratedinthischapterhowafeasibleregionwithnonsmoothboundary
could sometimes be described by a set of smooth constraints. Conversely, it is possible that a
single smooth constraint describes a feasible set with nonsmooth boundary. As an example,
consider the cone deﬁned by
)
(x1, x2, x3) | x2
3 ≥x2
1 + x2
2
*
(draw this set). Can you think of other examples?
✐
12.7 Show that the vector deﬁned by (12.14) satisﬁes both (12.12) and (12.13) when
the ﬁrst-order optimality condition (12.10) is not satisﬁed.(Hint: Use the H¨older inequality
(A.37).)
✐
12.8 Verify that for the sequence {zk} deﬁned by (12.35), the function f (x)  x1 +x2
satisﬁes f (zk+1) > f (zk) for k  2, 3, . . .. (Hint: Consider the trajectory z(s) 
(−

2 −1/s2, −1/s)T and show that the function h(s)
def f (z(s)) has h′(s) > 0 for
all s ≥2.)
✐
12.9 Consider the problem (12.9). Specify two feasible sequences that approach the
maximizing point (1, 1), and show that neither sequence is a decreasing sequence for f .
✐
12.10 Verify that neither the LICQ nor the MFCQ holds for the constraint set deﬁned
by (12.72) at x∗ (0, 0).
✐
12.11 Consider the feasible set in IR2 deﬁned by x2 ≥0, x2 ≤x2
1. Does the linear
approximation F1 of Deﬁnition 12.4 adequately capture the geometry of the feasible set near
x∗ (0, 0)? Is the LICQ or MFCQ satisﬁed?
✐
12.12 It is trivial to construct an example of a feasible set and a feasible point x∗at
which the LICQ is satisﬁed but the constraints are nonlinear. Give an example of the reverse
situation, that is, where the active constraints are linear but the LICQ is not satisﬁed.
✐
12.13 Show that for the feasible region deﬁned by
(x1 −1)2 + (x2 −1)2 ≤2,
(x1 −1)2 + (x2 + 1)2 ≤2,

1 2 . 6 .
A G e o m e t r i c V i e w p o i n t
359
x1 ≥0,
the MFCQ is satisﬁed at x∗ (0, 0) but the LICQ is not satisﬁed.
✐
12.14 Verify (12.70).
✐
12.15 Consider the half space deﬁned by H  {x ∈IRn | aT x +α ≥0} where a ∈IRn
and α ∈IR are given. Formulate and solve the optimization problem for ﬁnding the point x
in H that has the smallest Euclidean norm.
✐
12.16 (Fletcher [83]) Solve the problem
min
x
x1 + x2 subject to x2
1 + x2
2  1
by eliminating the variable x2. Show that the choice of sign for a square root operation during
the elimination process is critical; the “wrong” choice leads to an incorrect answer.
✐
12.17 Prove that when the KKT conditions (12.30) and the LICQ are satisﬁed at a
point x∗, the Lagrange multiplier λ∗in (12.30) is unique.
✐
12.18 Consider the problem of ﬁnding the point on the parabola y  1
5(x −1)2 that
is closest to (x, y)  (1, 2), in the Euclidean norm sense. We can formulate this problem as
min f (x, y)  (x −1)2 + (y −2)2
subject to (x −1)2  5y.
(a) Find all the KKT points for this problem. Is the LICQ satisﬁed?
(b) Which of these points are solutions?
(c) By directly substituting the constraint into the objective function and eliminating the
variable x, we obtain an unconstrained optimization problem. Show that the solutions
of this problem cannot be solutions of the original problem.
✐
12.19 Consider the problem
min
x∈IR2 f (x)  −2x1 + x2
subject to

(1 −x1)3 −x2
≥
0
x2 + 0.25x2
1 −1
≥
0.
The optimal solution is x∗ (0, 1)T , where both constraints are active. Does the LICQ hold
at this point? Are the KKT conditions satisﬁed?
✐
12.20 Find the minima of the function f (x)  x1x2 on the unit circle x2
1 + x2
2  1.
Illustrate this problem geometrically.
✐
12.21 Find the maxima of f (x)  x1x2 over the unit disk deﬁned by the inequality
constraint 1 −x2
1 −x2
2 ≥0.

Chapter13

Linear
Programming: The
Simplex Method
Dantzig’s development of the simplex method in the late 1940s marks the start of the modern
era in optimization. This method made it possible for economists to formulate large models
and analyze them in a systematic and efﬁcient way. Dantzig’s discovery coincided with the
development of the ﬁrst digital computers, and the simplex method became one of the
earliest important applications of this new and revolutionary technology. From those days
to the present, computer implementations of the simplex method have been continually
improved and reﬁned. They have beneﬁted particularly from interactions with numerical
analysis, a branch of mathematics that also came into its own with the appearance of digital
computers, and have now reached a high level of sophistication.
Today, linear programming and the simplex method continue to hold sway as the most
widely used of all optimization tools. Since 1950, generations of workers in management,
economics, ﬁnance, and engineering have been trained in the business of formulating linear
models and solving them with simplex-based software. Often, the situations they model are
actually nonlinear, but linear programming is appealing because of the advanced state of the
software, guaranteed convergence to a global minimum, and the fact that uncertainty in the
data can make complicated nonlinear models look like overkill. Nonlinear programming
may make inroads as software develops, and a new class of methods known as interior-point

362
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
*
x
optimal point 
feasible polytope
c
Figure 13.1
A linear program in two dimensions with solution at x∗.
methods (see Chapter 14) has proved to be faster for some linear programming problems,
but the continued importance of the simplex method is assured for the foreseeable future.
LINEAR PROGRAMMING
Linear programs have a linear objective function and linear constraints, which may
include both equalities and inequalities. The feasible set is a polytope, that is, a convex,
connected set with ﬂat, polygonal faces. The contours of the objective function are planar.
Figure 13.1 depicts a linear program in two-dimensional space. Contours of the objective
function are indicated by the dotted lines. The solution in this case is unique—a single vertex.
A simple reorientation of the polytope or the objective gradient c could, however, make the
solution nonunique; the optimal value cT x could be the same on an entire edge. In higher
dimensions, the set of optimal points can be a single vertex, an edge or face, or even the
entire feasible set!
Linear programs are usually stated and analyzed in the following standard form:
min cT x, subject to Ax  b, x ≥0,
(13.1)

C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
363
where c and x are vectors in IRn, b is a vector in IRm, and A is an m×n matrix. Simple devices
can be used to transform any linear program to this form. For instance, given the problem
min cT x, subject to Ax ≥b
(without any bounds on x), we can convert the inequality constraints to equalities by
introducing a vector of surplus variables z and writing
min cT x, subject to Ax −z  b, z ≥0.
(13.2)
This form is still not quite standard, since not all the variables are constrained to be nonnega-
tive. We deal with this by splitting x into its nonnegative and nonpositive parts, x  x+−x−,
where x+  max(x, 0) ≥0 and x− max(−x, 0) ≥0. The problem (13.2) can now be
written as
min


c
−c
0


T 

x+
x−
z

, s.t.

A
−A
−I



x+
x−
z

 b,


x+
x−
z

≥0,
which clearly has the same form as (13.1). Inequality constraints of the form x ≤u or
Ax ≤b can be dealt with by adding slack variables to make up the difference between the
left- and right-hand-sides. Hence
x ≤u ⇔x + w  u, w ≥0,
Ax ≤b ⇔Ax + y  b, y ≥0.
We can also convert a “maximize” objective max cT x into the “minimize” form of (13.1) by
simply negating c to obtain min(−c)T x.
Many linear programs arise from models of transshipment and distribution networks.
Theseproblemshavemuchadditionalstructureintheirconstraints;special-purposesimplex
algorithms that exploit this structure are highly efﬁcient. We do not discuss these network-
ﬂow problems further in this book, except to note that the subject is important and complex,
and that a number of excellent texts are available (see, for example, Ahuja, Magnanti, and
Orlin [1]).
For the standard formulation (13.1), we will assume throughout that m < n. Other-
wise, the system Ax  b contains redundant rows, is infeasible, or deﬁnes a unique point.
When m ≥n, factorizations such as the QR or LU factorization (see the Appendix) can be
used to transform the system Ax  b to one with a coefﬁcient matrix of full row rank and,
in some cases, decide that the feasible region is empty or consists of a single point.

364
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
13.1
OPTIMALITY AND DUALITY
OPTIMALITY CONDITIONS
Optimality conditions for the problem (13.1) can be derived from the theory of Chap-
ter 12. Only the ﬁrst-order conditions—the Karush–Kuhn–Tucker (KKT) conditions—are
needed. Convexity of the problem ensures that these conditions are sufﬁcient for a global
minimum, as we show below by a simple argument. (We do not need to refer to the second-
order conditions from Chapter 12, which are not informative in any case because the Hessian
of the Lagrangian for (13.1) is zero.)
The tools we developed in Chapter 12 make derivation of optimality and duality
theory for linear programming much easier than in other treatments of the subject, where
this theory has to be developed more or less from scratch.
The KKT conditions follow from Theorem 12.1. As stated in Chapter 12, this theorem
requires linear independence of the active constraint gradients (LICQ). However, as we
showed in the section on constraint qualiﬁcations, the result continues to hold for dependent
constraints, provided that they are linear, as is the case here.
We partition the Lagrange multipliers for the problem (13.1) into two vectors π and
s, where π ∈IRm is the multiplier vector for the equality constraints Ax  b, while s ∈IRn
is the multiplier vector for the bound constraints x ≥0. Using the deﬁnition (12.28), we
can write the Lagrangian function for (13.1) as
L(x, π, s)  cT x −πT (Ax −b) −sT x.
(13.3)
Applying Theorem 12.1, we ﬁnd that the ﬁrst-order necessary conditions for x∗to be a
solution of (13.1) are that there exist vectors π and s such that
AT π + s  c,
(13.4a)
Ax  b,
(13.4b)
x ≥0,
(13.4c)
s ≥0,
(13.4d)
xisi  0, i  1, 2, . . . , n.
(13.4e)
The complementarity condition (13.4e), which essentially says that at least one of the com-
ponents xi and si must be zero for each i  1, 2, . . . , n, is often written in the alternative
form xT s  0. Because of the nonnegativity conditions (13.4c), (13.4d), the two forms are
identical.
Let (x∗, π∗, s∗) denote a vector triple that satisﬁes (13.4). By combining the three
equalities (13.4a), (13.4d), and (13.4e), we ﬁnd that
cT x∗ (AT π∗+ s∗)T x∗ (Ax∗)T π∗ bT π∗.
(13.5)

1 3 . 1 .
O p t i m a l i t y a n d D u a l i t y
365
As we shall see in a moment, bT π is the objective function for the dual problem to (13.1),
so (13.5) indicates that the primal and dual objectives are equal for vector triples (x, π, s)
that satisfy (13.4).
It is easy to show directly that the conditions (13.4) are sufﬁcient for x∗to be a global
solution of (13.1). Let ¯x be any other feasible point, so that A¯x  b and ¯x ≥0. Then
cT ¯x  (Aπ∗+ s∗)T ¯x  bT π∗+ ¯xT s∗≥bT π∗ cT x∗.
(13.6)
We have used (13.4) and (13.5) here; the inequality relation follows trivially from ¯x ≥0 and
s∗≥0. The inequality (13.6) tells us that no other feasible point can have a lower objective
value than cT x∗. We can say more: The feasible point ¯x is optimal if and only if
¯xT s∗ 0,
since otherwise the inequality in (13.6) is strict. In other words, when s∗
i > 0, then we must
have ¯xi  0 for all solutions ¯x of (13.1).
THE DUAL PROBLEM
Given the data c, b, and A, which deﬁne the problem (13.1), we can deﬁne another,
closely related, problem as follows:
max bT π, subject to AT π ≤c.
(13.7)
This problem is called the dual problem for (13.1). In contrast, (13.1) is often referred to as
the primal.
The primal and dual problems are two sides of the same coin, as we see when we write
down the KKT conditions for (13.7). Let us ﬁrst rewrite (13.7) in the form
min −bT π subject to c −AT π ≥0,
to ﬁt the formulation (12.1) from Chapter 12. By using x ∈IRn to denote the Lagrange
multipliers for the constraints AT π ≤c, we write the Lagrangian function as
¯L(π, x)  −bT π −xT (c −AT π).
Noting again that the conclusions of Theorem 12.1 continue to hold if the linear indepen-
dence assumption is replaced by linearity of all constraints, we ﬁnd the ﬁrst-order necessary
condition for π to be optimal for (13.7) to be that there exist a vector x such that
Ax  b,
(13.8a)
AT π ≤c,
(13.8b)

366
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
x ≥0,
(13.8c)
xi(c −AT π)i  0, i  1, 2, . . . , n.
(13.8d)
If we deﬁne s  c −AT π and substitute in (13.8), we ﬁnd that the conditions (13.8)
and (13.4) are identical! The optimal Lagrange multipliers π in the primal problem are the
optimal variables in the dual problem, while the optimal Lagrange multipliers x in the dual
problem are the optimal variables in the primal problem.
The primal–dual relationship is symmetric; by taking the dual of the dual, we recover
the primal. To see this, we restate (13.7) in standard form by introducing the slack vector s
(so that AT π + s  c) and splitting the unbounded variables π as π  π+ −π−, where
π+ ≥0, and π−≥0. We can now write the dual as
min


−b
b
0


T 

π+
π−
s

s.t.

AT
−AT
I



π+
π−
s

 c,


π+
π−
s

≥0,
(13.9)
which clearly has the standard form (13.1). The dual of (13.9) is now
max cT z subject to


A
−A
I

z ≤


−b
b
0

.
Now Az ≤−b and −Az ≤b together imply that Az  −b, so we obtain the equivalent
problem
min −cT z subject to Az  −b, z ≤0.
By making the identiﬁcation z  −x, we recover (13.1), as claimed.
Given a feasible vector x for the primal—that is, Ax  b and x ≥0—and a feasible
point (π, s) for the dual—that is, AT π + s  c, s ≥0—we have as in (13.6) that
0 ≤xT s  xT (c −AT π)  cT x −bT π.
(13.10)
Therefore, we have cT x ≥bT π when both the primal and dual variables are feasible—the
dual objective function is a lower bound on the primal objective function. At a solution, the
gap between primal and dual shrinks to zero, as we show in the following theorem.
Theorem 13.1 (Duality Theorem of Linear Programming).
(i) If either problem (13.1) or (13.7) has a solution with ﬁnite optimal objective value, then
so does the other, and the objective values are equal.

1 3 . 1 .
O p t i m a l i t y a n d D u a l i t y
367
(ii) If either problem (13.1) or (13.7) has an unbounded objective, then the other problem has
no feasible points.
Proof.
For (i), suppose that (13.1) has a ﬁnite optimal solution. Then because of Theo-
rem 12.1, there are vectors π and s such that (x, π, s) satisﬁes (13.4). Since (13.4) and (13.8)
are equivalent, it follows that π is a solution of the dual problem (13.7), since there exists a
vector x that satisﬁes (13.8). Because xT s  0, it follows from (13.10) that cT x  bT π, so
the optimal objective values are equal.
We can make a symmetric argument if we start by assuming that the dual problem
(13.7) has a solution.
For (ii), suppose that the primal objective value is unbounded below. Then there must
exist a direction d ∈IRn along which cT x decreases without violating feasibility. That is,
cT d < 0,
Ad  0,
d ≥0.
Suppose now that there does exist a feasible point π for the dual problem (13.7), that is
AT π ≤c. Multiplying from the left by dT , using the nonnegativity of d, we obtain
0  dT AT π ≤dT c < 0,
giving a contradiction.
Again, we can make a symmetric argument to prove (ii) if we start by assuming that
the dual objective is unbounded below.
□
As we showed in the discussion following Theorem 12.1, the multiplier values π and
s for (13.1) tell us how sensitive the optimal objective value is to perturbations in the con-
straints. In fact, the process of ﬁnding (π, s) for a given optimal x is often called sensitivity
analysis. We can make a simple direct argument to illustrate this dependence. If a small
change b is made to the vector b (the right-hand-side in (13.1) and objective gradient in
(13.7)), then we would usually expect small perturbations in the primal and dual solutions.
If these perturbations (x, π, s) are small enough, we know that provided the problem
is not degenerate (deﬁned below), the vectors s and x have zeros in the same locations
as s and x, respectively. Since x and s are complementary (see (13.4e)), it follows that
xT s  xT s  (x)T s  (x)T s  0.
Now we have from the duality theorem that
cT (x + x)  (b + b)T (π + π).
Since
cT x  bT π,
A(x + x)  b + b,
AT π  −s,

368
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
we have
cT x  (b + b)T π + bT π
 (x + x)T AT π + bT π
 −(x + x)T s + bT π  bT π.
In particular, if b  ϵej, where ej is the jth unit vector in IRm, we have for all ϵ sufﬁciently
small that
cT x  ϵπj.
(13.11)
That is, the change in primal objective is linear in the value of πj (cf. (12.33)) for small
perturbations in the components of the right-hand-side bj.
13.2
GEOMETRY OF THE FEASIBLE SET
BASIC FEASIBLE POINTS
We assume for the remainder of the chapter that
The matrix A in (13.1) has full row rank.
(13.12)
In practice, a preprocessing phase is applied to the user-supplied data to remove some
redundancies from the given constraints and eliminate some of the variables. Reformulation
by adding slack, surplus, and artiﬁcial variables is also used to force A to satisfy the property
(13.12).
Suppose that x is a feasible point with at most m nonzero components. Suppose, too,
that we can identify a subset B(x) of the index set {1, 2, . . . , n} such that
• B(x) contains exactly m indices;
• i /∈B(x)
⇒
xi  0;
• the m × m matrix B deﬁned by
B  [Ai]i∈B(x)
(13.13)
is nonsingular, where Ai is the ith column of A.
If all these conditions are true, we call x a basic feasible point for (13.1).
The simplex method generates a sequence of iterates xk all of which are basic feasible
points. Since we want the iterates to converge to a solution of (13.1), the simplex strategy
will make sense only if

1 3 . 2 .
G e o m e t r y o f t h e F e a s i b l e S e t
369
(a) the problem has basic feasible points; and
(b) at least one such point is a basic optimal point, that is, a solution of (13.1) that is also a
basic feasible point.
Happily, both (a) and (b) are true under minimal assumptions.
Theorem 13.2 (Fundamental Theorem of Linear Programming).
(i) If there is a feasible point for (13.1), then there is a basic feasible point.
(ii) If (13.1) has solutions, then at least one such solution is a basic optimal point.
(iii) If (13.1) is feasible and bounded, then it has an optimal solution.
Proof.
Among all feasible vectors x, choose one with the minimal number of nonzero
components, and denote this number by p. Without loss of generality, assume that the
nonzeros are x1, x2, . . . , xp, so we have
p

i1
Aixi  b.
Suppose ﬁrst that the columns A1, A2, . . . , Ap are linearly dependent. Then we can
express one of them (Ap, say) in terms of the others, and write
Ai 
p−1

i1
Aizi,
(13.14)
for some scalars z1, z2, . . . , zp−1. It is easy to check that the vector
x(ϵ)  x + ϵ(z1, z2, . . . , zp−1, −1, 0, 0, . . . , 0)T  x + ϵz
(13.15)
satisﬁes Ax(ϵ)  b for any scalar ϵ. In addition, since xi > 0 for i  1, 2, . . . , p, we also
have xi(ϵ) > 0 for the same indices i  1, 2, . . . , p and all ϵ sufﬁciently small in magnitude.
However, there is a value ¯ϵ ∈(0, xp] such that xi(¯ϵ)  0 for some i  1, 2, . . . , p. Hence,
x(¯ϵ) is feasible and has at most p −1 nonzero components, contradicting our choice of p
as the minimal number of nonzeros.
Therefore, columns A1, A2, . . . , Ap must be linearly independent, and so p ≤m. If
p  m, we are done, since then x is a basic feasible point and B(x) is simply {1, 2, . . . , m}.
Otherwise, p < m, and because A has full row rank, we can choose m −p columns from
amongAp+1, Ap+2, . . . , An tobuildupasetofmlinearlyindependentvectors.Weconstruct
B(x) by adding the corresponding indices to {1, 2, . . . , p}. The proof of (i) is complete.
Theproofof(ii)isquitesimilar.Letx∗beasolutionwithaminimalnumberofnonzero
components p, and assume again that x∗
1, x∗
2, . . . , x∗
p are the nonzeros. If the columns

370
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
A1, A2, . . . , Ap are linearly dependent, we deﬁne
x∗(ϵ)  x∗+ ϵz,
where z is chosen exactly as in (13.14), (13.15). It is easy to check that x∗(ϵ) will be feasible
for all ϵ sufﬁciently small, both positive and negative. Hence, since x∗is optimal, we must
have
cT (x∗+ ϵz) ≥cT x∗
⇒
ϵcT z ≥0
for all |ϵ| sufﬁciently small. Therefore, cT z  0, and so cT x∗(ϵ)  cT x∗for all ϵ. The same
logic as in the proof of (i) can be applied to ﬁnd ¯ϵ > 0 such that x∗(¯ϵ) is feasible and optimal,
with at most p −1 nonzero components. This contradicts our choice of p as the minimal
number of nonzeros, so the columns A1, A2, . . . , Ap must be linearly independent. We can
now apply the same logic as above to conclude that x∗is already a basic feasible point and
therefore a basic optimal point.
The ﬁnal statement (iii) is a consequence of ﬁnite termination of the simplex method.
We comment on the latter property in the next section.
□
The terminology we use here is not standard, as the following table shows:
our terminology
standard terminology
basic feasible point
basic feasible solution
basic optimal point
optimal basic feasible solution
The standard terms arose because “solution” and “feasible solution” were originally used
as synonyms for “feasible point.” However, as the discipline of optimization developed, the
word“solution”tookonamorespeciﬁcandintuitivemeaning(asin“solutiontotheproblem
. . . ”). We keep the terminology of this chapter consistent with the rest of the book by sticking
to this more modern usage.
VERTICES OF THE FEASIBLE POLYTOPE
The feasible set deﬁned by the linear constraints is a polytope, and the vertices of this
polytope are the points that do not lie on a straight line between two other points in the set.
Geometrically, they are easily recognizable; see Figure 13.2.
Algebraically, the vertices are exactly the basic feasible points that we described above.
Wethereforehaveanimportantrelationshipbetweenthealgebraicandgeometricviewpoints
and a useful aid to understanding how the simplex method works.
Theorem 13.3.
All basic feasible points for (13.1) are vertices of the feasible polytope {x | Ax  b, x ≥0},
and vice versa.

1 3 . 2 .
G e o m e t r y o f t h e F e a s i b l e S e t
371
*
*
*
*
*
*
*
*
*
*
Figure 13.2
Vertices of a three-dimensional polytope (indicated by ∗).
Proof.
Let x be a basic feasible point and assume without loss of generality that B(x) 
{1, 2, . . . , m}. The matrix B  [Ai]i1,2,...,m is therefore nonsingular, and
xm+1  xm+2  · · ·  xn  0.
(13.16)
Suppose that x lies on a straight line between two other feasible points y and z. Then we
can ﬁnd α ∈(0, 1) such that x  αy + (1 −α)z. Because of (13.16) and the fact that α and
1 −α are both positive, we must have yi  zi  0 for i  m + 1, m + 2, . . . , n. Writing
xB  (x1, x2, . . . , xm)T and deﬁning yB and zB likewise, we have from Ax  Ay  Az  b
that
BxB  ByB  BzB  b,
and so, by nonsingularity of B, we have xB  yB  zB. Therefore, x  y  z, contradicting
our assertion that y and z are two feasible points other than x. Therefore, x is a vertex.
Conversely, let x be a vertex of the feasible polytope, and suppose that the nonzero
components of x are x1, x2, . . . , xp. If the corresponding columns A1, A2, . . . , Ap are lin-
early dependent, then we can construct the vector x(ϵ)  x + ϵz as in (13.15). Since x(ϵ) is
feasible for all ϵ with sufﬁciently small magnitude, we can deﬁne ˆϵ > 0 such that x(ˆϵ) and
x(−ˆϵ) are both feasible. Since x  x(0) obviously lies on a straight line between these two

372
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
points, it cannot be a vertex. Hence our assertion that A1, A2, . . . , Ap are linearly dependent
must be incorrect, so these columns must be linearly independent and p ≤m. The same
arguments as in the proof of Theorem 13.2 can now be used to show that x is a basic feasible
point, completing our proof.
□
We conclude this discussion of the geometry of the feasible set with a deﬁnition of
degeneracy. This term has a variety of meanings in optimization, as we discuss in Chapter 16.
For the purposes of this chapter, we use the following deﬁnition.
Deﬁnition 13.1 (Degenerate Linear Program).
A linear program (13.1) is said to be degenerate if there exists at least one basic feasible
point that has fewer than m nonzero components.
Naturally, nondegenerate linear programs are those for which this deﬁnition is not
satisﬁed.
13.3
THE SIMPLEX METHOD
OUTLINE OF THE METHOD
As we just described, all iterates of the simplex method are basic feasible points for
(13.1) and therefore vertices of the feasible polytope. Most steps consist of a move from
one vertex to an adjacent one for which the set of basic indices B(x) differs in exactly one
component. On most steps (but not all), the value of the primal objective function cT x is
decreased. Another type of step occurs when the problem is unbounded: The step is an edge
along which the objective function is reduced, and along which we can move inﬁnitely far
without ever reaching a vertex.
The major issue at each simplex iteration is to decide which index to change in the
basis set B. Unless the step is a direction of unboundedness, one index must be removed
from B and replaced by another from outside B. We can get some insight into how this
decision is made by looking again at the KKT conditions (13.4) to see how they relate to the
algorithm.
From B and (13.4), we can derive values for not just the primal variable x but also the
dual variables π and s, as we now show. We deﬁne the index set N as the complement of B,
that is,
N  {1, 2, . . . , n}\B.
(13.17)
Just as B is the column submatrix of A that corresponds to the indices i ∈B, we use N
to denote the submatrix N  [Ai]i∈N . We also partition the n-element vectors x, s, and c
according to the index sets B and N, using the notation
xB  [xi]i∈B,
xN  [xi]i∈N ,
sB  [si]i∈B,
sN  [si]i∈N .

1 3 . 3 .
T h e S i m p l e x M e t h o d
373
From the KKT condition (13.4b), we have that
Ax  BxB + NxN  b.
The primal variable x for this simplex iterate is deﬁned as
xB  B−1b,
xN  0.
(13.18)
Since we are dealing only with basic feasible points, we know that B is nonsingular and
that xB ≥0, so this choice of x satisﬁes two of the KKT conditions: the equality constraints
(13.4b) and the nonnegativity condition (13.4c).
We choose s to satisfy the complementarity condition (13.4e) by setting sB  0. The
remaining components π and sN can be found by partitioning this condition into B and N
components and using sB  0 to obtain
BT π  cB,
NT π + sN  cN.
(13.19)
Since B is square and nonsingular, the ﬁrst equation uniquely deﬁnes π as
π  B−T cB.
(13.20)
The second equation in (13.19) implies a value for sN:
sN  cN −NT π  cN −(B−1N)T cB.
(13.21)
Computation of the vector sN is often referred to as pricing. The components of sN are often
called the reduced costs of the nonbasic variables xN.
The only KKT condition that we have not enforced explicitly is the nonnegativity
condition s ≥0. The basic components sB certainly satisfy this condition, by our choice
sB  0. If the vector sN deﬁned by (13.21) also satisﬁes sN ≥0, we have found an optimal
vector triple (x, π, s), so the algorithm can terminate and declare success. The usual case,
however, is that one or more of the components of sN are negative, so the condition s ≥0 is
violated. The new index to enter the basic index set B—the entering index—is now chosen
to be one of the indices q ∈N for which sq < 0. As we show below, the objective cT x will
decrease when we allow xq to become positive if and only if q has the property that sq < 0.
Our procedure for altering B and changing x and s accordingly is as follows:
• allow xq to increase from zero during the next step;
• ﬁx all other components of xN at zero;
• ﬁgure out the effect of increasing xq on the current basic vector xB, given that we want
to stay feasible with respect to the equality constraints Ax  b;

374
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
• keep increasing xq until one of the components of xB (corresponding to xp, say) is
driven to zero, or determining that no such component exists (the unbounded case);
• remove index p (known as the leaving index) from B and replace it with the entering
index q.
It is easy to formalize this procedure in algebraic terms. Since we want both the new
iterate x+ and the current iterate x to satisfy Ax  b, and since xN  0 and x+
i
 0 for
i ∈N\{q}, we have
Ax+  Bx+
B + Aqx+
q  BxB  Ax.
By multiplying this expression by B−1 and rearranging, we obtain
x+
B  xB −B−1Aqx+
q .
(13.22)
We show in a moment that the direction −B−1Aq is a descent direction for cT x. Geometri-
cally speaking, (13.22) is a move along an edge of the feasible polytope that decreases cT x.
We continue to move along this edge until a new vertex is encountered. We have to stop
at this vertex, since by deﬁnition we cannot move any further without leaving the feasible
region. At the new vertex, a new constraint xi ≥0 must have become active, that is, one
of the components xi, i ∈B, has decreased to zero. This index i is the one that is removed
from the basis.
Figure 13.3 shows a path traversed by the simplex method for a problem in IR2. In this
example the function cT x is decreased at each iteration, and the optimal vertex x∗is found
in three steps.
FINITE TERMINATION OF THE SIMPLEX METHOD
Let us now verify that the step deﬁned by (13.22) leads to a decrease in cT x. By using
the deﬁnition (13.22) of x+
B together with
x+
N  (0, . . . , 0, x+
q , 0, . . . , 0)T ,
we have
cT x+  cT
B x+
B + cT
N x+
N
 cT
B x+
B + cqx+
q
 cT
B xB −cT
B B−1Aqx+
q + cqx+
q .
(13.23)
Now, from (13.20) we have cT
B B−1  πT , while from (13.19) we have AT
q π  cq −sq, since
Aq is a column of N. Therefore,
cT
B B−1Aqx+
q  πT Aqx+
q  (cq −sq)x+
q ,

1 3 . 3 .
T h e S i m p l e x M e t h o d
375
simplex path
1
0
2
3
c
Figure 13.3
Simplex iterates for a two-dimensional problem.
so by substituting in (13.23) we obtain
cT x+  cT
B xB −(cq −sq)x+
q + cqx+
q  cT
B xB −sqx+
q .
Since xN  0, we have cT x  cT
B xB and therefore
cT x+  cT x −sqx+
q .
(13.24)
Since we chose q such that sq < 0, and since x+
q > 0 if we are able to move at all along the
edge, it follows from (13.24) that the step (13.22) produces a decrease in the primal objective
function cT x.
If the problem is nondegenerate (see Deﬁnition 13.1), then we are guaranteed that
x+
q > 0, so we can be assured of a strict decrease in the objective function cT x at every
simplex step. We can therefore prove the following result concerning termination of the
simplex method.
Theorem 13.4.
Provided that the linear program (13.1) is nondegenerate and bounded, the simplex
method terminates at a basic optimal point.
Proof.
The simplex method cannot visit the same basic feasible point x at two different
iterations, because it attains a strict decrease at each iteration. Since each subset of m indices
drawn from the set {1, 2, . . . , n} is associated with at most one basic feasible point, it follows
that no basis B can be visited at two different simplex iterations. The number of possible

376
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
bases is at most
	n
m

(which is the number of ways to choose the m elements of a basis B from
among the n possible indices), so there can be only a ﬁnite number of iterations. Since the
method is always able to take a step away from a nonoptimal basic feasible point, the point
of termination must be a basic optimal point.
□
Note that this result gives us a proof of Theorem 13.2 (iii) for the nondegenerate case.
A SINGLE STEP OF THE METHOD
Wehavecoveredmostofthemechanicsoftakingasinglestepofthesimplexmethod.To
make subsequent discussions easier to follow, we summarize our description in a semiformal
way.
Procedure 13.1 (One Step of Simplex).
Given B, N, xB  B−1b ≥0, xN  0;
Solve BT π  cB for π,
Compute sN  cN −NT π;
if sN ≥0
STOP; (* optimal point found *)
Select q ∈N with sq < 0 as the entering index;
Solve Bt  Aq for t;
if t ≤0
STOP; (* problem is unbounded *)
Calculate x+
q  mini | ti>0 (xB)i/ti, and use p to denote the index of the basic
variable for which this minimum is achieved;
Update x+
B  xB −tx+
q , x+
N  (0, . . . , 0, x+
q , 0, . . . , 0)T ;
Change B by adding q and removing p.
We need to ﬂesh out this description with speciﬁcs of three important points. These
are as follows.
• Linear algebra issues—maintaining an LU factorization of B that can be used to solve
for π and t.
• Selection of the entering index q from among the negative components of sN. (In
general, there are many such components.)
• Handling of degenerate steps, in which x+
q  0, so that x is not changed.
Proper handling of these issues is crucial to the efﬁciency of a simplex implementation. We
give some details in the next three sections.

1 3 . 4 .
L i n e a r A l g e b r a i n t h e S i m p l e x M e t h o d
377
13.4
LINEAR ALGEBRA IN THE SIMPLEX METHOD
We have to solve two linear systems involving the matrix B at each step; namely,
BT π  cB,
Bt  Aq.
(13.25)
We never calculate the inverse matrix B−1 explicitly just to solve these systems, since this
calculation is unnecessarily expensive. Instead, we calculate or maintain some factorization
of B—usually an LU factorization—and use triangular substitutions with the factors to
recover π and t. It is less expensive to update the factorization than to calculate it afresh at
each iteration, because the matrix B changes by just a single column between iterations.
The standard factorization/updating procedures start with an LU factorization of B
at the ﬁrst iteration of the simplex algorithm. Since in practical applications B is large
and sparse, its rows and columns are rearranged during the factorization to maintain both
numerical stability and sparsity of the L and U factors. One successful pivot strategy that
trades off between these two aims was proposed by Markowitz in 1957 [158]; it is still used
as the basis of many practical sparse LU algorithms. Other considerations may also enter
into our choice of pivot. As we discuss below, it may help to improve the efﬁciency of the
updating procedures if as many as possible of the leading columns of U contain just a single
nonzero: the diagonal element. Many heuristics have been devised for choosing row and
column permutations that produce this and other desirable structural features.
Let us assume for simplicity that row and column permutations are already
incorporated in B, so that we write the initial LU factorization as
LU  B,
(13.26)
(L is unit lower triangular, U is upper triangular). The system Bt  Aq can then be solved
by the following two-step procedure:
L¯t  Aq,
Ut  ¯t.
(13.27)
Similarly, the system BT π  cB is solved by performing the following two triangular
substitutions:
U T ¯π  cB,
LT π  ¯π.
We now discuss a procedure for updating the factors L and U after one step of the
simplex method, when the index q is removed from B and replaced by the index p. The
corresponding change to the basis matrix B is that the column Aq is removed from B
and replaced by Ap. We call the resulting matrix B+ and note that if we rewrite (13.26) as
U  L−1B, it is easy to see that the modiﬁed matrix L−1B+ will be upper triangular except
in the column occupied by Ap. That is, L−1B+ has the form shown in Figure 13.4.

378
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
column p
Figure 13.4
L−1B+, which is upper
triangular except for the
column occupied by Ap.
Figure 13.5
After cyclic row and column
permutation P1, the
non–upper triangular part of
P1L−1B+P T
1 appears in the
last row.
Wenowperformacyclicpermutationthatmovescolumn p tothelastcolumnposition
m and moves columns p + 1, p + 2, . . . , m one position to the left to make room for it.
If we apply the same permutation to rows p through m, the net effect is to move the non–
upper triangular part to the last row of the matrix, as shown in Figure 13.5. If we denote the
permutation matrix by P1, the matrix of Figure 13.5 is P1L−1B+P T
1 .

1 3 . 4 .
L i n e a r A l g e b r a i n t h e S i m p l e x M e t h o d
379
Finally, we perform sparse Gaussian elimination on the matrix P1L−1B+P T
1 to restore
upper triangular form. That is, we ﬁnd L1 and U1 (lower and upper triangular, respectively)
such that
P1L−1B+P T
1  L1U1.
(13.28)
It is easy to show that L1 and U1 have a simple form. The lower triangular matrix L1 differs
from the identity only in the last row, while U1 is identical to P1L−1B+P T
1 except that
the (m, m) element is changed and the off-diagonal elements in the last row have been
eliminated.
We give details of this process for the case of m  5. Using the notation
L−1B  U 


u11
u12
u13
u14
u15
u22
u23
u24
u25
u33
u34
u35
u44
u45
u55


,
L−1Aq 


w1
w2
w3
w4
w5


,
and supposing that Ap occurs in the second column of B (so that the second column is
replaced by L−1Aq), we have
L−1B+ 


u11
w1
u13
u14
u15
w2
u23
u24
u25
w3
u33
u34
u35
w4
u44
u45
w5
u55


.
After the cyclic permutation P1, we have
P1L−1B+P1
T 


u11
u13
u14
u15
w1
u33
u34
u35
w3
u44
u45
w4
u55
w5
u23
u24
u25
w2


.
(13.29)

380
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
The factors L1 and U1 are now as follows:
L1 


1
1
1
1
0
l52
l53
l54
1


,
U1 


u11
u13
u14
u15
w1
u33
u34
u35
w3
u44
u45
w4
u55
w5
ˆw2


,
(13.30)
for some values of l52, l53, l54, and ˆw2 (see the exercises).
The result of this updating process is the factorization (13.28), which we can rewrite
as follows:
B+  L+U +,
where L+  LP T
1 L1, U +  U1P1.
(13.31)
There is no need to calculate L+ and U + explicitly. Rather, the nonzero elements in L1 and
the last column of U1, and the permutation information in P1, can be stored in compact
form, so that triangular substitutions involving L+ and U + can be performed by applying a
number of permutations and sparse triangular substitutions involving these various factors.
Subsequent simplex iterates give rise to similar updating procedures. At each step,
we need to store the same type of information: the permutation matrix and the nonzero
elements in the last row of L1 and the last column of U1.
The procedure we have just outlined is due to Forrest and Tomlin. It is quite efﬁcient,
because it requires the storage of little data at each update and does not require much
movement of data in memory. Its major disadvantage is possible numerical instability. Large
elements in the factors of a matrix are a sure indicator of instability, and the multipliers in
the L1 factor (l52 in (13.30), for example) may be very large. An earlier scheme of Bartels
and Golub allowed swapping of rows to avoid these problems. For instance, if |u33| < |u23|
in (13.29), we could swap rows 2 and 5 to ensure that the subsequent multiplier l52 in the
L1 factor does not exceed 1 in magnitude.
The Bartels–Golub procedure pays a price for its superior stability properties. The
lower right corner of the upper triangular factor is generally altered and possibly ﬁlled in
during each updating step. The ﬁll-in can be reduced by choosing a permutation of B during
the initial factorization to achieve a certain structure in the initial upper triangular factor
U. Speciﬁcally, we choose the permutation such that U has the structure
U 

D
U12
0
U22

,
(13.32)
where D is square (say, ˆm× ˆm) and diagonal. Then, at the ﬁrst update, ﬁll-in can only appear
in the last m −ˆm columns of U. Since these columns are moved one place to the left during
the update, the updated factor U1 also has the form of (13.32), but with D one dimension

1 3 . 5 .
O t h e r ( I m p o r t a n t ) D e t a i l s
381
smaller (that is, ˆm −1 × ˆm −1 and diagonal). Consequently, during the second update, we
may get ﬁll-in in the last m −ˆm + 1 columns of the U factor, and so on. This procedure
(due to Saunders) can sharply restrict the amount of ﬁll-in and produce an algorithm that
is both efﬁcient and stable.
All these updating schemes require us to keep a record of permutations and of certain
nonzero elements of the sparse triangular factors. Although this information can often be
stored in a highly compact form—just a few bytes of storage for each update—the total
amount of space may build up to unreasonable levels after many such updates have been
performed. As the number of updates builds up, so does the time needed to solve for the
vectors t and π in Procedure 13.1. If an unstable updating procedure is used, numerical
errors may also come into play, blocking further progress by the simplex algorithm. For
all these reasons, most simplex implementations periodically calculate a fresh LU factoriza-
tion of the current basis matrix B and discard the updates. The new factorization uses the
same permutation strategies that we apply to the very ﬁrst factorization, which balance the
requirements of stability, sparsity, and structure.
13.5
OTHER (IMPORTANT) DETAILS
PRICING AND SELECTION OF THE ENTERING INDEX
There are usually many negative components of sN at each step. We need to select one
of these, sq, as the entering variable. How do we make this selection? Ideally, we would like to
choose the sequence of entering variables that gets us to the solution x∗in the fewest possible
steps, but we rarely have the global perspective needed to implement this strategy. Instead,
we use more shortsighted but practical strategies that obtain a signiﬁcant decrease in cT x
on just the present iteration. There is usually a tradeoff between the effort spent searching
for a good entering variable q that achieves this aim and the decrease in cT x to be obtained
from pivoting on q. Different pivot strategies resolve the tradeoff in different ways.
Dantzig’s original selection rule is one of the simplest. It calculates sN  NT π and
selects the most negative sq as the entering component. This rule is motivated by (13.24). It
gives the maximum improvement in cT x per unit step in the entering variable xq. A large
reduction in cT x is not guaranteed, however. It could be that we can increase x+
q only a tiny
amount from zero before reaching the next vertex. As we see later, it is even possible that we
cannot move at all!
To avoid calculation of the entire vector sN, partial pricing strategies are a popular
option. At each iteration, these methods evaluate just a subvector of sN by multiplying π by a
columnsubmatrixofN.Themostnegative sq fromthissubvectorischosenasenteringpivot.
To give all the components of N a chance at entering the basis, partial pricing strategies cycle
through the nonbasic elements, changing the subvector of sN they evaluate at each iteration
so that no component is ignored for too long.

382
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
Neither of these strategies guarantees that we can make a substantial move along the
chosen edge before ﬁnding a new vertex. Multiple pricing strategies are more thorough: For
a small subset of indices from N, they evaluate not just the prices sq but the distance that
can be moved along the resulting edge and the consequent reduction in objective function,
namely sqx+
q . The index subset is initially chosen from the most negative components of sN;
ten such components is a typical number. For each component, we compute t  B−1Aq
and x+
q as in the algorithm outline above, and choose the entering variable as the index that
minimizes sqx+
q . For subsequent iterations we deal just with this same index set, essentially
ﬁxing all the components of xN outside this set at zero and solving a reduced linear program.
Eventually,weﬁndthatallthepricessq forindicesinthesubsetarenonnegative,sowechoose
a new index subset by computing the whole vector sN and repeat the process. This approach
has the advantage that we operate only on a few columns Aq of the nonbasic matrix N at a
time; the remainder of N does not even need to be in main memory at all. After the great
improvements in price and speed of computer memory in recent years, however, savings
such as this are less important than they once were.
There is plenty of scope to deﬁne heuristics that combine ideas from multiple and
partial pricing. At every iteration we can update the subset for which we compute the
entering variable value x+
q by
• retainingthemostpromisingindicesfromthepreviousiteration(i.e.,themostnegative
sq’s); and
• pricing a new part of the vector sN and choosing the most negative components sq
from this part.
Strategies like this can combine the thoroughness of multiple pricing with the advantages of
partial pricing, which brings in “new blood” in the form of new parts of N at each iteration,
at relatively low cost.
A sophisticated rule known as steepest edge chooses the most downhill direction from
among all the candidates—theonethatproducesthelargestdecreaseincT x perunitdistance
moved along the edge. By contrast, Dantzig’s rule maximizes the decrease in cT x per unit
change in x+
q , which is not the same thing. (A small change in x+
q can correspond to a large
distance moved along the edge.) During the pivoting step, the overall change in x is
x+ 

x+
B
x+
N



xB
xN

+

−B−1Aq
eq

x+
q  x + ηqx+
q ,
where eq is the unit vector with a 1 in the position corresponding to the index q ∈N and
zeros elsewhere, and
ηq 

−B−1Aq
eq



−t
eq

,
(13.33)

1 3 . 5 .
O t h e r ( I m p o r t a n t ) D e t a i l s
383
where we have used the fact that t  B−1Aq; see Procedure 13.1. The change in cT x per
unit step along ηq is given by
cT ηq
∥ηq∥.
(13.34)
The steepest-edge rule chooses q to minimize this quantity.
If we had to compute each ηq explicitly to perform the minimization in (13.34), the
steepest-edge strategy would be prohibitively expensive—as expensive as the strategy of
seeking the best possible reduction in cT x. Goldfarb and Reid showed that edge steepness
for all indices i ∈N can, in fact, be maintained quite economically. Instead of performing
one linear system solution procedure with BT (to ﬁnd π from BT π  cB in Procedure 13.1),
their method requires two such procedures with this same coefﬁcient matrix. In both cases, a
number of inner products involving the columns Ai for i ∈N are required. We now outline
their steepest-edge procedure.
First, note that we know already the numerator cT ηq in (13.34) without calculating
ηq, because we proved in the discussion leading up to (13.24) that cT ηq  sq. For the
denominator, there is a simple recurrence for keeping track of each norm ∥ηq∥, which is
updated from step to step. To derive this recurrence, suppose that Ap is the column to be
removed from the basis matrix B to make way for Aq. Assuming without loss of generality
that Ap occupies the ﬁrst column of B, we can express the pivot algebraically as follows:
B+  B + (Aq −Ap)eT
1  B + (Aq −Be1)eT
1 ,
(13.35)
where e1  (1, 0, 0, . . . , 0)T . Choosing an index i ∈N with i ̸ q, we want to see how
∥ηi∥is affected by the update (13.35).
For convenience, we use γi to denote ∥ηi∥2, and note from (13.33) that
γi  ∥ηi∥2  ∥B−1Ai∥2 + 1,
γ +
i
 ∥η+
i ∥2  ∥(B+)−1Ai∥2 + 1.
By applying the Sherman–Morrison formula (A.55) to the rank-one update formula in
(13.35), we obtain
(B+)−1  B−1 −(B−1Aq −e1)eT
1 B−1
1 + eT
1 (B−1Aq −e1)  B−1 −(t −e1)eT
1 B−1
eT
1 t
,
where again we have used the fact that t  B−1Aq. Therefore, we have that
(B+)−1Ai  B−1Ai −eT
1 B−1Ai
eT
1 t
(t −e1).

384
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
By substituting for (B+)−1Ai in (13.35) and performing some simple algebra (see the
exercises), we obtain
γ +
i
 γi −2
eT
1 B−1Ai
eT
1 t

AT
i B−T t +
eT
1 B−1Ai
eT
1 t
2
γq.
(13.36)
We now solve the following two linear systems to obtain ˆt and r:
BT ˆt  t,
BT r  e1.
(13.37)
The formula (13.36) then becomes
γ +
i
 γj −2
 rT Ai
rT Aq

ˆtT Ai +
 rT Ai
rT Aq
2
γq.
(13.38)
Hence, once the systems (13.37) have been solved, each γ +
i
can be updated at the cost of
evaluating the two inner products rT Ai and ˆtT Ai.
The steepest-edge strategy does not guarantee that we can take a long step before
reaching another vertex, but it has proved to be highly effective in practice. Recent testing
(Goldfarb and Forrest [114]) has shown it to be superior even to interior-point methods on
some very large problems.
STARTING THE SIMPLEX METHOD
The simplex method requires a basic feasible starting point ¯x and a corresponding
initial basic index set B ⊂{1, 2, . . . , n} with |B|  m for which
• the m × m matrix B deﬁned by (13.13) is nonsingular; and
• ¯xB ≥0 and ¯xN  0, where N is the complement of B (13.17).
The problem of ﬁnding this initial point and basis may itself be nontrivial—in fact, its
difﬁculty is equivalent to that of actually ﬁnding the solution of a linear program. One
approachfordealingwiththisdifﬁcultyisthe two-phaseorPhase-I/Phase-IIapproach,which
proceeds as follows. In Phase I, we set up an auxiliary problem, a linear program different
from (13.1), and solve it with the simplex method. The Phase-I problem is constructed so
that an initial basic feasible point is trivial to determine, and so that its solution gives a basic
feasible initial point for Phase II. In Phase II, a second linear program very similar to the
original problem (13.1) is solved, starting from the Phase-I solution. The solution of the
original problem (13.1) can be extracted easily from the solution of the Phase-II problem.
In Phase I we introduce artiﬁcial variables z into (13.1) and redeﬁne the objective
function to be the sum of these artiﬁcial variables. To be speciﬁc, the Phase-I problem is
min eT z, subject to Ax + Ez  b, (x, z) ≥0,
(13.39)

1 3 . 5 .
O t h e r ( I m p o r t a n t ) D e t a i l s
385
where z ∈IRm, e  (1, 1, . . . , 1)T , and E is a diagonal matrix whose diagonal elements are
Ejj  +1 if bj ≥0,
Ejj  −1 if bj < 0.
It is easy to see that the point (x, z) deﬁned by
x  0,
zj  |bj|,
j  1, 2, . . . , m,
(13.40)
is a basic feasible point for (13.39). Obviously, this point satisﬁes the constraints in (13.39),
while the initial basis matrix B is simply the matrix E, which is clearly nonsingular.
At any feasible point for (13.39), the artiﬁcial variables z represent the amounts by
which the constraints Ax  b are violated by the x component. The objective function is
simply the sum of these violations, so by minimizing this sum we are forcing x to become
feasible for the original problem (13.1). In fact, it is not difﬁcult to see that the Phase-I
problem (13.39) has an optimal objective value of zero if and only if the original problem
(13.1) has feasible points by using the following argument: If there exists a vector (˜x, ˜z) that
is feasible for (13.39) such that eT ˜z  0, we must have ˜z  0, and therefore A˜x  b and
˜x ≥0, so ˜x is feasible for the original problem (13.1). Conversely, if ˜x is feasible for (13.1),
then the point (˜x, 0) is feasible for (13.39) with an objective value of 0. Since the objective
in (13.39) is obviously nonnegative at all feasible points, then ( ˜x, 0) must be optimal for
(13.39), verifying our claim.
We can now apply the simplex method to (13.39) from the initial point (13.40). If it
terminates at an optimal solution for which the objective eT z is positive, we conclude by
the argument above that the original problem (13.1) is infeasible. Otherwise, the simplex
method identiﬁes a point (˜x, ˜z) with eT ˜z  0, which is a basic feasible point for the following
linear program, which is the Phase-II problem:
min cT x subject to Ax + z  b, x ≥0, 0 ≥z ≥0.
(13.41)
We can note immediately that this problem is equivalent to (13.1), because any solution
(and indeed any feasible point) must have z  0. We need to retain the artiﬁcial variables
z in Phase II, however, since some components of z may be present in the optimal basis
from Phase I that we are using as the initial basis for (13.41), though of course the values
˜zj of these components must be zero. In fact, we can modify (13.41) to include only those
components of z that are present in the optimal basis for (13.39), omitting the others.
The problem (13.41) is not quite in standard form because of the two-sided bounds
on z. However, it is easy to modify the simplex method described above to handle these
additional bounds (we omit the details). We can customize the simplex algorithm slightly
by deleting each component of z from the problem (13.41) as soon as it is swapped out of
the basis. This strategy ensures that components of z do not repeatedly enter and leave the
basis, and therefore avoids unnecessary simplex iterations with little added complication to
the algorithm.

386
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
If (x∗, z∗) is a basic solution of (13.41), it must have z∗ 0, and so x∗is a solution of
(13.1). In fact, x∗is a basic solution of (13.1), though this claim is not completely obvious
becausetheﬁnalbasisB forthePhase-IIproblemmaystillcontaincomponentsofz∗,making
it unsuitable as an optimal basis for (13.1). Since we know that A has full row rank, however,
we can construct an optimal basis for (13.1) in a postprocessing phase: We can extract the
indices that correspond to x components from B and add extra indices to B in a way that
maintains nonsingularity of the submatrix B deﬁned by (13.13).
A ﬁnal point to note is that in many problems we do not need to add a complete set of
m artiﬁcial variables to form the Phase-I problem. This observation is particularly relevant
when slack and surplus variables have already been added to the problem formulation, as in
(13.2), to obtain a linear program with inequality constraints in standard form (13.1). Some
of these slack/surplus variables can play the roles of artiﬁcial variables, making it unnecessary
to include such variables explicitly.
We illustrate this point with the following example.
❏Example 13.1
Consider the inequality-constrained linear program deﬁned by
min 3x1 + x2 + x3
subject to
2x1
+
x2
+
x3
≤
2,
x1
−
x2
−
x3
≤
−1,
x ≥0.
By adding slack variables to both inequality constraints, we obtain the following equivalent
problem in standard form:
min 3x1 + x2 + x3
subject to
2x1
+
x2
+
x3
+
x4

2,
x1
−
x2
−
x3
+
x5

−1,
x ≥0.
By inspection, it is easy to see that the vector x  (0, 0, 0, 2, 0) is feasible with respect to
the ﬁrst linear constraint and the lower bound x ≥0, though it does not satisfy the second
constraint. Hence, in forming the Phase-I problem, we add just a single artiﬁcial variable z2
to take care of the infeasibility in this second constraint, and obtain
min z2
subject to
(13.42)
2x1
+
x2
+
x3
+
x4

2
x1
−
x2
−
x3
+
x5
−
z2

−1
,
(x, z2) ≥0.
It is easy to see that the vector
(x, z2)  ((0, 0, 0, 2, 0), 1)

1 3 . 5 .
O t h e r ( I m p o r t a n t ) D e t a i l s
387
is feasible with respect to (13.42). In fact, it is a basic feasible point, since the matrix B
corresponding to the initial basis is
B 

1
0
0
−1

.
In this example, the variable x4 plays the role of artiﬁcial variable for the ﬁrst constraint.
There was no need to add an explicit artiﬁcial variable z1.
❐
DEGENERATE STEPS AND CYCLING
The third issue arises from the fact that the step x+
q computed by the procedure above
may be zero. There may well be an index i such that (xB)i  0 while ti > 0, so the algorithm
cannot move any distance at all along the feasible direction without leaving the feasible set.
A step of this kind is known as a degenerate step. Although we have not moved anywhere,
the step may still be useful because we have changed one component of the basis B. The
new basis may be closer to the optimal basis, so in that sense the algorithm may have made
progress towards the solution. Although no reduction in cT x is made at a degenerate step,
it may be laying the groundwork for reductions in cT x at subsequent steps.
However, a problem known as cycling can occur when we take a number of consecutive
degenerate steps. After making a number of alterations to the basis set without changing the
objective value, it is possible that we return to an earlier basis set B! If we continue to apply
our algorithm from this point, we will repeat the same cycle, returning to B ad inﬁnitum
and never terminating.
Cycling was thought originally to be a rare phenomenon, but it has been observed
with increasing frequency in the large linear programs that arise as relaxations of integer
programming problems. Since integer programs are an important source of linear programs,
it is essential for practical simplex codes to incorporate a cycling avoidance strategy.
We describe the perturbation strategy by ﬁrst showing how a perturbation to the right-
hand-side b of the constraints in (13.1) affects the basic solutions and the pivot strategy.
Then, we outline how these perturbations can be used to avoid cycling.
Suppose we perturb the right-hand-side vector b to b(ϵ) deﬁned by
b(ϵ)  b + E


ϵ
ϵ2
...
ϵm


,
where E is a nonsingular matrix and ϵ > 0 is a small constant. The perturbation in b
induces a perturbation in each basic solution xB, so that by deﬁning xB(ϵ) as the solution of

388
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
BxB(ϵ)  b(ϵ), we obtain
xB(ϵ)  xB + B−1E


ϵ
ϵ2
...
ϵm


 xB +
m

k1
(B−1E)·kϵk,
(13.43)
where (B−1E)·k denotes the kth column of B−1E. We choose E so that the perturbation
vector m
k1(B−1E)·kϵk is strictly positive for all sufﬁciently small positive values of ϵ.
With this perturbation, we claim that ties cannot arise in the choice of variable to leave
the basis. From Procedure 13.1 above, we have a tie if there are two indices i, l ∈B such that
(xB(ϵ))i
ti
 (xB(ϵ))l
tl
,
for all ϵ > 0.
(13.44)
This equality can occur only if all the coefﬁcients in the expansion (13.43) are identical for
the two rows i and l. In particular, we must have
(B−1E)ik
ti
 (B−1E)lk
tl
,
k  1, 2, . . . , m.
Hence, rows i and l of the matrix B−1E are multiples of each other, so B−1E is singular.
This contradicts nonsingularity of B and E, so ties cannot occur, and there is a unique
minimizing index p. Moreover, if a step of simplex is taken with this leaving index p, there
is a range of ϵ > 0 for which all variables remaining in the basis (xB(ϵ))i, i ̸ p, remain
strictly positive.
How can this device be used to avoid cycling? Suppose we are at a basis B for which
there are zero components of xB. At this point, we choose the perturbation matrix E so that
the perturbation (13.43) is positive for some range of ϵ > 0. For instance, if we choose E to
be the current basis B, we have from (13.43) that
xB(ϵ)  xB + B−1E


ϵ
ϵ2
...
ϵm


 xB +


ϵ
ϵ2
...
ϵm


> xB,
for all ϵ > 0. We now perform a step of simplex for the perturbed problem, choosing the
unique leaving index p as above.
After the simplex step, the basis components (xB(ϵ))i, i ̸ p, carried over from the
previous iteration are strictly positive for sufﬁciently small ϵ, as is the entering component
xq. Hence, it will be possible to take a nonzero step again at the next simplex iteration, and

1 3 . 6 .
W h e r e D o e s t h e S i m p l e x M e t h o d F i t ?
389
because ties (13.44) are not possible at this iteration either for sufﬁciently small ϵ, the leaving
variable will be uniquely determined as well.
The reasoning of the previous paragraph holds only if the value of ϵ is sufﬁciently
small. There are robust strategies, such as the lexicographic strategy, that avoid an explicit
choice of ϵ but rather ask questions of the “what if?” variety to break ties. Given a set of tied
indices ¯B ⊂B with
(xB)i
ti
 (xB)l
tl
,
for all i, l ∈¯B,
such a strategy would ask, “What if we made a perturbation of the type (13.43)? Which index
from ¯B would be preferred then?” Implementation of this idea is not difﬁcult: We evaluate
successive coefﬁcients of ϵk, k  1, 2, . . ., in the expansion (13.43) for the tying elements.
If there is an element i ∈¯B with
(B−1E)i1 > (B−1E)l1,
for all l ∈¯B,
l ̸ i,
Then clearly (xB(ϵ))i > (xB(ϵ))l for all ϵ sufﬁciently small, so we choose i as the leaving
index. Otherwise, we retain in ¯B the indices i that tie for the largest value of (B−1E)i1 and
evaluate the second column of B−1E. If one value of (B−1E)i2 dominates, we choose it as the
leaving index. Otherwise, we consider coefﬁcients (B−1E)ik for successively higher-order k
until just one candidate index remains.
13.6
WHERE DOES THE SIMPLEX METHOD FIT?
In linear programming, as in all optimization problems in which inequality constraints (in-
cluding bounds) are present, the fundamental issue is to partition the inequality constraints
into those that are active at the solution and those that are inactive. (The inactive constraints
are, of course, those for which a strict inequality holds at the solution.) The simplex method
belongs to a general class of algorithms for constrained optimization known as active set
methods, which explicitly maintain estimates of the active and inactive index sets that are
updated at each step of the algorithm. (In the case of simplex and linear programming, B is
the set of “probably inactive” indices—those for which the bound xi ≥0 is inactive—while
N is the set of “probably active” indices.) Like most active set methods, the simplex method
makes only modest changes to these index sets at each step: A single index is transferred
from B into N, and vice versa.
Active set algorithms for quadratic programming, bound-constrained optimization,
and nonlinear programming use the same basic strategy as simplex of deﬁning a set of
“probably active” constraint indices and taking a step toward the solution of a reduced
problem in which these constraints are satisﬁed as equalities. When nonlinearity enters the
problem, many of the features that make the simplex method so effective no longer apply.
For instance, it is no longer true in general that at least n −m of the bounds x ≥0 are

390
C h a p t e r
1 3 .
T h e S i m p l e x M e t h o d
active at the solution. The specialized linear algebra techniques no longer apply, and it may
no longer be possible to obtain an exact solution to the reduced problem at each iteration.
Nevertheless, the simplex method is rightly viewed as the antecedent of the active set class.
One undesirable feature of the simplex method attracted attention from its earliest
days.Thoughhighlyefﬁcientonalmostallpracticalproblems(themethodgenerallyrequires
at most 2m to 3m iterations, where m is the row dimension of the constraint matrix in
(13.1)), there are pathological problems on which the algorithm performs very poorly.
Klee and Minty [144] presented an n-dimensional problem whose feasible polytope has 2n
vertices, for which the simplex method visits every single vertex before reaching the optimal
point! This example veriﬁed that the complexity of the simplex method is exponential;
roughly speaking, its running time can be an exponential function of the dimension of the
problem. For many years, theoreticians searched for a linear programming algorithm that
has polynomial complexity, that is, an algorithm in which the running time is bounded by a
polynomial function of the size of the input. In the late 1970s, Khachiyan [142] described
an ellipsoid method that indeed has polynomial complexity but turned out to be much too
slow in practice. In the mid-1980s, Karmarkar [140] described a polynomial algorithm that
approaches the solution through the interior of the feasible polytope rather than working its
way around the boundary as the simplex method does. Karmarkar’s announcement marked
the start of intense research in the ﬁeld of interior-point methods, which are the subject of
the next chapter.
NOTES AND REFERENCES
An alternative procedure for performing the Phase-I calculation of an initial basis was
describedbyWolfe[247].Thistechniquedoesnotrequireartiﬁcialvariablestobeintroduced
in the problem formulation, but rather starts at any point x that satisﬁes Ax  b with at
most m nonzero components in x. (Note that we do not require the basic part xB to consist
of all positive components.) Phase I then consists in solving the problem
min
x

xi<0
−xi
subject to Ax  b,
and terminating when an objective value of 0 is attained. This problem is not a linear
program—its objective is only piecewise linear—but it can be solved by the simplex method
nonetheless. The key is to redeﬁne the cost vector f at each iteration x such that fi  −1
for xi < 0 and fi  0 otherwise.

1 3 . 6 .
W h e r e D o e s t h e S i m p l e x M e t h o d F i t ?
391
✐
E x e r c i s e s
✐
13.1 Consider the overdetermined linear system Ax  b with m rows and n columns
(m > n). When we apply Gaussian elimination with complete pivoting to A, we obtain
PAQ  L

U11
U12
0
0

,
where P and Q are permutation matrices, L is m × m lower triangular, U11 is ¯m × ¯m upper
triangular and nonsingular, U12 is ¯m × (n −¯m), and ¯m ≤n is the rank of A.
(a) Show that the system Ax  b is feasible if the last m −¯m components of L−1Pb are
zero, and infeasible otherwise.
(b) When ¯m  n, ﬁnd the unique solution of Ax  b.
(c) Show that the reduced system formed from the ﬁrst ¯m rows of PA and the ﬁrst ¯m
components of Pb is equivalent to Ax  b (i.e., a solution of one system also solves
the other).
✐
13.2 Convert the following linear program to standard form:
max
x,y cT x + dT y subject to A1x  b1, A2x + B2y ≤b2, l ≤y ≤u,
where there are no explicit bounds on x.
✐
13.3 Show that the dual of the linear program
min cT x subject to Ax ≥b, x ≥0,
is
max bT π subject to AT π ≤c, π ≥0.
✐
13.4 Show that when m ≤n and the rows of A are linearly dependent in (13.1), then
the matrix B in (13.13) is singular, and therefore there are no basic feasible points.
✐
13.5 Verify formula (13.36).
✐
13.6 Calculate the values of l52, l53, l54, and ˆw2 in (13.30), by equating the last row of
L1U1 to the last row of the matrix in (13.29).
✐
13.7 By extending the procedure (13.27) appropriately, show how the factorization
(13.31) can be used to solve linear systems with coefﬁcient matrix B+ efﬁciently.

Chapter14

Linear
Programming:
Interior-Point
Methods
In the 1980s it was discovered that many large linear programs could be solved efﬁciently
by formulating them as nonlinear problems and solving them with various modiﬁcations of
nonlinearalgorithmssuchasNewton’smethod.Onecharacteristicofthesemethodswasthat
they required all iterates to satisfy the inequality constraints in the problem strictly, so they
soon became known as interior-point methods. By the early 1990s, one class—primal–dual
methods—had distinguished itself as the most efﬁcient practical approach and proved to be
a strong competitor to the simplex method on large problems. These methods are the focus
of this chapter.
The motivation for interior-point methods arose from the desire to ﬁnd algorithms
with better theoretical properties than the simplex method. As we mentioned in Chapter 13,
the simplex method can be quite inefﬁcient on certain problems. Roughly speaking, the time
required to solve a linear program may be exponential in the size of the problem, as measured
by the number of unknowns and the amount of storage needed for the problem data. In
practice, the simplex method is much more efﬁcient than this bound would suggest, but
its poor worst-case complexity motivated the development of new algorithms with better
guaranteed performance. Among them is the ellipsoid method proposed by Khachiyan [142],
which ﬁnds a solution in time that is at worst polynomial in the problem size. Unfortunately,

394
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
this method approaches its worst-case bound on all problems and is not competitive with
the simplex method.
Karmarkar’s projective algorithm [140], announced in 1984, also has the polynomial
complexity property, but it came with the added inducement of good practical behavior.
The initial claims of excellent performance on large linear programs were never fully borne
out, but the announcement prompted a great deal of research activity and a wide array
of methods described by such labels as “afﬁne-scaling,” “logarithmic barrier,” “potential-
reduction,” “path-following,” “primal–dual,” and “infeasible interior-point.” All are related
to Karmarkar’s original algorithm, and to the log-barrier approach of Section 17.2, but many
of the approaches can be motivated and analyzed independently of the earlier methods.
Interior-pointmethodssharecommonfeaturesthatdistinguishthemfromthesimplex
method. Each interior-point iteration is expensive to compute and can make signiﬁcant
progress towards the solution, while the simplex method usually requires a larger number of
inexpensiveiterations.Thesimplexmethodworksitswayaroundtheboundaryofthefeasible
polytope, testing a sequence of vertices in turn until it ﬁnds the optimal one. Interior-point
methods approach the boundary of the feasible set only in the limit. They may approach the
solution either from the interior or the exterior of the feasible region, but they never actually
lie on the boundary of this region.
In this chapter we outline some of the basic ideas behind primal–dual interior-point
methods, including the relationship to Newton’s method and homotopy methods and the
ideas of the central path and central neighborhoods. We sketch the important methods in
this class, with an emphasis on path-following methods. We describe in detail a practical
predictor–corrector algorithm proposed by Mehrotra, which is the basis of much of the
current generation of software.
14.1
PRIMAL–DUAL METHODS
OUTLINE
We consider the linear programming problem in standard form; that is,
min cT x, subject to Ax  b, x ≥0,
(14.1)
where c and x are vectors in IRn, b is a vector in IRm, and A is an m × n matrix. The dual
problem for (14.1) is
max bT λ, subject to AT λ + s  c, s ≥0,
(14.2)
where λ is a vector in IRm and s is a vector in IRn. As shown in Chapter 13, primal–dual
solutions of (14.1), (14.2) are characterized by the Karush–Kuhn–Tucker conditions (13.4),

1 4 . 1 .
P r i m a l – D u a l M e t h o d s
395
which we restate here as follows:
AT λ + s  c,
(14.3a)
Ax  b,
(14.3b)
xisi  0, i  1, 2, . . . , n,
(14.3c)
(x, s) ≥0.
(14.3d)
Primal–dual methods ﬁnd solutions (x∗, λ∗, s∗) of this system by applying variants of New-
ton’s method to the three equalities in (14.3) and modifying the search directions and step
lengthssothattheinequalities(x, s) ≥0aresatisﬁedstrictlyateveryiteration.Theequations
(14.3a), (14.3b), (14.3c) are only mildly nonlinear and so are not difﬁcult to solve by them-
selves. However, the problem becomes much more difﬁcult when we add the nonnegativity
requirement (14.3d). The nonnegativity condition is the source of all the complications in
the design and analysis of interior-point methods.
To derive primal–dual interior-point methods, we restate the optimality conditions
(14.3) in a slightly different form by means of a mapping F from IR2n+m to IR2n+m:
F(x, λ, s) 


AT λ + s −c
Ax −b
XSe

 0,
(14.4a)
(x, s) ≥0,
(14.4b)
where
X  diag(x1, x2, . . . , xn),
S  diag(s1, s2, . . . , sn),
(14.5)
and e  (1, 1, . . . , 1)T . Primal–dual methods generate iterates (xk, λk, sk) that satisfy the
bounds (14.4b) strictly, that is, xk > 0 and sk > 0. This property is the origin of the term
interior-point. By respecting these bounds, the methods avoid spurious solutions, that is,
points that satisfy F(x, λ, s)  0 but not (x, s) ≥0. Spurious solutions abound, and do not
provide useful information about solutions of (14.1) or (14.2), so it makes sense to exclude
them altogether from the region of search.
Many interior-point methods actually require the iterates to be strictly feasible; that is,
each(xk, λk, sk)mustsatisfythelinearequalityconstraintsfortheprimalanddualproblems.
If we deﬁne the primal–dual feasible set F and strictly feasible set Fo by
F  {(x, λ, s) | Ax  b, AT λ + s  c, (x, s) ≥0},
(14.6a)
Fo  {(x, λ, s) | Ax  b, AT λ + s  c, (x, s) > 0},
(14.6b)

396
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
the strict feasibility condition can be written concisely as
(xk, λk, sk) ∈Fo.
Like most iterative algorithms in optimization, primal–dual interior-point methods
have two basic ingredients: a procedure for determining the step and a measure of the
desirability of each point in the search space. As mentioned above, the search direction
procedure has its origins in Newton’s method for the nonlinear equations (14.4a). Newton’s
method forms a linear model for F around the current point and obtains the search direction
(x, λ, s) by solving the following system of linear equations:
J(x, λ, s)


x
s
λ

 −F(x, λ, s),
where J is the Jacobian of F. (See Chapter 11 for a detailed discussion of Newton’s method
for nonlinear systems.) If the current point is strictly feasible (that is, (x, λ, s) ∈Fo), the
Newton step equations become


0
AT
I
A
0
0
S
0
X




x
λ
s




0
0
−XSe

.
(14.7)
A full step along this direction usually is not permissible, since it would violate the bound
(x, s) ≥0. To avoid this difﬁculty, we perform a line search along the Newton direction so
that the new iterate is
(x, λ, s) + α(x, λ, s),
for some line search parameter α ∈(0, 1]. Unfortunately, we often can take only a small
step along the direction (α ≪1) before violating the condition (x, s) > 0; hence, the pure
Newton direction (14.7), which is known as the afﬁne scaling direction, often does not allow
us to make much progress toward a solution.
Primal–dual methods modify the basic Newton procedure in two important ways:
1. They bias the search direction toward the interior of the nonnegative orthant (x, s) ≥
0, so that we can move further along the direction before one of the components of
(x, s) becomes negative.
2. They keep the components of (x, s) from moving “too close” to the boundary of the
nonnegative orthant.
We consider these modiﬁcations in turn.

1 4 . 1 .
P r i m a l – D u a l M e t h o d s
397
THE CENTRAL PATH
The central path C is an arc of strictly feasible points that plays a vital role in primal–
dual algorithms. It is parametrized by a scalar τ > 0, and each point (xτ, λτ, sτ) ∈C solves
the following system:
AT λ + s  c,
(14.8a)
Ax  b,
(14.8b)
xisi  τ,
i  1, 2, . . . , n,
(14.8c)
(x, s) > 0.
(14.8d)
These conditions differ from the KKT conditions only in the term τ on the right-hand-side
of (14.8c). Instead of the complementarity condition (14.3c), we require that the pairwise
products xisi have the same value for all indices i. From (14.8), we can deﬁne the central
path as
C  {(xτ, λτ, sτ) | τ > 0}.
It can be shown that (xτ, λτ, sτ) is deﬁned uniquely for each τ > 0 if and only if Fo is
nonempty. A plot of C for a typical problem, projected into the space of primal variables x,
is shown in Figure 14.1.
C
central path neighborhood
Figure14.1
Centralpath,projectedintospaceofprimalvariablesx,showingatypical
neighborhood N.

398
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
Another way of deﬁning C is to use the mapping F deﬁned in (14.4) and write
F(xτ, λτ, sτ) 


0
0
τe

,
(xτ, sτ) > 0.
(14.9)
The equations (14.8) approximate (14.3) more and more closely as τ goes to zero. If
C converges to anything as τ ↓0, it must converge to a primal–dual solution of the linear
program. The central path thus guides us to a solution along a route that steers clear of
spurious solutions by keeping all x and s components strictly positive and decreasing the
pairwise products xisi, i  1, 2, . . . , n, to zero at roughly the same rate.
Primal–dual algorithms take Newton steps toward points on C for which τ > 0,
rather than pure Newton steps for F. Since these steps are biased toward the interior of the
nonnegative orthant deﬁned by (x, s) ≥0, it usually is possible to take longer steps along
them than along the pure Newton steps for F, before violating the positivity condition.
To describe the biased search direction, we introduce a centering parameter σ ∈[0, 1]
and a duality measure µ deﬁned by
µ  1
n
n

i1
xisi  xT s
n ,
(14.10)
which measures the average value of the pairwise products xisi. By writing τ  σµ and
applying Newton’s method to the system (14.9), we obtain


0
AT
I
A
0
0
S
0
X




x
λ
s




0
0
−XSe + σµe

.
(14.11)
The step (x, λ, s) is a Newton step toward the point (xσµ, λσµ, sσµ) ∈C, at which the
pairwise products xisi are all equal to σµ. In contrast, the step (14.7) aims directly for the
point at which the KKT conditions (14.3) are satisﬁed.
If σ  1, the equations (14.11) deﬁne a centering direction, a Newton step toward the
point (xµ, λµ, sµ) ∈C, at which all the pairwise products xisi are identical to µ. Centering
directions are usually biased strongly toward the interior of the nonnegative orthant and
make little, if any, progress in reducing the duality measure µ. However, by moving closer to
C, they set the scene for substantial progress on the next iteration. (Since the next iteration
starts near C, it will be able to take a relatively long step without leaving the nonnegative
orthant.) At the other extreme, the value σ  0 gives the standard Newton step (14.7),
sometimes known as the afﬁne-scaling direction. Many algorithms use intermediate values
of σ from the open interval (0, 1) to trade off between the twin goals of reducing µ and
improving centrality.

1 4 . 1 .
P r i m a l – D u a l M e t h o d s
399
A PRIMAL–DUAL FRAMEWORK
With these basic concepts in hand, we can deﬁne a general framework for primal–dual
algorithms.
Framework 14.1 (Primal–Dual).
Given (x0, λ0, s0) ∈Fo
for k  0, 1, 2, . . .
Solve


0
AT
I
A
0
0
Sk
0
Xk




xk
λk
sk




0
0
−XkSke + σkµke

,
(14.12)
where σk ∈[0, 1] and µk  (xk)T sk/n;
Set
(xk+1, λk+1, sk+1)  (xk, λk, sk) + αk(xk, λk, sk),
(14.13)
choosing αk such that (xk+1, sk+1) > 0.
end (for).
Thechoicesofcenteringparameterσk andsteplengthαk arecrucialtotheperformance
of the method. Techniques for controlling these parameters, directly and indirectly, give rise
to a wide variety of methods with varying theoretical properties.
So far, we have assumed that the starting point (x0, λ0, s0) is strictly feasible and, in
particular, that it satisﬁes the linear equations Ax0  b, AT λ0 + s0  c. All subsequent
iterates also respect these constraints, because of the zero right-hand-side terms in (14.12).
For most problems, however, a strictly feasible starting point is difﬁcult to ﬁnd.
Infeasible-interior-point methods require only that the components of x0 and s0 be strictly
positive. The search direction needs to be modiﬁed so that it improves feasibility as well
as centrality at each iteration, but this requirement entails only a slight change to the step
equation (14.11). If we deﬁne the residuals for the two linear equations as
rb  Ax −b,
rc  AT λ + s −c,
(14.14)
the modiﬁed step equation is


0
AT
I
A
0
0
S
0
X




x
λ
s




−rc
−rb
−XSe + σµe

.
(14.15)

400
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
The search direction is still a Newton step toward the point (xσµ, λσµ, sσµ) ∈C. It tries to
correct all the infeasibility in the equality constraints in a single step. If a full step is taken
at any iteration (that is, αk  1 for some k), the residuals rb and rc become zero, and all
subsequent iterates remain strictly feasible.
PATH-FOLLOWING METHODS
Path-following algorithms explicitly restrict the iterates to a neighborhood of the
central path C and follow C to a solution of the linear program. By preventing the iterates
from coming too close to the boundary of the nonnegative orthant, they ensure that search
directions calculated from each iterate make at least some minimal amount of progress
toward the solution.
A key ingredient of any optimization algorithm is a measure of the desirability of each
point in the search space. In path-following algorithms, the duality measure µ deﬁned by
(14.10) ﬁlls this role. The duality measure µk is forced to zero as k →∞, so the iterates
(xk, λk, sk) come closer and closer to satisfying the KKT conditions (14.3).
The two most interesting neighborhoods of C are the so-called 2-norm neighborhood
N2(θ) deﬁned by
N2(θ)  {(x, λ, s) ∈Fo | ∥XSe −µe∥2 ≤θµ},
(14.16)
for some θ ∈[0, 1), and the one-sided ∞-norm neighborhood N−∞(γ ) deﬁned by
N−∞(γ )  {(x, λ, s) ∈Fo | xisi ≥γ µ
all i  1, 2, . . . , n},
(14.17)
for some γ ∈(0, 1]. (Typical values of the parameters are θ  0.5 and γ  10−3.) If
a point lies in N−∞(γ ), each pairwise product xisi must be at least some small multiple
γ of their average value µ. This requirement is actually quite modest, and we can make
N−∞(γ ) encompass most of the feasible region F by choosing γ close to zero. The N2(θ)
neighborhood is more restrictive, since certain points in Fo do not belong to N2(θ) no
matter how close θ is chosen to its upper bound of 1.
The projection of the neighborhood N onto the space of x variables for a typical
problem is shown as the region between the dotted lines in Figure 14.1.
By keeping all iterates inside one or another of these neighborhoods, path-following
methods reduce all the pairwise products xisi to zero at more or less the same rate.
Path-following methods are akin to homotopy methods for general nonlinear equa-
tions, which also deﬁne a path to be followed to the solution. Traditional homotopy methods
stay in a tight tubular neighborhood of their path, making incremental changes to the pa-
rameter and chasing the homotopy path all the way to a solution. For primal–dual methods,
this neighborhood is conical rather than tubular, and it tends to be broad and loose for larger
values of the duality measure µ. It narrows as µ →0, however, because of the positivity
requirement (x, s) > 0.

1 4 . 1 .
P r i m a l – D u a l M e t h o d s
401
The algorithm we specify below, known as a long-step path-following algorithm, can
make rapid progress because of its use of the wide neighborhood N−∞(γ ), for γ close to
zero. It depends on two parameters σmin and σmax, which are upper and lower bounds on the
centering parameter σk. The search direction is, as usual, obtained by solving (14.12), and
we choose the step length αk to be as large as possible, subject to staying inside N−∞(γ ).
Here and in later analysis, we use the notation
(xk(α), λk(α), sk(α))
def (xk, λk, sk) + α(xk, λk, sk),
(14.18a)
µk(α)
def xk(α)T sk(α)/n.
(14.18b)
Algorithm 14.2 (Long-Step Path-Following).
Given γ , σmin, σmax with γ ∈(0, 1), 0 < σmin < σmax < 1,
and (x0, λ0, s0) ∈N−∞(γ );
for k  0, 1, 2, . . .
Choose σk ∈[σmin, σmax];
Solve (14.12) to obtain (xk, λk, sk);
Choose αk as the largest value of α in [0, 1] such that
(xk(α), λk(α), sk(α)) ∈N−∞(γ );
(14.19)
Set (xk+1, λk+1, sk+1)  (xk(αk), λk(αk), sk(αk));
end (for).
Typical behavior of the algorithm is illustrated in Figure 14.2 for the case of n  2.
The horizontal and vertical axes in this ﬁgure represent the pairwise products x1s1 and x2s2,
so the central path C is the line emanating from the origin at an angle of 45◦. (A point at the
origin of this illustration is a primal–dual solution if it also satisﬁes the feasibility conditions
(14.3a), (14.3b), and (14.3d).) In the unusual geometry of Figure 14.2, the search directions
(xk, λk, sk) transform to curves rather than straight lines.
As Figure 14.2 shows (and the analysis conﬁrms), the lower bound σmin on the cen-
tering parameter ensures that each search direction starts out by moving away from the
boundary of N−∞(γ ) and into the relative interior of this neighborhood. That is, small
steps along the search direction improve the centrality. Larger values of α take us outside the
neighborhood again, since the error in approximating the nonlinear system (14.9) by the
linear step equations (14.11) becomes more pronounced as α increases. Still, we are guar-
anteed that a certain minimum step can be taken before we reach the boundary of N−∞(γ ),
as we show in the analysis below.
We provide a complete analysis of Algorithm 14.2 in Section 14.4. It shows that the
theory of primal–dual methods can be understood without recourse to profound mathe-
matics. This algorithm is fairly efﬁcient in practice, but with a few more changes it becomes
the basis of a truly competitive method, which we discuss in the next section.

402
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
x2 s2
x1 s1
iterates
0
1
2
3
central path C
N
boundary of neighborhood
Figure 14.2
Iterates of Algorithm 14.2, plotted in (xs) space.
Aninfeasible-interior-pointvariantofAlgorithm14.2canbeconstructedbygeneraliz-
ing the deﬁnition of N−∞(γ ) to allow violation of the feasibility conditions. In this extended
neighborhood, the residual norms ∥rb∥and ∥rc∥are bounded by a constant multiple of the
duality measure µ. By squeezing µ to zero, we also force rb and rc to zero, so that the iterates
approach complementarity and feasibility simultaneously.
14.2
A PRACTICAL PRIMAL–DUAL ALGORITHM
Most existing interior-point codes for general-purpose linear programming problems are
based on a predictor–corrector algorithm proposed by Mehrotra [164]. The two key features
of this algorithm are
a. addition of a corrector step to the search direction of Framework 14.1, so that the
algorithm more closely follows a trajectory to the primal–dual solution set; and
b. adaptive choice of the centering parameter σ.
The method can be motivated by “shifting” the central path C so that it starts at our
current iterate (x, λ, s) and ends, as before, at the set of solution points . We denote this

1 4 . 2 .
A P r a c t i c a l P r i m a l – D u a l A l g o r i t h m
403
C
H
Figure 14.3
Central path C, and a trajectory H from the current (noncentral) point
(x, λ, s) to the solution set .
modiﬁed trajectory by H and parametrize it by the parameter τ ∈[0, 1), so that
H  {(ˆx(τ), ˆλ(τ), ˆs(τ)) | τ ∈[0, 1)},
with (ˆx(0), ˆλ(0), ˆs(0))  (x, λ, s) and, if the limit exists as τ ↑1, we have
lim
τ↑1 (ˆx(τ), ˆλ(τ), ˆs(τ)) ∈.
The relationship of H to the central path C is depicted in Figure 14.3.
Algorithms from Framework 14.1 can be thought of as ﬁrst-order methods, in that
they ﬁnd the tangent to a trajectory like H and perform a line search along it. This tangent
is known as the predictor step. Mehrotra’s algorithm takes the next logical step of calculating
the curvature of H at the current point, thereby obtaining a second-order approximation to
this trajectory. The curvature is used to deﬁne the corrector step, and it can be obtained at
a low marginal cost, since it reuses the matrix factors from the calculation of the predictor
step.
The second important feature of Mehrotra’s algorithm is that it chooses the centering
parameter σ adaptively, in contrast to algorithms from Framework 14.1, which assign a
value to σk prior to calculating the search direction. At each iteration, Mehrotra’s algorithm
ﬁrst calculates the afﬁne-scaling direction (the predictor step) and assesses its usefulness
as a search direction. If this direction yields a large reduction in µ without violating the
positivity condition (x, s) > 0, the algorithm concludes that little centering is needed, so
it chooses σ close to zero and calculates a centered search direction with this small value. If
the afﬁne-scaling direction is not so productive, the algorithm enforces a larger amount of
centering by choosing a value of σ closer to 1.

404
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
The algorithm thus combines three steps to form the search direction: a predictor step,
which allows us to determine the centering parameter σk, a corrector step using second-order
information of the path H leading toward the solution, and a centering step in which the
chosen value of σk is substituted in (14.15). We will see that computation of the centered
direction and the corrector step can be combined, so adaptive centering does not add further
to the cost of each iteration.
In concrete terms, the computation of the search direction (x, λ, s) proceeds
as follows. First, we calculate the predictor step (xaff, λaff, saff) by setting σ  0 in
(14.15), that is,


0
AT
I
A
0
0
S
0
X




xaff
λaff
saff




−rc
−rb
−XSe

.
(14.20)
To measure the effectiveness of this direction, we ﬁnd αpri
aff and αdual
aff
to be the longest step
lengths that can be taken along this direction before violating the nonnegativity conditions
(x, s) ≥0, with an upper bound of 1. Explicit formulae for these values are as follows:
αpri
aff
def min
#
1, min
i:xaff
i <0 −
xi
xaff
i
$
,
(14.21a)
αdual
aff
def min
#
1, min
i:saff
i <0 −
si
saff
i
$
.
(14.21b)
We deﬁne µaff to be the value of µ that would be obtained by a full step to the boundary,
that is,
µaff  (x + αpri
aff xaff)T (s + αpri
aff saff)/n,
(14.22)
and set the damping parameter σ to be
σ 
µaff
µ
3
.
It is easy to see that this choice has the effect mentioned above: When good progress is made
along the predictor direction, we have µaff ≪µ, so the σ obtained from this formula is
small; and conversely.
The corrector step is obtained by replacing the right-hand-side in (14.20) by (0, 0,
−Xaff, Saffe), while the centering step requires a right-hand-side of (0, 0, σµe). We can
obtain the complete Mehrotra step, which includes the predictor, corrector, and centering
step components, by adding the right-hand-sides for these three components and solving

1 4 . 2 .
A P r a c t i c a l P r i m a l – D u a l A l g o r i t h m
405
the following system:


0
AT
I
A
0
0
S
0
X




x
λ
s




−rc
−rb
−XSe −XaffSaffe + σµe

.
(14.23)
We calculate the maximum steps that can be taken along these directions before violating
the nonnegativity condition (x, s) > 0 by formulae similar to (14.21); namely,
αpri
max
def min

1, min
i:xi<0 −xk
i
xi

,
(14.24a)
αdual
max
def min

1, min
i:si<0 −sk
i
si

,
(14.24b)
and then choose the primal and dual step lengths as follows:
αpri
k
 min(1, ηαpri
max),
αdual
k
 min(1, ηαdual
max),
(14.25)
where η ∈[0.9, 1.0) is chosen so that η →1 near the solution, to accelerate the asymptotic
convergence. (For details of the choice of η and other elements of the algorithm such as the
choice of starting point (x0, λ0, s0) see Mehrotra [164].)
We summarize this discussion by specifying Mehrotra’s algorithm in the usual format.
Algorithm 14.3 (Mehrotra Predictor–Corrector Algorithm).
Given (x0, λ0, s0) with (x0, s0) > 0;
for k  0, 1, 2, . . .
Set (x, λ, s)  (xk, λk, sk) and solve (14.20) for (xaff, λaff, saff);
Calculate αpri
aff , αdual
aff , and µaff as in (14.21) and (14.22);
Set centering parameter to σ  (µaff/µ)3;
Solve (14.23) for (x, λ, s);
Calculate αpri
k
and αdual
k
from (14.25);
Set
xk+1  xk + αpri
k x,
(λk+1, sk+1)  (λk, sk) + αdual
k
(λ, s);
end (for).
ItisimportanttonotethatnoconvergencetheoryisavailableforMehrotra’salgorithm,
at least in the form in which it is described above. In fact, there are examples for which the
algorithm diverges. Simple safeguards could be incorporated into the method to force it into

406
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
theconvergenceframeworkofexistingmethods.However,mostprogramsdonotimplement
these safeguards, because the good practical performance of Mehrotra’s algorithm makes
them unnecessary.
SOLVING THE LINEAR SYSTEMS
Most of the computational effort in primal–dual methods is taken up in solving linear
systems such as (14.15), (14.20), and (14.23). The coefﬁcient matrix in these systems is
usually large and sparse, since the constraint matrix A is itself large and sparse in most
applications. The special structure in the step equations allows us to reformulate them as
systems with more compact symmetric coefﬁcient matrices, which are easier and cheaper to
factor than the original form.
The reformulation procedures are simple, as we show by applying them to the system
(14.15). Since the current point (x, λ, s) has x and s strictly positive, the diagonal matrices
X and S are nonsingular. Hence, by eliminating s from (14.15), we obtain the following
equivalent system:

0
A
AT
−D−2
 
λ
x



−rb
−rc + s −σµX−1e

,
(14.26a)
s  −s + σµX−1e −X−1Sx,
(14.26b)
where we have introduced the notation
D  S−1/2X1/2.
(14.27)
This form of the step equations usually is known as the augmented system. Since the matrix
X−1S is also diagonal and nonsingular, we can go a step further, eliminating x from
(14.26a) to obtain another equivalent form:
AD2AT λ  −rb + A(−S−1Xrc + x −σµS−1e),
(14.28a)
s  −rc −AT λ,
(14.28b)
x  −x + σµS−1e −S−1Xs.
(14.28c)
This form often is called the normal-equations form because the system (14.28a) can be
viewed as the normal equations (10.11) for a linear least-squares problem with coefﬁcient
matrix DAT .
Most implementations of primal–dual methods are based on the system (14.28). They
use direct sparse Cholesky algorithms to factor the matrix AD2AT , and then perform trian-
gular solves with the resulting sparse factors to obtain the step λ from (14.28a). The steps
s and x are recovered from (14.28b) and (14.28c). General-purpose sparse Cholesky
software can be applied to AD2AT , but a few modiﬁcations are needed because AD2AT

1 4 . 3 .
O t h e r P r i m a l – D u a l A l g o r i t h m s a n d E x t e n s i o n s
407
may be ill-conditioned or singular. Ill conditioning is often observed during the ﬁnal stages
of a primal–dual algorithm, when the elements of the diagonal weighting matrix D2 take on
both huge and tiny values.
The formulation (14.26) has received less attention than (14.28), mainly because algo-
rithmsandsoftwareforfactoringsparsesymmetricindeﬁnitematricesaremorecomplicated,
slower, and less prevalent than sparse Cholesky algorithms. Nevertheless, the formulation
(14.26) is cleaner and more ﬂexible than(14.28) in a number of respects. Fill-in of the co-
efﬁcient matrix in (14.28) caused by dense columns in A does not occur, and free variables
(that is, components of x with no explicit lower or upper bounds) can be handled without
resorting to the various artiﬁcial devices needed in the normal-equations form.
14.3
OTHER PRIMAL–DUAL ALGORITHMS AND EXTENSIONS
OTHER PATH-FOLLOWING METHODS
Framework 14.1 is the basis of a number of other algorithms of the path-following
variety. They are less important from a practical viewpoint, but we mention them here
because of their elegance and their strong theoretical properties.
Some path-following methods choose conservative values for the centering parameter
σ (that is, σ only slightly less than 1) so that unit steps α  1 can be taken along the resulting
direction from (14.11) without leaving the chosen neighborhood. These methods, which are
known as short-step path-following methods, make only slow progress toward the solution
because they require the iterates to stay inside a restrictive N2 neighborhood (14.16).
Better results are obtained with the predictor–corrector method, due to Mizuno, Todd,
and Ye [165], which uses two N2 neighborhoods, nested one inside the other. (Despite the
similar terminology, this algorithm is quite distinct from Algorithm 14.3 of Section 14.2.)
Every second step of this method is a predictor step, which starts in the inner neighborhood
and moves along the afﬁne-scaling direction (computed by setting σ  0 in (14.11)) to
the boundary of the outer neighborhood. The gap between neighborhood boundaries is
wide enough to allow this step to make signiﬁcant progress in reducing µ. Alternating with
the predictor steps are corrector steps (computed with σ  1 and α  1), which take the
next iterate back inside the inner neighborhood in preparation for the subsequent predictor
step. The predictor–corrector algorithm produces a sequence of duality measures µk that
converge superlinearly to zero, in contrast to the linear convergence that characterizes most
methods.
POTENTIAL-REDUCTION METHODS
Potential-reduction methods take steps of the same form as path-following methods,
but they do not explicitly follow the central path C and can be motivated independently of
it. They use a logarithmic potential function to measure the worth of each point in Fo and

408
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
aim to achieve a certain ﬁxed reduction in this function at each iteration. The primal–dual
potential function, which we denote generically by , usually has two important properties:
 →∞if xisi →0 for some i, while µ  xT s/n ̸→0,
(14.29a)
 →−∞if and only if (x, λ, s) →.
(14.29b)
The ﬁrst property (14.29a) stops any one of the pairwise products xisi from approaching
zero independently of the others, and therefore keeps the iterates away from the boundary
of the nonnegative orthant. The second property (14.29b) relates  to the solution set .
If our algorithm forces  to −∞, then (14.29b) ensures that the sequence approaches the
solution set.
An interesting primal–dual potential function is the Tanabe–Todd–Ye function ρ
deﬁned by
ρ(x, s)  ρ log xT s −
n

i1
log xisi,
(14.30)
for some parameter ρ > n (see [233], [236]). Like all algorithms based on Framework 14.1,
potential-reduction algorithms obtain their search directions by solving (14.12), for some
σk ∈(0, 1), and they take steps of length αk along these directions. For instance, the step
length αk may be chosen to approximately minimize ρ along the computed direction. The
“cookbook” value σk ≡n/(n + √n) is sufﬁcient to guarantee constant reduction in ρ at
every iteration. Hence, ρ will approach −∞, forcing convergence. Adaptive and heuristic
choices of σk and αk are also covered by the theory, provided that they at least match the
reduction in ρ obtained from the conservative theoretical values of these parameters.
EXTENSIONS
Primal–dualmethodsforlinearprogrammingcanbeextendedtowiderclassesofprob-
lems. There are simple extensions of the algorithm to the monotone linear complementarity
problem (LCP) and convex quadratic programming problems for which the convergence
and polynomial complexity properties of the linear programming algorithms are retained.
The LCP is the problem of ﬁnding vectors x and s in IRn that satisfy the following conditions:
s  Mx + q,
(x, s) ≥0,
xT s  0,
(14.31)
where M is a positive semideﬁnite n × n matrix and q ∈IRn. The similarity between (14.31)
and the KKT conditions (14.3) is obvious: The last two conditions in (14.31) correspond to
(14.3d) and (14.3c), respectively, while the condition s  Mx +q is similar to the equations
(14.3a) and (14.3b). For practical instances of the problem (14.31), see Cottle, Pang, and
Stone [59].

1 4 . 4 .
A n a l y s i s o f A l g o r i t h m 1 4 . 2
409
In convex quadratic programming, we minimize a convex quadratic objective subject
to linear constraints. A convex quadratic generalization of the standard form linear program
(14.1) is
min cT x + 1
2xT Gx, subject to Ax  b, x ≥0,
(14.32)
where G is a symmetric n × n positive semideﬁnite matrix. The KKT conditions for this
problem are similar to (14.3) and the linear complementarity problem (14.31). In fact, we
can show that any LCP can be formulated as a convex quadratic program, and vice versa.
See Section 16.7 for further discussion of interior-point methods for (14.32).
Extensions of primal–dual methods to nonlinear programming problems can be de-
vised by writing down the KKT conditions in a form similar to (14.3), adding slack variables
where necessary to convert all the inequality conditions to simple bounds of the type (14.3d).
As in Framework 14.1, the basic primal–dual step is found by applying Newton’s method
to the equality KKT conditions, curtailing the step length to ensure that the bounds are
satisﬁed strictly by all iterates. When the nonlinear programming problem is convex (that
is, its objective and constraint functions are all convex functions), global convergence of
primal–dual methods is not too difﬁcult to prove. Extensions to general nonlinear program-
ming problems are not so straightforward; we discuss some approaches under investigation
in Section 17.2.
Interior-point methods are proving to be effective in semideﬁnite programming, a class
of problems in which some of the variables are symmetric matrices that are constrained to
be positive semideﬁnite. This class, which has been the topic of concentrated research since
the early 1990s, has applications in many areas, including control theory and combinatorial
optimization. Further information on this increasingly important topic can be found in
Nesterov and Nemirovskii [181], Boyd et al. [26], and Vandenberghe and Boyd [240].
14.4
ANALYSIS OF ALGORITHM 14.2
We now present a detailed analysis of Algorithm 14.2. As is typical in interior-point methods,
the analysis builds from a purely technical lemma to a powerful theorem in just a few pages.
We start with the technical lemma—Lemma 14.1—and use it to prove Lemma 14.2, a bound
on the vector of pairwise products xisi, i  1, 2, . . . , n. Theorem 14.3 ﬁnds a lower
bound on αk and a corresponding estimate of the reduction in µ at each iteration. Global
convergence is an immediate consequence of this result. Theorem 14.4 goes a step further,
proving that O(n) iterations are required to identify a point for which µk < ϵ, for a given
tolerance ϵ ∈(0, 1).

410
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
Lemma 14.1.
Let u and v be any two vectors in IRn with uT v ≥0. Then
∥UV e∥≤2−3/2∥u + v∥2,
where
U  diag(u1, u2, . . . , un),
V  diag(v1, v2, . . . , vn).
Proof.
First, note that for any two scalars α and β with αβ ≥0, we have from the algebraic-
geometric mean inequality that

|αβ| ≤1
2|α + β|.
(14.33)
Since uT v ≥0, we have
0 ≤uT v 

uivi≥0
uivi +

uivi<0
uivi 

i∈P
|uivi| −

i∈M
|uivi|,
(14.34)
where we partitioned the index set {1, 2, . . . , n} as
P  {i | uivi ≥0},
M  {i | uivi < 0}.
Now,
∥UV e∥
	∥[uivi]i∈P∥2 + ∥[uivi]i∈M∥2
1/2
≤
	∥[uivi]i∈P∥2
1 + ∥[uivi]i∈M∥2
1

1/2
since ∥· ∥2 ≤∥· ∥1
≤
	
2 ∥[uivi]i∈P∥2
1

1/2
from (14.34)
≤
√
2

1
4(ui + vi)2

i∈P

1
from (14.33)
 2−3/2 
i∈P
(ui + vi)2
≤2−3/2
n

i1
(ui + vi)2
≤2−3/2∥u + v∥2,
completing the proof.
□
Lemma 14.2.
If (x, λ, s) ∈N−∞(γ ), then
∥XSe∥≤2−3/2(1 + 1/γ )nµ.

1 4 . 4 .
A n a l y s i s o f A l g o r i t h m 1 4 . 2
411
Proof.
It is easy to show using (14.12) that
xT s  0
(14.35)
(see the exercises). By multiplying the last block row in (14.12) by (XS)−1/2 and using the
deﬁnition D  X1/2S−1/2, we obtain
D−1x + Ds  (XS)−1/2(−XSe + σµe).
(14.36)
Because (D−1x)T (Ds)  xT s  0, we can apply Lemma 14.1 with u  D−1x
and v  Ds to obtain
∥XSe∥ ∥(D−1X)(DS)e∥
≤2−3/2∥D−1x + Ds∥2
from Lemma 14.1
 2−3/2∥(XS)−1/2(−XSe + σµe)∥2
from (14.36).
Expanding the squared Euclidean norm and using such relationships as xT s  nµ and
eT e  n, we obtain
∥XSe∥≤2−3/2

xT s −2σµeT e + σ 2µ2
n

i1
1
xisi

≤2−3/2

xT s −2σµeT e + σ 2µ2 n
γ µ

since xisi ≥γ µ
≤2−3/2

1 −2σ + σ 2
γ

nµ
≤2−3/2(1 + 1/γ )nµ,
as claimed.
□
Theorem 14.3.
Given the parameters γ , σmin, and σmax in Algorithm 14.2, there is a constant δ
independent of n such that
µk+1 ≤

1 −δ
n

µk,
(14.37)
for all k ≥0.
Proof.
We start by proving that
	
xk(α), λk(α), sk(α)

∈N−∞(γ ) for all α ∈

0, 23/2γ 1 −γ
1 + γ
σk
n

,
(14.38)

412
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
where
	
xk(α), λk(α), sk(α)

is deﬁned as in (14.18). It follows that αk is bounded below as
follows:
αk ≥23/2 σk
n γ 1 −γ
1 + γ .
(14.39)
For any i  1, 2, . . . , n, we have from Lemma 14.2 that
|xk
i sk
i | ≤∥XkSke∥2 ≤2−3/2(1 + 1/γ )nµk.
(14.40)
Using (14.12), we have from xk
i sk
i ≥γ µk and (14.40) that
xk
i (α)sk
i (α) 
	
xk
i + αxk
i

 	
sk
i + αsk
i

 xk
i sk
i + α
	
xk
i sk
i + sk
i xk
i

+ α2xk
i sk
i
≥xk
i sk
i (1 −α) + ασkµk −α2|xk
i sk
i |
≥γ (1 −α)µk + ασkµk −α22−3/2(1 + 1/γ )nµk.
By summing the n components of the equation Skxk + Xksk  −XkSke + σkµke (the
third block row from (14.12)), and using (14.35) and the deﬁnition of µk and µk(α) (see
(14.18)), we obtain
µk(α)  (1 −α(1 −σk))µk.
From these last two formulas, we can see that the proximity condition
xk
i (α)sk
i (α) ≥γ µk(α)
is satisﬁed, provided that
γ (1 −α)µk + ασkµk −α22−3/2(1 + 1/γ )nµk ≥γ (1 −α + ασk)µk.
Rearranging this expression, we obtain
ασkµk(1 −γ ) ≥α22−3/2nµk(1 + 1/γ ),
which is true if
α ≤23/2
n σkγ 1 −γ
1 + γ .
We have proved that
	
xk(α), λk(α), sk(α)

satisﬁes the proximity condition for N−∞(γ )
whenα liesintherangestatedin(14.38).Itisnotdifﬁculttoshowthat
	
xk(α), λk(α), sk(α)

∈

1 4 . 4 .
A n a l y s i s o f A l g o r i t h m 1 4 . 2
413
Fo forallα inthegivenrange(seetheexercises).Hence,wehaveproved(14.38)andtherefore
(14.39).
We complete the proof of the theorem by estimating the reduction in µ on the kth
step. Because of (14.35), (14.39), and the last block row of (14.11), we have
µk+1  xk(αk)T sk(αk)/n


(xk)T sk + αk
	
(xk)T sk + (sk)T xk
+ α2
k(xk)T sk
/n
(14.41)
 µk + αk
	
−(xk)T sk/n + σkµk

(14.42)
 (1 −αk(1 −σk))µk
(14.43)
≤

1 −23/2
n γ 1 −γ
1 + γ σk(1 −σk)

µk.
(14.44)
Now, the function σ(1 −σ) is a concave quadratic function of σ, so on any given interval
it attains its minimum value at one of the endpoints. Hence, we have
σk(1 −σk) ≥min {σmin(1 −σmin), σmax(1 −σmax)} , for all σk ∈[σmin, σmax].
The proof is completed by substituting this estimate into (14.44) and setting
δ  23/2γ 1 −γ
1 + γ min {σmin(1 −σmin), σmax(1 −σmax)} .
□
We conclude with the complexity result.
Theorem 14.4.
Given ϵ > 0 and γ ∈(0, 1), suppose the starting point (x0, λ0, s0) ∈N−∞(γ ) in
Algorithm 14.2 has
µ0 ≤1/ϵκ
(14.45)
for some positive constant κ. Then there is an index K with K  O(n log 1/ϵ) such that
µk ≤ϵ,
for all k ≥K.
Proof.
By taking logarithms of both sides in (14.37), we obtain
log µk+1 ≤log

1 −δ
n

+ log µk.
By repeatedly applying this formula and using (14.45), we have
log µk ≤k log

1 −δ
n

+ log µ0 ≤k log

1 −δ
n

+ κ log 1
ϵ .

414
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
The following well-known estimate for the log function,
log(1 + β) ≤β,
for all β > −1,
implies that
log µk ≤k

−δ
n

+ κ log 1
ϵ .
Therefore, the convergence criterion µk ≤ϵ is satisﬁed if we have
k

−δ
nω

+ κ log 1
ϵ ≤log ϵ.
This inequality holds for all k that satisfy
k ≥K  (1 + κ)nω
δ log 1
ϵ ,
so the proof is complete.
□
NOTES AND REFERENCES
For more details on the material of this chapter, see the book by Wright [255].
As noted earlier, Karmarkar’s method arose from a search for linear programming
algorithms with better worst-case behavior than the simplex method. The ﬁrst algorithm
with polynomial complexity, Khachiyan’s ellipsoid algorithm [142], was a computational
disappointment, but the execution times required by Karmarkar’s method were not too
much greater than simplex codes at the time of its introduction, particularly for large linear
programs. Karmarkar’s is a primal algorithm; that is, it is described, motivated, and im-
plemented purely in terms of the primal problem (14.1) without reference to the dual. At
each iteration, Karmarkar’s algorithm performs a projective transformation on the primal
feasible set that maps the current iterate xk to the center of the set and takes a step in the
feasible steepest descent direction for the transformed space. Progress toward optimality is
measured by a logarithmic potential function. Nice descriptions of the algorithm can be
found in Karmarkar’s original paper [140] and in Fletcher [83].
Karmarkar’s method falls outside the scope of this chapter, and in any case, its practical
performance does not appear to match the most efﬁcient primal–dual methods. The algo-
rithmswediscussedinthischapter—path-following,potential-reduction—havepolynomial
complexity, like Karmarkar’s method.
Historians of interior-point methods point out that many of the ideas that have been
examined since 1984 actually had their genesis in three works that preceded Karmarkar’s
paper. The ﬁrst of these is the book of Fiacco and McCormick [79] on logarithmic barrier

1 4 . 4 .
A n a l y s i s o f A l g o r i t h m 1 4 . 2
415
functions, which proves existence of the central path, among many other results. Further
analysis of the central path was carried out by McLinden [162], in the context of nonlinear
complementarity problems. Finally, there is Dikin’s paper [72], in which an interior-point
method known as primal afﬁne-scaling was originally proposed. The outburst of research on
primal–dual methods, which culminated in the efﬁcient software packages available today,
dates to the seminal paper of Megiddo [163].
Todd gives an excellent survey of potential reduction methods in [235]. He relates
the primal–dual potential reduction method mentioned above to pure primal potential
reduction methods, including Karmarkar’s original algorithm, and discusses extensions to
special classes of nonlinear problems.
For an introduction to complexity theory and its relationship to optimization, see the
book by Vavasis [241].
Interior-pointsoftwareforlinearprogrammingisnowwidelyavailable.Mostoftheim-
plementations are based on Algorithm 14.3, with additional “higher-order correction” steps
proposed by Gonzdio [116]. Because this software is typically less complex than simplex-
based codes, some of it is freely available for research and even for commercial purposes. See
the web page for this book for further information.
✐
E x e r c i s e s
✐
14.1 Thisexerciseillustratesthefactthatthebounds(x, s) ≥0areessentialinrelating
solutionsofthesystem(14.4a)tosolutionsofthelinearprogram(14.1)anditsdual.Consider
the following linear program in IR2:
min x1, subject to x1 + x2  1,
(x1, x2) ≥0.
Show that the primal–dual solution is
x∗
#
0
1
$
,
λ∗ 0,
s∗
#
1
0
$
.
Also verify that the system F(x, λ, s)  0 has the spurious solution
x 
#
1
0
$
,
λ  1,
s 
#
0
−1
$
,
which has no relation to the solution of the linear program.

416
C h a p t e r
1 4 .
I n t e r i o r - P o i n t M e t h o d s
✐
14.2
(i) Show that N2(θ1) ⊂N2(θ2) when 0 ≤θ1 < θ2 < 1 and that N−∞(γ1) ⊂N−∞(γ2)
for 0 < γ2 ≤γ1 ≤1.
(ii) Show that N2(θ) ⊂N−∞(γ ) if γ ≤1 −θ.
✐
14.3 Given an arbitrary point (x, λ, s) ∈Fo, ﬁnd the range of γ values for which
(x, λ, s) ∈N−∞(γ ). (The range depends on x and s.)
✐
14.4 For n  2, ﬁnd a point (x, s) > 0 for which the condition
∥XSe −µe∥2 ≤θµ
is not satisﬁed for any θ ∈[0, 1).
✐
14.5 Prove that the neighborhoods N−∞(1) (see (14.17)) and N2(0) (see (14.16))
coincide with the central path C.
✐
14.6 Show that ρ deﬁned by (14.30) has the property (14.29a).
✐
14.7 Prove that the coefﬁcient matrix in (14.11) is nonsingular if and only if A has
full row rank.
✐
14.8 Given (x, λ, s) satisfying (14.12), prove (14.35).
✐
14.9 Given that X and S are diagonal with positive diagonal elements, show that the
coefﬁcient matrix in (14.28a) is symmetric and positive deﬁnite if and only if A has full row
rank. Does this result continue to hold if we replace D by a diagonal matrix in which exactly
m of the diagonal elements are positive and the remainder are zero? (Here m is the number
of rows of A.)
✐
14.10 Given a point (x, λ, s) with (x, s) > 0, consider the trajectory H deﬁned by
F

ˆx(τ), ˆλ(τ), ˆs(τ)




(1 −τ)(AT λ + s −c)
(1 −τ)(Ax −b)
(1 −τ)XSe

,
(ˆx(τ), ˆs(τ)) ≥0,
for τ
∈
[0, 1], and note that

ˆx(0), ˆλ(0), ˆs(0)


(x, λ, s), while the limit of

ˆx(τ), ˆλ(τ), ˆs(τ)

as τ ↑1 will lie in the primal–dual solution set of the linear program.
Find equations for the ﬁrst, second, and third derivatives of H with respect to τ at τ  0.
Hence, write down a Taylor series approximation to H near the point (x, λ, s).
✐
14.11 Consider the following linear program, which contains “free variables” denoted
by y:
min cT x + dT y, subject to A1x + A2y  b, x ≥0.

1 4 . 4 .
A n a l y s i s o f A l g o r i t h m 1 4 . 2
417
By introducing Lagrange multipliers λ for the equality constraints and s for the bounds
x ≥0, write down optimality conditions for this problem in an analogous fashion to (14.3).
Following (14.4) and (14.11), use these conditions to derive the general step equations for
a primal–dual interior-point method. Express these equations in augmented system form
analogously to (14.26) and explain why it is not possible to reduce further to a formulation
like (14.28) in which the coefﬁcient matrix is symmetric positive deﬁnite.
✐
14.12 Program Algorithm 14.3 in Matlab. Choose η  0.99 uniformly in (14.25).
Test your code on a linear programming problem (14.1) generated by choosing A randomly,
and then setting x, s, b, and c as follows:
xi 

random positive number
i  1, 2, . . . , m,
0
i  m + 1, m + 2, . . . , n,
si 

random positive number
i  m + 1, m + 2, . . . , n
0
i  1, 2, . . . , m,
λ  random vector,
c  AT λ + s,
b  Ax.
Choose the starting point (x0, λ0, s0) with the components of x0 and s0 set to large positive
values.

Chapter15

Fundamentals
of Algorithms
for Nonlinear
Constrained
Optimization
We now begin our discussion of algorithms for solving the general constrained optimization
problem
min
x∈IRn f (x)
subject to ci(x)  0,
i ∈E,
ci(x) ≥0,
i ∈I,
(15.1)
where the objective function f and the constraint functions ci are all smooth, real-valued
functions on a subset of IRn, and I and E are ﬁnite index sets of inequality and equality
constraints, respectively. In Chapter 12 we used this general statement of the problem to
derive optimality conditions that characterize its solutions. This theory is useful for moti-
vating algorithms, but to be truly efﬁcient, the algorithm must take account of the particular
properties and structure of the objective and constraint functions. There are many impor-
tant special cases of (15.1) for which specialized algorithms are available. They include the
following:
Linearprogramming,wheretheobjectivefunction f andalltheconstraintsci arelinear
functions. This problem can be solved by the techniques of Chapters 13 and 14.

420
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
Quadratic programming, where the constraints ci are linear and the objective function
f is quadratic. Algorithms for solving this problem are discussed in Chapter 16.
Nonlinear programming, where at least some of the constraints ci are general nonlinear
functions.
Linearly constrained optimization where, as the name suggests, all the constraints ci
are linear.
Bound-constrained optimization, where the only constraints in the problem have the
form xi ≥li or xi ≤ui, where li and ui are lower and upper bounds on the ith
component of x.
Convexprogramming,wheretheobjectivefunctionf isconvex,theequalityconstraints
ci(x)  0, i ∈E, are linear, and the inequality constraint functions ci(x), i ∈I, are
concave.
These categories are neither mutually exclusive nor exhaustive, and some of the classes
can be divided into important subclasses. For instance, convex quadratic programming is
a subclass of quadratic programming in which the objective function is convex. This ﬁner
characterization is relevant to the discussion of algorithms; for example, it is easier to choose
a merit function if the problem is convex.
The constrained optimization algorithms described in the following chapters are it-
erative in nature. They generate a sequence of guesses for the solution x∗that, we hope,
tend towards a solution. They may also generate a sequence of guesses for the Lagrange
multipliers associated with the constraints. In deciding how to move from one iterate to the
next, the methods use information about the objective and constraint functions and their
derivatives, possibly combined with information gathered at earlier iterations and earlier
stages of the algorithm. They terminate when they have either identiﬁed an approximate
solution or when further progress seems to be impossible.
In this chapter we discuss some of the fundamental issues in the design of algorithms
for nonlinear constrained optimization problems. As in the chapters on unconstrained opti-
mization, we will only study algorithms for ﬁnding local minimizers for (15.1); the problem
of ﬁnding a global minimizer is outside the scope of this book.
INITIAL STUDY OF A PROBLEM
Before solving a constrained optimization problem by means of one of the algorithms
described in this book, it is useful ﬁrst to study the problem to see whether a simpliﬁcation
is possible. In some cases, it is possible to ﬁnd a solution without use of a computer. For
example, an examination of the constraints may show that the feasible region is empty or
that the objective function is not bounded in the feasible region (see Exercise 1).
It may also be possible to solve the KKT conditions (12.30) directly, as was done in
some examples in Chapter 12, by guessing which of the inequality constraints are active at

C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
421
the solution. Knowledge of the active constraints reduces the KKT conditions to a system of
equations that can be solved directly. This approach, however, is rarely practical. Even if the
active constraints can be identiﬁed—normally the most difﬁcult issue facing a constrained
optimization algorithm—we would still need to solve the resulting system of equations
numerically. As we see in Chapter 11, algorithms for nonlinear equations are not guaranteed
to ﬁnd a solution from arbitrary starting points. Therefore, we need nonlinear optimization
algorithms that directly address general problems of the form (15.1).
Once we are ready to solve the problem using an optimization algorithm, we must
decide under which of the categories listed above it falls. If the problem contains discrete
variables (for instance, binary variables for which the only permissible values are 0 and 1), we
cannot use the techniques of this book but must resort to discrete optimization algorithms.
See the algorithms described in Wolsey [249] and Nemhauser and Wolsey [179].
Attentionshouldalsobegiventothenatureoftheconstraints.Somemaybeconsidered
“hard” and others “soft” constraints. From the algorithmic point of view, hard constraints
are those that must be satisﬁed in order that the functions and constraints in (15.1) be
meaningful. Some of these functions may not even be deﬁned at infeasible points. For
instance, a variable is constrained to be positive because its square root is required in the
calculation of the objective function. Another example is a problem in which all the variables
must sum to zero to satisfy some conservation law.
Constrained optimization problems with soft constraints are sometimes recast by the
modeler as an unconstrained problem in which a penalty term including the constraints is
added to the objective function. As we will see in the next chapter, this penalty approach
usually introduces ill-conditioning, which may or may not be harmful depending on the
algorithm used for the unconstrained optimization. The user of optimization algorithms
must decide whether it is preferable to use one of the approaches in which the constraints
are treated explicitly or whether a penalty approach is adequate.
For problems with hard constraints that must be satisﬁed at all iterates, we must use
feasible algorithms. Usually not all the constraints are hard, and therefore these algorithms
choose an initial point that satisﬁes the hard constraints and produce a new iterate that is also
feasible for these constraints. Feasible algorithms are usually slower and more expensive than
algorithms that allow the iterates to be infeasible, since they cannot follow shortcuts to the
solution that cross infeasible territory. However, they have the advantage that the objective
function f can be used to judge the merit of each point. Since the constraints are always
satisﬁed, there is no need to introduce a more complex merit function that takes account of
the constraint violations.

422
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
15.1
CATEGORIZING OPTIMIZATION ALGORITHMS
We now outline the ideas presented in the rest of the book. There is no “standard taxonomy”
for nonlinear optimization algorithms; in the remaining chapters we have grouped the
various approaches as follows.
I In Chapter 17 we discuss penalty, barrier, and augmented Lagrangian methods, and also
provide a brief description of sequential linearly constrained methods. We now give a
brief overview of each of these approaches.
By combining the objective function and constraints into a penalty function we can
attack problem (15.1) by solving a sequence of unconstrained problems. For example,
if only equality constraints are present in (15.1), we can deﬁne the penalty function as
f (x) + 1
2µ

i∈E
c2
i (x),
(15.2)
where µ > 0 is referred to as a penalty parameter. We then minimize this unconstrained
function, for a series of increasing values of µ, until the solution of the constrained
optimization problem is identiﬁed to sufﬁcient accuracy.
If we use an exact penalty function, it may be possible to solve (15.1) with one single
unconstrained minimization. For the equality constrained problem, an exact penalty
function is obtained by setting
f (x) + 1
µ

i∈E
|ci(x)|,
forsomesufﬁcientlysmall(butpositive)choiceofµ.Often,however,exactpenaltyfunc-
tions are nondifferentiable, and to minimize them requires the solution of a sequence
of subproblems.
In barrier methods, we add terms to the objective that are insigniﬁcant when x is safely
in the interior of the feasible set but approach zero as x approaches the boundary. For
instance, if only inequality constraints are present in (15.1), the logarithmic barrier
function has the form
f (x) −µ

i∈I
log ci(x),
where µ > 0 is now referred to as a barrier parameter. The minimizers of this function
can be shown to approach solutions of the original constrained problem as µ ↓0, under
certain conditions. Again, the usual strategy is to ﬁnd approximate minimizers for a
decreasing sequence of values of µ.
In augmented Lagrangian methods, we deﬁne a function that combines the properties
of the Lagrangian function (12.28) and the quadratic penalty function (15.2). This

1 5 . 1 .
C a t e g o r i z i n g O p t i m i z a t i o n A l g o r i t h m s
423
so-called augmented Lagrangian function has the following form, when only equality
constraints are present in the problem (15.1):
LA(x, λ; µ)  f (x) −

i∈E
λici(x) + 1
2µ

i∈E
c2
i (x).
Methods based on this function proceed by ﬁxing λ to some estimate of the optimal
Lagrange multipliers and µ > 0 to some positive value, and then ﬁnding a value of
x that approximately minimizes LA. This new x iterate is then used to update λ, µ
may be decreased, and the process is repeated. We show later that this approach avoids
certainnumericaldifﬁcultiesassociatedwiththeminimizationofthepenaltyandbarrier
functions discussed above.
In sequential linearly constrained methods we minimize, at every iteration, a certain
Lagrangian function subject to a linearization of the constraints. These methods have
been used mainly to solve large problems.
II In Chapter 18 we describe methods based on sequential quadratic programming. Here
the idea is to model (15.1) by a quadratic subproblem at each iterate and to deﬁne the
search direction as the solution of this subproblem. More speciﬁcally, in the case of
equality constraints in (15.1), we deﬁne the search direction pk at the iterate (xk, λk) to
be the solution of
min
p
1
2pT Wkp + ∇f T
k p
(15.3a)
subject to Akp + ck  0.
(15.3b)
The objective in this subproblem is an approximation of the Lagrangian function and
the constraints are linearizations of the constraints in (15.1). The new iterate is obtained
by searching along this direction until a certain merit function is decreased.
Sequential quadratic programming methods have proved to be effective in practice.
They are the basis of some of the best software for solving both small and large con-
strained optimization problems. They typically require fewer function evaluations than
some of the other methods, at the expense of solving a relatively complicated quadratic
subproblem at each iteration.
III In Chapter 16 we consider algorithms for solving quadratic programming problems. We
consider this category separately because of its importance and because algorithms
can be tailored to its particular characteristics. We discuss active set and interior-point
methods. Active set quadratic programming methods are the basis for the sequential
quadratic programming methods mentioned above.
The algorithms in categories II and III make use of elimination techniques for the
constraints. As a background to those algorithms we now discuss this topic. Later in this
chapterwediscussmeritfunctions,whichareimportantcomponentsofsequentialquadratic

424
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
programming methods and some other algorithms. These discussions will set the stage for
our study of practical algorithms in the remaining chapters of the book.
A Study Note: The concepts that follow constitute background material. The reader may
wish to peruse only the next two sections and return to them as needed during study of
Chapters 17 and 18.
15.2
ELIMINATION OF VARIABLES
A natural approach for dealing with constrained optimization problems is to attempt to
eliminate the constraints so as to obtain an unconstrained problem, or at least to eliminate
some constraints and obtain a simpler problem. Elimination techniques must be used with
care, however, because they may alter the problem or introduce ill-conditioning.
We begin with an example in which it is safe and convenient to apply elimination of
variables. In the problem
min f (x)  f (x1, x2, x3, x4)
subject to
x1 + x2
3 −x4x5

0,
−x2 + x4 + x2
3

0,
there is no risk in setting
x1  x4x5 −x2
3,
x2  x4 + x2
3,
and minimizing the unconstrained function of two variables
h(x3, x4)  f (x4x3 −x2
3, x4 + x2
3, x3, x4).
This new problem can be solved by any means of one of the unconstrained algorithms
described in earlier chapters.
The dangers of nonlinear elimination are illustrated in the following example.
❏Example 15.1
(Fletcher [83])
Consider the problem
min x2 + y2
subject to (x −1)3  y2.
The contours of the objective function and the constraints are illustrated in Figure 15.1,
which shows that the solution is (x, y)  (1, 0).

1 5 . 2 .
E l i m i n a t i o n o f V a r i a b l e s
425
x  +y  =
2
2 4
x  +y  =1
2
2
= (x-1)
2
y
3
(1,0)
y
x
Figure 15.1
The danger of nonlinear elimination—the lower branch of the curve
should be excluded from the feasible set
We attempt to solve this problem by eliminating y. By doing so, we obtain
h(x)  x2 + (x −1)3.
Clearly,h(x) →−∞asx →−∞.Byblindlyapplyingthistransformationwemayconclude
that the problem is in fact unbounded, but this view ignores the fact that the constraint
(x −1)3  y2 implicitly imposes the bound x ≥1 that is active at the solution. This
bound should therefore be explicitly introduced into the problem if we are to perform the
elimination of y.
❐
This example shows that elimination of nonlinear equations may result in errors
that can be difﬁcult to trace. For this reason, nonlinear elimination is not used by most
optimization algorithms. Instead, many algorithms linearize the constraints and apply elim-
ination techniques to the simpliﬁed problem. We will now describe systematic procedures
for performing this elimination of linear constraints.

426
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
SIMPLE ELIMINATION FOR LINEAR CONSTRAINTS
Let us consider the minimization of a nonlinear function subject to a set of linear
equality constraints,
min f (x)
subject to Ax  b,
(15.4)
where A is an m × n matrix with m ≤n. Suppose for simplicity that A has full row rank.
(If such is not the case, we ﬁnd either that the problem is inconsistent, or that some of the
constraints are redundant and can be deleted without affecting the solution of the problem.)
Under this assumption, we can ﬁnd a subset of m columns of A that is linearly independent.
If we gather these columns into an m×m matrix B, and deﬁne an n×n permutation matrix
P that swaps these columns to the ﬁrst m column positions in A, we can write
AP  [B | N],
(15.5)
whereN denotesthen−mremainingcolumnsofA.(Thenotationhereisconsistentwiththat
of Chapter 13, where we discussed similar concepts in the context of linear programming.)
We deﬁne the subvectors xB ∈IRm and xN ∈IRn−m in such a way that
P T x 

xB
xN

,
(15.6)
and call xB the basic variables and B the basis matrix. Noting that PP T  I, we can rewrite
the constraint Ax  b as
b  Ax  AP(P T x)  BxB + NxN.
From this formula we deduce that the basic variables are given by
xB  B−1b −B−1NxN.
(15.7)
We can therefore compute a feasible point for the constraints Ax  b by choosing any value
of xN, and then setting xB according to the formula (15.7). The problem (15.4) is therefore
equivalent to the unconstrained problem
min
xN h(xN)
def f
#
P

B−1b −B−1NxN
xN
$
.
(15.8)
We refer to (15.7) as simple elimination of variables.
This discussion shows that a nonlinear optimization problem with linear equality
constraints is, from a mathematical point of view, the same as an unconstrained problem.

1 5 . 2 .
E l i m i n a t i o n o f V a r i a b l e s
427
❏Example 15.2
Consider the problem
min sin(x1 + x2) + x2
3 + 1
3(x4 + x4
5 + x6/2)
(15.9)
subject to
8x1 −6x2 + x3 + 9x4 + 4x5  6
3x1 + 2x2 −x4 + 6x5 + 4x6  −4.
(15.10)
By deﬁning the permutation matrix P so as to reorder the components of x as xT 
(x3, x6, x1, x2, x4, x5)T , we ﬁnd that the coefﬁcient matrix AP is
AP 

1
0
8
−6
9
4
0
4
3
2
−1
6

.
The basis matrix B is diagonal, and therefore easy to invert. We obtain from (15.7) that

x3
x6

 −


8
−6
9
4
3
4
1
2
−1
4
3
2




x1
x2
x4
x5


+

6
−1

.
(15.11)
By substituting for x3 and x6 in (15.9), the problem becomes
min
x1,x2,x4,x5 sin(x1 + x2) + (8x1 −6x2 + 9x4 + 4x5 −6)2
(15.12)
+ 1
3(x4 + x4
5 −[(1/2) + (3/8)x1 + (1/4)x2 −(1/8)x4 + (3/4)x5]).
We could have chosen two other columns of the coefﬁcient matrix A (that is, two
variables other than x3 and x6) as the basis for elimination in the system (15.10). For those
choices, however, the matrix B−1N would have been more complicated.
❐
Selectingasetofmindependentcolumnscanbedone,ingeneral,bymeansofGaussian
elimination. In the parlance of linear algebra, we can compute the row echelon form of the
matrix and choose the pivot columns as the columns of the basis B. Ideally, we would
like B to be easy to factorize and well conditioned. For this purpose, we can use a sparse

428
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
Gaussian elimination algorithm that attempts to preserve sparsity while keeping rounding
errors under control. A well-known implementation of this algorithm is MA48 from the
Harwell library [133]. As we discuss below, however, there is no guarantee that the Gaussian
elimination process will identify the best choice of basis matrix.
There is an interesting interpretation of the elimination-of-variables approach that we
have just described. To simplify the notation, we will assume from now on that the coefﬁcient
matrix is already given to us so that the basic columns appear in the ﬁrst m positions, that
is, P  I. (It is straightforward, but cumbersome, to adapt the arguments that follow to the
case of P ̸ I.)
From (15.6) and (15.7) we see that any feasible point x for the linear constraints in
(15.4) can be written as
x  Yb + ZxN,
(15.13)
where
Y 

B−1
0

,
Z 

−B−1N
I

.
(15.14)
Note that Z has n −m linearly independent columns (due to the presence of the identity
matrix in the lower block) and that it satisﬁes AZ  0. Therefore, Z is a basis for the null
space of A. In addition, the columns of Y and the columns of Z form a linearly independent
set, which implies that Y is a basis for the range space of AT . We note also from (15.14),(15.5)
that Yb is a particular solution of the linear constraints Ax  b.
In other words, the simple elimination technique expresses feasible points as the sum
ofaparticularsolutionofAx  b,theﬁrsttermin(15.13),plusadisplacementalongthenull
space(ortangent)spaceoftheconstraints—thesecondtermin(15.13).Therelations(15.13),
(15.14) indicate that the particular Yb solution is obtained by holding n −m components
of x at zero while relaxing the other m components until they reach the constraints; see
Figure 15.2. The displacement Yb is sometimes known as the coordinate relaxation step. A
different choice of basis would correspond in this ﬁgure to a particular solution along the
x2 axis.
Simple elimination is inexpensive but can give rise to numerical instabilities. If the
feasible set in Figure 15.2 consisted of a line that was almost parallel to the x1 axis, Then
a particular solution along this axis would be very large in magnitude. Since the total dis-
placement (15.13) is not large in general, we would compute x as the difference of very
large vectors, giving rise to numerical cancellation. In that situation it would be preferable
to choose a particular solution along the x2 axis, that is, to select a different basis. Selection
of the best basis is, however, not a straightforward task in general. Numerical errors can also
occur in the deﬁnition of Z when the basis matrix B is poorly conditioned.

1 5 . 2 .
E l i m i n a t i o n o f V a r i a b l e s
429
x2
x1
x = b
A
coordinate relaxation step
Figure 15.2
Simple elimination—we ﬁx x2 to zero and choose x1 to ensure that the
constraint is satisﬁed.
To overcome this danger we could deﬁne the particular solution Yb as the minimum-
norm step to the constraints. This approach is a special case of more general elimination
strategies, which we now describe.
GENERAL REDUCTION STRATEGIES FOR LINEAR CONSTRAINTS
In analogy with (15.13), we choose matrices Y ∈IRn×m and Z ∈IRn×(n−m) whose
columns form a linearly independent set, and express any solution of the linear constraints
Ax  b as
x  YxY + ZxZ,
(15.15)
for some vectors xY and xZ of dimensions m and n −m, respectively. We also require that Y
and Z be chosen so that Z has linearly independent columns and so that
AY
is nonsingular,
and
AZ  0.
(15.16)
(Note that AY is an m×m matrix.) As in simple elimination, we deﬁne Z to be a basis for the
null space of the constraints, but we now leave the choice of Y unspeciﬁed; see Figure 15.3.
By substituting (15.15) into the constraints Ax  b, we obtain
Ax  (AY)xY  b,

430
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
x2
x3
x1
Zx
Yx
Z
Y
Ax= b
Figure 15.3
General elimination.
so by nonsingularity of AY, xY can be written explicitly as follows:
xY  (AY)−1b.
(15.17)
By substituting this expression into (15.15), we conclude that any vector x of the form
x  Y(AY)−1b + ZxZ
(15.18)
will satisfy the constraints Ax  b for any choice of xZ ∈IRn−m. Therefore, the problem
(15.4) can be restated equivalently as the unconstrained problem
min
xZ f (Y(AY)−1b + ZxZ).
(15.19)
Ideally,wewouldliketochooseY insuchawaythatthematrixAY isaswellconditioned
as possible, since it needs to be factorized to give the particular solution Y(AY)−1b. We can
do this by computing Y and Z by means of a QR factorization of AT , which has the form
AT  

Q1
Q2


R
0

,
(15.20)
where

Q1
Q2

is orthogonal. The submatrices Q1 and Q2 have orthonormal columns
and are of dimension n × m and n × (n −m), while R is m × m upper triangular and

1 5 . 2 .
E l i m i n a t i o n o f V a r i a b l e s
431
nonsingular and  is an m × m permutation matrix. (See the discussion following (A.54)
in the Appendix for more details.)
We now deﬁne

Y
Z



Q1
Q2

,
(15.21)
so that Y and Z form an orthonormal basis of IRn. If we expand (15.20) and do a little
rearrangement, we obtain
AY  RT ,
AZ  0.
Therefore, Y and Z have the desired properties, and the condition number of AY is the
same as that of R, which in turn is the same as that of A itself. From (15.18) we see that any
solution of Ax  b can be expressed as
x  Q1R−T T b + Q2xZ,
for some vector xZ. The computation R−T T b can be carried out inexpensively, at the cost
of a single triangular substitution.
A simple computation shows that the particular solution Q1R−T T b can also be
written as
xp  AT (AAT )−1b,
(15.22)
and is therefore the minimum-norm solution of the constraints Ax  b, that is, the solution
of
min ∥Ax −b∥2.
See Figure 15.4 for an illustration of this step.
Eliminationviatheorthogonalbasis(15.21)isidealfromthepointofviewofnumerical
stability. The main cost associated with this reduction strategy is in the computation of the
QR factorization (15.20). Unfortunately, for problems in which A is large and sparse, a
sparse QR factorization can be much more costly to compute than the application of sparse
Gaussian elimination used in the simple elimination approach. Therefore, other elimination
strategies have been developed that seek a compromise between these two techniques; see
Exercise 6.
THE EFFECT OF INEQUALITY CONSTRAINTS
Elimination of variables is not always beneﬁcial if inequality constraints are present
alongsidetheequalities.Forinstance,ifproblem(15.9),(15.10)hadtheadditionalconstraint

432
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
x2
x1
x = b
A
A
]
[T
A
T
A
-1b
Figure 15.4
The minimum-norm step
x ≥0, then after eliminating the variables x3 and x6, we would be left with the problem of
minimizing the function in (15.12) subject to the constraints
(x1, x2, x4, x5) ≥0,
8x1 −6x2 + 9x4 + 4x5 ≤6,
(3/4)x1 + (1/2)x2 −(1/4)x4 + (3/2)x5 ≤−1.
Hence, the cost of eliminating the equality constraints (15.10) is to make the inequalities
more complicated than the simple bounds x ≥0. For many algorithms, this transformation
will not yield any beneﬁt.
If, however, we added the general inequality constraint 3x1 + 2x3 ≥1 to the problem
(15.9), (15.10), the elimination (15.11) transforms the problem into one of minimizing the
function in (15.12) subject to the inequality constraint
−13x1 + 12x2 −18x4 −8x5 ≥−11.
(15.23)
In this case, the inequality constraint does not become much more complicated af-
ter elimination of the equality constraints, so it is probably worthwhile to perform the
elimination.
15.3
MEASURING PROGRESS: MERIT FUNCTIONS
Suppose that an algorithm for solving the nonlinear programming problem (15.1) generates
a step that gives a substantial reduction in the objective function but leads us farther away
from the feasible region than the current iterate. Should we accept this step?

1 5 . 3 .
M e a s u r i n g P r o g r e s s : M e r i t F u n c t i o n s
433
Thisquestionisnoteasytoanswer.Inconstrainedoptimizationweareconfrontedwith
the often conﬂicting goals of reducing the objective function and satisfying the constraints,
and we must look for a measure that strikes the right balance between these two goals. Merit
functions are designed to quantify this balance and control the algorithm: A step p will be
accepted only if it leads to a sufﬁcient reduction in the merit function φ.
In unconstrained optimization, the objective function f is the natural choice for the
merit function. All the unconstrained optimization methods described in this book require
that f be decreased at each step (or at least after a certain number of iterations). In feasible
methods, in which the starting point and all subsequent iterates satisfy all the constraints in
the problem, the objective function is still an appropriate merit function. For example, when
all the constraints are linear, some active set methods (Chapters 16 and 18) use a phase-1
iteration to compute a feasible starting point and deﬁne each step of the algorithm so that
feasibility is retained at all subsequent iterates.
On the other hand, algorithms that allow iterates to violate at least some constraints
require some way to assess the quality of the steps and iterates. The most common way to
make this assessment is with a merit function.
A widely used merit function for the general nonlinear programming problem (15.1)
is the ℓ1 exact function deﬁned by
φ1(x; µ)  f (x) + 1
µ

i∈E
|ci(x)| + 1
µ

i∈I
[ci(x)]−,
(15.24)
whereweusethenotation[x]− max{0, −x}.Thepositivescalarµisthepenaltyparameter,
whichdeterminestheweightthatweassigntoconstraintsatisfactionrelativetominimization
of the objective. This merit function is called exact because for a range of values of the penalty
parameter µ, the solution to the nonlinear programming problem (15.1) is a local minimizer
of φ(x; µ). Note that φ(x; µ) is not differentiable due to the presence of the absolute value
and [·]−functions.
Another useful merit function is Fletcher’s augmented Lagrangian. For the case where
only equality constraints are present in (15.1), this merit function is deﬁned to be
φF(x; µ)  f (x) −λ(x)T c(x) + 1
2µ

i∈E
ci(x)2,
(15.25)
where µ > 0 is the penalty parameter, ∥· ∥denotes the ℓ2 norm, and
λ(x)  [A(x)A(x)T ]−1A(x)∇f (x)
(15.26)
are called the least-squares multiplier estimates. This merit function is differentiable, and
is also exact. To deﬁne this function for problems including inequality constraints we can
make use of slack variables.

434
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
We now give a more precise deﬁnition of exactness of a merit function. Since the pre-
vious discussion suggests that merit functions always make use of a scalar penalty parameter
µ > 0, we will write a general merit function as φ(x; µ), and assume that constraints are
penalized by decreasing µ.
Deﬁnition 15.1 (Exact Merit Function).
The merit function φ(x; µ) is said to be exact if there is a positive scalar µ∗such that
for any µ ∈(0, µ∗], any local solution of the nonlinear programming problem (15.1) is a local
minimizer of φ(x; µ).
One can show that the ℓ1 merit function φ(x; ν) is exact for all µ < µ∗, where
1
µ∗ max{|λ∗
i |, i ∈E; λ∗
i , i ∈I},
and where the λ∗
i denote the Lagrange multipliers associated with an optimal solution x∗.
Many algorithms based on the ℓ1 merit function contain heuristics for adjusting the penalty
parameter whenever the algorithm determines that the current value probably does not
satisfy µ > µ∗. (This decision could be based on an approximation to µ∗obtained from the
currentLagrangemultiplierestimates.)Preciserulesforsettingandchangingµaredescribed
in Chapter 18.
Fletcher’s augmented Lagrangian merit function φF is also exact. Since the threshold
value µ∗is not as simple to write as in the case of the ℓ1 merit function—because it in-
volves bounds on certain derivatives—we discuss the choice of µ in the context of speciﬁc
algorithms. See Section 18.5 for further discussion.
We now contrast some of the other properties of these two merit functions. The ℓ1
merit function is inexpensive to evaluate, since function and constraint values are available
at every iteration of constrained optimization algorithms. One of its potential drawbacks
is that it may reject steps that make good progress toward the solution—a phenomenon
known as the Maratos effect, which is described in Chapter 18. Several strategies have been
devised that seem to successfully remove the damaging effects of the Maratos effect, but they
introduce a certain degree of complexity into the algorithms.
Fletcher’saugmentedLagrangianmeritfunctionφF isdifferentiableanddoesnotsuffer
from the Maratos effect. Its main drawback is the expense of evaluating it at trial points—if
a line search is needed to ensure that φF decreases at each step—since (15.26) requires the
solution of a linear system. To circumvent this problem we can replace λ(xk + αp), after the
ﬁrst trial value λ(xk + p) has been evaluated, by the interpolant
λ(xk) + α(λ(xk + p) −λ(xk)),
and use this interpolant in the line search. Other potential drawbacks of Fletcher’s function
are that λ is not uniquely deﬁned by (15.26) when A loses rank, and that λ can be excessively
large when A is nearly rank deﬁcient.

1 5 . 3 .
M e a s u r i n g P r o g r e s s : M e r i t F u n c t i o n s
435
As noted above, φ1 has a simpler form than φF, but is not differentiable. In fact, as we
now show, exact penalty functions that have the form of φ1 must be nondifferentiable.
For simplicity, we restrict our attention to the case where only equality constraints are
present, and assemble the constraint functions ci(x), i ∈E, into a vector c(x). Consider a
merit function of the form
φ(x; µ)  f (x) + 1
µh(c(x)),
(15.27)
where h : IRm →IR is a function satisfying the properties h(y) ≥0 for all y ∈IRm and
h(0)  0. Suppose for contradiction that h is differentiable. Since h has a minimizer at zero,
we have from Theorem 2.2 that ∇h(0)  0. Now, if x∗is a local solution of the problem
(15.1), we have c(x∗)  0 and therefore ∇h(c(x∗))  0. Hence, since x∗is a local minimizer
of , we have that
0  ′(x∗)  ∇f (x∗) + 1
µ∇c(x∗)∇h(c(x∗))  ∇f (x∗).
However,itisnotgenerallytruethatthegradientoff vanishesatthesolutionofaconstrained
optimization problem, so our original assumption that h is differentiable must be incorrect,
and our claim is proved.
The ℓ1 merit function φ1 is a special case of (15.27) in which h(c)  ∥c∥1. Other exact
merit functions used in practice deﬁne h(x)  ∥x∥2 (not squared) and h(x)  ∥x∥∞. To
obtain exact, differentiable merit functions, we must extend the form (15.27) by including
additional terms in the merit function. (The second term in (15.25) serves this purpose.)
We conclude by describing a merit function that is used in several popular programs
for nonlinear programming, but that is quite different in nature. Supposing once again, for
simplicity, that the problem contains only equality constraints, we deﬁne the augmented
Lagrangian in x and λ as follows:
LA(x, λ; µ)  f (x) −λT c(x) + 1
2µ∥c(x)∥2
2.
(15.28)
If at the current point (xk, λk) the algorithm generates a search direction (px, pλ) (that is,
a step both in the primal and dual variables), we assess the acceptability of the proposed
new iterate by substituting (x, λ)  (xk + px, λk + pλ) into (15.28) and comparing it with
LA(xk, λk; µ).
Unlike the merit functions φ1 and φF described above, LA depends on the dual vari-
ables as well as the primals. A solution (x∗, λ∗) of the nonlinear programming problem is a
stationary point for LA(x, λ; µ), but in general not a minimizer. Nevertheless, some sequen-
tial quadratic programming programs use LA successfully as a merit function by adaptively
modifying µ and λ.

436
C h a p t e r
1 5 .
F u n d a m e n t a l s o f C o n s t r a i n e d A l g o r i t h m s
NOTES AND REFERENCES
General elimination techniques are described in Fletcher [83].
Merit functions have received much attention. Boggs and Tolle [23] survey much
of the work and provide numerous references. The ℓ1 function was ﬁrst suggested as a
merit function for sequential quadratic programming methods by Han [132]. The merit
augmented Lagrangian function (15.25) was proposed by Fletcher [81], whereas the primal–
dual function (15.28) was proposed by Wright [250] and Schittkowski [222].
✐
E x e r c i s e s
✐
15.1 Do the following problems have solutions? Explain.
min x1 + x2
subject to x2
1 + x2
2  2, 0 ≤x1 ≤1, 0 ≤x2 ≤1;
min x1 + x2
subject to x2
1 + x2
2 ≤1, x1 + x2  3;
min x1x2
subject to x1 + x2  2.
✐
15.2 Show that if in Example 15.1 we eliminate x in terms of y, then the correct
solution of the problem is obtained by performing unconstrained minimization.
✐
15.3 Explain why we can be sure that the matrix Z in (15.14) has linearly independent
columns, regardless of the choice of B and N.
✐
15.4 Show that the basis matrices (15.14) are linearly independent.
✐
15.5 Show that the particular solution Q1R−T T b can be written as (15.22).
✐
15.6 In this exercise we compute basis matrices that attempt to be a compromise
between the orthonormal basis (15.21) and simple elimination (15.14). Let us assume that
the basis matrix is given by the ﬁrst m columns of A, so that P  I in (15.5), and deﬁne
Y 

I
(B−1N)T

,
Z 

−B−1N
I

.
(a) Show that the columns of Y and Z are no longer of norm 1 and that the relations
AZ  0 and Y T Z  0 hold. Therefore, the columns of Y and Z form an independent set,
showing that this is a valid choice of the basis matrices. (b) Show that the particular solution
Y(AY)−1b deﬁned by this choice of Y is, as in the orthogonal factorization approach, the
minimum-norm solution (15.22) of Ax  b. More speciﬁcally, show that
Y(AY)−1  AT (AAT )−1.

1 5 . 3 .
M e a s u r i n g P r o g r e s s : M e r i t F u n c t i o n s
437
It follows that the matrix Y(AY)−1 is independent of the choice of basis matrix B in (15.5),
and its conditioning is determined by that of A alone. Note, however, that the matrix Z still
depends explicitly on B, so a careful choice of B is needed to ensure well conditioning in
this part of the computation.
✐
15.7 Verify that by adding the inequality constraint 3x1 + 2x3 ≥1 to the problem
(15.9), (15.10), the elimination (15.11) transforms the problem into one of minimizing the
function (15.12) subject to the inequality constraint (15.23).

Chapter16

Quadratic
Programming
An optimization problem with a quadratic objective function and linear constraints is called
a quadratic program. Problems of this type are important in their own right, and they also
arise as subproblems in methods for general constrained optimization, such as sequential
quadratic programming (Chapter 18) and augmented Lagrangian methods (Chapter 17).
The general quadratic program (QP) can be stated as
min
x
q(x)  1
2xT Gx + xT d
(16.1a)
subject to aT
i x  bi,
i ∈E,
(16.1b)
aT
i x ≥bi,
i ∈I,
(16.1c)
where G is a symmetric n × n matrix, E and I are ﬁnite sets of indices, and d, x, and
{ai}, i ∈E ∪I, are vectors with n elements. Quadratic programs can always be solved (or
can be shown to be infeasible) in a ﬁnite number of iterations, but the effort required to ﬁnd
a solution depends strongly on the characteristics of the objective function and the number
of inequality constraints. If the Hessian matrix G is positive semideﬁnite, we say that (16.1)
is a convex QP, and in this case the problem is sometimes not much more difﬁcult to solve

440
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
than a linear program. Nonconvex QPs, in which G is an indeﬁnite matrix, can be more
challenging, since they can have several stationary points and local minima.
In this chapter we limit ourselves to studying algorithms that ﬁnd the solution of a
convexquadraticprogramorastationarypointofageneral(nonconvex)quadraticprogram.
We start by considering an interesting application of quadratic programming.
AN EXAMPLE: PORTFOLIO OPTIMIZATION
Every investor knows that there is a tradeoff between risk and return: To increase the
expected return on investment, an investor must be willing to tolerate greater risks. Portfolio
theory studies how to model this tradeoff given a collection of n possible investments with
returns ri, i  1, 2, . . . , n. The returns ri are usually not known in advance, and are often
assumed to be random variables that follow a normal distribution. We can characterize these
variables by their expected value µi  E[ri] and their variance σ 2
i  E[(ri −µi)2]. The
variance measures the ﬂuctuations of the variable ri about its mean, so that larger values of
σi indicate riskier investments.
An investor constructs a portfolio by putting a fraction xi of the available funds into
investment i, for i  1, 2, . . . , n. Assuming that all available funds are invested and that
short-selling is not allowed, the constraints are n
i1 xi  1 and x ≥0. The return on the
portfolio is given by
R 
n

i1
xiri.
(16.2)
To measure the desirability of the portfolio, we need to obtain measures of its expected return
variance. The expected return is simply
E(R)  E
 n

i1
xiri


n

i1
xiE[ri]  xT µ.
The variance, too, can be calculated from elementary laws of statistics. It depends on the
covariances between each pair of investments, which are deﬁned by
ρij  E[(ri −µi)(rj −µj)]
σiσj
,
for i, j  1, 2, . . . , n.
The correlation measures the tendency of the return on investments i and j to move in the
same direction. Two investments whose returns tend to rise and fall together have a positive
covariance; the nearer ρij is to 1, the more closely the two investments track each other.
Investments whose returns tend to move in opposite directions have negative covariance.

1 6 . 1 .
E q u a l i t y - - C o n s t r a i n e d Q u a d r a t i c P r o g r a m s
441
The variance of the total portfolio R is then given by
E[(R −E[R])2] 
n

i1
n

j1
xixjσiσjρij  xT Gx,
where we have deﬁned the n × n symmetric matrix G by
Gij  ρijσiσj.
It can be shown that G is positive semideﬁnite.
We are interested in portfolios for which the expected return xT µ is large while the
variance xT Gx is small. In the model proposed by Markowitz [157], we combine these two
aims into a single objective function with the aid of a “risk tolerance parameter” denoted by
κ, and solve the following problem to ﬁnd the “optimal” portfolio:
max xT µ −κxT Gx,
subject to
n

i1
xi  1, x ≥0.
The parameter κ lies in the range [0, ∞), and its chosen value depends on the preferences of
the individual investor. Conservative investors would place more emphasis on minimizing
risk in their portfolio, so they would choose a large value of κ to increase the weight of the
variance measure in the objective function. More daring investors are prepared to take on
more risk in the hope of a higher expected return, so their value of κ would be closer to zero.
Thedifﬁcultyinapplyingthisportfoliooptimizationtechniquetoreal-lifeinvestinglies
in deﬁning the expected returns, variances, and covariances for the investments in question.
One possibility is to use historical data, deﬁning the quantities µi, σi, and ρij to be equal
to their historical values between the present day and, say, ﬁve years ago. It is not wise to
assume that future performance will mirror the past, of course. Moreover, historical data
will not be available for many interesting investments (such as start-up companies based on
new technology). Financial professionals often combine the historical data with their own
insights and expectations to produce values of µi, σi, and ρij.
16.1
EQUALITY--CONSTRAINED QUADRATIC PROGRAMS
We begin our discussion of algorithms for quadratic programming by considering the case
where only equality constraints are present. As we will see in this chapter, active set methods
for general quadratic programming solve an equality–constrained QP at each iteration.

442
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
PROPERTIES OF EQUALITY-CONSTRAINED QPS
Let us denote the number of constraints by m, assume that m ≤n, and write the
quadratic program as
min
x
q(x)
def 1
2xT Gx + xT d
(16.3a)
subject to
Ax  b,
(16.3b)
where A is the m × n Jacobian of constraints deﬁned by
A  [ai]T
i∈E.
For the present, we assume that A has full row rank (rank m), and that the constraints
(16.3b) are consistent. (Toward the end of the chapter we discuss the case in which A is rank
deﬁcient.)
The ﬁrst-order necessary conditions for x∗to be a solution of (16.3) state that there is
a vector λ∗such that the following system of equations is satisﬁed:

G
−AT
A
0
 
x∗
λ∗



−d
b

.
(16.4)
It is easy to derive these conditions as a consequence of the general result for ﬁrst-order
optimality conditions, Theorem 12.1. As in Chapter 12, we call λ∗the vector of Lagrange
multipliers. The system (16.4) can be rewritten in a form that is useful for computation by
expressing x∗as x∗ x + p, where x is some estimate of the solution and p is the desired
step. By introducing this notation and rearranging the equations, we obtain

G
AT
A
0
 
−p
λ∗



g
c

,
(16.5)
where
c  Ax −b,
g  d + Gx,
p  x∗−x.
(16.6)
The matrix in (16.5) is called the Karush–Kuhn–Tucker (KKT) matrix, and the fol-
lowing result gives conditions under which it is nonsingular. As in Chapter 15, we use Z to
denote the n × (n −m) matrix whose columns are a basis for the null space of A. That is, Z
has full rank and AZ  0.

1 6 . 1 .
E q u a l i t y - - C o n s t r a i n e d Q u a d r a t i c P r o g r a m s
443
Lemma 16.1.
Let A have full row rank, and assume that the reduced-Hessian matrix ZT GZ is positive
deﬁnite. Then the KKT matrix
K 

G
AT
A
0

(16.7)
is nonsingular, and there is a unique vector pair (x∗, λ∗) satisfying (16.4).
Proof.
Suppose there are vectors p and v such that

G
AT
A
0
 
p
v

 0.
(16.8)
Since Ap  0, we have from (16.8) that
0 

p
v
T 
G
AT
A
0
 
p
v

 pT Gp.
Since p lies in the null space of A, it can be written as p  Zu for some vector u ∈IRn−m.
Therefore, we have
0  pT Gp  uT ZT GZu,
which by positive deﬁniteness of ZT GZ implies that u  0. Therefore, p  0, and by (16.8),
AT v  0; and the full row rank of A implies that v  0. We conclude that equation (16.8)
is satisﬁed only if p  0 and v  0, so the matrix is nonsingular, as claimed.
□
❏Example 16.1
Consider the quadratic programming problem
min q(x)  3x2
1 + 2x1x2 + x1x3 + 2.5x2
2 + 2x2x3 + 2x2
3 −8x1 −3x2 −3x3,
subject to
x1 + x3  3,
x2 + x3  0.
(16.9)
We can write this problem in the form (16.3) by deﬁning
G 


6
2
1
2
5
2
1
2
4

,
d 


−8
−3
−3

,
A 

1
0
1
0
1
1

,
b 

3
0

.

444
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
The solution x∗and optimal Lagrange multiplier vector λ∗are given by
x∗


2
−1
1

,
λ∗

3
−2

.
In this example, the matrix G is positive deﬁnite, and the null-space basis matrix can be
deﬁned as
Z  (−1, −1, 1)T .
(16.10)
❐
We have seen that when the conditions of Lemma 16.1 are satisﬁed, then there is a
unique vector pair (x∗, λ∗) that satisﬁes the ﬁrst-order necessary conditions for (16.3). In
fact, the second-order sufﬁcient conditions (see Theorem 12.6) are also satisﬁed at (x∗, λ∗),
so x∗is a strict local minimizer of (16.3). However, we can use a direct argument to show
that x∗is actually a global solution of (16.3).
Theorem 16.2.
Suppose that the conditions of Lemma 16.1 are satisﬁed. Then the vector x∗satisfying
(16.4) is the unique global solution of (16.3).
Proof.
Let x be any other feasible point (satisfying Ax  b), and as before, we use p to
denote the difference x∗−x. Since Ax∗ Ax  b, we have that Ap  0. By substituting
into the objective function (16.3a), we obtain
q(x)  1
2(x∗−p)T G(x∗−p) + dT (x∗−p)
 1
2pT Gp −pT Gx∗−dT p + q(x∗).
(16.11)
From (16.4) we have that Gx∗ −d + AT λ∗, so from Ap  0 we have that
pT Gx∗ pT (−d + AT λ∗)  −pT d.
By substituting this relation into (16.11), we obtain
q(x)  1
2pT Gp + q(x∗).
Since p lies in the null space of A, we can write p  Zu for some vector u ∈IRn−m, so that
q(x)  1
2uT ZT GZu + q(x∗).

1 6 . 2 .
S o l v i n g t h e K K T S y s t e m
445
By positive deﬁniteness of ZT GZ, we conclude that q(x) > q(x∗) except when u  0, that
is, when x  x∗. Therefore, x∗is the unique global solution of (16.3).
□
When the projected Hessian matrix ZT GZ has zero or negative eigenvalues, the prob-
lem (16.3) does not have a bounded solution, except in a special case. To demonstrate this
claim, suppose that there is a vector pair (x∗, λ∗) that satisﬁes the KKT conditions (16.4).
Let u be some vector such that uT ZT GZu ≤0, and set p  Zu. Then for any α > 0, we
have that
A(x∗+ αp)  b,
so that x∗+ αp is feasible, while
q(x∗+ αp)  q(x∗) + αpT (Gx∗+ d) + 1
2α2pT Gp
 q(x∗) + 1
2α2pT Gp ≤q(x∗),
where we have used the facts that Gx∗+ d  AT λ∗from (16.4) and pT AT λ∗
uT ZT AT λ∗ 0. Therefore, from any x∗satisfying the KKT conditions, we can ﬁnd a
feasible direction p along which q does not increase. In fact, we can always ﬁnd a direction
of strict decrease for q unless ZT GZ has no negative eigenvalues. The only case in which
(16.3) has solutions is the one in which there exists some point x∗for which (16.4) is satis-
ﬁed, while ZT GZ is positive semideﬁnite. Even in this case, the solution is not a strict local
minimizer.
16.2
SOLVING THE KKT SYSTEM
In this section we discuss efﬁcient methods for solving the KKT system (16.4) (or,
alternatively, (16.5)).
The ﬁrst important observation is that if m ≥1, the KKT matrix is always indeﬁnite.
Thefollowingresultcharacterizesthe inertiaof(16.7)undertheassumptionsofLemma16.1.
The inertia of a matrix is the scalar triple that indicates the number of its positive, negative,
and zero eigenvalues.
Lemma 16.3.
Suppose that A has full row rank and that the reduced Hessian ZT GZ is positive deﬁnite.
Then the KKT matrix (16.7) has n positive eigenvalues, m negative eigenvalues, and no zero
eigenvalues.
This result follows from Theorem 16.6 given later in this chapter. Knowing that the
KKT system is indeﬁnite, we now describe the main techniques developed for its solution.

446
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
DIRECT SOLUTION OF THE KKT SYSTEM
One option for solving (16.5) is to perform a triangular factorization on the full KKT
matrix and then perform backward and forward substitution with the triangular factors.
We cannot use the Cholesky factorization algorithm because the KKT matrix is indeﬁnite.
Instead, we could use Gaussian elimination with partial pivoting (or a sparse variant of this
algorithm) to obtain the L and U factors, but this approach has the disadvantage that it
ignores the symmetry of the system.
The most effective strategy in this case is to use a symmetric indeﬁnite factorization. We
have described these types of factorizations in Chapter 6. For a general symmetric matrix
K, these factorizations have the form
P T KP  LBLT ,
(16.12)
where P is a permutation matrix, L is unit lower triangular, and B is block-diagonal with
either 1 × 1 or 2 × 2 blocks. The symmetric permutations deﬁned by the matrix P are
introduced for numerical stability of the computation and, in the case of large sparse K, to
maintain sparsity. The computational cost of symmetric indeﬁnite factorization (16.12) is
typically about half the cost of sparse Gaussian elimination.
To solve (16.5) we ﬁrst compute the factorization (16.12), substituting the KKT matrix
for K. We then perform the following sequence of operations to arrive at the solution:
solve Ly  P T

g
c

to obtain y;
solve B ˆy  y to obtain ˆy;
solve LT ¯y  ˆy to obtain ¯y;
set

−p
λ∗

 P ¯y.
Note that multiplications with the permutation matrices P and P T can be performed by
rearranging vector components, and are therefore inexpensive. Solution of the system B ˆy 
y entails solving a number of small 1×1 and 2×2 systems, so the number of operations is a
small multiple of the vector length (m+n), again quite inexpensive. Triangular substitutions
with L and LT are more signiﬁcant. Their precise cost depends on the amount of sparsity,
but is usually signiﬁcantly less than the cost of performing the factorization (16.12).
This approach of factoring the full (n + m) × (n + m) KKT matrix (16.7) is quite
effective on some problems. Difﬁculties may arise when the heuristics for choosing the
permutation matrix P are not able to do a very good job of maintaining sparsity in the L
factor, so that L becomes much more dense than the original coefﬁcient matrix.
An alternative to the direct factorization approach for the matrix in (16.5) is to apply
an iterative method. The conjugate gradient method is not recommended because it can be

1 6 . 2 .
S o l v i n g t h e K K T S y s t e m
447
unstable on systems that are not positive deﬁnite. Therefore, we must consider techniques
for general linear systems, or for symmetric indeﬁnite systems. Candidates include the QMR
and LSQR methods (see the Notes and References at the end of the chapter).
RANGE-SPACE METHOD
In the range-space method, we use the matrix G to perform block elimination on the
system (16.5). Assuming that G is positive deﬁnite, we multiply the ﬁrst equation in (16.5)
by AG−1 and then subtract the second equation to obtain a linear system in the vector λ∗
alone:
(AG−1AT )λ∗ (AG−1g −c).
(16.13)
We solve this symmetric positive deﬁnite system for λ∗, and then recover p from the ﬁrst
equation in (16.5) by solving
Gp  AT λ∗−g.
(16.14)
This approach requires us to perform operations with G−1, as well as to compute the
factorization of the m × m matrix AG−1AT . It is therefore most useful when:
• G is well conditioned and easy to invert (for instance, when G is diagonal or block-
diagonal);
• G−1 is known explicitly through a quasi-Newton updating formula; or
• the number of equality constraints m is small, so that the number of backsolves needed
to form the matrix AG−1AT is not too large.
The range-space approach is, in effect, a special case of the symmetric elimination
approach of the previous section. It corresponds to the case in which the ﬁrst n variables
in the system (16.5) are eliminated before we eliminate any of the last m variables. In other
words, it is obtained if we choose the matrix P in (16.12) as
P 

P1
0
0
P2

,
where P1 and P2 are permutation matrices of dimension n × n and m × m, respectively.
We can actually use a similar approach to the range-space method to derive an explicit
inverse formula for the KKT matrix in (16.5). This formula is

G
AT
A
0
−1


C
E
ET
F

,
(16.15)

448
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
with
C  G−1 −G−1AT (AG−1AT )−1AG−1,
E  G−1AT (AG−1AT )−1,
F  −(AG−1AT )−1.
The solution of (16.5) can be obtained by multiplying its right-hand-side by this matrix. If
we take advantage of common expressions and group terms appropriately, we recover the
approach (16.13), (16.14).
NULL-SPACE METHOD
The null-space method, which we now describe, does not require nonsingularity of
G and is therefore of wider applicability than the range-space method. It assumes only that
the conditions of Lemma 16.1 hold, namely, that A has full row rank and that ZT GZ is
positive deﬁnite. It requires, however, knowledge of the null-space basis matrix Z. Like the
range-space method, it exploits the block structure in the KKT system to decouple (16.5)
into two smaller systems.
Suppose that we partition the vector p in (16.5) into two components, as follows:
p  YpY + ZpZ,
(16.16)
where Z is the n × (n −m) null-space matrix, Y is any n × m matrix such that [Y | Z] is
nonsingular, pY is an m-vector, and pZ is an (n −m)-vector (see Chapter 15). As illustrated
in Figure 15.3, YpY is a particular solution of Ax  b, and ZpZ is a displacement along these
constraints.
By substituting p into the second equation of (16.5) and recalling that AZ  0, we
obtain
(AY)pY  −c.
(16.17)
Since A has rank m and [Y | Z] is n × n nonsingular, the product A[Y | Z]  [AY | 0] has
rank m. Therefore, AY is a nonsingular m × m matrix, and pY is well determined by the
equations (16.17). Meanwhile, we can substitute (16.16) into the ﬁrst equation of (16.5) to
obtain
−GYpY −GZpZ + AT λ∗ g
and multiply by ZT to obtain
(ZT GZ)pZ  −[ZT GYpY + ZT g].
(16.18)

1 6 . 2 .
S o l v i n g t h e K K T S y s t e m
449
This system, which can be solved by means of the Cholesky factorization of the (n −m) ×
(n −m) reduced-Hessian matrix ZT GZ, determines pZ, and hence the total displacement
p  YpY + ZpZ. To obtain the Lagrange multiplier, we multiply the ﬁrst equation of (16.5)
by Y T to obtain the linear system
(AY)T λ∗ Y T (g + Gp),
(16.19)
which can be solved for λ∗.
❏Example 16.2
Consider the problem (16.9) given in the previous example. We can choose
Y 


2/3
−1/3
−1/3
2/3
1/3
1/3


and Z is as in (16.10). For this particular choice of Y, we have AY  I.
Suppose we have x  (0, 0, 0)T in (16.5). Then
c  Ax −b  −b,
g  d + Gx  d  (−8, −3, −3)T .
Simple calculation shows that
pY  (3, 0)T ,
pZ  0,
so that
p  x∗−x  YpY + ZpZ  (2, −1, 1)T .
After recovering λ∗from (16.19), we conclude that
x∗ (2, −1, 1)T ,
λ∗ (3, −2)T .
❐
The null-space approach can be effective when the number of degrees of freedom
n −m is small. Its main drawback is the need for the null-space matrix Z, which as we have
seen in Chapter 15, can be expensive to compute in many large problems. The matrix Z is
not uniquely deﬁned, and if it is poorly chosen, the reduced system (16.18) may become ill
conditioned. If we choose Z to have orthonormal columns, as is normally done in software
for small and medium-sized problems, then the conditioning of ZT GZ is at least as good as

450
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
that of G itself. When A is large and sparse, however, this choice of Z is relatively expensive
to compute, so for practical reasons we are often forced to use one of the less reliable choices
of Z described in Chapter 15.
Thereducedsystem(16.18)alsocanbesolvedbymeansoftheconjugategradient(CG)
method. If we adopt this approach, it is not necessary to form the reduced Hessian ZT GZ
explicitly, since the CG method requires only that we compute matrix–vector products
involving this matrix. In fact, it is not even necessary to form Z explicitly, as long as we are
able to compute products of Z and ZT with arbitrary vectors. For some choices of Z and
for large problems, these products are much cheaper to compute than Z itself, as we have
seen in Chapter 15. A drawback of the conjugate gradient approach in the absence of an
explicit representation of Z is that standard preconditioning techniques, such as modiﬁed
Cholesky, cannot be used, and the development of effective preconditioners for this case is
still the subject of investigation.
It is difﬁcult to give hard and fast rules about the relative effectiveness of null-space and
range-space methods, since factors such as ﬁll-in during computation of Z vary signiﬁcantly
even among problems of the same dimension. In general, we can recommend the range-
space method when G is positive deﬁnite and AG−1AT can be computed cheaply (because
G is easy to invert or because m is small relative to n). Otherwise, the null-space method is
often preferable, in particular when it is much more expensive to compute factors of G than
to compute the null-space matrix Z and the factors of ZT GZ.
A METHOD BASED ON CONJUGACY
The range-space and null-space methods described above are useful in a wide variety
of settings and applications. The following approach is of more limited use, but has proved
to be the basis for efﬁcient methods for convex QP. It is applicable in the case where the
Hessian G is positive deﬁnite, and can be regarded as a null-space method that makes clever
use of conjugacy to provide a particularly simple formula for the step computation. (See
Chapter 5 for a deﬁnition of conjugacy.)
Thekeyideaistocomputeann×nnonsingularmatrixW withthefollowingproperties:
W T GW  I,
AW 

0
U

,
(16.20)
where U is an m × m upper triangular matrix. The ﬁrst equation in (16.20) states that the
columns of W are conjugate with respect to the Hessian G, whereas the second condition
implies that the ﬁrst n −m columns of W lie in the null space of A.
The matrix W can be constructed explicitly by using the QR and Cholesky factor-
izations. (These factorizations are described in Section A.2.) By using a variant of the QR
factorization (see Exercise 6), we can ﬁnd an orthogonal matrix Q and an m × m upper
triangular matrix ˆU such that
AQ 
%
0
ˆU
&
.

1 6 . 3 .
I n e q u a l i t y - C o n s t r a i n e d P r o b l e m s
451
SinceQT GQissymmetricandpositivedeﬁnite,wecancomputeitsCholeskydecomposition
QT GQ  LLT . By deﬁning W  QL−T , we ﬁnd that (16.20) is satisﬁed with U  ˆUL−T .
We can partition the columns of W to obtain the matrices Z and Y used in (16.16),
by writing
W 

Z
Y

,
where Z is the ﬁrst n −m columns and Y is the last m columns. By using this deﬁnition and
(16.20), we ﬁnd that
ZT GZ  I,
ZT GY  0,
Y T GY  I,
(16.21)
and
AY  U,
AZ  0.
(16.22)
These relations allow us to simplify the equations (16.17), (16.18), and (16.19) that are used
to solve for pY, pZ, and λ∗, respectively. We have
UpY  −c,
(16.23a)
pZ  −ZT g,
(16.23b)
U T λ∗ Y T g + pY,
(16.23c)
so these vectors can be recovered at the cost of two triangular substitutions involving U and
matrix–vector products involving Y T and ZT . The vector p can then be recovered from
(16.16).
16.3
INEQUALITY-CONSTRAINED PROBLEMS
In the remainder of the chapter we discuss several classes of algorithms for solving QPs that
contain inequality constraints and possibly equality constraints. Classical active-set methods
can be applied both to convex and nonconvex problems, and they have been the most
widely used methods since the 1970s. Gradient–projection methods attempt to accelerate the
solution process by allowing rapid changes in the active set, and are most efﬁcient when the
only constraints in the problem are bounds on the variables. Interior-point methods have
recently been shown to be effective for solving large convex quadratic programs.
We discuss these methods in the following sections. We mention also that quadratic
programs can also be solved by the augmented Lagrangian methods of Chapter 17, or by
means of an exact penalization method such as the Sℓ1QP approach discussed in Chapter 18.

452
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
OPTIMALITY CONDITIONS FOR INEQUALITY-CONSTRAINED PROBLEMS
Webeginourdiscussionwithabriefreviewoftheoptimalityconditionsforinequality-
constrained quadratic programming, and discuss some of the less obvious properties of the
solutions.
TheconclusionsofTheorem12.1canbeappliedto(16.1)bynotingthattheLagrangian
for this problem is
L(x, λ)  1
2xT Gx + xT d −

i∈I∪E
λi(aT
i x −bi).
(16.24)
In addition, we deﬁne the active set A(x∗) at an optimal point x∗as in (12.29) as the indices
of the constraints at which equality holds, that is,
A(x∗) 
)
i ∈E ∪I : aT
i x∗ bi
*
.
(16.25)
By simplifying the general conditions (12.30), we conclude that any solution x∗of (16.1)
satisﬁes the following ﬁrst-order conditions:
Gx∗+ d −

i∈A(x∗)
λ∗
i ai  0,
(16.26a)
aT
i x∗ bi,
for all i ∈A(x∗),
(16.26b)
aT
i x∗≥bi,
for all i ∈I\A(x∗),
(16.26c)
λ∗
i ≥0,
for all i ∈I ∩A(x∗).
(16.26d)
Atechnicalpoint:InTheorem12.1weassumedthatthelinearindependenceconstraint
qualiﬁcation (LICQ) was satisﬁed. As mentioned in Chapter 12, this theorem still holds if we
replace LICQ by other constraint qualiﬁcations, such as linearity of the constraints, which
is certainly satisﬁed for quadratic programming. Hence, in the optimality conditions for
quadratic programming given above we need not assume that the active constraints are
linearly dependent at the solution.
We omit a detailed discussion of the second-order conditions, which follow from the
theory of Section 12.4. Second-order sufﬁcient conditions for x∗to be a local minimizer are
satisﬁed if ZT GZ is positive deﬁnite, where Z is deﬁned to be a null-space basis matrix for
the active constraint Jacobian matrix
[ai]T
i∈A(x∗) .
As we showed in Theorem 16.2, x∗is actually a global solution for the equality-constrained
case when this condition holds. When G is not positive deﬁnite, the general problem (16.1)
may have more than one strict local minimizer at which the second-order necessary condi-
tions are satisﬁed. Such problems are referred to as being “nonconvex” or “indeﬁnite,” and

1 6 . 3 .
I n e q u a l i t y - C o n s t r a i n e d P r o b l e m s
453
x*
x**
x~
x
feasible
region
+
+
x**
x*
-
-
Figure 16.1
Nonconvex quadratic programs
they cause some complication for algorithms. Examples of indeﬁnite quadratic programs are
illustrated in Figure 16.1. On the left we have plotted the contours of a problem in which G
has one positive and one negative eigenvalue. We have indicated by + or −that the function
tends toward plus or minus inﬁnity in that direction. Note that x∗∗is a local maximizer, x∗
a local minimizer, and the center of the box is a stationary point. The picture on the right
in Figure 16.1, in which both eigenvalues of G are negative, shows a global maximizer at ˜x
and local minimizers at x∗and x∗∗.
DEGENERACY
A second property that causes difﬁculties for some algorithms is degeneracy. Unfortu-
nately, this term has been given a variety of meanings, which can cause confusion. Essentially,
it refers to situations in which either
(a) the active constraint gradients ai, i ∈A(x∗), are linearly dependent at the solution x∗,
or
(b) the strict complementarity condition of Deﬁnition 12.2 fails to hold, that is, the optimal
Lagrange multiplier vector λ∗has λ∗
i  0 for some active index i ∈A(x∗). Such
constraints are weakly active according to Deﬁnition 12.3.
Two examples of degeneracy are shown in Figure 16.2. In the left-hand picture, there
is a single active constraint at the solution x∗, which is also an unconstrained minimizer of
the objective function. In the notation of (16.26a), we have that Gx∗+ d  0, so that the
lone Lagrange multiplier must be zero. In the right-hand picture, three constraints are active

454
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
x*
x*
Figure 16.2
Degenerate solutions of quadratic programs.
at the solution. Since each of the three constraint gradients is a vector in IR2, they must be
linearly dependent.
A more subtle case of degeneracy is illustrated by the problem
min x2
1 + (x2 + 1)2
s.t.
x ≥0,
which has a solution at x∗ 0. The unconstrained minimizer does not lie on one of the
constraints,noraretheremorethann  2activeconstraintsatthesolution.Butthisproblem
is degenerate because the Lagrange multiplier associated with the constraint x1 ≥0 is zero
at the solution.
Degeneracy can cause problems for algorithms for two main reasons. First, linear inde-
pendence of the active constraint gradients can cause numerical difﬁculties in computation
of the null-space matrix Z, and it causes the matrix AG−1AT that arises in the range-space
method to become singular. (Here, A denotes the matrix whose rows are the active con-
straints.) Second, when the problem contains weakly active constraints, it is difﬁcult for the
algorithm to determine whether or not these constraints are active at the solution or not.
In the case of active-set methods and gradient projection methods described below, this
“indecisiveness” can cause the algorithm to zigzag, as the iterates move on and off the weakly
active constraints on successive iterations. Safeguards must be used in these algorithms to
prevent such behavior.

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
455
16.4
ACTIVE-SET METHODS FOR CONVEX QP
We now describe active-set methods, which are generally the most effective methods for
small- to medium-scale problems. We start by discussing the convex case, in which the
matrix G in (16.1a) is positive semideﬁnite. Since the feasible region deﬁned by (16.1b),
(16.1c) is a convex set, any local solution of the QP is a global minimizer. The case in which
G is an indeﬁnite matrix raises complications in the algorithms; our discussion of this case
appears in the next section.
Recall our deﬁnition (16.25) above of the active set A(x) at the optimal point x∗. We
will call it an optimal active set.
If A(x∗) were known in advance, we could ﬁnd the solution by applying one of the
techniques for equality-constrained QP of Section 16.2 to the problem
min
x
q(x)  1
2xT Gx + xT d
subject to
aT
i x  bi, i ∈A(x∗).
Of course, we usually don’t have prior knowledge of A(x∗), and as we will now see, de-
termination of this set is the main challenge facing algorithms for inequality-constrained
QP.
We have already encountered an active-set approach for linear programming in Chap-
ter 13, namely the simplex method. An active-set method starts by making a guess of the
optimal active set, and if this guess turns out to be incorrect, it repeatedly uses gradient and
Lagrange multiplier information to drop one index from the current estimate of A(x∗) and
add a new index. Active-set methods for QP differ from the simplex method in that the
iterates may not move from one vertex of the feasible region to another. Some iterates (and,
indeed, the solution of the problem) may lie at other points on the boundary or interior of
the feasible region.
Active-set methods for QP come in three varieties, known as primal, dual, and primal–
dual. We restrict our discussion to primal methods, which generate iterates that remain
feasible with respect to the primal problem (16.1) while steadily decreasing the primal
objective function q(·).
Primal active-set methods usually start by computing a feasible initial iterate x0, and
then ensure that all subsequent iterates remain feasible. They ﬁnd a step from one iterate to
the next by solving a quadratic subproblem in which a subset of the constraints in (16.1b),
(16.1c) is imposed as equalities. This subset is referred to as the working set and is denoted at
the kth iterate xk by Wk. It consists of all the equality constraints i ∈E (see 16.1b) together
with some—but not necessarily all—of the active inequality constraints. An important re-
quirement we impose on Wk is that the gradients ai of the constraints in the working set
be linearly independent, even when the full set of active constraints at that point has lin-
early dependent gradients. Later, we discuss how this condition can be enforced without
compromising convergence of the algorithm to a solution.

456
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
Given an iterate xk and the working set Wk, we ﬁrst check whether xk minimizes
the quadratic q in the subspace deﬁned by the working set. If not, we compute a step p
by solving an equality-constrained QP subproblem in which the constraints corresponding
to the working set Wk are regarded as equalities and all other constraints are temporarily
disregarded. To express this subproblem in terms of the step p, we deﬁne
p  x −xk,
gk  Gxk + d,
and by substituting for x into the objective function (16.1a), we ﬁnd that
q(x)  q(xk + p)  1
2pT Gp + gT
k p + c,
where c 
1
2xT
k Gxk + dT xk is a constant term. Since we can drop c from the objective
without changing the solution of the problem, we can write the QP subproblem to be solved
at the kth iteration as follows:
min
p
1
2pT Gp + gT
k p
(16.27a)
subject to aT
i p  0 for all i ∈Wk.
(16.27b)
We denote the solution of this subproblem by pk. Note that for each i ∈Wk, the term aT
i x
does not change as we move along pk, since we have aT
i (xk + pk)  aT
i xk  bi. It follows
that since the constraints in Wk were satisﬁed at xk, they are also satisﬁed at xk + αpk, for
any value of α. When G is positive deﬁnite, the solution of (16.27b) can be computed by any
of the techniques described in Section 16.2.
Suppose for the moment that the optimal pk from (16.27) is nonzero. We need to
decide how far to move along this direction. If xk + pk is feasible with respect to all the
constraints, we set xk+1  xk + pk. Otherwise, we set
xk+1  xk + αkpk,
(16.28)
where the step-length parameter αk is chosen to be the largest value in the range [0, 1) for
which all constraints are satisﬁed. We can derive an explicit deﬁnition of αk by considering
what happens to the constraints i /∈Wk, since the constraints i ∈Wk will certainly be
satisﬁed regardless of the choice of αk. If aT
i pk ≥0 for some i /∈Wk, then for all αk ≥0 we
have aT
i (xk +αkpk) ≥aT
i xk ≥bi. Hence, this constraint will be satisﬁed for all nonnegative
choices of the step-length parameter. Whenever aT
i pk < 0 for some i /∈Wk, however, we
have that aT
i (xk + αkpk) ≥bi only if
αk ≤bi −aT
i xk
aT
i pk
.

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
457
Since we want αk to be as large as possible in [0, 1] subject to retaining feasibility, we have
the following deﬁnition:
αk
def min
#
1,
min
i /∈Wk, aT
i pk<0
bi −aT
i xk
aT
i pk
$
.
(16.29)
Wecalltheconstraintsi forwhichtheminimumin(16.29)isachievedtheblockingconstraints.
(If αk  1 and no new constraints are active at xk + αkpk, then there are no blocking
constraints on this iteration.) Note that it is quite possible for αk to be zero, since we could
have aT
i pk < 0 for some constraint i that is active at xk but not a member of the current
working set Wk.
If αk < 1, that is, the step along pk was blocked by some constraint not in Wk, a new
working set Wk+1 is constructed by adding one of the blocking constraints to Wk.
We continue to iterate in this manner, adding constraints to the working set until we
reach a point ˆx that minimizes the quadratic objective function over its current working set
ˆW. It is easy to recognize such a point because the subproblem (16.27) has solution p  0.
Since p  0 satisﬁes the optimality conditions (16.5) for (16.27), we have that

i∈ˆW
ai ˆλi  g  Gˆx + d,
(16.30)
for some Lagrange multipliers ˆλi, i ∈
ˆW. It follows that ˆx and ˆλ satisfy the ﬁrst KKT
condition (16.26a), if we deﬁne the multipliers corresponding to the inequality constraints
that are not in the working set to be zero. Because of the control imposed on the step-length,
ˆx is also feasible with respect to all the constraints, so the second and third KKT conditions
(16.26b) and (16.26c) are satisﬁed by ˆx.
We now examine the signs of the multipliers corresponding to the inequality con-
straints in the working set, that is, the indices i ∈
ˆW ∩I. If these multipliers are all
nonnegative, the fourth KKT condition (16.26d) is also satisﬁed, so we conclude that ˆx is a
KKT point for the original problem (16.1). In fact, since G is positive semideﬁnite, we can
show that ˆx is a local minimizer. When G is positive deﬁnite, ˆx is a strict local minimizer.
If, on the other hand, one of the multipliers ˆλj, j ∈ˆW ∩I, is negative, the condition
(16.26d) is not satisﬁed, and the objective function q(·) may be decreased by dropping this
constraint, as shown in Section 12.2. We then remove an index j corresponding to one of
the negative multipliers from the working set and solve a new subproblem (16.27) for the
new step. We show in the following theorem that this strategy produces a direction p at the
next iteration that is feasible with respect to the dropped constraint. We continue to assume
that the vectors in the working set are linearly independent, and we defer a discussion of
how this can be achieved to the next section when the algorithm has been fully stated.

458
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
Theorem 16.4.
Suppose that the point ˆx satisﬁes ﬁrst-order conditions for the equality-constrained sub-
problem with working set ˆW; that is, equation (16.30) is satisﬁed along with aT
i ˆx  bi for all
i ∈ˆW. Suppose, too, that the constraint gradients ai, i ∈ˆW, are linearly independent, and that
there is an index j ∈ˆW such that ˆλj < 0. Finally, let p be the solution obtained by dropping
the constraint j and solving the following subproblem:
min
p
1
2pT Gp + (Gˆx + d)T p,
(16.31a)
subject to aT
i p  0, for all i ∈ˆW with i ̸ j.
(16.31b)
(This is the subproblem to be solved at the next iteration of the algorithm.) Then p is a feasible
direction for constraint j, that is, aT
j p ≥0. Moreover, if p satisﬁes second-order sufﬁcient
conditions for (16.31), then we have that aT
j p > 0, and p is a descent direction for q(·).
Proof.
Since p solves (16.31), we have from the results of Section 16.1 that there are
multipliers ˜λi, for all i ∈ˆW with i ̸ j, such that

i∈ˆW, i̸j
˜λiai  Gp + (Gˆx + d).
(16.32)
In addition, we have by second-order necessary conditions that if Z is a null-space basis
vector for the matrix
[ai]T
i∈ˆW, i̸j ,
then ZT GZ is positive semideﬁnite. Clearly, p has the form p  ZpZ for some vector pZ,
so it follows that pT Gp ≥0.
We have made the assumption that ˆx and ˆW satisfy the relation (16.30). By subtracting
(16.30) from (16.32), we obtain

i∈W, i̸j
(˜λi −ˆλi)ai −ˆλjaj  Gp.
(16.33)
By taking inner products of both sides with p and using the fact that aT
i p  0 for all i ∈ˆW
with i ̸ j, we have that
−ˆλjaT
j p  pT Gp.
(16.34)
Since pT Gp ≥0 and ˆλj < 0 by assumption, it follows immediately that aT
j p ≥0.
If the second-order sufﬁcient conditions are satisﬁed, we have that ZT GZ deﬁned
aboveispositivedeﬁnite.From(16.34)wecanhaveaT
j p  0onlyifpT Gp  pT
Z ZT GZpZ 

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
459
0, which happens only if pZ  0 and p  0. But if p  0, then by substituting into (16.33)
and using linear independence of ai for i ∈ˆW, we must have that ˆλj  0, which contradicts
our choice of j. We conclude that pT Gp > 0 in (16.34), and therefore aT
j p > 0 whenever
p satisﬁes the second-order sufﬁcient conditions for (16.31).
□
While any index j for which ˆλj < 0 usually will give directions along which the
algorithm can make progress, the most negative multiplier is often chosen in practice (and
in the algorithm speciﬁed below). This choice is motivated by the sensitivity analysis given
in Chapter 12, which shows that the rate of decrease in the objective function when one
constraint is removed is proportional to the magnitude of the Lagrange multiplier for that
constraint.
We conclude with a result that shows that whenever pk obtained from (16.27) is
nonzero and satisﬁes second-order sufﬁcient optimality conditions for the current working
set, then it is a direction of strict descent for q(·).
Theorem 16.5.
Suppose that the solution pk of (16.27) is nonzero and satisﬁes the second-order sufﬁcient
conditions for optimality for that problem. Then the function q(·) is strictly decreasing along
the direction pk.
Proof.
Since pk satisﬁes the second-order conditions, that is, ZT GZ is positive deﬁnite
for the matrix Z whose columns are a basis of the null space of the constraints (16.27b), we
have by applying Theorem 16.2 to (16.27) that pk is the unique global solution of (16.27).
Since p  0 is also a feasible point for (16.27), its objective value in (16.27a) must be larger
than that of pk, so we have
1
2pT
k Gpk + gT
k pk < 0.
Since pT
k Gpk ≥0 by convexity, this inequality implies that gT
k pk < 0. Therefore, we have
q(xk + αkpk)  q(xk) + αgT
k pk + 1
2α2pT
k Gpk < q(xk),
for all α > 0 sufﬁciently small.
□
A corollary of this result is that when G is positive deﬁnite—the strictly convex case—
the second-order sufﬁcient conditions are satisﬁed for all feasible subproblems of the form
(16.27), so that we obtain a strict decrease in q(·) whenever pk ̸ 0. This fact is signiﬁcant
in a later section, when we discuss ﬁnite termination of the algorithm.

460
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
SPECIFICATION OF THE ACTIVE-SET METHOD FOR CONVEX QP
Having given a complete description of the active-set algorithm for convex QP, it is
time for the following formal speciﬁcation:
Algorithm 16.1 (Active-Set Method for Convex QP).
Compute a feasible starting point x0;
Set W0 to be a subset of the active constraints at x0;
for
k  0, 1, 2, . . .
Solve (16.27) to ﬁnd pk;
if
pk  0
Compute Lagrange multipliers ˆλi that satisfy (16.30),
set ˆW  Wk;
if
ˆλi ≥0 for all i ∈Wk ∩I;
STOP with solution x∗ xk;
else
Set j  arg minj∈Wk∩I ˆλj;
xk+1  xk; Wk+1 ←Wk\{j};
else
(* pk ̸ 0 *)
Compute αk from (16.29);
xk+1 ←xk + αkpk;
if
there are blocking constraints
Obtain Wk+1 by adding one of the blocking
constraints to Wk+1;
else
Wk+1 ←Wk;
end (for)
Various techniques can be used to determine an initial feasible point. One such is to
use the “Phase I” approach described in Chapter 13. Though no signiﬁcant modiﬁcations
are needed to generalize this method from linear programming to quadratic programming,
we describe a variant here that allows the user to supply an initial estimate ˜x of the vector
x. This estimate need not be feasible, but prior knowledge of the QP may be used to select
a value of ˜x that is “not too infeasible,” which will reduce the work needed to perform the
Phase I step.
Given ˜x, we deﬁne the following feasibility linear program:
min
(x,z)
eT z
subject to aT
i x + γizi  bi,
i ∈E,
aT
i x + γizi ≥bi
i ∈I,
z ≥0,

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
461
where e  (1, . . . , 1)T , γi  −sign(aT
i ˜x −bi) for i ∈E, while γi  1 for i ∈I. A feasible
initial point for this problem is then
x  ˜x,
zi  |aT
i ˜x −bi| (i ∈E),
zi  max(bi −aT
i ˜x, 0) (i ∈I).
It is easy to verify that if ˜x is feasible for the original problem (16.1), then (˜x, 0) is optimal for
the feasibility subproblem. In general, if the original problem has feasible points, then the
optimal objective value in the subproblem is zero, and any solution of the subproblem yields
a feasible point for the original problem. The initial working set W0 for Algorithm16.1 can
be found by taking a linearly independent subset of the active constraints at the x component
of the solution of the feasibility problem.
An alternative approach is the so-called “big M” method, which does away with the
“Phase I” and instead includes a measure of infeasibility in the objective that is guaranteed
to be zero at the solution. That is, we introduce a scalar artiﬁcial variable t into (16.1) to
measure the constraint violation, and solve the problem
min
(x,t)
1
2xT Gx + xT d + Mt,
(16.35a)
subject to t ≥(aT
i x −bi),
i ∈E,
(16.35b)
t ≥−(aT
i x −bi),
i ∈E,
(16.35c)
t ≥bi −aT
i x,
i ∈I,
(16.35d)
t ≥0,
(16.35e)
for some large positive value of M. It can be shown by applying the theory of exact penalty
functions (see Chapter 15) that whenever there exist feasible points for the original problem
(16.1), then for all M sufﬁciently large, the solution of (16.35) will have t  0, with an x
component that is a solution for (16.1).
Our strategy is to use some heuristic to choose a value of M and solve (16.35) by
the usual means. If the solution we obtain has a positive value of t, we increase M and try
again. Note that a feasible point is easy to obtain for the subproblem (16.35): We set x  ˜x
(where, as before, ˜x is the user-supplied initial guess) and choose t large enough that all the
constraints in (16.35) are satisﬁed.
This approach is related to the Sℓ1QP method described in Chapter 18; the main
difference is that the “big M” method is based on the inﬁnity norm rather than on the ℓ1
norm. (See (12.7) in Chapter 12.)
AN EXAMPLE
In this section we use subscripts on the vector x to denote its components, while
superscripts denote the iteration index. For example, x4 denotes the fourth iterate of x,
while x1 denotes the ﬁrst component.

462
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
x1
x , x
2
3
x , x
0
1
x2
x4
(2,0)
(2,2)
x 5
(4,1)
(0,1)
Figure 16.3
Iterates of the active-set method.
❏Example 16.3
We apply Algorithm 16.1 to the following simple 2-dimensional problem, which is
illustrated in Figure 16.3.
min
x
q(x)  (x1 −1)2 + (x2 −2.5)2
(16.36a)
subject to
x1 −2x2 + 2 ≥0,
(16.36b)
−x1 −2x2 + 6 ≥0,
(16.36c)
−x1 + 2x2 + 2 ≥0,
(16.36d)
x1 ≥0,
(16.36e)
x2 ≥0.
(16.36f)
We label the constraints, in order, with the indices 1 through 5. For this problem it is
easy to determine a feasible initial point; suppose that we choose x0  (2, 0). Constraints 3
and 5 are active at this point, and we set W0  {3, 5}. (Note that we could just as validly have
chosen W0  {5} or W0  {3} or even W  ∅; each would lead the algorithm to perform
quite differently.)
Sincex0 liesonavertexofthefeasibleregion,itisobviouslyaminimizeroftheobjective
function q with respect to the working set W0; that is, the solution of (16.27) with k  0 is

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
463
p  0. We can then use (16.30) to ﬁnd the multipliers ˆλ3 and ˆλ5 associated with the active
constraints. Substitution of the data from our problem into (16.30) yields

−1
2

ˆλ3 +

0
1

ˆλ5 

2
−5

,
which has the solution (ˆλ3, ˆλ5)  (−2, −1).
We now remove constraint 3 from the working set, since it has the most negative
multiplier, and set W1  {5}. We begin iteration 1 by ﬁnding the solution of (16.27) for
k  1, which is p1  (−1, 0)T . The step-length formula (16.29) yields α1  1, and the new
iterate is x2  (1, 0).
There are no blocking constraints, so that W2  W1  {5}, and we ﬁnd at the start
of iteration 2 that the solution of (16.27) is again p2  0. From (16.30) we deduce that
the Lagrange multiplier for the lone working constraint is ˆλ5  −5, so we drop 5 from the
working set to obtain W3  ∅.
Iteration 3 starts by solving the unconstrained problem, to obtain the solution p3 
(0, 2.5). The formula (16.29) yields a step length of α3  0.6 and a new iterate x4  (1, 1.5).
There is a single blocking constraint—constraint 1—so we obtain W4  {1}. The solution
of (16.27) for k  4 is then p4  (0.4, 0.2), and the new step length is 1. There are no
blocking constraints on this step, so the next working set is unchanged: W5  {1}. The new
iterate is x5  (1.4, 1.7).
Finally, we solve (16.27) for k  5 to obtain a solution p5  0. The formula (16.30)
yields a multiplier ˆλ1  1.25, so we have found the solution. We set x∗ (1.4, 1.7) and
terminate.
❐
FURTHER REMARKS ON THE ACTIVE-SET METHOD
We noted above that there is ﬂexibility in the choice of the initial working set, and that
each initial choice leads to a different iteration sequence. When the initial active constraints
have independent gradients, as above, we can include them all in W0. Alternatively, we can
select a subset. For instance, if in the example above we have chosen W0  {3}, the ﬁrst
iterate would have yielded p0  (0.2, 0.1) and a new iterate of x1  (2.2, 0.1). If we had
chosen W0  {5}, we would have moved immediately to the new iterate x1  (1, 0), without
ﬁrst performing the operation of dropping the index 3, as is done in the example. Finally,
if we had selected W0  ∅, we would obtain p1  (−1, 2.5), α1 
2
3, a new iterate of
x1  ( 4
3, 5
3), and a new working set of W1  {1}. The solution x∗would have been found
on the next iteration.
Even if the initial working set W0 coincides with the initial active set, the sets Wk and
A(xk)maydifferatlateriterations.Forinstance,whenaparticularstepencountersmorethan
one blocking constraint, just one of them is added to the working set, so the identiﬁcation

464
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
between Wk and A(xk) is broken. Moreover, subsequent iterates differ in general according
to what choice is made.
We require that the constraint gradients in W0 be linearly independent, and our strat-
egy for modifying the working set ensures that this same property holds for all subsequent
working sets Wk. When we encounter a blocking constraint on a particular step, a constraint
that is not in the working set is encountered during the line search, and its constraint normal
cannot be a linear combination of the normals ai in the current working set (see Exercise
15). Hence, linear independence is maintained after the blocking constraint is added to the
working set. On the other hand, deletion of an index from the working set certainly does
not introduce linear dependence.
The strategy of removing the constraint corresponding to the most negative Lagrange
multiplier often works well in practice, but has the disadvantage that it is susceptible to
the scaling of the constraints. (By multiplying constraint i by some factor β > 0 we do
not change the geometry of the optimization problem, but we introduce a scaling of 1/β
to the corresponding multiplier λi.) Choice of the most negative multiplier is analogous to
Dantzig’soriginalpivotruleforthesimplexmethodinlinearprogramming(seeChapter13),
and as we saw there, more sophisticated strategies that were more resistant to scaling often
gave better results. We will not discuss these advanced features here.
Finally, we note that the strategy of adding or deleting at most one constraint at each
iteration of the Algorithm 16.1 places a natural lower bound on the number of iterations
needed to reach optimality. Suppose, for instance, that we have a problem in which m
constraints are active at the solution x∗, but that we start from a point x0 that is strictly
feasible with respect to all the inequality constraints. In this case, the algorithm will need
at least m iterations to move from x0 to x∗. Even more iterations would be required if the
algorithm adds some constraint j to the working set at some iteration, only to remove it at
a later step.
FINITE TERMINATION OF THE CONVEX QP ALGORITHM
It is not difﬁcult to show that Algorithm 16.1 converges for strictly convex QPs under
certain assumptions, that is, it identiﬁes the solution x∗in a ﬁnite number of iterations. This
claim is certainly true if we assume that the method always takes a nonzero step length αk
whenever the direction pk computed from (16.27) is nonzero. Our argument proceeds as
follows:
• If the solution of (16.27) is pk  0, the current point xk is the unique global mini-
mizer of q(·) for the working set Wk; see Theorem 16.5. If it is not the solution of the
original problem (16.1) (that is, at least one of the Lagrange multipliers is negative),
Theorems 16.4 and 16.5 together show that the step pk+1 computed after a constraint
is dropped will be a strict decrease direction for q(·). Therefore, because of our as-
sumption αk > 0, we have that the value of q is lower than q(xk) at all subsequent
iterations. It follows that the algorithm can never return to the working set Wk, since

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
465
subsequent iterates have values of q that are lower than the global minimizer for this
working set.
• The algorithm encounters an iterate k for which pk  0 solves (16.27) at least on
every nth iteration. To show this claim, note that whenever we have an iteration for
which pk ̸ 0, either we have αk  1 (in which case we reach the minimizer of q on
the current working set Wk, so that the next iteration will yield pk+1  0), or else a
constraint is added to the working set Wk. If the latter situation occurs repeatedly, then
after at most n iterations the working set will contain n indices, which correspond to
n linearly independent vectors. The solution of (16.27) will then be pk  0, since only
the zero vector will satisfy the constraints (16.27b).
• Taken together, the two statements above indicate that the algorithm ﬁnds the global
minimum of q on its current working set periodically (at least once every n iterations)
and that having done so, it never visits this particular working set again. It follows that
since there are only a ﬁnite number of possible working sets, the algorithm cannot
iterate forever. Eventually, it encounters a minimizer for a current working set that
satisﬁes optimality conditions for (16.1), and it terminates with a solution.
The assumption that we can always take a nonzero step along a nonzero descent
direction pk calculated from (16.27) guarantees that the algorithm does not undergo cycling.
This term refers to the situation in which a sequence of consecutive iterations results in no
movementiniteratex,whiletheworkingsetWk undergoesdeletionsandadditionsofindices
and eventually repeats itself. That is, for some integers k and l ≥1, we have that xk  xk+l
and Wk  Wk+l. At some point in the cycle, a constraint is dropped (as in Theorem 16.4)
but a new constraint i /∈Wk is encountered immediately without any movement along
the computed direction p. Procedures for handling degeneracy and cycling in quadratic
programming are similar to those for linear programming discussed in Chapter 13; we will
not discuss them here. Most QP implementations simply ignore the possibility of cycling.
UPDATING FACTORIZATIONS
We have seen that the step computation in the active-set method given in Al-
gorithm 16.1 requires the solution of the equality-constrained subproblem (16.27). As
mentioned at the beginning of this chapter, this computation amounts to solving the KKT
system (16.5). Since the working set can change by just one index at every iteration, the KKT
matrix differs in at most one row and one column from the previous iteration’s KKT matrix.
Indeed, G remains ﬁxed, whereas the matrix A of constraint gradients corresponding to the
current working set may change through addition or deletion of a single row.
It follows from this observation that we can compute the matrix factors needed to solve
(16.27) at the current iteration by updating the factors computed at the previous iteration,
rather than recomputing them from scratch. These updating techniques are crucial to the
efﬁciency of active-set methods.

466
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
We will limit our discussion to the case in which the step is computed by using the null-
space method (16.16)–(16.19). Suppose that A has m linearly independent rows, and assume
that the bases Y and Z are deﬁned by means of a QR factorization of A (see Chapter 15 for
details). Thus
AT   Q

R
0



Q1
Q2


R
0

(16.37)
(see (15.20)), where  is a permutation matrix; R is square, upper triangular and nonsin-
gular; Q 

Q1
Q2

is n × n orthogonal; and Q1 and R both have m columns and Q2
has n −m columns. As noted in Chapter 15, we can choose Z to be simply the orthonormal
matrix Q2.
Suppose that one constraint is added to the working set at the next iteration, so that the
new constraint matrix is ¯AT  [AT , a], where a is a column vector of length n such that ¯AT
retains full column rank. As we now show, there is an economical way to update the Q and
R factors in (16.37) to obtain new factors (and hence a new null-space basis matrix ¯Z, with
n −m −1 columns) for the expanded matrix ¯A. Note ﬁrst that since Q1QT
1 + Q2QT
2  I,
¯AT


0
0
1



A
a

 Q

R
QT
1 a
0
QT
2 a

.
(16.38)
We can now deﬁne an orthogonal matrix ˆQ that transforms the vector QT
2 a to a vector in
which all elements except the ﬁrst are zero. That is, we have
ˆQ(QT
2 a) 

γ
0

,
where γ is a scalar. (Since ˆQ is orthogonal, we have ∥QT
2 a∥ |γ |.) From (16.38) we now
have
¯AT


0
0
1

 Q


R
QT
1 a
0
ˆQT

γ
0


 Q

I
0
0
ˆQT
 

R
QT
1 a
0
γ
0
0

.
This factorization has the form
¯AT ¯  ¯Q

¯R
0

,

1 6 . 4 .
A c t i v e - S e t M e t h o d s f o r C o n v e x Q P
467
where
¯ 


0
0
1

, ¯Q  Q

I
0
0
ˆQT


%
Q1
Q2 ˆQT &
, ¯R 

R
QT
1 a
0
γ

.
We can therefore choose ¯Z to be the last n−m−1 columns of Q2 ˆQT . If we know Z explicitly
and need an explicit representation of ¯Z, we need to account for the cost of obtaining ˆQ and
the cost of forming the product Q2 ˆQT  Z ˆQT . Because of the special structure of ˆQ, this
cost is of order n(n −m), compared to the cost of computing (16.37) from scratch, which is
of order n2m. The updating strategy is much less expensive, especially when the null space
is small (that is, n −m ≪n).
An updating technique can also be designed for the case in which a row is removed
from A. This operation has the effect of deleting a column from R in (16.37), which disturbs
the upper triangular property of this matrix by introducing a number of nonzeros on the
diagonal immediately below the main diagonal of the matrix. Upper triangularity can be
restored by applying a sequence of plane rotations. These rotations introduce a number
of inexpensive transformations into Q, and the updated null-space matrix is obtained by
selecting the last n−m+1 columns from this matrix after the transformations are complete.
The new null-space basis in this case will have the form
¯Z 

Z
¯z

,
(16.39)
that is, the current matrix Z is augmented by a single column. The total cost of this oper-
ation varies with the location of the removed column in A, but is in all cases cheaper than
recomputing a QR factorization from scratch. For details of these procedures, see Gill et
al. [105, Section 5].
LetusnowconsiderthereducedHessian.Becauseofthespecialformof(16.27)wehave
c  0 in (16.5), and the step pY given in (16.17) is zero. Thus from (16.18), the null-space
component pZ is the solution of
(ZT GZ)pZ  −ZT g.
(16.40)
We can sometimes ﬁnd ways of updating the factorization of the reduced Hessian ZT GZ
after Z has changed. Suppose that we have the Cholesky factorization of the current reduced
Hessian, written as
ZT GZ  LLT ,
and that at the next step Z changes as in (16.39), gaining a column after deletion of a
constraint. A series of plane rotations can then be used to transform the Cholesky factor L
into the new factor ¯L for the new reduced Hessian ¯ZT G ¯Z.

468
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
Avarietyofothersimpliﬁcationsarepossible.Forexample,asdiscussedinSection16.6,
we can update the reduced gradient ZT g at the same time as we update Z to ¯Z.
16.5
ACTIVE-SET METHODS FOR INDEFINITE QP
We now consider the case in which the Hessian matrix G has some negative eigenvalues.
Algorithm 16.1, the active-set method for convex QP, can be adapted to this indeﬁnite case
by modifying the computation of the search direction and step length in certain situations.
To explain the need for the modiﬁcation, we consider the computation of a step by a
null-space method, that is, p  ZpZ, where pZ is given by (16.40). If the reduced Hessian
ZT GZ is positive deﬁnite, then this step p points to the minimizer of the subproblem
(16.27),andthelogicoftheiterationneednotbechanged.If ZT GZ hasnegativeeigenvalues,
however,p pointsonlytoasaddlepointof(16.27)andisthereforenotasuitablestep.Instead,
we seek an alternative direction sZ that is a direction of negative curvature for ZT GZ. We
then have that
q(x + αZsZ) →−∞
as α →∞.
(16.41)
Additionally, we can choose the sign of sZ so that ZsZ is a non–ascent direction for q at
the current point x, that is, ∇q(x)T ZsZ ≤0. By moving along the direction ZsZ, we will
encounter a constraint that can then be added to the working set for the next iteration.
(If we don’t ﬁnd such a constraint, the problem is unbounded.) If the reduced Hessian
for the new working set is not positive deﬁnite, we can repeat this process until enough
constraints have been added to make the reduced Hessian positive deﬁnite. A difﬁculty
with this general approach, however, is that if we allow the reduced Hessian to have several
negative eigenvalues, we need to compute its spectral factorization or symmetric indeﬁnite
factorization in order to obtain appropriate negative curvature directions, but it is difﬁcult
to make these methods efﬁcient when the reduced Hessian changes from one working set to
the next.
Inertia controlling methods are a practical class of algorithms for indeﬁnite QP that
never allow the reduced Hessian to have more than one negative eigenvalue. As in the convex
case, there is a preliminary phase in which a feasible starting point x0 is found. We place
the additional demand on x0 that it be either a vertex (in which case the reduced Hessian is
the null matrix) or a constrained stationary point at which the reduced Hessian is positive
deﬁnite. (We see below how these conditions can be met.) At each iteration, the algorithm
will either add or remove a constraint from the working set. If a constraint is added, the
reduced Hessian is of smaller dimension and must remain positive deﬁnite or be the null
matrix (see the exercises). Therefore, an indeﬁnite reduced Hessian can arise only when one
of the constraints is removed from the working set, which happens only when the current

1 6 . 5 .
A c t i v e - S e t M e t h o d s f o r I n d e f i n i t e Q P
469
point is a minimizer with respect to the current working set. In this case, we will choose the
new search direction to be a direction of negative curvature for the reduced Hessian.
There are various algorithms for indeﬁnite QP that differ in the way that indeﬁniteness
is detected, in the computation of the negative curvature direction, and in the handling of
the working set. We now discuss an algorithm that makes use of pseudo-constraints (as
proposed by Fletcher [80]) and that computes directions of negative curvature by means of
the LDLT factorization (as proposed by Gill and Murray [107]).
Suppose that the current reduced Hessian ZT GZ is positive deﬁnite and that it is
factored as ZT GZ  LDLT , where L is unit lower triangular and D is diagonal with
positive diagonal entries. We denote the number of elements in the current working set W
by t. After removing a constraint from the working set, the new null-space basis can be
chosen in the form Z+  [Z | z] (that is, one additional column), so that the new factors
have the form
L+ 

L
0
lT
1

,
D+ 

D
0
0
dn−t+1

,
for some vector l and element dn−t+1. If we discover that dn−t+1 in D is negative, we know
thatthereducedHessianisindeﬁniteonthemanifolddeﬁnedbythenewworkingset.Wecan
then compute a direction sZ of negative curvature for ZT GZ and a corresponding direction
s of negative curvature for G as follows:
LT
+sZ  en−t+1,
s  Z+sZ.
We can verify that these directions have the desired properties:
sT Gs  sT
Z ZT
+GZ+sZ
 sT
Z L+D+LT
+sZ
 eT
n−t+1D+en−t+1
 dn−t+1 < 0.
Before moving along this direction s, we will deﬁne the working set in a special way.
Suppose that i is the index of the constraint that is scheduled to be removed from the working
set—the index whose removal causes the reduced Hessian to become indeﬁnite and leads
to the negative curvature direction s derived above. Rather then removing i explicitly from
the working set, as we do in Algorithm 16.1, we leave it in the working set as a pseudo-
constraint. By doing so, we ensure that the reduced Hessian for this working set remains
positive deﬁnite. We now move along the negative curvature direction s until we encounter
a constraint, and then continue to take steps in the usual manner, adding constraints until
the solution of an equality-constrained subproblem is found. If at this point we can safely
delete the pseudo-constraint i from the working set while retaining positive deﬁniteness of

470
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
the reduced Hessian, then we do so. If not, we retain it in the working set until a similar
opportunity arises on a later iteration.
We illustrate this strategy with a simple example.
ILLUSTRATION
Consider the following indeﬁnite quadratic program in two variables:
min 1
2xT

1
0
0
−1

x,
(16.42a)
subject to x2 ≥0,
(16.42b)
x1 + 2x2 ≥2,
(16.42c)
−5x1 + 4x2 ≤10,
(16.42d)
x1 ≤3.
(16.42e)
We use superscripts to number the iterates of the algorithm, and use subscripts to
denote components of a vector. We choose the initial point x1  (2, 0) and deﬁne the
working set as W  {1, 2}, where we have numbered the constraints in the order they
appear in (16.42); see Figure 16.4. Since x1 is a vertex, it is the solution with respect to the
3
2
1
x  +2x  =2
1
2
x 1
x  =0
2
x  =3
1
4
4
x
-5x  +4x  =10
2
1
x 3
x2
-2
2
Figure 16.4
Iterates of indeﬁnite QP algorithm.

1 6 . 5 .
A c t i v e - S e t M e t h o d s f o r I n d e f i n i t e Q P
471
initial working set W. We compute Lagrange multipliers by solving the system

0
1
1
2
 
λ1
λ2



2
0

,
giving (λ1, λ2)  (−4, 2). We must therefore remove the ﬁrst constraint from the working
set.
The working set is redeﬁned as W  {2}, and we can deﬁne the null-space basis as
Z  (2, −1)T . Since ZT GZ  3, we have that L  1 and D  3, which is positive. We solve
the equality-constrained subproblem for the current working set, and ﬁnd that the solution
is x2  (−2
3, 4
3)T , and λ2  −2
3.
Since this multiplier is negative, we must remove the second constraint from the
working set. We then have W  ∅and could deﬁne
Z 

2
1
−1
2

,
where the second column has been chosen to be perpendicular to the ﬁrst. To simplify the
computations, however, let us deﬁne Z  I, so that L  I and D  G. This D factor has
a negative element in the second diagonal element; see (16.42a). We have now encountered
the situation where an indeﬁnite QP algorithm must differ from an algorithm for convex
QP.
By solving the system LT sZ  en−t  e2 we ﬁnd that sZ  (0, 1)T and the direction
of negative curvature is given by s  ZsZ  (0, 1)T . The direction s is indeed a descent
direction, because we have
∇q(x2)T s 

−2/3
−4/3
T 
0
1

< 0.
(If this inequality were not true, we could simply change the sign of s.) Before moving along
this direction, we include the second constraint as a pseudo-constraint, so that the working
set is redeﬁned as W  {2}.
We now compute the new iterate x3  x2 + αs  (−2
3, 5
3), where α  1
3 is the step
length to the newly encountered third constraint. We add this constraint to the working set,
which is now W  {2, 3}, and update the LDLT factorization as we change the working set
from {2} to {2, 3}. Since the current iterate is a vertex, it is the solution with respect to the
current working set. We also ﬁnd that both L and D are null matrices.
We will now try to remove the pseudo-constraint. We attempt to solve the equality-
constrained subproblem with respect to the working set {3}, but ﬁnd that Z  [4, 5]T and
ZT GZ  −9, so that L  1 and D  −9. In other words, removal of the pseudo-constraint
would yield a reduced Hessian with negative eigenvalues, so we must retain the second

472
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
constraint as a pseudo-constraint for the time being. However, we can use the hypothetical
deletion to calculate a negative-curvature search direction, to move us away from the current
iterate. By solving LT sZ  e2−1  e1 we ﬁnd that sZ  1, and so the desired direction is
s  ZsZ  (4, 5)T . It is easy to verify that s is a descent direction.
We now compute the new iterate x4  x3 + αs  (3, 25/4)T , and ﬁnd that the step
length α  11/12 leads us to the fourth constraint. By adding this constraint to the working
set, we obtain W  {2, 3, 4}. It is now possible to drop the pseudo-constraint 2, since by
doing so the working set would become {3, 4} and the reduced Hessian is the null matrix,
which has no negative eigenvalues. In other words, x4 solves the subproblem with respect to
the working set {3, 4}.
Continuing with the iteration, we compute Lagrange multipliers at x4 by solving the
following system:

3
−25/4



5
−1
−4
0
 
λ3
λ4

.
We obtain (λ3, λ4)  (25/16, 77/16), and we conclude that x4 is a local solution of the
quadratic program (16.42). (In fact, it is also a global solution).
One drawback of this approach is that by including more constraints in the working
set, we increase the possibility of degeneracy.
CHOICE OF STARTING POINT
We mentioned above that the initial point must be chosen so as to be a vertex, or at
least a point at which the reduced-Hessian matrix is positive deﬁnite. To ensure that this
property holds, we may need to introduce artiﬁcial constraints that are active at the initial
point and are linearly independent with respect to the other constraints in the working set.
For example, consider the quadratic program
min
−x1x2
subject to −1 ≤x1 ≤1,
−1 ≤x2 ≤1.
If the starting point is (0, 0)T , no constraints are active, and the reduced Hessian is indeﬁnite
for the working set W0  ∅. By introducing the artiﬁcial constraints x1  0 and x1−x2  0,
the initial point has the desired properties, so we can start the active-set procedure described
above. Once the minimizer for the current working set has been computed, the sign of each
artiﬁcial constraint normal is chosen so that its multiplier is nonpositive and the constraint
may be deleted. Temporary constraints are usually deleted ﬁrst, if there is a choice.

1 6 . 5 .
A c t i v e - S e t M e t h o d s f o r I n d e f i n i t e Q P
473
FAILURE OF THE ACTIVE-SET METHOD
When G is not positive deﬁnite, the inertia controlling algorithm just described is
not guaranteed to ﬁnd a local minimizer of the quadratic program. In fact, there may
exist nonoptimal points at which any active-set method will ﬁnd it difﬁcult to proceed.
These points satisfy the ﬁrst-order optimality conditions (i.e., they are stationary points),
and their reduced Hessian is positive deﬁnite, but at least one of the Lagrange multipliers
corresponding to the inequality constraints is zero. At such a point, it is not possible to
compute a direction that improves the objective function by deleting only one constraint at
a time. We may have to delete two or more constraints at once in order to make progress.
This phenomenon is illustrated by the problem
min −x1x2
subject to 0 ≤x1 ≤1, 0 ≤x2 ≤1.
Suppose that the starting point is (0, 0) and that both active constraints are included in the
initial working set. It is easy to verify that this point is a stationary point, and it is certainly a
minimizer on the given working set by virtue of being a vertex. However, if we delete either
constraint from the working set, the new reduced Hessian is singular. No feasible direction
of negative curvature can therefore be computed by removing only one constraint, so the
active-set method will terminate at this point, which is not a local solution of the quadratic
program. We can decrease the objective by moving along the direction (1, 1), for instance.
Even though this type of failure is possible, it is not very common, and rounding
errors tend to move the algorithm away from such difﬁcult points. Various devices have
been proposed to decrease the probability of failure, but none of them can guarantee that a
solution will be found in all cases. In the example above, we could move x1 from zero to 1
while holding x2 ﬁxed at zero, and this would not affect the value of the objective q. Next we
could move x2 from zero to 1 to obtain the optimal solution.
In general, the cause of this difﬁculty is that ﬁrst-order optimality does not lead to
second-order optimality for active-set methods.
DETECTING INDEFINITENESS USING THE LBLT FACTORIZATION
We can also use the symmetric indeﬁnite factorization algorithm (16.12) to determine
whether the reduced Hessian ZT BZ is positive deﬁnite. Since this factorization is useful
in other optimization methods, such as interior-point methods and sequential quadratic
programming methods, we present some of its fundamental properties.
Recall that the inertia of a symmetric matrix K is deﬁned as the triplet formed by the
number of positive, negative, and zero eigenvalues of K. We write
inertia(K)  (n+, n−, n0).

474
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
Sylvester’s law of inertia states that if C is a nonsingular matrix, then inertia(CT KC) 
inertia(K);see,forexample,[115].Thefollowingresultfollowsfromtherepeatedapplication
of Sylvester’s law.
Theorem 16.6.
Let K be deﬁned by (16.7) and suppose that A has rank m. Then
inertia(K)  inertia(ZT GZ) + (m, m, 0).
This theorem implies that if ZT GZ is positive deﬁnite, then the inertia of the KKT
matrix is (n, m, 0), as stated in Lemma 16.3. Note that Theorem 16.6 also shows that K has
at least m positive eigenvalues, even if G is negative deﬁnite.
Let us suppose that we compute the symmetric indeﬁnite factorization (16.12) of the
KKT matrix K. To simplify the discussion, we assume that the permutation matrix P is the
identity, and write
K  LBLT .
Sylvester’s theorem implies that B and K have the same inertia. Moreover, since the 2 × 2
blocks in B are normally constructed so as to have one positive and one negative eigenvalue,
we can compute the inertia of K by examining the blocks of B. In particular, the number
of positive eigenvalues of K equals the number of 2 × 2 blocks plus the number of positive
1 × 1 blocks.
The symmetric indeﬁnite factorization can therefore be used in inertia controlling
methods for indeﬁnite quadratic programming to control the logic of the algorithm.
16.6
THE GRADIENT--PROJECTION METHOD
In the classical active-set method described in the previous two sections, the active set and
working set change slowly, usually by a single index at each iteration. As a result, this method
may require many iterations to converge on large-scale problems. For instance, if the starting
point x0 has no active constraints, while 200 constraints are active at the solution, then at
least 200 iterations of the active-set method will be required to reach the solution.
The gradient projection method is designed to make rapid changes to the active set.
It is most efﬁcient when the constraints are simple in form—in particular, when there are
only bounds on the variables. Accordingly, we will restrict our attention to the following
bound-constrained problem:
min
x
q(x)  1
2xT Gx + xT d
(16.43a)
subject tol ≤x ≤u,
(16.43b)

1 6 . 6 .
T h e G r a d i e n t - - P r o j e c t i o n M e t h o d
475
where G is symmetric and l and u are are vectors of upper and lower bounds on the compo-
nents of x. (As we see later, the dual of a convex quadratic program is a bound-constrained
problem; thus the gradient projection method is of wide applicability.) The feasible region
deﬁned by (16.43b) is sometimes called a “box” because of its rectangular shape. Some
components of x may lack an upper or a lower bound; we handle these cases by setting the
appropriate components of l and u to −∞and +∞, respectively. We do not make any pos-
itive deﬁniteness assumptions on G, since the gradient projection approach can be applied
to both convex and nonconvex problems.
Each iteration of the gradient projection algorithm consists of two stages. In the ﬁrst
stage, we search along the steepest descent direction from the current point x, that is, the
direction −g, where, as in (16.6), g  Gx + d. When a bound is encountered, the search
direction is “bent” so that it stays feasible. We search along this piecewise path and locate the
ﬁrst local minimizer of q, which we denote by xc and refer to as the Cauchy point, by analogy
with our terminology of Chapter 4. The working set W is now deﬁned to be the set of bound
constraints that are active at the Cauchy point, i.e., W  A(xc). In the second stage of each
gradient projection iteration, we “explore” the face of the feasible box on which the Cauchy
point lies by solving a subproblem in which the active components xi for i ∈A(xc) are ﬁxed
at the values xc
i . In this section we denote the iteration number by a superscript (i.e., xk)
and use subscripts to denote the elements of a vector.
CAUCHY POINT COMPUTATION
We now derive an explicit expression for the piecewise linear path obtained by pro-
jecting the steepest descent direction onto the feasible box, and outline the search procedure
for identifying the ﬁrst local minimum of q along this path.
We deﬁne the projection of an arbitrary point x onto the feasible region (16.43b) as
follows: The ith component is given by
P(x, l, u)i 



li
if
xi < li,
xi
if
xi ∈[li, ui],
ui
if
xi > ui.
(16.44)
The piecewise linear path x(t) starting at the reference point x0 and obtained by projecting
the steepest descent direction at x0 onto the feasible region (16.43b) is thus given by
x(t)  P(x0 −tg, l, u),
(16.45)
where g  ∇q(x0); see Figure 16.5. We then compute the Cauchy point xc, which is deﬁned
as the ﬁrst local minimizer of the univariate, piecewise quadratic q(x(t)), for t ≥0. This
minimizer is obtained by examining each of the line segments that make up x(t). To perform

476
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
this search, we need to determine the values of t at which the kinks in x(t), or breakpoints,
occur. We ﬁrst identify the values of t for which each component reaches its bound along
the chosen direction −g. These values ¯ti are given by the following explicit formulae:
¯ti 



(x0
i −ui)/gi
if gi < 0 and ui < +∞,
(x0
i −li)/gi
if gi > 0 and li > −∞,
∞
otherwise.
(16.46)
The components of x(t) for any t are therefore
xi(t) 

x0
i −tgi
if t ≤¯ti,
x0
i −¯tigi
otherwise.
This has a simple geometrical interpretation: A component xi(t) will move at the rate given
by the projection of the gradient along this direction, and will remain ﬁxed once this variable
reaches one of its bounds.
Tosearchfortheﬁrstlocalminimizeralong P(x0−tg, l, u),weeliminatetheduplicate
values and zero values of ¯ti from the set {¯t1, ¯t2, . . . , ¯tn}, and sort the remaining numbers into
an ordered sequence t1, t2, t3, . . . such that 0 < t1 < t2 < t3 ≤· · ·. We now examine the
x(t  )
2
x(t  )
3
1
x(t  )
x
x-tg
Figure 16.5
The piecewise linear path x(t), for an example in IR3.

1 6 . 6 .
T h e G r a d i e n t - - P r o j e c t i o n M e t h o d
477
intervals [0, t1], [t1, t2], [t2, t3], . . . in turn. Suppose we have examined the intervals up to
[tj−2, tj−1] for some j, and determined that the local minimizer is at some value t ≥tj−1.
For the interval [tj−1, tj] between the (j −1)st and jth breakpoints, we have that
x(t)  x(tj−1) + tpj−1,
where
t  t −tj−1,
t ∈[0, tj −tj−1],
and
pj−1 

−gi
if tj−1 < ¯ti,
0
otherwise.
(16.47)
By using this notation, we write the quadratic (16.43a) on the line segment [x(tj−1), x(tj)]
as
q(x(t))  dT (x(tj−1) + tpj−1) + 1
2(x(tj−1) + tpj−1)T G(x(tj−1) + tpj−1),
where t ∈[0, tj −tj−1].
By expanding and grouping the coefﬁcients of 1, t, and (t)2, we ﬁnd that
q(x(t))  fj−1 + f ′
j−1t + 1
2f ′′
j−1(t)2,
t ∈[0, tj −tj−1],
(16.48)
where the coefﬁcients fj−1, f ′
j−1, and f ′′
j−1 are deﬁned by
fj−1  dT x(tj−1) + 1
2x(tj−1)T Gx(tj−1),
fj−1  dT pj−1 + x(tj−1)T Gpj−1,
f ′′
j−1  (pj−1)T Gpj−1.
By differentiating (16.48) with respect to t and equating to zero, we obtain t∗
−f ′
j−1/f ′′
j−1. If t∗∈[0, tj −tj−1) and f ′′
j−1 > 0, we conclude that there is a local minimizer
of q(x(t)) at
t  tj−1 + t∗.
Otherwise, there is a minimizer at t  tj−1 when f ′
j−1 > 0. In all other cases we move on to
the next interval [tj, tj+1] and continue the search. We need to calculate the new direction
pj from (16.47), and we use this new value to calculate fj, f ′
j, and f ′′
j . Since pj differs from
pj−1 typically in just one component, computational savings can sometimes be made by
updating these coefﬁcients rather than computing them from scratch.

478
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
SUBSPACE MINIMIZATION
After the Cauchy point xc has been computed, the components of xc that are at their
lower or upper bounds deﬁne the active set A(xc). In the second stage of the gradient
projection iteration, we approximately solve the QP obtained by ﬁxing these components at
the values xc
i . The subproblem is therefore
min
x
q(x)  1
2xT Gx + xT d
(16.49a)
subject to xi  xc
i ,
i ∈A(xc),
(16.49b)
li ≤xi ≤ui,
i /∈A(xc).
(16.49c)
Itisnotnecessarytosolvethisproblemexactly.Norisitdesirable,sincethesubproblem
maybealmostasdifﬁcultastheoriginalproblem(16.43).Infact,toobtainglobalconvergence
all we require of the approximate solution x+ of (16.49) is that q(x+) ≤q(xc) and that x+
be feasible with respect to the constraints (16.49b), (16.49c). A strategy that is intermediate
between choosing x+  xc as the approximate solution (on the one hand) and solving
(16.49) exactly (on the other hand) is to ignore the bound constraints (16.49c) and apply an
iterative method to the problem (16.49a), (16.49b). The use of conjugate gradient iterations
incombinationwiththenull-spaceapproachforequality-constrainedproblems,asdiscussed
in Section 16.2, is one possibility. The natural starting point for this process would be xc. We
could terminate the iterations as soon as a bound (16.49c) is encountered by the algorithm
or as soon as the conjugate gradient iteration generates a direction of negative curvature; see
Algorithm 4.3 in Section 4.1. Note that the null-space basis matrix Z for this problem has a
particularly simple form.
We conclude by summarizing the gradient projection algorithm for quadratic
programming and stating a convergence theorem.
Algorithm 16.2 (Gradient–Projection Method for QP).
Compute a feasible starting point x0;
for
k  0, 1, 2, . . .
if xk satisﬁes the KKT conditions for (16.43)
STOP with solution x∗ xk;
Set x  xk and ﬁnd the Cauchy point xc;
Find an approximate solution x+ of (16.49) such that q(x+) ≤q(xc)
and x+ is feasible;
xk+1 ←x+;
end (for)
When applied to problems that satisfy strict complementarity (that is, problems for
which all Lagrange multipliers associated with an active bound at the optimizer x∗are
nonzero), the active sets A(xc) generated by the gradient projection algorithm eventually
settle down. That is, constraint indices cannot repeatedly enter and leave the active set on

1 6 . 7 .
I n t e r i o r - P o i n t M e t h o d s
479
successive iterations. When the problem is degenerate, the active set may not settle down,
but various devices have been proposed to prevent this undesirable behavior from taking
place.
Finally, we note that while gradient projection methods can be applied in principle to
problems with general linear constraints, signiﬁcant computation is required to apply the
projection operator P. If the constraint set is deﬁned as aT
i x ≥bi, i ∈I, we must solve the
following convex quadratic program to compute the projection of a given point ¯x onto the
feasible set:
min
x
∥x −¯x∥2
subject to aT
i x ≥bi for all i ∈I.
The expense of solving this “ projection subproblem” may approach the cost of solving the
original quadratic program, so it is usually not economical to apply gradient projection to
this case.
16.7
INTERIOR-POINT METHODS
The primal–dual interior-point approach can be applied to convex quadratic programs
through a simple extension of the linear-programming algorithms described in Chapter 14.
The resulting algorithms are simple to describe, relatively easy to implement, and quite
efﬁcient on certain types of problems. Extensions of interior-point methods to nonconvex
problems are currently under investigation and will not be discussed here.
For simplicity, we restrict our attention to convex quadratic programs with inequality
constraints, which we write as follows:
min
x
q(x)
def 1
2xT Gx + xT d
(16.50a)
subject to
Ax ≥b,
(16.50b)
where G is symmetric and positive semideﬁnite, and where the m × n matrix A and right-
hand-side b are deﬁned by
A  [ai]i∈I,
b  [bi]i∈I,
I  {1, 2, . . . , m}.
(Equalityconstraintscanbeaccommodatedwithsimplechangestotheapproachesdescribed
below.) We can specialize the KKT conditions (12.30) to obtain the following set of necessary
conditions for (16.50): If x∗is a solution of (16.50), there is a Lagrange multiplier vector λ∗
such that the following conditions are satisﬁed for (x, λ)  (x∗, λ∗):
Gx −AT λ + d  0,
Ax −b ≥0,

480
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
(Ax −b)iλi  0,
i  1, 2, . . . , m,
λ ≥0.
By introducing the slack vector y  Ax −b, we can rewrite these conditions as
Gx −AT λ + d  0,
(16.51a)
Ax −y −b  0,
(16.51b)
yiλi  0,
i  1, 2, . . . , m,
(16.51c)
(y, λ) ≥0.
(16.51d)
It is easy to see the close correspondence between (16.51) and the KKT conditions
(14.3) for the linear programming problem (14.1). As in the case of linear programming, the
KKT conditions are not only necessary but also sufﬁcient, because the objective function is
convex and the feasible region is convex. Hence, we can solve the convex quadratic program
(16.50) by ﬁnding solutions of the system (16.51).
AsinChapter14,wecanrewrite(16.51)asaconstrainedsystemofnonlinearequations
and derive primal–dual interior-point algorithms by applying modiﬁcations of Newton’s
method to this system. Analogously to (14.4), we deﬁne
F(x, y, λ) 


Gx −AT λ + d
Ax −y −b
Ye

,
(y, λ) ≥0,
where
Y  diag(y1, y2, . . . , ym),
  diag(λ1, λ2, . . . , λm),
e  (1, 1, . . . , 1)T .
Given a current iterate (x, y, λ) that satisﬁes (y, λ) > 0, we can deﬁne a duality measure µ
by
µ  1
m
m

i1
yiλi  yT λ
m ,
(16.52)
similarly to (14.10).
The central path C is the set of points (xτ, yτ, λτ) (τ > 0) such that
F(xτ, yτ, λτ) 


0
0
τe

,
(yτ, λτ) > 0

1 6 . 7 .
I n t e r i o r - P o i n t M e t h o d s
481
(cf. (14.9)). The generic step (x, y, λ) is a Newton-like step from the current point
(x, y, λ) toward the point (xσµ, yσµ, λσµ) on the central path, where σ ∈[0, 1] is a pa-
rameter chosen by the algorithm. As in (14.15), we ﬁnd that this step satisﬁes the following
linear system:


G
−AT
0
A
0
−I
0
Y





x
s
λ




−rd
−rb
−Se + σµe

,
(16.53)
where
rd  Gx −AT λ + d,
rb  Ax −y −b.
As in Framework 14.1 of Chapter 14, we obtain the next iterate by setting
(x+, y+, λ+)  (x, y, λ) + α(x, y, λ),
where α is chosen to retain the inequality (y+, λ+) > 0 and possibly to satisfy various other
conditions.
Mehrotra’s predictor–corrector algorithm can also be extended to convex quadratic
programming with the exception of one aspect: The step lengths in primal variables (x, y)
and dual variables λ cannot be different, as they are in the linear programming case. The
reason is that the primal and dual variables are coupled through the matrix G, so different
step lengths can disturb feasibility of the equation (16.51a).
The major computational operation is the solution of the system (16.53) at each
iteration of the interior-point method. As in Chapter 14, this system may be restated in
more compact forms. The augmented system form is

G
−AT
A
−1Y
 
x
s



−rd
−rb + (−y + σµ−1e)

,
(16.54)
and a symmetric indeﬁnite factorization scheme can be applied to the coefﬁcient matrix.
The normal equations form is
(G + AT (Y −1)A)x  −rd + AT (Y −1)[−rb −y + σµ−1e],
which can be solved by means of a modiﬁed Cholesky algorithm. Note that the factorization
must be recomputed at each iteration, because the change in y and λ leads to changes in the
nonzero components of AT (Y −1)A.

482
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
EXTENSIONS AND COMPARISON WITH ACTIVE-SET METHODS
It is possible to extend all the algorithms of Chapter 14 to the problem (16.50). To
obtain path-following methods, we deﬁne the strictly feasible set Fo by
Fo 
)
(x, y, λ) | Gx −AT λ + d  0, Ax −y −b  0, (y, λ) > 0
*
,
and the central path neighborhoods N−∞(γ ) and N2(θ) by
N2(θ)  {(x, y, s) ∈Fo | ∥YSe −µe∥≤θµ},
N−∞(γ )  {(x, y, s) ∈Fo | yisi ≥γ µ, for all i  1, 2, . . . , m},
where θ ∈[0, 1) and γ ∈(0, 1] (cf. (14.16), (14.17)). As before, all iterates of path-following
algorithms are constrained to belong to one or other of these neighborhoods. Potential
reduction algorithms are obtained by redeﬁning the Tanabe–Todd–Ye potential function ρ
as
ρ(y, λ)  ρ log yT λ −
m

i1
log yiλi,
for some ρ > m (cf. (14.30)). As in Chapter 14, iterates of these methods are restricted to
Fo, steps are obtained by solving (16.53) with rd  0 and rb  0, and the step length α is
chosen to force a substantial reduction in ρ at each iteration.
The basic difference between interior-point and active-set methods for convex QP
is that active-set methods generally require a large number of steps in which each search
direction is relatively inexpensive to compute, while interior-point methods take a smaller
number of more expensive steps. As is the case in linear programming, however, the active-
set methods are more complicated to implement, particularly if the program tries to take
advantage of sparsity in G and A. In this situation, the factorization updating procedures at
each active set are quite difﬁcult to implement efﬁciently, as the need to maintain sparsity
adds complications. By contrast, the nonzero structure of the matrix to be factored at each
interior-point iteration does not change (only the numerical values), so standard sparse
factorization software can be used to obtain the steps.
16.8
DUALITY
We conclude this chapter with a brief discussion of duality for convex quadratic program-
ming. Duality can be a useful tool in quadratic programming because in some classes of
problems we can take advantage of the special structure of the dual to solve the problem
more efﬁciently.

1 6 . 8 .
D u a l i t y
483
If G is positive deﬁnite, the dual of
min
x
1
2xT Gx + xT d
(16.55a)
subject to
Ax ≥b
(16.55b)
is given by
max
x,λ
1
2xT Gx + xT d −λT (Ax −b)
subject to
Gx + d −AT λ  0,
λ ≥0.
(16.56a)
By eliminating x from the second equation, we obtain the bound-constrained problem
max
λ
−1
2λT (AG−1AT )λ + λT (b + AG−1d) −1
2dT G−1d
(16.57a)
subject to
λ ≥0.
(16.57b)
This problem can be solved by means of the gradient projection method, which nor-
mally allows us to identify the active set more rapidly than with a classical active set methods.
Moreover, in some areas of application, such as entropy maximization, it is possible to show
that the bounds in (16.57b) are not active at the solution. In this case, the problem (16.57)
becomes an unconstrained quadratic optimization problem in λ.
For large problems, the complicated form of the objective (16.57a) can be a drawback.
If a direct solver is used, then we need to explicitly form AG−1AT . An alternative is to factor
G and to apply the conjugate gradient method to AG−1AT .
NOTES AND REFERENCES
If G is not positive deﬁnite, the problem of determining whether a feasible point for
(16.1) is a global minimizer is an NP-hard problem; see Murty and Kabadi [176].
Markowitz formulated the portfolio optimization problem in 1952 [157]. See his
book [159] for further details.
For a discussion on the QMR method see Freund and Nachtigal [94]. The LSQR
approach is equivalent to applying the conjugate gradient to the normal equations for the
system (16.5); see Paige and Saunders [188].
We have assumed throughout this chapter that all equality-constrained quadratic
programs have linearly independent constraints, i.e., that A has rank m. If this is not the case,
redundant constraints can be removed from the problem. This can be done, for example,
by computing a QR factorization of AT , which normally (but not always) gives a good
indication of the rank of A and of the rows that can be removed. In the large-scale case,

484
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
Gaussian elimination techniques are typically used, but this makes it more difﬁcult to decide
which constraints should be removed.
The ﬁrst inertia-controlling method for indeﬁnite quadratic programming was pro-
posed by Fletcher [80]. See also Gill et al. [110] and Gould [120] for a discussion of methods
for general quadratic programming.
For further discussion on the gradient projection method see, for example, Conn,
Gould, and Toint [51] and Burke and Mor´e [32].
In some areas of application, the KKT matrix is not just sparse but also contains special
structure. For instance, the quadratic programs that arise in optimal control and model
predictive control have banded matrices G and A (see Wright [253]). This structure is more
easily exploited in the interior-point case, because the matrix in (16.54) can be reordered to
have block-banded structure, for which efﬁcient algorithms are available. When active-set
methods are applied to this problem, however, the advantages of bandedness and sparsity
are lost after just a few updates of the basis.
Further details of interior-point methods for convex quadratic programming can be
found in Chapter 8 of Wright [255].
✐
E x e r c i s e s
✐
16.1 Solve the following quadratic program and illustrate it geometrically.
max f (x)  2x1 + 3x2 + 4x2
1 + 2x1x2 + x2
2,
subject to x1 −x2 ≥0,
x1 + x2 ≤4,
x1 ≤3.
✐
16.2 The problem of ﬁnding the shortest distance from a point x0 to the hyperplane
{x|Ax  b} where A has full row rank can be formulated as the quadratic program
min 1
2(x −x0)T (x −x0)
s.t.
Ax  b.
Show that the optimal multiplier is
λ∗ (AAT )−1(b −Ax0),
and that the solution is
x∗ x0 −AT (AAT )−1(b −Ax0).

1 6 . 8 .
D u a l i t y
485
Show that in the special case where A is a row vector, the shortest distance from x0 to the
solution set of Ax  b is
|b −Ax0|
||A||
.
✐
16.3 Use Theorem 12.1 to verify that the ﬁrst-order necessary conditions for (16.3)
are given by (16.4).
✐
16.4 Use Theorem 12.6 to show that if the conditions of Lemma 16.1 hold, then the
second-order sufﬁcient conditions for (16.3) are satisﬁed by the vector pair (x∗, λ∗) that
satisﬁes (16.4).
✐
16.5 Consider (16.3) and suppose that the projected Hessian matrix ZT GZ has a
negative eigenvalue; that is, uT ZT GZu < 0 for some vector u. Show that if there exists any
vector pair (x∗, λ∗) that satisﬁes (16.4), then the point x∗is only a stationary point of (16.3)
and not a local minimizer. (Hint: Consider the function q(x∗+ αZu) for α ̸ 0, and use
an expansion like that in the proof of Theorem 16.2.)
✐
16.6 By using the QR factorization and a permutation matrix, show that for a full-
rank m×n matrix A (with m < n) we can ﬁnd an orthogonal matrix Q and an m×m upper
triangular matrix ˆU such that AQ 
%
0
ˆU
&
. (Hint: Start by applying the standard QR
factorization to AT .)
✐
16.7 Verify that the ﬁrst-order conditions for optimality of (16.1) are equivalent to
(16.26) when we make use ot the active set deﬁnition (16.25).
✐
16.8 For each of the alternative choices of initial working set W0 in the example
(16.36) (that is, W0  {3}, W0  {5}, and W0  ∅) work through the ﬁrst two iterations of
Algorithm 16.1.
✐
16.9 Program Algorithm 16.1 and use it to solve
min x2
1 + 2x2
2 −2x1 −6x2 −2x1x2
subject to 1
2x1 + 1
2x2 ≤1,
−x1 + 2x2 ≤2,
x1, x2 ≥0.
Choose three initial starting points: one in the interior of the feasible region, one at a vertex,
and one on the boundary of the feasible region (not a vertex).
✐
16.10 Consider the problem (16.3a)–(16.3b), and assume that A has full column
rank. Prove the following three statements: (a) The quadratic program has a strong local
minimizer at a point x∗satisfying the Lagrange equations Gx∗+ d  Aλ∗if and only if

486
C h a p t e r
1 6 .
Q u a d r a t i c P r o g r a m m i n g
ZT GZ is positive deﬁnite. (b) The quadratic program has an inﬁnite number of solutions
if the ﬁrst equations in (16.4) are compatible and if the reduced Hessian ZT GZ is positive
semideﬁnite and singular. (c) There are no ﬁnite solutions if either ZT GZ is indeﬁnite or if
the ﬁrst equations in (16.4) are inconsistent.
✐
16.11 Assume that A ̸ 0. Show that the KKT matrix (16.7) is indeﬁnite.
✐
16.12 Consider the quadratic program
max  6x1 + 4x2 −13 −x2
1 −x2
2,
subject to x1 + x2 ≤3,
x1 ≥0, x2 ≥0.
First solve it graphically and then use your program implementing the active-set method
given in Algorithm 16.1.
✐
16.13 Prove that if the KKT matrix (16.7) is nonsingular, then A must have full rank.
✐
16.14 Explain why it is that the size of the Lagrange multipliers is dependent on the
scaling of the objective function and constraints.
✐
16.15 Using (16.27) and (16.29), explain brieﬂy why the gradient of each blocking
constraint cannot be a linear combination of the constraint gradients in the current working
set Wk. More speciﬁcally, suppose that the initial working set W0 in Algorithm 16.1 is chosen
sothatthegradientsoftheconstraintsbelongingtothisworkingsetarelinearlyindependent.
Show that the step selection (16.29) guarantees that constraint gradients of all subsequent
working sets remain linearly independent.
✐
16.16 Let W be an n × n symmetric matrix and suppose that Z is of dimension n × t.
Suppose that ZT WZ is positive deﬁnite and that ¯Z  [Z, z], i.e., that we have appended a
column to Z. Show that ¯ZT W ¯Z is positive deﬁnite.
✐
16.17 Find a null-space basis matrix Z for the problem (16.49a), (16.49b).
✐
16.18 Write down KKT conditions for the following convex quadratic program with
mixed equality and inequality constraints:
min q(x)  1
2xT Gx + xT d,
s.t. Ax ≥b,
¯Ax  ¯b,
where G is symmetric and positive semideﬁnite. Use these conditions to derive an analogue
of the generic primal–dual step (16.53) for this problem.

Chapter17

Penalty, Barrier,
and Augmented
Lagrangian
Methods
An important class of methods for constrained optimization seeks the solution by replac-
ing the original constrained problem by a sequence of unconstrained subproblems. In this
chapter we describe three important algorithms in this class. The quadratic penalty method
replaces the constraints by penalty terms in the objective function, where each penalty term
is a multiple of the square of the constraint violation. Because of its simplicity and intu-
itive appeal, this approach is often used in computations, although it has some important
disadvantages. This method can be considered a precursor to the method of multipliers or
augmented Lagrangian method, in which explicit Lagrange multiplier estimates are used to
avoid the ill-conditioning that is inherent in the quadratic penalty function.
A somewhat different approach is used in the log-barrier method, in which logarithmic
terms prevent feasible iterates from moving too close to the boundary of the feasible region.
This method is interesting in its own right, and it also provides the foundation of primal and
primal–dual interior-point methods, which have proved to be important in linear program-
ming (see Chapter 14), convex quadratic programming (see Chapter 16), and semideﬁnite
programming, and may yet make their mark in general constrained optimization as well.
In Section 17.3 we describe exact penalty function approaches, in which a single
unconstrained problem (rather than a sequence) takes the place of the original constrained

490
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
problem. Not surprisingly, the resulting function is often difﬁcult to minimize. Section 17.5
describes sequential linearly constrained methods, an important class of practical methods
for large constrained optimization problems.
17.1
THE QUADRATIC PENALTY METHOD
MOTIVATION
One fundamental approach to constrained optimization is to replace the original
problem by a penalty function that consists of
• the original objective of the constrained optimization problem, plus
• one additional term for each constraint, which is positive when the current point x
violates that constraint and zero otherwise.
Most approaches deﬁne a sequence of such penalty functions, in which the penalty terms for
the constraint violations are multiplied by some positive coefﬁcient. By making this coef-
ﬁcient larger and larger, we penalize constraint violations more and more severely, thereby
forcing the minimizer of the penalty function closer and closer to the feasible region for the
constrained problem.
Suchapproachesaresometimesknownasexteriorpenaltymethods,becausethepenalty
term for each constraint is nonzero only when x is infeasible with respect to that constraint.
Often,theminimizersofthepenaltyfunctionsareinfeasiblewithrespecttotheoriginalprob-
lem, and approach feasibility only in the limit as the penalty parameter grows increasingly
large.
Thesimplestpenaltyfunctionofthistypeisthequadraticpenaltyfunction,inwhichthe
penalty terms are the squares of the constraint violations. We devote most of our discussion
in this section to the equality-constrained problem
min
x
f (x)
subject to ci(x)  0, i ∈E,
(17.1)
which is a special case of (12.1). The quadratic penalty function Q(x; µ) for this formulation
is
Q(x; µ)
def f (x) + 1
2µ

i∈E
c2
i (x),
(17.2)
where µ > 0 is the penalty parameter. By driving µ to zero, we penalize the constraint
violations with increasing severity. It makes good intuitive sense to consider a sequence of
values {µk} with µk ↓0 as k →∞, and to seek the approximate minimizer xk of Q(x; µk)
for each k. Because the penalty terms in (17.2) are smooth (each term c2
i has at least as many

1 7 . 1 .
T h e Q u a d r a t i c P e n a l t y M e t h o d
491
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Figure 17.1
Contours of Q(x; µ) from (17.4) for µ  1, contour spacing 0.5.
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Figure 17.2
Contours of Q(x; µ) from (17.4) for µ  0.1, contour spacing 2.
derivatives as ci itself), we can use techniques from unconstrained optimization to search
for xk. The approximate minimizers xk, xk−1, etc, can be used to identify a good starting
point for the minimization of Q(·; µk+1) at iteration k + 1. By choosing the sequence {µk}
and the starting points wisely, it may be possible to perform just a few steps of unconstrained
minimization for each value µk.

492
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
❏Example 17.1
Consider the problem (12.9) from Chapter 12, that is,
min x1 + x2
s.t.
x2
1 + x2
2 −2  0,
(17.3)
for which the solution is (−1, −1)T and the quadratic penalty function is
Q(x; µ)  x1 + x2 + 1
2µ
	
x2
1 + x2
2 −2

2 .
(17.4)
We plot the contours of this function in Figures 17.1 and 17.2. In Figure 17.1 we have
µ  1, and we observe a mimizer of Q near the point (−1.1, −1.1)T . (There is also a local
maximizer near x  (0.3, 0.3)T .) In Figure 17.2 we have µ  0.1, so points that do not lie
on the feasible circle deﬁned by x2
1 + x2
2  2 suffer a much greater penalty than in the ﬁrst
ﬁgure—the “trough” of low values of Q is clearly evident. The minimizer in this ﬁgure is
much closer to the solution (−1, −1)T of the problem (17.3). A local maximum lies near
(0, 0)T , and Q goes rapidly to ∞outside the circle x2
1 + x2
2  2.
❐
For the general constrained optimization problem (12.1), which contains inequality
constraints as well as equality constraints, we can deﬁne Q as
Q(x; µ)
def f (x) + 1
2µ

i∈E
c2
i (x) + 1
2µ

i∈I
	
[ci(x)]−
2 ,
(17.5)
where [y]−denotes max(−y, 0). In this case, Q may be less smooth than the objective and
constraint functions. For instance, if one of the inequality constraints is x1 ≥0, then the
function min(0, x1)2 has a discontinuous second derivative (see the exercises), so that Q is
no longer twice continuously differentiable.
ALGORITHMIC FRAMEWORK
A general framework for algorithms based on the penalty function (17.2) can be
speciﬁed as follows.
Framework 17.1 (Quadratic Penalty).
Given µ0 > 0, tolerance τ0 > 0, starting point xs
0;
for k  0, 1, 2, . . .
Find an approximate minimizer xk of Q(·; µk), starting at xs
k,
and terminating when ∥∇Q(x; µk)∥≤τk;
if ﬁnal convergence test satisﬁed

1 7 . 1 .
T h e Q u a d r a t i c P e n a l t y M e t h o d
493
STOP with approximate solution xk;
Choose new penalty parameter µk+1 ∈(0, µk);
Choose new starting point xs
k+1;
end (for)
The parameter sequence {µk} can be chosen adaptively, based on the difﬁculty of min-
imizing the penalty function at each iteration. When minimization of Q(x; µk) proves to
be expensive for some k, we choose µk+1 to be only modestly smaller than µk; for instance
µk+1  0.7µk. If we ﬁnd the approximate minimizer of Q(x; µk) cheaply, we could try a
more ambitious reduction, for instance µk+1  0.1µk. The convergence theory for Frame-
work 17.1 allows wide latitude in the choice of tolerances τk; it requires only that lim τk  0,
to ensure that the minimization is carried out more and more accurately.
When only equality constraints are present, Q(x; µk) is smooth, so the algorithms
for unconstrained minimization described in the ﬁrst chapters of the book can be used
to identify the approximate solution xk. However, the minimization of Q(x; µk) becomes
more and more difﬁcult to perform when µk becomes small, unless we use special techniques
to calculate the search directions. For one thing, the Hessian ∇2
xxQ(x; µk) becomes quite
ill conditioned near the minimizer. This property alone is enough to make unconstrained
minimization algorithms such as quasi-Newton and conjugate gradient perform poorly.
Newton’s method, on the other hand, is not sensitive to ill conditioning of the Hessian, but
it, too, may encounter difﬁculties for small µk for two other reasons. First, ill conditioning
of ∇2
xxQ(x; µk) might be expected to cause problems when we come to solve the linear
equations to calculate the Newton step. We discuss this issue further at the end of the section,
where we show that these effects are not so severe and that a reformulation of the Newton
equationsispossible.Second,evenwhenx isclosetotheminimizerofQ(·; µk),thequadratic
Taylor series approximation to Q(x; µk) about x is a reasonable approximation of the true
function only in a small neighborhood of x. This property can be seen in Figure 17.2, where
the contours of Q near the minimizer have a banana shape, rather than the elliptical shape
that characterizes quadratic functions. Since Newton’s method is based on the quadratic
model, the steps that it generates may not make rapid progress toward the minimizer of
Q(x; µk), unless we are quite close to the minimizer. This difﬁculty can be overcome partly
by judicious choice of the starting point xs
k+1.
Despite the intuitive appeal and simplicity of Framework 17.1, we point out that the
method of multipliers of Section 17.4 would generally be more effective, since it tends to
avoid the ill conditioning that causes problems in Framework 17.1.
CONVERGENCE OF THE QUADRATIC PENALTY FUNCTION
We describe some convergence properties of this approach in the following two
theorems.

494
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
Theorem 17.1.
Suppose that each xk is the exact global minimizer of Q(x; µk) in Framework 17.1 above,
and that µk ↓0. Then every limit point x∗of the sequence {xk} is a solution of the problem
(17.1).
Proof.
Let ¯x be a global solution of (17.1), that is,
f (¯x) ≤f (x)
for all x with ci(x)  0, i ∈E.
Since xk minimizes Q(·; µk) for each k, we have that Q(xk; µk) ≤Q(¯x; µk), which leads to
the inequality
f (xk) +
1
2µk

i∈E
c2
i (xk) ≤f (¯x) +
1
2µk

i∈E
c2
i (¯x)  f (¯x).
(17.6)
By rearranging this expression, we obtain

i∈E
c2
i (xk) ≤2µk[f (¯x) −f (xk)].
(17.7)
Suppose that x∗is a limit point of {xk}, so that there is an inﬁnite subsequence K such that
lim
k∈K xk  x∗.
By taking the limit as k →∞, k ∈K, on both sides of side of (17.7), we obtain

i∈E
c2
i (x∗)  lim
k∈K

i∈E
c2
i (xk) ≤lim
k∈K 2µk[f (¯x) −f (xk)]  0,
where the last equality follows from µk ↓0. Therefore, we have that ci(x∗)  0 for all i ∈E,
so that x∗is feasible. Moreover, by taking the limit as k →∞for k ∈K in (17.6), we have
by nonnegativity of µk and of each ci(xk)2 that
f (x∗) ≤f (x∗) + lim
k∈K
1
2µk

i∈E
c2
i (xk) ≤f (¯x).
Since x∗is a feasible point whose objective value is no larger than that of the global minimizer
¯x, we conclude that x∗, too, is a global minimizer, as claimed.
□
Since this result requires us to ﬁnd the global minimizer for each subproblem, its very
desirable property of convergence to the global solution of (17.1) may be difﬁcult to realize in
practice.Thenextresultconcernsconvergencepropertiesofthesequence {xk}whenweallow
inexact (but increasingly accurate) minimizations of Q(·; µk). In contrast to Theorem 17.1,

1 7 . 1 .
T h e Q u a d r a t i c P e n a l t y M e t h o d
495
it shows that the sequence is attracted to KKT points (that is, points satisfying ﬁrst-order
necessary conditions; see (12.30)), rather than to a global minimizer. It also shows that the
quantities ci(xk)/µk may be used as estimates of the Lagrange multipliers λ∗
i in certain
circumstances. This observation is important for the analysis of Section 17.4.
Theorem 17.2.
If the tolerances τk in Framework 17.1 above satisfy
lim
k→∞τk  0
and the penalty parameters satisfy µk ↓0, then for all limit points x∗of the sequence {xk}
at which the constraint gradients ∇ci(x∗) are linearly independent, we have that x∗is a KKT
point for the problem (17.1). For such points, we have for the inﬁnite subsequence K such that
limk∈K xk  x∗that
lim
k∈K −ci(xk)/µk  λ∗
i ,
for all i ∈E,
(17.8)
where λ∗is multiplier vector that satisﬁes the KKT conditions (12.30).
Proof.
By differentiating Q(x; µk) in (17.2), we obtain
∇xQ(xk; µk)  ∇f (xk) +

i∈E
ci(xk)
µk
∇ci(xk).
(17.9)
By applying the termination criterion for Framework 17.1, we have that
∇f (xk) +

i∈E
ci(xk)
µk
∇ci(xk)
 ≤τk.
(17.10)
By rearranging this expression (and in particular using the inequality ∥a∥−∥b∥≤∥a+b∥),
we obtain


i∈E
ci(xk)∇ci(xk)
 ≤µk [τk + ∥∇f (xk)∥] .
(17.11)
When we take limits as k →∞for k ∈K, the bracketed term on the right-hand-side
approaches ∥∇f (x∗)∥, so because µk ↓0, the right-hand-side approaches zero. From the
corresponding limit on the left-hand-side, we obtain

i∈E
ci(x∗)∇ci(x∗)  0.
(17.12)

496
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
Since by assumption the constraint gradients ∇ci(x∗) are linearly independent, we have that
ci(x∗)  0 for all i ∈E, so x∗is feasible.
Since x∗is feasible, the second KKT condition (12.30b) is satisﬁed. We need to check
the ﬁrst KKT condition (12.30a) as well, and to show that the limit (17.8) holds.
By using A(x) to denote the matrix of constraint gradients, that is,
A(x)T  [∇ci(x)]i∈E ,
(17.13)
and λk to denote the vector −c(xk)/µk, we have as in (17.10) that
A(xk)T λk  ∇f (xk) −∇xQ(xk; µ),
∥∇xQ(xk; µ)∥≤τk.
(17.14)
For all k ∈K sufﬁciently large, the matrix A(xk) has full column rank, so that A(xk)A(xk)T
is nonsingular. By multiplying (17.14) by A(xk) and rearranging, we have that
λk 

A(xk)A(xk)T −1 A(xk) [∇f (xk) −∇xQ(xk; µ)] .
Hence by taking the limit as k ∈K goes to ∞, we ﬁnd that
lim
k∈K λk  λ∗

A(x∗)A(x∗)T −1 A(x∗)∇f (x∗).
By taking limits in (17.10), we conclude that
∇f (x∗) −A(x∗)T λ∗ 0,
so that λ∗satisﬁes the ﬁrst KKT condition (12.30a). We conclude from (12.30) that x∗is a
KKT point, with unique Lagrange multiplier vector λ∗.
□
When the constraint gradients are linearly dependent, we cannot deduce from (17.12)
that ci(x∗)  0, so the limit point may not be feasible. Even if it is feasible, and even if
the limiting multiplier value λ∗exists and the KKT conditions (12.30) hold, the necessary
conditions for x∗to be a solution of the linear program are still not satisﬁed because these
conditions require linear independence of the constraint gradients (see Theorem 12.1).
When the limit x∗is not feasible, we have from (17.12) that it is at least a stationary point for
the function ∥c(x)∥2. Newton-type algorithms can always be attracted to infeasible points of
this type. We see the same effect in Chapter 11, in our discussion of methods for nonlinear
equations that use the sum-of-squares merit function ∥r(x)∥2. Such methods cannot be
guaranteed to ﬁnd a root, but only a stationary point or minimizer of the merit function. In
thecaseinwhichthenonlinearprogram(17.1)isinfeasible,wewilloftenobserveconvergence
of the quadratic-penalty method to stationary points or minimizers of ∥c(x)∥2.
We conclude this section by examining the nature of the ill conditioning (mentioned
above) in the Hessian ∇2
xxQ(x; µk). An understanding of the properties of this matrix, and

1 7 . 1 .
T h e Q u a d r a t i c P e n a l t y M e t h o d
497
the similar Hessians that arise in the other methods of this chapter, is essential in choosing
effective algorithms for the minimization problem and for the linear algebra calculations at
each iteration.
The Hessian is given by the formula
∇2
xxQ(x; µk)  ∇2f (x) +

i∈E
ci(x)
µk
∇2ci(x) + 1
µk
A(x)T A(x),
(17.15)
where we have used the deﬁnition (17.13) of A(x). When x is close to the minimizer of
Q(·; µk) and the conditions of Theorem 17.2 are satisﬁed, we have from (17.8) that the ﬁrst
two terms on the right-hand-side of (17.15) are approximately equal to the Hessian of the
Lagrangian function deﬁned in (12.28). To be speciﬁc, we have
∇2
xxQ(x; µk) ≈∇2
xxL(x, λ∗) + 1
µk
A(x)T A(x),
(17.16)
when x is close to the minimizer of Q(·; µk). We see from this expression that ∇2
xxQ(x; µk)
is approximately equal to the sum of
• a matrix whose elements are independent of µk (the Lagrangian term), and
• a matrix of rank |E| whose nonzero eigenvalues are of order 1/µk (the summation
term in (17.16)).
The number of constraints |E| is usually fewer than n. In this case, the summation term is
singular, and the overall matrix has some of its eigenvalues approaching a constant, while
others are of order 1/µk. Since µk is approaching zero, the increasing ill conditioning of
Q(x; µk) is apparent.
One consequence of the ill conditioning is possible inaccuracy in the calculation of
the Newton step for Q(x; µk), which is obtained by solving the following system:
∇2
xxQ(x; µ)p  −∇xQ(x; µ).
(17.17)
Signiﬁcant roundoff errors will appear in p regardless of the solution technique used, and
algorithms will break down as the matrix becomes numerically singular. The presence of
roundoff error may not disqualify p from being a good direction of progress for Newton’s
method, however. As in the Newton/log-barrier step equations (17.28), it could be that the
error is concentrated in a direction along which Q does not vary signiﬁcantly, so that it has
little effect on the overall quality of the step.
In any case, we can use an alternative formulation of the equations (17.17) that avoids
the ill conditioning. By introducing a “dummy vector” ζ and using the expression (17.15),

498
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
we see that (17.17) is equivalent to the system


∇2f (x) +

i∈E
ci(x)
µk
∇2ci(x)
A(x)T
A(x)
−µkI



p
ζ



−∇xQ(x; µ)
0

,
(17.18)
in the sense that the same vector p solves both systems. As µk ↓0, however, the coefﬁcient
matrix in (17.18) approaches a well-conditioned limit when the second-order sufﬁcient
conditions of Theorem 12.6 are satisﬁed at x∗.
17.2
THE LOGARITHMIC BARRIER METHOD
PROPERTIES OF LOGARITHMIC BARRIER FUNCTIONS
We start by describing the concept of barrier functions in terms of inequality-
constrained optimization problems. Given the problem
min
x
f (x)
subject to ci(x) ≥0,
i ∈I,
(17.19)
the strictly feasible region is deﬁned by
Fo def {x ∈IRn | ci(x) > 0 for all i ∈I};
(17.20)
we assume that Fo is nonempty for purposes of this discussion. Barrier functions for this
problem have the properties that
• they are inﬁnite everywhere except in Fo;
• they are smooth inside Fo;
• their value approaches ∞as x approaches the boundary of Fo.
The most important barrier function is the logarithmic barrier function, which for the
constraint set ci(x) ≥0, i ∈I, has the form
−

i∈I
log ci(x),
(17.21)
where log(·) denotes the natural logarithm. For the inequality-constrained optimization
problem (17.19), the combined objective/barrier function is given by
P(x; µ)  f (x) −µ

i∈I
log ci(x),
(17.22)

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
499
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
1
2
3
4
5
6
7
8
Figure 17.3
Plots of P(x; µ) for µ  1.0, 0.4, 0.1, 0.01.
where µ is referred to here as the barrier parameter. From now on, we refer to P(x; µ) itself
as the “logarithmic barrier function for the problem (17.19),” or simply the “log barrier
function” for short. As for the quadratic penalty function (17.2), we can show that the
minimizer of P (x; µ), which we denote by x(µ), approaches a solution of (17.19) as µ ↓0,
under certain conditions; see Theorems 17.3 and 17.4.
❏Example 17.2
Consider the following problem in a single variable x:
min x
subject to x ≥0, 1 −x ≥0,
for which we have
P(x; µ)  x −µ log x −µ log(1 −x).
We graph this function for different values of µ in Figure 17.3. Naturally, for small values of
µ, the function P (x; µ) is close to the objective f over most of the feasible set; it approaches

500
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
∞only in narrow “boundary layers.” (In Figure 17.3, the curve P(x; 0.01) is almost indis-
tinguishable from f (x) to the resolution of our plot, though this function also approaches
∞when x is very close to the endpoints 0 and 1.) Also, it is clear that as µ ↓0, the minimizer
x(µ) of P (x; µ) is approaching the solution x∗ 0 of the constrained problem.
❐
Since the minimizer x(µ) of P(x; µ) lies in the strictly feasible set Fo (where no
constraints are active), we can in principle search for it by using the unconstrained min-
imization algorithms described in the ﬁrst part of this book. (These methods need to be
modiﬁed slightly to keep the iterates inside Fo.) Unfortunately, the minimizer x(µ) be-
comes more and more difﬁcult to ﬁnd as µ ↓0. The scaling of the function P(x; µ) becomes
poorer and poorer, and the quadratic Taylor series approximation (on which Newton-like
methods are based) does not adequately capture the behavior of the true function P(x; µ),
except in a small neighborhood of x(µ). These difﬁculties can be illustrated by the following
two-variable example.
❏Example 17.3
Consider the problem
min (x1 + 0.5)2 + (x2 −0.5)2
subject to x1 ∈[0, 1], x2 ∈[0, 1],
(17.23)
for which the log-barrier function is
P (x; µ)  (x1 + 0.5)2 + (x2 −0.5)2
(17.24)
−µ

log x1 + log(1 −x1) + log x2 + log(1 −x2)

.
Contours of this function for the values µ  1, µ  0.1, and µ  0.01 are plotted in
Figure 17.4. From Figure 17.3 we see that except near the boundary of the feasible region,
the contours approach those of the parabolic objective function as µ ↓0. Note that the
contours that surround the minimizer in the ﬁrst two plots are not too eccentric and not
too far from being elliptical, indicating that most unconstrained minimization algorithms
can be applied successfully to identify the minimizers x(µ).
For the value µ  0.01, however, the contours are more elongated and less elliptical
as the minimizer x(µ) is pushed toward the “boundary layer” of the barrier function. (A
closeup is shown in Figure 17.5.) As in the case of the quadratic penalty function, the
elongated nature of the contours indicates poor scaling, which causes poor performance of
unconstrained optimization methods such as quasi-Newton, steepest descent and conjugate
gradient. Newton’smethodisinsensitivetothepoorscaling,butthenonellipticalproperty—
the contours in Figure 17.5 are almost straight along the left edge while being circular along

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
501
the right edge—indicates that the quadratic approximation on which Newton’s method is
based does not capture well the behavior of the true log-barrier function. Hence, Newton’s
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 17.4
Contours of P(x; µ) from
(17.24) for (top to bottom)
µ  1, 0.1, 0.01.

502
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
method, too, may not show rapid convergence to x(µ) except in a small neighborhood of
this point.
❐
For more insight into the problem of poor scaling, we examine the structure of the
gradient and Hessian of P(x; µ). We have
∇xP(x; µ)  ∇f (x) −

i∈I
µ
ci(x)∇ci(x),
(17.25)
∇2
xxP(x; µ)  ∇2f (x) −

i∈I
µ
ci(x)∇2ci(x)
(17.26)
+

i∈I
µ
c2
i (x)∇ci(x)∇ci(x)T .
When x is close to the minimizer x(µ) and µ is small, we see from Theorem 17.4 and (17.32)
that the optimal Lagrange multipliers λ∗
i , i ∈I, can be estimated as follows:
λ∗
i ≈µ/ci(x),
i ∈I.
By substituting into (17.26), and using the deﬁnition (12.28) of the Lagrangian L(x, λ), we
ﬁnd that
∇2
xxP(x; µ) ≈∇2
xxL(x, λ∗) +

i∈I
1
µ(λ∗
i )2∇ci(x)∇ci(x)T .
(17.27)
NotethesimilarityofthisexpressiontotheHessianofthequadraticpenaltyfunction(17.16).
Asimilaranalysisoftheeigenvaluesappliesto(17.27)asinthatcase.Ift inequalityconstraints
0.05
0.1
0.15
0.2
0.25
0.35
0.4
0.45
0.5
0.55
0.6
0.65
Figure 17.5
Close-up of contours of
P(x; µ) from (17.24) for
µ  .01

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
503
areactiveatx∗,andifweassumethatthelinearindependenceconstraintqualiﬁcation(LICQ)
is satisﬁed at the minimizer x(µ), then t of the eigenvalues of ∇2
xxP(x; µ) have size of order
1/µ, while the remaining n −t are of order 1. Except in the special cases of t  0 and t  n,
the matrix ∇2
xxP (x; µ) becomes increasingly ill conditioned near the minimizer x(µ), as µ
approaches zero.
As in the case of the quadratic penalty function, the properties of the Hessian of
P (x; µ) may lead to complications in solving the Newton step equations, which are
∇2
xxP(x; µ)p  −∇xP(x; µ).
(17.28)
It is possible to reformulate this system in a similar manner to the reformulation (17.18) of
the Newton equations for Q(x; µk), though the presence of inequality constraints rather
than equalities makes the reformulation somewhat less obvious. In any case, elimination
techniques applied directly to (17.28) yield computed steps p that are good directions of
progress for the Newton method, despite the roundoff error caused by the ill conditioning.
We discuss this surprising observation further in the Notes and References at the end of the
chapter.
ALGORITHMS BASED ON THE LOG-BARRIER FUNCTION
Algorithms based on the log-barrier function aim to identify approximate minima of
P(x; µ) for a decreasing sequence of values of µ. The generic algorithm is very similar to
the quadratic penalty function framework, Framework 17.1, and can be written as follows.
Framework 17.2 (Log-Barrier).
Given µ0 > 0, tolerance τ0 > 0, starting point xs
0;
for k  0, 1, 2, . . .
Find an approximate minimizer xk of P(·; µk), starting at xs
k,
and terminating when ∥∇P(x; µk)∥≤τk;
if ﬁnal convergence test satisﬁed
STOP with approximate solution xk;
Choose new barrier parameter µk+1 ∈(0, µk);
Choose new starting point xs
k+1;
end (for)
As we noted above, the ill conditioning of ∇2
xxP(x; µ) for small µ makes Newton’s
method the only really effective technique for ﬁnding the approximate minimizer xk at each
iteration of this framework. Even Newton’s method encounters difﬁculties (discussed above)
because of the inadequacy of the quadratic Taylor series approximation on which it is based,
but various techniques can be used to ensure that Newton’s method enters the domain of
convergence for P(x; µk) in just a few steps and thereafter converges rapidly to x(µk). These
techniques include the following:

504
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
• Enhancing Newton’s method with a technique to ensure global convergence, such as a
line search or trust-region mechanism. They must ensure that the iterates stay within
the domain of P(x; µ), that is, the strictly feasible region Fo, and they should also
take account of the characteristics of the log-barrier function.
• Agoodstartingpointxs
k forminimizationofP(x; µk)canbeobtainedbyextrapolating
along the path deﬁned by the previous approximate minimizers xk−1, xk−2, . . . . Alter-
natively, we can calculate the approximate tangent vector to the path {x(µ) | µ > 0}
at the point xk−1 by performing total differentiation of ∇xP(x; µ) with respect to µ
to obtain
∇2
xxP(x; µ)˙x + ∂
∂µ∇xP(x; µ)  0.
By calculating the second term explicitly, we obtain
∇2
xxP(x; µ)˙x −

i∈I
1
ci(x)∇ci(x)  0.
(17.29)
By substituting x  xk−1 and µ  µk−1 into this expression, we obtain the approx-
imate tangent ˙x, which we can then use to obtain a starting point xs
k for step k of
Framework 17.2 as follows:
xs
k  xk−1 + (µk −µk−1)˙x.
(17.30)
Variousheuristicshavebeenproposedforthechoiceofthenewbarrierparameterµk+1
ateachiteration.Onecommoningredientisthatweusuallymakeanambitiouschoiceofµk+1
(µk+1  0.2µk or µk+1  0.1µk, say) if the minimization problem for P(x; µk) was not too
difﬁcult to solve and if a good starting point xs
k+1 can be proposed with some conﬁdence.
Since software based on the Framework 17.2 has not made its way into the mainstream
(at least not yet), we cannot point to a deﬁnitive “best” set of heuristics; determination of
reliable, efﬁcient heuristics is a question that requires extensive computational testing with
a representative set of nonlinear programming applications.
Despitethedifﬁcultiesinvolvedinimplementinglog-barriermethodseffectively,inter-
esthasrevivedinrecentyearspartlybecauseoftheirconnectiontoprimal–dualinterior-point
methods, which have been shown to perform extremely well for large linear programming
and convex quadratic programming problems. We explain this connection further below.
Remarkably, log-barrier methods remain an active area of research today, almost 45 years
after they were originally proposed by Frisch [95] and 30 years after the publication of an
authoritative study by Fiacco and McCormick [79].

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
505
PROPERTIES OF THE LOG-BARRIER FUNCTION AND FRAMEWORK 17.2
We now describe some of the main properties of the log-barrier function P(x; µ)
deﬁned in (17.22), together with a basic convergence result associated with Framework 17.2.
We start by stating a theorem about the convex programming problem. In this result,
we use the following notation:
M is the set of solutions of (17.19);
f ∗is the optimal value of the objective function f .
Theorem 17.3.
Suppose that f and −ci, i ∈I, in (17.19) are all convex functions, and that the strictly
feasible region Fo deﬁned by (17.20) is nonempty. Let {µk} be a decreasing sequence such that
µk ↓0, and assume that the solution set M is nonempty and bounded. Then the following
statements are true.
(i) For any µ > 0, P(x; µ) is convex in Fo and attains a minimizer x(µ) (not necessarily
unique) on Fo. Any local minimizer x(µ) is also a global minimizer of P(x; µ).
(ii) Any sequence of minimizers x(µk) has a convergent subsequence, and all possible limit
points of such sequences lie in M.
(iii) f (x(µk)) →f ∗and P(x(µk); µk) →f ∗, for any sequence of minimizers {x(µk)}.
For a proof of this result, see M. Wright [251, Theorem 5].
Note that it is possible to have convex programs for which the set M is empty (consider
min x subject to −x ≥0) or for which M is unbounded (consider min(x1,x2) x1 subject to
x1 ≥0), in which cases the conclusions of the theorem do not apply in general.
For general inequality-constrained problems (17.19), the corresponding result is more
local in nature. Given a “well-behaved” local solution x∗of the problem (that is, one that
satisﬁes the second-order sufﬁcient conditions of Theorem 12.6 along with strict com-
plementarity and a constraint qualiﬁcation), the log barrier function P(x; µ) has a local
minimizer close to x∗for all sufﬁciently small values of µ. (We state this result formally
below.) However, when the strictly feasible region Fo is unbounded, the barrier function
P (x; µ) may be unbounded below. It is also possible for a sequence of local minimizers of
P (x; µk) to converge to a point that is not a solution of (17.19) as µk ↓0. For examples, see
the exercises.
There is an important relationship between the minimizer of P(x; µ) and a point
(x, λ) satisfying the KKT conditions for the optimization problem (17.19), which are given
in (12.30). At a minimizer x(µ), the gradient of P(x; µ) with respect to x is zero, that is,
∇xP(x(µ); µ)  ∇f (x(µ)) −

i∈I
µ
ci(x(µ))∇ci(x(µ))  0.
(17.31)

506
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
If we deﬁne a Lagrange multiplier estimate by
λi(µ)
def
µ
ci(x(µ)),
i ∈I,
(17.32)
we can write (17.31) as
∇f (x(µ)) −

i∈I
λi(µ)∇ci(x(µ))  0.
(17.33)
This condition is identical to the ﬁrst KKT condition ∇xL(x, λ)  0 for the problem (17.19),
where the Lagrangian function L is given by
L(x, λ)  f (x) −

i∈I
λici(x);
(17.34)
(see (12.28) for the general deﬁnition of L). Let us check the other KKT conditions for the
inequality-constrained problem (17.19), which are as follows:
ci(x) ≥0,
for all i ∈I,
(17.35a)
λi ≥0,
for all i ∈I,
(17.35b)
λici(x)  0,
for all i ∈I.
(17.35c)
The nonnegativity conditions (17.35a) and (17.35b) are clearly satisﬁed by x  x(µ),
λ  λ(µ); in fact, the quantities ci(x(µ)) and λi(µ) are strictly positive for all i ∈I. The
only KKT condition that fails to be satisﬁed is the complementarity condition (17.35c). By
the deﬁnition (17.32), we have that
λi(µ)ci(x(µ))  µ,
for all i ∈I,
(17.36)
where µ is strictly positive.
From the observations above, we see that as µ ↓0, a minimizer x(µ) of P(x; µ)
and its associated Lagrange multiplier estimates λ(µ) come closer and closer to satisfying
the KKT conditions of (17.19). In fact, if some additional assumptions are satisﬁed at the
solution x∗of (17.19), we can show that (x(µ), λ(µ)) approaches the optimal primal–dual
solution (x∗, λ∗) as µ ↓0.
Theorem 17.4.
Suppose that Fo is nonempty and that x∗is a local solution of (17.19) at which the
KKT conditions are satisﬁed for some λ∗. Suppose, too, that the linear independence constraint
qualiﬁcation (LICQ) (Deﬁnition 12.1), the strict complementarity condition (Deﬁnition 12.2),
and that the second-order sufﬁcient conditions (Theorem 12.6) hold at (x∗, λ∗). Then the
following statements are true.

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
507
(i) There is a unique continuously differentiable vector function x(µ), deﬁned for all suf-
ﬁciently small µ by the statement that x(µ) is a local minimizer of P(x; µ) in some
neighborhood of x∗, such that limµ↓0 x(µ)  x∗.
(ii) For the function x(µ) in (i), the Lagrange multiplier estimates λ(µ) deﬁned by (17.32)
converge to λ∗as µ ↓0.
(iii) The Hessian ∇2
xxP(x; µ) is positive deﬁnite for all µ sufﬁciently small.
For a proof of this result, see Fiacco and McCormick [79, Theorem 12] or M. Wright [251,
Theorem 8].
The trajectory Cp deﬁned by
Cp
def {x(µ) | µ > 0}
(17.37)
is often referred to as the central path (or sometimes as the primal central path, to distinguish
it from the primal–dual central path Cpd deﬁned below).
HANDLING EQUALITY CONSTRAINTS
We have assumed in the analysis so far that the problem has only inequality constraints
andthatthestrictlyfeasibleregion Fo (17.20)isnonempty.Wenowdiscussonewayinwhich
the log-barrier technique can be applied to general nonlinear constrained problems of the
form (12.1), which we restate here as follows:
min
x
f (x)
subject to

ci(x)  0,
i ∈E,
ci(x) ≥0,
i ∈I.
(17.38)
We cannot simply replace the equality constraint ci(x)  0 by two inequalities—namely,
ci(x) ≥0 and −ci(x) ≥0—to force it into the form (17.19), since then no point x can
simultaneously satisfy both inequalities strictly. In other words, Fo will be empty.
One approach for dealing with equality constraints is to include quadratic penalty
terms for these constraints in the objective. If we assume for simplicity that the coefﬁ-
cient of the quadratic penalty term is 1/µ, where µ is the barrier parameter, then the
log-barrier/quadratic penalty function to be minimized for each value of µ is
B(x; µ)
def f (x) −µ

i∈I
log ci(x) + 1
2µ

i∈E
c2
i (x).
(17.39)
As expected, the properties of this function combine those of the log barrier function (17.22)
and the quadratic penalty function (17.2). Algorithms can be designed around a framework
similar to Frameworks 17.1 and 17.2; that is, successive reductions of the parameter µ
alternate with approximate minimizations of the function B(·; µ). The same issues arise:

508
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
the need to drive µ close to zero to obtain an accurate approximation of the true solution x∗;
ill-conditioning of the Hessian ∇2
xxB(x; µ) when µ is small; the need for careful selection of
a starting point for the minimization problem after µ is reduced; and the need for specialized
line searches.
Minimization of B(·; µ) still requires us to identify an initial point that is strictly
feasible with respect to the inequality constraints. This task becomes trivial if we introduce
slack variables si, i ∈I, into the formulation (17.38) and apply the method to the equivalent
problem
min
(x,s) f (x)
subject to



ci(x)  0,
i ∈E,
ci(x) −si  0,
i ∈I,
si ≥0,
i ∈I.
(17.40)
The function corresponding to (17.39) for this problem is
f (x) −µ

i∈I
log si + 1
2µ

i∈E
c2
i (x) + 1
2µ

i∈I
(ci(x) −si)2.
Any point (x, s) for which s > 0 (strict positivity) lies in the domain of this function.
RELATIONSHIP TO PRIMAL–DUAL METHODS
Bymodifyingthelog-barrierviewpointslightly,wecanderiveanewclassofalgorithms
known as primal–dual interior-point methods, which has been the focus of much research
activity in the ﬁeld of optimization since 1984. In primal–dual methods, the Lagrange mul-
tipliers are treated as independent variables in the computations, with equal status to the
primal variables x. As in the log-barrier approach, however, we still seek the primal–dual
pair (x(µ), λ(µ)) that satisﬁes the approximate conditions (17.33), (17.35a), (17.35b), and
(17.36) for successively smaller values of µ.
We explain the basic features of primal–dual methods only for the inequality-
constrained formulation (17.19); the extension to handle equality constraints as well is
simple (see the exercises). Suppose we rewrite the system of approximate KKT conditions
for the problem (17.19) (namely, (17.33), (17.35a), (17.35b), and (17.36)), reformulating
slightly by introducing slack variables si, i ∈I, to obtain
∇f (x) −

i∈I
λi∇ci(x)  0,
(17.41a)
c(x) −s  0,
(17.41b)
λisi  µ,
for all i ∈I,
(17.41c)
(λ, s) ≥0.
(17.41d)

1 7 . 2 .
T h e L o g a r i t h m i c B a r r i e r M e t h o d
509
Note that we have gathered the components ci(x), i ∈I, into a vector c(x); similarly for
λi and si, i ∈I. As noted above, the pair (x(µ), λ(µ)) deﬁned by (17.31) and (17.32)—
along with s(µ) deﬁned as c(x(µ))—solve the system (17.41), which we can view as a
bound-constrained nonlinear system of equations. We deﬁne the primal–dual central path
Cpd as
Cpd
def {(x(µ), λ(µ), s(µ)) | µ > 0}.
Note that the projection of Cpd onto the space of primal variables x yields the primal central
path Cp deﬁned by (17.37).
The bound constraints (17.41d) play a vital role: Primal–dual points (x, λ, s) that
satisfytheﬁrstthreeconditionsin(17.41)butviolatecondition(17.41d)typicallyliefarfrom
solutions of (17.19). Hence, most primal–dual algorithms require the λ and s components
of their iterates to be strictly positive, so that the inequality (17.41d) is satisﬁed by all iterates.
These methods aim to satisfy the other conditions (17.41a), (17.41b), and (17.41c) only in
the limit. Primal–dual steps are usually generated by applying a modiﬁed Newton method
for nonlinear equations to the system formed by the equality conditions (17.41a), (17.41b),
and (17.41c). In the log-barrier method, by contrast, we eliminate s and λ directly from
the nonlinear system (17.41) (by using (17.41d) and (17.41b)) before applying Newton’s
method.
To show the form of the modiﬁed Newton step, we group the ﬁrst three conditions in
(17.41) into a vector function Fµ, that is,
Fµ(x, λ, s)
def


∇f (x) −A(x)T λ
c(x) −s
Se −µe

,
(17.42)
where  and S are the diagonal matrices whose diagonal elements are λi, i ∈I, and si,
i ∈I, respectively, e is the vector whose components are all 1, and A(x) is the matrix of
constraint gradients, that is,
A(x)T  [∇ci(x)]i∈I.
The modiﬁed Newton equations for Fµ(x, λ, s) usually have the form
DFµ(x, λ, s)


x
λ
s

 −Fµ(x, λ, s) +


0
0
rλs

,

510
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
so by substitution from (17.42) we obtain


∇2
xxL(x, λ)
−A(x)T
0
A(x)
0
−I
0
S





x
λ
s




−∇f (x) + A(x)T λ
−c(x) + s
−Se + µe + rλs

,
(17.43)
where rλs is a modiﬁcation term that when chosen suitably is key to good theoretical and
practical performance of the method. (See Chapter 14 for a discussion of this term in the
context of linear programming.)
Note that the structure of the matrix in (17.43) is similar to the matrices in (14.11),
(14.20), and (16.53) from Chapters 14 and 16. In fact—if we allow for differences in notation
and in the problem formulation—we can see that these formulae are simply specializations
of the system (17.43) to the cases of linear and quadratic programming. Those chapters also
described the various other ingredients needed to build a complete algorithm around the
step formula (17.43). Such ingredients—a strategy for decreasing µ to zero, a way to calculate
the step length α along the calculated step, a technique for choosing the modiﬁcation term
rλs to improve the quality of the step, requirements on the conditions that must be satisﬁed
by the iterates (x, λ, s)—are needed also in the nonlinear case, but the nonlinearity gives rise
to other requirements for the algorithm as well. The choice of a merit function to govern step
selection for the problem of solving Fµ(x, λ, s)  0 is important. In recent work, authors
have proposed using the function B(x; µ) (17.39) in this role, or exact penalty functions
that combine the barrier function and constraints. We discuss these issues a little further in
the Notes and References at the end of the chapter. We omit details, however, because this
topic is a subject of recent and ongoing research, and substantial further efforts in analysis
and computation will be needed before clear trends emerge.
We see from (17.41) that we can motivate primal–dual methods independently of
log-barrier methods by simply viewing the system (17.41) as a perturbation of the KKT
system for (17.19). Just as in other areas of optimization, we are able to motivate a particular
approach in a number of different ways, all of which shed a different light on the properties
of the method.
17.3
EXACT PENALTY FUNCTIONS
Neither the quadratic penalty function nor the log-barrier function is an exact penalty
function. By contrast, we gave examples of exact penalty functions in Section 15.3. These are
functions with the property that for certain choices of the parameter µ, a single minimization
yields the exact solution of the nonlinear programming problem (see Deﬁnition 15.1). (In
Chapter 15 we used the term “merit function” instead of penalty function because we were
discussing their use as a measure of the quality of the step, but the terms “merit function”
and “penalty function” can be thought of as synonymous.)

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
511
An interesting class of algorithms for solving general nonlinear programming prob-
lems is based on minimizing the ℓ1 exact penalty function (15.24), which we restate for
convenience here as follows:
φ1(x; µ)  f (x) + 1
µ

i∈E
|ci(x)| + 1
µ

i∈I
[ci(x)]−.
(17.44)
(Asbefore,weusethenotation[x]− max{0, −x}.)Forallsufﬁcientlysmall,positivevalues
ofµ,oneminimizationofthisunconstrainedfunctionwillyieldthesolutionofthenonlinear
program. It is difﬁcult to determine µ a priori in most practical applications, however; rules
for adjusting this parameter during the course of the computation must be devised.
Minimization of φ1(x; µ) is made difﬁcult by the fact that it is nonsmooth—the ﬁrst
derivative is not deﬁned at any x for which ci(x)  0 for some i ∈E ∪I. General techniques
for nondifferentiable optimization, such as bundle methods [137], are not efﬁcient. One
approach that has proved to be effective in practice is to solve a sequence of subproblems in
which the objective function f in (17.44) is replaced by a quadratic approximation whose
Hessian is the Hessian of the Lagrangian for the nonlinear programming problem, and in
which the constraint terms ci are replaced by linear approximations about the current iterate
xk.Theresultingmethodturnsouttobecloselyrelatedtosequentialquadraticprogramming
methods, and is one of the most powerful techniques for solving constrained optimization
problems. For more details, see our discussion in Section 18.5.
Some other algorithms are based on the minimization of exact differentiable merit
functions related to the augmented Lagrangian; see (15.25), for example. These algorithms
also require the solution of a sequence of subproblems, but it has not yet been established
whether they can be the basis for reliable and efﬁcient software for nonlinear programming.
17.4
AUGMENTED LAGRANGIAN METHOD
WenowdiscussanalgorithmknownasthemethodofmultipliersortheaugmentedLagrangian
method. This algorithm is related to the quadratic penalty algorithm of Section 17.1, but
it reduces the possibility of ill conditioning of the subproblems that are generated in this
approach by introducing explicit Lagrange multiplier estimates at each step into the function
to be minimized. It also tends to yield less ill conditioned subproblems than does the log-
barrierapproach,anditdispenseswiththeneedforiteratestostaystrictlyfeasiblewithrespect
to the inequality constraints. It does not introduce nonsmoothness (as do methods based
on the ℓ1 penalty function (17.44), for instance), and implementations can be constructed
from standard software for unconstrained or bound-constrained optimization.
The method of multipliers is the basis for a practical implementation of high quality—
LANCELOT—which is sketched below.
In this section we use superscripts (usually k and k + 1) on the Lagrange multiplier
estimates to denote iteration index, and subscripts (usually i) to denote the component
indices of the vector λ.

512
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
MOTIVATION AND ALGORITHM FRAMEWORK
Weﬁrstconsidertheequality-constrainedproblem(17.1).Thequadraticpenaltyfunc-
tion Q(x; µ) (17.2) penalizes constraint violations by squaring the infeasibilities and scaling
them by 1/(2µ). As we see from Theorem 17.2, however, the approximate minimizers xk of
Q(x; µk) do not quite satisfy the feasibility conditions ci(x)  0, i ∈E. Instead, they are
perturbed slightly (see (17.8)) to approximately satisfy
ci(xk)  −µkλ∗
i ,
for all i ∈E.
(17.45)
To be sure, this perturbation vanishes as µk ↓0, but one may ask whether we can alter the
function Q(x; µk) to avoid this systematic perturbation—that is, to make the approximate
minimizers more nearly satisfy the equality constraints ci(x)  0. By doing so, we may
avoid the need to decrease µ to zero, and thereby avoid the ill conditioning and numerical
problems associated with Q(x; µ) for small values of this penalty parameter.
The augmented Lagrangian function LA(x, λ; µ) achieves these goals by including an
explicit estimate of the Lagrange multipliers λ, based on the formula (17.8), in the objective.
From the deﬁnition
LA(x, λ; µ)
def f (x) −

i∈E
λici(x) + 1
2µ

i∈E
c2
i (x),
(17.46)
we see that the augmented Lagrangian differs from the (standard) Lagrangian (12.28) for
(17.1) by the presence of the squared terms, while it differs from the quadratic penalty
function (17.2) in the presence of the summation term involving the λ. In this sense, it is
a combination of the Lagrangian and quadratic penalty functions. When we differentiate
with respect to x, we obtain
∇xLA(x, λ; µ)  ∇f (x) −

i∈E
[λi −ci(x)/µ]∇ci(x).
(17.47)
We now design an algorithm that ﬁxes the barrier parameter µ to some value µk > 0
at its kth iteration (as in Frameworks 17.1 and 17.2), ﬁxes λ at the current estimate λk, and
performs minimization with respect to x. Using xk to denote the approximate minimizer of
LA(x, λ; µk), we can use the logic in the proof of Theorem 17.2 to deduce that
λ∗
i ≈λk
i −ci(xk)/µk,
for all i ∈E.
(17.48)
By rearranging this expression, we have that
ci(xk) ≈−µk(λ∗
i −λk
i ),
for all i ∈E,
so we conclude that if λk is close to the optimal multiplier vector λ∗, the infeasibility in xk
will be much smaller than µk, rather than being proportional to µk as in (17.45).

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
513
How can we update the multiplier estimates λk from iteration to iteration, so that they
approximate λ∗more and more accurately, based on current information? Equation (17.48)
immediately suggests the formula
λk+1
i
 λk
i −ci(xk)/µk,
for all i ∈E.
(17.49)
This discussion motivates the following algorithmic framework.
Framework 17.3 (Method of Multipliers–Equality Constraints).
Given µ0 > 0, tolerance τ0 > 0, starting points xs
0 and λ0;
for k  0, 1, 2, . . .
Find an approximate minimizer xk of LA(·, λk; µk), starting at xs
k,
and terminating when ∥∇xLA(x, λk; µk)∥≤τk;
if ﬁnal convergence test satisﬁed
STOP with approximate solution xk;
Update Lagrange multipliers using (17.49) to obtain λk+1;
Choose new penalty parameter µk+1 ∈(0, µk);
Set starting point for the next iteration to xs
k+1  xk;
end (for)
We show below that convergence of this method can be assured without decreasing µ
to a very small value. Ill conditioning is therefore less of a problem than in Frameworks 17.1
and 17.2, so the choice of starting point xs
k+1 for each value of k in Framework 17.3 is less
critical. In Framework 17.3 we simply start the search at iteration k + 1 from the previous
approximate minimizer xk. Bertsekas [9, p. 347] suggests setting the tolerance τk to
τk  min(ϵk, γk∥c(xk)∥),
where {ϵk} and {γk} are two sequences decreasing to 0. An alternative choice is used by the
code LANCELOT (see Algorithm 17.4).
❏Example 17.4
Consider again problem (17.3), for which the augmented Lagrangian is
LA(x, λ; µ)  x1 + x2 −λ(x2
1 + x2
2 −2) + 1
2µ(x2
1 + x2
2 −2)2.
(17.50)
The primal solution is x∗ (−1, −1)T , and the optimal Lagrange multiplier is λ∗ −0.5.
Supposethatatiteratek wehaveµk  1(asinFigure17.1),whilethecurrentmultiplier
estimate is λk  −0.4. Figure 17.6 plots the function LA(x, −0.4; 1). Note that the spacing
of the contours indicates that the conditioning of this problem is similar to that of the

514
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Figure 17.6
Contours of LA(x, λ; µ) from (17.4) for λ  −0.4 and µ  1, contour
spacing 0.5.
quadratic penalty function Q(x; 1) illustrated in Figure 17.1. However, the minimizing
value of xk ≈(−1.02, −1.02) is much closer to the solution x∗ (−1, −1)T than is the
minimizing value of Q(x; 1), which is approximately (−1.1, −1.1). This example shows
that the inclusion of the Lagrange multiplier term in the function LA(x, λ; µ) can result in
a substantial improvement over the quadratic penalty method, as a way to reformulate the
constrained optimization problem (17.1).
❐
EXTENSION TO INEQUALITY CONSTRAINTS
When the problem formulation contains general inequality constraints—as in the for-
mulation (17.38)—we can convert it to a problem with equality constraints and bound
constraints by introducing slack variables si and replacing the inequalities ci(x) ≥0, i ∈I,
by
ci(x) −si  0,
si ≥0,
for all i ∈I.
(17.51)
This transformation gives rise to a problem containing equality constraints and bound con-
straints, and can be solved by the algorithm in LANCELOT, which treats bound constraints
explicitly (see (17.66)).
An alternative to handling the bounds directly in the subproblem is to eliminate the
slack variables si, i ∈I, directly from the minimization procedure in a manner that we

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
515
now describe. Supposing for simplicity that there are no equality constraints (E  ∅), the
introduction of slack variables transforms the problem to the following form:
min
x,s f (x)
subject to ci(x) −si  0,
si ≥0,
for all i ∈I.
By deﬁning the augmented Lagrangian in terms of the constraints ci(x) −si  0 and
applying the bound constraints si ≥0 explicitly, we obtain the following subproblem to be
solved at iteration k of Framework 17.3:
min
x,s f (x) −

i∈I
λk
i (ci(x) −si) +
1
2µk

i∈I
(ci(x) −si)2
(17.52a)
subject to si ≥0, for all i ∈I.
(17.52b)
Each si occurs in just two terms of (17.52a), which is in fact a convex quadratic function with
respect to each of these slack variables. We can therefore perform an explicit minimization
in (17.52) with respect to each of the si separately. The partial derivative of the subprob-
lem objective (17.52a) with respect to si is (λk)i −(1/µk)(ci(x) −si). The unconstrained
minimizer of (17.52a) with respect to si occurs when this partial derivative equals zero, that
is,
si  ci(x) −µλk
i .
(17.53)
If this unconstrained minimizer is smaller than the lower bound of 0, then since (17.52a) is
convex in si, the optimal value of si in (17.52) is 0. Summarizing, we ﬁnd that the optimal
values of si in (17.52) are
si  max(ci(x) −µλk
i , 0),
for all i ∈I.
(17.54)
We can use this formula to substitute for s and obtain an equivalent form of the
subproblem (17.52) in x alone. By isolating the terms involving si, we obtain that
−λk
i (ci(x) −si) + 1
2µ(ci(x) −si)2




−λk
i ci(x) + 1
2µc2
i (x)
if ci(x) −µλk
i ≤0,
−µ
2 (λk
i )2
otherwise.
(17.55)

516
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
By deﬁning the function ψ(t, σ; µ) of the scalar arguments t, σ, and µ to have the form of
the right-hand-side in (17.55), that is,
ψ(t, σ; µ)
def



−σt + 1
2µt2
if t −µσ ≤0,
−µ
2 σ 2
otherwise,
(17.56)
we obtain the following transformed subproblem:
min
x
LA(x, λk; µk)
def f (x) +

i∈I
ψ(ci(x), λk
i ; µk).
(17.57)
This deﬁnition of LA represents a natural extension of the augmented Lagrangian to
the inequality-constrained case. With this extension, we can apply Framework 17.3 to the
case of inequality constraints as well, with one further modiﬁcation: Once the approximate
solution xk is obtained, we use the following formula to update the Lagrange multipliers:
λk+1
i
 max(λk
i −ci(xk)/µk, 0),
for all i ∈I.
(17.58)
This formula follows from the fact that the derivative of LA(x, λ; µk) in (17.57) with respect
to x should be close to zero at the approximate solution xk. By using (17.55), we obtain
∇xLA(xk, λk; µk)  ∇f (xk) −

i∈I | ci(x)≤µλk
i

λk
i −ci(xk)/µk

∇ci(xk) ≈0.
The update formula (17.58) follows by comparing this formula to the ﬁrst KKT condition
(12.30a), which we can state as
∇f (x∗) −

i∈I | ci(x∗)0
λ∗
i ∇ci(x∗)  0.
It makes intuitive sense to maintain nonnegativity of the components of λ, since we know
from the KKT conditions (12.30d) that the optimal Lagrange multipliers for the inequality
constraints are nonnegative.
Each of the functions ψ(ci(x), λi; µ) is continuously differentiable with respect to x,
but there is in general a discontinuity in the second derivative with respect to x wherever
ci(x)  µλi for some i ∈I. However, when the strict complementarity condition holds
(Deﬁnition 12.2), the approximate minimizer xk of each subproblem (17.57) is usually not
close to the nonsmooth regions, so it does not usually interfere with the algorithm that solves
the subproblem. To verify this claim, we consider separately the cases in which constraint i
is active and inactive. When constraint i is active, then for a sufﬁciently advanced iterate k
we have ci(xk) ≈0, while λk
i and µk are both signiﬁcantly larger than zero. When the ith

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
517
constraint is inactive, we have ci(xk) > 0 while λk
i ≈0. In both cases, we can expect the
point xk to comfortably avoid the region in which the discontinuity condition ci(x)  µλi
is satisﬁed.
PROPERTIES OF THE AUGMENTED LAGRANGIAN
We now prove two results that justify the use of the augmented Lagrangian function
and the method of multipliers. For simplicity, we conﬁne our attention to the case of equality
constraints only, that is, the problem (17.1) for which the augmented Lagrangian is given
by (17.46).
The ﬁrst result validates the approach of Framework 17.3 by showing that when we
have knowledge of the exact Lagrange multiplier vector λ∗, the solution x∗of (17.1) is a strict
minimizer of LA(x, λ∗; µ) for all µ sufﬁciently small. Although we do not know λ∗exactly
in practice, the result and its proof strongly suggest that we can obtain a good estimate of x∗
by minimizing LA(x, λ; µ) even when µ is not particularly close to zero, provided that λ is
a reasonable estimate of λ∗. (We observed these properties already in Example 17.4.)
Theorem 17.5.
Let x∗be a local solution of (17.1) at which the LICQ is satisﬁed (that is, the gradients
∇ci(x∗), i ∈E, are linearly independent vectors), and the second-order sufﬁcient conditions
speciﬁed in Theorem 12.6 are satisﬁed for λ  λ∗. Then there is a threshold value ¯µ such that
for all µ ∈(0, ¯µ], x∗is a strict local minimizer of LA(x, λ∗; µ).
Proof.
We prove the result by showing that x∗satisﬁes the second-order sufﬁcient
conditions to be a strict local minimizer of LA(x, λ∗; µ) (see Theorem 2.4); that is,
∇xLA(x∗, λ∗; µ)  0,
∇2
xxLA(x∗, λ∗; µ) positive deﬁnite.
(17.59)
By using the KKT conditions (12.30) and the formula (17.47), we have that
∇xLA(x∗, λ∗; µ)  ∇f (x∗) −

i∈E
[λ∗
i −ci(x∗)/µ]∇ci(x∗)
 ∇f (x∗) −

i∈E
λ∗
i ∇ci(x∗)  ∇xL(x∗, λ∗)  0,
verifying the ﬁrst part of (17.59) (independently of µ).
We now prove the second part of (17.59) by showing that uT ∇2
xxLA(x∗, λ∗; µ)u > 0
for all u ∈IRn and all µ > 0 sufﬁciently small. From (17.13), we have
AT  [∇c∗
i ]i∈E,

518
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
and note that by LICQ (Deﬁnition 12.1), A has full row rank. Note, too, that
∇2
xxLA(x∗, λ∗; µ)  ∇2
xxL(x∗, λ∗) + 1
µAT A.
(17.60)
By the fundamental theorem of algebra, we can partition any u ∈IRn into components in
NullA and RangeAT , and write
u  w + AT v,
where w ∈NullA and v is a vector in IR|E|. By using (17.60) and the properties of w and v,
we have that
uT ∇2
xxLA(x∗, λ∗; µ)u  wT ∇2
xxL(x∗, λ∗)w + 2wT ∇2
xxL(x∗, λ∗)AT v
(17.61)
+ vT A∇2
xxL(x∗, λ∗)AT v + vT A
	
(1/µ)AT A

AT v.
We seek bounds on the three terms on the right-hand-side of this expression.
Because of (12.63) and compactness of the unit sphere intersected with NullA, there
is a scalar a > 0 such that
wT ∇xxL(x∗, λ∗)w ≥a∥w∥2,
for all w ∈NullA,
giving us a lower bound on the ﬁrst right-hand-side term in (17.61). For the second term,
deﬁne b
def ∥∇2
xxL(x∗, λ∗)AT ∥, so that
2wT ∇2
xxL(x∗, λ∗)AT v ≥−2b ∥w∥∥v∥,
giving a lower bound for the second term. For the third term, if we deﬁne c
def
∥A∇2
xxL(x∗, λ∗)AT ∥, we obtain
vT A∇2
xxL(x∗, λ∗)AT v ≥−c∥v∥2.
Finally, if d is the smallest eigenvalue of AAT , we have that
vT A
 1
µAT A

AT v ≥1
µ∥AAT v∥2 ≥d2
µ ∥v∥2.
By substituting these lower bounds into (17.61), we obtain
uT ∇2
xxLA(x∗, λ∗; µ)u ≥a∥w∥2 −2b∥v∥∥w∥+ (d2/µ −c)∥v∥2
 a [∥w∥−(b/a)∥v∥]2 + (d2/µ −c −b2/a)∥v∥2. (17.62)

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
519
The ﬁrst term on the right-hand-side of this expression is clearly nonnegative. Since d > 0
by the full rank of A, the second term is also nonnegative, provided that we choose ¯µ to be
any value such that
d2
¯µ −c −b2
a > 0
and choose µ ∈(0, ¯µ]. In fact, the right-hand-side of (17.62) is strictly positive unless v  0
and w  0, which implies that ∇2
xxLA(x∗, λ∗; µ) is positive deﬁnite, as required. Hence, we
have veriﬁed (17.59) and completed the proof.
□
Thesecondresult,givenbyBertsekas[9,Proposition4.2.3],describesthemorerealistic
situation of λ ̸ λ∗. It gives conditions under which there is a minimizer of LA(x, λ; µ) that
lies close to x∗and gives error bounds on both xk and the updated multiplier estimate λk+1
obtained from solving the subproblem at iteration k.
Theorem 17.6.
Suppose that the assumptions of Theorem 17.5 are satisﬁed at x∗and λ∗, and let ¯µ be
chosen as in that theorem. Then there exist positive scalars δ, ϵ, and M such that the following
claims hold:
(a) For all λk and µk satisfying
∥λk −λ∗∥≤δ/µk,
µk ≤¯µ,
(17.63)
the problem
min
x
LA(x, λk; µk)
subject to ∥x −x∗∥≤ϵ
has a unique solution xk. Moreover, we have
∥xk −x∗∥≤Mµk∥λk −λ∗∥.
(b) For all λk and µk that satisfy (17.63), we have
∥λk+1 −λ∗∥≤Mµk∥λk −λ∗∥,
where λk+1 is given by the formula (17.49).
(c) For all λk and µk that satisfy (17.63), the matrix ∇2
xxLA(xk, λk) is positive deﬁnite and
the constraint gradients ∇ci(xk), i ∈E, are linearly independent.

520
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
PRACTICAL IMPLEMENTATION
We now outline a practical algorithm that forms the basis of the LANCELOT im-
plementation of Conn, Gould, and Toint [53]. This program transforms inequalities into
equalities by means of slack variables, as in (17.51), and solves problems of the form
min
x∈IRn f (x)
subject to

ci(x)  0,
i  1, 2, . . . , m,
l ≤x ≤u.
(17.64)
The slacks are considered to be part of the vector x, and l and u are vectors of lower and
upper bounds. (Some components of l may be set to −∞, signifying that there is no lower
bound on the components of x in question; similarly for u.) LANCELOT is also designed
to be efﬁcient on special cases of (17.64); for example, bound-constrained problems (in
which m  0) and unconstrained problems. It can also take advantage of partially separable
structure in the objective function and constraints (see Chapter 9).
The augmented Lagrangian function used in LANCELOT [53] incorporates only the
equality constraints from (17.64), that is,
LA(x, λ; µ)  f (x) −
m

i1
λici(x) + 1
2µ
m

i1
c2
i (x).
(17.65)
The bound constraints are enforced explicitly in the subproblem, which has the form
min
x
LA(x, λ; µ)
subject to l ≤x ≤u.
(17.66)
The multiplier estimates λ and penalty parameter µ are held ﬁxed during each subproblem.
To ﬁnd an approximate solution of (17.66), the algorithm forms a quadratic model of
LA(x, λ; µ) and applies the gradient projection method described in Section 16.6 to obtain
a new iterate. The Hessian of the quadratic model uses either exact second derivatives or
quasi-Newton estimates of the Hessian of LA. We refer the reader to [53] and the references
therein for details.
By specializing the KKT conditions (12.30) to the problem (17.66) (see the exercises),
we ﬁnd that the ﬁrst-order necessary condition for x to be a solution of (17.66) is that
P[l,u]∇LA(x, λ; µ)  0,
(17.67)
where P[l,u]g is the projection of the vector g ∈IRn onto the rectangular box [l, u] deﬁned
by
	
P[l,u]g

i 



min(0, gi)
if xi  li,
gi
if xi ∈(li, ui),
max(0, gi)
if xi  ui.
for all i  1, 2, . . . , n.
(17.68)

1 7 . 4 .
A u g m e n t e d L a g r a n g i a n M e t h o d
521
The framework for LANCELOT, stated below, allows the subproblems (17.66) to be solved
inexactly and includes procedures for updating the tolerance used for the approximate
solution of the subproblem and for updating the penalty parameter µ. We omit details
of the gradient-projection procedure for solving the subproblem.
Algorithm 17.4 (LANCELOT–Method of Multipliers).
Choose positive constants ¯η, ¯ω, ¯µ ≤1, τ < 1, ¯γ < 1, αω, βω, αη, βη, α∗, β∗
satisfying αη < min(1, αω), βη < min(1, βω);
Choose λ0 ∈Rm;
Set µ0  ¯µ, α0  min(µ0, ¯γ ), ω0  ¯ω(α0)αω, η0  ¯η(α0)αη;
for k  0, 1, 2, . . .
Find an approximate solution xk of the subproblem (17.66) such that
P[l,u]∇LA(xk, λk; µk)
 ≤ωk,
where the projection operator P[l,u] is deﬁned as in (17.68).
if ∥c(xk)∥≤ηk
(* test for convergence *)
if ∥c(xk)∥≤η∗and
P[l,u]∇LA(xk, λk; µk)
 ≤ω∗
STOP with approximate solution xk;
end (if)
(* update multipliers, tighten tolerances *)
λk+1  λk −c(xk)/µk;
µk+1  µk;
αk+1  µk+1;
ηk+1  ηkα
βη
k+1;
ωk+1  ωkαβω
k+1;
else
(* decrease penalty parameter, tighten tolerances *)
λk+1  λk;
µk+1  τµk;
αk+1  µk+1 ¯γ ;
ηk+1  ¯ηα
βη
k+1;
ωk+1  ¯ωαβω
k+1;
end (if)
end (for)
The main branch in the algorithm tests the norm of c(xk) against the tolerance ηk.
If ∥c(xk)∥> ηk (the else branch), we decrease the penalty parameter µk to ensure that
the next subproblem will place more emphasis on decreasing the constraint violations. If
∥c(xk)∥≤ηk (the then branch), we decide that the current value of µk is doing a good job
of maintaining near-feasibility of the iterate xk, so we update the multipliers according to

522
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
formula (17.49) without decreasing µ. In both cases, we decrease the tolerances ωk and ηk
to force the subsequent primal iterates xk+1, xk+2, . . . to be increasingly accurate solutions
of the subproblem.
17.5
SEQUENTIAL LINEARLY CONSTRAINED METHODS
The principal idea behind sequential linearly constrained (SLC), or reduced Lagrangian,
methods is to generate a step by minimizing the Lagrangian subject to linearizations of
the constraints. In contrast to the sequential quadratic programming methods described in
Chapter 18, which minimize a quadratic approximation of the Lagrangian subject to linear
constraints, the subproblem in SLC methods uses a nonlinear objective function.
Let us ﬁrst consider the equality-constrained problem (17.1). The SLC subproblem in
this case takes the form
min
x
Fk(x)
subject to
∇ci(xk)T (x −xk) + ci(xk)  0, for all i ∈E.
(17.69)
There are several possible choices for Fk(x). Early SLC methods deﬁned
Fk(x)  f (x) −

i∈E
λk
i ¯ck
i (x),
(17.70)
where λk is the current Lagrange multiplier estimate and ¯ck
i (x) is the difference between
ci(x) and its linearization at xk, that is,
¯ck
i (x)  ci(x) −ci(xk) −∇ci(xk)T (x −xk).
(17.71)
One can show that as xk converges to a solution x∗, the Lagrange multiplier associated with
(17.69) converges to the optimal multiplier. Therefore, we take the multiplier estimate λk in
(17.70) to be the multiplier of the subproblem (17.69) at the previous iteration.
To obtain reliable convergence from remote starting points, the most popular SLC
method deﬁnes Fk to be the augmented Lagrangian function
Fk(x)  f (x) −

i∈E
λk
i ¯ck
i (x) + 1
2µ

i∈E
[¯ck
i (x)]2,
(17.72)
where µ is a positive penalty parameter. Note that this deﬁnition differs from (17.46) in that
the original constraints ci(x) have been replaced by ¯ck
i (x) from (17.71). The new deﬁnition
is reasonable, however, since all feasible points x must satisfy ¯ck
i (x)  0 whenever xk is a
solution of the equality-constrained problem (17.1).
SLC methods are mainly used to solve large problems, and the model function (17.72)
isincorporatedintheverysuccessfulimplementationMINOS[175].Sincethestepcomputa-
tion requires the minimization of the nonlinear subproblem (17.69), several iterations—and

1 7 . 5 .
S e q u e n t i a l L i n e a r l y C o n s t r a i n e d M e t h o d s
523
thus several function and constraint evaluations—are required to generate the new iterate.
Few “outer” iterations are required, however, in comparison with most other optimization
methods.
Newton or quasi-Newton iterations can be used to solve the linearly constrained
problem (17.69), and the sparse elimination techniques of Chapter 15 are used to exploit
sparsity in the constraint gradients.
For nonlinear optimization problems with inequality constraints (17.19), we can in-
troduce slacks where necessary and transform all inequalities into equalities together with
bound constraints. The subproblem is then given by (17.69) with the additional bounds
l ≤x ≤u,
where x now contains the variables and slacks, and l and u are suitable vectors of bounds. It
can be shown that in a neighborhood of a solution satisfying the second-order sufﬁciency
conditions, this subproblem will identify the optimal active set.
A drawback of SLC methods is that the subproblem must be solved quite accurately to
ensure that the Lagrange multiplier estimates are of good quality, so that a large number of
function evaluations may be required per outer iteration. Even though it is widely believed
that SQP methods are preferable to SLC methods, the now classical MINOS package im-
plementing an SLC method still ranks among the most effective implementations for large
nonlinear programming.
NOTES AND REFERENCES
The quadratic penalty function was ﬁrst proposed by Courant [60]. Gould [118]
addresses the issue of stable determination of the Newton step for Q(x; µk). His formula
(2.2) differs from our formula (17.17) in the right-hand-side, but both systems give rise to
the same p component. A reformulation of the log-barrier system (17.28) is also discussed
in [118], but it requires an estimate of the active set to be made. Gould [119] also discusses
the use of an extrapolation technique for ﬁnding a good starting point xs
k for each major
iterate of Framework 17.1. Analogous techniques for the log-barrier function are discussed
by Conn, Gould, and Toint [55] and Dussault [78].
For a discussion of the history of barrier function methods, see Nash [178]. The term
“interior-point,” which is widely used in connection with both log-barrier and primal–dual
methods that insist on strict satisfaction of the inequality constraints at each iteration, ap-
pears to originate in the book of Fiacco and McCormick [79]. As mentioned above, this book
has remained the standard reference in the area of log-barrier functions, and its importance
only increased after 1984 when interior-point methods for nonlinear programming became
a topic of intense research following the publication of Karmarkar’s paper [140]. Fiacco
and McCormick [79] also introduce the barrier/quadratic penalty function (17.39) and the
central path Cp.
Reformulation of the problem with a linear objective, which causes the Newton step
for P (·; µk+1) taken from the previous approximate minimizer xk to eventually pass close

524
C h a p t e r
1 7 .
P E N A L T Y , B A R R I E R , A N D L A G R A N G I A N . . .
to x(µk+1), is discussed by Wright and Jarre [258]. Analysis of the Newton/log-barrier
method, including the size of the region of convergence for the minimizer of P(·; µ) and of
the possibility of superlinear convergence, can be found in S. Wright [254].
Standard error analysis indicates that when the solution of the system (17.28) is com-
puted in a ﬁnite-precision environment, the ill conditioning in ∇2
xxP(x; µ) leads to large
errors in the step p. Remedies based on exploiting the structure in ∇2
xxP(x; µ) exposed by
(17.27) have been proposed, but more detailed analysis has shown that because of the special
structure of both the gradient and the Hessian of P(x; µ), the errors in the computed step
p are less signiﬁcant than a naive analysis would indicate, so that the computed p remains
a good search direction for the Newton algorithm until µ becomes quite small. For details,
see M. Wright [252] and S. Wright [256]. (These papers actually deal with primal–dual
algorithms for the problem (17.19), but their major conclusions can be applied to the sys-
tem (17.28) as well.) Specialized line search techniques that are suitable for use with barrier
functions are described by Murray and Wright [174] and Fletcher and McCann [87].
Techniques based on the log-barrier/quadratic penalty function deﬁned in (17.39)
have been investigated by Gould [119] and Dussault [78].
Primal–dual methods have been investigated in a number of recent papers. Forsgren
and Gill [91] use the function B(x; µ) (17.39) as a merit function for steps generated by a
primal–dualapproachforsolvingthesystemFµ(x, λ, s)  0.Gay,Overton,andWright[99]
add a Lagrangian term 
i∈I∪E λici(x) to B(x; µ) to obtain their primary merit function.
Byrd, Hribar, and Nocedal [34] use a nondifferentiable ℓ2 merit function for the barrier
problem. When the problem (12.1) is a convex program, algorithms more closely related
to the linear programming primal–dual algorithms of Chapter 14 than the ones described
above may be appropriate. For example, Ralph and Wright [211] describe a method with
good global and local convergence properties.
The method of multipliers was proposed by Hestenes [134] and Powell [195], and the
deﬁnitive work in this area is the book of Bertsekas [8]. Chapters 1–3 of Bertsekas’s book con-
tains a thorough motivation of the method that outlines its connections to other approaches.
Other introductory discussions are given by Fletcher [83, Section 12.2], and Polak [193, Sec-
tion 2.8]. The extension to inequality constraints was described by Rockafellar [216] and
Powell [198].
SLC methods were proposed by Robinson [215] and Rosen and Kreuser [218]. The
MINOS implementation is due to Murtagh and Saunders [175]. Our discussion of SLC
methods is based on that of Gill et al. [112].
✐
E x e r c i s e s
✐
17.1 For z ∈IR, show that the function min(0, z)2 has a discontinuous second deriva-
tive at z  0. (It follows that quadratic penalty function (17.5) may not have continuous
second derivatives even when f and ci, i ∈E ∪I, in (12.1) are all twice continuously
differentiable.)

1 7 . 5 .
S e q u e n t i a l L i n e a r l y C o n s t r a i n e d M e t h o d s
525
✐
17.2 Consider the scalar minimization problem:
min
x
1
1 + x2 ,
subject to x ≥1.
Write down P (x; µ) for this problem, and show that P(x; µ) is unbounded below for any
positive value of µ. (See Powell [197] and M. Wright [251].)
✐
17.3 Consider the scalar minimization problem
min x
subject to x2 ≥0, x + 1 ≥0,
for which the solution is x∗ −1. Write down P(x; µ) for this problem and ﬁnd its local
minimizers. Show that for any sequence {µk} such that µk ↓0, there exists a corresponding
sequence of local minimizers x(µk) that converges to 0.
✐
17.4 By building on (17.26), ﬁnd the third-derivative tensor for P(x; µ). Use the
approximation λ∗
i ≈µ/ci(x), i ∈I, to identify the largest term in this expression, and
estimate its size. (The large size of this tensor is an indicator of the inadequacy of the Taylor
series quadratic approximation to P(x; µ) discussed in the text.)
✐
17.5 Supposethatthecurrentpointxk istheexactminimizerof P(x; µk).Writedown
the formula for the Newton step for P(x; µk+1) from the current point xk, using the fact
that ∇P (xk; µk)  0 to eliminate the term ∇f (xk) from the right-hand-side. How does the
resulting step differ from the predictor step (17.29), (17.30) obtained by extrapolating along
the central path?
✐
17.6 Modify the system of nonlinear equations (17.41) that is used as the basis of
primal–dual methods for the case in which equality constraints are also present (that is, the
problem has the form (17.38)).
✐
17.7 Verify that the KKT conditions for the bound-constrained problem
min
x∈IRn φ(x)
subject to l ≤x ≤u
are equivalent to the compactly stated condition
P[l,u]∇φ(x)  0,
where the projection operator P[l,u] onto the rectangular box [l, u] is deﬁned in (17.68).
✐
17.8 Show that the function ψ(t, σ; µ) deﬁned in (17.56) has a discontinuity in its
second derivative with respect to t when t  µσ. Assuming that ci : IRn →IR is twice con-
tinuously differentiable, write down the second partial derivative matrix of ψ(ci(x), λi; µ)
with respect to x for the two cases ci(x) < µλi and ci(x) ≥µλi.

Chapter18

Sequential
Quadratic
Programming
One of the most effective methods for nonlinearly constrained optimization generates steps
by solving quadratic subproblems. This sequential quadratic programming (SQP) approach
can be used both in line search and trust-region frameworks, and it is appropriate for small
or large problems. Unlike sequential linearly constrained methods (Chapter 17), which are
effective when most of the constraints are linear, SQP methods show their strength when
solving problems with signiﬁcant nonlinearities.
Our development of SQP methods will be done in two stages. First we will present a
local algorithm that motivates the SQP approach and that allows us to introduce the step
computation and Hessian approximation techniques in a simple setting. We then consider
practicallinesearchandtrust-regionmethodsthatachieveconvergencefromremotestarting
points.

528
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
18.1
LOCAL SQP METHOD
Let us begin by considering the equality-constrained problem
min f (x)
(18.1a)
subject to c(x)  0,
(18.1b)
where f : IRn →IR and c : IRn →IRm are smooth functions. Problems containing only
equality constraints are not very common in practice, but an understanding of (18.1) is
crucial in the design of SQP methods for problems with general constraints.
The essential idea of SQP is to model (18.1) at the current iterate xk by a quadratic
programming subproblem and to use the minimizer of this subproblem to deﬁne a new
iterate xk+1. The challenge is to design the quadratic subproblem so that it yields a good step
for the underlying constrained optimization problem and so that the overall SQP algorithm
has good convergence properties and good practical performance. Perhaps the simplest
derivation of SQP methods, which we now present, views them as an application of Newton’s
method to the KKT optimality conditions for (18.1).
From (12.28) we know that the Lagrangian function for this problem is L(x, λ) 
f (x) −λT c(x). We use A(x) to denote the Jacobian matrix of the constraints, that is,
A(x)T  [∇c1(x), ∇c2(x), . . . , ∇cm(x)],
(18.2)
where ci(x) is the ith component of the vector c(x). By specializing the ﬁrst-order (KKT)
conditions (12.30) to the equality-constrained case, we obtain a system of n + m equations
in the n + m unknowns x and λ:
F(x, λ) 

∇f (x) −A(x)T λ
c(x)

 0.
(18.3)
If A∗has full rank, any solution (x∗, λ∗) of the equality-constrained problem (18.1) satisﬁes
(18.3). One approach that suggests itself is to solve the nonlinear equations (18.3) by using
Newton’s method, as described in Chapter 11.
The Jacobian of (18.3) is given by

W(x, λ)
−A(x)T
A(x)
0

,
(18.4)
where W denotes the Hessian of the Lagrangian,
W(x, λ)  ∇2
xxL(x, λ).
(18.5)

1 8 . 1 .
L o c a l S Q P M e t h o d
529
The Newton step from the iterate (xk, λk) is thus given by

xk+1
λk+1



xk
λk

+

pk
pλ

,
(18.6)
where pk and pλ solve the KKT system

Wk
−AT
k
Ak
0
 
pk
pλ



−∇fk + AT
k λk
−ck

.
(18.7)
This iteration, which is sometimes called the Newton–Lagrange method, is well-deﬁned when
the KKT matrix is nonsingular. We saw in Chapter 16 that nonsingularity is a consequence
of the following conditions.
Assumption 18.1.
(a) The constraint Jacobian Ak has full row rank.
(b) The matrix Wk is positive deﬁnite on the tangent space of the constraints, i.e., dT Wkd > 0
for all d ̸ 0 such that Akd  0.
The ﬁrst assumption is the linear independence constraint qualiﬁcation discussed in
Chapter 12 (see Deﬁnition 12.1), which we assume throughout this chapter. The second
condition holds whenever (x, λ) is close to the optimum (x∗, λ∗) and the second-order
sufﬁcient condition is satisﬁed at the solution (see Theorem 12.6). The Newton iteration
(18.6), (18.7) can be shown to be quadratically convergent under these assumptions and
constitutes an excellent algorithm for solving equality-constrained problems, provided that
the starting point is close enough to x∗.
SQP FRAMEWORK
There is an alternative way to view the iteration (18.6), (18.7). Suppose that at the
iterate (xk, λk) we deﬁne the quadratic program
min
p
1
2pT Wkp + ∇f T
k p
(18.8a)
subject to Akp + ck  0.
(18.8b)
If Assumptions 18.1 hold, this problem has a unique solution (pk, µk) that satisﬁes
Wkpk + ∇fk −AT
k µk  0,
(18.9a)
Akpk + ck  0.
(18.9b)

530
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
A key observation is that pk and µk can be identiﬁed with the solution of the Newton
equations (18.7). If we subtract AT
k λk from both sides of the ﬁrst equation in (18.7), we
obtain

Wk
−AT
k
Ak
0
 
pk
λk+1



−∇fk
−ck

.
(18.10)
Hence, by nonsingularity of the coefﬁcient matrix, we have that p  pk and λk+1  µk.
We refer to this interesting relationship as the equivalence between SQP and Newton’s
method: If Assumptions 18.1 hold at xk, then the new iterate (xk+1, λk+1) can be deﬁned
either as the solution of the quadratic program (18.8) or as the iterate generated by Newton’s
method (18.6), (18.7) applied to the optimality conditions of the problem. These alternative
interpretations are quite useful: The Newton point of view facilitates the analysis, whereas
the SQP framework enables us to derive practical algorithms and to extend the technique to
the inequality-constrained case.
We now state the SQP method in its simplest form.
Algorithm 18.1 (Local SQP Algorithm).
Choose an initial pair (x0, λ0);
for
k  0, 1, 2, . . .
Evaluate fk, ∇fk, Wk  W(xk, λk), ck, and Ak;
Solve (18.8) to obtain pk and µk;
xk+1 ←xk + pk; λk+1 ←µk;
if convergence test satisﬁed
STOP with approximate solution (xk+1, λk+1);
end (for).
It is straightforward to establish a local convergence result for this algorithm, since we know
that it is equivalent to Newton’s method applied to the nonlinear system F(x, λ)  0.
More speciﬁcally, if Assumptions 18.1 hold at a solution (x∗, λ∗) of (18.1), if f and c
are twice differentiable with Lipschitz continuous second derivatives, and if the initial point
(x0, λ0)issufﬁcientlycloseto(x∗, λ∗),thentheiteratesgeneratedbyAlgorithm18.1converge
quadratically to (x∗, λ∗) (see Section 18.10).
We should note in passing that in the objective (18.8a) of the quadratic program
we could replace the linear term ∇f T
k p by ∇xL(xk, λk)T p, since the constraint (18.8b)
makes the two choices equivalent. In this case, (18.8a) is a quadratic approximation of the
Lagrangian function, and this leads to an alternative motivation of the SQP method. We
replace the nonlinear program (18.1) by the problem of minimizing the Lagrangian subject
to the equality constraints (18.1b). By making a quadratic approximation of the Lagrangian
and a linear approximation of the constraints we obtain (18.8).

1 8 . 1 .
L o c a l S Q P M e t h o d
531
INEQUALITY CONSTRAINTS
The SQP framework can be extended easily to the general nonlinear programming
problem
min
f (x)
(18.11a)
subject to ci(x)  0,
i ∈E,
(18.11b)
ci(x) ≥0,
i ∈I.
(18.11c)
To deﬁne the subproblem we now linearize both the inequality and equality constraints to
obtain
min 1
2pT Wkp + ∇f T
k p
(18.12a)
subject to ∇ci(xk)T p + ci(xk)  0,
i ∈E,
(18.12b)
∇ci(xk)T p + ci(xk) ≥0,
i ∈I.
(18.12c)
We can use one of the algorithms for quadratic programming described in Chapter 16 to
solve this problem.
A local SQP method for (18.11) follows from Algorithm 18.1 above, with one modiﬁ-
cation: The step pk and the new multiplier estimate λk+1 are deﬁned as the solution and the
corresponding Lagrange multiplier of (18.12). The following result shows that this approach
eventually identiﬁes the optimal active set for the inequality-constrained problem (18.11).
Recall that strict complementarity is said to hold at a solution pair (x∗, λ∗) if there is no
index i ∈I such that λ∗
i  ci(x∗)  0.
Theorem 18.1.
Suppose that x∗is a solution point of (18.11). Assume that the Jacobian A∗of the active
constraints at x∗has full row rank, that dT W∗d > 0 for all d ̸ 0 such that A∗d  0, and
that strict complementarity holds. Then if (xk, λk) is sufﬁciently close to (x∗, λ∗), there is a local
solution of the subproblem (18.12) whose active set Ak is the same as the active set A(x∗) of the
nonlinear program (18.11) at x∗.
AstheiteratesoftheSQPmethodapproachaminimizersatisfyingtheconditionsgiveninthe
theorem, the active set will remain ﬁxed. The subproblem (18.12) behaves like an equality-
constrained quadratic program, since we can eventually ignore the inequality constraints
that do not fall into the active set A(x∗), while treating the active constraints as equality
constraints.
IQP VS. EQP
There are two ways of implementing the SQP method for solving the general nonlinear
programming problem (18.11). The ﬁrst approach solves at every iteration the quadratic

532
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
subprogram (18.12), taking the active set at the solution of this subproblem as a guess of
the optimal active set. This approach is referred to as the IQP (inequality-constrained QP)
approach, and has proved to be quite successful in practice. Its main drawback is the expense
of solving the general quadratic program (18.12), which can be high when the problem
is large. As the iterates of the SQP method converge to the solution, however, solving the
quadratic subproblem becomes very economical if we carry information from the previous
iteration to make a good guess of the optimal solution of the current subproblem. This
hot-start strategy is described later.
The second approach selects a subset of constraints at each iteration to be the so-
called working set, and solves only equality-constrained subproblems of the form (18.8),
where the constraints in the working sets are imposed as equalities and all other constraints
are ignored. The working set is updated at every iteration by rules based on the Lagrange
multiplier estimates, or by solving an auxiliary subproblem. This EQP approach has the
advantage that the equality-constrained quadratic subproblems are less expensive to solve
than (18.12) and require less sophisticated software.
An example of an EQP method is the gradient projection method described in Sec-
tion 16.6. In this method, the working set is determined by minimizing the quadratic model
along the path obtained by projecting the steepest descent direction onto the feasible region.
Another variant of the EQP method makes use of the method of successive linear program-
ming. This approach obtains a linear program by omitting the quadratic term pT Wkp from
(18.12a), applying a trust-region constraint ∥p∥≤k on the step p (see (18.45c)), and
taking the active set of this subproblem to be the working set for the current iteration. It
then ﬁxes the constraints in the working set and solves an equality-constrained quadratic
program (with the term pT Wkp reinserted) to obtain the step.
18.2
PREVIEW OF PRACTICAL SQP METHODS
To be practical, an SQP method must be able to converge from remote starting points and
on nonconvex problems. We now outline how the local SQP strategy can be adapted to meet
these goals.
We begin by drawing an analogy with unconstrained optimization. In its simplest
form, the Newton iteration for minimizing a function f takes a step to the minimum of the
quadratic model
mk(p)  fk + ∇f T
k p + 1
2pT ∇2fkp.
This framework is useful near the solution where the Hessian ∇2f (xk) is normally positive
deﬁnite and the quadratic model has a well-deﬁned minimizer. When xk is not close to the
solution, however, the model function mk may not be convex. Trust-region methods ensure

1 8 . 2 .
P r e v i e w o f P r a c t i c a l S Q P M e t h o d s
533
that the new iterate is always well-deﬁned and useful by restricting the candidate step pk
to some neighborhood of the origin. Line search methods modify the Hessian in mk(p) to
make it positive deﬁnite (possibly replacing it by a quasi-Newton approximation Bk), to
ensure that pk is a descent direction for the objective function f .
Similar strategies are used to globalize SQP methods. If Wk is positive deﬁnite on the
tangent space of the constraints, the quadratic subproblem (18.8) has a unique solution.
When Wk does not have this property, line search methods either replace it by a positive
deﬁnite approximation Bk or modify Wk directly during the process of matrix factorization.
A third possibility is to deﬁne Wk as the Hessian of an augmented Lagrangian function having
certain convexity properties. In all these cases, the subproblem (18.8) will be well-deﬁned.
Trust-region SQP methods add a constraint to the subproblem, limiting the step to
a region where the model (18.8) is considered to be reliable. Because they impose a trust-
region bound on the step, they are able to use Hessians Wk that fail to satisfy the convexity
properties. Complications may arise, however, because the inclusion of the trust region may
cause the subproblem to become infeasible. At some iterations, it is necessary to relax the
constraints, which complicates the algorithm and increases its computational cost. Due to
these tradeoffs, neither one of the two SQP approaches—line search or trust region—can
be regarded as clearly superior to the other.
The technique used to solve the line search and trust-region subproblems has a great
impact in the efﬁciency and robustness of SQP methods, particularly for large problems. For
line search methods, the quadratic programming algorithms described in Chapter 16 can
be used, whereas trust-region methods require special techniques.
Another important question in the development of SQP methods is the choice of a
merit function that guides the algorithms toward the solution. In unconstrained optimiza-
tion the merit function is simply the objective f , and it is ﬁxed throughout the course of the
minimization. SQP methods can use any of the merit functions discussed in Chapter 15, but
the parameters of these merit functions may need to be adjusted on some iterations to ensure
that the direction obtained from the subproblem is indeed a descent direction with respect
to this function. The rules for updating these parameters require careful consideration, since
they have a strong inﬂuence on the practical behavior of SQP methods.
In the remainder of this chapter we expand on these ideas to produce practical SQP
algorithms. We ﬁrst discuss a variety of techniques for solving the quadratic subproblems
(18.8) and (18.12), and observe the effect that they have in the form of the SQP iteration. We
then consider various formulations of the quadratic model that ensure their adequacy in a
linesearchcontext,studyingthecaseinwhichWk istheexactLagrangianHessianandalsothe
case in which it is a quasi-Newton approximation. We present two important merit functions
and show that the SQP directions are descent directions for these functions. This discussion
will set the stage for our presentation of practical line search and trust-region SQP methods.

534
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
18.3
STEP COMPUTATION
EQUALITY CONSTRAINTS
We begin by considering the equality-constrained quadratic program (18.8). We have
assumedsofarthattheHessianofthismodelisdeﬁnedastheHessianoftheLagrangianWk 
∇2
xxL(xkλk) (as in (18.5)). In later sections we will replace this matrix by a quasi-Newton
approximation Bk, but the discussion in this section is independent of this choice.
We have noted already that if Assumptions 18.1 hold, the solution of (18.8) is given
by the KKT system (18.10). In Chapter 16 we described a variety of techniques for solving
this system. We review these techniques here, pointing out some simpliﬁcations that can be
made when we know that the system is derived from an SQP subproblem, rather than from
a general QP.
Direct Solution of the KKT System.
Theﬁrstalternativeistosolvethefull(n+m)×(n+m)KKTsystem(18.10)withasymmetric
indeﬁnite factorization. In this approach, the KKT matrix is factored as LDLT , where D is
a block diagonal matrix with blocks of size 1 × 1 and 2 × 2 and L is unit lower triangular.
Two popular algorithms for computing this factorization are the methods of Bunch and
Kaufman [30] (for the dense case), and the method of Duff and Reid [75] (for the sparse
case). This approach is often called the augmented system approach. It provides us directly
with a step pk in the primal variables, and a new Lagrange multiplier estimate λk+1.
The KKT system (18.10) can also be solved by iterative methods such as the QMR and
LSQR iterations; see the Notes and References in Chapter 16. Early termination of iteration is
a delicate matter, since the resulting search direction may not lead toward a minimizer. Due
to this difﬁculty, iterative methods for the full KKT system are not yet commonly employed
in practical implementations of the SQP method. The topics of termination conditions and
preconditioning of the KKT system are subjects of current research.
Dual or Range-Space Approach.
If Wk is positive deﬁnite, we can decouple the KKT system (18.10) and solve the following
two systems in sequence to obtain λk+1 and pk:
(AkW −1
k AT
k )λk+1  AkW −1
k ∇fk −ck,
(18.13a)
Wkpk  −∇fk + AT
k λk+1.
(18.13b)
This approach is particularly effective for quasi-Newton methods that explicitly maintain
positive deﬁnite approximations Hk to the matrix W −1
k . If we are using a direct method to
solve (18.13a), we simply form the product AkW −1
k AT
k each time W −1
k
is updated. If we are
using an iterative method such as conjugate gradient, the explicit availability of W −1
k
makes
it easy to calculate matrix–vector products involving this matrix.

1 8 . 3 .
S t e p C o m p u t a t i o n
535
Null-Space Method.
The null-space approach is at the heart of many SQP methods. It requires knowledge of the
matrices Yk and Zk that span the range space of AT and null space of Ak, respectively; see
Chapter 15. By writing
pk  YkpY + ZkpZ,
we can substitute into (18.10) to obtain the following systems to be solved for for pY and pZ:
(AkYk)pY  −ck,
(18.14a)
	
ZT
k WkZk

pZ  −ZT
k WkYkpY −ZT
k ∇fk.
(18.14b)
The Lagrange multipliers λk+1, which we refer to as QP multipliers for reasons that
become clear below, can be obtained by solving
(AkYk)T λk+1  Y T
k (∇fk + Wkpk).
(18.15)
Theonlyassumptionwemakeon Wk isthatthereducedHessianZT
k WkZk ispositivedeﬁnite.
See Chapter 16 for a more extensive discussion of the null-space method.
There are several variants of this approach in which we compute an approximation
of the pure KKT step pk. The ﬁrst variant is simply to delete the term involving pk from
the right-hand-side of (18.15), thereby decoupling the computations of pk and λk+1. This
simpliﬁcation can be justiﬁed by observing that pk converges to zero as we approach the
solution, whereas ∇fk normally does not. Therefore, the new multipliers will be good esti-
mates of the QP multipliers near the solution. If we happen to choose Yk  AT
k (which is a
valid choice for Yk when Ak has full row rank; see Chapter 15), we obtain
ˆλk+1  (AkAT
k )−1Ak∇fk.
(18.16)
These are called the least-squares multipliers because they can also be derived by solving
the problem
min
λ
∇fk −AT
k λ
2
2 .
(18.17)
It is clear from this relation that the least-squares multipliers are useful even when the current
iterateisfarfromthesolution,becausetheyseektosatisfytheﬁrst-orderoptimalitycondition
in (18.3) as closely as possible. In practice, it is desirable to use the most current information
when computing least-squares multipliers. Therefore, in Algorithm 18.3 of Section 18.6 we
ﬁrstcomputethesteppk using(18.14),evaluatethegradientsofthefunctionsandconstraints

536
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
at the new point xk+1, and deﬁne λk+1 by (18.16) but with the terms on the right-hand side
evaluated at xk+1.
Conceptually, the use of least-squares multipliers transforms the SQP method from
an iteration in x and λ to a purely primal iteration in the x variable alone.
A second set of variants, known as reduced-Hessian methods, go one step further and
remove the cross term ZT
k WkYkpY in (18.14b), thereby yielding the system
(ZT
k WkZk)pZ  −ZT
k ∇fk.
(18.18)
This variant has the advantage that we need to store (or approximate) and factorize only the
matrix ZT
k WkZk, not the cross-term matrix ZT
k WkYk. As we show later, dropping the cross
term is justiﬁed because the normal component pY usually converges to zero faster than the
tangential component pZ, thereby making (18.18) a good approximation to (18.14b).
INEQUALITY CONSTRAINTS
The search direction in line search SQP methods for inequality-constrained optimiza-
tion is obtained by solving the subproblem (18.12). We can do this by means of the active-set
method for quadratic programming (Algorithm 16.1) described in Chapter 16. If Wk is
not convex, then we need to introduce the variations for indeﬁnite quadratic programming
described in that chapter.
We can make signiﬁcant savings in the solution of the quadratic subproblem by so-
called hot-start procedures. Speciﬁcally, we can use the solution ˜p of the previous quadratic
subproblem as the starting point for the current subproblem. Two phase-I iterations that can
take advantage of a good starting point were described in Chapter 16. We can also initialize
the working set for each QP subproblem to be the ﬁnal active set from the previous SQP
iteration. Additionally, it is sometimes possible—particularly when the problem contains
only linear constraints—to reuse or update certain matrix factorizations from the previous
iteration. Hot-start procedures are crucial to the efﬁciency of line search SQP methods.
A common difﬁculty in line search SQP methods is that the linearizations (18.12b),
(18.12c) of the nonlinear constraints may give rise to an infeasible subproblem. Consider,
for example, the case where n  1 and where the constraints are x ≤1 and x2 ≥0. When
we linearize these constraints at xk  3, we obtain the inequalities
3 + p ≤1
and
9 + 6p ≥0,
which are inconsistent.
To overcome this difﬁculty, we can deﬁne a relaxation of the SQP subproblem that is
guaranteed to be feasible. For example, the SNOPT program [108] for large-scale optimiza-
tion ﬁrst attempts to solve (18.12a)–(18.12c), but if this quadratic program is found to be

1 8 . 4 .
T h e H e s s i a n o f t h e Q u a d r a t i c M o d e l
537
infeasible, it solves the auxiliary problem
min f (x) + γ eT (v + w)
(18.19a)
subject to ci(x) −vi + wi  0,
i ∈E
(18.19b)
ci(x) −vi + wi ≥0,
i ∈I,
(18.19c)
v ≥0,
w ≥0.
(18.19d)
Here γ is a nonnegative penalty parameter, and eT  (1, . . . , 1). If the nonlinear problem
(18.11) has a feasible solution and γ is sufﬁciently large, the solutions to (18.19) and (18.11)
are identical. If, on the other hand, there is no feasible solution to the nonlinear problem and
γ is large enough, then the auxiliary problem (18.19) usually determines a “good” infeasible
point.Thechoiceofγ requiresheuristics;SNOPTusesthevalueγ  100∥∇f (xs)∥,wherexs
is the ﬁrst iterate at which inconsistent linearized constraints were detected. If a subproblem
isfoundtobeinfeasible,thenallsubsequentiteratesarecomputedbysolvingthesubproblem
(18.19).
An alternative approach for solving the quadratic subproblem (18.12) is to use an
interior-point method; see Section 16.7. For general problems this approach is competitive
with active-set methods only on early iterations, when the active sets change substantially
fromiterationtoiterationandnotmuchistobegainedfromhot-startinformation.(Interior-
point methods cannot take advantage of prior information about the solution or active set
to the same extent as active-set methods.) On some problems with special structure (for
example, certain applications in control) interior-point methods are better able to exploit
the structure than active-set methods, and therefore become competitive.
The step computation in trust-region SQP methods adds a trust-region bound to the
subproblem, and also reformulates the constraints in (18.12a)–(18.12c) to ensure feasibility
of the subproblem. We defer detailed discussion to Section 18.8, but mention here that the
Sℓ1QP method formulates a subproblem in which the linearized constraints are moved to
the objective of the quadratic program in the form of a penalty term. The method described
in Section 18.9 indirectly introduces a relaxation of the constraints by ﬁrst computing a
normal step by means of (18.47).
18.4
THE HESSIAN OF THE QUADRATIC MODEL
LetusnowconsiderthechoiceofthematrixWk inthequadraticmodel(18.8a).Forsimplicity,
we focus ﬁrst on the equality-constrained optimization problem (18.1).
The equivalence between SQP and Newton’s method applied to the optimality condi-
tions (18.3) is based on the choice of Wk as the Hessian of the Lagrangian, ∇2
xxL(xk, λk) (see
(18.5)). This choice leads to a quadratic rate of convergence under reasonable assumptions
and often also produces rapid progress when the iterates are distant from the solution. How-
ever, this matrix is made up of second derivatives of the objective function and constraints,

538
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
which may not be easy to compute. Moreover, it may not always be positive deﬁnite on the
constraint null space. In this section we discuss various alternative choices for Wk.
FULL QUASI-NEWTON APPROXIMATIONS
The ﬁrst idea that comes to mind is to maintain a quasi-Newton approximation Bk
to the full Lagrangian Hessian ∇2
xxL(xk, λk). Since the BFGS formula has proved to be very
successful in the context of unconstrained optimization, we can try to employ it in this case
as well.
The update for Bk that results from the step from iterate k to iterate k + 1 will make
use of the vectors sk and yk, which can be deﬁned as follows:
sk  xk+1 −xk,
yk  ∇xL(xk+1, λk+1) −∇xL(xk, λk+1).
(18.20)
We then compute the new approximation Bk+1 using the BFGS formula (8.19). We can view
this as the application of quasi-Newton updating to the case where the objective function
is given by the Lagrangian L(x, λ) (with λ ﬁxed). These deﬁnitions immediately reveal the
strengths and weaknesses of this approach.
If ∇2
xxL is positive deﬁnite in the region where the minimization takes place, the
quasi-Newton approximations {Bk} will reﬂect some of the curvature information of the
problem, and the iteration will converge robustly and rapidly, just as in the unconstrained
BFGS method. If, however, ∇2
xxL contains negative eigenvalues, then the BFGS approach of
approximating it with a positive deﬁnite matrix may be ineffective. In fact, BFGS updating
requires that sk and yk satisfy the curvature condition sT
k yk > 0, which may not hold when
sk and yk are deﬁned by (18.20), even when the iterates are close to the solution.
To overcome this difﬁculty, we could skip the BFGS update if the condition
sT
k yk ≥θsT
k Bksk
(18.21)
is not satisﬁed, where θ is a positive parameter (10−2, say). This skipping strategy has been
used in some SQP implementations, and it has performed well on many problems. On other
problems, however, it yields poor performance or even failure, so it cannot be regarded as
adequate for general-purpose algorithms.
A more effective modiﬁcation ensures that the update is always well-deﬁned by
modifying the deﬁnition of yk.
Procedure 18.2 (Damped BFGS Updating for SQP).
Deﬁne sk and yk as in (18.20) and set
rk  θkyk + (1 −θk)Bksk,
where the scalar θk is deﬁned as

1 8 . 4 .
T h e H e s s i a n o f t h e Q u a d r a t i c M o d e l
539
θk 

1
if sT
k yk ≥0.2sT
k Bksk,
(0.8sT
k Bksk)/(sT
k Bksk −sT
k yk)
if sT
k yk < 0.2sT
k Bksk.
.
(18.22)
Update Bk as follows:
Bk+1  Bk −BksksT
k Bk
sT
k Bksk
+ rkrT
k
sT
k rk
.
(18.23)
The formula (18.23) is simply the standard BFGS update formula, with yk replaced by
rk. It guarantees that Bk+1 is positive deﬁnite, since it is easy to show that when θk ̸ 1 we
have
sT
k rk  0.2sT
k Bksk > 0.
(18.24)
To gain more insight into this strategy, note that when θk  0 we have Bk+1  Bk, and that
θk  1 gives the (possibly indeﬁnite) matrix produced by the unmodiﬁed BFGS update. A
value θk ∈(0, 1) thus produces a matrix that interpolates the current approximation Bk
and the one produced by the unmodiﬁed BFGS formula. The choice of θk ensures that the
new approximation stays close enough to the current approximation Bk to ensure positive
deﬁniteness.
Damped BFGS updating has been incorporated in various SQP programs and has
performed well on many problems. Nevertheless, numerical experiments show that it, too,
can behave poorly on difﬁcult problems. It still fails to address the underlying problem that
the Lagrangian Hessian may not be positive deﬁnite. In this setting, SR1 updating (8.24) is
more appropriate, and is indeed a good choice for trust-region SQP methods. Line search
methods, however, cannot accept indeﬁnite Hessian approximations and would therefore
need to modify the SR1 formula, which is not totally desirable.
The strategy described next takes a different approach. It modiﬁes the Lagrangian
Hessian directly by adding terms to the Lagrangian function, the effect of which is to ensure
positive deﬁniteness.
HESSIAN OF AUGMENTED LAGRANGIAN
Let us consider the augmented Lagrangian function deﬁned by
LA(x, λ; µ)  f (x) −λT c(x) + 1
2µ∥c(x)∥2,
(18.25)
for some positive scalar µ. We have shown in Chapter 17 that at a minimizer (x∗, λ∗)
satisfying the second-order sufﬁciency condition, the Hessian of this function, which is
∇2
xxLA  ∇2
xxL(x∗, λ∗) + µ−1A(x∗)T A(x∗),
(18.26)

540
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
is positive deﬁnite for all µ smaller than a certain threshold value µ∗. Note that the last term
in (18.25) adds positive curvature to the Lagrangian on the space spanned by the columns
of A(x)T while leaving the curvature on the null space of A(x) unchanged. We could now
choose the matrix Wk in the quadratic subproblem (18.8) to be ∇2
xxLA(xk, λk; µ), or some
quasi-Newton approximation Bk to this matrix. For sufﬁciently small choices of µ, this
Hessian will always be positive deﬁnite and can be used directly in line search SQP methods.
This strategy is not without its difﬁculties. The threshold value µ∗depends on quan-
tities that are normally not known, such as bounds on the second derivatives of the problem
functions, so it can be difﬁcult to choose an appropriate value for µ. Choices of µ that are
too small can result in the last term in (18.26) dominating the original Lagrangian Hessian,
sometimes leading to poor practical performance. If µ is too large, the Hessian of the aug-
mented Lagrangian may not be positive deﬁnite and LA could be nonconvex, so that the
curvature condition of yT
k s may not be satisﬁed.
A variant of this approach is based on a vector yA
k deﬁned by
yA
k ≡∇xLA(xk+1, λk+1; µ) −∇xLA(xk, λk+1; µ)
 yk + µ−1AT
k+1ck+1,
where we have used the deﬁnition (18.20) to derive the second equality. One can show that
near the solution there is a maximum value of µ that guarantees uniform positiveness of
(yA
k )T s. One can then design an algorithm that selects µ adaptively to satisfy a positiveness
criterion and replaces yk by yA
k in the BFGS update formula. There is as yet insufﬁcient nu-
mericalexperiencetoknowwhetherthisapproachleadstorobustandefﬁcientSQPmethods.
REDUCED-HESSIAN APPROXIMATIONS
The two approaches just mentioned compute or approximate a full n × n Hessian. An
alternativeistoapproximateonlythereducedHessianoftheLagrangianZT
k ∇2
xxL(xk, λk)Zk,
a matrix of smaller dimension that under the standard assumptions is positive deﬁnite in
a neighborhood of the solution. This approach is used in reduced-Hessian quasi-Newton
methods, which compute the search direction from the formulae (18.14), (18.16) and a
quasi-Newton variant of (18.18). We restate these systems as follows:
λk  (AkAT
k )−1Ak∇fk,
(18.27a)
(AkYk)pY  −ck,
(18.27b)
MkpZ  −ZT
k ∇fk.
(18.27c)
We use Mk to distinguish the reduced-Hessian approximation from the full Hessian approx-
imation Bk. Note also that we have now deﬁned the least-squares multipliers λk so as to use
the most recent gradient information.

1 8 . 4 .
T h e H e s s i a n o f t h e Q u a d r a t i c M o d e l
541
We now discuss how the quasi-Newton approximations Mk to the reduced Hessian
ZT
k ∇2
xxL(xk, λk)Zk can be constructed. As before, let us deﬁne Wk  ∇2
xxL(xk, λk), and
supposethatwehavejusttakenastepαkpZ from(xk, λk)to(xk+1, λk+1).ByTaylor’stheorem,
we have
Wk+1αkpk ≈[∇xL(xk + αkpk, λk+1) −∇xL(xk, λk+1)] ,
where pk  xk+1 −xk  ZkpZ + YkpY. By premultiplying by ZT
k , we have
ZT
k Wk+1ZkαkpZ
(18.28)
≈−ZT
k Wk+1YkαkpY + ZT
k [∇xL(xk + αkpk, λk+1) −∇xL(xk, λk+1)] .
The secant equation for Mk is obtained by dropping the cross term ZT
k Wk+1YkαkpY (using
the rationale mentioned in the previous section), which yields
Mk+1sk  yk,
(18.29)
where sk and yk are deﬁned by
sk  αkpZ,
(18.30a)
yk  ZT
k [∇xL(xk + αkpk, λk+1) −∇xL(xk, λk+1)] .
(18.30b)
We can therefore apply the BFGS formula (8.19) using these deﬁnitions for the correc-
tion vectors sk and yk to deﬁne the new approximation Mk+1. In Section 18.7 we discuss
procedures for ensuring that the curvature condition sT
k yk > 0 holds.
One aspect of this description requires clariﬁcation. In the left-hand-side of (18.28) we
have ZT
k Wk+1Zk rather than ZT
k+1Wk+1Zk+1. We could have used Zk+1 in (18.28), avoiding
an inconsistency of indices, and by a longer argument show that yk can be deﬁned by
(18.30b) with Zk replaced by Zk+1. But this more complex argument is not necessary, since
the convergence properties of the simpler algorithm we described above are just as strong.
There are several variations of (18.30b) that have similar properties. One such
formula is
yk  ZT
k [∇f (xk+1) −∇f (xk)].
(18.31)
A more satisfying approach, proposed by Coleman and Conn [45], is to use curvature
information gathered along the tangent space of the constraints, not along the full steps of
the algorithm. In this approach, yk is deﬁned by
yk  ZT
k [∇xL(xk + ZkpZ, λk+1) −∇xL(xk, λk+1)],
(18.32)

542
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
which requires an additional evaluation of the gradient of the function and constraints at
the intermediate point xk + ZkpZ. One can show (see the exercises) that this deﬁnition of yk
guaranteespositivenessofyT
k sk inaneighborhoodofasolutionpoint,sothatBFGSupdating
can be safely applied once we are close enough to a solution. Since the cost of the additional
gradients at the intermediate point is signiﬁcant, practical implementations apply (18.32)
only at some iterations and in the rest of the iterations use (18.30b).
More details on reduced-Hessian methods will be given in Section 18.7.
18.5
MERIT FUNCTIONS AND DESCENT
To ensure that the SQP method converges from remote starting points it is common to use
a merit function φ to control the size of the steps (in line search methods) or to determine
whether a step is acceptable and whether the trust-region radius needs to be modiﬁed (in
trust-region methods). It plays the role of the objective function in unconstrained optimiza-
tion, since we insist that each step provide a sufﬁcient reduction in it. A variety of merit
functions have been used in conjunction with SQP methods. Here we focus on the non-
differentiable ℓ1 merit function and on Fletcher’s exact and differentiable function. These
two merit functions are representative of most of those used in practice. To simplify the
discussion, we focus our attention on the equality-constrained problem (18.1).
Although the merit function is needed to induce global convergence, we do not want
it to interfere with “good” steps—those that make progress toward a solution. In this section
we discuss the conditions on the problem and on the merit functions that ensure that the
functions show a decrease on a step generated by the SQP method.
Letusbeginwiththeℓ1 meritfunction(see(15.24)),whichfortheequality-constrained
problem (18.1) is deﬁned as
φ1(x; µ)  f (x) + 1
µ∥c(x)∥1,
(18.33)
where µ > 0 is called the penalty parameter. This function is not differentiable everywhere;
speciﬁcally, at points x for which one or more components of c(x) are zero, the gradient
is not deﬁned. However, it always has a directional derivative (see (A.14) in the Appendix
for background on directional derivatives). The following result describes the directional
derivative along the direction pk generated by the SQP subproblem.
Lemma 18.2.
Let pk and λk+1 be generated by the SQP iteration (18.10). Then the directional derivative
of φ1 in the direction pk satisﬁes
D(φ1(xk; µ); pk) ≤−pT
k Wkpk −(µ−1 −∥λk+1∥∞)∥ck∥1.
(18.34)

1 8 . 5 .
M e r i t F u n c t i o n s a n d D e s c e n t
543
Proof.
By applying Taylor’s theorem (see (2.5)) to f and ci, i  1, 2, . . . , m, we obtain
φ1(xk + αp; µ) −φ1(xk; µ)  f (xk + αp) −fk + µ−1∥c(xk + αp)∥1 −µ−1∥ck∥1
≤α∇f T
k p + γ α2∥p∥2 + µ−1∥ck + αAkp∥1 −µ−1∥ck∥1,
where the positive constant γ bounds the second-derivative terms in f and c. If p  pk is
given by (18.10), we have that Akpk  −ck, so for α ≤1 we have that
φ1(xk + αpk; µ) −φ1(xk; µ) ≤α[∇f T
k pk −µ−1∥ck∥1] + α2γ ∥pk∥2.
By arguing similarly, we also obtain the following lower bound:
φ1(xk + αpk; µ) −φ1(xk; µ) ≥α[∇f T
k pk −µ−1∥ck∥1] −α2γ ∥pk∥2.
Taking limits, we conclude that the directional derivative of φ1 in the direction pk is given
by
D(φ1(xk; µ); pk)  ∇f T
k pk −µ−1∥ck∥1.
The fact that pk satisﬁes the ﬁrst equation in (18.10) implies that
D(φ1(xk; µ); pk)  −pT
k Wkpk + pT
k AT
k λk+1 −µ−1∥ck∥1.
From the second equation in (18.10), we can replace the term pT
k AT
k λk+1 in this expression
by −cT
k λk+1. By making this substitution in the expression above and invoking the inequality
−cT
k λk+1 ≤∥ck∥1∥λk+1∥∞,
we obtain (18.34).
□
It follows from (18.34) that pk will be a descent direction for φ1 if Wk is positive
deﬁnite and µ is sufﬁciently small. A more detailed analysis shows that this assumption on
Wk can be relaxed, and that all that is necessary is that Wk be positive deﬁnite on the tangent
space of the constraints. A suitable value for the penalty parameter is obtained by choosing
a constant ¯δ > 0 and deﬁning µ at every iteration to be
µ−1  ∥λk+1∥∞+ ¯δ.
(18.35)
We now consider Fletcher’s augmented Lagrangian merit function (15.25), which is
deﬁned to be
φF(x; µ)
def f (x) −λ(x)T c(x) + 1
2µ∥c(x)∥2,
(18.36)

544
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
where µ > 0 is the penalty parameter, ∥· ∥denotes the ℓ2 norm, and
λ(x)  [A(x)A(x)T ]−1A(x)∇f (x)
(18.37)
are the least-squares multiplier estimates (18.17).
Fletcher’s merit function (15.25) is differentiable with gradient
∇φF(xk; µ)  ∇fk −AT
k λk −(λ′
k)T ck + µ−1AT
k ck,
where λ′
k is the m × n Jacobian of λ(x) evaluated at xk. If pk satisﬁes the SQP equation
(18.10), we have
∇φF(xk; µ)T pk  ∇f T
k pk + λT
k ck −cT
k λ′
kpk −µ−1∥ck∥2.
Let us write pk  ZkpZ + AT
k pY, where Zk is a basis for the null-space of Ak, and where we
have deﬁned Yk  AT
k . Recalling (18.27b), we have that
AT
k pY  −AT
k [AkAT
k ]−1ck,
which, when combined with (18.37) evaluated at x  xk, implies that ∇f T
k AT
k pY  −λT
k ck.
We thus obtain
∇φF(xk; µ)T pk  ∇f T
k ZkpZ + ∇f T
k AT
k pY + λT
k ck −cT
k λ′
kpk −µ−1∥ck∥2
 ∇f T
k ZkpZ −cT
k λ′
kpk −µ−1∥ck∥2.
We note from (18.10) that Wkpk  WkZkpZ + WkAT
k pY  −∇fk. By using this we obtain
∇φF(xk; µ)T pk  −pT
Z (ZT
k WkZk)pZ −pT
Y AkWkZkpZ
(18.38)
−cT
k λ′
kpk −µ−1∥ck∥2.
Thus pk will be a descent direction for Fletcher’s merit function if the reduced Hessian of
the Lagrangian ZT
k WkZk is positive deﬁnite and if µ satisﬁes the condition
µ−1 >

−1
2pT
Z ZT
k WkZkpZ −pT
Y AkWkZkpZ −cT
k λ′
kpk
∥ck∥2

+ ¯δ,
(18.39)
for some positive constant ¯δ. (If ∥ck∥ 0, the descent property holds for any value of µ.)
The factor 1
2 is somewhat arbitrary, and other values can be used; its objective is to ensure
that the directional derivative is at least as large as a fraction of −pT
Z ZT
k WkZkpZ. We note
that the formula (18.39) for µ is not as simple as the corresponding formula for the ℓ1 merit

1 8 . 6 .
A L i n e S e a r c h S Q P M e t h o d
545
function, and its value will depend on the singular values of both Ak and ZT
k WkZk. However,
this choice of µ is practical, and it allows us to establish global convergence results.
We summarize the results of this section, which couple the step-generation procedure
with the choice of merit function in SQP methods.
Theorem 18.3.
Suppose that xk is not a stationary point of the equality-constrained nonlinear problem
(18.1), and that the reduced Hessian ZT
k WkZk is positive deﬁnite. Then the search direction pk
generated by the pure SQP iteration (18.10) is a descent direction for the ℓ1 merit function φ1
if µ is given by (18.35). It is a descent direction for Fletcher’s function φF if µ satisﬁes (18.39).
In practice, as well as for the purposes of the analysis, it is desirable for the merit
function parameters {µk} to eventually stay the same as the iterates converge to the solution.
To make this scenario more likely, we use an update rule that leaves the current value of µ
unchanged whenever its value seems to be adequate, and that decreases it by a signiﬁcant
amount otherwise. Choose a constant δ > 0 and deﬁne
µk 

µk−1
if µ−1
k−1 ≥γ + δ,
(γ + 2δ)−1
otherwise,
(18.40)
where we set γ to ∥λk+1∥∞in the case of the ℓ1 merit function, and to the term inside square
brackets in (18.39) for Fletcher’s function φF.
The rule (18.40) forces the penalty parameter to be monotonically decreasing, but this
is not always desirable in practice. In some cases, µk takes on a very small value in the early
iterations, and as a result the constraints are highly penalized during the rest of the run.
Therefore, some SQP implementations include heuristics that allow µk to increase at certain
iterations, without interfering with the global convergence properties of the iteration.
The descent properties described above assume that the step is obtained by means of
the pure SQP iteration (18.10). This assumption holds in many line search algorithms, but
other variants of SQP such as reduced-Hessian methods and trust-region approaches may
compute the search direction differently. In all such cases, however, the descent properties
can be analyzed by some adaptation of the analysis above.
18.6
A LINE SEARCH SQP METHOD
From the discussion in the previous sections, we can see that there is a wide variety of line
search SQP methods that differ in the way the Hessian approximation is computed, in the
choice of the merit function, and in the step-computation procedure. We now incorporate
the ideas discussed so far into a practical quasi-Newton algorithm for solving the nonlinear
programming problem (18.11) with equality and inequality constraints.

546
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
Algorithm 18.3 (SQP Algorithm for Nonlinear Programming).
Choose parameters η ∈(0, 0.5), τ ∈(0, 1); Choose an initial pair (x0, λ0);
Choose an initial n × n symmetric positive deﬁnite Hessian
approximation B0;
Evaluate f0, ∇f0, c0, A0;
for
k  0, 1, 2, . . .
if termination test is satisﬁed
STOP;
Compute pk by solving (18.12)
Choose µk such that pk is a descent direction for φ at xk;
Set αk  1;
while φ(xk + αkpk; µk) > φ(xk, µk) + ηαkDφ(xk; pk)
Reset αk ←τααk for some τα ∈(0, τ);
end (while)
Set xk+1  xk + αkpk;
Evaluate fk+1, ∇fk+1, ck+1, Ak+1;
Compute λk+1 by solving
λk+1  −[Ak+1AT
k+1]−1Ak+1∇fk+1;
Set
sk  αkpk,
yk  ∇xL(xk+1, λk+1) −∇xL(xk, λk+1);
Obtain Bk+1 by updating Bk using a quasi-Newton formula;
end (for)
A version of this algorithm that uses second-derivative approximation is easily ob-
tained by replacing Bk by the Hessian of the Lagrangian Wk. As mentioned in Section 18.1,
precautions should be taken so that the constraints in (18.12) are consistent. We have left
freedom in the choice of the quasi-Newton approximation, and this algorithm requires only
that it be positive deﬁnite. Therefore, one option is to deﬁne Bk by means of the damped
BFGS update discussed in the previous section. We have also left the choice of the merit
function unspeciﬁed.
18.7
REDUCED-HESSIAN SQP METHODS
We now return to the case where it is advantageous to approximate only the reduced Hessian
of the Lagrangian (as opposed to the full Hessian, as in the algorithm given in the previous

1 8 . 7 .
R e d u c e d - H e s s i a n S Q P M e t h o d s
547
section). We have mentioned that methods that follow this approach are called reduced-
Hessian methods, and they have proved to be very effective in many areas of application,
such as optimal control.
Reduced-Hessian quasi-Newton methods are designed for solving problems in which
second derivatives are difﬁcult to compute, and in which the number of degrees of freedom
in the problem, (n −m), is small. They update an (n −m) × (n −m) approximation Mk
of ZT
k WkZk, and are required to satisfy (18.29). The strength of these methods is that when
n−m is small, Mk is of high quality, and computation of the null-space component pZ of the
step via (18.27c) is inexpensive. Another advantage, compared to full-Hessian quasi-Newton
approximations, is that the reduced Hessian is much more likely to be positive deﬁnite, even
when the current iterate is some distance from the solution, so that the safeguarding mecha-
nism in the quasi-Newton update will be required less often in line search implementations.
In this section we discuss some of the salient properties of reduced-Hessian methods and
then present a practical implementation.
SOME PROPERTIES OF REDUCED-HESSIAN METHODS
Let us consider ﬁrst the equality-constrained case and review the argument that leads
to the tangential step (18.18).
IfweweretoretainpY andcomputethesteppk byusingaquasi-Newtonapproximation
to the Hessian Wk, we would note that the pY component is based on a Newton-like iteration
applied to the constraints c(x)  0 (see (18.27b)), so that normally we can expect pY
to converge to zero quadratically. By contrast, the tangential component pZ converges only
superlinearly, since a quasi-Newton approximation to the Hessian is used in its computation.
Hence, it is common to observe in practice that
∥pY∥
∥pZ∥→0,
(18.41)
abehaviorthatiscommonlyknownastangentialconvergence.WededucethatpY isultimately
less signiﬁcant, so we are justiﬁed in dropping it from the computation (that is, setting
pY  0). By doing so, we eliminate the need to maintain an approximation to ZT
k WkYk. The
price we pay is that the total step pk is no longer a solution of the KKT system (18.10), but
rather an approximation to it. As a result, the overall convergence rate drops from 1-step
superlinear to 2-stepsuperlinear.Thisdifferencedoesnotappeartobesigniﬁcantinpractice.
In fact, as we discuss in Section 18.10, practical methods often achieve one-step superlinear
convergence in any case.
In the Coleman–Conn method based on (18.32), the step computation (18.27c),
(18.27b) takes the form
MkpZ  −ZT
k ∇fk,
(18.42a)
AkYkpY  −c(xk + ZkpZ).
(18.42b)

548
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
Note that the constraints are evaluated at the intermediate iterate xk + ZkpZ. One can show
that this variant achieves a one-step superlinear convergence rate and avoids the Maratos
effect described in Section 18.11.
UPDATE CRITERIA FOR REDUCED-HESSIAN UPDATING
Let us now consider the global behavior of reduced-Hessian methods and examine
the question of whether the curvature condition yT
k sk > 0 can be expected to hold near the
solution. Let us deﬁne the “averaged” Lagrangian Hessian ¯Wk over the step pk to be
¯Wk 
 1
0
∇2
xxL(xk + τpk, λk+1)dτ.
(18.43)
Then for our ﬁrst deﬁnition (18.30b) of yk, we have by Taylor’s theorem that
yk  ¯Wkpk  ¯WkZkpZ + ¯WkYkpY.
If we take sk to be the full step pk, we thus obtain
yT
k sk  pT
Z ZT
k ¯WkZkpZ + pZZT
k ¯WkYkpY.
(18.44)
Near the solution, the ﬁrst term on the right-hand-side is positive under our assumption of
second-order sufﬁciency, but the last term is of uncertain sign. Since reduced-Hessian quasi-
Newtonmethodsnormallyexhibittangentialconvergence(18.41),pY convergestozerofaster
than does pZ. Therefore, the ﬁrst term on the right-hand-side of (18.44) will eventually dom-
inate the second term, resulting in a positive value for yT
k sk. However, we cannot guarantee
this property, even arbitrarily close to a solution, so a safeguarding mechanism is needed to
ensure robustness.
To prevent a bad quasi-Newton update from taking place we could simply skip the
update if a condition like (18.21) does not hold. Skipping the BFGS update is more justiﬁable
in the case of reduced-Hessian methods than for full-Hessian methods. For one thing, it
occurs less often; the discussion of the previous paragraph indicates that yT
k sk is usually
signiﬁcantly positive near the solution. In situations in which the update is skipped, the
second term in (18.44) outweighs the ﬁrst term, suggesting that the step component pY is
not small relative to pZ. Since the normal component pY is determined by ﬁrst-derivative
information (see (18.27b)), which is known accurately, it generally makes good progress
toward the solution. The behavior of the step component pZ, which depends on the reduced-
Hessian approximation, is therefore of less concern in this particular situation, so we can
afford to skip an update.
This discussion motivates the following skipping rule, which explicitly monitors the
magnitudes of the normal and tangential components of the step.

1 8 . 7 .
R e d u c e d - H e s s i a n S Q P M e t h o d s
549
Procedure 18.4 (Update–Skip).
Given a sequence of positive numbers γk with ∞
k1 γk < ∞;
if yT
k sk > 0 and ∥pY∥≤γk∥pZ∥;
Compute sk and yk from (18.30);
Update Mk with the BFGS formula to obtain Mk+1;
else
Set Bk+1  Bk;
end (if).
The choice of the sequence γk is somewhat arbitrary; values such as γk  0.1k−1.1
have been used in practice.
In the Coleman–Conn method, yk is given by (18.32). In this case, we modify the
deﬁnition of the averaged Lagrangian Hessian ¯Wk by replacing the term τpk inside the
integral (18.43) by τpZ. We then obtain for yT
k sk that
yT
k sk  pT
Z ZT
k ¯WkZkpZ,
guaranteeing that yT
k sk will be positive near the solution under our second-order sufﬁcient
assumptions. Away from the solution, however, this term can be negative or very small.
Recall that in the unconstrained case, we used a line search strategy to ensure that yT
k sk is
sufﬁciently large. A similar effect can be achieved in the constrained case by performing a
curved line search that roughly follows the constraints [101].
CHANGES OF BASES
All the discussion in this section has been framed in the context of an equality-
constrained problem because this simpliﬁes the presentation and because the concepts
pertained to a single step of the algorithm. We now consider the effects of changes in
the working set, and this highlights one of the potential weaknesses of reduced-Hessian
quasi-Newton methods.
Since the working set (the set of indices that represents our best guess of the optimal
active set) changes from iteration k to iteration k +1, so does the size of the reduced-Hessian
matrix. If one or more indices are added to the active set at a given iteration, the active
constraint gradients can be used to project the current reduced-Hessian approximation Mk
onto a smaller approximation Mk+1 at the new iterate. If, on the other hand, one or more
indices are deleted from the active set, the matrix Mk+1 has larger dimension than Mk. It is
not obvious how the new rows and columns of Mk+1 can be initialized; some iterations may
be required before these new entries accumulate enough information to start contributing
to the quality of the steps pk. Thus if constraints are added and deleted frequently, the
quasi-Newton approximations will not be of high quality. Fortunately, these difﬁculties will
disappear as the iterates converge to the solution, since the working set tends to settle down.

550
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
Several procedures have been proposed to cope with this case where the Hessian
approximation increases in dimension, but none of them is completely satisfying. Therefore,
for simplicity, we will focus our speciﬁcation of a practical reduced-Hessian method given
below on the equality-constrained problem (18.1) and refer the reader to [108] for a detailed
treatment of inequality constraints.
Even when the dimension of the working set does not change, there may be an abrupt
change in the null-space basis Z from one step to the next due to a different deﬁnition of
basic variables in (15.14). In addition, if the selection of the basic variables is unfortunate,
Z may be unnecessarily badly conditioned, which can introduce roundoff errors in the step
computation. All of these potential pitfalls need to be addressed in robust reduced-Hessian
methods, showing that their implementations can be quite sophisticated; see [108], [86],
[146].
A PRACTICAL REDUCED-HESSIAN METHOD
The discussion above shows that there is a great deal of complexity in a practical
reduced-Hessian method, especially if we intend it to solve large problems. Nevertheless, the
framework for this type of method, for the case of equality constraints, can be speciﬁed as
follows.
Algorithm 18.5 (Reduced-Hessian Method—Equality).
Choose parameters η ∈(0, 0.5), τ ∈(0, 1);
Choose an initial pair (x0, λ0);
Choose an initial (n −m) × (n −m) symmetric positive deﬁnite
reduced-Hessian approximation M0;
Evaluate f0, ∇f0, c0, A0;
Compute Y0 and Z0 that span the range space of AT
0 and
null space of A0, respectively;
for
k  0, 1, 2, . . .
if termination test is satisﬁed
STOP;
Compute pY and pZ by solving
(AkYk)pY  −ck,
MkpZ  −ZT
k ∇fk;
Set pk  YkpY + ZkpZ;
Choose µk such that pk is a descent direction for φ at xk;
Set αk  1;
while φ(xk + αkpk; µk) > φ(xk, µk) + ηαkDφ(xk; pk)
Reset αk ←τααk for some τα ∈(0, τ);
end (while)

1 8 . 8 .
T r u s t - R e g i o n S Q P M e t h o d s
551
Set xk+1  xk + αkpk;
Evaluate fk+1, ∇fk+1, ck+1, Ak+1;
Compute Yk+1 and Zk+1 that span the range space of AT
k+1 and
null space of Ak+1, respectively;
Compute λk+1 by solving
λk+1  −[Y T
k+1AT
k+1]−1Y T
k+1∇fk+1;
Set
sk  αkpZ,
yk  ZT
k [∇xL(xk+1, λk+1) −∇xL(xk, λk+1)];
if update criterion is satisﬁed
obtain Mk+1 by updating Mk via formula (8.19);
else
Mk+1 ←Mk;
end (if)
end (for)
The matrix Mk should be a quasi-Newton approximation of the reduced Hessian of
the Lagrangian, modiﬁed if necessary so that it is sufﬁciently positive deﬁnite.
18.8
TRUST-REGION SQP METHODS
Trust-region implementations of the SQP approach have several attractive properties.
Among them are the fact that they provide a means for treating the case where active
constraint gradients are linearly dependent, and that they can make direct use of second-
derivative information. Nevertheless, some of the algorithms described in this section have
only recently been proposed and have been fully developed only for the equality-constrained
case.
Therefore, we begin by focusing on the equality-constrained problem (18.1), for which
the quadratic subproblem is (18.8). By adding a trust-region constraint, we obtain the new
model
min
p
1
2pT Wkp + ∇f T
k p
(18.45a)
subject to Akp + ck  0,
(18.45b)
∥p∥≤k.
(18.45c)

552
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
1
p
∆k
p2
c
+
p
A
k
k
=0
Figure 18.1
Inconsistent constraints in trust-region model.
We assume for the moment that ∥· ∥denotes the ℓ2 norm, even though in practice it is
common to work with a scaled trust region of the form ∥Sp∥≤k. The trust-region
radius k will be updated depending on how the predicted reduction in the merit function
compares to the actual reduction. If there is good agreement, the trust-region radius is
unaltered or increased, whereas if the agreement is poor, the radius is decreased.
The inclusion of the ℓ2 trust-region constraint makes the subproblem (18.45) consid-
erably more difﬁcult to solve than the quadratic program (18.8). Moreover, the new model
may not always have a solution because the constraints (18.45b), (18.45c) can be inconsis-
tent, as illustrated in Figure 18.1. In this example, any step p that satisﬁes the linearized
constraints must lie outside the trust region.
To resolve the possible conﬂict between satisfying the linear constraints (18.45b) and
the trust-region constraint (18.45c), it is not appropriate simply to increase k until the set
of steps p satisfying the linear constraints (18.45b) intersects the trust region. This approach
would defeat the purpose of using the trust region in the ﬁrst place as a way to deﬁne a
region within which we trust the model (18.45a), (18.45b) to accurately reﬂect the behavior
of the true objective and constraint functions in (18.1), and it would harm the convergence
properties of the algorithm. A more appropriate viewpoint is that there is no reason to try
to satisfy the equality constraints exactly at every step; rather, we should aim to improve
the feasibility of these constraints at each step and to satisfy them exactly only in the limit.
This point of view leads to three different techniques for reformulating the trust-region
subproblem, which we describe in turn.

1 8 . 8 .
T r u s t - R e g i o n S Q P M e t h o d s
553
p1
p2
=0
+
p
Ak
θck
k
+
p
A
c =0
k
Figure 18.2
Relaxed constraints becoming consistent.
APPROACH I: SHIFTING THE CONSTRAINTS
In this approach we replace (18.45b) by the shifted constraint
Akp + θck  0,
(18.46)
where θ ∈(0, 1] is chosen small enough that the constraint set (18.45b), (18.45c) is feasible.
It is not difﬁcult to ﬁnd values of θ that achieve this goal. In the example of Figure 18.1, we
see that θ has the effect of shifting the line corresponding to the linearized constraints to a
parallel line that intersects the trust region; see Figure 18.2.
Though it is easy to ﬁnd a value of θk that makes the constraint set (18.45b), (18.45c)
feasible, it is not a simple matter to make a choice of this parameter that ensures good
practical performance of the algorithm. The value of θk strongly inﬂuences performance
because it controls whether the computed step p tends more to satisfy constraint feasibility
or to minimize the objective function. If we always choose θk to be much smaller than its
maximum acceptable value (that is, if the shifted constraint line in Figure 18.2 almost passes
through the current iterate xk), then the step computed in the subproblem (18.45) will
tend to do little to improve the feasibility of the constraints c(x)  0, focusing instead on
reduction of the objective function f .
The following implementation of the constraint-shifting approach overcomes this
difﬁculty by computing the step in two stages. In the ﬁrst stage, we ignore the objective

554
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
function altogether, and investigate instead just how close we can come to satisfying the
linearized constraints (18.45b) while staying well inside the trust region (18.45c). To be
precise, we solve the following “normal subproblem”:
min
v
∥Akv + ck∥2
(18.47a)
subject to ∥v∥2 ≤ζk,
(18.47b)
where ζ is some parameter in the interval (0, 1); a typical value is ζ  0.8. We call the
solution vk of this subproblem the normal step. We now demand that the total step pk of the
trust-region method give as much reduction toward satisfying the constraints as the normal
step. This is achieved by replacing θck by −Akvk in (18.46), yielding the new subproblem
min
p
1
2pT Wkp + ∇f T
k p
(18.48a)
subject to Akp  Akvk,
(18.48b)
∥p∥2 ≤k.
(18.48c)
It is clear that the constraints (18.48b), (18.48c) of this subproblem are consistent,
since the choice p  vk satisﬁes both of these constraints.
The new subproblem (18.48) differs from the trust-region subproblems for un-
constrained optimization described in Chapter 4 because of the presence of the equality
constraints (18.48b). However, as we show in the next section, where we outline a prac-
tical implementation of this approach, we can eliminate these constraints and obtain a
subproblem that can be solved by the trust-region techniques described in Chapter 4.
APPROACH II: TWO ELLIPTICAL CONSTRAINTS
Another modiﬁcation of the approach is to reformulate the SQP subproblem as
min
p
1
2pT Wkp + ∇f T
k p
(18.49a)
subject to ∥Akp + ck∥2 ≤πk,
(18.49b)
∥p∥≤k.
(18.49c)
(Note that this problem reduces to (18.45) when πk  0.) There are several ways to choose
the bound πk. The ﬁrst is to demand that the step pk be at least as good toward satisfying
the constraints as a steepest descent step. We deﬁne
πk  ∥AkpC + ck∥2,

1 8 . 8 .
T r u s t - R e g i o n S Q P M e t h o d s
555
where pC is the Cauchy point for the problem
min
v
m(v)  ∥Akv + ck∥2
2
subject to ∥v∥2 ≤k.
(18.50)
(Recall from Chapter 4 that the Cauchy point is the minimizer of m along the steepest descent
direction −∇m(0) subject to the trust-region constraint.) It is clear that this choice of πk
makes the constraints (18.49b), (18.49c) consistent, since pC satisﬁes both constraints. Since
this choice forces the step obtained by solving (18.50) to make at least as much progress
toward feasibility as the Cauchy point, one can show that the iteration has good global
convergence properties.
An alternative way to select πk is to deﬁne it to be any number satisfying
min
∥p∥2≤b1k ∥Akp + ck∥2
2 ≤πk ≤
min
∥p∥2≤b2k ∥Akp + ck∥2
2,
where b1 and b2 are two constants such that 0 < b2 ≤b1 < 1. For the standard choices of
b1 and b2, this is a more stringent condition that forces greater progress toward satisfying
feasibility, and it may provide better algorithm performance.
Regardless of the value of πk, subproblem (18.49) is more difﬁcult to solve than a stan-
dard trust-region problem. Various techniques for ﬁnding exact or approximate solutions
have been proposed, and all are satisfactory for the case in which n is small or Ak and Wk
are dense. However, efﬁcient algorithms for this subproblem are still not established for the
large-scale case. For this reason, we will not give further details on this method.
APPROACH III: Sℓ1QP (SEQUENTIAL ℓ1 QUADRATIC PROGRAMMING)
The two approaches above were developed speciﬁcally with equality-constrained opti-
mizationinmind,anditisnottrivialtoextendthemtoinequality-constrainedproblems.The
approach to be described here, however, handles inequality constraints in a straightforward
way, so we describe it in terms of the general problem (18.11).
The SQP quadratic programming subproblem is now given by (18.12). As before, we
facethedifﬁcultythatadditionofatrust-regionboundtothismodelmaycausetheconstraint
set for this subproblem to become infeasible. To avoid this difﬁculty, the Sℓ1QP approach
moves the linearized constraints (18.12b), (18.12c) into the objective of the quadratic pro-
gram, in the form of an ℓ1 penalty term, leaving only the trust region as a constraint. This
strategy yields the following subproblem:
min
p
∇f T
k p + 1
2pT Wkp + 1
µk

i∈E
|ci(xk) + ∇ci(xk)T p|
+ 1
µk

i∈I
[ci(xk) + ∇ci(xk)T p]−
(18.51)
subject to ∥p∥∞≤k,

556
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
where we use the notation [x]− max{0, −x}. The positive scalar µk is the penalty parame-
ter, which deﬁnes the weight that we assign to constraint satisfaction relative to minimization
of the objective.
The trust region uses the ℓ∞norm because it makes the subproblem easier to solve.
Indeed, by introducing slacks and artiﬁcial variables into (18.51), we can formulate this
problem as a quadratic program and solve it with standard algorithms for quadratic pro-
gramming. These algorithms can be modiﬁed to take advantage of the special structure
in (18.51), and they should make use of so-called hot-start information. This consists of
good initial guesses for the initial point and optimal active set for (18.51), reuse of matrix
factorizations, and so on.
We use the ℓ1 merit function φ1,
φ1(x; µ)  f (x) + 1
µ

i∈E
|ci(x)| + 1
µ

i∈I
[ci(x)]−,
(18.52)
to decide on the usefulness of the step generated by solving (18.51). In fact, the subproblem
(18.51) can be viewed as an approximation to (18.52) in which we set µ  µk, replace each
constraint function ci by its linearization, and replace the “smooth part” f by a quadratic
function whose curvature term includes information from both objective and constraint
functions. The Sℓ1QP algorithm accepts the approximate solution of (18.51) as a step if the
ratio of actual to predicted reduction in φ1 is not too small. Otherwise, it decreases the trust-
region radius k and re-solves the subproblem to obtain a new candidate step. Other details
of implementation follow those of trust-region methods for unconstrained optimization.
This approach has many attractive properties. Not only does the formulation (18.51)
overcome the possible inconsistency between the linearized constraints and the trust region,
it also allows relaxation of the regularity assumption on Ak. In the subproblem (18.51),
the matrix Wk can be deﬁned as the exact Hessian of the Lagrangian or a quasi-Newton
approximation, and there is no requirement that it be positive deﬁnite. Most importantly,
when µk (equivalently, µ in (18.52)) is chosen to be sufﬁciently large, local minimizers of the
merit function φ1 normally correspond to local solutions of (18.11), so that the algorithm
has good global convergence properties.
It has been conjectured, but not fully established, that the Sℓ1QP algorithm can be
very sensitive to the choice of penalty parameter µk. As noted above, it should be chosen
small enough to ensure that local minimizers of φ1 (with µ  µk) correspond to local
solutions of (18.11). There exist simple examples for which a value of µk that is just above
the required threshold can produce an objective in (18.51) that is unbounded below, leading
to erratic behavior of the algorithm. On the other hand, as in any penalty function approach
(Chapter 17), very small values of µ will cause the constraint violation terms to “swamp” the
objective function term, possibly leading to behavior similar to unmodiﬁed SQP (at best)
and to termination at a point that is feasible but not optimal (at worst).
There is another complication with this basic Sℓ1QP approach, which is known as the
Maratos effect. In this phenomenon, steps that make good progress toward the solution of

1 8 . 8 .
T r u s t - R e g i o n S Q P M e t h o d s
557
(18.11) are rejected because they cause an increase in the merit function φ1. Left unattended,
the Maratos effect can cause the algorithm to repeatedly reject good steps, and therefore to
become extremely slow on some problems. Fortunately, the Maratos effect can be overcome
by applying the techniques described in Section 18.11. For concreteness we now describe
how one of these techniques—the second-order correction—can be applied in this context.
We ﬁrst solve (18.51) to obtain a step pk. If this step gives rise to an increase in the
merit function φ1, then this may be an indication that our linear approximations to the
constraints are not sufﬁciently accurate. To overcome this we could resolve (18.51) with the
linear terms ci(xk) + ∇ci(xk)T p replaced by quadratic approximations,
ci(xk) + ∇ci(xk)T p + 1
2pT ∇2ci(xk)p.
(18.53)
But this is not practical, even if the Hessians of the constraints are individually available,
because the subproblem becomes very hard to solve. Instead, we evaluate the constraint
values at the new point xk + pk and make use of the following approximations. Ignoring
third-order terms, we have
ci(xk + pk)  ci(xk) + ∇ci(xk)T pk + 1
2pT
k ∇2ci(x)T pk.
(18.54)
Eventhoughwedon’tknowwhatthestepp wouldbeifweusedthequadraticapproximations
(18.53) in the subproblem, we will assume that
pT ∇2ci(x)T p  pT
k ∇2ci(x)T pk.
(18.55)
Making this substitution in (18.53) and using (18.54) yields the second-order correction
subproblem
min
p
∇f T
k p + 1
2pT Wkp + 1
µk

i∈E
|di + ∇ci(xk)T p| + 1
µk

i∈I
[di + ∇ci(xk)T p]−
subject to ∥p∥∞≤k,
(18.56)
where
di  ci(xk + pk) −∇ci(xk)T pk.
This problem has a similar form to (18.51) and can be solved with the same techniques.
Since the active set at the solution of (18.56) is often the same as, or similar to, that of
the original problem (18.51), hot-start techniques can be used to solve (18.56) with little
additional effort.
The second-order correction step requires evaluation of the constraints ci(xk + pk)
for i ∈E ∪I, and therefore it is preferable not to apply it every time that the merit function
increases. Moreover, the crucial assumption (18.55) will normally not hold if the step is

558
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
large, and we need to make sure that the trust region is asymptotically large enough not to
interfere with the correction step. As a result it is necessary to use sophisticated heuristics to
maximize the beneﬁt of the second-order correction step; see Fletcher [83, Chapter 14].
18.9
A PRACTICAL TRUST-REGION SQP ALGORITHM
In the previous section we have described three practical frameworks for trust-region SQP
methods. We will now elaborate on Approach I, which is based on (18.47) and (18.48), and
give a detailed algorithm. There is a considerable amount of ﬂexibility in the computation of
the normal step v and the total step p, particularly since exact solutions of the normal sub-
problem (18.47) or the main trust-region subproblem (18.48) are not needed; approximate
solutions often sufﬁce.
Thenormalsubproblem(18.47)canbeapproximatelysolvedbytheconjugategradient
method(seeAlgorithm4.3inChapter4)orbymeansofthedoglegmethod.Thelattercannot
be implemented exactly as in Chapter 4, because the Newton step is not uniquely deﬁned:
Singularity of the Hessian in (18.47a) ensures that there are inﬁnitely many “Newton steps”
that minimize the quadratic model. If we assume that Ak has full rank and choose our
particular “Newton step” in the dogleg method to be the vector of minimum Euclidean
norm that satisﬁes Akv + ck  0, we have that
pB  −AT
k [AkAT
k ]−1ck.
As deﬁned in Chapter 4, the Cauchy point is the minimizing point for the objective (18.47a)
along the direction −AT
k c, so it, too, lies in the range space of AT
k . Since the dogleg step vk
is a linear combination of these two directions, it, too, lies in the range space of AT
k .
Let us now consider the solution of the main subproblem (18.48). An efﬁcient tech-
nique, which is suitable for small or large problems, is to use a reduced-Hessian approach.
Since the normal step vk is in the range space of AT
k , we express the total step pk as
pk  vk + Zkuk,
for some
u ∈Rn−m,
(18.57)
where Zk is a basis for the null space of AT
k . That is, we ﬁx the component of pk in the range
space of AT
k to be exactly vk, the normal-step vector, and allow no other movement in this
direction. By substituting this form of pk into (18.48a), (18.48b), we obtain the following
tangential subproblem in the reduced variable u:
min
u
mk(u)
def (∇fk + Wkvk)T Zku + 1
2uT ZT
k WkZku
(18.58a)
subject to
∥Zku∥2 ≤
1
2
k −∥vk∥2
2.
(18.58b)

1 8 . 9 .
A P r a c t i c a l T r u s t - R e g i o n S Q P A l g o r i t h m
559
x*
Z
c(x)=c(x
u
)
c(x)=0
k
p
x
v
x
x
k
k
k
k k
1
2
Figure 18.3
The step pk  vk + Zkuk of the trust-region method.
(Note that we have dropped all terms in the objective (18.58a) that are independent of u,
sincetheydonotaffectthesolution.)Thesimpleform(18.58b)ofthetrust-regionconstraint
is a consequence of the fact that the components vk and Zkuk in (18.57) are orthogonal.
We cannot treat (18.58b) as a scaled trust-region constraint, since the matrix Zk is not even
square, in general. Nevertheless, we can write (18.58b) as
uT S2
ku ≤2
k,
with S2
k ≡ZT
k Zk,
where Sk is positive deﬁnite, since Zk has linearly independent columns.
We call the solution Zkuk of (18.58) the tangent step. It cannot be computed by the
dogleg method, unless we have an assurance that the reduced Hessian ZT
k WkZk is positive
deﬁnite. But the conjugate gradient (CG) method, as given in Algorithm 4.3 of Chapter 4,
can always be applied to this subproblem. Effective ways of preconditioning the CG iteration
in this case are the subject of continuing investigation.
Figure 18.3 illustrates the normal and tangent steps for a simple problem with two
unknowns and one nonlinear equality constraint. The dashed elliptical lines represent level
curves of the objective f (x), with the minimum in the lower right part of the picture. The
equality constraint is represented as a solid line, and the broken circle is the trust-region
constraint. The tangent space of the constraints is shown as a dotted line through the point
xk (Zk has just a single column in this example), and the normal step is perpendicular to
this space. This same manifold translated to the point xk + vk deﬁnes the possible set of

560
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
tangential steps, as shown by the lower dotted line. The ﬁnal step pk in this case reaches all
the way to the trust-region constraint.
A merit function that ﬁts well with this approach is the nondifferentiable ℓ2 function
φ(x, µ)  f (x) + 1
µ∥c(x)∥2.
(18.59)
(Note that the ℓ2 norm is not squared). The use of the ℓ2 norm is consistent here and in the
normal subproblem, but other merit functions can also be used. The actual reduction in this
function at the candidate step pk is deﬁned as
ared  φ(xk, µk) −φ(xk + pk, µk),
while the predicted reduction is
pred  mk(0) −mk(u) + µ−1
k vpred,
where mk is deﬁned in (18.58a), and vpred is the reduction in the model (18.47a) obtained
by the normal step vk, and is given by
vpred  ∥c(xk)∥−∥c(xk) + Akvk∥.
We require that µk be small enough that pred be positive and proportional to vpred, i.e.,
pred ≥ρµ−1
k vpred,
(18.60)
where 0 < ρ < 1 (for example, ρ  0.3). We can enforce this inequality by choosing the
penalty parameter µk such that
µ−1
k
≥mk(0) −mk(u)
(1 −ρ)vpred .
(18.61)
Wecannowgiveaprecisedescriptionofthistrust-regionSQPmethodfortheequality-
constrained optimization problem (18.1).
Algorithm 18.6 (Trust-Region SQP).
Choose constants ϵ > 0 and η, ζ, γ ∈(0, 1);
Choose starting point x0, initial trust region 0 > 0;
for k  0, 1, 2, . . .
Compute fk, ck, ∇fk, Ak;
Compute multiplier estimates ˆλk by (18.16);
if ∥∇fk −AT
k ˆλk∥∞< ϵ and ∥ck∥∞< ϵ
STOP with approximate solution xk;

1 8 . 1 0 .
R a t e o f C o n v e r g e n c e
561
Solve the normal subproblem (18.47) for vk;
Compute matrix Zk whose columns span the null space of Ak;
Compute or update Wk;
Solve (18.58) for uk;
Set pk  vk + Zkuk;
Compute ρk  ared/pred;
if ρk > η
Set xk+1  xk + pk;
Choose k+1 to satisfy k+1 ≥k;
else
Set xk+1  xk;
Choose k+1 to satisfy k+1 ≤γ ∥pk∥;
end (for).
The constant ζ that deﬁnes the smaller trust region for the normal problem (18.47) is
commonly chosen to be 0.8, but this value is not critical to the performance of the method.
Algorithm 18.6 can be extended to problems with inequality constraints in two differ-
ent ways. One is by using an active-set-type approach, and the other is in an interior-point
framework, where an adaptation of Algorithm 18.6 is used to solve barrier problems; see
the Notes and References. As in any null-space method, the drawback of deﬁning the step
pk in terms of two components vk and Zku is the need to compute the null-space basis Zk.
Nevertheless, one can rearrange the CG iteration to bypass the computation of Z altogether
(see the exercises).
Unfortunately, the merit function (18.59) suffers from the Maratos effect, and it is
therefore appropriate to include in Algorithm 18.6 the watchdog technique or the second-
order correction term (18.67) described in Section 18.11. If we use the latter, it is not efﬁcient
to apply this correction after every step that increases the merit function, as in the Sℓ1QP
method. A heuristic that has proved to be effective in practice is to apply the second-order
correction only if the normal step is signiﬁcantly smaller than both the tangential step and
the trust-region radius.
18.10
RATE OF CONVERGENCE
We now derive conditions that guarantee the local convergence of SQP methods, as well
as conditions that ensure a superlinear rate of convergence. For simplicity, we limit our
discussion to Algorithm 18.1 given in Section 18.1 for equality-constrained optimization,
butwewillconsiderbothexactHessianandquasi-Newtonversionsofit.Theresultspresented
herecanbeappliedtoalgorithmsforinequality-constrainedproblems,oncetheactivesethas
settled (see Theorem 18.1), and they can also be invoked in studying global algorithms, since
in the vicinity of a solution that satisﬁes the assumptions below, the globalization strategies

562
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
normally do not interfere with the good convergence behavior of the local algorithm. (An
exception to this comment is provided by the Maratos effect, which we discuss in the next
section.)
We begin by listing a set of assumptions on the problem and the quasi-Newton ap-
proximations Bk that will be useful in this section. Each of the results below makes use of
one or both of these conditions.
Assumption 18.2.
(a) At the solution point x∗with optimal Lagrange multipliers λ∗, the constraint Jacobian A∗
has full row rank, and the Hessian of the Lagrangian ∇2
xxL(x∗, λ∗) is positive deﬁnite on
the tangent space of the constraints.
(b) The sequences {Bk} and {B−1
k } are bounded, that is, there exists a constant β2 such that
∥Bk∥≤β2
∥B−1
k ∥≤β2,
for all k.
We ﬁrst consider a Newton SQP method that uses exact second derivatives.
Theorem 18.4.
Suppose that Assumption 18.2(a) holds and that f and c are twice differentiable, with
Lipschitz continuous second derivatives, in a neighborhood of (x∗, λ∗). Then if x0 and λ0 are
sufﬁciently close to x∗and λ∗, the pairs (xk, λk) generated by Algorithm 18.1 with Wk deﬁned
as the Hessian of the Lagrangian converge quadratically to (x∗, λ∗).
The proof follows directly from Theorem 11.2.
We turn now to quasi-Newton variants of Algorithm 18.1, in which the augmented
Lagrangian Hessian Wk  ∇2
xxL(xk, λk) is replaced by a quasi-Newton approximation Bk.
We will make use of the projection matrix Pk deﬁned for each iterate k by
Pk  I −AT
k

AkAT
k
−1 Ak  ZkZT
k ,
where Zk has orthonormal columns. This matrix projects any vector in IRn into the null
space of the constraint gradients.
The projected Hessian PkWk plays a crucial role in the analysis. To motivate this, we
multiply the ﬁrst equation of the KKT system (18.10) by Pk, and use the fact that PkAT
k  0
to obtain
PkWkpk  −Pk∇fk.
This equation shows that the step is completely determined by the one-sided projection
of Wk and is independent of the rest of Wk. It also suggests that a quasi-Newton method
will be locally convergent if PkBk is a reasonable approximation of PkWk, and that it will

1 8 . 1 0 .
R a t e o f C o n v e r g e n c e
563
be superlinearly convergent if PkBk approximates PkWk accurately. To make the second
statement more precise, we present a result that quantiﬁes the quality of this approximation
in a way that can be viewed as an extension of the Dennis and Mor´e characterization (3.5)
of superlinear convergence from the unconstrained case to the equality-constrained case.
Theorem 18.5 (Boggs, Tolle, and Wang [24]).
Suppose that Assumption 18.2(a) holds and that the iterates xk generated by Algo-
rithm 18.1 with quasi-Newton approximate Hessians Bk converge to x∗. Then xk converges
superlinearly if and only if the Hessian approximation Bk satisﬁes
lim
k→∞
∥Pk(Bk −W∗)(xk+1 −xk)∥
∥xk+1 −xk∥
 0.
(18.62)
Let us apply these results to the quasi-Newton updating schemes discussed earlier in
this chapter. We begin with the full BFGS approximation based on (18.20). To guarantee that
it is always well-deﬁned we make the (strong) assumption that the Hessian of the Lagrangian
is positive deﬁnite at the solution.
Theorem 18.6.
Suppose that W∗and B0 are symmetric and positive deﬁnite, and that Assump-
tions 18.2(a), (b) hold. If ∥x0 −x∗∥and ∥B0 −W∗∥are sufﬁciently small, then the iterates
xk generated by Algorithm 18.1 with BFGS Hessian approximations Bk deﬁned by (18.20) and
(18.23) (with rk  sk) satisfy the limit (18.62). Therefore, the iterates xk converge superlinearly
to x∗.
For the damped BFGS updating strategy given in Procedure 18.2, a weaker result has
been obtained. If we assume that the iterates xk converge to a solution point x∗, we can show
that the rate of convergence is R-superlinear (not the usual Q-superlinear; see Chapter 2).
Similar results can be obtained for the updating strategies based on the Hessian of the
augmented Lagrangian (18.25).
CONVERGENCE RATE OF REDUCED-HESSIAN METHODS
Reduced-Hessian SQP methods update an approximation Mk to ZT
k WkZk. From the
deﬁnition of Pk we see that ZkMkZT
k can be considered as an approximation to the two-
sided projection PkWkPk. Therefore, since reduced-Hessian methods do not approximate
the one-sided projection PkWk, we cannot expect (18.62) to hold. For these methods we can
derive a condition for superlinear convergence by writing (18.62) as
lim
k→∞
Pk(Bk −W∗)Pk(xk+1 −xk)
∥xk+1 −xk∥
(18.63)
+Pk(Bk −W∗)(I −Pk)(xk+1 −xk)
∥xk+1 −xk∥

 0.

564
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
To use this limit, we deﬁne the n × n matrix Bk  ZkMkZT
k . The following result shows
that it is necessary only for the ﬁrst term in (18.63) to go to zero to obtain a weaker form of
superlinear convergence, namely, two-step superlinear convergence.
Theorem 18.7.
Suppose that Assumption 18.2(a) holds, and that the matrices Bk are bounded. Assume
also that the iterates xk generated by Algorithm 18.1 with approximate Hessians Bk converge to
x∗, and that
lim
k→∞
∥Pk(Bk −W∗)Pk(xk+1 −xk)∥
∥xk+1 −xk∥
 0.
(18.64)
Then the sequence {xk} converges to x∗two-step superlinearly, i.e.,
lim
k→∞
∥xk+2 −x∗∥
∥xk −x∗∥
 0.
We have mentioned earlier that reduced-Hessian methods often exhibit tangential
convergence, meaning that the normal component of the step is much smaller than the
tangential component; see (18.41). In other words, the ratio
∥(I −Pk)(xk+1 −xk)∥
∥xk+1 −xk∥
tends to zero. Then it follows from (18.63) and (18.64) that the rate of convergence is actually
one-step superlinear. This rate will occur even when the term Pk(Bk −W∗)(I −Pk) is not
small, as is the case when we ignore this cross term (see (18.18)).
Let us consider a local reduced-Hessian method using BFGS updating. The iteration
is xk+1  xk + YkpY + ZkpZ, where pY and pZ are given by (18.27). The reduced-Hessian
approximation Mk is updated by the BFGS formula using the correction vectors (18.30), and
the initial approximation M0 is symmetric and positive deﬁnite. If we make the assumption
that the null space bases Zk used to deﬁne the correction vectors (18.30) vary smoothly, i.e.,
∥Zk −Z∗∥ O(∥xk −x∗∥),
(18.65)
then we can establish the following result that makes use of Theorem (18.7).
Theorem 18.8.
Suppose that Assumption 18.2(a) holds. Let xk be generated by the reduced-Hessian
method just described. If the sequence {xk} converges to x∗R-linearly, then {Mk} and {M−1
k }
are uniformly bounded and xk converges two-step superlinearly.
It is necessary to make use of the smoothness assumption (18.65) on Zk for the
following reason. There are many null-space bases for the matrix of constraint gradients Ak

1 8 . 1 1 .
T h e M a r a t o s E f f e c t
565
(Chapter 15). If the choice of this null-space basis changes too much from one iterate to
the next, superlinear convergence will be impeded because the quasi-Newton update will
be based on correction vectors sk and yk that exhibit jumps. Indeed, in many cases, any
procedure for computing Zk as a function of Ak alone will have discontinuities, even for
full-rank Ak. Before computing a basis Zk we need to take into consideration the choice
made during the previous iteration.
Two procedures have been proposed to cope with this difﬁculty. One is to obtain Zk
by computing a QR factorization of Ak (see (A.54)) in which the inherent arbitrary choices
in the factorization algorithm are made, near the solution, in the same way as in computing
Zk−1 from Ak−1. As a result, when the matrices Ak are close to A∗, the same choice of sign
will be made at each step. Another procedure consists in applying the orthogonal factor of
the QR factorization of Ak−1 to Ak, and then computing the QR factorization of QT
k−1Ak to
obtain Qk, and thus Zk. One can show that either of these two procedures satisﬁes (18.65).
18.11
THE MARATOS EFFECT
We noted in Section 18.9 that some merit functions can impede the rapid convergence
behavior of SQP methods by rejecting steps that make good progress toward a solution. This
undesirable phenomenon is often called the Maratos effect, because it was ﬁrst observed by
Maratos [156]. It is illustrated by the following example, in which the SQP steps pk give rise
to a quadratic convergence rate but cause an increase both in the objective function value
and the constraint norm. As a result, these steps will be rejected by many merit functions.
❏Example 18.1
(Powell [209])
Consider the problem
min f (x1, x2)  2(x2
1 + x2
2 −1) −x1,
subject to
x2
1 + x2
2 −1  0.
It is easy to verify (see Figure 18.4) that the optimal solution is x∗ (1, 0)T , that the
corresponding Lagrange multiplier is λ∗ 3
2, and that ∇2
xxL(x∗, λ∗)  I.
Let us consider an iterate xk of the form xk  (cos θ, sin θ)T , which is feasible for any
value of θ. We now generate a search direction pk by solving the subproblem (18.8) with
Bk  ∇2
xxL(x∗, λ∗)  I. Since
f (xk)  −cos θ,
∇f (xk) 
#
4 cos θ −1
4 sin θ
$
,
A(xk)T 
#
2 cos θ
2 sin θ
$
,

566
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
contours of f
x  +x  =1
2
2
1
2
(0)
x
(1)
x
x*
Figure 18.4
The Maratos effect: Example 18.1.
the quadratic subproblem (18.8) takes the form
min −cos θ + (4 cos θ −1)p1 + 4 sin θp2 + 1
2p2
1 + 1
2p2
2
subject to p2  −cot θp1.
By solving this subproblem, we obtain
pk 
#
sin2 θ
−sin θ cos θ
$
,
(18.66)
which yields a new trial point
xk + pk 
#
cos θ + sin2 θ
sin θ(1 −cos θ)
$
.
If sin θ ̸ 0, we have that
∥xk + pk −x∗∥2  2 sin2(θ/2),
∥xk −x∗∥2  2| sin(θ/2)|,

1 8 . 1 1 .
T h e M a r a t o s E f f e c t
567
and therefore
∥xk + pk −x∗∥2
∥xk −x∗∥2
2
 1
2.
Hence, this step approaches the solution at a rate consistent with Q-quadratic convergence.
Note, however, that
f (xk + pk)  sin2 θ −cos θ > −cos θ  f (xk),
c(xk + pk)  sin2 θ > c(xk)  0,
so that both the objective function value and constraint violation increase over this step.
In Figure 18.4 we illustrate the case where θ  π/2 and the SQP method moves from
x(0)  (1, 0) to x(1)  (1, 1). We have chosen a large value of θ for clarity, but note that we
have shown that the step will be rejected for any nonzero value of θ.
❐
This example shows that any merit function of the form
(x; µ)  f (x) + 1
µh(c(x))
(where h(·) is a nonnegative function satisfying h(0)  0) will reject the step (18.66), so
that any algorithm based on a merit function of this type will suffer from the Maratos effect.
Examples of such merit functions include the smooth ℓ2 merit function f (x)+µ−1∥c(x)∥2
2
and the nondifferentiable ℓ1 function f (x) + µ−1∥c(x)∥1.
If no measures are taken, the Maratos effect can dramatically slow down SQP methods.
Not only does it interfere with good steps away from the solution, but it can also prevent su-
perlinear convergence from taking place. Techniques for avoiding the Maratos effect include
the following.
• We can use a merit function that does not suffer from the Maratos effect. An example
is Fletcher’s augmented Lagrangian function (18.36), for which one can show that
steps generated by Algorithm 18.1 will be accepted in a neighborhood of a solution
point that satisﬁes the second-order sufﬁcient conditions.
• We can use a second-order correction in which we add to pk a step p′
k, which is
computed at c(xk +pk) and which provides sufﬁcient decrease in the constraints. See,
for example, (18.56), which is the second-order correction subproblem for the Sℓ1QP
method.
• We may allow the merit function  to increase on certain iterations, that is, we can
use a nonmonotone strategy. An example is the watchdog technique discussed below.
We now discuss the last two approaches.

568
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
SECOND-ORDER CORRECTION
By adding a correction term that provides further decrease in the constraints, the
SQP iteration overcomes the difﬁculties associated with the Maratos effect. We have already
described this second-order correction technique in Section 18.8 in connection with the
Sℓ1QP algorithm, which is a trust-region algorithm based on the nondifferentiable ℓ1 merit
function. Here, we outline the way in which the same idea can be applied to a line search
algorithm, also based on the ℓ1 merit function.
Given an SQP step pk, the second-order correction step wk is deﬁned to be
wk  −AT
k (AkAT
k )−1c(xk + pk).
(18.67)
Note that wk has the property that is satisﬁes a linearization of the constraints c at the point
xk + pk, that is,
Akwk + c(xk + pk)  0.
In fact, wk is the minimum-norm solution of this equation. The motivation is similar to
that used in (18.53) and (18.54), which gives the corresponding strategy for the trust-region
Sℓ1QP algorithm. The effect of the correction step wk, which is normal to the constraints, is
to decrease the quantity ∥c(x)∥to the order of ∥xk −x∗∥3. By doing so, we guarantee that
the step from xk to xk + pk + wk will decrease the merit function, at least near the solution.
The price we pay is an additional evaluation of the constraint function c at xk + pk.
Following is a detailed implementation of the second-order correction step in a line
search SQP method in which the Hessian approximation is updated by a quasi-Newton
formula. As before, we denote the directional derivative of the merit function φ1 in the
direction pk by D(φ1(xk; µ); pk). It is understood that the penalty parameter µ is held ﬁxed
until a successful step is computed.
Algorithm 18.7 (SQP Algorithm with Second-Order Correction).
Choose parameters η ∈(0, 0.5) and τ1, τ2 with 0 < τ1 < τ2 < 1;
Choose initial point x0 and approximate Hessian B0;
for
k  0, 1, 2, . . .
Evaluate fk, ∇fk, ck, Ak;
if termination test is satisﬁed
STOP;
Compute the SQP step pk;
Set αk ←1, newpoint ←false ;
while not newpoint
if φ1(xk + αkpk) ≤φ1(xk) + ηαkDφ1(xk; pk)
Set xk+1 ←xk + αkpk;
Set newpoint ←true;
else if αk  1

1 8 . 1 1 .
T h e M a r a t o s E f f e c t
569
Compute wk from (18.67);
if φ1(xk + pk + wk) ≤φ1(xk) + ηDφ1(xk; pk)
Set xk+1 ←xk + pk + wk;
Set newpoint ←true;
end (if)
Choose new αk in [τ1αk, τ2αk];
else
Choose new αk in [τ1αk, τ2αk];
end (if)
end (while)
Update Bk using a quasi-Newton formula to obtain Bk+1;
end (for)
One can show that after a ﬁnite number of iterations of Algorithm 18.7, the value
αk  1 always produces a new iterate xk+1, which can have the form either xk+1  xk + pk
or xk+1  xk + pk + wk. Therefore, the merit function does not interfere with the iteration,
and superlinear convergence is obtained, as for the local algorithm.
Note that the step wk can be deﬁned more generally in terms of any matrix Yk whose
columns are a basis for the subspace spanned by the columns of Ak.
The second-order correction strategy is effective in practice. The additional cost of
performing the extra constraint function evaluation in (18.67) is outweighed by added
robustness and efﬁciency.
WATCHDOG (NONMONOTONE) STRATEGY
The inefﬁciencies caused by the Maratos effect can also be avoided by occasionally
accepting steps that increase the merit function. (Such steps are called “relaxed steps.”)
There is a limit to our liberality, however: If a sufﬁcient reduction of the merit function has
not been obtained within a certain number of iterates of the relaxed step (ˆt iterates, say),
then we return to the iterate before the relaxed step and perform a normal step, using a line
search or some other technique to force a reduction in the ℓ1 merit function.
Our hope is, of course, that the relaxed step is a good step in the sense of making
progress toward the solution, even though it increases the merit function. The step taken
immediately after the relaxed step serves a similar purpose to the second-order correction
step above; that is, it corrects the SQP step for its not taking sufﬁcient account of curvature
information for the constraint functions in its formulation of the SQP subproblem.
We now describe a particular instance of this technique, which is often called the
watchdog strategy. We set ˆt  1, so that we allow the merit function to increase on just a
single step before insisting on a sufﬁcient decrease of some type. As above, we focus our
discussion on a line search SQP algorithm that uses the ℓ1 merit function. We assume that
the penalty parameter µk is not changed until a successful cycle has been completed. For
simplicity we omit the details of the calculation of step length αk and the termination test.

570
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
Algorithm 18.8 (Watchdog).
Choose constant η ∈(0, 0.5);
Choose initial point x0 and approximate Hessian B0;
Set k ←0, S ←{0};
repeat
Evaluate fk, ∇fk, ck, Ak;
if termination test satisﬁed
STOP with approximate solution xk;
Compute the SQP step pk;
Set xk+1 ←xk + pk;
Update Bk using a quasi-Newton formula to obtain Bk+1;
if φ1(xk+1) ≤φ1(xk) + ηDφ1(xk; pk)
k ←k + 1;
S ←S ∪{k};
else
Compute the SQP step pk+1;
Find αk+1 such that
φ1(xk+2) ≤φ1(xk+1) + ηαk+1Dφ1(xk+1; pk+1);
Set xk+2 ←xk+1 + αk+1pk+1;
Update Bk+1 using a quasi-Newton formula to obtain Bk+2;
if φ1(xk+1) ≤φ(xk) or φ1(xk+2) ≤φ1(xk) + ηDφ1(xk; pk)
k ←k + 2;
S ←S ∪{k};
else if φ1(xk+2) > φ1(xk)
Find αk such that φ1(xk+3) ≤φ1(xk) + ηαkDφ1(xk; pk);
Compute xk+3  xk + αkpk;
else
Compute the SQP step pk+2;
Find αk+2 such that
φ1(xk+3) ≤φ1(xk+2) + ηαk+2Dφ1(xk+2; pk+2);
Set xk+3 ←xk+2 + αk+2pk+2;
Update Bk+2 using a quasi-Newton formula to obtain Bk+3;
k ←k + 3;
S ←S ∪{k};
end (if)
end (if)
end (repeat)
Aquasi-Newtonupdateatxk+1 isalwaysperformedusinginformationfromtheimme-
diately preceding step xk+1 −xk. The set S is not required by the algorithm and is introduced
only to identify the iterates for which a sufﬁcient merit function reduction was obtained.
Note that at least one third of the iterates have their indices in S. By using this fact, one can

1 8 . 1 1 .
T h e M a r a t o s E f f e c t
571
show that the SQP method using the watchdog technique is locally convergent, that for all
sufﬁciently large k the step length is αk  1, and that the rate of convergence is superlinear.
In practice, it may be advantageous to allow increases in the merit function for more
than 1 iteration. Values of ˆt such as 5 or 10 are typical. For simplicity, we have assumed that
the matrix is updated at each iterate along the direction moved to reach that iterate, even
though in practice it may be preferable not to do so at certain iterates that will be rejected.
As this discussion indicates, careful implementations of the watchdog technique have
a certain degree of complexity, but the added complexity is worthwhile because the approach
has shown good practical performance. A potential advantage of the watchdog technique
over the second-order correction strategy is that it may require fewer evaluations of the
constraints. Indeed, in the best case, most of the steps will be full SQP steps, and there
will rarely be a need to return to an earlier point. There is insufﬁcient numerical experience,
however, to allow us to make broad claims about the relative merits of watchdog and second-
order correction strategies.
NOTES AND REFERENCES
SQP methods were ﬁrst proposed in 1963 by Wilson [245] and were developed in the
1970s (Garcia-Palomares and Mangasarian [97], Han [131, 132], Powell [202, 205, 204]. See
Boggs and Tolle [23] for a literature survey. The successive linear programming approach
outlined at the end of Section 18.1 is described by Fletcher and Sainz de la Maza [89]. The
relation between SQP and augmented Lagrangian methods is explored by Tapia [234]
Another method that depends on x alone can be derived by applying Newton’s method
to the reduced form of the ﬁrst-order KKT equations,
F(x) ≡

Z(x)T g(x)
c(x)

 0.
(18.68)
ThissystemofnnonlinearequationsinnunknownsdoesnotinvolveanyLagrangemultiplier
estimates and is an excellent example of a primal method for nonlinear optimization. The
Jacobian of this system requires the computation of the derivative Z′(x), which is not directly
available. However, when we drop terms that involve Z′, we obtain a scheme that is related
to SQP methods (Goodman [117]).
A procedure for coping with inconsistent linearized constraints that is different from
the one presented in Section 18.3 is described by Powell [202].
Reduced-Hessian SQP methods were perhaps ﬁrst proposed by Murray and Wright
and Coleman and Conn; see [23] for a literature survey. Since then, many papers have been
written on the subject, and the algorithms have proved to be useful in applications such as
chemical process control and trajectory optimization.
Some analysis shows that several—but not all—of the good properties of BFGS updat-
ing are preserved by damped BFGS updating. Numerical experiments exposing the weakness
of the approach are reported by Powell [208].

572
C h a p t e r
1 8 .
S e q u e n t i a l Q u a d r a t i c P r o g r a m m i n g
Most SQP methods are normally infeasible methods, meaning that neither the initial
point nor any of the subsequent iterates need to be feasible. This can be advantageous when
the problem contains signiﬁcantly nonlinear constraints, because it can be computationally
expensive to stay inside the feasible region. In some applications, however, the functions
that make up the problem are not always deﬁned outside of the feasible region. To cope with
this phenomenon, feasible SQP methods have been developed; see, for example, Panier and
Tits [189].
Many SQP codes treat linear constraints differently from nonlinear constraints. There
is a ﬁrst phase in which the iterates become feasible with respect to all linear constraints, and
all subsequent iterates remain feasible with respect to them. This is the case in NPSOL [111]
and SNOPT [108]. To deal with inconsistent or nearly dependent constraints, these codes
make use of a ﬂexible mode in which the constraints of the problem are relaxed.
The interior-point method for nonlinear programming described by Byrd, Hribar,
and Nocedal [34] uses Algorithm 18.6 to solve the equality-constrained subproblems arising
in barrier methods.
Second-ordercorrectionstrategieswereproposedbyColemanandConn[44],Fletcher
[82], Gabay [96], and Mayne and Polak [161]. The watchdog technique was proposed by
Chamberlain et al. [40].
Procedures for computing a smoothly varying sequence of null-space matrices Zk are
described by Coleman and Sorensen [50] and Gill et al. [109].
More realistic convergence results have been proved for reduced-Hessian quasi-
Newton methods; see Byrd and Nocedal [36].
✐
E x e r c i s e s
✐
18.1 Prove Theorem 18.4.
✐
18.2 Write a program that implements Algorithm 18.1. Use it to solve the problem
min ex1x2x3x4x5 −1
2(x3
1 + x3
2 + 1)2
subject to x2
1 + x2
2 + x2
3 + x2
4 + x2
5 −10  0,
x2x3 −5x4x5  0,
x3
1 + x3
2 + 1  0.
Use the starting point x0  (−1.71, 1.59, 1.82, −0.763, −0.763). The solution is x∗
(−1.8, 1.7, 1.9, −0.8, −0.8).
✐
18.3 Show that the damped BFGS updating satisﬁes (18.24).
✐
18.4 Suppose that x∗is a solution of problem (18.1) where the second-order sufﬁ-
ciency conditions hold. Show that yT
k sk > 0 in a neighborhood of this point when yk and sk

1 8 . 1 1 .
T h e M a r a t o s E f f e c t
573
are deﬁned by (18.30a), (18.32). What assumption do you need to make on the multiplier
estimates?
✐
18.5 Prove (18.38).
✐
18.6 Consider the constraint x2
1 + x2
2  1. Write the linearized constraints (18.8b) at
the following points: (0, 0), (0, 1), (0.1, 0.02), −(0.1, 0.02).
✐
18.7 Write a program that implements the reduced-Hessian method (EQ) and use it
to solve the problem given in Exercise 3.
✐
18.8 Show that the normal problem (18.47a)–(18.47b) always has a solution vk lying
in the range space of AT
k . Hint: First show that if the trust-region constraint (18.47b) is
active, then vk lies in the range space of AT
k ; then show that if the trust region is inactive,
then the minimum-norm solution of (18.47a) lies in the range space of AT
k .
✐
18.9 Rewrite (18.51) as a quadratic program.
✐
18.10 Let us consider the solution of the tangential problem (18.58a)–(18.58b) by
means of the conjugate gradient method. We perform the change of variables ˜u  Zku that
transforms (18.58b) into a spherical trust region. Applying CG to the transformed problem
is nothing but a preconditioned CG iteration with preconditioner [ZT
k Zk]−1. Noting that
the solution u of this problem needs to be multiplied by Zk to obtain the tangent step Zkuk,
we see that it is convenient to perform this multiplication by Zk directly in the CG iteration.
Show that by doing this the null-space basis Zk enters in the CG iteration only through
products of the form
Zk

ZT
k Zk
−1 ZT
k w,
where w is a vector. Use this to show that this multiplication can be performed as
AT
k

AkAT
k
−1 Akw.
(18.69)
(This procedure shows that the Algorithm 18.6 can be implemented without computing
Zk.) Finally, show that the computation (18.69) can also be performed by solving

I
AT
k
Ak
0

.

Appendix A

Background
Material
A.1
ELEMENTS OF ANALYSIS, GEOMETRY, TOPOLOGY
TOPOLOGY OF THE EUCLIDEAN SPACE IRn
Let F be a subset of IRn, and suppose that {xk} is a sequence of points belonging to F.
We say that a sequence {xk} converges to some point x, written limk→∞xk  x, if for any
ϵ > 0, there is an index K such that
∥xk −x∥≤ϵ,
for all k ≥K.
For example, the sequence {xk} deﬁned by xk  (1−2−k, 1/k2)T converges to (1, 0)T . Other
examples of convergent sequences are given in Chapter 2, where the rate of convergence is
also discussed.
We say that ˆx ∈IRn is an accumulation point or limit point for {xk} if there is some
inﬁnite subsequence of indices k1, k2, k3, . . . such that
lim
i→∞xki  ˆx.

576
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
Another way to deﬁne a limit point is to say that for any ϵ > 0 and all positive integers K,
we have
∥xk −x∥≤ϵ,
for some k ≥K.
An example is given by the sequence

1
1

,

1/2
1/2

,

1
1

,

1/4
1/4

,

1
1

,

1/8
1/8

, . . . ,
(A.1)
which has exactly two limit points: ˆx  (0, 0)T and ˆx  (1, 1)T . A sequence can even have
an inﬁnite number of limit points. An example is the sequence xk  sin k, for which every
point in the interval [−1, 1] is a limit point.
If {tk} is a sequence of real numbers, we can deﬁne two other concepts: the lim inf and
lim sup. The lim inf is the smallest accumulation point, while the lim sup is the largest. That
is, we write ˆt  lim supk→∞tk if the following two conditions hold:
(i) there is an inﬁnite subsequence k1, k2, . . . such that limi→∞tki  ˆt, and
(ii) there is no other limit point t such that t > ˆt.
The sequence 1, 1
2, 1, 1
4, 1, 1
8, . . . has a lim inf of 0 and a lim sup of 1.
The set F is bounded if there is some real number M > 0 such that
∥x∥≤M,
for all x ∈F.
A subset F ⊂IRn is open if for every x ∈F, we can ﬁnd a positive number ϵ > 0 such
that the ball of radius ϵ around x is contained in F; that is,
{y ∈IRn | ∥y −x∥≤ϵ} ⊂F.
The set F is closed if for all possible sequences of points {xk} in F, all limit points of {xk}
are elements of F. For instance, the set F  (0, 1) ∪(2, 10) is an open subset of IR, while
F  [0, 1] ∪[2, 5] is a closed subset of IR. The set F  (0, 1] is a subset of IR that is neither
open nor closed.
The interior of a set F, denoted by intF, is the largest open set contained in F. The
closure of F, denoted by clF, is the smallest closed set containing F. In other words, we have
x ∈clF
if limk→∞xk  x for some sequence {xk} of points in F.
If F  (−1, 1] ∪[2, 4), then
clF  [−1, 1] ∪[2, 4],
intF  (−1, 1) ∪(2, 4).

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
577
Note that if F is open, then intF  F, while if F is closed, then clF  F.
The set F is compact if every sequence {xk} of points in F has at least one limit point,
and all such limit points are in F. (This deﬁnition is equivalent to the more formal one
involving covers of F.) The following is a central result in topology:
F ∈IRn is closed and bounded ⇒F is compact.
Given a point x ∈IRn, we call N ∈IRn a neighborhood of x if it is an open set containing
x. An especially useful neighborhood is the open ball of radius ϵ around x, which is denoted
by IB(x, ϵ); that is,
IB(x, ϵ)  {y | ∥y −x∥< ϵ}.
A cone is a set F with the property that for all x ∈F we have
x ∈F ⇒αx ∈F, for all α ≥0.
(A.2)
For instance, the set F ⊂IR2 deﬁned by
{(x1, x2) | x1 > 0, x2 ≥0}
is a cone in IR2.
Finally, we deﬁne the afﬁne hull and relative interior of a set. Given F ⊂IRn, the afﬁne
hull is the smallest afﬁne set containing F, that is,
affF  {x | x is a linear combination of vectors in F}.
(A.3)
Forinstance,whenF isthe“ice-creamcone”deﬁnedasbelowby(A.26),wehaveaffF  IR3,
while if F is the set of two isolated points F  {(1, 0, 0), (0, 2, 0)}, we have
affF  {(x1, x2, 0) | for all x1 and x2}.
The relative interior riF of the set S is its interior relative to affF. If x ∈F, then
x ∈riF if there is an ϵ > 0 such that
(x + ϵB) ∩affF ⊂F.
Referring again to the ice-cream cone (A.26), we have that
riF 

x ∈IR3
 x3 > 2
1
x2
1 + x2
2

.

578
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
For the set of two isolated points F  {(1, 0, 0), (0, 2, 0)}, we have riF  ∅. For the set F
deﬁned by
F
def {x ∈IR3 | x1 ∈[0, 1], x2 ∈[0, 1], x3  0},
we have that
affF  IR × IR × {0},
riF  {x ∈IR3 | x1 ∈(0, 1), x2 ∈(0, 1), x3  0}.
CONTINUITY AND LIMITS
Let f be a function that maps some domain D ⊂IRn to the space IRm. For some point
x0 ∈clD, we write
lim
x→x0 f (x)  f0
(A.4)
(spoken “the limit of f (x) as x approaches x0 is f0”) if for all ϵ > 0, there is a value δ > 0
such that
∥x −x0∥< δ and x ∈D
⇒
∥f (x) −f0∥< ϵ.
We say that f is continuous at x0 if x0 ∈D and the expression (A.4) holds with f0  f (x0).
We say that f is continuous on its domain D if f is continuous for all x0 ∈D.
An example is provided by the function
f (x) 

−x
if x ∈[−1, 1], x ̸ 0,
5
for all other x ∈[−10, 10].
(A.5)
This function is deﬁned on the domain [−10, 10] and is continuous at all points of the
domain except the points x  0, x  1, and x  −1. At x  0, the expression (A.4) holds
with f0  0, but the function is not continuous at this point because f0 ̸ f (0)  5. At
x  −1, the limit (A.4) is not deﬁned, because the function values in the neighborhood of
this point are close to both 5 and −1, depending on whether x is slightly smaller or slightly
larger than −1. Hence, the function is certainly not continuous at this point. The same
comments apply to the point x  1.
In the special case of n  1 (that is, the argument of f is a real scalar), we can also
deﬁne the one-sided limit. Given x0 ∈clD, We write
lim
x↓x0 f (x)  f0
(A.6)

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
579
(spoken “the limit of f (x) as x approaches x0 from above is f0”) if for all ϵ > 0, there is a
value δ > 0 such that
x0 < x < x0 + δ and x ∈D
⇒
∥f (x) −f0∥< ϵ.
Similarly, we write
lim
x↑x0 f (x)  f0
(A.7)
(spoken “the limit of f (x) as x approaches x0 from below is f0”) if for all ϵ > 0, there is a
value δ > 0 such that
x0 −δ < x < x0 and x ∈D
⇒
∥f (x) −f0∥< ϵ.
For the function deﬁned in (A.5), we have that
lim
x↓1 f (x)  5,
lim
x↑1 f (x)  1.
The function f is said to be Lipschitz continuous if there is a constant M > 0 such that
for any two points x0, x1 in D, we have
∥f (x1) −f (x0)∥≤M∥x1 −x0∥.
(A.8)
The function f is locally Lipschitz continuous at a point x0 ∈intD if there is some neigh-
borhood N with x0 ∈N ⊂D such that the property (A.8) holds for all x0 and x1 in
N.
DERIVATIVES
Let φ : IR →IR be a real-valued function of a real variable (sometimes known as a
univariate function). The ﬁrst derivative φ′(α) is deﬁned by
dφ
dα  φ′(α)
def lim
ϵ→0
φ(α + ϵ) −φ(α)
ϵ
.
(A.9)
The second derivative is obtained by substituting φ by φ′ in this same formula; that is,
d2φ
dα2  φ′′(α)
def lim
ϵ→0
φ′(α + ϵ) −φ′(α)
ϵ
.
(A.10)
(We assume implicitly that φ is smooth enough that these limits exist.) Suppose now that α
in turn depends on another quantity β (we denote this dependence by writing α  α(β)).

580
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
We can use the chain rule to calculate the derivative of φ with respect to β:
dφ(α(β))
dβ
 dφ
dα
dα
dβ  φ′(α)α′(β).
(A.11)
Consider now the function f : IRn →IR, which is a real-valued function of n
independent variables. We typically gather the variables into a vector x, which we can
write componentwise as x  (x1, x2, . . . , xn). The n-vector of ﬁrst derivatives of f —the
gradient—is deﬁned as
∇f (x) 


∂f
∂x1
...
∂f
∂xn


.
(A.12)
(The notation “∇” is used frequently in the optimization literature to denote the ﬁrst deriva-
tive. In cases of ambiguity, a subscript such as “x” or “t” is added to ∇to indicate the variable
withrespecttowhichthedifferentiationtakesplace.)Eachpartialderivative∂f/∂xi measures
the sensitivity of the function to just one of the components of x; that is,
∂f /∂xi
def lim
ϵ→0
f (x1, . . . , xi−1, xi + ϵ, xi+1, . . . , xn) −f (x1, . . . , xi−1, xi, xi+1, . . . , xn)
ϵ
 f (x + ϵei) −f (x)
ϵ
,
where ei is the vector (0, 0, . . . , 0, 1, 0, . . . , 0), where the 1 appears in the ith position. The
matrix of second partial derivatives of f is known as the Hessian, and is deﬁned as
∇2f (x) 


∂2f
∂x2
1
∂2f
∂x1∂x2
· · ·
∂2f
∂x1∂xn
∂2f
∂x2∂x1
∂2f
∂x2
2
· · ·
∂2f
∂x2∂xn
...
...
...
∂2f
∂xn∂x1
∂2f
∂xn∂x2
· · ·
∂2f
∂x2n


.
We say that f is differentiable if all ﬁrst partial derivatives of f exist, and continuously
differentiableifinadditionthesederivativesarecontinuousfunctionsofx.Similarly,f istwice
differentiable if all second partial derivatives of f exist and twice continuously differentiable if

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
581
they are also continuous. Note that when f is twice continuously differentiable, the Hessian
is a symmetric matrix, since
∂2f
∂xi∂xj

∂2f
∂xj∂xi
,
for all i, j  1, 2, . . . , n.
When the vector x in turn depends on another vector t (that is, x  x(t)), the chain
rule (A.11) for the univariate function can be extended as follows:
∇tf (x(t)) 
n

i1
∂f
∂xi
∇xi(t).
(A.13)
DIRECTIONAL DERIVATIVES
If f is continuously differentiable and p ∈IRn, then the directional derivative of f in
the direction p is given by
D(f (x); p)
def lim
ϵ→0
f (x + ϵp) −f (x)
ϵ
 ∇f (x)T p.
(A.14)
To verify this formula, we deﬁne the function
φ(α)  f (x + αp)  f (y(α)),
(A.15)
where y(α)  x + αp. Note that
lim
ϵ→0
f (x + ϵp) −f (x)
ϵ
 lim
ϵ→0
φ(ϵ) −φ(0)
ϵ
 φ′(0).
By applying the chain rule (A.13) to f (y(α)), we obtain
φ′(α) 
n

i1
∂f (y(α))
∂yi
∇yi(α)
(A.16)

n

i1
∂f (y(α))
∂yi
pi  ∇f (y(α))T p  ∇f (x + αp)T p.
We obtain (A.14) by setting α  0 and comparing the last two expressions.
The directional derivative is sometimes deﬁned even when the function f itself is not
differentiable. For instance, if f (x)  ∥x∥1, we have from the deﬁnition (A.14) that
D(∥x∥1; p)  lim
ϵ→0
∥x + ϵp∥1 −∥x∥1
ϵ
 lim
ϵ→0
n
i1 |xi + ϵpi| −n
i1 |xi|
ϵ
.

582
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
If xi > 0, we have |xi + ϵpi|  |xi| + ϵpi for all ϵ sufﬁciently small. If xi < 0, we have
|xi + ϵpi|  |xi| −ϵpi, while if xi  0, we have |xi + ϵpi|  ϵ|pi|. Therefore, we have
D(∥x∥1; p) 

i|xi<0
−pi +

i|xi>0
pi +

i|xi0
|pi|,
so the directional derivative of this function exists for any x and p. Its ﬁrst derivative does
not exist, however, whenever any of the components of x are zero.
❏Example A.1
Let f : IR2 →IR be deﬁned by f (x1, x2)  x2
1 + x1x2, where x1  sin t1 + t2
2 and
x2  (t1 + t2)2. The chain rule (A.13) yields
∇tf (x(t))

n

i1
∂f
∂xi
∇xi(t)
 (2x1 + x2)

cos t1
2t2

+ x1

2(t1 + t2)
2(t1 + t2)


	
2
	
sin t1 + t2
2

+ (t1 + t2)2

cos t1
2t2

+
	
sin t1 + t2
2


2(t1 + t2)
2(t1 + t2)

.
If, on the other hand, we substitute directly for x into the deﬁnition of f , we obtain
f (x(t)) 
	
sin t1 + t2
2

2 +
	
sin t1 + t2
2

(t1 + t2)2.
The reader should verify that the gradient of this expression is identical to the one obtained
above by applying the chain rule.
❐
MEAN VALUE THEOREM
We now recall the mean value theorem for univariate functions. Given a continuously
differentiable function φ : IR →IR and two real numbers α0 and α1 that satisfy α1 > α0, we
have that
φ(α1)  φ(α0) + φ′(ξ)(α1 −α0)
(A.17)

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
583
for some ξ ∈(α0, α1). An extension of this result to a multivariate function f : IRn →IR is
that for any vector p we have
f (x + p)  f (x) + ∇f (x + αp)T p,
(A.18)
for some α ∈(0, 1). (This result can be proved by deﬁning φ(α)  f (x + αp), α0  0, and
α1  1 and applying the chain rule, as above.)
❏Example A.2
Consider f : IR2 →IR deﬁned by f (x)  x3
1 + 3x1x2
2, and let x  (0, 0)T and
p  (1, 2)T . It is easy to verify that f (x)  0 and f (x + p)  13. Since
∇f (x + αp) 

3(x1 + αp1)2 + 3(x2 + αp2)2
6(x1 + αp1)(x2 + αp2)



15α2
12α2

,
we have that ∇f (x + αp)T p  39α2. Hence the relation (A.18) holds when we set α 
1/
√
13, which lies in the open interval (0, 1), as claimed.
❐
An alternative expression to (A.18) can be stated for twice differentiable functions: We
have
f (x + p)  f (x) + ∇f (x)T p + 1
2pT ∇2f (x + αp)T p,
(A.19)
for some α ∈(0, 1). In fact, this expression is one form of Taylor’s theorem, Theorem 2.1 in
Chapter 2, to which we refer throughout the book.
IMPLICIT FUNCTION THEOREM
The implicit function theorem lies behind a number of important results in local
convergence theory of optimization algorithms and in the characterization of optimality
(see Chapter 12). We present a brief outline here based on the discussion in Lang [147,
p. 131].
Theorem A.1 (Implicit Function Theorem).
Let h : IRn × IRm →IRn be a function such that
(i) h(z∗, 0)  0 for some z∗∈IRn,
(ii) thefunctionh(·, ·)isLipschitzcontinuouslydifferentiableinsomeneighborhoodof(z∗, 0),
and

584
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
(iii) ∇zh(z, t) is nonsingular at the point (z, t)  (z∗, 0).
Then the function z : IRm →IRn deﬁned implicitly by h(z(t), t)  0 is well-deﬁned and
Lipschitz continuous for t ∈IRm in some neighborhood of the origin.
This theorem is frequently applied to parametrized systems of linear equations, in
which z is obtained as the solution of
M(t)z  g(t),
where M(·) ∈IRn×n has M(0) nonsingular, and g(·) ∈IRn. To apply the theorem, we deﬁne
h(z, t)  M(t)z −g(t).
If M(·) and g(·) are Lipschitz continuously differentiable in some neighborhood of 0, the
theorem implies that z(t)  M(t)−1g(t) is a Lipschitz continuous function of t for all t in
some neighborhood of 0.
GEOMETRY OF FEASIBLE SETS
In describing the theory of constrained optimization, we assume for the most part that
the set of feasible points is described by algebraic equations. That is, we aim to minimize a
function f over the set  deﬁned by
  {x ∈IRn | ci(x)  0, i ∈E; ci(x) ≥0, i ∈I},
(A.20)
where E and I are the index sets of equality and inequality constraints, respectively (see also
(12.2)). In some situations, however, it is useful to abandon the algebraic description and
merely consider the set  on its own merits, as a general subset of . By doing this, we no
longer tie  down to any particular algebraic description. (Note that for any given set ,
there may be inﬁnitely many collections of constraint functions ci, i ∈E ∪I, such that the
identiﬁcation (A.20) holds.)
There are advantages and disadvantages to taking the purely geometric viewpoint. By
not tying ourselves to a particular algebraic description, we avoid the theoretical compli-
cations caused by redundant, linearly dependent, or insufﬁciently smooth constraints. We
also avoid some practical problems such as poor scaling. On the other hand, most of the
development of theory, algorithms, and software for constrained optimization assumes that
an algebraic description of  is available. If we choose instead to work explicitly with ,
we must reformulate the optimality conditions and algorithms in terms of this set, without
reference to its algebraic description. Many such tools are available, and they have been ap-
plied successfully in a number of applications, including optimal control. (See, for example,
Clarke [42], Dunn [77].) We describe a few of the relevant concepts here.

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
585
Given a constrained optimization problem formulated as
min f (x) subject to x ∈,
(A.21)
where  is a closed subset of IRn, most of the theory revolves around tangent cones and
normal cones to the set  at various feasible points x ∈. A deﬁnition of a tangent vector
is given by Clarke [42, Theorem 2.4.5].
Deﬁnition A.1 (Tangent).
A vector w ∈IRn is tangent to  at x ∈ if for all vector sequences {xi} with xi →x
and xi ∈, and all positive scalar sequences ti ↓0, there is a sequence wi →w such that
xi + tiwi ∈ for all i.
The tangent cone T(x) is the collection of all tangent vectors to  at x. The normal cone
N(x) is simply the orthogonal complement to the tangent cone; that is,
N(x) 
)
v | vT w ≤0 for all w ∈T(x)
*
.
(A.22)
Note that the zero vector belongs to both T and N.
It is not difﬁcult to verify that both T and N are indeed cones according to the
deﬁnition (A.2). To prove this for T, we take w ∈T(x) and show that αw ∈T(x) for
some given α ≥0. Let the sequence {xi} and {ti} be given, as in Deﬁnition A.1, and deﬁne
the sequence {¯ti} by ¯ti  ti/α. Since {¯ti} is also a valid sequence of scalars according to
Deﬁnition A.1, there is a sequence of vectors { ¯wi} such that xi + ¯ti ¯wi ∈ and ¯wi →w. By
deﬁning wi  α ¯wi, we have that xi + tiwi  xi + ¯ti ¯wi ∈. Hence we have identiﬁed a
sequence {wi} satisfying the conditions of Deﬁnition A.1 with the property that wi →αw,
so we conclude that αw ∈T(x), as required.
As an example, consider the set deﬁned by a single equality constraint:
  {x ∈IRn | h(x)  0},
(A.23)
where h : IRn →IR is smooth, and let x be a particular point for which ∇h(x) ̸ 0. If
w ∈T(x) for some x ∈, we have from Deﬁnition A.1 that there is a sequence wi →w
such that x + tiwi ∈ for any sequence ti ↓0. (We have made the legal choice xi ≡x in
the deﬁnition.) Therefore, we have
h(x + tiwi) for all i,
h(x)  0.
By using this fact together with a Taylor series expansion and smoothness of h, we ﬁnd that
0  1
ti
[h(x + tiwi) −h(x)]  wT
i ∇h(x) + o(ti)/ti →wT ∇h(x).

586
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
feasible set
Figure A.1
Feasible set deﬁned by x1 ≥x2
2,
x2 ≥x2
1.
An argument based on the implicit function theorem can be used to prove the converse,
which is that wT ∇h(x)  0 ⇒w ∈T(x). Hence we have
T(x) 
)
w | wT ∇h(x)  0
*
 Null
	
∇h(x)T 
.
(A.24)
It follows from the deﬁnition (A.22) that the normal cone is
N(x)  {α∇h(x) | α ∈IR}  Range(∇h(x)).
(A.25)
For the next example, consider a set  deﬁned by two parabolic constraints:
  {x ∈IR2 | x1 ≥x2
2, x2 ≥x2
1}
(see Figure A.1). The tangent and normal cones at the most interesting point—the origin—
are given by
T(0, 0)  {w ∈IR2 | w1 ≥0, w2 ≥0},
N(0, 0)  {v ∈IR2 | v1 ≤0, v2 ≤0}.
To show that, for instance, the tangent vector (0, 1) ﬁts into Deﬁnition A.1, suppose we
are given the feasible sequence xi  (1/i2, 1/i) →(0, 0) and the positive scalar sequence
ti  1/i ↓0. If we deﬁne wi 
	 1
3i , 1

, it is easy to check that xi + tiwi is feasible for each i
and that wi →(0, 1).
A three-dimensional example is given by the “ice-cream cone” deﬁned by
 

x ∈IR3 | x3 ≥2
1
x2
1 + x2
2

(A.26)

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
587
x
x
x
1
2
3
Figure A.2
“Ice-cream cone” set.
(see Figure A.2). At the origin x  0, the tangent cone coincides with the feasible set:
T(0)  . The normal cone is deﬁned as
N(0) 

v | v3 ≤−1
2
1
v2
1 + v2
2

.
To verify that vT w ≤0 for all v ∈N(0), w ∈T(0), we have
vT w  v1w1 + v2w2 + v3w3
≤v1w1 + v2w2 −1
2(v2
1 + v2
2)1/2(2)(w2
1 + w2
2)1/2


(v1w1 + v2w2)2 −
1
(v2
1 + v2
2)(w2
1 + w2
2)

1
v2
1w2
1 + v2
2w2
2 + 2v1v2w1w2 −
1
v2
1w2
1 + v2
2w2
2 + v2
2w2
1 + v2
1w2
2.
Since
v2
1w2
2 + v2
2w2
1 ≥2v1v2w2w2,
the second term outweighs the ﬁrst, so we have vT w ≤0, as required.
A ﬁnal example is given by the two-dimensional subset
 
)
x ∈IR2 | x2 ≥0, x2 ≤x3
1
*
,
(A.27)

588
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
T
N
Figure A.3
Feasible set deﬁned by x2 ≥0, x2 ≤x3
1, showing tangent and normal
cones.
illustrated in Figure A.3. The tangent cone at the origin is
T(0)  {(w1, 0) | w1 ≥0},
(A.28)
that is, all positive multiples of the vector (1, 0). The normal cone is
N(0)  {v | v1 ≤0}.
(A.29)
Finally, we recall the deﬁnitions of afﬁne hull affF and relative interior riF from
earlier in the chapter. For the set  deﬁned by (A.27), we have that
ri 
)
x | x2 > 0, x2 < x3
1
*
,
riT(0)  {(w1, 0) | w1 > 0},
riN(0)  {(v1, v2) | v1 < 0}.

A . 1 .
E l e m e n t s o f A n a l y s i s , G e o m e t r y , T o p o l o g y
589
ORDER NOTATION
In much of our analysis we are concerned with how the members of a sequence behave
eventually, that is, when we get far enough along in the sequence. For instance, we might ask
whether the elements of the sequence are bounded, or whether they are similar in size to the
elements of a corresponding sequence, or whether they are decreasing and, if so, how rapidly.
Order notation is useful shorthand to use when questions like these are being examined. It
saves us deﬁning many constants that clutter up the argument and the analysis.
We will use three varieties of order notation: O(·), o(·), and (·). Given two
nonnegative inﬁnite sequences of scalars {ηk} and {νk}, we write
ηk  O(νk)
if there is a positive constant C such that
|ηk| ≤C|νk|
for all k sufﬁciently large. We write
ηk  o(νk)
if the sequence of ratios {ηk/νk} approaches zero, that is,
lim
k→∞
ηk
νk
 0.
Finally, we write
ηk  (νk)
if there are two constants C0 and C1 with 0 < C0 ≤C1 < ∞such that
C0|νk| ≤|ηk| ≤C1|νk|,
that is, the corresponding elements of both sequences stay in the same ballpark for all k. This
deﬁnition is equivalent to saying that ηk  O(νk) and νk  O(ηk).
The same notation is often used in the context of quantities that depend continuously
on each other as well. For instance, if η(·) is a function that maps IR to IR, we write
η(ν)  O(ν)
if there is a constant C such that |η(ν)| ≤C|ν| for all ν ∈IR. (Typically, we are interested
only in values of ν that are either very large or very close to zero; this should be clear from

590
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
the context. Similarly, we use
η(ν)  o(ν)
(A.30)
to indicate that the ratio η(ν)/ν approaches zero either as ν →0 or ν →∞. (Again, the
precise meaning should be clear from the context.)
As a slight variant on the deﬁnitions above, we write
ηk  O(1)
to indicate that there is a constant C such that |ηk| ≤C for all k, while
ηk  o(1)
indicates that limk→∞ηk  0. We sometimes use vector and matrix quantities as arguments,
and in these cases the deﬁnitions above are intended to apply to the norms of these quantities.
For instance, if f : IRn →IRn, we write f (x)  O(∥x∥) if there is a constant C > 0 such
that ∥f (x)∥≤C∥x∥for all x in the domain of f . Typically, as above, we are interested only
in some subdomain of f , usually a small neighborhood of 0. As before, the precise meaning
should be clear from the context.
ROOT-FINDING FOR SCALAR EQUATIONS
In Chapter 11 we discussed methods for ﬁnding solutions of nonlinear systems of
equations F(x)  0, where F : IRn →IRn. Here we discuss brieﬂy the case of scalar
equations (n  1), for which the algorithm is easy to illustrate. Scalar root-ﬁnding is needed
in the trust-region algorithms of Chapter 4, for instance. Of course, the general theorems of
Chapter 11 can be applied to derive rigorous convergence results for this special case.
The basic step of Newton’s method (Algorithm Newton of Chapter 11) in the scalar
case is simply
pk  −F(xk)/F ′(xk),
xk+1 ←xk + pk
(A.31)
(cf. (11.9)). Graphically, such a step involves taking the tangent to the graph of F at the
point xk and taking the next iterate to be the intersection of this tangent with the x axis
(see Figure A.4). Clearly, if the function F is nearly linear, the tangent will be quite a good
approximation to F itself, so the Newton iterate will be quite close to the true root of F.
ThesecantmethodforscalarequationscanbeviewedasthespecializationofBroyden’s
method to the case of n  1. The issues are simpler in this case, however, since the secant
equation (11.26) completely determines the value of the 1 × 1 approximate Hessian Bk.
That is, we do not need to apply extra conditions to ensure that Bk is fully determined. By

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
591
xk
xk+1
xk
F(   )
tangent
Figure A.4
One step of Newton’s method for a scalar equation.
combining (11.24) with (11.26), we ﬁnd that the secant method for the case of n  1 is
deﬁned by
Bk  (F(xk) −F(xk−1))/(xk −xk−1),
(A.32a)
pk  −F(xk)/Bk,
xk+1  xk + pk.
(A.32b)
By illustrating this algorithm, we see the origin of the term “secant.” Bk approximates the
slope of the function at xk by taking the secant through the points (xk−1, F(xk−1) and
(xk, F(xk)), and xk+1 is obtained by ﬁnding the intersection of this secant with the x axis.
The method is illustrated in Figure A.5.
A.2
ELEMENTS OF LINEAR ALGEBRA
VECTORS AND MATRICES
In this book we work exclusively with vectors and matrices whose components are
real numbers. Vectors are usually denoted by lowercase roman characters, and matrices by

592
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
xk
xk+1
xk+2
secant
Figure A.5
One step of the secant method for a scalar equation.
uppercase roman characters. The space of real vectors of length n is denoted by IRn, while
the space of real m × n matrices is denoted by IRm×n.
We say that a matrix A ∈IRn×n is symmetric if A  AT . A symmetric matrix A is
positive deﬁnite if there is a positive constant α such that
xT Ax ≥α∥x∥2,
for all x ∈IRn.
(A.33)
It is positive semideﬁnite if the relationship (A.33) holds with α  0, that is, xT Ax ≥0 for
all x ∈IRn.
NORMS
For a vector x ∈IRn, we deﬁne the following norms:
∥x∥1
def
n

i1
|xi|,
(A.34a)
∥x∥2
def
# n

i1
x2
i
$1/2
 (xT x)1/2,
(A.34b)

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
593
∥x∥∞
def max
i1,...,n |xi|.
(A.34c)
The norm ∥·∥2 is often called the Euclidean norm. All these norms measure the length of the
vector in some sense, and they are equivalent in the sense that each one is bounded above
and below by a multiple of the other. To be precise, we have
∥x∥∞≤∥x∥2 ≤√n∥x∥∞,
∥x∥∞≤∥x∥1 ≤n∥x∥∞,
(A.35)
and so on. In general, a norm is any mapping ∥· ∥from IRn to the nonnegative real numbers
that satisﬁes the following properties:
∥x + z∥≤∥x∥+ ∥z∥,
for all x, z ∈IRn;
(A.36a)
∥x∥ 0 ⇒x  0;
(A.36b)
∥αx∥ |α|∥x∥,
for all α ∈IR and x ∈IRn.
(A.36c)
Equality holds in (A.36a) if and only if one of the vectors x and z is a nonnegative scalar
multiple of the other.
Another interesting property that holds for the Euclidean norm ∥· ∥ ∥· ∥2 is the
H¨older inequality, which states that
xT z
 ≤∥x∥∥z∥,
(A.37)
with equality if and only if one of these vectors is a nonnegative multiple of the other. We
can prove this result as follows:
0 ≤∥αx + z∥2  α2∥x∥2 + 2αxT z + ∥z∥2.
The right-hand-side is a convex function of α, and it satisﬁes the required nonnegativity
property only if there exist fewer than 2 distinct real roots, that is,
(2xT z)2 ≤4∥x∥2∥z∥2,
proving (A.37). Equality occurs when the quadratic α has exactly one real root (that is,
|xT z|  ∥x∥∥z∥) and when αx + z  0 for some α, as claimed.
We can derive deﬁnitions for certain matrix norms from these vector norm deﬁni-
tions. If we let ∥· ∥be generic notation for the three norms listed in (A.34), we deﬁne the
corresponding matrix norm as
∥A∥
def sup
x̸0
∥Ax∥
∥x∥.
(A.38)

594
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
The matrix norms deﬁned in this way are said to be consistent with the vector norms (A.34).
Explicit formulae for these norms are as follows:
∥A∥1  max
j1,...,n
m

i1
|Aij|,
(A.39a)
∥A∥2  λ1(AT A)1/2,
where λ1(·) denotes the largest eigenvalue,
(A.39b)
∥A∥∞ max
i1,...,m
n

j1
|Aij|.
(A.39c)
The Frobenius norm ∥A∥F of the matrix A is deﬁned by
∥A∥F 
# m

i1
n

j1
a2
ij
$1/2
.
(A.40)
This norm is useful for many purposes, but it is not consistent with any vector norm. Once
again, these various matrix norms are equivalent with each other in a sense similar to (A.35).
For the Euclidean norm ∥· ∥ ∥· ∥2, the following property holds:
∥AB∥≤∥A∥∥B∥,
(A.41)
for all matrices A and B with consistent dimensions.
The condition number of a nonsingular matrix is deﬁned as
κ(A)  ∥A∥∥A−1∥,
(A.42)
where any matrix norm can be used in the deﬁnition. We distinguish the different norms
by the use of a subscript: κ1(·), κ2(·), and κ∞(·), respectively. (Of course, different norm
deﬁnitions give different measures of condition number, in general.)
Norms also have a meaning for scalar, vector, and matrix-valued functions that are
deﬁned on a particular domain. In these cases, we can deﬁne Hilbert spaces of functions for
which the inner product and norm are deﬁned in terms of an integral over the domain. We
omit details, since all the development of this book takes place in the space IRn, though many
of the algorithms can be extended to more general Hilbert spaces. However, we mention for
purposes of Newton analysis that the following inequality holds for functions of the type
that we consider in this book:

 b
a
F(t)
 ≤
 b
a
∥F(t)∥dt,
where F is a scalar-, vector-, or matrix-valued function on the interval [a, b].

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
595
SUBSPACES
Given the Euclidean space IRn, the subset S ⊂IRn is a subspace of IRn if the following
property holds: If x and y are any two elements of S, then
αx + βy ∈S,
for all α, β ∈IR.
For instance, S is a subspace of IR2 if it consists of (i) the whole space IRn; (ii) any line passing
through the origin; (iii) the origin alone; or (iv) the empty set.
Given any set of vectors ai ∈IRn, i  1, 2, . . . , m, the set
S 
)
w ∈IRn | aT
i w  0, i  1, 2, . . . , m
*
(A.43)
is a subspace. However, the set
)
w ∈IRn | aT
i w ≥0, i  1, 2, . . . , m
*
(A.44)
is not in general a subspace. For example, if we have n  2, m  1, and a1  (1, 0)T , this set
would consist of all vectors (w1, w2)T with w1 ≥0, but then given two vectors x  (1, 0)T
and y  (2, 3) in this set, it is easy to choose multiples α and β such that αx + βy has a
negative ﬁrst component, and so lies outside the set.
Sets of the forms (A.43) and (A.44) arise in the discussion of second-order optimality
conditions for constrained optimization.
For any subspace S of the Euclidean space IRn, the set of vectors s1, s2, . . . , sm in S is
called a linearly independent set if there are no real numbers α1, α2, . . . , αm such that
α1s2 + α2s2 + · · · + αmsm,
unless we make the trivial choice α1  α2  · · ·  αm  0. Another way to deﬁne linear
independence is to say that none of the vectors s1, s2, . . . , sm can be written as a linear
combination of the other vectors in this set. We say that this set of vectors is a spanning set
for S if any vector s ∈S can be written as
s  α1s2 + α2s2 + · · · + αmsm,
for some particular choice of the coefﬁcients α1, α2, . . . , αm.
If the vectors s1, s2, . . . , sm are both linearly independent and a spanning set for S,
we call them a basis. In this case, m (the number of elements in the basis) is referred to as
the dimension of S. Notationally, we write dim(S) to denote the dimension of S. Note that
there are many ways to choose a basis of S in general, but that all bases do, in fact, contain
the same number of vectors.

596
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
If A is any real matrix, the null space is the subspace
Null(A)  {w | Aw  0},
while the range space is
Range(A)  {w | w  Av for some vector v}.
The fundamental theorem of linear algebra states that
Null(A) ⊕Range(AT )  IRn,
where n is the number of columns in A.
EIGENVALUES, EIGENVECTORS, AND THE SINGULAR-VALUE
DECOMPOSITION
A scalar value λ is an eigenvalue of the n × n matrix A if there is a nonzero vector x
such that
Aq  λq.
The vector q is called an eigenvector of A. The matrix A is nonsingular if none of its eigen-
values are zero. The eigenvalues of symmetric matrices are all real numbers, while non
symmetric matrices may have imaginary eigenvalues. If the matrix is positive deﬁnite as well
as symmetric, its eigenvalues are all positive real numbers.
All matrices A ∈IRm×n can be decomposed as a product of three matrices with special
properties, as follows:
A  USV T .
(A.45)
Here, U and V are orthogonal matrices of dimension m × m and n × n, respectively, which
meansthattheysatisfytherelationsU T U  UU T  I andV T V  V V T  I.ThematrixS
is a diagonal matrix of dimension m×n, wih diagonal elements σi, i  1, 2, . . . , min(m, n),
that satisfy
σ1 ≥σ2 ≥· · · ≥σmin(m,n) ≥0.
These diagonal values are called the singular values of A, and (A.45) is called the singular-
value decomposition.

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
597
When A is symmetric, its n real eigenvalues λ1, λ2, . . . , λn and their associated
eigenvectors q1, q2, . . . , qn can be used to write a spectral decomposition of A as follows:
A 
n

i1
λiqiqT
i .
This decomposition can be restated in matrix form by deﬁning
  diag(λ1, λ2, · · · , λn),
Q  [q1 | q2 | . . . | qn],
and writing
A  QQT .
(A.46)
In fact, when A is positive deﬁnite as well as symmetric, this decomposition is identical to the
singular-value decomposition (A.45), where we deﬁne U  V  Q and S  . Note that
the singular values σi, i  1, 2, . . . , m, and the eigenvalues λi, i  1, 2, . . . , m, coincide in
this case.
In the case of the Euclidean norm (A.39b), we have for symmetric positive deﬁnite
matrices A that the singular values and eigenvalues of A coincide, and that
∥A∥ σ1(A)  largest eigenvalue of A,
∥A−1∥ σn(A)−1  inverse of smallest eigenvalue of A.
Hence, we have for all x ∈IRn that
σn(A)∥x∥2  ∥x∥2/∥A−1∥≤xT Ax ≤∥A∥∥x∥2  σ1(A)∥x∥2.
For an orthogonal matrix Q, we have for the Euclidean norm that
∥Qx∥ ∥x∥,
and that all the singular values of this matrix are equal to 1.
DETERMINANT AND TRACE
The trace of an n × n matrix A is deﬁned by
trace(A) 
n

i1
Aii.
(A.47)

598
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
If the eigenvalues of A are denoted by λ1, λ2, . . . , λn, it can be shown that
trace(A) 
n

i1
λi,
(A.48)
that is, the trace of the matrix is the sum of its eigenvalues.
The determinant of an n × n matrix A is the product of its eigenvalues; that is,
det A 
n
(
i1
λi.
(A.49)
The determinant has several appealing (and revealing) properties. For instance,
det A  0 if and only if A is singular;
det AB  (det A)(det B);
det A−1  1/ det A.
Recall that any orthogonal matrix A has the property that QQT  QT Q  I, so that
Q−1  QT . It follows from the property of the determinant that det Q  det QT  ±1.
The properties above are used in the analysis of Chapters 6 and 8.
MATRIX FACTORIZATIONS: CHOLESKY, LU, QR
Matrix factorizations are important both in the design of algorithms and in their
analysis. One such factorization is the singular-value decomposition deﬁned above in (A.45).
Here we deﬁne the other important factorizations.
All the factorization algorithms described below make use of permutation matrices.
Suppose that we wish to exchange the ﬁrst and fourth rows of a matrix A. We can perform
this operation by premultiplying A by a permutation matrix P, which is constructed by
interchanging the ﬁrst and fourth rows of an identity matrix that contains the same number
of rows as A. Suppose, for example, that A is a 5 × 5 matrix. The appropriate choice of P
would be
P 


0
0
0
1
0
0
1
0
0
0
0
0
1
0
0
1
0
0
0
0
0
0
0
0
1


.
A similar technique is used to to ﬁnd a permutation matrix P that exchanges columns of a
matrix.

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
599
The LU factorization of a matrix A ∈IRn×n is deﬁned as
PA  LU,
(A.50)
where
P is an n × n permutation matrix (that is, it is obtained by rearranging the rows of
the n × n identity matrix),
L is unit lower triangular (that is, lower triangular with diagonal elements equal to 1,
and
U is upper triangular.
This factorization can be used to solve a linear system of the form Ax  b efﬁciently by the
following three-step process:
form ˜b  Pb by permuting the elements of b;
solve Lz  ˜b by performing triangular forward-substitution, to obtain the vector z;
solve Ux  z by performing triangular back-substitution, to obtain the solution
vector x.
The factorization (A.50) can be found by using Gaussian elimination with row partial piv-
oting, an algorithm that requires approximately 2n3/3 ﬂoating-point operations when A is
dense. Standard software that implements this algorithm (notably, LAPACK [4]) is readily
available. The method can be stated as follows:
Algorithm A.1 (Gaussian Elimination with Row Partial Pivoting).
Given A ∈IRn×n;
Set P ←I, L ←0;
for i  1, 2, . . . , n
ﬁnd the index j ∈{i, i + 1, . . . , n} such that |Aji|  maxki,i+1,...,n |Aki|;
if Aij  0
stop; (* matrix A is singular *)
if i ̸ j
swap rows i and j of matrices A and L;
(* elimination step*)
Lii ←1;
for k  i + 1, i + 2, . . . , n
Lki ←Aki/Aii;
for l  i + 1, i + 2, . . . , n
Akl ←Akl −LkiAil;
end (for)
end (if)

600
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
end (for)
U ←upper triangular part of A.
Variants of the basic algorithm allow for rearrangement of the columns as well as
the rows during the factorization, but these do not add to the practical stability properties
of the algorithm. Column pivoting may, however, improve the performance of Gaussian
elimination when the matrix A is sparse, by ensuring that the factors L and U are also
reasonably sparse.
Gaussian elimination can be applied also to the case in which A is not square. When
A is m×n, with m > n, the standard row pivoting algorithm produces a factorization of the
form (A.50), where L ∈IRm×n is unit lower triangular and U ∈IRn×n is upper triangular.
When m < n, we can ﬁnd an LU factorization of AT rather than A, that is, we obtain
PAT 

L1
L2

U,
(A.51)
where L1 is m × m (square) unit lower triangular, U is m × m upper triangular, and L2 is a
general (n −m) × m matrix. If A has full row rank, we can use this factorization to calculate
its null space explicitly as the space spanned by the columns of the matrix
M  P T

L−T
1 LT
2
−I

U −T .
(A.52)
(It is easy to check that M has dimensions n × (n −m) and that AM  0; we leave this an
exercise.)
When A ∈IRn×n is symmetric positive deﬁnite, it is possible to compute a simi-
lar but more specialized factorization at about half the cost—about n3/3 operations. This
factorization, known as the Cholesky factorization, produces a matrix L such that
A  LLT .
(A.53)
(If we require L to have positive diagonal elements, it is uniquely deﬁned by this formula.)
The algorithm can be speciﬁed as follows.
Algorithm A.2 (Cholesky Factorization).
Given A ∈IRn×n symmetric positive deﬁnite;
for i  1, 2, . . . , n;
Lii ←√Aii;
for j  i + 1, i + 2, . . . , n
Lji ←Aji/Lii;
for k  i + 1, i + 2, . . . , j
Ajk ←Ajk −LjiLki;

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
601
end (for)
end (for)
end (for)
Note that this algorithm references only the lower triangular elements of A; in fact,
it is only necessary to store these elements in any case, since by symmetry they are simply
duplicated in the upper triangular positions.
Unlike the case of Gaussian elimination, the Cholesky algorithm can produce a valid
factorization of a symmetric positive deﬁnite matrix without swapping any rows or columns.
However,symmetricpermutation(thatis,reorderingtherowsandcolumnsinthesameway)
can be used to improve the sparsity of the factor L. In this case, the algorithm produces a
permutation of the form
P T AP  LLT
for some permutation matrix P.
The Cholesky factorization can be used to compute solutions of the system Ax  b
by performing triangular forward- and back-substitutions with L and LT , respectively, as
in the case of L and U factors produced by Gaussian elimination.
Another useful factorization of rectangular matrices A ∈IRm×n has the form
AP  QR,
(A.54)
where
P is an n × n permutation matrix,
A is m × m orthogonal, and
R is m × n upper triangular.
In the case of a square matrix m  n, this factorization can be used to compute solutions of
linear systems of the form Ax  b via the following procedure:
set ˜b  QT b;
solve Rz  ˜b for z by performing back-substitution;
set x  P T z by rearranging the elements of x.
ForadensematrixA,thecostofcomputingtheQRfactorizationisabout4m2n/3operations.
In the case of a square matrix, this is about twice as many operations as required to compute
an LU factorization via Gaussian elimination. Unlike the case of Gaussian elimination, the
QR factorization procedure cannot be modiﬁed in general to ensure efﬁciency on sparse
matrices. That is, no matter how the column permutation matrix P is chosen, the factors Q

602
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
and R will be dense in general. (This remains true even if we allow row pivoting as well as
column pivoting.)
Algorithms to perform QR factorization are almost as simple as algorithms for Gaus-
sian elimination and for Cholesky factorization. The most widely used algorithms work
by applying a sequence of special orthogonal matrices to A, known either as Householder
transformations or Givens rotations, depending on the algorithm. We omit the details, and
refer instead to Golub and Van Loan [115, Chapter 5] for a complete description.
In the case of a rectangular matrix A with m < n, we can use the QR factorization of
AT to ﬁnd a matrix whose columns span the null space of A. To be speciﬁc, we write
AT P  QR 

Q1
Q2

R,
where Q1 consists of the ﬁrst m columns of Q, and Q2 contains the last n −m columns. It is
easy to show that columns of the matrix Q2 span the null space of A. This procedure yields
a more satisfactory basis matrix for the null space than the Gaussian elimination procedure
(A.52), because the columns of Q2 are orthogonal to each other and have unit length. It may
be more expensive to compute, however, particularly in the case in which A is sparse.
When A has full column rank, we can make an identiﬁcation between the R factor in
(A.54) and the Cholesky factorization. By multiplying the formula (A.54) by its transpose,
we obtain
P T AT AP  RT QT QR  RT R,
andbycomparisonwith(A.53),weseethatRT issimplytheCholeskyfactorofthesymmetric
positive deﬁnite matrix P T AT AP. Recalling that L is uniquely deﬁned when we restrict its
diagonal elements to be positive, this observation implies that R is also uniquely deﬁned
for a given choice of permutation matrix P, provided that we enforce positiveness of the
diagonals of R. Note, too, that since we can rearrange (A.54) to read APR−1  Q, we can
conclude that Q is also uniquely deﬁned under these conditions.
Note that by deﬁnition of the Euclidean norm and the property (A.41), and the fact
that the Euclidean norms of the matrices P and Q in (A.54) are both 1, we have that
∥A∥ ∥QRP T ∥≤∥Q∥∥R∥∥P T ∥ ∥R∥,
while
∥R∥ ∥QT AP∥≤∥QT ∥∥A∥∥P∥ ∥A∥.
We conclude from these two inequalities that ∥A∥ ∥R∥. When A is square, we have by a
similar argument that ∥A−1∥ ∥R−1∥. Hence the Euclidean-norm condition number of
A can be estimated by substituting R for A in the expression (A.42). This observation is

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
603
signiﬁcant because various techniques are available for estimating the condition number of
triangular matrices R; see Golub and Van Loan [115, pp. 128–130] for a discussion.
SHERMAN–MORRISON–WOODBURY FORMULA
If the square nonsingular matrix A undergoes a rank-one update to become
¯A  A + abT ,
where a, b ∈IRn, then if ¯A is nonsingular, we have
¯A−1  A−1 −A−1abT A−1
1 + bT A−1a .
(A.55)
It is easy to verify this formula: Simply multiply the deﬁnitions of ¯A and ¯A−1 together and
check that they produce the identity.
This formula can be extended to higher-rank updates. Let U and V be matrices in
IRn×p for some p between 1 and n. If we deﬁne
ˆA  A + UV T ,
then
ˆA−1  A−1 −A−1U(I + V T A−1U)−1V T A−1.
(A.56)
We can use this formula to solve linear systems of the form ¯Ax  d. Since
x  ˆA−1d  A−1d −A−1U(I + V T A−1U)−1V T A−1d,
we see that x can be found by solving p+1 linear systems with the matrix A (to obtain A−1d
and A−1U), inverting the p × p matrix I + V T A−1U, and performing some elementary
matrix algebra. Inversion of the p × p matrix I + V T A−1U is inexpensive when p ≪n.
INTERLACING EIGENVALUE THEOREM
The following result is proved in Golub and Van Loan [115, Theorem 8.1.8].
Theorem A.2 (Interlacing Eigenvalue Theorem).
Let A ∈IRn×n be a symmetric matrix with eigenvalues λ1, λ2, . . . , λn satisfying
λ1 ≥λ2 ≥· · · ≥λn,

604
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
and let z ∈IRn be a vector with ∥z∥ 1, and α ∈IR be a scalar. Then if we denote the
eigenvalues of A + αzzT by ξ1, ξ2, . . . , ξn (in decreasing order), we have for α > 0 that
ξ1 ≥λ1 ≥ξ2 ≥λ2 ≥ξ3 ≥· · · ≥ξn ≥λn,
with
n

i1
ξi −λi  α.
(A.57)
If α < 0, we have that
λ1 ≥ξ1 ≥λ2 ≥ξ2 ≥λ3 ≥· · · ≥λn ≥ξn,
where the relationship (A.57) is again satisﬁed.
Informally stated, the eigenvalues of the modiﬁed matrix “interlace” the eigenvalues of the
original matrix, with nonnegative adjustments if the coefﬁcient α is positive, and nonpos-
itive adjustments if α is negative. The total magnitude of the adjustments equals α, whose
magnitude is identical to the Euclidean norm ∥αzzT ∥2 of the modiﬁcation.
ERROR ANALYSIS AND FLOATING-POINT ARITHMETIC
In most of this book our algorithms and analysis deal with real numbers. Modern
digital computers, however, cannot store or compute with general real numbers. Instead,
theyworkwithasubsetknownasﬂoating-pointnumbers.Anyquantitiesthatarestoredonthe
computer, whether they are read directly from a ﬁle or program or arise as the intermediate
result of a computation, must be approximated by a ﬂoating-point number. In general, then,
the numbers that are produced by practical computation differ from those that would be
produced if the arithmetic were exact. Of course, we try to perform our computations in
such a way that these differences are as tiny as possible.
Discussion of errors requires us to distinguish between absolute error and relative error.
If x is some exact quantity (scalar, vector, matrix) and ˜x is its approximate value, the absolute
error is the norm of the difference, namely, ∥x −˜x∥. (In general, any of the norms (A.34a),
(A.34b), and (A.34c) can be used in this deﬁnition.) The relative error is the ratio of the
absolute error to the size of the exact quantity, that is,
∥x −˜x∥
∥x∥
.
When this ratio is signiﬁcantly less than one, we can replace the denominator by the size of
the approximate quantity—that is, ∥˜x∥—without affecting its value very much.

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
605
Most computationsassociatedwithoptimizationalgorithmsareperformedindouble-
precision arithmetic. Double-precision numbers are stored in words of length 64 bits. Most
of these bits (say t) are devoted to storing the fractional part, while the remainder encode
the exponent e and other information, such as the sign of the number, or an indication of
whether it is zero or “undeﬁned.” Typically, the fractional part has the form
.d1d2 . . . dt,
where each di, i  1, 2, . . . , t, is either zero or one. (In some systems d1 is implicitly assumed
to be 1 and is not stored.) The value of the ﬂoating-point number is then
t
i1
di2−i × 2e.
The value 2−t is known as unit roundoff and is denoted by u. Any real number whose
absolute value lies in the range [2L, 2U] (where L and U are lower and upper bounds on
the value of the exponent e) can be approximated to within a relative accuracy of u by a
ﬂoating-point number, that is,
ﬂ(x)  x(1 + ϵ),
where |ϵ| ≤u,
(A.58)
where ﬂ(·) denotes ﬂoating-point approximation. The value of u for double-precision com-
putations is typically about 10−15. In other words, if the real number x and its ﬂoating-point
approximation are both written as base-10 numbers (the usual fashion), they agree to at
least 15 digits.
For further information on ﬂoating-point computations, see Golub and Van
Loan [115, Section 2.4] and Higham [136].
When an arithmetic operation is performed with one or two ﬂoating-point numbers,
the result must also be stored as a ﬂoating-point number. This process introduces a small
roundoff error, whose size can be quantiﬁed in terms of the size of the arguments. If x and y
are two ﬂoating-point numbers, we have that
|ﬂ(x ∗y) −x ∗y| ≤u|x ∗y|,
(A.59)
where ∗denotes any of the operations +, −, ×, ÷.
Although the error in a single ﬂoating-point operation appears benign, more signiﬁ-
cant errors may occur when the arguments x and y are ﬂoating-point approximations of two
real numbers, or when a sequence of computations are performed in succession. Suppose,
for instance, that x and y are large real numbers whose values are very similar. When we
store them in a computer, we approximate them with ﬂoating-point numbers ﬂ(x) and ﬂ(y)

606
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
that satisfy
ﬂ(x)  x + ϵx,
ﬂ(y)  y + ϵy,
where |ϵx| ≤u|x|, |ϵy| ≤u|y|.
If we take the difference of the two stored numbers, we obtain a ﬁnal result ﬂ(ﬂ(x) −ﬂ(y))
that satisﬁes
ﬂ(ﬂ(x) −ﬂ(y))  (ﬂ(x) −ﬂ(y))(1 + ϵxy),
where |ϵxy| ≤u.
By combining these expressions, we ﬁnd that the difference between this result and the true
value x −y may be as large as
ϵx + ϵy + ϵxy,
which is bounded by u(|x| + |y| + |x −y|). Hence, since x and y are large and close
together, the relative error is approximately 2u|x|/|x −y|, which may be quite large, since
|x| ≫|x −y|.
This phenomenon is known as cancellation. It can also be explained (less formally)
by noting that if both x and y are accurate to k digits, and if they agree in the ﬁrst ¯k
digits, then their difference will contain only about k −¯k signiﬁcant digits—the ﬁrst ¯k digits
cancel each other out. This observation is the reason for the well-known adage of numerical
computing—that one should avoid taking the difference of two similar numbers if at all
possible.
CONDITIONING AND STABILITY
Conditioning and stability are two terms that are used frequently in connection with
numerical computations. Unfortunately, their meaning sometimes varies from author to
author, but the general deﬁnitions below are widely accepted, and we adhere to them in this
book.
Conditioning is a property of the numerical problem at hand (whether it is a linear
algebra problem, an optimization problem, a differential equations problem, or whatever).
A problem is said to be well conditioned if its solution is not affected greatly by small
perturbations to the data that deﬁne the problem. Otherwise, it is said to be ill conditioned.
A simple example is given by the following 2 × 2 system of linear equations:

1
2
1
1
 
x1
x2



3
2

.

A . 2 .
E l e m e n t s o f L i n e a r A l g e b r a
607
By computing the inverse of the coefﬁcient matrix, we ﬁnd that the solution is simply

x1
x2



−1
2
1
−1
 
3
2



1
1

.
If we replace the ﬁrst right-hand-side element by 3.00001, the solution becomes (x1, x2)T 
(0.99999, 1.00001)T , which is only slightly different from its exact value (1, 1)T . We would
note similar insensitivity if we were to perturb the other elements of the right-hand-side or
elements of the coefﬁcient matrix. We conclude that this problem is well conditioned. On
the other hand, the problem

1.00001
1
1
1
 
x1
x2



2.00001
2

is ill conditioned. Its exact solution is x  (1, 1)T , but if we change the ﬁrst element of the
right-hand-side from 2.00001 to 2, the solution would change drastically to x  (0, 2)T .
For general square linear systems Ax  b where A ∈IRn×n, the condition number of
the matrix (deﬁned in (A.42)) can be used to quantify the conditioning. Speciﬁcally, if we
perturb A to ˜A and b to ˜b and take ˜x to be the solution of the perturbed system ˜A˜x  ˜b, it
can be shown that
∥x −˜x∥
∥x∥
≈κ(A)

∥A −˜A∥
∥A∥
+ ∥b −˜b∥
∥b∥

(see, for instance, Golub and Van Loan [115, Section 2.7]). Hence, a large condition number
κ(A) indicates that the problem Ax  b is ill conditioned, while a modest value indicates
well conditioning.
Note that the concept of conditioning has nothing to do with the particular algorithm
that is used to solve the problem, only with the numerical problem itself.
Stability, on the other hand, is a property of the algorithm. An algorithm is stable if it
is guaranteed to produce accurate answers to all well-conditioned problems in its class, even
when ﬂoating-point arithmetic is used.
As an example, consider again the linear equations Ax  b. We can show that Algo-
rithm A.1, in combination with triangular substitution, yields a computed solution ˜x whose
relative error is approximately
∥x −˜x∥
∥x∥
≈κ(A)growth(A)
∥A∥
u,
(A.60)
where growth(A) is the size of the largest element that arises in A during execution of
Algorithm A.1. In the worst case, we can show that growth(A)/∥A∥may be around 2n−1,
which indicates that Algorithm A.1 is an unstable algorithm, since even for modest n (say,

608
A p p e n d i x
A .
B a c k g r o u n d M a t e r i a l
n  200), the right-hand-side of (A.60) may be large even when κ(A) is modest. In practice,
however, large growth factors are rarely observed, so we conclude that Algorithm A.1 is stable
for all practical purposes.
Gaussian elimination without pivoting, on the other hand, is deﬁnitely unstable. If we
omit the possible exchange of rows in Algorithm A.1, the algorithm will fail to produce a
factorization even of some well-conditioned matrices, such as
A 

0
1
1
2

.
For systems Ax  b in which A is symmetric positive deﬁnite, the Cholesky factoriza-
tionincombinationwithtriangularsubstitutionconstitutesastablealgorithmforproducing
a solution x.

References
[1] R. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algorithms, and
Applications, Prentice-Hall, Englewood Cliffs, N.J., 1993.
[2] H. Akaike, On a successive transformation of probability distribution and its application to the
analysis of the optimum gradient method, Annals of the Institute of Statistical Mathematics, 11
(1959), pp. 1–17.
[3] M. Al-Baali, Descent property and global convergence of the Fletcher-Reeves method with inexact
line search, I.M.A. Journal on Numerical Analysis, 5 (1985), pp. 121–124.
[4] E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-
marling, A. McKenney, S. Ostrouchov, and D. Sorensen, LAPACK User’s Guide, SIAM,
Philadelphia, 1992.
[5] B. M. Averick, R. G. Carter, J. J. Mor´e, and G. Xue, The MINPACK-2 test problem collection,
Preprint MCS–P153–0692, Mathematics and Computer Science Division, Argonne National
Laboratory, Argonne, Ill., 1992.
[6] P. Baptist and J. Stoer, On the relation between quadratic termination and convergence properties
of minimization algorithms, Part II: Applications, Numerische Mathematik, 28 (1977), pp. 367–
392.
[7] M. Bazaraa, H. Sherali, and C. Shetty, Nonlinear Programming, Theory and Applications.,
John Wiley & Sons, New York, second ed., 1993.

610
R e f e r e n c e s
[8] D. P. Bertsekas, Constrained Optimization and Lagrange Multiplier Methods, Academic Press,
New York, 1982.
[9]
, Nonlinear Programming, Athena Scientiﬁc, Belmont, Mass., 1995.
[10] M. Berz, C. Bischof, C. F. Corliss, and A. Griewank, eds., Computational Differentiation:
Techniques, Applications, and Tools, SIAM Publications, Philadelphia, Penn., 1996.
[11] J. R. Birge and F. Louveaux, Introduction to Stochastic Programming, Springer-Verlag, New York,
1997.
[12] C. Bischof, A. Bouaricha, P. Khademi, and J. J. Mor´e, Computing gradients in large-scale
optimization using automatic differentiation, INFORMS Journal on Computing, 9 (1997),
pp. 185–194.
[13] C. Bischof, A. Carle, P. Khademi, and A. Mauer, ADIFOR 2.0: Automatic differentiation of
FORTRAN 77 programs, IEEE Computational Science & Engineering, 3 (1996), pp. 18–32.
[14] C.Bischof,G.Corliss,andA.Griewank,Structuredsecond-andhigher-orderderivativesthrough
univariate Taylor series, Optimization Methods and Software, 2 (1993), pp. 211–232.
[15] C. Bischof and M. R. Haghighat, On hierarchical differentiation, in Computational Differen-
tiation: Techniques, Applications, and Tools, M. Berz, C. Bischof, G. Corliss, and A. Griewank,
eds., SIAM, Philadelphia, 1996, pp. 83–94.
[16] C. Bischof, P. Khademi, A. Bouaricha, and A. Carle, Efﬁcient computation of gradients and Jaco-
bians by transparent exploitation of sparsity in automatic differentiation, Optimization Methods
and Software, 7 (1996), pp. 1–39.
[17] C.Bischof,L.Roh,andA.Mauer,ADIC:Anextensible automatic differentiation tool for ANSI-C,
Software—Practice and Experience, 27 (1997), pp. 1427–1456.
[18] ˚A. Bj¨orck, Least squares methods, in Handbook of Numerical Analysis, P. G. Ciarlet and J. L.
Lions, eds., Elsevier/North-Holland, Amsterdam, The Netherlands, 1990.
[19]
, Numerical Methods for Least Squares Problems, SIAM Publications, Philadelphia, Penn.,
1996.
[20] P. T. Boggs, R. H. Byrd, and R. B. Schnabel, A stable and efﬁcient algorithm for nonlinear
orthogonal distance regression, SIAM Journal on Scientiﬁc and Statistical Computing, 8 (1987),
pp. 1052–1078.
[21] P.T.Boggs,J.R.Donaldson,R.H.Byrd,andR.B.Schnabel,ODRPACK—Softwareforweighted
orthogonaldistanceregression,ACMTransactionsonMathematicalSoftware,15(1981),pp.348–
364.
[22] P. T. Boggs and J. W. Tolle, Convergence properties of a class of rank-two updates, SIAM Journal
on Optimization, 4 (1994), pp. 262–287.
[23]
, Sequential quadratic programming, Acta Numerica, 4 (1996), pp. 1–51.
[24] P. T. Boggs, J. W. Tolle, and P. Wang, On the local convergence of quasi-Newton methods for
constrained optimization, SIAM Journal on Control and Optimization, 20 (1982), pp. 161–171.
[25] I. Bongartz, A. R. Conn, N. I. M. Gould, and P. L. Toint, CUTE: Constrained and unconstrained
testing environment, Research Report, IBM T.J. Watson Research Center, Yorktown Heights, NY,
1993.
[26] S. Boyd, L. El Ghaoui, E. Feron, and V. Balakrishnan, Linear Matrix Inequalities in Systems
and Control Theory, SIAM Publications, Phildelphia, 1994.
[27] R. P. Brent, Algorithms for minimization without derivatives, Prentice Hall, Englewood Cliffs,
N.J., 1973.
[28] A. Buckley and A. LeNir, QN-like variable storage conjugate gradients, Mathematical
Programming, 27 (1983), pp. 155–175.

R e f e r e n c e s
611
[29] R. Bulirsch and J. Stoer, Introduction to Numerical Analysis, Springer-Verlag, New York, 1980.
[30] J. R. Bunch and L. Kaufman, Some stable methods for calculating inertia and solving symmetric
linear systems, Mathematics of Computation, 31 (1977), pp. 163–179.
[31] J. R. Bunch and B. N. Parlett, Direct methods for solving symmetric indeﬁnite systems of linear
equations, SIAM Journal on Numerical Analysis, 8 (1971), pp. 639–655.
[32] J. V. Burke and J. J. Mor´e, Exposing constraints, SIAM Journal on Optimization, 4 (1994),
pp. 573–595.
[33] W.Burmeister,DieKonvergenzordnungdesFletcher-PowellAlgorithmus,Z.Angew.Math.Mech.,
53 (1973), pp. 693–699.
[34] R. H. Byrd, M. E. Hribar, and J. Nocedal, An interior-point algorithm for large-scale nonlinear
programming, Technical Report 97/05, Optimization Technology Center, Argonne National
Laboratory and Northwestern University, July 1997.
[35] R. H. Byrd, H. F. Khalfan, and R. B. Schnabel, Analysis of a symmetric rank-one trust region
method, SIAM Journal on Optimization, 6 (1996), pp. 1025–1039.
[36] R. H. Byrd and J. Nocedal, An analysis of reduced Hessian methods for constrained optimization,
Mathematical Programming, 49 (1991), pp. 285–323.
[37] R. H. Byrd, J. Nocedal, and R. B. Schnabel, Representations of quasi-Newton matrices and their
use in limited-memory methods, Mathematical Programming, Series A, 63 (1994), pp. 129–156.
[38] R. H. Byrd, J. Nocedal, and Y. Yuan, Global convergence of a class of quasi-Newton methods on
convex problems, SIAM Journal on Numerical Analysis, 24 (1987), pp. 1171–1190.
[39] R. H. Byrd, R. B. Schnabel, and G. A. Schultz, Approximate solution of the trust regions prob-
lem by minimization over two-dimensional subspaces, Mathematical Programming, 40 (1988),
pp. 247–263.
[40] R. Chamberlain, C. Lemar´echal, H. C. Pedersen, and M. J. D. Powell, The watchdog technique
for forcing convergence in algorithms for constrained optimization, Mathematical Programming,
16 (1982), pp. 1–17.
[41] S. H. Cheng and H. J. Higham, A modiﬁed Cholesky algorithm based on a symmetric indeﬁnite
factorization, technical report, University of Manchester, 1996.
[42] F. H. Clarke, Optimization and Nonsmooth Analysis, John Wiley & Sons, New York, 1983.
[43] A. Cohen, Rate of convergence of several conjugate gradient algorithms, SIAM Journal on
Numerical Analysis, 9 (1972), pp. 248–259.
[44] T.F.ColemanandA.R.Conn,Non-linearprogrammingviaanexactpenalty-function:Asymptotic
analysis, Mathematical Programming, 24 (1982), pp. 123–136.
[45] T. F. Coleman and A. R. Conn, On the local convergence of a quasi-Newton method for the
nonlinear programming problem, SIAM Journal on Numerical Analysis, 21 (1984), pp. 755–769.
[46] T. F. Coleman, B. Garbow, and J. J. Mor´e, Software for estimating sparse Jacobian matrices, ACM
Transactions on Mathematical Software, 10 (1984), pp. 329–345.
[47]
, Software for estimating sparse Hessian matrices, ACM Transactions on Mathematical
Software, 11 (1985), pp. 363–377.
[48] T. F. Coleman and J. J. Mor´e, Estimation of sparse Jacobian matrices and graph coloring problems,
SIAM Journal on Numerical Analysis, 20 (1983), pp. 187–209.
[49]
, Estimation of sparse Hessian matrices and graph coloring problems, Mathematical
Programming, 28 (1984), pp. 243–270.
[50] T. F. Coleman and D. C. Sorensen, A note on the computation of an orthonormal basis for the
null space of a matrix, Mathematical Programming, 29 (1984), pp. 234–242.

612
R e f e r e n c e s
[51] A. R. Conn, N. I. M. Gould, and P. L. Toint, Testing a class of algorithms for solving minimzation
problems with simple bounds on the variables, Mathematics of Computation, 50 (1988), pp. 399–
430.
[52]
, Convergence of quasi-Newton matrices generated by the symmetric rank one update,
Mathematical Programming, 50 (1991), pp. 177–195.
[53]
, LANCELOT: a FORTRAN package for large-scale nonlinear optimization (Release A),
no. 17 in Springer Series in Computational Mathematics, Springer-Verlag, New York, 1992.
[54]
, Numerical experiments with the LANCELOT package (Release A) for large-scale nonlinear
optimization, Report 92/16, Department of Mathematics, University of Namur, Belgium, 1992.
[55]
, A note on using alternative second-order models for the subproblems arising in barrier
function methods for minimization, Numerische Mathematik, 68 (1994), pp. 17–33.
[56] W. J. Cook, W. H. Cunningham, W. R. Pulleyblank, and A. Schrijver, Combinatorial
Optimization, John Wiley & Sons, New York, 1997.
[57] B. F. Corliss and L. B. Rall, An introduction to automatic differentiation, in Computational
Differentiation: Techniques, Applications, and Tools, M. Berz, C. Bischof, G. F. Corliss, and
A. Griewank, eds., SIAM Publications, Philadelphia, Penn., 1996, ch. 1.
[58] T. H. Cormen, C. E. Leisserson, and R. L. Rivest, Introduction to Algorithms, MIT Press, 1990.
[59] R. W. Cottle, J.-S. Pang, and R. E. Stone, The Linear Complementarity Problem, Academic
Press, San Diego, 1992.
[60] R. Courant, Variational methods for the solution of problems with equilibrium and vibration,
Bull. Amer. Math. Soc., 49 (1943), pp. 1–23.
[61] H. P. Crowder and P. Wolfe, Linear convergence of the conjugate gradient method, IBM Journal
of Research and Development, 16 (1972), pp. 431–433.
[62] A. Curtis, M. J. D. Powell, and J. Reid, On the estimation of sparse Jacobian matrices, Journal
of the Institute of Mathematics and its Applications, 13 (1974), pp. 117–120.
[63] G. B. Dantzig, Linear Programming and Extensions, Princeton University Press, Princeton, New
Jersey, 1963.
[64] W. C. Davidon, Variable metric method for minimization, Technical Report ANL–5990 (revised),
Argonne National Laboratory, Argonne, Il, 1959.
[65]
, Variable metric method for minimization, SIAM Journal on Optimization, 1 (1991),
pp. 1–17.
[66] R. S. Dembo, S. C. Eisenstat, and T. Steihaug, Inexact Newton methods, SIAM Journal on
Numerical Analysis, 19 (1982), pp. 400–408.
[67] J. E. Dennis, D. M. Gay, and R. E. Welsch, Algorithm 573 — NL2SOL, An adaptive nonlinear
least-squares algorithm, ACM Transactions on Mathematical Software, 7 (1981), pp. 348–368.
[68] J. E. Dennis and J. J. Mor´e, Quasi-Newton methods, motivation and theory, SIAM Review, 19
(1977), pp. 46–89.
[69] J. E. Dennis and R. B. Schnabel, Numerical Methods for Unconstrained Optimization, Prentice-
Hall, Englewood Cliffs, NJ, 1983. Reprinted by SIAM Publications, 1993.
[70] J. E. Dennis and R. B. Schnabel, A view of unconstrained optimization, in Optimization, vol. 1 of
Handbooks in Operations Research and Management, Elsevier Science Publishers, Amsterdam,
the Netherlands, 1989, pp. 1–72.
[71] P. Deuflhard, R. W. Freund, and A. Walter, Fast secant methods for the iterative solution of
large nonsymmetric linear systems, Impact of Computing in Science and Engineering, 2 (1990),
pp. 244–276.

R e f e r e n c e s
613
[72] I. I. Dikin, Iterative solution of problems of linear and quadratic programming, Soviet
Mathematics-Doklady, 8 (1967), pp. 674–675.
[73] I. S. Duff, J. Nocedal, and J. K. Reid, The use of linear programming for the solution of sparse
sets of nonlinear equations, SIAM Journal on Scientiﬁc and Statistical Computing, 8 (1987),
pp. 99–108.
[74] I. S. Duff and J. K. Reid, The multifrontal solution of indeﬁnite sparse symmetric linear equations,
ACM Transactions on Mathematical Software, 9 (1983), pp. 302–325.
[75]
, The design of MA48, a code for direct solution of sparse unsymmetric linear systems of
equations, ACM Transactions on Mathematical Software, 22 (1996), pp. 187–226.
[76] I. S. Duff, J. K. Reid, N. Munksgaard, and H. B. Neilsen, Direct solution of sets of linear equations
whose matrix is sparse symmetric and indeﬁnite, Journal of the Institute of Mathematics and its
Applications, 23 (1979), pp. 235–250.
[77] J. C. Dunn, A projected Newton method for minimization problems with nonlinear inequality
constraints, Numerische Mathematik, 53 (1988), pp. 377–409.
[78] J. Dussault, Numerical stability and efﬁciency of penalty algorithms, SIAM Journal on Numerical
Analysis, 32 (1995), pp. 296–317.
[79] A. V. Fiacco and G. P. McCormick, Nonlinear Programming: Sequential Unconstrained Mini-
mization Techniques, John Wiley & Sons, New York, NY, 1968. reprinted by SIAM Publications,
1990.
[80] R.Fletcher,Ageneralquadraticprogrammingalgorithm,JournaloftheInstituteofMathematics
and its Applications, 7 (1971), pp. 76–91.
[81]
, A class of methods for nonlinear programming, iii: Rates of convergence, in Numerical
Methods for Non-Linear Optimization, F. A. Lootsma, ed., Academic Press, London and New
York, 1972, pp. 371–382.
[82]
, Second order corrections for non-differentiable optimization, in Numerical Analysis,
D. Grifﬁths, ed., Springer Verlag, 1982, pp. 85–114. Proceedings Dundee 1981.
[83]
, Practical Methods of Optimization, John Wiley & Sons, New York, second ed., 1987.
[84]
, An optimal positive deﬁnite update for sparse Hessian matrices, SIAM Journal on
Optimization, 5 (1995), pp. 192–218.
[85] R.Fletcher,A.Grothey,andS.Leyffer,ComputingsparseHessianandJacobianapproximations
with optimal hereditary properties, technical report, Department of Mathematics, University of
Dundee, 1996.
[86] R. Fletcher and S. Leyffer, Nonlinear programming without a penalty function, Tech. Rep.
NA/171, Department of Mathematics, University of Dundee, September 1997.
[87] R. Fletcher and A. P. McCann, Acceleration techniques for nonlinear programming, in
Optimization, R. Fletcher, ed., Academic Press, London, 1969, pp. 203–214.
[88] R. Fletcher and C. M. Reeves, Function minimization by conjugate gradients, Computer Journal,
7 (1964), pp. 149–154.
[89] R. Fletcher and E. Sainz de la Maza, Nonlinear programming and nonsmooth optimization by
successive linear programming, Mathematical Programming, (1989), pp. 235–256.
[90] C. Floudas and P. Pardalos, eds., Recent Advances in Global Optimization, Princeton University
Press, Princeton, NJ, 1992.
[91] A. Forsgren and P. E. Gill, Primal-dual interior methods for nonconvex nonlinear programming,
SIAM Journal on Optimization, 8 (1998), pp. 1132–1152.
[92] R. Fourer, D. M. Gay, and B. W. Kernighan, AMPL: A Modeling Language for Mathematical
Programming, The Scientiﬁc Press, South San Francisco, Calif., 1993.

614
R e f e r e n c e s
[93] R. Fourer and S. Mehrotra, Solving symmetric indeﬁnite systems in an interior-point method
for linear programming, Mathematical Programming, 62 (1993), pp. 15–39.
[94] R. Freund and N. Nachtigal, QMR: A quasi-minimal residual method for non-Hermitian linear
systems, Numerische Mathematik, 60 (1991), pp. 315–339.
[95] K. R. Frisch, The logarithmic potential method of convex programming, Technical Report,
University Institute of Economics, Oslo, Norway, 1955.
[96] D.Gabay,Reducedquasi-Newtonmethodswithfeasibilityimprovementfornonlinearlyconstrained
optimization, Mathematical Programming Studies, 16 (1982), pp. 18–44.
[97] U. M. Garcia-Palomares and O. L. Mangasarian, Superlinearly convergent quasi-Newton meth-
ods for nonlinearly constrained optimization problems, Mathematical Programming, 11 (1976),
pp. 1–13.
[98] D. M. Gay, More AD of nonlinear AMPL models: computing Hessian information and exploiting
partial separability. To appear in the Proceedings of the Second International Workshop on
Computational Differentiation, 1996.
[99] D. M. Gay, M. L. Overton, and M. H. Wright, A primal-dual interior method for nonconvex
nonlinear programming, Technical Report 97-4-08, Computing Sciences Research Center, Bell
Laboratories, Murray Hill, N.J., July 1997.
[100] R.-P. Ge and M. J. D. Powell, The convergence of variable metric matrices in unconstrained
optimization, Mathematical Programming, 27 (1983), pp. 123–143.
[101] J. Gilbert, Maintaining the positive deﬁniteness of the matrices in reduced secant methods for
equality constrained optimization, Mathematical Programming, 50 (1991), pp. 1–28.
[102] J. Gilbert and C. Lemar´echal, Some numerical experiments with variable-storage quasi-Newton
algorithms, Mathematical Programming, Series B, 45 (1989), pp. 407–435.
[103] J. Gilbert and J. Nocedal, Global convergence properties of conjugate gradient methods for
optimization, SIAM Journal on Optimization, 2 (1992), pp. 21–42.
[104] P. Gill, W. Murray, and M. H. Wright, Practical Optimization, Academic Press, 1981.
[105] P. E. Gill, G. H. Golub, W. Murray, and M. A. Saunders, Methods for modifying matrix
factorizations, Mathematics of Computation, 28 (1974), pp. 505–535.
[106] P. E. Gill and M. W. Leonard, Limited-memory reduced-Hessian methods for unconstrained
optimization, Numerical Analysis Report NA 97-1, University of California, San Diego, 1997.
[107] P. E. Gill and W. Murray, Numerically stable methods for quadratic programming, Mathematical
Programming, 14 (1978), pp. 349–372.
[108] P. E. Gill, W. Murray, and M. A. Saunders, User’s guide for SNOPT (Version 5.3): A FOR-
TRAN package for large-scale nonlinear programming, Technical Report NA 97-4, Department
of Mathematics, University of California, San Diego, 1997.
[109] P. E. Gill, W. Murray, M. A. Saunders, G. W. Stewart, and M. H. Wright, Properties of a
representation of a basis for the null space, Mathematical Programming, 33 (1985), pp. 172–186.
[110] P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright, User’s guide for SOL/QPSOL,
Technical Report SOL84–6, Department of Operations Research, Stanford University, Stanford,
California, 1984.
[111]
, User’s guide for NPSOL (Version 4.0): A FORTRAN package for nonlinear programming,
Technical Report SOL 86-2, Department of Operations Research, Stanford University, Stanford,
CA, 1986.
[112] P. E. Gill, W. Murray, M. A. Saunders, and M. H. Wright, Constrained nonlinear programming,
inOptimization,vol.1ofHandbooksinOperationsResearchandManagement,ElsevierScience
Publishers, Amsterdam, the Netherlands, 1989, pp. 171–210.

R e f e r e n c e s
615
[113] D. Goldfarb, Curvilinear path steplength algorithms for minimization which use directions of
negative curvature, Mathematical Programming, 18 (1980), pp. 31–40.
[114] D. Goldfarb and J. Forrest, Steepest edge simplex algorithms for linear programming,
Mathematical Programming, 57 (1992), pp. 341–374.
[115] G. H. Golub and C. F. Van Loan, Matrix Computations, The Johns Hopkins University Press,
Baltimore, 3rd ed., 1996.
[116] J. Gondzio, Multiple centrality corrections in a primal-dual method for linear programming,
Computational Optimization and Applications, 6 (1996), pp. 137–156.
[117] J. Goodman, Newton’s method for constrained optimization, Mathematical Programming, 33
(1985), pp. 162–171.
[118] N. I. M. Gould, On the accurate determination of search directions for simple differentiable penalty
functions, I.M.A. Journal on Numerical Analysis, 6 (1986), pp. 357–372.
[119]
, On the convergence of a sequential penalty function method for constrained minimization,
SIAM Journal on Numerical Analysis, 26 (1989), pp. 107–128.
[120]
, An algorithm for large scale quadratic programming, I.M.A. Journal on Numerical
Analysis, 11 (1991), pp. 299–324.
[121] N. I. M. Gould, S. Lucidi, M. Roma, and P. L. Toint, Solving the trust-region subproblem using
the Lanczos method. To appear in SIAM Journal on Optimization, 1998.
[122] A.Griewank,Achievinglogarithmicgrowthoftemporalandspatialcomplexityinreverseautomatic
differentiation, Optimization Methods and Software, 1 (1992), pp. 35–54.
[123]
, Automatic directional differentiation of nonsmooth composite functions, in Seventh
French-German Conference on Optimization, 1994.
[124]
, Computational Differentiation and Optimization, in Mathematical Programming: State
of the Art 1994, J. R. Birge and K. G. Murty, eds., The University of Michigan, Michigan, USA,
1994, pp. 102–131.
[125] A. Griewank and G. F. Corliss, eds., Automatic Differentition of Algorithms, SIAM Publications,
Philadelphia, Penn., 1991.
[126] A. Griewank, D. Juedes, and J. Utke, ADOL-C, A package for the automatic differentiation of
algorithms written in C/C++, ACM Transactions on Mathematical Software, 22 (1996), pp. 131–
167.
[127] A. Griewank and P. L. Toint, Local convergence analysis of partitioned quasi-Newton updates,
Numerische Mathematik, 39 (1982), pp. 429–448.
[128]
, On the unconstrained optimization of partially separable objective functions, in Nonlinear
Optimization 1981, M. J. D. Powell, ed., Academic Press, London, 1982, pp. 301–312.
[129]
,Partitionedvariablemetricupdatesforlargestructuredoptimizationproblems,Numerische
Mathematik, 39 (1982), pp. 119–137.
[130] J. Grimm, L. Pottier, and N. Rostaing-Schmidt, Optimal time and minimum space time product
for reversing a certain class of programs, in Computational Differentiation, Techniques, Appli-
cations, and Tools, M. Berz, C. Bischof, G. Corliss, and A. Griewank, eds., SIAM, Philadelphia,
1996, pp. 95–106.
[131] S. P. Han, Superlinearly convergent variable metric algorithms for general nonlinear programming
problems, Mathematical Programming, 11 (1976), pp. 263–282.
[132]
,Agloballyconvergentmethodfornonlinearprogramming,JournalofOptimizationTheory
and Applications, 22 (1977), pp. 297–309.
[133] Harwell Subroutine Library, Release 10, Advanced Computing Department, AEA Industrial
Technology, Harwell Laboratory, Oxfordshire, United Kingdom, 1990.

616
R e f e r e n c e s
[134] M. R. Hestenes, Multiplier and gradient methods, Journal of Optimization Theory and
Applications, 4 (1969), pp. 303–320.
[135] M. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving linear systems, Journal
of Research of the National Bureau of Standards, 49 (1952), pp. 409–436.
[136] N. J. Higham, Accuracy and Stability of Numerical Algorithms, SIAM Publications, Philadelphia,
1996.
[137] J.-B. Hiriart-Urruty and C. Lemar´echal, Convex Analysis and Minimization Algorithms,
Springer-Verlag, Berlin, New York, 1993.
[138] J. E. Dennis, Jr. and R. B. Schnabel, Numerical Methods for Unconstrained Optimization and
Nonlinear Equations, Prentice-Hall, Englewood Cliffs, N.J., 1983.
[139] P. Kall and S. W. Wallace, Stochastic Programming, John Wiley & Sons, New York, 1994.
[140] N. Karmarkar, A new polynomial-time algorithm for linear programming, Combinatorics, 4
(1984), pp. 373–395.
[141] C. T. Kelley, Iterative Methods for Linear and Nonlinear Equations, SIAM Publications,
Philadelphia, Penn., 1995.
[142] L. G. Khachiyan, A polynomial algorithm in linear programming, Soviet Mathematics Doklady,
20 (1979), pp. 191–194.
[143] H. F. Khalfan, R. H. Byrd, and R. B. Schnabel, A theoretical and experimental study of the
symmetric rank one update, SIAM Journal on Optimization, 3 (1993), pp. 1–24.
[144] V. Klee and G. J. Minty, How good is the simplex algorithm? in Inequalities, O. Shisha, ed.,
Academic Press, New York, 1972, pp. 159–175.
[145] H. W. Kuhn and A. W. Tucker, Nonlinear programming, in Proceedings of the Second Berkeley
Symposium on Mathematical Statistics and Probability, J. Neyman, ed., Berkeley, CA, 1951,
University of California Press, pp. 481–492.
[146] M. Lalee, J. Nocedal, and T. Plantenga, On the implementation of an algorithm for large-scale
equality constrained optimization, SIAM Journal on Optimization, (1998), pp. 682–706.
[147] S. Lang, Real Analysis, Addison-Wesley, Reading, MA, second ed., 1983.
[148] C. L. Lawson and R. J. Hanson, Solving Least Squares Problems, Prentice-Hall, Englewood Cliffs,
NJ, 1974.
[149] C. Lemar´echal, A view of line searches, in Optimization and Optimal Control, W. Oettli and
J. Stoer, eds., no. 30 in Lecture Notes in Control and Information Science, Springer-Verlag,
1981, pp. 59–78.
[150] K. Levenberg, A method for the solution of certain non-linear problems in least squares, Quarterly
of Applied Mathematics, 2 (1944), pp. 164–168.
[151] D. C. Liu and J. Nocedal, On the limited-memory BFGS method for large scale optimization,
Mathematical Programming, 45 (1989), pp. 503–528.
[152] D. Luenberger, Introduction to Linear and Nonlinear Programming, Addison Wesley, second ed.,
1984.
[153] Macsyma User’s Guide, second ed., 1996.
[154] O.L.Mangasarian,NonlinearProgramming,McGraw-Hill,NewYork,1969.ReprintedbySIAM
Publications, 1995.
[155] O. L. Mangasarian and L. L. Schumaker, Discrete splines via mathematical programming, SIAM
Journal on Control, 9 (1971), pp. 174–183.
[156] N. Maratos, Exact penalty function algorithms for ﬁnite dimensional and control optimization
problems, Ph.D. thesis, University of London, 1978.

R e f e r e n c e s
617
[157] H. M. Markowitz, Portfolio selection, Journal of Finance, 8 (1952), pp. 77–91.
[158]
,Theeliminationformoftheinverseanditsapplicationtolinearprogramming,Management
Science, 3 (1957), pp. 255–269.
[159]
, Portfolio Selection: Efﬁcient Diversiﬁcation of Investments, Basil Blackwell, Cambridge,
Mass., 1991.
[160] D. W. Marquardt, An algorithm for least squares estimation of non-linear parameters, SIAM
Journal, 11 (1963), pp. 431–441.
[161] D. Q. Mayne and E. Polak, A superlinearly convergent algorithm for constrained optimization
problems, Mathematical Programming Studies, 16 (1982), pp. 45–61.
[162] L. McLinden, An analogue of Moreau’s proximation theorem, with applications to the nonlinear
complementarity problem, Paciﬁc Journal of Mathematics, 88 (1980), pp. 101–161.
[163] N. Megiddo, Pathways to the optimal set in linear programming, in Progress in Mathematical
Programming: Interior-Point and Related Methods, N. Megiddo, ed., Springer-Verlag, New
York, N.Y., 1989, ch. 8, pp. 131–158.
[164] S. Mehrotra, On the implementation of a primal-dual interior point method, SIAM Journal on
Optimization, 2 (1992), pp. 575–601.
[165] S. Mizuno, M. Todd, and Y. Ye, On adaptive step primal-dual interior-point algorithms for linear
programming, Mathematics of Operations Research, 18 (1993), pp. 964–981.
[166] J. J. Mor´e, The Levenberg-Marquardt algorithm: Implementation and theory, in Lecture Notes in
Mathematics, No. 630–Numerical Analysis, G. Watson, ed., Springer-Verlag, 1978, pp. 105–116.
[167]
, Recent developments in algorithms and software for trust region methods, in Mathematical
Programming: The State of the Art, Springer-Verlag, Berlin, 1983, pp. 258–287.
[168]
,Acollectionofnonlinearmodelproblems,inComputationalSolutionofNonlinearSystems
of Equations, vol. 26 of Lectures in Applied Mathematics, American Mathematical Society,
Providence, R.I., 1990, pp. 723–762.
[169] J. J. Mor´e and D. C. Sorensen, On the use of directions of negative curvature in a modiﬁed Newton
method, Mathematical Programming, 16 (1979), pp. 1–20.
[170]
, Computing a trust region step, SIAM Journal on Scientiﬁc and Statistical Computing, 4
(1983), pp. 553–572.
[171]
, Newton’s method, in Studies in Numerical Analysis, vol. 24 of MAA Studies in
Mathematics, The Mathematical Association of America, 1984, pp. 29–82.
[172] J. J. Mor´e and D. J. Thuente, Line search algorithms with guaranteed sufﬁcient decrease, ACM
Transactions on Mathematical Software, 20 (1994), pp. 286–307.
[173] J. J. Mor´e and S. J. Wright, Optimization Software Guide, SIAM Publications, Philadelphia,
1993.
[174] W. Murray and M. H. Wright, Line search procedures for the logarithmic barrier function, SIAM
Journal on Optimization, 4 (1994), pp. 229–246.
[175] B. A. Murtagh and M. A. Saunders, MINOS 5.1 User’s guide, Technical Report SOL-83-20R,
Stanford University, 1987.
[176] K. G. Murty and S. N. Kabadi, Some NP-complete problems in quadratic and nonlinear
programming, Mathematical Programming, 19 (1987), pp. 200–212.
[177] S. G. Nash, Newton-type minimization via the Lanczos method, SIAM Journal on Numerical
Analysis, 21 (1984), pp. 553–572.
[178]
, SUMT (Revisited), Operations Research, 46 (1998), pp. 763–775.

618
R e f e r e n c e s
[179] G. L. Nemhauser and L. A. Wolsey, Integer and Combinatorial Optimization, John Wiley &
Sons, New York, 1988.
[180] A. S. Nemirovskii and D. B. Yudin, Problem complexity and method efﬁciency, John Wiley &
Sons, New York, 1983.
[181] Y.E.NesterovandA.S.Nemirovskii,InteriorPointPolynomialMethodsinConvexProgramming,
SIAM Publications, Philadelphia, 1994.
[182] G. N. Newsam and J. D. Ramsdell, Estimation of sparse Jacobian matrices, SIAM Journal on
Algebraic and Discrete Methods, 4 (1983), pp. 404–418.
[183] J. Nocedal, Updating quasi-Newton matrices with limited storage, Mathematics of Computation,
35 (1980), pp. 773–782.
[184]
, Theory of algorithms for unconstrained optimization, Acta Numerica, 1 (1992), pp. 199–
242.
[185] J. M. Ortega and W. C. Rheinboldt, Iterative solution of nonlinear equations in several variables,
Academic Press, New York and London, 1970.
[186] M. R. Osborne, Nonlinear least squares—the Levenberg algorithm revisited, Journal of the
Australian Mathematical Society, Series B, 19 (1976), pp. 343–357.
[187]
, Finite Algorithms in Optimization and Data Analysis, John Wiley & Sons, 1985.
[188] C. C. Paige and M. A. Saunders, LSQR: An algorithm for sparse linear equations and sparse least
squares, ACM Transactions on Mathematical Software, 8 (1982), pp. 43–71.
[189] E. R. Panier and A. L. Tits, On combining feasibility, descent and superlinear convergence in
inequality constrained optimization, Mathematical Programming, 59 (1993), pp. 261–276.
[190] C. H. Papadimitriou and K. Steiglitz, Combinatorial Optimization: Algorithms and Complexity,
Prentice Hall, Englewood Cliffs, NJ, 1982.
[191] R. J. Plemmons, Least squares computations for geodetic and related problems, in High Speed
Computing, R. Williamson, ed., University of Illinois Press, 1989, pp. 198–200.
[192] R. J. Plemmons and R. White, Substructuring methods for computing the nullspace of equilibrium
matrices, SIAM Journal on Matrix Analysis and Applications, 11 (1990), pp. 1–22.
[193] E. Polak, Optimization: Algorithms and Consistent Approximations, no. 124 in Applied
Mathematical Sciences, Springer, 1997.
[194] E. Polak and G. Ribi`ere, Note sur la convergence de m´ethodes de directions conjugu´ees, Revue
Franc¸aise d’Informatique et de Recherche Op´erationnelle, 16 (1969), pp. 35–43.
[195] M. J. D. Powell, A method for nonlinear constraints in minimization problems, in Optimization,
R. Fletcher, ed., Academic Press, New York, NY, 1969, pp. 283–298.
[196]
, A hybrid method for nonlinear equations, in Numerical Methods for Nonlinear Algebraic
Equations, P. Rabinowitz, ed., Gordon & Breach, London, 1970, pp. 87–114.
[197]
,Problemsrelatedtounconstrainedoptimization,inNumericalMethodsforUnconstrained
Optimization, W. Murray, ed., Academic Press, 1972, pp. 29–55.
[198]
, On search directions for minimization algorithms, Mathematical Programming, 4 (1973),
pp. 193–201.
[199]
, Convergence properties of a class of minimization algorithms, in Nonlinear Programming
2, O. L. Mangasarian, R. R. Meyer, and S. M. Robinson, eds., Academic Press, New York, 1975,
pp. 1–27.
[200]
, Some convergence properties of the conjugate gradient method, Mathematical Program-
ming, 11 (1976), pp. 42–49.

R e f e r e n c e s
619
[201]
, Some global convergence properties of a variable metric algorithm for minimization without
exact line searches, in Nonlinear Programming, SIAM-AMS Proceedings, Vol. IX, R. W. Cottle
and C. E. Lemke, eds., SIAM Publications, 1976, pp. 53–72.
[202]
, A fast algorithm for nonlinearly constrained optimization calculations, in Numerical
Analysis Dundee 1977, G. A. Watson, ed., Springer Verlag, Berlin, 1977, pp. 144–157.
[203]
, Restart procedures for the conjugate gradient method, Mathematical Programming, 12
(1977), pp. 241–254.
[204]
, Algorithms for nonlinear constraints that use Lagrangian functions, Mathematical
Programming, 14 (1978), pp. 224–248.
[205]
, The convergence of variable metric methods for nonlinearly constrained optimization
calculations, in Nonlinear Programming 3, Academic Press, New York and London, 1978,
pp. 27–63.
[206]
, On the rate of convergence of variable metric algorithms for unconstrained optimization,
Technical Report DAMTP 1983/NA7, Department of Applied Mathematics and Theoretical
Physics, Cambridge University, 1983.
[207]
, Nonconvex minimization calculations and the conjugate gradient method, Lecture Notes
in Mathematics, 1066 (1984), pp. 122–141.
[208]
, The performance of two subroutines for constrained optimization on some difﬁcult test
problems, in Numerical Optimization, P. T. Boggs, R. H. Byrd, and R. B. Schnabel, eds., SIAM
Publications, Philadelphia, 1984.
[209]
, Convergence properties of algorithms for nonlinear optimization, SIAM Review, 28 (1986),
pp. 487–500.
[210] M. J. D. Powell and P. L. Toint, On the estimation of sparse Hessian matrices, SIAM Journal on
Numerical Analysis, 16 (1979), pp. 1060–1074.
[211] D. Ralph and S. J. Wright, Superlinear convergence of an interior-point method for monotone
variational inequalities, in Complementarity and Variational Problems: State of the Art, SIAM
Publications, Philadelphia, Penn., 1997, pp. 345–385.
[212] Z. Ren and K. Moffatt, Quantitative analysis of synchotron Laue diffraction patterns in
macromolecular crystallography, Journal of Applied Crystallography, 28 (1995), pp. 461–481.
[213] J. M. Restrepo, G. K. Leaf, and A. Griewank, Circumventing storage limitations in variational
data assimilation studies, Preprint ANL/MCS-P515-0595, Mathematics and Computer Science
Division, Argonne National Laboratory, Argonne, Ill., 1995.
[214] K. Ritter, On the rate of superlinear convergence of a class of variable metric methods, Numerische
Mathematik, 35 (1980), pp. 293–313.
[215] S. M. Robinson, A quadratically convergent algorithm for general nonlinear programming
problems, Mathematical Programming, (1972), pp. 145–156.
[216] R. T. Rockafellar, The multiplier method of Hestenes and Powell applied to convex programming,
Journal of Optimization Theory and Applications, 12 (1973), pp. 555–562.
[217]
, Lagrange multipliers and optimality, SIAM Review, 35 (1993), pp. 183–238.
[218] J. B. Rosen and J. Kreuser, A gradient projection algorithm for nonlinear constraints, in Numerical
Methods for Non-Linear Optimization, F. A. Lootsma, ed., Academic Press, London and New
York, 1972, pp. 297–300.
[219] N.Rostaing,S.Dalmas,andA.Galligo,AutomaticdifferentiationinOdyssee,Tellus,45a(1993),
pp. 558–568.
[220] Y. Saad, Iterative Methods for Sparse Linear Systems, PWS Publishing Company, 1996.

620
R e f e r e n c e s
[221] Y. Saad and M. Schultz, GMRES: A generalized minimal residual algorithm for solving non-
symmetric linear systems, SIAM Journal on Scientiﬁc and Statistical Computing, 7 (1986),
pp. 856–869.
[222] K. Schittkowski, The nonlinear programming method of Wilson, Han and Powell with an
augmented Lagrangian type line search function, Numerische Mathematik, (1981), pp. 83–114.
[223] R. B. Schnabel and E. Eskow, A new modiﬁed Cholesky factorization, SIAM Journal on Scientiﬁc
Computing, 11 (1991), pp. 1136–1158.
[224] R. B. Schnabel and P. D. Frank, Tensor methods for nonlinear equations, SIAM Journal on
Numerical Analysis, 21 (1984), pp. 815–843.
[225] G. Schuller, On the order of convergence of certain quasi-Newton methods, Numerische
Mathematik, 23 (1974), pp. 181–192.
[226] G. A. Schultz, R. B. Schnabel, and R. H. Byrd, A family of trust-region-based algorithms for un-
constrained minimization with strong global convergence properties, SIAM Journal on Numerical
Analysis, 22 (1985), pp. 47–67.
[227] G. A. F. Seber and C. J. Wild, Nonlinear Regression, John Wiley & Sons, New York, 1989.
[228] D. F. Shanno and K. H. Phua, Remark on Algorithm 500: Minimization of unconstrained
multivariate functions, ACM Transactions on Mathematical Software, 6 (1980), pp. 618–622.
[229] A. Sherman, On Newton-iterative methods for the solution of systems of nonlinear equations,
SIAM Journal on Numerical Analysis, 15 (1978), pp. 755–771.
[230] D. D. Siegel, Implementing and modifying Broyden class updates for large scale optimization,,
Technical Report AMTP 1992/NA12, Department of Applied Mathematics and Theoretical
Physics, University of Cambridge, 1992.
[231] T. Steihaug, The conjugate gradient method and trust regions in large scale optimization, SIAM
Journal on Numerical Analysis, 20 (1983), pp. 626–637.
[232] J.Stoer,Ontherelationbetweenquadraticterminationandconvergencepropertiesofminimization
algorithms. Part I: Theory, Numerische Mathematik, 28 (1977), pp. 343–366.
[233] K. Tanabe, Centered Newton method for mathematical programming, in System Modeling and
Optimization: Proceedings of the 13th IFIP conference, vol. 113 of Lecture Notes in Control
and Information Systems, Berlin, 1988, Springer-Verlag, pp. 197–206.
[234] R. A. Tapia, Quasi-Newton methods for equality constrained optimization: Equivalence of existing
methods and a new implementation, in Nonlinear Programming 3 (O. Mangasarian, R. Meyer,
and S. Robinson, eds), Academic Press, New York, NY, (1978) pp. 125–164.
[235] M. J. Todd, Potential reduction methods in mathematical programming, Mathematical
Programming, Series B, 76 (1997), pp. 3–45.
[236] M. J. Todd and Y. Ye, A centered projective algorithm for linear programming, Mathematics of
Operations Research, 15 (1990), pp. 508–529.
[237] P. L. Toint, On sparse and symmetric matrix updating subject to a linear equation, Mathematics
of Computation, 31 (1977), pp. 954–961.
[238]
, Towards an efﬁcient sparsity exploiting Newton method for minimization, in Sparse
Matrices and Their Uses, Academic Press, New York, 1981, pp. 57–87.
[239]
, On large-scale nonlinear least squares calculations, SIAM Journal on Scientiﬁc and
Statistical Computing, 8 (1987), pp. 416–435.
[240] L. Vandenberghe and S. Boyd, Semideﬁnite programming, SIAM Review, 38 (1996), pp. 49–95.
[241] S. A. Vavasis, Nonlinear Optimization, Oxford University Press, New York and Oxford, 1991.
[242] H. Walker, Implementation of the GMRES method using Householder transformations, SIAM
Journal on Scientiﬁc and Statistical Computing, 9 (1989), pp. 815–825.

R e f e r e n c e s
621
[243] Waterloo Maple Software, Inc, Maple V software package, 1994.
[244] L. T. Watson, Numerical linear algebra aspects of globally convergent homotopy methods, SIAM
Review, 28 (1986), pp. 529–545.
[245] R. B. Wilson, A simplicial algorithm for concave programming, Ph.D. thesis, Graduate School of
Business Administration, Harvard University, 1963.
[246] W. L. Winston, Operations Research, Wadsworth Publishing Co., 3rd ed., 1997.
[247] P. Wolfe, The composite simplex algorithm, SIAM Review, 7 (1965), pp. 42–54.
[248] S. Wolfram, The Mathematica Book, Cambridge University Press and Wolfram Media, Inc.,
third ed., 1996.
[249] L. A. Wolsey, Integer Programming, Wiley–Interscience Series in Discrete Mathematics and
Optimization, John Wiley & Sons, New York, NY, 1998.
[250] M. H. Wright, Numerical Methods for Nonlinearly Constrained Optimization, Ph.D. thesis,
Stanford University, Stanford University, CA, 1976.
[251]
, Interior methods for constrained optimization, in Acta Numerica 1992, Cambridge
University Press, 1992, pp. 341–407.
[252]
, Ill-conditioning and computational error in interior methods for nonlinear programming,
SIAM Journal on Optimization, 9 (1999), pp. 84–111.
[253] S. J. Wright, Applying new optimization algorithms to model predictive control, in Chemical
Process Control-V, J. C. Kantor, ed., CACHE, 1997.
[254]
, On the convergence of the Newton/log-barrier method, Preprint ANL/MCS-P681-0897,
Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill.,
August 1997.
[255]
, Primal-Dual Interior-Point Methods, SIAM Publications, Philadelphia, Pa, 1997.
[256]
, Effects of ﬁnite-precision arithmetic on interior-point methods for nonlinear programming,
Preprint MCS–P705–0198, Mathematics and Computer Science Division, Argonne National
Laboratory, Argonne, Ill., January 1998.
[257] S. J. Wright and J. N. Holt, An inexact Levenberg-Marquardt method for large sparse nonlinear
least squares problems, Journal of the Australian Mathematical Society, Series B, 26 (1985),
pp. 387–403.
[258] S. J. Wright and F. Jarre, The role of linear objective functions in barrier methods, Preprint
MCS-P485-1294, Mathematics and Computer Science Division, Argonne National Laboratory,
Argonne, Ill., 1994. Revised 1998. To appear in Mathematical Programming, Series A.
[259] C. Zhu, R. H. Byrd, P. Lu, and J. Nocedal, Algorithm 778: L-BFGS-B, FORTRAN subroutines
for large scale bound constrained optimization, ACM Transactions on Mathematical Software, 23
(1997), pp. 550–560.

Index
Accumulation point, see Limit point
Active set, 331, 336, 345, 347, 353, 422
deﬁnition of, 327
Afﬁne scaling
direction, 398, 400, 405, 409
method, 417
Alternating variables method, 53, 104
Angle test, 47
Applications
design optimization, 1
ﬁnance, 7, 342
portfolio optimization, 1
transportation, 4, 7
Armijo line search, 38, 139, 141
Augmented Lagrangian function, 424–425
as merit function, 437
deﬁnition, 514
exactness of, 519–521
example, 515
Augmented Lagrangian method, 424, 491,
495, 526
convergence, 521
framework for, 515, 518
inequality constraints, 516–518
LANCELOT code, 513, 515, 516,
521–523
motivation, 513–515
Automatic differentiation, 136, 140, 165
adjoint variables, 180, 181
and graph-coloring algorithms, 184,
188–190
basis in elementary arithmetic, 176
checkpointing, 182
common expressions, 184
computational graph, 177–178, 180,
182, 183, 185, 187
computational requirements, 178–179,
182, 186, 188, 190
forward mode, 178–179, 285

624
I n d e x
Automatic (cont.)
forward sweep, 177, 180, 182, 185–187,
190
foundations in elementary arithmetic,
166
Hessian calculation
forward mode, 185–187
interpolation formulae, 186–187
reverse mode, 187–188
intermediate variables, 177–180, 184,
190
Jacobian calculation, 183–185
forward mode, 184
reverse mode, 184–185
limitations of, 188–189
nonlinear least-squares and, 252
reverse mode, 179–182
reverse sweep, 180–182, 190
seed vectors, 178, 179, 184, 185, 188
software, 166, 182, 189
Backtracking, 41
Barrier functions, 500
Barrier methods, 424
Barrier parameter, 424
Basic variables, 428
Basis, see Subspace, basis
Basis matrix, 428–430
BFGS method, 25, 31, 194–201
damping, 540
implementation, 200–201
properties, 199–200, 219
self-correction, 200
skipping, 201, 540, 550
Bound-constrained optimization, 96, 422,
476, 513
Boundary layer, 502
Boundedness
of sets, 578
Broyden class, 207
Broyden’s method, 279, 280, 292, 311, 592
derivation of, 286–288
limited-memory variants, 290
rate of convergence, 288–289
statement of algorithm, 288
Calculus of variations, 9
Cancellation error, see Floating-point
arithmetic, cancellation
Cauchy point, 68–77, 99, 159, 160, 263,
477
calculation of, 69–70, 95–96
for nonlinear equations, 299–300
role in global convergence, 87–89
Cauchy–Schwarz inequality, 98
Central path, 399–400, 402, 417, 509, 511
neighborhoods of, 402–404, 409, 412,
415, 484
Chain rule, 31, 166, 176, 178–180, 185,
243, 582, 583
Choleksy factorization
modiﬁed, 155
Cholesky factorization, 82, 199, 201, 219,
256–257, 264, 296, 300, 602–604,
610
incomplete, 158
modiﬁed, 141, 144–150, 162, 163
bounded modiﬁed factorization
property, 141
Gershgorin modiﬁcation, 150
sparse, 408–409
stability of, 147, 610
Complementarity, 399
Complementarity condition, 78, 323, 328,
344, 508
strict, 328, 330, 348–350, 508, 518
Complexity of algorithms, 392, 395, 396,
410, 415–417
Conditioning, see also Matrix, condition
number, 426, 429, 430, 432,
608–609
ill conditioned, 31, 495, 505, 510, 514,
608, 609
well conditioned, 608, 609
Cone, 579
Cone of feasible directions, see Tangent
cone
Conjugacy, 102
Conjugate direction method, 102
expanding subspace minimization, 106
termination of, 103
Conjugate gradient method, 75, 101–132,
135, 139, 154, 163, 270, 285
n-step quadratic convergence, 132

I n d e x
625
relation to limited memory, 227
VA14, 229
clustering, 115
condition number, 117
CONMIN, 124, 229
expanding subspace, 76, 112
Fletcher–Reeves, see Fletcher–Reeves
method
global convergence, 46
Hestenes–Stiefel, 122
Krylov subspace, 112
nonlinear, 26, 120–131
numerical performance, 124
optimal polynomial, 113
optimal process, 112
Polak–Ribi`ere, see Polak–Ribi`ere
method
practical version, 111
preconditioned, 118, 154
rate of convergence, 112
restarts, 122
superlinear convergence, 132
superquadratic, 132
termination, 114, 122
Constrained optimization, 6
linear, 422
nonlinear, 6, 184, 363, 421, 422, 491,
492, 494, 509
Constraint qualiﬁcations, 328, 336–339,
345, 351–353, 357
linear independence (LICQ), 328, 329,
336, 337, 339, 341, 343, 348, 351,
353, 355, 359, 360, 366, 454, 497,
498, 505, 508, 519
Mangasarian–Fromovitz (MFCQ), 353,
359
Constraints, 1, 2, 319, 421
bounds, 434, 511, 516, 522
equality, 315
hard and soft, 423
inequality, 315
Continuation methods for nonlinear
equations, 280, 312
convergence of, 308–310
formulation as initial-value ODE,
306–307
motivation, 304–305
predictor–corrector method, 307–308
zero path, 305–307, 309, 310, 312
divergence of, 309–310
tangent, 306–307, 309
turning point, 305, 308
Convergence, rate of, 28–30, 162
n-step quadratic, 132
linear, 28, 29, 137, 267
quadratic, 24, 29, 30, 32, 138, 142, 262
sublinear, 32
superlinear, 8, 24, 29, 30, 32, 71, 132,
138, 198, 200, 218, 220, 267, 269,
311, 409, 525
superquadratic convergence, 132
Convex programming, 6, 8, 347, 350–351,
422, 507
Convexity, 8
of functions, 8, 17, 31, 135, 163, 256, 350
of sets, 8, 31, 350, 358
Coordinate descent method, 53, see
Alternating variables method
Coordinate relaxation step, 430
Data-ﬁtting problems, 12–13, 254
Degeneracy, 328, 455
of linear program, 373
Dennis and Mor´e characterization, 50
Descent direction, 22, 31, 35
DFP method, 197
Differential equations
ordinary, 307
partial, 188, 310
Directional derivative, 178, 179, 583–584
Discrete optimization, 4–5, 423
Dual variables, see also Lagrange
multipliers, 437
Duality, 357
in linear programming, 367–370
Eigenvalues, 79, 258, 349, 598, 605
negative, 74, 94
of symmetric matrix, 504, 599
Eigenvectors, 79, 258, 598
Element function, 235
Elimination of Variables, 425
linear equality constraints, 427–434
nonlinear, 426–427

626
I n d e x
Elimination (cont.)
when inequality constraints are present,
434
Ellipsoid algorithm, 392, 395, 416
Error
absolute, 606
relative, 167, 257, 258, 606, 609
truncation, 188
Errors-in-variables models, 271
Feasible sequences, 332–339, 343–345, 347
limiting directions of, 332–339,
341–342, 345–346, 351
Feasible set, 3, 315, 317, 350, 351, 491
geometric properties of, 351, 353–357,
359, 586–590
primal, 365
primal–dual, 397, 398, 402, 409, 415
strictly, 500, 507, 509
Finite differencing, 136, 140, 165–176, 188,
274, 285
and graph-coloring algorithms, 172–176
central-difference formula, 165,
168–169, 173, 174, 189
forward-difference formula, 167, 168,
173, 174, 189
gradient approximation, 166–169
Hessian approximation, 173–176
Jacobian approximation, 169–173, 290
First-order feasible descent direction,
322–327
First-order optimality conditions, see
also Karush–Kuhn–Tucker (KKT)
conditions, 86, 282, 319–342, 351,
354, 358, 498
derivation of, 331–342
examples, 319–327, 329–330, 332–335
fundamental principle, 335
unconstrained optimization, 15–16, 437
Fixed-regressor model, 253
Fletcher–Reeves method, 101, 120–131
convergence of, 124
numerical performance, 124
Floating-point arithmetic, 188, 607–609
cancellation, 430, 607–608
double-precision, 607
roundoff error, 167, 189, 257, 499, 505,
525, 607
unit roundoff, 167, 189, 607
Floating-point numbers, 606
exponent, 607
fractional part, 607
Forcing sequence, see Newton’s method,
inexact, forcing sequence
Function
continuous, 580
continuously differentiable, 582
derivatives of, 581–585
differentiable, 582
Lipschitz continuous, 581, 586
Lipschitz continuously differentiable,
585, 586
locally Lipschitz continuous, 581
one-sided limit, 580
univariate, 581, 583
Fundamental theorem of algebra, 520, 598
Gauss–Newton method, 259–263, 267,
269, 272, 274, 282
connection to linear least squares, 260
line search in, 259, 260
performance on large-residual
problems, 267
Gaussian elimination, 144, 429, 603, 604
sparse, 136, 429, 434
stability of, 610
with row partial pivoting, 601–602,
609–610
Global convergence, 87–94, 263, 280
Global minimizer, 13–14, 17, 422, 496, 507
Global optimization, 6, 8
Global solution, see also Global minimizer,
6, 78, 84–87, 316, 347, 350, 358
GMRES algorithm, 285
Goldstein condition, 41, 139, 141
Gradient, 582
generalized, 18
Gradient–projection method, 453,
476–481, 486, 522
Group partial separability, see Partially
separable function, group partially
separable
H¨older inequality, 595
Harwell subroutine library
VA14, 123

I n d e x
627
Hessian, 15, 20, 24, 26, 582
average, 196, 197
Homotopy map, 304
Homotopy methods, see Continuation
methods for nonlinear equations
Implicit function theorem, 337, 338, 355,
585–586, 588
Inertia of a matrix, 151, 447, 475
Inexact Newton method, see Newton’s
method, inexact
Integer programming, 5
branch-and-bound algorithm, 5
Integral equations, 310
Interior-point methods, see Primal–dual
interior-point methods, 392
Interlacing eigenvalue theorem, 605–606
Invariant subspace, see Partially separable
optimization, invariant subspace
Jacobian, 252, 256, 260, 261, 274, 338, 398
Karmarkar’s algorithm, 392, 396, 416
Karush–Kuhn–Tucker (KKT) conditions,
342, 343, 345, 347, 348, 353, 357,
359, 360, 422, 475, 497, 498,
507–510, 512, 518, 519, 522, 527
for general constrained problem, 328
for linear programming, 366–367, 374,
375, 397, 399, 402, 410, 411
Krylov subspace, 108
Lagrange multipliers, 321, 322, 330–331,
339, 342, 345, 348, 349, 353, 359,
366–368, 419, 422, 510
estimates of, 497, 504, 508, 509, 513,
514, 521, 524
Lagrangian function, 86, 321, 323, 325,
342, 343, 347, 508
for constrained optimization, 327
for linear program, 366, 367
Hessian of, 342–345, 347, 348, 366
projected Hessian of, 349
LANCELOT, see Augmented Lagrangian
method, LANCELOT code
Lanczos method, 74
LAPACK, 601
Least-squares problems, linear, 256–259
applications of, 273
LSQR algorithm, 270
normal equations, 256–257, 264, 269,
275, 408
solution via QR factorization, 257
solution via SVD, 257–258
Least-squares problems, nonlinear, 13, 183
applications of, 251, 253–254
Dennis–Gay–Welsch algorithm,
267–269
Fletcher–Xu algorithm, 267
large-residual problems, 266–269
large-scale problems, 269–270
Levenberg–Marquardt method, see
Levenberg–Marquardt method
scaling of, 266
software for, 268, 274
statistical justiﬁcation of, 255
structure of objective, 252, 259
Least-squares problems, total, 271
Level set, 94, 263
Levenberg–Marquardt method, 262–266,
269, 272
as trust-region method, 262–264, 300
for nonlinear equations, 300
implementation via orthogonal
transformations, 264–266
inexact, 270
local convergence of, 266
performance on large-residual
problems, 267
lim inf, lim sup, 578
Limit point, 31, 89, 94, 98, 496, 497, 507,
577–578
Limited memory method, 25, 224–233,
247
compact representation, 230–232
for nonlinear equations, 247
L-BFGS, 224–233
L-BFGS algorithm, 226
memoryless BFGS method, 227
performance of, 227
relation to CG, 227
scaling, 226
SR1, 232
two-loop recursion, 225
Line search, see also Step length selection

628
I n d e x
Line (cont.)
Armijo, 38
backtracking, 41
curvature condition, 38
for log-barrier function, 506, 510, 526
Goldstein, 41
inexact, 37
Newton’s method with, 22–24
Nonlinear conjugate gradient methods
with, 26
quasi-Newton methods with, 24–25
search directions, 21–26
strong Wolfe, 39
sufﬁcient decrease, 37
Wolfe conditions, 37
Line search method, 19–20, 35–55, 65, 66,
69, 252
for nonlinear equations, 278, 293–298
global convergence of, 294–297
local convergence of, 298
poor performance of, 296
Linear complementarity problem, 410–411
Linear programming, 4, 6, 9, 301, 351, 421,
491, 512
artiﬁcial variables, 370, 386–389
basic feasible points, 370–374
dual problem, 367–370
feasible polytope, 364
vertices of, 372–373
fundamental theorem of, 371–372
primal solution set, 364
slack/surplus variables, 365, 368, 370,
388, 411
splitting variables, 365, 368
standard form, 364–365, 411
Linearly dependent, 350, 586
Linearly independent, 353, 431, 497, 498,
519, 521
Lipschitz continuity, see also Function,
Lipschitz continuous, 281–284,
286, 294, 295, 298, 301, 302, 311
Local minimizer, 13, 15, 280, 527
isolated, 14, 31
strict, 13, 15, 16, 31, 519
weak, 13
Local solution, see also Local minimizer,
316–317, 330, 332, 335, 341, 343,
345, 350, 354, 358, 437
isolated, 317
strict, 317, 345, 347, 348
strong, 317
Log-barrier function, 417, 424, 525–527
deﬁnition, 500–501
difﬁculty of minimizing, 502, 510
examples, 501–504
ill conditioned Hessian of, 504–505
possible unboundedness, 507
properties, 507–509
Log-barrier method, 491, 505–506
convergence of, 508–509
extrapolation, 506
modiﬁcation for equality constraints,
509–510
relationship to primal–dual
interior-point methods, 506,
510–512
LSQR method, 449, 485, 536
LU factorization, 601–602
Maratos effect, 436, 550, 558, 567–573
example of, 567
remedies, 569
Matrix
condition number, 257, 596, 604, 609
determinant, 600
diagonal, 258, 408, 429
full-rank, 306, 308, 309, 498, 604
indeﬁnite, 73, 74
lower triangular, 601, 602
nonsingular, 338, 350, 596, 605
null space, 306, 337, 348, 430, 431, 598,
602, 604
orthogonal, 257, 258, 350, 432, 598, 603
permutation, 257, 429, 601
positive deﬁnite, 16, 23, 30, 67, 74, 75,
349, 594
positive semideﬁnite, 16, 78, 349, 410,
411, 594
range space, 430, 598
rank-deﬁcient, 259
rank-one, 25
rank-two, 25
singular, 350

I n d e x
629
symmetric, 25, 67, 408, 411, 594
symmetric indeﬁnite, 409
symmetric positive deﬁnite, 602
trace, 599–600
upper triangular, 257, 350, 601–603
Matrix, sparse, 408, 409, 602, 603
Cholesky factorization, 409
Maximum likelihood estimate, 255
Merit function, see also Penalty function,
422, 425, 434–438
ℓ1, 301, 435–437, 544–547, 558
choice of parameter, 547
ℓ2, 526
exact, 435–437
deﬁnition of, 435
nonsmoothness of, 436–437
smooth, 513
Fletcher’s augmented Lagrangian,
435–436, 544–547
choice of parameter, 547
for feasible methods, 434
for nonlinear equations, 279, 292–294,
296, 298, 299, 301, 304, 310–312,
498
for primal–dual interior-point methods,
512
for SQP, 544–547
Method of multipliers, see Augmented
Lagrangian methods
Minimum surface problem, 238, 244, 246
MINOS, see Sequential linearly
constrained methods, MINOS
Modeling, 1–2, 9, 12, 253–255
Negative curvature direction, 73, 75, 76,
139–143, 156, 157, 161–163, 470,
471, 480
Neighborhood, 13, 15, 31, 579
Network optimization, 365
Newton’s method, 26, 252, 259, 261, 267
for log-barrier function, 502, 505, 525,
526
for nonlinear equations, 278, 280–284,
288, 290, 292, 293, 295–298, 302,
304, 308, 311
cycling, 292
inexact, 284–286, 297
for quadratic penalty function, 495, 499
global convergence, 45
Hessian-free, 140, 156
in one variable, 78, 81, 93, 592
inexact, 136–138, 156, 162, 185
forcing sequence, 136–138, 140, 156,
162, 285
Lanczos, 157
large scale, 135–162
LANCELOT, 159
line search method, 139–142
MINPACK-2, 159
trust-region method, 154
modiﬁed, 141–142
adding a multiple of I, 144
eigenvalue modiﬁcation, 143–144
Newton–CG, 136, 139–141, 156–159,
161–163, 173
preconditioned, 157–159
rate of convergence, 29, 51, 137–138,
155, 159, 282–284, 288–289, 298
scale invariance, 27
Newton–Lagrange method, see Sequential
quadratic programming
Nondifferentiable optimization, 513
Nonlinear complementarity problems, 417
Nonlinear equations, 169, 183, 185, 423,
592
degenerate solution, 281, 283, 290, 292,
311
examples of, 278–279, 296, 310
merit function, see Merit function, for
nonlinear equations
multiple solutions, 279–280
primal–dual interior-point methods,
relation to, 398, 511
quasi-Newton methods, see Broyden’s
method
relationship to least squares, 278–279,
282, 298, 300–301, 311
relationship to optimization, 278
solution, 278
statement of problem, 277–278
Nonlinear least-squares, see Least-squares
problem, nonlinear
Nonlinear programming, see Constrained
optimization, nonlinear, 301
Nonmonotone algorithms, 19

630
I n d e x
Nonnegative orthant, 97
Nonsmooth functions, 6, 18, 317, 318, 358
Norm
Euclidean, 26, 144, 257, 288, 311, 595,
596, 599, 604
Frobenius, 144, 196, 197, 596
matrix, 595–596
consistent with vector norms, 596
vector, 594–595
Normal cone, 354–357, 587–590
Normal distribution, 255
Normal step, 556
Null space, see Matrix, null space
Numerical analysis, 363
Objective function, 1, 2, 11, 315, 421
One-dimensional minimization, 19, 55
Optimal control, 586
Optimality conditions, see also First-order
optimality conditions, Second-
order optimality conditions, 2, 8,
315–316
for unconstrained local minimizer,
15–17
Order notation, 591–592
Orthogonal distance regression, 271–273
contrast with least squares, 271–272
structure of objective, 272–273
Orthogonal transformations, 257, 264–266
Givens, 264, 604
Householder, 264, 604
Outliers, 267
Partially separable function, 25, 183,
235–237, 270
automatic detection, 183, 241
deﬁnition, 183, 241
group partially separable, 243
vs. sparsity, 242
Partially separable optimization, 235–247
BFGS, 246
compactifying matrix, 236
element variables, 236
internal variables, 237, 239
invariant subspace, 239, 240
Newton’s method, 244
quasi-Newton method, 237, 245
SR1, 246
Penalty function, see also Merit function,
492
exact, 424, 491, 512–513
ℓ1, 512–513
quadratic, 424, 492–494, 504, 505, 509,
525, 526
difﬁculty of minimizing, 495
Hessian of, 498–499
relationship to augmented
Lagrangian, 513
Penalty methods, 424
exterior, 492
Penalty parameter, 435, 492, 514, 522–524
Pivoting, 257, 610
Polak–Ribi`ere method, 121
convergence of, 130
Polak–Ribi`ere method
numerical performance, 124
Portfolio optimization, 442–443, 485
Preconditioners, 118–120
banded, 119
incomplete Cholesky, 119
SSOR, 119
Primal–dual interior-point methods, 475,
491, 525, 527
centering parameter, 400, 401, 403–405,
409
complexity of, 396, 410, 415–416
contrasts with simplex method, 364, 396
convex quadratic programs, 410–411
corrector step, 404–406, 409
duality measure, 400
infeasible-interior-point algorithms,
401–404
linear algebra issues, 408–409
Mehrotra’s predictor–corrector
algorithm, 396, 404–408, 483
nonlinear programs, 411, 510–512, 526
path-following algorithms, 402–409,
484
long-step, 411–416
predictor–corrector (Mizuno-
Todd-Ye) algorithm,
409
short-step, 409
potential function, 410
Tanabe–Todd–Ye, 410, 484

I n d e x
631
potential-reduction algorithms,
409–410, 484
predictor step, 405–406, 409
quadratic programming, 481–484
relationship to Newton’s method, 397,
398, 402
software, 417
Probability density function, 255
Projected Hessian, 564
two-sided, 565
QMR method, 449, 485, 536
QR factorization, 257, 264, 297, 300, 307,
349, 432, 434, 603–605
cost of, 603
relationship to Cholesky factorization,
604
Quadratic penalty method, 491, 494–495,
513
convergence of, 495–500
Quadratic programming, 422, 425,
441–486
active set methods, 457–476
big M method, 463
blocking constraint, 459
convex, 491
cycling, 467
duality, 484
indeﬁnite, 470–476
inertia controlling methods, 470–474
inertia controlling method, 486
initial working set, 465
interior-point method, 481–484
null-space method, 450–452
optimal active set, 457
optimality conditions, 454
phase I, 462
pseudo-constraint, 471
range-space method, 449–450
termination, 466
updating factorizations, 467
working set, 457–467
Quasi-Newton approximate Hessian, 24,
25, 71, 522, 592
Quasi-Newton method, see also
Limited-memory method, 26, 252,
267, 495, 502
BFGS, see BFGS method, 267
bounded deterioration, 219
Broyden class, see Broyden class
curvature condition, 195
DFP, see DFP method, 247, 269
for nonlinear equations, see Broyden’s
method
for partially separable functions, 25
global convergence, 45
large-scale, 223–247
limited memory, see Limited memory
method
rate of convergence, 29, 49
secant equation, 24, 25, 195, 197,
268–269, 287, 592
sparse, see Sparse quasi-Newton method
SR1, see SR1 method
symmetric-rank-one (SR1), 25
Range space, see Matrix, range space
Relative interior, 590
Residuals, 12, 251, 260, 266–269, 274
vector of, 18, 169, 252
Robustness, 7
Root, see Nonlinear equations, solution
Root-ﬁnding algorithm, 265
Rootﬁnding algorithm, see also Newton’s
method, in one variable, 264,
592–593
for trust-region subproblem, 78–83
Rosenbrock function
extended, 248
Roundoff error, see Floating-point
arithmetic, roundoff error
Row echelon form, 429
Sℓ1QP method, 301, 557–560
Saddle point, 30, 94
Scale invariance, 196, 199
Scaling, 27–28, 94–97, 331, 502, 504
example of poor scaling, 27
matrix, 95
scale invariance, 28
Schur complement, 152
Secant method, see also Quasi-Newton
method, 287, 592–593
Second-order correction, 558, 569–571
Second-order optimality conditions, 330,
342–350, 597

632
I n d e x
Second-order (cont.)
necessary, 94, 343–349
sufﬁcient, 345–349, 508, 519
unconstrained optimization, 16–17
Semideﬁnite programming, 411, 491
Sensitivity, 582, 609
Sensitivity analysis, 2, 166, 258, 330–331,
357, 369
Separable problem, 235
Sequential linearly constrained methods,
424, 425, 491, 523–526
MINOS, 524–526
Sequential quadratic programming, 425,
475, 513, 524, 529–573
augmented Lagrangian Hessian, 541
Coleman–Conn method, 543, 549, 551
derivation, 530–533
full quasi-Newton Hessian, 540
identiﬁcation of optimal active set, 533
IQP vs. EQP, 534
KKT system, 282, 531
least-squares multipliers, 537
line search algorithm, 547
local algorithm, 532
QP multipliers, 537
rate of convergence, 563–567
reduced-Hessian approximation,
542–544
reduced-Hessian method, 538, 548–553
properties, 549
Sℓ1QP method, see Sℓ1QP method
shifting constraints, 555
step computation, 536–539
direct, 536
for inequalities, 538
iterative, 536
null-space, 537
range-space, 536
tangential convergence, 549
trust-region method, 553–563
two elliptical constraints, 556
Set
afﬁne hull of, 579
closed, 578
closure of, 578
compact, 579
interior of, 578
open, 578
relative interior of, 579
Sherman–Morrison–Woodbury formula,
197, 198, 202, 220, 290, 385, 605
Simplex method
as active-set method, 391–392
basic index set B, 370–375, 386
complexity of, 392
cycling, 389
avoidance of, 389–391
degenerate steps, 378
description of single iteration, 374–378
discovery of, 363
entering index, 375–376, 378, 383–386
ﬁnite termination of, 377–378
initialization, 386–389
leaving index, 375, 376, 378
linear algebra issues, 378–383
Phase I/Phase II, 386–389
pricing, 375, 383–384
multiple, 384
partial, 383
steepest-edge rule, 384–386
Singular values, 260, 598
Singular-value decomposition (SVD), 258,
274, 275, 311, 598
Slack variables, see also Linear
programming, slack/surplus
variables, 510, 516, 517, 521
Smooth functions, 11, 15, 317–319, 342
SNOPT, 538
Sparse quasi-Newton method, 233–235,
247
SR1 method, 202, 219
algorithm, 204
properties, 205
safeguarding, 203
skipping, 203, 218
Stability, 608–610
Starting point, 19
Stationary point, 8, 16, 30, 296, 437, 498
Steepest descent direction, 21, 22, 68, 72,
145
Steepest descent method, 22, 26, 27, 35, 71,
94, 502
rate of convergence, 29, 47, 49
Step length, 19, 35

I n d e x
633
unit, 23, 31
Step length selection, see also Line search,
55–61
bracketing phase, 55
cubic interpolation, 57
for Wolfe conditions, 58
initial step length, 58
interpolation in, 56
selection phase, 55
Stochastic optimization, 7
Strict complementarity, see
Complementarity condition, strict
Subgradient, 18
Subspace, 349
basis, 430, 597
orthonormal, 433
dimension, 597
linearly independent set, 597
spanning set, 597
Successive linear programming, 534
Sufﬁcient reduction, 69, 70, 89
Sum of absolute values, 254
Sum of squares, see Least-squares problem,
nonlinear
Symbolic differentiation, 166
Symmetric indeﬁnite factorization, 448,
475, 476
Bunch–Kaufman, 153
Bunch–Parlett, 152
modiﬁed, 151–154, 162
sparse, 153
Symmetric rank-one update, see SR1
method
Tangent, 506
Tangent cone, 339, 354–357, 587–590
Tangential step, 560
Taylor series, 16, 23, 30, 31, 66, 67, 281,
320, 342, 344, 345, 495, 502, 505,
527, 587
Taylor’s theorem, 16, 21, 22, 24, 90, 122,
137, 138, 165–169, 173, 174, 196,
281, 287, 302, 335, 337, 338, 344,
346, 354–356, 585
statement of, 15
Tensor methods, 280
computational results, 292
derivation, 290–292
performance on degenerate problems,
292
Termination criterion, 93
Triangular substitution, 433, 601, 603, 609,
610
Truncated Newton method, see Newton’s
method, Newton–CG
Trust region
boundary, 68, 72, 73, 76, 77, 94
box-shaped, 20, 301
choice of size for, 65–66, 91
elliptical, 20, 66, 95, 96, 99
radius, 20, 26, 67–69, 71, 262, 302
spherical, 94, 262
Trust-region method, 19–20, 68, 81–82,
87, 89, 90, 92–94, 252, 262, 592
contrast with line search method, 20, 65
dogleg method, 68, 71–74, 77, 78, 87,
89, 93, 98, 154–155, 161, 299–301
double-dogleg method, 98
for log-barrier function, 506
for nonlinear equations, 278, 279, 293,
298–304, 311
global convergence of, 300–302
local convergence of, 302–304
global convergence, 69, 70, 74, 76, 77,
87–94, 155, 156
local convergence, 159–162
Newton variant, 26–27, 67, 77, 94
software, 97
Steihaug’s approach, 68, 75–77, 87, 89,
93, 480
strategy for adjusting radius, 68
subproblem, 20, 26–27, 67, 70, 71, 74,
87, 93, 95–97, 262, 263
approximate solution of, 67, 69
exact solution of, 77–78
hard case, 82–84
nearly exact solution of, 68–69, 74,
78–87, 89, 97, 156, 162, 300–301
two-dimensional subspace
minimization, 68, 74, 78, 87, 89,
97, 99, 154–155
Unconstrained optimization, 6, 11–30,
350, 358, 426, 432, 493, 495, 502,
513
Unit ball, 86, 520

634
I n d e x
Unit roundoff, see Floating-point
arithmetic, unit roundoff
Variable metric method, see Quasi-Newton
method
Variable storage method, see Limited
memory method
Watchdog technique, 569–573
Weakly active, 331
Wolfe conditions, 37–41, 87, 131, 139, 141,
194, 195, 198–201, 204, 218, 226,
260, 294, 295, 298
scale invariance of, 41
strong, 39, 40, 121, 122, 124–128, 195,
200, 220, 226
Zoutendijk condition, 43–46, 127, 142,
214, 295


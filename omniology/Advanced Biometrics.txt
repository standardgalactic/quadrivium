David Zhang · Guangming Lu
Lei Zhang
Advanced 
Biometrics
www.ebook3000.com

Advanced Biometrics

David Zhang • Guangming Lu • Lei Zhang
Advanced Biometrics
www.ebook3000.com

David Zhang
The Hong Kong Polytechnic University
Hong Kong, China
Guangming Lu
Shenzhen Graduate School
Harbin Institute of Technology
Shenzhen, China
Lei Zhang
The Hong Kong Polytechnic University
Hong Kong, China
ISBN 978-3-319-61544-8
ISBN 978-3-319-61545-5
(eBook)
DOI 10.1007/978-3-319-61545-5
Library of Congress Control Number: 2017946764
© Springer International Publishing AG 2018
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of
the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or
dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt
from the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with
regard to jurisdictional claims in published maps and institutional afﬁliations.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer International Publishing AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Biometric technologies, such as ﬁngerprint, face, iris, and palmprint, have been
well studied and introduced in many research books. However, these technologies
have their own advantages and disadvantages, and there is not one kind of biometric
technology that can be ﬁt for different applications. Many new biometric technol-
ogies have been developed in recent years, especially for some new applications.
This book describes some new biometric technologies, such as High-Resolution
ﬁngerprint, Finger-Knuckle-Print, Hand back skin texture, 3D ﬁngerprint,
Tongueprint, and 3D ear. There are 15 chapters, and except the overview chapter
(Chap. 1) and recapitulation chapter (Chap. 15), the other 13 chapters are divided
into four parts, including High-Resolution Fingerprint Recognition, Finger-
Knuckle-Print Veriﬁcation, Other Hand-Based Biometrics, and Some New Head-
Based Biometrics. Many efﬁcient feature extraction, matching, and fusion algo-
rithms are introduced, and some potential systems have been developed in
this book.
A comprehensive introduction to both theoretical issues and practical imple-
mentation in biometric authentication is given in this book. It will serve as a
textbook or as a useful reference for graduate students and researchers in the ﬁelds
of computer science, electrical engineering, systems science, and information
technology. Researchers and practitioners in industry and R&D laboratories work-
ing on security system design, biometrics, immigration, law enforcement, control,
and pattern recognition will also ﬁnd much of interest in this book.
The work is supported by the NSFC funds under project No. 61271344 and
61332011, Shenzhen Fundamental Research fund JCYJ20150403161923528,
JCYJ20140508160910917, and Key Laboratory of Network Oriented Intelligent
Computation, Shenzhen, China.
Hong Kong, China
David Zhang
Harbin, China
Guangming Lu
Hong Kong, China
Lei Zhang
Feb 2017
v
www.ebook3000.com

Contents
1
Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Why Biometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
History of Biometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Biometrics Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.4
Recent Biometrics Development . . . . . . . . . . . . . . . . . . . . . . .
7
1.4.1
Multi-biometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4.2
3D Biometrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4.3
Multispectral Biometrics . . . . . . . . . . . . . . . . . . . . . .
9
1.5
Arrangement of This Book . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
Part I
High Resolution Fingerprint Recognition
2
High Resolution Partial Fingerprint Alignment . . . . . . . . . . . . . . .
15
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2.1.1
High Resolution Partial Fingerprint . . . . . . . . . . . . . . .
17
2.1.2
Fingerprint Alignment . . . . . . . . . . . . . . . . . . . . . . . .
18
2.1.3
Partial Fingerprint Alignment Based on Pores . . . . . . .
19
2.2
Feature Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.2.1
Ridge and Valley Extraction . . . . . . . . . . . . . . . . . . . .
20
2.2.2
Pore Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
2.2.3
Pore-Valley Descriptors . . . . . . . . . . . . . . . . . . . . . . .
22
2.3
PVD-Based Partial Fingerprint Alignment . . . . . . . . . . . . . . . .
24
2.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.4.1
The Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.4.2
The Neighborhood Size and Sampling Rate . . . . . . . . .
29
2.4.3
Corresponding Feature Point Detection . . . . . . . . . . . .
30
2.4.4
Alignment Transformation Estimation . . . . . . . . . . . . .
32
vii

2.4.5
Partial Fingerprint Recognition . . . . . . . . . . . . . . . . . .
34
2.4.6
Computational Complexity Analysis . . . . . . . . . . . . . .
37
2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
3
Fingerprint Pore Modeling and Extraction . . . . . . . . . . . . . . . . . . .
41
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.2
Review of Existing Pore Extraction Methods . . . . . . . . . . . . . .
43
3.3
Dynamic Anisotropic Pore Model (DAPM) . . . . . . . . . . . . . . .
44
3.4
Adaptive Pore Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46
3.4.1
DAPM Parameter Estimation . . . . . . . . . . . . . . . . . . .
46
3.4.2
Implementation Issues . . . . . . . . . . . . . . . . . . . . . . . .
47
3.4.3
The Pore Extraction Algorithm . . . . . . . . . . . . . . . . . .
49
3.5
Experiments and Performance Evaluation . . . . . . . . . . . . . . . .
52
3.5.1
Pore Detection Accuracy . . . . . . . . . . . . . . . . . . . . . .
53
3.5.2
Pore Based Partial-Fingerprint Recognition . . . . . . . . .
55
3.5.3
Pore Based Full-Size Fingerprint Recognition . . . . . . .
58
3.5.4
Computational Complexity Analysis . . . . . . . . . . . . . .
59
3.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
4
The Resolution for Fingerprint Recognition . . . . . . . . . . . . . . . . . .
65
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
4.2
Multiresolution Fingerprint Images Collecting . . . . . . . . . . . . .
67
4.2.1
Acquisition Device . . . . . . . . . . . . . . . . . . . . . . . . . .
67
4.2.2
Fingerprint Samples . . . . . . . . . . . . . . . . . . . . . . . . . .
68
4.2.3
Implementation of Multiresolution . . . . . . . . . . . . . . .
68
4.3
Selecting Resolution Criteria Using Minutiae and Pores . . . . . .
71
4.4
Experiments and Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.4.1
Minimum Resolution Required for Pore Extraction . . .
75
4.4.2
Resolution Based on the Established Criteria . . . . . . . .
76
4.4.3
Analysis of Ridge Width . . . . . . . . . . . . . . . . . . . . . .
77
4.4.4
Fingerprint Recognition Accuracy . . . . . . . . . . . . . . . .
78
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
80
Part II
Finger-Knuckle-Print Veriﬁcation
5
Finger-Knuckle-Print Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
5.2
The Finger-Knuckle-Print (FKP) Recognition System . . . . . . . .
87
5.3
ROI (Region of Interest) Extraction . . . . . . . . . . . . . . . . . . . . .
90
viii
Contents
www.ebook3000.com

5.4
FKP Feature Extraction and Matching . . . . . . . . . . . . . . . . . . .
96
5.4.1
Improved Competitive Coding (ImCompCode) . . . . . .
96
5.4.2
Magnitude Coding (MagCode) . . . . . . . . . . . . . . . . . .
98
5.4.3
FKP Matching . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
5.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5.5.1
Database Establishment . . . . . . . . . . . . . . . . . . . . . . .
99
5.5.2
Selection of the Image Resolution . . . . . . . . . . . . . . . .
100
5.5.3
FKP Veriﬁcation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101
5.5.4
Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
5.5.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
5.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
6
Local Features for Finger-Knuckle-Print Recognition . . . . . . . . . .
111
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
6.2
Analysis of Local Features . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
6.3
Extraction and Matching of Local Features . . . . . . . . . . . . . . .
117
6.3.1
Step 1: Phase Congruency (PC) . . . . . . . . . . . . . . . . .
117
6.3.2
Local Feature Extraction and Coding . . . . . . . . . . . . .
118
6.3.3
Matching of Local Feature Maps . . . . . . . . . . . . . . . .
120
6.3.4
Integration of Local Features . . . . . . . . . . . . . . . . . . .
121
6.4
Experimental Results and Discussions . . . . . . . . . . . . . . . . . . .
121
6.4.1
FKP Database and the Test Protocol . . . . . . . . . . . . . .
121
6.4.2
Determination of Parameters . . . . . . . . . . . . . . . . . . .
122
6.4.3
Performance of Local Features . . . . . . . . . . . . . . . . . .
123
6.4.4
Further Discussions . . . . . . . . . . . . . . . . . . . . . . . . . .
124
6.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128
7
Global Information for Finger-Knuckle-Print Recognition . . . . . . .
131
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131
7.2
Local Feature Extraction and Matching . . . . . . . . . . . . . . . . . .
134
7.3
Global Feature Extraction and Matching . . . . . . . . . . . . . . . . .
137
7.3.1
From Local to Global . . . . . . . . . . . . . . . . . . . . . . . . .
137
7.3.2
Phase-Only Correlation (POC) . . . . . . . . . . . . . . . . . .
138
7.3.3
Band-Limited Phase-Only Correlation (BLPOC) . . . . .
139
7.4
Local-Global Information Combination . . . . . . . . . . . . . . . . . .
141
7.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
7.5.1
FKP Database and Test Protocol . . . . . . . . . . . . . . . . .
143
7.5.2
Determination of Parameters . . . . . . . . . . . . . . . . . . .
144
7.5.3
FKP Veriﬁcation Results . . . . . . . . . . . . . . . . . . . . . .
144
7.5.4
Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
7.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
Contents
ix

8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive
Binary Fusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
8.2
Competitive Coding Based FKP Veriﬁcation . . . . . . . . . . . . . .
153
8.3
Recognition via Reconstruction . . . . . . . . . . . . . . . . . . . . . . . .
154
8.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154
8.3.2
Reconstruction with l1-Norm Sparse Regularization . . .
156
8.3.3
Reconstruction with l2-Norm Regularization . . . . . . . .
158
8.3.4
Patch Based Reconstruction . . . . . . . . . . . . . . . . . . . .
159
8.4
Veriﬁcation by Binary Score Level Fusion . . . . . . . . . . . . . . . .
161
8.4.1
Popular Score Level Fusion Methods . . . . . . . . . . . . .
162
8.4.2
Adaptive Binary Fusion . . . . . . . . . . . . . . . . . . . . . . .
163
8.5
Summary of the Veriﬁcation Algorithm . . . . . . . . . . . . . . . . . .
165
8.6
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
8.6.1
Comparison Between Different Fusion Rules . . . . . . . .
166
8.6.2
Experiment Settings and Parameter Selection . . . . . . .
166
8.6.3
FKP Veriﬁcation Results . . . . . . . . . . . . . . . . . . . . . .
168
8.6.4
Integrating with Global Features . . . . . . . . . . . . . . . . .
170
8.6.5
Discussions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
8.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
172
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
Part III
Other Hand-Based Biometrics
9
3D Fingerprint Reconstruction and Recognition . . . . . . . . . . . . . . .
177
9.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177
9.2
3D Fingerprint Reconstruction System . . . . . . . . . . . . . . . . . . .
180
9.3
Fingerprint Feature Correspondences Establishment . . . . . . . . .
182
9.3.1
Correspondences Establishment Based on SIFT . . . . . .
182
9.3.2
Correspondences Establishment Based on
Ridge Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
9.3.3
Correspondences Establishment Based on Minutiae . . .
188
9.4
Finger Shape Model Estimation . . . . . . . . . . . . . . . . . . . . . . . .
189
9.5
Curvature Features Extraction and Matching . . . . . . . . . . . . . .
192
9.6
Experimental Results and Analysis . . . . . . . . . . . . . . . . . . . . .
197
9.6.1
3D Fingerprint Reconstruction System Error
Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
9.6.2
Comparison and Analysis of Reconstruction
Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
9.6.3
Validation of Estimated Finger Shape Model . . . . . . . .
199
9.6.4
Reconstruction System Computation Time Analysis . . .
203
9.6.5
3D Fingerprint Recognition . . . . . . . . . . . . . . . . . . . .
204
9.7
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
207
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
x
Contents
www.ebook3000.com

10
Hand Back Skin Texture for Personal Identiﬁcation . . . . . . . . . . .
213
10.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
10.2
Hand Back Skin Texture Imaging System . . . . . . . . . . . . . . . .
216
10.3
HBST Feature Extraction and Classiﬁcation . . . . . . . . . . . . . . .
218
10.3.1
MR8 Filter Bank . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
10.3.2
Texton Learning Based on SR . . . . . . . . . . . . . . . . . .
221
10.3.3
Feature Extraction and Classiﬁcation . . . . . . . . . . . . .
223
10.4
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
10.4.1
Database Establishment . . . . . . . . . . . . . . . . . . . . . . .
224
10.4.2
Personal Identiﬁcation . . . . . . . . . . . . . . . . . . . . . . . .
226
10.4.3
Gender Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
229
10.4.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230
10.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
231
11
Line Scan Palmprint Recognition System . . . . . . . . . . . . . . . . . . . .
235
11.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
11.2
Existing System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
11.2.1
Flatbed Scanner . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237
11.2.2
Web Camera Based Systems . . . . . . . . . . . . . . . . . . . .
237
11.2.3
Palmprint Systems with Pegged Flat Platen Surface . . .
238
11.3
Line Scan Palmprint System Design . . . . . . . . . . . . . . . . . . . .
239
11.3.1
Line Scan Imaging Scheme . . . . . . . . . . . . . . . . . . . .
239
11.3.2
System Framework . . . . . . . . . . . . . . . . . . . . . . . . . .
242
11.3.3
Line Scan Image Sensor-CIS Module . . . . . . . . . . . . .
243
11.3.4
Synchronizing Unit . . . . . . . . . . . . . . . . . . . . . . . . . .
246
11.3.5
Controller Board . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248
11.3.6
The Device . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
11.3.7
ROI Extraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
249
11.4
Experiment and Comparison . . . . . . . . . . . . . . . . . . . . . . . . . .
251
11.4.1
Line Scan Palmprint Database . . . . . . . . . . . . . . . . . .
251
11.4.2
Veriﬁcation Experiment . . . . . . . . . . . . . . . . . . . . . . .
252
11.4.3
Comparisons with Current Palmprint Systems . . . . . . .
253
11.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
12
Door Knob Hand Recognition System . . . . . . . . . . . . . . . . . . . . . . .
259
12.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
12.2
Ergonomic Biometrics Design . . . . . . . . . . . . . . . . . . . . . . . . .
260
12.2.1
Development of Biometric Systems . . . . . . . . . . . . . .
260
12.2.2
Ergonomics Studies in Biometrics . . . . . . . . . . . . . . . .
261
12.2.3
Ergonomic Biometrics Design Model . . . . . . . . . . . . .
263
Contents
xi

12.3
Door Knob Hand Recognition System . . . . . . . . . . . . . . . . . . .
264
12.3.1
Concept and Framework . . . . . . . . . . . . . . . . . . . . . . .
264
12.3.2
Imaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
12.3.3
Feature Extraction and Classiﬁcation . . . . . . . . . . . . .
269
12.4
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
12.4.1
Data Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
12.4.2
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . .
275
12.4.3
Comparison with Conventional Biometrics . . . . . . . . .
277
12.4.4
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
278
12.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280
Part IV
Some New Head-Based Biometrics
13
Dynamic Tongueprint Recognition . . . . . . . . . . . . . . . . . . . . . . . . .
287
13.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
13.2
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
290
13.3
Tongue Squirm and Its Applications . . . . . . . . . . . . . . . . . . . .
292
13.3.1
Liveness Detection . . . . . . . . . . . . . . . . . . . . . . . . . . .
292
13.3.2
Squirm Features Extraction . . . . . . . . . . . . . . . . . . . . .
295
13.4
Extraction of Static Physiological Features . . . . . . . . . . . . . . . .
296
13.4.1
Geometric Features . . . . . . . . . . . . . . . . . . . . . . . . . .
296
13.4.2
Crack Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
13.4.3
Texture Features . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
13.5
Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
13.5.1
Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
13.5.2
Identiﬁcation Experiments . . . . . . . . . . . . . . . . . . . . .
303
13.5.3
Veriﬁcation Experiments . . . . . . . . . . . . . . . . . . . . . .
304
13.5.4
Liveness Detection Experiments . . . . . . . . . . . . . . . . .
305
13.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
306
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
307
14
Online 3D Ear Recognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
14.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
14.2
Scanner Design for Online 3D Ear Acquisition . . . . . . . . . . . . .
311
14.3
3D Ear Global and Local Feature Classes . . . . . . . . . . . . . . . . .
313
14.3.1
Global Feature Class . . . . . . . . . . . . . . . . . . . . . . . . .
313
14.3.2
Local Feature Class . . . . . . . . . . . . . . . . . . . . . . . . . .
316
14.4
Experimental Results and Discussion . . . . . . . . . . . . . . . . . . . .
321
14.4.1
Feature Optimization . . . . . . . . . . . . . . . . . . . . . . . . .
321
14.4.2
Matching Using Local Features . . . . . . . . . . . . . . . . .
322
14.4.3
Recognition with Global Feature Indexing . . . . . . . . . .
324
14.4.4
Performance Analysis . . . . . . . . . . . . . . . . . . . . . . . .
325
14.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
xii
Contents
www.ebook3000.com

15
Book Review and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
15.1
Book Recapitulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
15.2
Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
Contents
xiii

Chapter 1
Overview
Abstract This chapter gives an overview of biometrics technology, its develop-
ment history, typical biometrics systems, and current research hotspots. Then, the
chapter arrangement is introduced.
Keywords Biometrics • 3D biometrics • Multispectral biometrics
1.1
Why Biometrics
With the increasing attention on security vulnerabilities and transactional fraudu-
lent in industries and societies, highly reliable and accessible personal authentica-
tion and identiﬁcation techniques become an inevitable demand for human
societies. Biometrics has emerged to meet this need and even has developed to
the science combining biology technology and information technology to utilize
physiological or behavioral characteristics in human body to deal with identiﬁca-
tion of individuals. In particular, biometrics technologies focus on the technologies
to automatically authenticate the still traits of the human such as the DNA, ears,
palmprint, hand and ﬁnger geometries, ﬁngerprint, faces, irises, footprint, retina
and tooth or the dynamic traits of the human such as voice, gait, keystroke and
signature. What’s more, it seems that biometrics will and is being a dominant
component of the world and remarkably increasing number of biometrics systems
have been developed to satisfy the research and commercial need. Biometrics
systems have been widely applied to a variety of government and private domains
as a technology with respect to security and convenience. Furthermore, biometrics
has shown its overwhelming superiority to replace or enhance traditional identiﬁ-
cation
methods,
such
as
token-based
approaches
and
knowledge-based
approaches. This book presents a literature review of the biometrics area. And
subsequently some new biometrics technologies and systems are discussed and
highlighted.
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_1
1
www.ebook3000.com

1.2
History of Biometrics
The term biometric comes from the Greek words bios and metrikos. Biometrics
deals with identiﬁcation of individuals based on their biological or behavioral
characteristics (Jain and Ross 2002). Biometrics combines biology technology
and information technology to exploit physical features or behavioral features in
human body to identify a person’s identity so as to replace or enhance traditional
personal identiﬁcation methods.
With the increasing concerns on security breaches and transaction fraud, highly
reliable and convenient personal veriﬁcation and identiﬁcation technologies are
more and more requisite in our social activities and national services. Biometrics is
implemented to two main aspects applications, identity veriﬁcation and identity
recognition. Identity veriﬁcation is to require that the system has a binary option,
acceptance or rejection, in response to the person’s claim when the person claims to
an identity. However, identity recognition is to require that the system to retrieve
the pre-existing database of characteristics and identify the one that matched the
characteristics of the unknown individual being presented. Historically, the devel-
opment of biometric technologies is originated from different history background.
Personal identiﬁcation is to associate the identity to a particular individual.
Identiﬁcation can be viewed as the form of recognition or veriﬁcation which is
known as authentication (Jain et al. 2000). Knowledge-based and token-based
personal identiﬁcation techniques have been treated as the two traditional widely
used techniques (Miller 1994). The knowledge-based approaches authenticate the
identity of an individual according to what he/she knows. Any individuals with
certain secret knowledge, such as personal identiﬁcation numbers or a password for
telephone calling, membership or credit cards, and then answers to questions,
would receive the associated service. In the token-based approach, the identity of
a person is veriﬁed according to what he/she has. Anyone possessed a certain
physical object (token), e.g., keys or ID cards, is authorized to receive the associ-
ated service.
Both the token-based and the knowledge-based approaches, however, have some
inherent limitations, because they are not based on any intrinsic biological nature
from each individual for personal identiﬁcation. These drawbacks usually are fatal
important for personal identiﬁcation. For example, tokens may be stolen, lost,
forgotten, effaced or misplaced, and even tokens are easy to fool. Knowledge-
based techniques also exists ﬂaws. For example, the personal identiﬁcation number
(PIN) may be forgotten by a valid user or guessed by a fraud.
Because the characteristics of knowledge-based and token-based personal iden-
tiﬁcation techniques are unable to be unique, distinctive and distinguishable, they
cannot meet the security requirements of electronic interconnection of the infor-
mation society on account of the fraudulent vulnerability. In order to make break-
through on veriﬁcation systems, powerful identiﬁcation systems have never been
more in demand. Biometric recognition is an emerging personal recognition tech-
nology developed to overcome the inherent limitations of the traditional personal
2
1
Overview

recognition approaches (Jain et al. 1999; Zhang 2000, 2002). Biometrics which
exploits the instinct of distinctive physiological characteristics has emerged to
enhance the identiﬁcation techniques by automatically verifying or recognizing
the identity of a living person. Compared with the token-based and the knowledge-
based methods, biometric identiﬁers cannot be easily forged, shared, forgotten, or
lost, and thus can provide better security, higher efﬁciency, and increased user
convenience. The appearance of biometrics makes authentication easier, faster and
more accurate. Moreover, biometrical identiﬁers are more competent and reliable
compare to the traditional identiﬁcation techniques and also have gained great
reputation.
1.3
Biometrics Systems
Generally, a biometric system is a computer system implemented by exploiting
corresponding biometric identiﬁcation methods, techniques, and technologies. Bio-
metric systems can be regarded as pattern recognition systems, where a feature set
is ﬁrst extracted from the acquired data, and then compared with the stored template
set to make a decision on the identity of an individual. A biometric system can be
applied to two ﬁelds, veriﬁcation and identiﬁcation. In veriﬁcation mode, the
decision is whether a person is “who he claims to be?” In identiﬁcation mode, the
decision is “whose biometric data is this?” A biometric system is thus formalized
into a two-class or multi-class pattern recognition system.
A biometric system usually includes four major modules: data acquisition,
feature extraction, matching, and system database (Jian et al. 2007). In the data
acquisition module, the biometric data of an individual is acquired using a capture
sensor such as ﬁngerprint sensor and digital camera for face. In the feature extrac-
tion module, the acquired data is processed to extract a set of discriminative
features. In the matching module, the features are compared with the stored
template set to make a decision on the identity of an individual. In the system
database module, a database is built and maintained to store the biometric templates
of the enrolled users. Feature extraction and matching are two of the most chal-
lenging problems in biometric recognition research, and have attracted researchers
from different backgrounds: biometrics, computer vision, pattern recognition, sig-
nal processing, and neural networks.
Fingerprint Identiﬁcation System
Fingerprints identiﬁcation system has been used for many centuries on basis of the
essence in its uniqueness and distinguishability. The application of ﬁnger identiﬁ-
cation provides solid and infallible methods of personal identiﬁcation because of its
high accuracy (Maio et al. 2002). Fingerprint identiﬁcation mainly depends on the
pattern of ridges and valleys on the surface of ﬁngertip, whereas these patterns are
determined during the ﬁrst 7 months of each person. For decades, law enforcement
has been classifying and determining identity by matching key points of ridge
1.3
Biometrics Systems
3
www.ebook3000.com

endings and bifurcations. The uniqueness of the ﬁngerprints is that no two ﬁnger-
prints have been found alike in the many billions of human, even for the identical
twins or the prints on each ﬁnger of the same person (Maio et al. 2002).
Facial Identiﬁcation System
Face recognition started from computer vision and research on face recognition
goes back to the earliest days of artiﬁcial intelligence and computer vision. Images
of a human face are highly suitable for use as a biometric trait for personal
authentication because they can be acquired non-intrusively, hands-free, continu-
ously, and usually in a way that is acceptable to most users (Zhao et al. 2003). A
general deﬁnition of face recognition problem is to verify or recognize one or more
persons from a stored image and face database by comparing facial distinguishing
characteristics. The authentication of a person by their facial image can be done in a
number of different ways, such as by capturing an image of the face in the visible
spectrum using an inexpensive camera or by using the infrared patterns of facial
heat emission. Face recognition or veriﬁcation mainly depends on characteristics by
the overall facial structure, distance between mouth, nose, eyes and jaw edges, and
then to ﬁnd a match by comparing these features from a database of face images.
Hand and Finger Geometry Identiﬁcation System
Hand and ﬁnger geometry-base veriﬁcation and recognition is an important branch
of biometrics to automatically verify and recognize individuals based on the
distinguishing hand geometric characteristics. Hand and ﬁnger geometry recogni-
tion technique exploits a number of characteristics from the human hand such as
ﬁnger length, ﬁnger width, ﬁnger area, ﬁnger thickness, palm width and curvature
of the ﬁngers at certain points to make personal veriﬁcation and recognition (Zhang
and Kanhangad 2011). Hand and ﬁnger geometry recognition technique obsesses
dramatical merits. For example, hand-based system just utilizes simple imaging
requirement by virtue of extracting features from low-resolution hand images.
What’s more, hand-based system is competent with the capability of operating
and unaffected under turbulent and hash environmental conditions such as dry
weather, or individual anomalies such as dirt on the hand, dry skin. These external
factors do not appear to have any negative effects on the veriﬁcation and recogni-
tion accuracy of hand and ﬁnger geometry technique. Furthermore, the low data-
storage requirement is additional superiority. The biometric system based on hand
and ﬁnger geometry has been used in physical access control in commercial and
residential applications, in time and attendance systems. Additionally, hand and
ﬁnger geometry technique can be effectively implemented and inexpensive. More-
over, acquisition and authentication system are efﬁciently obtained.
Parmprint Identiﬁcation System
Palm is the inside part of human hand from the wrist to the end of our ﬁngers and
palmprint is the skin patterns of a palm, composed of the physical characteristics of
the skin patterns of a palm. The palmprint recognition implements the matching
characteristics by the pattern of ridges and valleys much like the ﬁngerprints. The
palms of the human obsess much larger area compare to ﬁnger, and as a result,
4
1
Overview

palmprints are expected to be much more distinctive and palmprint identiﬁcation
are anticipated to be more robust and effective. Furthermore, additional
distinguishing characteristics of palmprints also can be provided as supplements
for identiﬁcation, such as wrinkle, texture and principles lines. Point features also
can be used to palmprint identiﬁcation systems, such as minutiae points, delta
points and datum points. In the early stage, palm print recognition techniques
have been investigated to extract and match the singular points and minutia points
from high resolution palm print images. High resolution palm print scanner,
however, is expensive, and is time consuming to capture a palm print image,
which restricts the potential applications of online palm print recognition systems.
Subsequently, online capture device has been developed to collect real time low
resolution palm print image, and low resolution palm print recognition has gradu-
ally received considerable recent interest in biometric community (Zhang et al.
2003; Zhang and Lu 2013; Xu et al. 2015). A great many different low resolution
palmprint recognition algorithms have been developed, which can be roughly
grouped into three categories: holistic-based, feature-based, and hybrid methods.
To this end, palmprint identiﬁcation system usually combines all the features of the
palm such as hand geometry, ridge and valley features, principle lines, texture,
wrinkles and point features to build a highly accurate system.
Iris Identiﬁcation System
The patterns of the iris, the colored area that surrounds the pupil, are thought to be
unique. Iris recognition is to recognize a person by utilizing and analyzing the
random pattern of the iris. Iris features can be more easily obtained than other
features from eyes, like retina. It is shown that an iris has more details than a
ﬁngerprint or palmprint. Feature set extracted using iris detailed and unique texture
will remain stable and immutable over decades of life. Iris recognition system
exploits textures with striations, contraction furrows, pits, collagenous ﬁbers, ﬁla-
ment, crypts (darkened areas and resembling excavations), serpentine vasculature,
rings and freckles for personal identiﬁcation. Iris patterns can be obtained through a
video-based image acquisition system. Iris scanning devices have been used in
personal authentication applications. Furthermore, these visual textures of the iris
are established during the ﬁrst 2 years of life which is the fetal development period
for human. More complex textures of iris can contribute more useful distinctive
identiﬁcation features for personal authentication. It has been demonstrated that
iris-based biometric system can work with individuals without regard to ethnicity or
nationality. Moreover, iris identiﬁcation technique can recognize the identity of
claimed person with dramatic efﬁciency and this technology can be embraced in
large-scale identiﬁcation system.
Ear Identiﬁcation System
Earprints identiﬁcation is to utilize the salient characteristics of the ear for personal
identiﬁcation by exploiting the shape of the ear and the structure or contours of the
congenital human tissue of the auricle. The ear identiﬁcation technique matches the
distance of distinguishing points on the auricle of the ear. There is evidence to show
that the shape of the ear and the structure of the cartilaginous tissue of the pinna are
1.3
Biometrics Systems
5
www.ebook3000.com

distinctive. As a result, the ear-based biometric system can be used for authenti-
cating personal identity. Some scientists have demonstrated that the human ear is
differentiated enough to each individual and can be practically used as a biometric
by its appearance. Moreover, some researcher has claimed that the ear is fully
formed when you are born and it will stay similar except for a litter descendent of
your lobe (Iannarelli 1989). Furthermore, some scientists have been trying to use
the ear’s appearance in 2D intensity and 3D ear shape for personal identiﬁcation
(Liu et al. 2015a, b; Abaza et al. 2013).
Gait Identiﬁcation System
From ancient time, human can recognition your familiars by one’s walk. Gait
identiﬁcation is an outstanding biological or behavioral identiﬁcation technique to
analyze the walking ability of humans and animals. The aim of gain recognition is
to detect, classify and identify humans by the fastest speed. Gait identiﬁcation
usually resort to some distinctive characteristics such as, stop length, stride length,
speed, dynamic base, progression line, foot angle and hip angle. Moreover, some
factors also can inﬂuence the gain identiﬁcation, for instance, external factors (such
as footwear and clothing), physical or intrinsic factors (such as weight and height,
male or female and age), pathological factors, emotional factors and etc. Gait can be
supposed to be dramatic distinctive and the stride of each person still be unique
enough to be used for identiﬁcation even among a group of people. Thus, gait
recognition technique is to use the peculiar way of one’s walk for personal
identiﬁcation.
Voice Identiﬁcation System
Voice recognition is also one of the oldest approaches for identiﬁcation by humans.
Voice recognition is also commonly referred to as speaker recognition. The voice
identiﬁcation is the process for recognizing a person by analyzing both the behav-
ioral features of someone and the physical structure of someone’s vocal tract. That
is, voice recognition is developed by analyzing the shape and size of the appendages
such as vocal tracts, mouth, nasal cavities and lips. Voice authentication uses the
acoustic features of voice, which have been found to be different between individ-
uals. These acoustic patterns reﬂect both anatomic (e.g., size and shape of the throat
and mouth) and behavioral patterns (e.g., voice pitch, speaking style) of an indi-
vidual. The incorporation of learned patterns into the voice templates (the latter
called “voiceprints”) has allowed speaker identiﬁcation to be recognized as a
“behavioral biometric”. Voice-based personal authentication systems employ
three styles of spoken input: text-dependent, text-prompted and text-independent.
Most voice authentication applications use text-dependent input, which involves
selection and enrollment of one or more voice passwords. Text-prompted input is
used whenever there is concern about imposters.
Signature Identiﬁcation System
Signature identiﬁcation is categorized to the behavioral biometric technique. Gen-
eral signature identiﬁcation concludes two modes, that is, static signature and
dynamic
signature
identiﬁcation
techniques.
Static
signature
identiﬁcation
6
1
Overview

technique mainly refers to make identiﬁcation on still signature, i.e. recognizing by
measuring and analyzing the shape of the pre-captured signature by photo scanner
or optical scanner. On the other hand, the mode of dynamic signature identiﬁcation
is to recognition the signature sign on live using the characteristics of signature such
as the stroke order, azimuth, the pressure applied, inclination, pen up/down, the
speed of writing, horizontal and vertical coordinates of inﬂection point. However,
the core point required by the signature biometric technique is not the image of the
signature but behavioral patterns, i.e. how to sign. This technology has potential
applications in e-business, where signatures can be an accepted method of personal
authentication.
Keystroke Identiﬁcation System
Keystroke identiﬁcation technology is an important member of the biometrics and
is categorized to physiological and behavioral biometrics by different measure-
ments. Keystroke identiﬁcation is considered to be the best way to authenticate a
person on account of minimizing the impact on privacy. Keystroke identiﬁcation
technique is the process of measuring and analyzing human’s typing rhythm on
digital devices such as, computer keyboard, mobile phone or touch screen panel.
The way and the manner in which we type on our computer keyboard varies from
individual to individual and is considered to be a unique behavioral biometric.
Keystroke identiﬁcation is probably one of the easiest biometrics forms to imple-
ment and manage, because all we needed for this identiﬁcation is the existing
computer and keyboard that is already in place and use.
1.4
Recent Biometrics Development
1.4.1
Multi-biometrics
Veriﬁcation or identiﬁcation accuracy is always the ﬁrst-of-all objective for bio-
metric systems. Unibiometric system, the biometric system using a single biometric
characteristic, usually suffers from some limitations and cannot provide satisfactory
recognition performance. For example, manual workers with damaged or dirty
hands may not be able to provide high-quality ﬁngerprint images, and thus failure
to enrol would happen for single ﬁngerprint recognition system.
Multi-biometric systems, which integrate information from multiple biometric
traits, provide some effective means to enhance the performance and reliability of
the biometric system. To combine information from individual biometric traits,
there are three categories of fusion strategies, feature level fusion, matching score
level fusion, and decision level fusion. In feature level fusion, the data obtained
from each sensor is used to compute a feature vector. As the feature extracted from
one biometric trait is independent of that extracted from the other, it is reasonable to
concatenate the two vectors into a single new vector for performing multi-biometric
based personal authentication. Note that the new feature vector now has a higher
1.4
Recent Biometrics Development
7
www.ebook3000.com

dimensionality than the original feature vector generated from each sensor. Feature
reduction techniques may be employed to extract useful features from the set of the
new feature vector. In matching score level fusion, each subsystem using one
biometric trait of the multi-biometric system provides a matching score indicating
the proximity of the feature vector with the template vector. These scores can be
combined to assert the veracity of the claimed identity. In decision level fusion each
sensor ﬁrst acquire one of multiple biometric traits and the resulting feature vectors
are individually classiﬁed into the two decisions—accept or reject the claimed
identity. Then a scheme that exploits the known decisions to make the ﬁnal decision
is used. In the ﬁeld of multi-biometrics, a great number of studies of feature level
fusion, matching score level fusion and decision level fusion have been made.
Though fusion of multi-biometrics are generally recognized as three classes as
described above, in real-world applications of multi-modal biometric it is possible
that the “Fusion Process” may be simultaneously involved in different levels such
as in both the matching score level and the decision level.
Multi-biometric system is designed to overcome the limitations of any single
biometric systems by fusing information from multiple biometric traits. The fusion
can be implemented in either of three levels, feature level, matching score level, and
decision level. In feature level fusion, a new feature vector is constructed using the
concatenation rule (Ross and Govindarajan 2005), the parallel rule (Yang et al.
2003; Yang and Yang 2002), or the competitive rule (Kong et al. 2006). In
matching score level fusion, a number of transformation-based (Jain et al. 2005;
Zuo et al. 2007), classiﬁer-based (Fierrez-Aguilar et al. 2005), and density-based
(Ulery et al. 2006; Nandakumar et al. 2006) score fusion methods have been used to
combine scores of multiple scores. In decision level fusion, boolean conjunctions,
weighted decision methods, classical inference, Bayesian inference, Dempster-
Shafer method, and voting have been proposed to make the ﬁnal recognition
decision (Gokberk et al. 2003).
1.4.2
3D Biometrics
In the past decade, biometrics recognition has been growing rapidly, and many
biometrics systems have been widely used in various applications. However, most
of the biometrics recognition techniques are based on 1D signal or 2D images.
There are many limitations of 1D and 2D biometrics technologies until now:
•
Fingerprints may be distorted and unreadable or unidentiﬁable if the person’s
ﬁngertip has dirt on it, or if the ﬁnger is twisted during the process of ﬁnger-
printing. In an ink ﬁngerprint, twisting could cause the ink to blur, distorting the
shape of the ﬁngerprint and potentially making it unreadable.
•
It is found that with age, the voice of a person differs. Also when the person has
ﬂu or throat infection the voice changes, or if there are too much noise in the
environment this method may not authenticate correctly.
8
1
Overview

•
For Iris recognition, if people affected with diabetes, the eyes get affected
resulting in differences.
•
The conventional 2D palmprint recognition is a fast and effective personal
authentication method, but 2D palmprint images can be easily counterfeited.
Although 2D biometrics recognition techniques can achieve high accuracy, the
2D features can be easily counterfeited and much 3D feature structural information
is lost. Therefore, it is of high interest to explore new biometrics recognition
techniques: 3D Biometrics.
With the development of 3D techniques, it is possible to capture 3D character-
istics in real time. Recently, 3D techniques have been used in biometrics authen-
tication, such as 3D face, 3D ﬁngerprint, 3D palmrpint and 3D ear recognition, and
shown many advantages, such as:
•
3D biometrics is much more robust to illumination and pose variations than 2D
biometrics.
•
3D range data may offer a richer information source for feature extraction. And
usually it also can fuse with 2D biometrics to enhance the system accuracy.
•
3D biometrics systems are more robust to attack, since 3D information is more
difﬁcult to be duplicated or counterfeited.
3D biometrics technologies have been the new trend in this research ﬁeld. There
are some commercial devices which can obtain the 3D information of an object,
such as Konica Minolta Vivid 9i/910, Cyberware whole body color 3D scanner, and
so on. These commercial 3D scanners have high speed and accuracy, and can be
used for 3D biometrics information collection.
1.4.3
Multispectral Biometrics
Versatility, usability, and security are some of the required characteristics of
biometric system. Such system must have the capability to acquire and process
biometric data at different times of day and night, in a variety of weather and
environmental conditions, and be resistant to spooﬁng attempts. Multispectral bio-
metrics is one of the few technologies shown to solve many of the aforementioned
issues (Li and Jain 2009).
Multispectral imaging is possible to simultaneously capture images of an object
in the visible spectrum and beyond. It has been extensively used in the ﬁeld of
remote sensing, medical imaging, and compute vision to analyze information in
multiple bands of the electromagnetic spectrum. Multispectral imaging could
provide more information than single modality. Usually, complementary features
could be extracted. Thus, better recognition and spoof detection ability are easy to
be achieved.
1.4
Recent Biometrics Development
9
www.ebook3000.com

1.5
Arrangement of This Book
In this book, we would like to introduce some new biometric technologies. There
are 15 chapters, and except this chapter and recapitulation chapter (Chap. 15), the
other 13 chapters are in four parts, including High Resolution Fingerprint Recog-
nition, Finger-Knuckle-Print Veriﬁcation, Other Hand-Based Biometrics, and
Some New Head-Based Biometrics. The biometrics devices, feature extraction
and matching algorithms of each biometrics system are presented.
Part I
This part focuses on high resolution ﬁngerprint recognition technology. Chapter 2
introduces the high resolution partial ﬁngerprint alignment using pore-valley
descriptors. Adaptive ﬁngerprint pore modeling and extraction method is intro-
duced in Chap. 3, and Chap. 4 discusses the reference high resolution of ﬁngerprint
image by using minutiae and pores.
Part II
This part discusses Finger-Knuckle-Print veriﬁcation technologies. Chapter 5 intro-
duces an online Finger-Knuckle-Print veriﬁcation system. Chapter 6 studies phase
congruency induced local features for FKP veriﬁcation. Chapter 7 presents ensem-
ble of local and global information for Finger-Knuckle-Print veriﬁcation. In
Chap. 8, FKP veriﬁcation method with score level adaptive binary fusion is
introduced.
Part III
This part has four chapters, focusing on some new hand-based biometric technol-
ogies. Chapter 9 introduces 3D ﬁngerprint reconstruction and recognition method.
Chapter 10 reports a new method for personal identiﬁcation based on hand back
skin texture patterns. Chapter 11 presents a novel line scan palmprint recognition
system. In Chap. 12, another hand-based biometric system, door knob hand recog-
nition system, is proposed.
Part IV
This part contains two chapters. Chapter 13 introduces a new biometric identiﬁer:
dynamic tongueprint. Chapter 14 gives a novel line scan palmprint recognition
system.
References
Abaza A, Ross A, Hebert C, MAF H, Nixon MS (2013) A survey on ear biometrics. ACM Comput
Surv 45(2):94–111
Fierrez-Aguilar J, Ortega-Garcia J, Gonzalez-Rodriguez J, Bigun J (2005) Discriminative multi-
modal biometric authentication based on quality measure. Pattern Recogn 38(5):777–779
Gokberk B, Salah A, Akarun L (2003) Rank-based decision fusion for 3D shape-based face
recognition. Lect Notes Comput Sci 3546:1019–1028
10
1
Overview

Iannarelli A (1989) Ear identiﬁcation. Paramont, Fremont
Jain A, Ross A (2002) Introduction to biometrics. In: Biometrics. Springer, New York, pp 1–41
Jain A, Bolle A, Pankanti S (1999) Biometrics: personal identiﬁcation in networked society.
Kluwer Academic, Norwell
Jain A, Hong L, Pankanti S (2000) Biometric identiﬁcation. Commun ACM 43(2):90–98
Jain A, Nandakumar K, Ross A (2005) Score normalization in multimodel biometric systems.
Pattern Recogn 38(5):2270–2285
Jain A, Flynn P, Ross A (2007) Handbook of biometrics. Springer, New York
Kong A, Zhang D, Kamel M (2006) Palmprint identiﬁcation using feature-level fusion. Pattern
Recogn 39:478–487
Li S, Jain A (2009) Encyclopedia of biometrics. Springer, New York
Liu Y, Lu G, Zhang D (2015a) An effective 3D ear acquisition system. PLoS One 10(6):e0129439
Liu Y, Zhang B, Zhang D (2015b) Ear-parotic face angle: a unique feature for 3D ear recognition.
Pattern Recogn Lett 53:9–15
Maio D, Maltoni D, Cappelli R, Wayman JL, Jain A (2002) FVC2002: ﬁngerprint veriﬁcation
competition. In: Proceedings of international conference on pattern recognition (ICPR), Que-
bec City, QC, Canada, pp 744–747
Miller B (1994) Vital signs of identity [biometrics]. IEEE Spectr 31(2):22–30
Nandakumar K, Chen Y, Jain A, Dass S (2006) Quality-based score level fusion in multibiometric
systems. In: Proceedings of international conference on pattern recognition, Hong Kong, pp
473–476
Ross A, Govindarajan R (2005) Feature level fusion using hand and face biometrics. In: Pro-
ceedings of SPIE conference on biometric technology for human identiﬁcation, pp 196–204
Ulery B, Hicklin A, Watson C, Fellner W, Hallinan P (2006) Studies of biometric fusion. NIST
technical report IR7346
Xu Y, Fei L, Zhang D (2015) Combining left and right palmprint images for more accurate
personal identiﬁcation. IEEE Trans Image Process 24(2):549–559
Yang J, Yang JY (2002) Generalized K–L transform based combined feature extraction. Pattern
Recogn 35(1):295–297
Yang J, Yang JY, Zhang D, JF L (2003) Feature fusion: parallel strategy vs. serial strategy. Pattern
Recogn 36(6):1369–1381
Zhang D (2000) Automated biometrics: technologies and systems. Kluwer Academic, Boston
Zhang D (2002) Biometrics solutions for authentication in an e-world. Kluwer Academic,
Dordrecht
Zhang D, Kanhangad V (2011) Hand geometry recognition. In: Encyclopedia of cryptography and
security. Springer, New York
Zhang D, Lu G (2013) 3D biometrics. Springer, New York
Zhang D, Kong WK, You J, Wong M (2003) On-line palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhao W, Chellappa R, Rosenfeld A (2003) Face recognition: a literature survey. ACM Comput Sur
36(4):399–458
Zuo W, Wang K, Zhang D, Zhang H (2007) Combination of two novel LDA-based methods for
face recognition. Neurocomputing 70(4–6):735–742
References
11
www.ebook3000.com

Part I
High Resolution Fingerprint Recognition

Chapter 2
High Resolution Partial Fingerprint Alignment
Abstract This chapter discusses the alignment of high resolution partial ﬁnger-
prints, which is a crucial step in partial ﬁngerprint recognition. The previously
developed
ﬁngerprint
alignment
methods,
including
minutia-based
and
non-minutia feature based ones, are unsuitable for partial ﬁngerprints because
small ﬁngerprint fragments often do not have enough features required by these
methods. In this chapter, we propose a new approach to aligning high resolution
partial ﬁngerprints based on pores, a type of ﬁngerprint ﬁne ridge features that are
abundant on even small ﬁngerprint areas. Pores are ﬁrst extracted from the ﬁnger-
print images by using a difference of Gaussian ﬁltering approach. After pore
detection, a novel Pore-Valley Descriptor (PVD) is proposed to characterize
pores based on their locations and orientations, as well as the ridge orientation
ﬁelds and valley structures around them. A PVD-based coarse-to-ﬁne pore
matching algorithm is then developed to locate pore correspondences. Once the
corresponding pores are determined, the alignment transformation between two
partial ﬁngerprints can be estimated. The proposed method is compared with
representative minutia based and orientation ﬁeld based methods using the
established high resolution partial ﬁngerprint dataset and two ﬁngerprint matchers.
The experimental results show that the PVD-based method can more accurately
locate corresponding feature points, estimate the alignment transformations, and
hence signiﬁcantly improve the accuracy of high resolution partial ﬁngerprint
recognition.
Keywords Fingerprint
alignment
•
Partial
ﬁngerprints
•
High
resolution
ﬁngerprints • Pores
2.1
Introduction
Automatic ﬁngerprint recognition systems (AFRS) have been nowadays widely
used in personal identiﬁcation applications such as access control (Ratha and Bolle
2004; Maltoni et al. 2003). Roughly speaking, there are three types of ﬁngerprint
matching methods: minutia-based, correlation-based, and image-based (Maltoni
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_2
15
www.ebook3000.com

et al. 2003; Nanni and Lumini 2008). In minutia-based approaches, minutiae
(i.e. endings and bifurcations of ﬁngerprint ridges) are extracted and matched to
measure the similarity between ﬁngerprints (Jain et al. 1997; Tico and Kuosmanen
2003; Jiang and Yau 2000; Kovacs-Vajna 2000; Feng 2008). These minutia-based
methods are now the most widely used ones (Ratha and Bolle 2004; Maltoni et al.
2003). Different from the minutia-based approaches, both correlation-based and
image-based methods compare ﬁngerprints in a holistic way. The correlation-based
methods spatially correlate two ﬁngerprint images to compute the similarity
between them (Bazen et al. 2000), while the image-based methods ﬁrst generate a
feature vector from each ﬁngerprint image and then compute their similarity based
on the feature vectors (Jain et al. 2000; Ross et al. 2003; Teoh et al. 2004; Nanni and
Lumini 2007, 2008). No matter what kind of ﬁngerprint matchers are used, the
ﬁngerprint images usually have to be aligned when matching them. Later in this
section, we will discuss more about the ﬁngerprint alignment methods.
In order to further improve the accuracy of AFRS, people are now exploring
more features in addition to minutiae on ﬁngerprints. The recently developed high
resolution ﬁngerprint scanners make it possible to reliably extract level-3 features
such as pores. Pores have been used as useful supplementary features for a long
time in forensic applications (Bazen et al. 2000; CDEFFS 2008). Researchers have
also studied the beneﬁt of including pores in AFRS and validated the feasibility of
pore based AFRS (Kryszczuk et al. 2004a, b; Roddy and Stosz 1997; Stosz and
Alyea 1994; Jain et al. 2007; Parthasaradhi et al. 2005). Using pores in AFRS has
two advantages. First, pores are more difﬁcult to be damaged or mimicked than
minutiae (Parthasaradhi et al. 2005). Second, pores are abundant on ﬁngerprints.
Even a small ﬁngerprint fragment could have a number of pores (refer to Fig. 2.1).
Therefore, pores are particularly useful in high resolution partial ﬁngerprint recog-
nition where the number of minutiae is very limited. In this chapter, we focus on the
alignment of high resolution partial ﬁngerprints and investigate the methods for
high resolution ﬁngerprint image processing.
(a)
(b)  
1
2
3
4
5
Fig. 2.1 An example high resolution partial ﬁngerprint. It has only ﬁve minutiae as marked in (a),
but hundreds of pores as marked in (b)
16
2
High Resolution Partial Fingerprint Alignment

2.1.1
High Resolution Partial Fingerprint
In a live-scan AFRS, a user puts his/her ﬁnger against the prism and the contact
ﬁngerprint region will be captured in the resulting image. A small contact region
between the ﬁnger and the prism will lead to a small partial ﬁngerprint image. On
such small ﬁngerprint region, there could be very limited minutiae available for
recognition. A natural way to solve the partial ﬁngerprint recognition problem is to
make full use of other ﬁne ﬁngerprint features abundant on the small ﬁngerprint
fragments. Sweat pores are such kind of features and high resolution ﬁngerprint
imaging makes it possible to reliably extract the sweat pores on ﬁngerprints
(CDEFFS 2008).
Most existing high resolution ﬁngerprint recognition methods use full-size
ﬁngerprint images which capture large ﬁngerprint areas. However, to capture the
full ﬁngerprints, high resolution ﬁngerprint images should have much bigger sizes
than conventional low resolution ﬁngerprint images. As a result, much more
computational resources are required to process the images. Considering the
increasing demand of AFRS on mobile devices and other small portable devices,
small ﬁngerprint scanners and limited computational resources are very common
(Jea and Govindaraju 2005). Consequently, the algorithms for aligning and
matching partial ﬁngerprint images are becoming important. Therefore, this chap-
ter, different from previous study of high resolution ﬁngerprint recognition, uses
high resolution partial ﬁngerprint images to study the partial ﬁngerprint image
alignment problem and a feasible algorithm will be proposed.
Although some methods have been proposed to construct full ﬁngerprint tem-
plates from a number of partial ﬁngerprint images (Choi et al. 2007), it is expensive
or even impossible to collect sufﬁcient ﬁngerprint fragments to construct a reliable
full ﬁngerprint template. Moreover, some errors (e.g. spurious features) could be
introduced in the construction process. Thus, it is meaningful and very useful if
algorithms can be developed for aligning and matching partial ﬁngerprints to partial
ﬁngerprints.
Some researchers have studied the problem of matching a partial ﬁngerprint to
full template ﬁngerprints. In (Jea and Govindaraju 2005), Jea and Govindaraju
proposed a minutia-based approach to matching incomplete or partial ﬁngerprints
with full ﬁngerprint templates. Their approach uses brute-force matching when the
input ﬁngerprints are small and few minutiae are presented, and uses secondary
feature matching otherwise. Since this approach is based on minutiae, it is very
likely to produce false matches when there are very few minutiae, and it is not
applicable when there are no minutiae on the ﬁngerprint fragments. Kryszczuk et al.
(2004a, b) proposed to utilize pore locations to match ﬁngerprint fragments. Using
high resolution ﬁngerprint images (approx. 2000 dpi in (Kryszczuk et al. 2004a, b)),
they studied how pores might be used in matching partial ﬁngerprints and showed
that the smaller the ﬁngerprint fragments, the greater the beneﬁts of using pores. In
their method, Kryszczuk et al. aligned ﬁngerprints by searching for the transfor-
mation parameters which maximize the correlation between the input ﬁngerprint
2.1
Introduction
17
www.ebook3000.com

fragment and the candidate part on the full ﬁngerprint template. Very recently,
Chen and Jain (2007). 2005 employed minutiae, dots, and incipient ridges, to align
and match partial ﬁngerprints with full template ﬁngerprints.
One drawback of most of the above approaches in aligning fragmental ﬁnger-
prints is that they are mainly based on the features which are probably very few
(e.g. minutiae) or even do not exist (e.g. dots and incipient ridges) on small
ﬁngerprint fragments (refer to Fig. 2.1). When the template ﬁngerprints are also
small ﬁngerprint fragments, it will become difﬁcult to get correct results due to the
lack of features. In (Kryszczuk et al. 2004a, b), Kryszczuk et al. proposed a
correlation-based blind searching approach to fragmental ﬁngerprint alignment.
As we will show later, however, this method has limited accuracy because it has
to discretize the transformation parameter space.
2.1.2
Fingerprint Alignment
Fingerprint alignment or registration is a crucial step in ﬁngerprint recognition. Its
goal is to retrieve the transformation parameters between ﬁngerprint images and
then align them for matching. Some non-rigid deformation or distortion could occur
in ﬁngerprint image acquisition. It is very costly to model and remedy such
distortions in ﬁngerprint registration, and they can be compensated to some extent
in subsequent ﬁngerprint matching. Thus, the majority of existing ﬁngerprint
alignment methods considers only translation and rotation, although some deform-
able models (Cappelli et al. 2001; Ross et al. 2005) have been proposed. According
to the features used, existing ﬁngerprint alignment methods can be divided into two
categories, minutia based and non-minutia feature based methods. Minutia based
methods are now the most widely used ones (Huvanandana et al. 2000; Jain et al.
1997; Tico and Kuosmanen 2003; Jiang and Yau 2000; Kovacs-Vajna 2000; Feng
2008; Ratha et al. 1996; Chang et al. 1997; Chen et al. 2006a, b; Nanni and Lumini
2007). Non-minutia feature based methods (Bazen et al. 2000; Zhang and Wang
2002; Jain et al. 2000;Yager and Amin 2005, 2006; Liu et al. 2006; Ross et al. 2002)
include those using image intensity values, orientation ﬁelds, cores, etc. One
problem in applying these methods to partial ﬁngerprints is that the features
required by them could be very few on the fragments. Consequently, they will
either lead to incorrect results or be not applicable.
There are roughly two kinds of methods for estimating alignment transforma-
tions. The ﬁrst kind of methods quantizes the transformation parameters into ﬁnite
sets of discrete values and searches for the best solution in the quantized parameter
space (Kryszczuk et al. 2004a, b; Bazen et al. 2000; Ratha et al. 1996; Chang et al.
1997; Yager and Amin 2005, 2006; Liu et al. 2006; Ross et al. 2002). The alignment
accuracy of these methods is thus limited due to the quantization. The second kind
of methods ﬁrst detects corresponding feature points (or reference points) on
ﬁngerprints and then estimates the alignment transformation based on the detected
corresponding points (Huvanandana et al. 2000; Jain et al. 1997; Tico and
18
2
High Resolution Partial Fingerprint Alignment

Kuosmanen 2003; Jiang and Yau 2000; Kovacs-Vajna 2000; Feng 2008; Zhang and
Wang 2002; Jain et al. 2000; Chen et al. 2006a, b). Most of such methods make use
of minutiae as the feature points. As discussed before, however, it is problematic to
align partial ﬁngerprints based on minutiae because of the lack of such features on
the ﬁngerprint fragments.
2.1.3
Partial Fingerprint Alignment Based on Pores
Following the second kind of alignment methods, we need to ﬁnd some reference
points other than minutiae on ﬁngerprints for the purpose of aligning partial
ﬁngerprints. One possible solution is to use sufﬁciently densely sampled points
on ridges as such reference points. However, it is hard, or even impossible, to
ensure that identical points are sampled on different ﬁngerprint images, and a too
dense sampling of points will make the matching computationally prohibitive. On
the contrary, sweat pores (as well as minutiae) are unique biological characteristics
and are persistent on a ﬁnger throughout the life. Compared with minutiae, they are
much more abundant on small partial ﬁngerprints. Therefore, the pores can serve as
reliable reference points in aligning partial ﬁngerprint images. Although pore
shapes and sizes are also important and biophysically distinctive features (Bindra
et al. 2000), they cannot be reliably captured on ﬁngerprint images because they are
greatly affected by the pressure of the ﬁngertip against the scanner. On the other
hand, the pore statuses can change between open and close from time to time.
Therefore, in general only the locations of pores are used in recognizing the pores
and the ﬁngerprints (Bazen et al. 2000).
Considering the plenty of pores on partial ﬁngerprints, in this chapter we
introduce, to the best of our knowledge, for the ﬁrst time an approach to aligning
partial ﬁngerprints based on the pores reliably extracted from high resolution partial
ﬁngerprint images. This approach, by making use of the pores on ﬁngerprints as
reference feature points, can effectively align partial ﬁngerprints and estimate the
transformation between them even when there is a small overlap and large trans-
lation and rotation. We ﬁrst propose an efﬁcient method to extract pores, and then
present a descriptor of pores, namely the Pore-Valley Descriptor (PVD), to deter-
mine the correspondences between them. The PVD describes a pore using its
location and orientation, the ridge orientation inconsistency in its neighborhood,
and the structure of valleys surrounding it. The partial ﬁngerprints are ﬁrst matched
based on their PVDs, and the obtained pore correspondences are further reﬁned
using the global geometrical relationship between the pores. The transformation
parameters are then calculated from the best matched pores. The experiments
demonstrate that the proposed PVD-based alignment method can effectively detect
corresponding pores and then accurately estimate the transformation between
partial ﬁngerprints. It is also shown that the proposed alignment method can
signiﬁcantly improve the recognition accuracy of partial ﬁngerprint recognition.
2.1
Introduction
19
www.ebook3000.com

2.2
Feature Extraction
The ﬁngerprint features, including pores, ridges and valleys, will be used in the
proposed method. The extraction of ridge orientations and frequencies and ridge
maps has been well studied in the literature (Ratha and Bolle 2004; Maltoni et al.
2003). In this chapter, we use the classical methods proposed by Jain et al. (Jain
et al. 1997; Hong et al. 1998) to extract ridge orientations, frequencies and ridge
maps. Because ridges and valleys are complementary on ﬁngerprints, it is a simple
matter to get skeleton valley maps by thinning the valleys on the complement of
ridge maps. To extract pores, we divide the ﬁngerprint into blocks and use Gaussian
matched ﬁlters to extract them block by block. The scales of Gaussian ﬁlters are
adaptively determined according to the ridge frequencies on the blocks. After
extracting orientation ﬁelds, valleys, and pores, we can then generate the Pore-
Valley descriptor for each pore. Next we describe the feature extraction methods in
detail.
2.2.1
Ridge and Valley Extraction
The considered partial ﬁngerprint image has a higher resolution (approx. 1200 dpi
in this chapter) than the conventional ﬁngerprints (about 500 dpi) so that level-3
features such as pores can be reliably extracted from them. To extract ridges and
valleys, it is not necessary to directly work on images of such a high resolution. In
order to save computational cost, we smooth the image and down-sample it to half
of its original resolution, and use the method in (Hong et al. 1998) to calculate the
ridge orientations and frequencies. Based on local ridge orientations and frequen-
cies, a bank of Gabor ﬁlters are used to enhance the ridges on the ﬁngerprint. The
enhanced ﬁngerprint image is then binarized to obtain the binary ridge map.
On ﬁngerprints, valleys and ridges are complementary to each other. Therefore,
we can easily get the binary valley map as the complement of the binary ridge map.
In order to exclude the effect of background on complement calculation, the
ﬁngerprint region mask (Hong et al. 1998) is employed to ﬁlter out the background
if any. The binary valley map is then thinned to make all valleys be single-pixel
lines. On the resulting skeleton valley map, there could be some false and broken
valleys due to scars and noise. Thus we post-process it by connecting valley endings
if they are very close and have opposite directions, and by removing valley
segments between valley endings and/or valley bifurcations if they are very short
or their orientations differ much from the local ridge orientations. Finally, we
up-sample the obtained ridge orientation and frequency images, binary ridge map
and skeleton valley map to the original resolution. Figure 2.2b shows the skeleton
valley map extracted from the original ﬁngerprint fragment in Fig. 2.2a.
20
2
High Resolution Partial Fingerprint Alignment

2.2.2
Pore Extraction
Referring to Figs. 2.1 and 2.2a, on the ﬁngerprint images captured using an optical
contact ﬁngerprint sensor, ridges (valleys) appear as dark (bright) lines, whereas
pores are bright blobs on ridges, either isolated (i.e. closed pores) or connected with
valleys (i.e. open pores). In general pores are circle-like structures and their spatial
distributions are similar to 2-D Gaussian functions. Meanwhile, the cross-sections
of valleys are 1-D Gaussian-like functions with different scales. To be speciﬁc,
valleys usually have bigger scales than pores. Based on this observation, we use two
2-D Gaussian ﬁlters, one with a small scale and the other with a large scale, to
enhance the image. The difference between their outputs can then give an initial
pore extraction result. This procedure is basically the DoG (difference of Gaussian)
ﬁltering, which is a classic blob detection approach. The difﬁculty here is how to
estimate the scales of the Gaussian ﬁlters.
Considering that the scale of either pores or valleys is usually not uniform across
a ﬁngerprint image and different ﬁngerprints could have different ridge/valley
frequencies, we partition the ﬁngerprint into a number of blocks and estimate
adaptively the scales of Gaussian ﬁlters for each block. Take a block image IB as
Fig. 2.2 (a) Original ﬁngerprint image; (b) extracted skeleton valley map; Gaussian ﬁltering
output (c) at a small scale and (d) at a large scale; (e) difference between (c) and (d); (f) extracted
pores after post-processing (pores are marked by red circles) (Color ﬁgure online)
2.2
Feature Extraction
21
www.ebook3000.com

an example. Suppose the mean ridge period over this block is p. It is a good measure
of the scale in its corresponding ﬁngerprint block. Thus, we set the standard
deviations of the two Gaussian ﬁlters to k1p and k2p respectively (0 < k1 < k2 are
two constants). The outputs of them are
F1 ¼ Gk1p
∗IB, F2 ¼ Gk2p
∗IB
ð2:1Þ
Gσ x; y
ð
Þ ¼
1ﬃﬃﬃﬃﬃ
2π
p
σ
ex2þy2
2σ2  mG,
xj j, yj j  3σ
ð2:2Þ
where ‘*’ denotes convolution and mG is used to normalize the Gaussian ﬁlter to be
zero-mean. Note that the settings of k1 and k2 should take into consideration the
ridge and valley widths and the size of pores. In our experiments, we empirically
chose the values for them based on the ﬁngerprint database we used. The ﬁltering
outputs F1 and F2are further normalized to [0, 1] and binarized, resulting in B1 and
B2. The small scale Gaussian ﬁlter Gk1p will enhance both pores and valleys,
whereas the large scale ﬁlter Gk2p will enhance valleys only. Therefore, subtracting
B2 from B2, we obtain the initial result of pore extraction: PB ¼ B1  B2.
To remove possible spurious pores from the initial pore extraction result PB, we
apply the following constraints to post-process the result. (1) Pores should reside on
ridges only. To implement this constraint, we use the binary ridge map as a mask to
ﬁlter the extracted pores. (2) Pores are circle-like features. We require that for a true
pore, the eccentricity of its region should be less than a threshold. From Fig. 2.2e, f,
it can be seen that this operation can successfully remove the spurious pores caused
by valley contours, i.e. those line-shaped features in Fig. 2.2e. (3) Pores should be
within a range of valid sizes. We measure the size of a pore by counting the pixels
inside its region. In our experiments, we set the size between 3 and 30. (4) The mean
intensity of a true pore region should be large enough and its variance should be
small. Otherwise, the detected pores are viewed as false ones caused by noise.
Finally, we get the extracted pore image. Figure 2.2c–f illustrate the pore extraction
process of the ﬁngerprint in Fig. 2.2a. It is worth mentioning that some other
methods based on similar assumption (i.e. pores are circle-like features) have also
been proposed in the literature (Jain et al. 2007; Ray et al. 2005). Compared with
those methods, the pore extraction method proposed here takes into consideration
the varying pore scales and thus has better pore extraction accuracy according to
our experiments. Since it is out of the scope of this chapter, we do not make further
discussion on this topic here due to the limit of space.
2.2.3
Pore-Valley Descriptors
In order to use pores to align ﬁngerprints, a descriptor is needed to describe the pore
features so that the correspondences between pores can be accurately determined. A
good descriptor should be invariant to the deformations of rotation and translation,
22
2
High Resolution Partial Fingerprint Alignment

which are very common when capturing ﬁngerprints. Most previous studies on pore
based ﬁngerprint recognition (Kryszczuk et al. 2004a, b; Stosz and Alyea 1994; Jain
et al. 2007) describe a pore simply by its location because they compare the pores
on two ﬁngerprints with the alignment between the two ﬁngerprints known or
estimated beforehand. However, if the alignment is not given, it is not sufﬁcient
to tell one individual pore from others by using only the location feature. Thus, it is
necessary to employ some other information which can be useful in distinguishing
pores. According to recent work on minutia-based ﬁngerprint recognition methods,
the ridge and valley structures and the ridge orientation ﬁeld surrounding minutiae
are also very important in minutia matching (Tico and Kuosmanen 2003; Feng
2008). Thus in this section we describe pores by using the neighboring valley
structures and ridge orientation ﬁeld. We call the resulting descriptor the Pore-
Valley Descriptor (PVD).
The basic attribute of a pore is its location (X, Y), which is deﬁned as the column
and row coordinates of the center of its mass. In this chapter, for the purpose of
alignment, we introduce the orientation feature θ for a pore. It is deﬁned as the ridge
orientation at (X, Y). Referring to Fig. 2.3, in order to sample the valley structures in
the pore’s neighborhood, we establish a local polar coordinate system by setting the
pore’s location as origin and the pore’s orientation as the polar axis pointing to the
right/bottom side. The polar angle is set as the counterclockwise angle from the
polar axis. A circular neighborhood, denoted by Np, is then chosen. It is centered at
the origin with radius being Rn ¼ knpmax, where pmax is the maximum ridge period
on the ﬁngerprint and kn is a parameter to control the neighborhood size. Some
radial lines are drawn starting from φ1 ¼ 0
 with a degree step θs until φm ¼ m  θs,
where m ¼ b360∘/θsc is the total number of radial lines.
0
90
L1,2
L1,1
L9,5
n9 =5
L9,1
n1=2
Fig. 2.3 Illustration of a
Pore-Valley Descriptor with
kn ¼ 4 and θs ¼ 22.5
2.2
Feature Extraction
23
www.ebook3000.com

For each line, we ﬁnd where it intersects with valleys in the neighborhood. These
intersections together with the pore give rise to a number of line segments. We
number these segments from inside to outside and calculate their lengths. As shown
in Fig. 2.3, a degree of 22.5 is taken as the step and hence 16 lines are employed.
Taking the 0 degree and 180 degree lines as examples, the former has two segments
and the latter has ﬁve segments. The ridge orientation ﬁeld in the pore’s neighbor-
hood is another important feature. We deﬁne the ridge orientation inconsistency
(OIC) in Np as follows to exploit this information:
OIC Np


¼ 1
jNpj
X
i;j
ð
Þ2Np
cos 2OF i;j
ð
Þ
ð
Þmcos
½
2þ sin 2OF i;j
ð
Þ
ð
Þmsin
½
2
n
o
ð2:3Þ
where OF is the ridge orientation ﬁeld, jNpj denotes the number of pixels in Np,
mcos ¼ P
i;j
ð
Þ2Npcos 2OF i;j
ð
Þ
ð
Þ= j Np j and msin ¼ P
i;j
ð
Þ2Npsin 2OF i;j
ð
Þ
ð
Þ= j Np j.
With the above mentioned features, we deﬁne the PVD as the following feature
vectorΘ:
Θ ¼
X; Y; θ; OIC Np


; S
*
1; S
*
2; . . . ; S
*
m


ð2:4Þ
S
*
k ¼ nk; Lk,1; Lk,2; . . . ; Lk,nk
½
,
k ¼ 1, 2, . . . , m
ð2:5Þ
where nk is the number of line segments along the kth line, and Lk , n is the length of
the nth segment (1  n  nk) along the kth line.
The OIC component and the sampled valley structure features in the proposed
PVD are invariant to rotation and translation because they are calculated in circular
neighborhood of the pore which is intrinsically rotation-invariant and they are
deﬁned with respect to the local coordinate system of the pore. The OIC component
is a coarse feature which captures the overall ridge ﬂow pattern information in the
neighborhood of a pore on a very coarse level. It will be used as an initial step to
roughly match the pores. The sampled valley structure features are ﬁne features.
They will be used as the second step to accurately match pores. The pore locations
and orientations will be used to double check pore correspondences. Finally, the
transformation between ﬁngerprints will be estimated based on the locations and
orientations of their corresponding pores. In the next section, we will present the
proposed PVD-based alignment algorithm.
2.3
PVD-Based Partial Fingerprint Alignment
This chapter aims to align partial ﬁngerprints by using pores. To this end, we need
to ﬁrst identify pore correspondences on ﬁngerprints. However, even a small
ﬁngerprint fragment can carry many pores (hundreds in the 6.24  4.68 mm2
fragments used in our experiments), making it very time consuming to match
24
2
High Resolution Partial Fingerprint Alignment

pores in pairs directly using their surrounding valley structures (i.e. the segment
lengths recorded in the PVD). Therefore, a coarse-to-ﬁne matching strategy is
necessary. The OIC components in the PVD can serve for the coarse matching.
Given two pores, we ﬁrst compare their OIC features. If the absolute difference
between their OIC features is larger than a given threshold Toic, they will not be
matched; otherwise, proceed to the next ﬁne matching step.
Coarse matching will eliminate a large number of false matches. In the subse-
quent ﬁne matching, we compare the valley structures in the two pores’ neighbor-
hoods. According to the deﬁnition of PVD, each pore is associated with several
groups of line segments which capture the information of its surrounding valleys.
We compare these segments group by group. When comparing the segments in the
kth group, where there are n1
k and n2
k segments in the two pores’ descriptors, we ﬁrst
ﬁnd the common segments in the group, i.e. the ﬁrst n
_
k ¼ min n1
k; n2
k


segments.
The dissimilarity between the two pores is then deﬁned as
X
m
k¼1
X
n
_
k
n¼1
j L1
k,n  L2
k,n j
n
_
k
þ n1
k  n2
k

2
n1
k  n2
k
0
@
1
A
ð2:6Þ
The ﬁrst term in the formula calculates the mean absolute difference between all
common segments in each group, and the second term is to penalize the missing
segments. The smaller the dissimilarity is, the more similar the two pores are. After
comparing all possible pairs of pores which pass coarse matching, each pair of
pores is assigned with a dissimilarity calculated by Eq. 2.6. They are then sorted
ascendingly according to the dissimilarities, producing the initial correspondences
between the pores.
The top K initial pore correspondences (i.e. those with the smallest degree of
dissimilarity) are further double checked to get the ﬁnal pairs of corresponding
pores for alignment transformation estimation. The purpose of double checking is
to calculate the supports for all pore correspondences based on the global geomet-
rical relationship between the pores. At the beginning of double checking, the
supports to all pore correspondences are initialized to zero. Figure 2.4 illustrates
the relevant measures we use.
Fig. 2.4 Illustration of the
relevant measures used in
pore correspondence double
checking
2.3
PVD-Based Partial Fingerprint Alignment
25
www.ebook3000.com

Assume
P1
1; P1
2


and
P2
1; P2
2


are two pairs of corresponding pores among the
top ones. To check them, we compare (1) the distances, denoted by d1 and d2,
between the pores on the two ﬁngerprints; and (2) the angles, denoted by
α1
1; α1
2


and
α2
1; α2
2


, between their orientations and the lines connecting them. If both the
distance differences and the angle differences are below the given thresholds Td and
Tα, i.e.
d1  d2
j
j  Td,
α1
1  α1
2
		
		  Tα, α2
1  α2
2
		
		  Tα
ð2:7Þ
the supports for these two correspondences are increased by 1; otherwise, the
support for the correspondence with higher dissimilarity is decreased by 1, whereas
the support for the other one stays the same. After checking all the top
K correspondences two by two, those with a non-negative support are taken as
the ﬁnal pore correspondences. If none of the correspondences has non-negative
support, the two ﬁngerprints cannot be aligned.
If some corresponding pores are found, we can then estimate the transformation
between the two ﬁngerprints. Here, we consider rotation and translation (since all
the ﬁngerprints are captured by the same type of scanner, we assume that the scaling
factor is one) as follows
~X2
~Y2


¼
cos β
 sin β
sin β
cos β


X2
Y2


þ
ΔX
ΔY


¼ R X2
Y2


þ t
ð2:8Þ
where (X2, Y2) are the coordinates of a pore on the second ﬁngerprint and
~X2; ~Y2


are its transformed coordinates in the ﬁrst ﬁngerprint’s coordinate system,
R ¼
cos β
 sin β
sin β
cos β


is the rotation matrix and t ¼
ΔX
ΔY


is the translation
vector. Our goal is to estimate the transformation parameters (β, ΔX, ΔY), where β
is the rotation angle and ΔX and ΔY are the column and row translations
respectively.
If there is only one pair of corresponding pores found on the two ﬁngerprints, we
directly estimate the transformation parameters by the locations and orientations of
the two pores, (X1, Y1, θ1) and (X2, Y2, θ2), as follows:
β ¼
β1
if abs β1
ð
Þ  abs β2
ð
Þ
β2
else

ð2:9Þ
ΔX ¼ X1  X2 cos β þ Y2 sin β
ð2:10Þ
ΔY ¼ Y1  X2 sin β  Y2 cos β
ð2:11Þ
where β1 ¼ θ1  θ2 and β2 ¼ sgn(β1)  (|β1|  π).
If there are more than one pairs of corresponding pores, we employ the method
similar to (Haralick et al. 1989) to estimate the rotation and translation parameters
26
2
High Resolution Partial Fingerprint Alignment

based on the locations of the corresponding pores. Let
X i
1; Y i
1


ji ¼ 1; 2; . . . ; C


and
X i
2; Y i
2


ji ¼ 1; 2; . . . ; C


be C pairs of corresponding pores. We determine
R and t by minimizing
1
C
X
C
i¼1
X i
1
Y i
1


 R X i
2
Y i
2


 t


2
ð2:12Þ
Following the proof in (Haralick et al. 1989), it is easy to show that
t ¼
X1
Y1


 R
X2
Y2


ð2:13Þ
where Xj ¼
PC
i¼1 X i
j


=C, Yj ¼
PC
i¼1 Y i
j


=C, j ¼ 1 , 2. Let
B ¼ 1
C
PC
i¼1 X i
1  X1


X i
2  X2


PC
i¼1 Y i
1  Y1


X i
2  X2


PC
i¼1 X i
1  X1


Y i
2  Y2


PC
i¼1 Y i
1  Y1


Y i
2  Y2


"
#
ð2:14Þ
and its singular value decomposition be B ¼ UDV, then R ¼ VUT and β ¼ arcsin
(R21), R21 where is the entry at the second row and ﬁrst column ofR.
2.4
Experiments
In general, the feature of pores can only be reliably extracted from ﬁngerprints with
a resolution of at least 1000 dpi (CDEFFS 2008). So far there is no such free
ﬁngerprint image database available in the public domain. Therefore, we
established a set of high resolution partial ﬁngerprint images by using a custom-
built ﬁngerprint scanner of approximate 1200 dpi (refer to Fig. 2.5 for example
images). With the established high resolution partial ﬁngerprint image dataset, we
evaluate the proposed ﬁngerprint alignment method in comparison with a minutia-
based method and an orientation ﬁeld-based method. Next in subsection 2.4.1 we
ﬁrst introduce the collected dataset of high resolution partial ﬁngerprint images. In
subsection 2.4.2 we investigate the two parameters involved in the method. Sub-
section 2.4.3 compares the proposed method with the minutia based method in
corresponding feature point detection. Subsection 2.4.4 compares the proposed
method with the orientation ﬁeld based method in alignment transformation esti-
mation. In subsection 2.4.5 we compare the three methods in terms of ﬁngerprint
recognition accuracy. Finally, in subsection 2.4.6, we analyze the computational
complexity of the method.
2.4
Experiments
27
www.ebook3000.com

2.4.1
The Dataset
We ﬁrst collected 210 partial ﬁngerprint images from 35 ﬁngers as the training set
for parameter selection and evaluation, and then collected 1480 ﬁngerprint frag-
ments from 148 ﬁngers (including the ﬁngers in the training set) as the test set for
performance evaluation. The data were collected in two sessions (about 2 weeks
apart). Most of the participants are students and staff in our institute, whose ages are
between 20 and 50 years old. In the training set, there are three images captured
from each ﬁnger in each session; whereas in the test set, each ﬁnger has ﬁve images
scanned in each of the two sessions.
The resolution of these ﬁngerprint images is approximately 1200 dpi and their
spatial size is 320 pixels in width and 240 pixels in height. Therefore, they cover an
area of about 6.5 mm by 4.9 mm on ﬁngertips. When capturing the ﬁngerprint
images, we simply asked the participants to naturally put their ﬁngers against the
prism of the scanner without any exaggeration of ﬁngerprint deformation. As a
result, typical transformations between different impressions of the same ﬁnger in
Fig. 2.5 Example ﬁngerprint images used in the experiments. Their quality indexes are (a)
0.8777, (b) 0.7543, (c) 0.6086, and (d) 0.5531 according to the frequency domain quality index
deﬁned in Chen et al. (2005)
28
2
High Resolution Partial Fingerprint Alignment

the dataset include translations with tens of pixels and rotations by around eight
degrees. The maximal translations and rotations are respectively about 200 pixels
and 20 degrees. Hence, the minimal overlap between a ﬁnger’s different impres-
sions is about one fourth of the ﬁngerprint image area. In subsequent experiments,
we will give representative examples of these cases.
The resolution of these ﬁngerprint images is approximately 1200 dpi and their
spatial size is 320 pixels in width and 240 pixels in height. Therefore, they cover an
area of about 6.5 mm by 4.9 mm on ﬁngertips. When capturing the ﬁngerprint
images, we simply asked the participants to naturally put their ﬁngers against the
prism of the scanner without any exaggeration of ﬁngerprint deformation. As a
result, typical transformations between different impressions of the same ﬁnger in
the dataset include translations with tens of pixels and rotations by around eight
degrees. The maximal translations and rotations are respectively about 200 pixels
and 20 degrees. Hence, the minimal overlap between a ﬁnger’s different impres-
sions is about one fourth of the ﬁngerprint image area. In subsequent experiments,
we will give representative examples of these cases.
2.4.2
The Neighborhood Size and Sampling Rate
The proposed alignment method uses the valley structures in the neighborhood of
pores. The valley structures are sampled along a number of different directions,
determined by the degree step θs. Here we refer to θs as the sampling rate of
directions. Obviously, the neighborhood size and sampling rate are two critical
parameters in the proposed alignment method. We set the neighborhood size as kn
times the maximum ridge period. Intuitively, a small kn or large θs will cost less
computational resource but will make the resulting PVDs less discriminative,
whereas a large kn or small θs will lead to more noise-sensitive and costly PVDs.
We evaluated the effect of kn and θs on the accuracy of corresponding feature point
detection using 50 pairs of ﬁngerprints which were randomly chosen from the
training set. Each pair is from the same ﬁnger but taken at different sessions.
These ﬁngerprints show different quality. Some example images are shown in
Fig. 2.5.
We used two measures to evaluate the accuracy: the percentage of correct top
one pore correspondence (M1) and the average percentage of correct correspon-
dences among the top ﬁve pore correspondences (M2). Let N be the total number of
pairs of ﬁngerprint images, and NT1the number of pairs of ﬁngerprints on which the
top one pore correspondence is correct. We also counted the number of correct pore
correspondences among the top ﬁve correspondences on each pair of ﬁngerprints.
Denote by N i
T5 the number of correct pore correspondences among the top ﬁve
correspondences on the ith pair of ﬁngerprints. Then the two measures M1 and M2
are deﬁned as
2.4
Experiments
29
www.ebook3000.com

M1 ¼ NT1=N
ð2:15Þ
M2 ¼ 1
N
X
N
i¼1
N i
T5=5
ð2:16Þ
We investigated several combinations of different values for kn and θs,
i.e. kn 2 {3, 3.5, 4, 4.5, 5} and θs 2 {15∘, 18∘, 20∘, 22.5∘, 30∘, 45∘}. Table 2.1 lists
the results on the 50 pairs of ﬁngerprints. From the results, we can see that the
best accuracy is obtained at a sampling rate o θs ¼ 20∘or θs ¼ 22.5∘, and no
signiﬁcant difference is observed between these two different sampling rates.
With respect to the neighborhood size, it appears that kn ¼ 4 is a good choice.
Furthermore, it was observed that neither too small nor too large neighborhoods can
produce the best accuracy. In our following experiments, considering both the
accuracy and the computational cost, we set kn ¼ 4 and θs ¼ 22.5∘. Note that the
settings of these two parameters should be dependent on the resolution of ﬁnger-
print images and the population from which the ﬁngerprint images are captured. If a
different ﬁngerprint image dataset is used, the above training process has to be done
again by using a subset of the ﬁngerprint images in that dataset.
2.4.3
Corresponding Feature Point Detection
Detecting feature point correspondences is an important step in the proposed
alignment method as well as in many state-of-the-art minutia-based methods. The
optimal alignment transformation is estimated based on the detected corresponding
feature
points
(i.e.
pores
or
minutiae).
Considering
the
signiﬁcance
of
corresponding feature point detection, we carried out experiments to compare the
proposed method with a representative minutia-based method (Chen et al. 2006a) in
terms of corresponding feature point detection accuracy. In the experiments, we
used 200 pairs of partial ﬁngerprints randomly chosen from the training set for
evaluation. In each pair, the two ﬁngerprints are from the same ﬁnger but were
captured at different sessions.
Table 2.1 Accuracies (%) of corresponding pore detection on 50 pairs of ﬁngerprints under
different settings of kn and θs
M1/M2 (%)
θs
15
18
20
22.5
30
45
kn
3
32/45.2
44/51.6
50/57.5
50/58.2
46/52.5
40/49.1
3.5
52/60.1
62/75.2
74/80.2
78/82.5
70/69.6
58/62.5
4
66/74.6
80/80.5
96/95.1
98/94.7
94/88.5
80/78.2
4.5
76/80.2
84/86
88/90.5
86/89.1
80/78.1
72/70.6
5
54/49
62/56.7
66/60.5
60/61.7
54/59.2
52/52.1
30
2
High Resolution Partial Fingerprint Alignment

Figure 2.6 shows some example pairs of ﬁngerprint fragments with the detected
corresponding minutiae (left column) or pores (right column). When there are more
than ﬁve pairs of corresponding minutiae or pores, we show only the ﬁrst ﬁve pairs.
In Fig. 2.6a, b, both methods can correctly ﬁnd the top ﬁve feature point corre-
spondences. However, when the ﬁngerprint quality changes between sessions, for
example because of perspiration, the minutiae based method will tend to detect
false minutiae and hence false minutia correspondences. In Fig. 2.6c, broken
valleys occur on the second ﬁngerprint. As a result, the detected two minutia
correspondences are incorrect. Instead, the proposed PVD-based method is more
robust and can correctly detect the corresponding pores as shown in Fig. 2.6d.
The ﬁngerprint fragments in Fig. 2.6e have large deformation and small overlap.
Consequently, few (fewer than ten) minutiae can be found in their overlapping
region. In this case, the minutia-based method fails again because there lack
sufﬁcient minutiae. Actually, even when two partial ﬁngerprints overlap much,
there could still be very few minutiae available on them because of the small
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
Fig. 2.6 Examples of corresponding feature point detection results using minutia based (left
column) and PVD based (right column) methods
2.4
Experiments
31
www.ebook3000.com

ﬁngerprint areas. As can be seen in Fig. 2.6g, some false correspondences are
detected on the two fragments due to insufﬁcient minutiae. In contrast, as shown
in Fig. 2.6f, h, the results by the proposed PVD-based method on these partial
ﬁngerprints are much better.
We calculated the two measures, M1 and M2, for the two methods on all the
200 pairs of partial ﬁngerprints. The results are listed in Table 2.2. It can be seen
that the minutia-based method works poorly whereas the proposed PVD-based
method can detect the corresponding feature points with a very high accuracy,
achieving signiﬁcant improvements over the minutia-based method. This demon-
strates that the PVD-based alignment method can cope with various ﬁngerprint
fragments more accurately than the minutia based method, largely thanks to the
abundance and distinctiveness of pores on ﬁngerprints. Since the alignment trans-
formation estimation is based on the detected corresponding feature points, it is
obvious that the PVD-based method will also estimate the alignment transformation
more accurately than the minutia-based method. Next, we compare the PVD-based
method with an orientation ﬁeld-based method in terms of alignment transforma-
tion estimation accuracy.
2.4.4
Alignment Transformation Estimation
After obtaining the pore correspondences on two ﬁngerprints, we can then estimate
the alignment transformation between them based on the corresponding pores. To
quantitatively evaluate the performance of the proposed method in alignment
transformation estimation, we need some ground truth ﬁngerprint fragment pairs.
To this end, we randomly chose ten pairs of ﬁngerprints from the test set (each pair
was captured from the same ﬁnger but in two different sessions), and manually
computed their transformations as the ground truth. Because we consider only
translation and rotation here, we need at least two pairs of corresponding feature
points on a pair of ﬁngerprints to calculate the transformation between them.
Therefore, we ﬁrst manually marked two pairs of corresponding feature points on
each of the ten pairs of ﬁngerprints. Based on the coordinates of the two pairs of
corresponding feature points, we then directly computed the transformation
between the pair of ﬁngerprints by solving a set of equations. The obtained ground
truth on the ten pairs of ﬁngerprints is given in 2.3. The ﬁrst three pairs of
ﬁngerprints are shown in Fig. 2.7. From Table 2.3, we can see that these chosen
ﬁngerprint pairs display translations from less than 10 pixels to about 180 pixels and
rotations from less than ﬁve degrees to more than 10 degrees. In our experiments,
Table 2.2 Accuracies of
corresponding feature point
detection by the two methods
M1
M2
Minutia based method (Chen et al. 2006b)
40%
35.1%
PVD based method
98%
95.5%
32
2
High Resolution Partial Fingerprint Alignment

we have observed that typical transformations in the dataset are translations by tens
of pixels and rotations by around eight degrees. In this part, we compared our
proposed method with the steepest descent orientation ﬁeld (OF) based alignment
method (Yager and Amin 2006) in terms of alignment transformation estimation
accuracy using the chosen ﬁngerprint pairs.
Table 2.3 lists the estimation results by the OF based method (the step sizes of
translation and rotation are set as one pixel and one degree respectively) and the
proposed method on the chosen ﬁngerprint pairs. Figure 2.8 illustrates the aligned
ﬁngerprint images by overlaying the ﬁrst image with the transformed second image
in the pair shown in Fig. 2.7. Obviously, the PVD-based method estimates the
Fig. 2.7 Three of the ten chosen pairs of ﬁngerprint fragments. (a) and (b), (c) and (d), (e) and (f)
are the ﬁrst three pairs of ﬁngerprints listed in Table 2.3
Table 2.3 Alignment transformation ground truth and estimation results by the two methods
Index of
ﬁngerprint pair
Ground truth
OF based method
(Yager and Amin 2006)
PVD based method
ΔY
ΔX
β
ΔY
ΔX
β
ΔY
ΔX
β
01
56
23
11.01
11
3
2.00
61
33
13.27
02
90
33
15.95
43
51
1.00
93
26
16.89
03
100
181
2.87
8
72
31.02
91
176
2.93
04
11
8
3.16
1
1
0.00
11
6
1.98
05
90
8
4.19
100
0
1.00
89
6
3.88
06
69
74
5.44
23
21
0.00
72
78
9.33
07
78
137
3.45
45
24
0.00
76
142
6.40
08
87
2
0.96
74
1
1.00
93
7
2.23
09
73
39
4.40
69
47
3.00
79
50
0.60
10
19
4
11.25
4
2
1.00
12
1
8.61
2.4
Experiments
33
www.ebook3000.com

transformation parameters much more accurately and it does not have the initial-
ization and quantization problems, which will affect greatly the performance of OF
based method. Moreover, there is no guarantee that the OF based method will
always converge to the global optimal solution. In fact, it can be easily trapped at
local minima, for example the third pair of ﬁngerprints which has small overlap
(refer to the last column in Figs. 2.7 and 2.8). In Table 2.4, we list the average
absolute errors of the two methods over the chosen ten ﬁngerprint pairs. These
results clearly demonstrate that the PVD-based method can recover the transfor-
mation between partial ﬁngerprints more accurately.
2.4.5
Partial Fingerprint Recognition
We have also evaluated the proposed alignment method in partial ﬁngerprint
recognition by setting up a simple partial ﬁngerprint recognition system as shown
in Fig. 2.9. In this system, the alignment transformation is ﬁrst estimated between
an input ﬁngerprint image and a template ﬁngerprint image by using one of the
three methods: minutia-based method, orientation ﬁeld based method, and the
Fig. 2.8 Alignment results of PVD based method (a, c, e) and OF based method (b, d, f) on the
ﬁngerprint pairs shown in Fig. 2.7a, b, Fig. 2.7c, d, and Fig. 2.7e, f
Table 2.4 Average absolute errors (AAE) by the two methods
AAE (ΔY)
AAE (Δx)
AAE (β)
OF based method (Yager and Amin 2006)
33.9
48.5
8.5
PVD based method
4.2
5.4
2.5
34
2
High Resolution Partial Fingerprint Alignment

proposed PVD based method. As for the matcher, we employed two different
approaches. The ﬁrst one is a minutia and pore based matcher (called MINU-
PORE matcher). It matches the minutiae and pores on the ﬁngerprints, and then
fuses the match scores of minutiae and pores to give a similarity score between the
ﬁngerprints. The second approach is an image-based matcher called GLBP matcher
based on Gabor and Local Binary Patterns (LBP), recently proposed by Nanni and
Lumini (2007). Please note that the purpose of the experiments here is to compare
the contributions of the three different alignment methods to a ﬁngerprint matcher.
Therefore, we did not do any optimization on the matchers but considered only the
relative improvement between the three alignment methods.
The MINU-PORE matcher we implemented works as follows. The minutiae and
pores on the input ﬁngerprint image are transformed into the coordinate system of
the template ﬁngerprint according to the estimated transformation. Minutiae and
pores on the two ﬁngerprint images are then matched separately. Two minutiae are
thought to be matched if the difference between their locations and the difference
between their directions are both below the given thresholds (15 pixels for location
differences and 30 degrees for direction differences in our experiments). As for two
pores, if the difference between their locations is below a given threshold (15 pixels
in our experiments), they are matched. The minutia matching score is deﬁned as the
ratio between the number of matched minutiae to the total number of minutiae, and
the pore matching score is deﬁned similarly. The ﬁnal matching score is obtained by
fusing the minutia and pore matching scores using the summation rule.
By using the MINU-PORE matcher on the test set, we conducted the following
matches. (1) Genuine matches: each of the ﬁngerprint images in the second session
was matched with all the ﬁngerprint images of the same ﬁnger in the ﬁrst session,
resulting in 3700 genuine match scores. (2) Imposter matches: the ﬁrst ﬁngerprint
image of each ﬁnger in the second session was matched with the ﬁrst ﬁngerprint
images of all the other ﬁngers in the ﬁrst session, resulting in 21,756 imposter match
scores. Based on the obtained match scores, we calculated the equal error rate
(EER) of each of the three alignment methods: 29.5% by the PVD based alignment
method, 38.66% by the Minutia based alignment method, and 41.03% by the OF
based alignment method. The receiver operating characteristic (ROC) curves of the
Fig. 2.9 The simple partial ﬁngerprint recognition system used in the experiments. FI and FT
denote the input and template ﬁngerprints respectively
2.4
Experiments
35
www.ebook3000.com

three methods are plotted in Fig. 2.6. It can be clearly seen that the proposed PVD
based alignment method makes much improvement in EER, speciﬁcally 23.69%
over the minutia based method and 28.1% over the OF based method (Fig. 2.10).
As for the GLBP matcher, we ﬁrst transform the input ﬁngerprint image into the
coordinate system of the template ﬁngerprint image according to the estimated
alignment transformation, then extract the Gabor-LBP feature vectors from the
transformed input ﬁngerprint image and the template ﬁngerprint image (we directly
took the conﬁguration parameters from (Nanni and Lumini 2007)), and ﬁnally
calculate the Euclidean distance between the Gabor-LBP feature vectors of the
input ﬁngerprint image and the template ﬁngerprint image. By using the GLBP
matcher, we carried out the same matching scheme as in the MINU-PORE matcher.
As a result, the PVD-based alignment method leads to the EER of 34.85%, the
minutia based alignment method 39.98%, and the OF based alignment method
45.11%. Figure 2.11 shows the corresponding ROC curves. Compared with the
other two methods, the proposed PVD based alignment method achieves 12.83%
and 22.74% improvement in EER respectively. In all the experiments, it is observed
that matching errors are largely caused by inaccurate alignments. This validates that
the proposed alignment method is more suitable for partial ﬁngerprints and can
signiﬁcantly improve the accuracy of partial ﬁngerprint recognition. Although the
EER obtained here is relatively high, this is because the recognition of partial
ﬁngerprint images is itself very challenging due to the limited feature.
100
90
80
70
60
PVD based alignment
Minutia based alignment
OF based alignment
50
40
30
20
0
5
10
15
20
25
30
35
40
FAR (%)
FAR (%)
Fig. 2.10 The ROC curves of the MINU-PORE matcher by using the three alignment methods
36
2
High Resolution Partial Fingerprint Alignment

2.4.6
Computational Complexity Analysis
The proposed PVD based alignment method has the following main steps for each
pair of ﬁngerprint images to be aligned: (A) ridge orientation and frequency
estimation; (B) ridge and valley extraction; (C) pore extraction; (D) PVD genera-
tion;
(E)
PVD
comparison;
(F)
pore
correspondence
reﬁnement;
and
(G) transformation estimation. The ﬁrst two steps (A) and (B) are common to
most automatic ﬁngerprint recognition systems. The last step (G) involves a
singular value decomposition of a 2*2 matrix, which can be implemented very
efﬁciently. We have implemented the method by using Matlab and executed it on a
PC with a 2.13 GHz Intel(R) Core(TM) 2 6400 CPU and RAM of 2GB. It takes
about 0.02 ms to estimate the transformation from a set of corresponding feature
points. The step F, pore correspondence reﬁnement, needs to calculate some
Euclidean distances and angles, which can also be done in about 0.02 ms. The
step C, pore extraction, is a little bit more time-consuming. The pore extraction
method we used in this chapter is a ﬁltering based approach, which extracts pores
by some linear ﬁlter operations. In our experiments, it takes about two seconds to
extract the pores from a ﬁngerprint image.
The most time-consuming steps are PVD generation (Step D) and comparison
(Step E). Although it does not take much time to generate the PVD for one pore
(about 0.02 s) or to compare the PVD of two pores (about 0.02 ms), processing the
whole set of pores on ﬁngerprints takes more time because of the large quantity of
100
90
80
70
60
PVD based alignment
Minutia based alignment
OF based alignment
50
40
30
0
5
10
15
20
25
30
35
40
FAR (%)
FAR (%)
Fig. 2.11 The ROC curves of the GLBP matcher by using the three alignment methods
2.4
Experiments
37
www.ebook3000.com

pores. With regard to the ﬁngerprint images used in our experiments, there are
averagely around 500 pores on a ﬁngerprint fragment. Therefore, it takes in average
about 10 s and 5 s respectively to generate the PVD for the pores on a ﬁngerprint
fragment and to compare the PVD on two ﬁngerprint fragments. Considering that
we did not optimize the code and that the Matlab code itself has low efﬁciency, we
expect that the computational cost can be much reduced after optimization and the
speed can be signiﬁcantly improved by using languages like C/C++. Compared
with the proposed method, the minutia based method is more efﬁcient, taking
usually less than one second for either extracting or matching the minutiae (but
using C/C++ implementation). As for the OF based method, the time needed to
align two ﬁngerprints depends on a number of factors, such as the amount of
transformation between the ﬁngerprints, the initial estimation of the transformation,
and the step sizes used in the search process. Therefore, it is difﬁcult to draw a
conclusion on its efﬁciency. In our experiments, the OF based method can some-
times converge in less than one second, but sometimes converge after more than
one minute. Generally speaking, the proposed method achieves much higher
alignment
accuracy
than
the
other
two
approaches
with
an
acceptable
computational cost.
2.5
Summary
A new approach was proposed in this chapter to aligning partial high resolution
ﬁngerprints using pores. After pore detection, a novel descriptor, namely Pore-
Valley Descriptor (PVD), was deﬁned to describe pores based on their local
characteristics. Then a coarse-to-ﬁne pore matching method was used to ﬁnd the
pore correspondences based on PVD. With the detected corresponding pores, we
estimated the alignment transformation between the ﬁngerprint fragments. To
evaluate the performance of the proposed PVD based high resolution partial
ﬁngerprint alignment method, we established a set of partial ﬁngerprint images
and used them to compare the proposed method with state-of-the-art minutia-based
and orientation ﬁeld-based ﬁngerprint alignment methods. The experimental results
demonstrated that the PVD-based method can more accurately detect the
corresponding feature points and hence estimate better the alignment transforma-
tion. It was also shown in our experiments that the accuracy of partial ﬁngerprint
recognition can be signiﬁcantly improved by using the PVD based alignment
method.
One important issue in high resolution ﬁngerprint recognition is the stability of
pores. Despite that not all pores will appear in the ﬁngerprint images of the same
person but captured at different times, we experimentally found that usually there
will be enough corresponding pores that can be detected on the ﬁngerprint images
from the same person. It is interesting and very important to further investigate the
statistical characteristics of pores on ﬁngerprint images.
38
2
High Resolution Partial Fingerprint Alignment

Although the PVD based alignment method proposed in this chapter is designed
for high resolution partial ﬁngerprint recognition, it is not limited to partial ﬁnger-
prints. It can also be applied to full ﬁngerprint images. One problem may be the
expensive computational cost caused by the large amount of pore features. One
solution could be to perform coarse registration ﬁrst by using OF based schemes
and then apply the PVD based method for a ﬁne estimation of the alignment
transformation. It also deserves to do more investigation of the discriminative
power of pores.
References
Bazen A, Verwaaijen G, Gerez S, Veelenturf L, Zwaag B (2000) A correlation-based ﬁngerprint
veriﬁcation system. In: Proceedings of the workshop on circuits systems and signal processing,
pp 205–213
Bindra B, Jasuja O, Singla A (2000) Poroscopy: a method of personal identiﬁcation revisited.
Internet J Forensic Med Toxicol 1(1)
Cappelli R, Maio D, Maltoni D (2001) Modeling plastic distortion in ﬁngerprint images. In:
Proceedings of ICAPR, Rio de Janeiro
CDEFFS (2008) Data format for the interchange of extended ﬁngerprint and palmprint features,
Working Draft Version 0.2. http://ﬁngerprint.nist.gov/standard/cdeffs/index.html
Chang S, Hsu W, Wu G (1997) Fast algorithm for point pattern matching: invariant to translations,
rotations and scale changes. Pattern Recogn 30:311–320
Chen Y, Jain A (2007) Dots and incipients: extended features for partial ﬁngerprint matching.
Paper presented at biometric symposium, BCC, Baltimore, Sept 2007
Chen Y, Dass S, Jain A (2005) Fingerprint quality indices for predicting authentication perfor-
mance. In: Proceedings of AVBPA, pp 160–170
Chen X, Tian J, Yang X (2006a) A new algorithm for distorted ﬁngerprints matching based on
normalized fuzzy similarity measure. IEEE Trans Image Process 15:767–776
Chen X, Tian J, Yang X, Zhang Y (2006b) An algorithm for distorted ﬁngerprint matching based
on local triangle feature set. IEEE Trans Inf Forensics Secur 1:169–177
Choi K, Choi H, Lee S, Kim J (2007) Fingerprint image mosaicking by recursive ridge mapping.
IEEE Trans Syst Man Cybern B 37:1191–1203
Feng J (2008) Combining minutiae descriptors for ﬁngerprint matching. Pattern Recogn
41:342–352
Haralick R, Joo H, Lee C, Zhuang X, Vaidya V, Kim M (1989) Pose estimation from
corresponding point data. IEEE Trans Syst Man Cybern 19:1426–1446
Hong L, Wan Y, Jain A (1998) Fingerprint image enhancement: algorithms and performance
evaluation. IEEE Trans Pattern Anal Mach Intell 20(8):777–789
Huvanandana S, Kim C, Hwang J (2000) Reliable and fast ﬁngerprint identiﬁcation for security
applications. Proc Int Conf Image Process 2:503–506
Jain A, Hong L, Bolle R (1997) On-line ﬁngerprint veriﬁcation. IEEE Trans Pattern Anal Mach
Intell 19:302–314
Jain A, Prabhakar S, Hong L, Pankanti S (2000) Filterbank-based ﬁngerprint matching. IEEE
Trans Image Process 9:846–859
Jain A, Che Y, Demirkus M (2007) Pores and ridges: ﬁngerprint matching using level 3 features.
IEEE Trans Pattern Anal Mach Intell 29:15–27
Jea T, Govindaraju V (2005) A minutia-based partial ﬁngerprint recognition system. Pattern
Recogn 38:1672–1684
References
39
www.ebook3000.com

Jiang X, Yau WY (2000) Fingerprint minutiae matching based on the local and global structures.
Proc Int Conf Pattern Recogn 2:1042–1045
Kovacs-Vajna Z (2000) A ﬁngerprint veriﬁcation system based on triangular matching and
dynamic time warping. IEEE Trans Pattern Anal Mach Intell 22:1266–1276
Kryszczuk K, Drygajlo A, Morier P (2004a) Extraction of level 2 and level 3 features for
fragmentary ﬁngerprints. In: Proceedings of the Second COST Action 275 workshop, Vigo,
Spain, pp 83–88
Kryszczuk K, Morier P, Drygajlo A (2004b) Study of the distinctiveness of level 2 and level
3 features in fragmentary ﬁngerprint comparison, BioAW2004. LNCS 3087:124–133
Liu L, Jiang T, Yang J, Zhu C (2006) Fingerprint registration by maximization of mutual
information. IEEE Trans Image Process 15:1100–1110
Maltoni D, Maio D, Jain A, Prabhakar S (2003) Handbook of ﬁngerprint recognition. Springer,
New York
Nanni L, Lumini A (2007) A hybrid wavelet-based ﬁngerprint matcher. Pattern Recogn 40
(11):3146–3151
Nanni L, Lumini A (2008) Local binary patterns for a hybrid ﬁngerprint matcher. Pattern Recogn
41(11):3461–3466
Parthasaradhi S, Derakhshani R, Hornak L, Schuckers SAC (2005) Time-series detection of
perspiration as a liveness test in ﬁngerprint devices. IEEE Trans Syst Man Cybern C
35:335–343
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, New York
Ratha N, Karu K, Chen S, Jain A (1996) A real-time matching system for large ﬁngerprint
databases. IEEE Trans Pattern Anal Mach Intell 18:799–813
Ray M, Meenen P, Adhami R (2005) A novel approach to ﬁngerprint pore extraction. Proceedings
of the 37th south-eastern symposium on system theory, pp 282–286
Roddy A, Stosz J (1997) Fingerprint features – statistical analysis and system performance
estimates. Proc IEEE 85:1390–1421
Ross A, Reisman J, Jain A (2002) Fingerprint matching using feature space correlation. In:
Proceedings of post-European conference on computer vision workshop on biometric authen-
tication, LNCS 2359, pp 48–57
Ross A, Jain A, Reisman J (2003) A hybrid ﬁngerprint matcher. Pattern Recogn 36:1661–1673
Ross A, Dass S, Jain A (2005) A deformable model for ﬁngerprint matching. Pattern Recogn
38:95–103
Stosz JD, Alyea L (1994) Automated system for ﬁngerprint authentication using pores and ridge
structure. In: Proceedings of SPIE conference on automatic systems for the identiﬁcation and
inspection of humans, San Diego, vol 2277, pp 210–223
Teoh A, Ngo D, Song O (2004) An efﬁcient ﬁngerprint veriﬁcation system using integrated
wavelet and Fourier-Mellin invariant transform. Image Vis Comput 22(6):503–513
Tico M, Kuosmanen P (2003) Fingerprint matching using an orientation-based minutia descriptor.
IEEE Trans Pattern Anal Mach Intell 25:1009–1014
Yager N, Amin A (2005) Coarse ﬁngerprint registration using orientation ﬁelds. EURASIP J Appl
Signal Process (13):2043–2053
Yager N, Amin A (2006) Fingerprint alignment using a two stage optimization. Pattern Recogn
27:317–324
Zhang W, Wang Y (2002) Core-based structure matching algorithm of ﬁngerprint veriﬁcation.
Proc Int Conf Pattern Recogn 1:70–74
40
2
High Resolution Partial Fingerprint Alignment

Chapter 3
Fingerprint Pore Modeling and Extraction
Abstract Sweat pores on ﬁngerprints have proven to be discriminative features
and have recently been successfully employed in automatic ﬁngerprint recognition
systems (AFRS), where the extraction of ﬁngerprint pores is a critical step. Most of
the existing pore extraction methods detect pores by using a static isotropic pore
model; however, their detection accuracy is not satisfactory due to the limited
approximation capability of static isotropic models to various types of pores. This
chapter presents a dynamic anisotropic pore model to describe pores more accu-
rately by using orientation and scale parameters. An adaptive pore extraction
method is then developed based on the proposed dynamic anisotropic pore
model. The ﬁngerprint image is ﬁrst partitioned into well-deﬁned, ill-posed, and
background blocks. According to the dominant ridge orientation and frequency on
each foreground block, a local instantiation of appropriate pore model is obtained.
Finally, the pores are extracted by ﬁltering the block with the adaptively generated
pore model. Extensive experiments are performed on the high resolution ﬁngerprint
databases we established. The results demonstrate that the proposed method can
detect pores more accurately and robustly, and consequently improve the ﬁnger-
print recognition accuracy of pore-based AFRS.
Keywords Automatic ﬁngerprint recognition • Pore extraction • Pore models
3.1
Introduction
Fingerprint is the most widely used biometric characteristic for personal identiﬁ-
cation because of its uniqueness and stability over time (Zhang 2000; Jain et al.
2007a, b; Maltoni et al. 2003). Most of the existing automatic ﬁngerprint recogni-
tion systems (AFRS) use the minutia features on ﬁngerprints, i.e. the terminations
and bifurcations of ﬁngerprint ridges (Ratha and Bolle 2004; Ratha et al. 1996; Jain
et al. 1997), for recognition. Although they can achieve good recognition accuracy
and have been used in many civil applications, their performance still needs much
improvement when a large population is involved or a high security level is
required. One solution to enhancing the accuracy of AFRS is to employ more
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_3
41
www.ebook3000.com

features on ﬁngerprints other than only minutiae (Jain et al. 2007a, b; Ross et al.
2003; He et al. 2006).
Generally, ﬁngerprint features can be divided into three levels (CDEFFS 2009).
Level 1 features (e.g. overall ﬁngerprint ridge patterns) and Level 2 features
(e.g. minutiae) have been extensively studied and they are employed in most
existing AFRS. Level 3 features, however, are ignored in many AFRS even though
they are also very distinctive and have been used for a long time in the forensic
community (Ashbaugh 1999; Bindra et al. 2000). Level 3 features refer to ridge
dimensional attributes such as ridge contours and pores, which are ﬁne details on
ridges and require high resolution imaging techniques to reliably capture (CDEFFS
2009). Such requirements limit the use of Level 3 features in conventional AFRS.
Thanks to the advances in imaging techniques and the demand for more secure
biometric systems, recently researchers have been paying more and more attention
to using Level 3 features in AFRS. Roddy and Stosz (1997), and Parsons et al.
(2008) statistically analyzed the discriminative power of pores and validated the
effectiveness of pore conﬁguration in personal identiﬁcation. The ﬁrst AFRS using
pores was developed by Stosz and Alyea (1994). They combined minutiae and
pores to recognize persons. Subsequently, Kryszczuk et al. (2004a, b) investigated
the contribution of pores to fragmentary ﬁngerprint recognition and showed that the
smaller the ﬁngerprint fragments, the greater the beneﬁt of using pores. Recently,
Jain et al. (2006, 2007a, b) proposed a high resolution ﬁngerprint recognition
system which uses features from all the three levels (i.e. ridge orientation ﬁelds,
minutiae, ridge contours, and pores).
A critical step in the pore based AFRS is the extraction of pores from ﬁngerprint
images. Existing methods extract pores by using skeleton-tracking-based (Roddy
and Stosz 1997; Stosz and Alyea 1994; Kryszczuk et al. 2004a, b) or ﬁltering-based
approaches (Jain et al. 2006, 2007a, b; Ray et al. 2005; Parsons et al. 2008). The
skeleton-tracking-based approaches are quite time-consuming and work well only
with very high quality ﬁngerprint images (Jain et al. 2006, 2007a, b). The ﬁltering-
based approaches are more efﬁcient and more robust. They use static isotropic pore
models to detect pores. As we will see later, however, the pores on real ﬁngerprint
images appear anisotropic and vary greatly in scale from ﬁngerprint to ﬁngerprint
and from region to region.
In this chapter we will ﬁrst propose a novel dynamic anisotropic pore model
which describes the pores more ﬂexibly and accurately than the previous models.
With the proposed pore model, we will then develop an adaptive pore extraction
method to accurately and robustly extract pores. To evaluate the proposed method,
we established two sets of high resolution ﬁngerprint images and conducted exten-
sive experiments on them. The results demonstrated that the proposed pore model
and pore extraction method can detect pores more accurately and robustly than the
previous methods, and the extracted pore features can consequently improve the
recognition accuracy of pore based AFRS.
42
3
Fingerprint Pore Modeling and Extraction

3.2
Review of Existing Pore Extraction Methods
Existing pore extraction methods can be classiﬁed into two categories, skeleton-
tracking-based methods and ﬁltering-based methods. All earlier works (Roddy and
Stosz 1997; Stosz and Alyea 1994; Kryszczuk et al. 2004a, b) are skeleton-tracking-
based methods. They ﬁrst binarize and skeletonize the ﬁngerprint image and then
track the ﬁngerprint skeletons. A pore is detected when certain criteria are met
during the tracking. As pointed out in (Jain et al. 2006, 2007a, b), however,
skeletonization is computationally expensive and very sensitive to noise and it
works well only on very high resolution ﬁngerprint images of high quality. For
example, the ﬁngerprint images used in (Roddy and Stosz 1997; Stosz and Alyea
1994; Kryszczuk et al. 2004a, b) are all at least 2000 dpi. Recently proposed
approaches are ﬁltering-based methods that detect pores by using pore models to
ﬁlter ﬁngerprint images. Figure 3.1 shows three typical isotropic pore models:
Ray’s model (Ray et al. 2005), Jain’s model (Jain et al. 2006, 2007a, b), and the
DoG (difference of Gaussian) model (Parsons et al. 2008).
Ray et al. (2005) proposed an approach to extracting pores from ﬁngerprint
images based on the pore model in Fig. 3.1, which is a modiﬁed 2-dimensional
Gaussian function. They ﬁrst calculated an error map for the ﬁngerprint image, with
each entry in this map being the sum of the squared errors between the pore model
and the local area surrounding the pixel. The error map is then binarized such that
only areas of high pore probability (i.e. low error) are retained. In these areas, the
pores are detected as the local minima in a [(2rm)]  (2rm) neighborhood. In Ray
et al. (2005), the authors used unitary parameters r (the variance of the Gaussian)
and rm to detect pores. However, the pore scales and ridge/valley widths could vary
greatly from one ﬁngerprint to another ﬁngerprint or from one region to another
region in the same ﬁngerprint (referring to Fig. 3.2 for examples). Moreover, Ray’s
pore model is isotropic, yet as we can see from Fig. 3.2 that the appearance of open
pores on real ﬁngerprint images is not isotropic.
More recently, Jain et al. (2006, 2007a, b) proposed to use the Mexican hat
wavelet transform to extract pores based on the observation that pore regions
typically have a high negative frequency response as intensity values change
abruptly from bright to dark at the pores. The Mexican hat wavelet actually serves
as the pore model, and its scale parameter is experimentally set for speciﬁc datasets.
Figure 3.1b shows the Mexican hat wavelet. Obviously, it is also isotropic. This
Fig. 3.1 Three typical pore models: (a) Ray’s model (Ray et al. 2005), (b) Jain’s model (Jain et al.
2006, 2007a, b), and (c) the DoG model (Parsons et al. 2008), which are all isotropic
3.2
Review of Existing Pore Extraction Methods
43
www.ebook3000.com

pore model is also limited in that it cannot adapt itself to different ﬁngerprints or
different regions on a ﬁngerprint.
Another pore extraction method was proposed by Parsons et al. (2008). Its basic
idea is to use a band-pass ﬁlter to detect circle-like features. In other words, the
method assumes that pores appear as circular objects on ﬁngerprint images, and the
pore is thus modeled by the DoG ﬁlter. Figure 3.1c shows this pore model. In
Parsons et al. (2008), the authors did not consider the variation of pore scales in
ﬁngerprint images but simply used a unitary scale in their model. To deal with the
limitations caused by unitary scale, we have recently proposed in Zhao et al. (2010)
an adaptive DoG-based pore extraction method. It divides a ﬁngerprint image into
blocks and deﬁnes for each block a DoG ﬁlter according to the local ridge period on
the block. One limitation of the DoG-based methods is that the pore models are
isotropic. The underlying assumption that pores are circular features does not hold
well on real ﬁngerprint images. In this chapter, we will propose another novel pore
model and extraction method, which can well solve the problems with existing pore
models and extraction methods. Next, we introduce the new pore model ﬁrst.
3.3
Dynamic Anisotropic Pore Model (DAPM)
Sweat pores reside on ﬁnger ridges and may be either closed or open (Ashbaugh
1999). As can be seen in Fig. 3.2, a closed pore looks like an isolated bright blob on
the dark ridge, whereas an open pore, which is perspiring, is connected with its
neighboring bright valleys. To investigate the spatial appearances of pores on
ﬁngerprint images, we manually marked and cropped hundreds of pores on many
ﬁngerprint images, including both open and closed pores. We generalized three
types of representative pore structures, which are illustrated in Fig. 3.3. It can be
seen that the two open pores (b) and (c) are not isotropic. Along the ridge direction,
Fig. 3.2 Two ﬁngerprint images with very different ridge and valley widths. A closed pore is
marked on the left image and two open pores are marked on the right image
44
3
Fingerprint Pore Modeling and Extraction

all the three types of pores appear with Gaussian-shaped proﬁles. Furthermore, the
width of Gaussian proﬁle will vary from one pore to another.
These observations clearly show that the previously proposed pore models (refer
to Sect. 3.2) are not accurate enough to model the various pores because they are
isotropic and static (i.e. using a unitary scale). In order to represent the pores more
accurately, we propose here a new pore model which has two parameters to adjust
scale and orientation. When applying this model to a real pore, these two param-
eters are adaptively determined according to the local ridge features (i.e. ridge
orientation and frequency). Therefore, we name the proposed model as the dynamic
anisotropic pore model (DAPM), which is deﬁned as follows:
P0 i; j
ð
Þ ¼ e j2
2σ2  cos
π
3σi


3σ  i, j  3σ
(
ð3:1Þ
Pθ i; j
ð
Þ ¼ Rot P0; θ
ð
Þ ¼ ebj
2
2σ2  cos
π
3σ
bi


bi ¼ i cos θ
ð Þ  j sin θ
ð Þ, bj ¼ i sin θ
ð Þ þ j cos θ
ð Þ
3σ  i, j  3σ
8
>
>
<
>
>
:
ð3:2Þ
Equation (3.1) is the reference model (i.e. the zero-degree model) and Eq. (3.2)
is the rotated model. Here, σ is the scale parameter which is used to control the pore
size. It can be determined by the local ridge frequency. θ is the orientation
parameter which is used to control the direction of the pore model. It can be
estimated by the local ridge orientation. Figure 3.4 shows some example instances
of the proposed DAPM. With the proposed DAPM, next we present an adaptive
pore extraction method in Sect. 3.4.
Fig. 3.3 The appearance of three typical pores on real ﬁngerprint images. (a) is a closed pore, and
(b) and (c) are open pores. (d)–(f) are the corresponding intensity proﬁles across them along the
ridge orientation. All the proﬁles are Gaussian-shaped
3.3
Dynamic Anisotropic Pore Model (DAPM)
45
www.ebook3000.com

3.4
Adaptive Pore Extraction
Pore extraction is essentially a problem of object detection. Generally, given a
model of an object, we can detect the object by using the model as a matched ﬁlter.
When convoluting an image with a matched ﬁlter describing the desired object,
strong responses will be obtained at the locations of the object on the image. The
techniques of matched ﬁlters have been successfully used in many applications, for
example, vessel detection on retinal images (Sofka and Stewart 2006). In this
section, we will ﬁrst estimate the parameters in the DAPM to instantiate the pore
model, and then discuss the important implementation issues in using the instanti-
ated pore models to extract pores. Finally, the adaptive pore extraction algorithm
will be presented.
3.4.1
DAPM Parameter Estimation
The matched ﬁlters for pore extraction can be generated by instantiating the pore
models. In order to instantiate the DAPM in Eqs. (3.1) and (3.2), it is necessary to
initialize two parameters, orientation θ and scale σ. As for the orientation parameter
θ, an intuitive way is to set it as the local ﬁngerprint ridge orientation. To estimate
the ridge orientation ﬁeld on the ﬁngerprint, we ﬁrst smooth the ﬁngerprint image by
using a smoothing kernel and then calculate the gradients along the x and y directions
by using some derivative operator (e.g. the Sobel operator). Let Gx(i,j) and Gy(i,j) be
the gradients at the pixel (i,j), and the squared gradients be Gxx(i,j) ¼ Gx(i,j)  Gx(i,j),
Gxy(i,j) ¼ Gx(i,j)  Gy(i,j), and Gyy(i,j) ¼ Gy(i,j)  Gy(i,j). The squared gradients are
then smoothed by using a Gaussian kernel, resulting in Gxx, Gxy, and Gyy. The ridge
orientation at (i,j) is estimated by
O ij
ð Þ ¼ π
2 þ 1
2  arctan
Gxx i; j
ð
Þ  Gyy i; j
ð
Þ
2  Gxy i; j
ð
Þ


ð3:3Þ
Fig. 3.4 Example instances of the dynamic anisotropic pore model. (a) θ ¼ 0, (b) θ ¼ 45, and (c)
θ ¼ 90
46
3
Fingerprint Pore Modeling and Extraction

which is in the range of [0, π]. For more details on ﬁngerprint ridge orientation
ﬁeld estimation, please refer to Bazen and Gerez (2002).
With regard to the scale parameter σ, if we can estimate the range of pore scales,
we can then use a bank of multi-scale matched ﬁlters to detect the pores; however,
this is very time-consuming. Therefore, we estimate and use the maximum valid
pore scale when designing the matched ﬁlters in this chapter. As shown in Sect. 3.2,
the pores are located on ridges. Consequently, the pore scales should be restricted
by the ridge widths. This motivates us to associate the maximum pore scale with the
local ﬁngerprint ridge period by a ratio, i.e. σ ¼ τ/k, where τ is the local ridge period
(or the reciprocal of local ridge frequency) and kis a positive constant. In this
chapter, we empirically set k ¼ 12. The local ridge frequency is estimated in a local
window by using the projection-based method in Hong et al. (1998).
3.4.2
Implementation Issues
With the estimated parameters θ and σ in Sect. 3.4.1, an adaptive pore model can be
instantiated for each pixel and then we can apply it as a matched ﬁlter to extracting
pores from the ﬁngerprint image. However, there will be two problems if directly
applying the matched ﬁlters in a pixel-wise way. Next, we discuss these issues in
detail and present the solutions to practical implementation.
The ﬁrst problem is the computational cost. Obviously, it will be very expensive
to calculate the DAPM in a pixel-wise way. Noting that in a local region on the
ﬁngerprint, the ridges run nearly parallel with each other, and the intervals between
them vary slightly, we could therefore calculate a common DAPM in a local region
to detect pores. The second problem is that on some parts of a ﬁngerprint image it is
difﬁcult to get an accurate estimate of the local ridge orientation and frequency,
which is needed in order to initialize an accurate instance of DAPM. For example,
on the image shown in Fig. 3.5a, the region highlighted by the red circle is mashed
and no dominant orientation can be obtained. The sharp change of ridge orientation
at the singular points of a ﬁngerprint will also raise difﬁculties in estimating the
ridge orientation and frequency surrounding the singular points.
To deal with these issues, we propose a block-wise approach to implementing
the matched ﬁlters for pore extraction. This approach deﬁnes three kinds of blocks
on ﬁngerprint images: well-deﬁned blocks, ill-posed blocks, and background
blocks. Well-deﬁned and ill-posed blocks are both foreground ﬁngerprint regions.
On a well-deﬁned block, it is able to directly estimate a dominant ridge orientation
and a ridge frequency. On an ill-posed block, there is not a dominant ridge
orientation but the ridge frequency can be estimated by interpolation of the fre-
quencies on its neighboring blocks.
The block partition and classiﬁcation is performed in a hierarchical way. First, a
large block size is applied to the image. For each block B, the following structure
tensor is calculated
3.4
Adaptive Pore Extraction
47
www.ebook3000.com

J ¼ 1
NB
X
i2B
∇Bi∇BiT ¼
j11
j12
j21
j22


ð3:4Þ
where NB denotes the number of pixels in the block, ∇Bi ¼
∂Bi=∂x;∂Bi=∂yÞ
T

is the
gradient vector at pixel i, and ‘T’ represents the transpose operator. The structure
tensor J contains information of ridge orientation in the block and the eigenvalues
of J can be used to measure the consistency of ridge orientation. Speciﬁcally, we
use the orientation certainty (OC) deﬁned as follows (Chen et al. 2005):
OC ¼ λ1  λ2
ð
Þ2
λ1 þ λ2
ð
Þ2 ¼ j11  j22
ð
Þ2 þ 4 j12
2
j11 þ j22
ð
Þ2
ð3:5Þ
where λ1 and λ2 are the two eigenvalues of 2  2 structure tensor J and we
assume λ1  λ2. This quantity of OC can indicate how strongly the energy is
concentrated along the ridge orientation. If there is a dominant ridge orientation,
then λ1  λ2 and OC will be close to 1. Otherwise, λ1 and λ2 will not differ much and
consequently OC will be close to 0.
We also calculate a measurement related to the intensity contrast (IC) of the
block as follows:
IC ¼ std B
ð Þ
ð3:6Þ
where std denotes the standard deviation. The purpose of this is to exclude the
background from the foreground ﬁngerprint. The two measurements, OC and IC,
are evaluated with pre-speciﬁed thresholds. If both of them are above the thresh-
olds, the block is recorded as a well-deﬁned block and will not be further
partitioned. Otherwise, the block larger than the minimum size is evenly partitioned
into four equal sub-blocks, each of which is further examined. Suppose a block has
(a)              
(b)
 
Fig. 3.5 (a) A ﬁngerprint image. The ridge orientation and frequency cannot be accurately
estimated on the region marked by the red circle. (b) The partition result. The dominant ridge
orientations of the well-deﬁned blocks are shown by the green lines (Color ﬁgure online)
48
3
Fingerprint Pore Modeling and Extraction

reached the minimum size, this block will be marked as a well-deﬁned block if its
OC and IC measures are above the thresholds; it is marked as an ill-posed block if
its OC measure is less than the threshold but the IC measure is above the threshold;
otherwise, it is marked as a background block. Figure 3.5b shows the partition result
of the image in Fig. 3.5a. The dominant ridge orientations of the well-deﬁned
blocks are shown by the green lines.
After partitioning the ﬁngerprint image into the three kinds of blocks, the pores
can be extracted from each of the foreground (well-deﬁned or ill-posed) blocks. For
a well-deﬁned block, the dominant ridge orientation and the mean ridge frequency
on it can be calculated directly, and hence the DAPM can be consequently instan-
tiated. For an ill-posed block, there is no dominant ridge orientation but the mean
ridge frequency can be calculated by interpolating the mean ridge frequencies of its
neighboring blocks. Hence, as a compromise, we apply to the ill-posed blocks the
adaptive DoG based pore models (Zhao et al. 2010). Next we discuss on how to
calculate the dominant ridge orientation of a well-deﬁned block and the mean ridge
frequency on a foreground block.
The dominant orientation of a well-deﬁned block is deﬁned as the average
orientation of the ridge orientation ﬁeld on the block. To average the orientation
ﬁeld of block B, denoted by BOF, we ﬁrst multiply the orientation angle at each pixel
by 2, and then calculate its cosine and sine values. Finally, the dominant orientation
of the block is calculated as
BDO ¼ 1
2 arctan
aver sin 2  BOF
ð
Þ
ð
Þ
aver cos 2  BOF
ð
Þ
ð
Þ


ð3:7Þ
where aver(F) denotes the average of the elements in F.
For each well-deﬁned block, the average ridge frequency on the block is
calculated by using the method in Hong et al. (1998). The ridge frequencies on
the ill-posed blocks are estimated by interpolating their surrounding blocks whose
ridge frequencies have already been calculated. Speciﬁcally, after the ridge fre-
quencies on well-deﬁned blocks have been calculated, we iteratively check the
ﬁngerprint image until all the ridge frequencies of the foreground blocks have been
calculated. If the ridge frequency of a foreground block has not been calculated, we
take the mean of the ridge frequencies of its neighboring blocks as its ridge
frequency. Finally, all foreground ﬁngerprint blocks, no matter with or without
dominant orientation, are assigned with ridge frequencies.
3.4.3
The Pore Extraction Algorithm
We now summarize the complete adaptive ﬁngerprint pore extraction algorithm. As
shown in Fig. 3.6, the proposed pore extraction algorithm consists of ﬁve main
steps. Take the ﬁngerprint fragment in Fig. 3.7a, which is a part of Fig. 3.5a, as an
3.4
Adaptive Pore Extraction
49
www.ebook3000.com

example. The ﬁrst step is to partition the ﬁngerprint image into a number of blocks,
each being a well-deﬁned block, an ill-posed block or a background block (see
Fig. 3.7b). In the second step, the ridge orientation ﬁeld of the ﬁngerprint image is
calculated. Meanwhile, the mean ridge frequencies on all foreground blocks are
estimated, which form the ridge frequency map of the ﬁngerprint image (see
Fig. 3.7c). It then proceeds to the third step, in which the binary ridge map of the
ﬁngerprint image is calculated as follows. Based on the estimated ridge orientation
ﬁeld and ridge frequency map, the ﬁngerprint image is ﬁrst enhanced by using a
bank of Gabor ﬁlters (Hong et al. 1998) to enhance the bright valleys and suppress
dark ridges. In order to extract the ridges from the ﬁngerprint image, we binarize the
enhanced image and calculate its complement, where the ridge pixels have value
‘1’. With this complement image, we can readily obtain the binary ridge map by
setting the corresponding ridge pixels in the foreground ﬁngerprint blocks to ‘1’ and
the other pixels to ‘0’. This binary ridge map (see Fig. 3.7d) will be used in the post-
processing step to remove spurious pores because pores can only locate on ridges.
It is worth mentioning that the ﬁrst three steps can be performed on a down-
sampled small image of the original high resolution ﬁngerprint image because they
do not depend on the Level 3 features. In our experiments, we down-sampled the
images to half of their original resolution and then carried out steps A, B, and
C. Afterwards, the obtained image partition result, ridge orientation ﬁeld, ridge
frequency map, and the ridge map were all up-sampled to the original resolution.
They will be used in the subsequent pore detection and post-processing. Working
on the down-sampled images can reduce a lot the computational cost. The pore
detection and post-processing are performed on the original ﬁngerprint images
because the Level 3 pore features can hardly be reliably extracted in the down-
sampled low resolution ﬁngerprint images.
Step A: Partition
Background
Blocks
Adaptive DoG
based Pore
Detection
Extracted
Pores
Ridge
Map
DAPM based
Pore Detection
III-
posed
Blocks
WelI-
defined
Blocks
Ridge
Orientation
Field
Ridge
Frequency
Map
Step D: Pore Detection
Step E: Post-
Step C: Ridge
Step B: Ridge Orientation and
Frequency Estimation
Map Extraction
processing
Fig. 3.6 The main steps of the proposed pore extraction method
50
3
Fingerprint Pore Modeling and Extraction

In the pore detection step, the foreground ﬁngerprint blocks are processed one by
one to detect pores on them. A local instantiation of the DAPM is established for
each well-deﬁned block based on the local ridge orientation and frequency on that
block, and a local instantiation of the adaptive DoG based pore model is established
for each ill-posed block based on the local ridge frequency on the block. Applying
the adaptively instantiated pore model to the block as a matched ﬁlter will enhance
the pores while suppressing valleys and noise. A threshold is then applied to the
ﬁltering response to segment out the candidate pores on the block. After processing
all the blocks, we obtain a binary image where the candidate pores have the value
‘1’ and other pixels have value ‘0’. This binary image gives the initial pore
extraction result (pore map). Figure 3.7e shows an example. We can see that
there could have some spurious and false pores in this map.
The last step is to remove the spurious and false pores from the initial pore
extraction result. In previous work, most methods remove false pores by applying
the restraint that pores should reside only on ridges (Jain et al. 2006, 2007a, b; Ray
et al. 2005, Ratha et al. 1996) and that the size of pores should be within a valid
range (Jain et al. 2006; 2007a, b; Ratha et al. 1996). Some researchers also propose
to reﬁne the pore extraction result based on the intensity contrast (Parsons et al.
2008). In Parsons et al. (2008), a PCA method is applied to a set of extracted
Fig. 3.7 (a) is a ﬁngerprint fragment; (b) shows the blocks on it (the value in red is the orientation
certainty and the value in green is the intensity contrast); (c) displays the estimated dominant ridge
orientations and periods (the green lines denote the orientations on well-deﬁned blocks, and if there
is no orientation shown on a block, the block is an ill-posed block); (d) is the ridge map; (e) is the
initial pore map; and (f) shows the ﬁnal detected pores (marked by circles) (Color ﬁgure online)
3.4
Adaptive Pore Extraction
51
www.ebook3000.com

putative pores to estimate a model for the gray level distribution over pores. This
model is then used to exclude falsely detected pores based on a method of
minimizing squared error. This method is however greatly affected by the chosen
putative pores.
In this chapter, we take the following steps to post-process the extracted candi-
date pores. First, we use the binary ridge map as a mask to ﬁlter the pore map. In this
step, the pixels which are not on ridges are removed. Second, we sort the remaining
candidate pore pixels according to their gray level values descendingly and then
discard the last 5% pixels because they are more probably caused by noise. Third,
we identify all the connected components on the pore map, and each component is
taken as a candidate pore. We check the size of each component, i.e. the number of
pixels it has. If the size is out of the pre-speciﬁed range of valid pore size (from 3 to
30 in our experiments), the candidate pore is removed from the pore map. The ﬁnal
pore map is obtained after the above reﬁnement. We record the extracted pores by
recording the coordinates of their mass centers. See Fig. 3.7f for an example. More
examples of different ﬁngerprint fragments are given in Fig. 3.8.
3.5
Experiments and Performance Evaluation
To evaluate the proposed ﬁngerprint pore extraction method, a high resolution
ﬁngerprint image dataset is required. It is well accepted that the ﬁngerprint image
resolution should be at least 1000 dpi to reliably capture the Level 3 features such as
pores (CDEFFS 2009). Unfortunately, so far there is no such high resolution
ﬁngerprint image database free available in the public domain. So we built an
optical ﬁngerprint scanner by ourselves, which could collect the ﬁngerprint images
at a resolution about 1200 dpi. Figure 3.9 shows the scanner we developed. It uses a
CCD camera (Lumenera Camera LU135M) to capture the ﬁngerprint image when
the ﬁnger touches against the prism of the scanner.
Two databases have been established by using the scanner we developed. The
ﬁrst database (denoted as DBI) is a partial ﬁngerprint image database where the
image size is 320 pixels in width and 240 pixels in height. The second database
(denoted as DBII) contains full-size ﬁngerprint images which are 640 pixels in
width and 480 pixels in height. Both databases have 1480 ﬁngerprint images taken
Fig. 3.8 Some example pore extraction results obtained by using the proposed method
52
3
Fingerprint Pore Modeling and Extraction

from 148 ﬁngers with each ﬁnger having ten samples in two sessions. Five images
were captured for each ﬁnger in each of the two sessions which were about two
weeks apart.
Using the established databases, we have conducted extensive experiments to
evaluate the proposed pore extraction method in comparison with three state-of-the-
art methods [Jain’s method (Jain et al. 2006, 2007a, b), Ray’s method (Ray et al.
2005), and the adaptive DoG based method (Zhao et al. 2010)]. Three types of
experiments were conducted. First, we compared the proposed method with its
counterparts in terms of pore detection accuracy using a set of ﬁngerprint images
chosen from DBI. Second, using the partial ﬁngerprint image database DBI and a
minutia-pore-based ﬁngerprint matcher, we evaluated the ﬁngerprint recognition
performance by using the pores extracted by the proposed method and the other
three methods. Third, we evaluated the ﬁngerprint recognition performance of the
four methods on the full-size ﬁngerprint image database DBII. In the following, we
present the experiments in detail.
3.5.1
Pore Detection Accuracy
We ﬁrst assess the pore detection accuracy of the proposed method. For this
purpose, we chose a set of 24 ﬁngerprint images from DBI. The chosen images
have relatively good quality so that the pores on them can be easily marked.
Figure 3.10 shows two example ﬁngerprint images. We manually marked the
pores on these ﬁngerprint images as the ground truth for our experiments. We
then used our proposed method, Jain’s method, Ray’s method, and the adaptive
DoG based method to extract pores on them. Figure 3.11 shows the pore extraction
results of the four methods on the ﬁngerprint image in Fig. 3.10b. On this ﬁngerprint
fragment, the ridges on the left hand side are thinner than those on the right hand
side, and both open and closed pores can be observed. From Fig. 3.11a, b, we can
Fig. 3.9 (a) The high-resolution ﬁngerprint scanner we developed and (b) its inner structure
3.5
Experiments and Performance Evaluation
53
www.ebook3000.com

see that due to the unitary scale they use, Ray’s method and Jain’s method cannot
work consistently well on the left and the right parts of the ﬁngerprint image
because of the varying ridge widths and pore sizes. In addition, all the three
comparison methods miss many open pores because their isotropic pore models
Fig. 3.10 Two example ﬁngerprint images used in the experiments
Fig. 3.11 Example pore extraction results of (a) Ray’s method, (b) Jain’s method, (c) the adaptive
DoG based method, and (d) the proposed method. The pores are manually marked by bright dots
and the detected pores are marked by red circles (Color ﬁgure online)
54
3
Fingerprint Pore Modeling and Extraction

cannot accurately handle open pores. In contrast, the proposed method successfully
detects most of the pores on both the left and the right parts of the ﬁngerprint image
no matter they are open or closed. This demonstrates that the proposed DAPM
model can better adapt to varying ridge widths and pore sizes, and can better cope
with both closed and open pores.
In addition to the visual evaluation of the pore detection results, we calculated
the average detection accuracy on the 24 ﬁngerprint images by using two metrics:
RT (true detection rate) and RF (false detection rate). RT is deﬁned as the ratio of the
number of detected true pores to the number of all true pores, while RF is deﬁned as
the ratio of the number of falsely detected pores to the total number of detected
pores. A good pore extraction algorithm should have a high RT and a low RF
simultaneously. Table 3.1 lists the average detection accuracy and the standard
deviation of detection accuracy of the four methods. According to the average
detection accuracy listed in Table 3.1, the proposed method achieves not only the
highest true detection rate but also the lowest false detection rate. With regard to the
standard deviation, as shown in Table 3.1, the proposed method again achieves the
smallest deviation over the whole image set for both true detection rate and false
detection rate. As for the other three methods, none beats its counterparts in all
cases. From these results, we can see that the proposed method can detect pores on
ﬁngerprint images more accurately and more robustly.
3.5.2
Pore Based Partial-Fingerprint Recognition
Since the purpose of pore extraction is to introduce new features for ﬁngerprint
recognition, it is necessary to test how the pores extracted by the methods will
contribute to a ﬁngerprint recognition system. According to (Kryszczuk et al.
2004a, b; Zhao et al. 2009), the ﬁngerprint recognition beneﬁts more from the
pores when the used ﬁngerprint images cover small ﬁngerprint area. Therefore, in
order to emphasize the contribution of pores, we evaluated in this sub-section the
improvement of ﬁngerprint recognition accuracy made by the extracted pores based
on the partial ﬁngerprint image database DBI.
We implemented an AFRS like the one in Zhao et al. (2009) which is based on
minutiae and pores. The block diagram of the AFRS is shown in Fig. 3.12. It
consists of ﬁve main modules, i.e. minutia extraction, pore extraction, minutia
matching, pore matching, and match score fusion. We use the methods in Feng
(2008) for minutia extraction and matching modules. The pore matching is
Table 3.1 The average pore detection accuracy (%) and the standard deviation of the four
methods on the 24 ﬁngerprint images
Ray’s method
Jain’s method
Adaptive DoG based method
The proposed method
RT
60.6 (11.9)
75.9 (7.5)
80.8 (6.5)
84.8 (4.5)
RF
30.5 (10.9)
23.0 (8.2)
22.2 (9.0)
17.6 (6.3)
3.5
Experiments and Performance Evaluation
55
www.ebook3000.com

accomplished by using the direct pore matching method in Zhao et al. (2009). It
ﬁrstly establishes initial correspondences between the pores on two ﬁngerprints
based on their local features, and then uses the RANSAC (Random Sample
Consensus) algorithm (Hartley and Zisserman 2003) to reﬁne the pore correspon-
dences, and ﬁnally calculates a pore-based similarity score between the two ﬁnger-
prints based on the number of corresponding pores. The pore matching is
independent of minutia matching in this method. This method is very suitable for
small partial ﬁngerprint recognition where the minutia matching results are often
unreliable due to the limited number of minutiae on the small ﬁngerprint fragments
(Zhao et al. 2009). The pore match score and the minutia match score are ﬁnally
fused by using a simple weighted summation scheme to give the ﬁnal match score
between two ﬁngerprint images (before fusion, both match scores are normalized to
the range between 0 and 1) as follows
MS ¼ ω  MSminu þ 1  ω
ð
Þ  MSpore
ð3:8Þ
where ω is the weight of minutiae with respect to pores.
By using database DBI and the above described AFRS, we evaluated the
ﬁngerprint recognition performance of the four pore extraction methods. Consid-
ering the expensive computational cost, the following matches were carried out:
(1) Genuine matches: Each of the ﬁngerprint images in the second session was
matched with all the ﬁngerprint images in the ﬁrst session, leading to 3700 genuine
matches, and (2) Imposter matches: the ﬁrst ﬁngerprint image of each ﬁnger in the
second session was matched with the ﬁrst ﬁngerprint image of all the other ﬁngers
in the ﬁrst session, resulting in 21,756 imposter matches. Figure 3.13 shows the
equal error rates (EER) obtained by the four methods on DBI under different
weights. By using only minutiae, the EER is 17.67%. The EERs when using only
pores (i.e. ω ¼ 0) are respectively 21.96% by Ray’s method, 21.53% by Jain’s
method, 22.99% by the adaptive DoG based method, and 20.49% by the proposed
method. By fusing minutiae and pores, the best results are 12.41% (ω ¼ 0.9), 12.4%
(ω ¼ 0.8), 14.18% (ω ¼ 0.9), and 11.51% (ω ¼ 0.8) by the four methods
respectively. Figure 3.14 shows their receiver operating characteristics (ROC)
curves when the best EERs are obtained. It is seen that the proposed method
leads to the best recognition results. The improvement of recognition accuracy
made by fusing the pore features over using only minutia features are 29.77%,
29.82%, 19.75% and 34.86% respectively by the four methods.
Minutia Extraction
Pore Extraction
Minutia Matching
Direct Pore Matching
Score 
Fution
Fig. 3.12 Block diagram of the AFRS used in partial ﬁngerprint recognition experiments
56
3
Fingerprint Pore Modeling and Extraction

24
The proposed method
Jain’s method
Ray’s method
AdaptiveDoG based method
22
20
18
16
14
12
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Weight of minutiae with respect to pores
EER (%)
1
Fig. 3.13 The EERs of the four methods on DBI with different fusing weights
18
16
14
12
10
8
6
4
9
10
11
12
13
The proposed method (EER=11.51%)
Jain’s method (EER=12.4%)
Ray’s method (EER=12.41%)
AdaptiveDoG based method (EER=14.18%)
FAR (%)
FRR (%)
14
15
16
17
Fig. 3.14 The ROC curves of the four methods on DBI when the lowest EERs are obtained
3.5
Experiments and Performance Evaluation
57
www.ebook3000.com

3.5.3
Pore Based Full-Size Fingerprint Recognition
The experiments in this sub-section were to evaluate the contribution of the
extracted pores to full-size ﬁngerprint recognition. We compared the four pore
extraction methods by using a different AFRS in Jain et al. (2006), which is
appropriate for full-size ﬁngerprint images, and using the full-size ﬁngerprint
image database DBII. Figure 3.15 shows the block diagram of the AFRS. We
used the same minutia extraction and matching modules and the same score fusion
module as in the last sub-section, but implemented the pore matching module by
using the ICP (iterative closest point) based method as in (Jain et al. 2006, 2007a,
b). This is because on ﬁngerprint images covering large ﬁngerprint area, there are
sufﬁcient minutiae to provide reliable minutia match results. We can thus compare
the pores locally in the neighborhoods of matched minutiae. In this way, the pores
can be matched much more efﬁciently. Speciﬁcally, after matching the minutiae on
two ﬁngerprints, the pores lying in the neighborhoods of each pair of matched
minutiae are matched by using the ICP algorithm (Jain et al. 2006, 2007a, b),
resulting in N match scores (N is the number of pairs of matched minutiae), which
are deﬁned as the summation of two terms: the mean distance between all matched
pores and the percentage of unmatched pores. The pore match score between the
two ﬁngerprints is ﬁnally deﬁned as the average of the ﬁrst three smallest match
scores.
By using the above AFRS, we matched pair-wise all the ﬁngerprint images in
DBII (avoiding symmetric matches), generating 6660 genuine match scores and
1,087,800 imposter match scores. Figure 3.16 presents the EERs obtained by the
four methods on DBII. Because the ﬁngerprint images in DBII are full-size ﬁnger-
print images and have more minutiae, it can be seen that the EER of using only
minutiae is 0.61%, which is much better than that obtained on DBI (17.67%,
referring to Sect. 3.5.2). When only using pores, the EER of Ray’s method is
9.45%, Jain’s method 8.82%, the adaptive DoG based method 10.85%, and the
proposed method 7.81%. The best results of these methods after fusion with minutia
match scores are 0.59% (ω ¼ 0.9), 0.6% (ω ¼ 0.9), 0.56% (ω ¼ 0.8), and 0.53%
(ω ¼ 0.7). Figure 3.17 shows their corresponding ROC curves when the best results
are obtained. The proposed method improves on the best EERs of Ray’s method,
Jain’s method, and the adaptive DoG based method by 10.17%, 11.67%, and 5.36%
respectively.
Minutia Extraction
Minutia Matching
Pore Extraction
ICP-based Pore
Score 
Fusion
Minutia
Match Score
Pore Match 
Score
Pairs of Matched 
Minutiae
Fig. 3.15 Block diagram of the AFRS used in full-size ﬁngerprint recognition experiments
58
3
Fingerprint Pore Modeling and Extraction

3.5.4
Computational Complexity Analysis
Before closing this section, we would like to brieﬂy analyse the computational
complexity of the methods. As shown in Fig. 3.6, the proposed pore extraction
method has ﬁve main steps: (A) partition, (B) ridge orientation and frequency
estimation, (C) ridge map extraction, (D) pore detection, and (E) post-processing.
12
a
b
10
8
6
4
2
0
0
0.8
0.75
0.65
0.55
0.7
0.6
0.1
0.6
0.7
0.8
0.9
1
0.2
0.3
0.4
0.5
0.6
0.7
The proposed method
Jain’s method
Ray’s method
AdaptiveDoG based method
The proposed method
Jain’s method
Ray’s method
AdaptiveDoG based method
0.8
Weight of minutiae with respect to pores
Weight of minutiae with respect to pores
EER (%)
EER (%)
0.9
1
Fig. 3.16 The EERs of the four methods on DBII when different weights are used. (b) is the
zoom-in of (a) when the weight is from 0.6 to 1
3.5
Experiments and Performance Evaluation
59
www.ebook3000.com

Among these steps, the steps (B) and (C) are common to most automatic ﬁngerprint
recognition systems. The step (A) needs to calculate the two measurements, OC and
IC, for each of the blocks, which can be done very efﬁciently. On a PC with
2.13 GHz Intel(R) Core(TM) 2 6400 CPU and RAM of 2GB, the Matlab imple-
mentation of the method took about 0.05 ms in average to conduct the step (A) for
one partial ﬁngerprint image used in the experiments (note that down-sampling was
applied in the experiments). In the pore detection step (D), the main operation is the
convolution of pore models with the ﬁngerprint image. This is essentially common
to all ﬁltering-based pore extraction methods including the three counterpart
methods considered in the experiments here. However, because Jain’s and Ray’s
methods both apply a single pore ﬁlter to the whole ﬁngerprint image, they are more
efﬁcient than the Adaptive DoG-based method and the method proposed here which
apply different pore ﬁlters to different blocks. Speciﬁcally, Jain’s and Ray’s
methods took less than 0.1 ms to detect the pores on one partial ﬁngerprint
image, whereas the other two methods used about 0.5 ms. The last post-processing
step (E) is also common to all pore extraction methods. Using our Matlab imple-
mentation, it took about 0.4 ms to carry out all the post-processing operations
deﬁned in Sect. 3.4.3. From the above analysis, we can see that the proposed pore
extraction method is a little more complex than Jain’s and Ray’s methods due to the
more elaborated pore models on which it is based. However, considering the gain of
accuracy by the proposed method, the increased computational complexity is
deserved. More importantly, its computational cost is still acceptable (with the
Matlab implementation, it took about 1–2 s to extract the pores on one partial
ﬁngerprint image), and we expect that the computational cost can be much reduced
by using languages like C/C++ and after optimization.
2.5
2
1
10–3
10–2
10–1
100
FAR (log)
FRR (%)
101
1.5
The proposed method (EER=0.53%)
Jain’s method (EER=0.6%)
Ray’s method (EER=0.59%)
AdaptiveDoG based method (EER=0.56%)
0.5
Fig. 3.17 The ROC curves of the four methods on DBII when the best results are obtained
60
3
Fingerprint Pore Modeling and Extraction

3.6
Summary
This chapter presented a dynamic anisotropic pore model (DAPM). It differs from
previous pore models in that it is anisotropic and dynamic so that it can more
accurately represent pores by using orientation and scale parameters. A novel
adaptive pore extraction method was then developed based on the DAPM. The
ﬁngerprint image was partitioned into well-deﬁned, ill-posed and background
blocks according to the orientation certainty and intensity contrast on the blocks.
For each well-deﬁned block, the ridge orientation and frequency were estimated
directly, while for each ill-posed block, the ridge frequency was estimated by
interpolating the ridge frequencies on its neighboring blocks. The local instances
of the DAPM were then instantiated for the well-deﬁned blocks based on the
estimated orientations and frequencies of them. The instantiated pore models
were taken as the matched ﬁlters and applied to the blocks to detect the pores
thereon. In the post-processing step, some constraints were used to remove possible
spurious and false pores in the detection result.
We have established two high-resolution ﬁngerprint databases to evaluate the
proposed method in comparison with three state-of-the-art pore extraction methods.
The proposed method obtained a true detection rate as 84.8% and a false detection
rate as 17.6%, and corresponding deviations 4.5% and 6.3%. In contrast, the best
true detection rate and false detection rate of existing methods were respectively
80.8% and 22.2%, and their deviations were 6.5% and 8.2%. These experimental
results demonstrated that the proposed DAPM is more accurate and robust than the
previous models. We consequently evaluated the pore based ﬁngerprint recognition
systems. The experiments show that by using pores as additional features to the
conventional minutia features, higher recognition accuracy can be obtained. Since
the proposed DAPM achieves higher pore detection accuracy, it obtains the best
ﬁngerprint recognition accuracy among the state-of-the-art pore extraction methods
on both partial and full-size ﬁngerprint image databases. On the partial ﬁngerprint
image database, by fusing minutia and pore match scores it improves the recogni-
tion accuracy of using only minutiae by 34.86%, whereas the improvements made
by the other methods are all below 30%. On the full-size ﬁngerprint image database,
again, the proposed method achieves the best EER at 0.53%, which improves the
other methods by over 5% to about 12%.
In this chapter, we experimentally set the scale parameter of the proposed pore
model according to the local ridge periods by a ratio. In our future work, we are
going to determine it in a more adaptive way based on the automatic scale selection
theory (Sofka and Stewart 2006; Lindeberg 1998). Currently, the proposed pore
extraction method is mainly for live-scan ﬁngerprints. In the future, we are also
going to extend the proposed method to more challenging latent ﬁngerprints.
3.6
Summary
61
www.ebook3000.com

References
Ashbaugh D (1999) Quantitative–qualitative friction ridge analysis: an introduction to basic and
advanced ridgeology. CRC Press, Boca Raton
Bazen A, Gerez S (2002) Systematic methods for the computation of the directional ﬁelds and
singular points of ﬁngerprints. IEEE Trans Pattern Anal Mach Intell 24(7):905–919
Bindra B, Jasuja O, Singla A (2000) Poroscopy: a method of personal identiﬁcation revisited.
Internet J Forensic Med Toxicol 1(1)
CDEFFS (2009) Data format for the interchange of extended ﬁngerprint and palmprint features.
Working draft version 0.4, June. http://ﬁngerprint.nist.gov/standard/cdeffs/index.html
Chen Y, Dass S, Jain A (2005) Fingerprint quality indices for predicting authentication perfor-
mance. In: Proceedings of AVBPA2005, pp 160–170
Feng J (2008) Combining minutiae descriptors for ﬁngerprint matching. Pattern Recogn
41:342–352
Hartley R, Zisserman A (2003) Multiple view geometry in computer vision, 2nd edn. Cambridge
University, Cambridge
He Y, Tian J, Li L, Chen H, Yang X (2006) Fingerprint matching based on global comprehensive
similarity. IEEE Trans Pattern Anal Mach Intell 28(6):850–862
Hong L, Wan Y, Jain A (1998) Fingerprint image enhancement: algorithms and performance
evaluation. IEEE Trans Pattern Anal Mach Intell 20(8):777–789
Jain A, Hong L, Bolle R (1997) On-line ﬁngerprint veriﬁcation. IEEE Trans Pattern Anal Mach
Intell 19(4):302–314
Jain A, Chen Y, Demirkus M (2006) Pores and ridges: ﬁngerprint matching using level 3 features.
In: Proceedings of ICPR06, vol 4, pp 477–480
Jain A, Chen Y, Demirkus M (2007a) Pores and ridges: ﬁngerprint matching using level 3 features.
IEEE Trans Pattern Anal Mach Intell 29(1):15–27
Jain A, Flynn P, Ross A (2007b) Handbook of biometrics. Springer, New York
Kryszczuk K, Drygajlo A, Morier P (2004a) Extraction of level 2 and level 3 features for
fragmentary ﬁngerprints. In: Proceedings of the 2nd COST Action 275 workshop, pp 83–88
Kryszczuk K, Morier P, Drygajlo A (2004b) Study of the distinctiveness of level 2 and level
3 features in fragmentary ﬁngerprint comparison. In: BioAW2004, LNCS 3087, pp 124–133
Lindeberg T (1998) Edge detection and ridge detection with automatic scale section. Int J Comput
Vis 30:117–156
Maltoni D, Maio D, Jain A, Prabhakar S (2003) Handbook of ﬁngerprint recognition. Springer,
New York
Parsons N, Smith JQ, Thonnes E, Wang L, Wilson RG (2008) Rotationally invariant statistics for
examining the evidence from the pores in ﬁngerprints. Law Probab Risk 7:1–14
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, New York
Ratha N, Karu K, Chen S, Jain A (1996) A real-time matching system for large ﬁngerprint
databases. IEEE Trans Pattern Anal Mach Intell 18(8):799–813
Ray M, Meenen P, Adhami R (2005) A novel approach to ﬁngerprint pore extraction. In: Pro-
ceedings of the 37th south-eastern symposium on system theory, pp 282–286
Roddy A, Stosz J (1997) Fingerprint features – statistical analysis and system performance
estimates. Proc IEEE 85(9):1390–1421
Ross A, Jain A, Reisman J (2003) A hybrid ﬁngerprint matcher. Pattern Recogn 36:1661–1673
Sofka M, Stewart C (2006) Retinal vessel centerline extraction using multiscale matched ﬁlters,
conﬁdence and edge measures. IEEE Trans Med Imaging 25(12):1531–1546
Stosz J, Alyea L (1994) Automated system for ﬁngerprint authentication using pores and ridge
structure. In: Proceedings of SPIE conference on automatic systems for the identiﬁcation and
inspection of humans, San Diego, vol 2277, pp 210–223
62
3
Fingerprint Pore Modeling and Extraction

Zhang D (2000) Automated biometrics: technologies and systems. Kluwer and Academic,
Dordrecht
Zhao Q, Zhang L, Zhang D, Luo N (2009) Direct pore matching for ﬁngerprint recognition. In:
Proceedings of ICB2009, pp 597–606
Zhao Q, Zhang D, Zhang L, Luo N (2010) High resolution partial ﬁngerprint alignment using pore-
valley descriptors. Pattern Recogn 43(3):1050–1061
References
63
www.ebook3000.com

Chapter 4
The Resolution for Fingerprint Recognition
Abstract High-resolution automated ﬁngerprint recognition systems (AFRS) offer
higher security because they are able to make use of level 3 features, such as pores,
that are not available in lower-resolution (<500 dpi) images. One of the main
parameters affecting the quality of a digital ﬁngerprint image and issues such as
cost, interoperability, and performance of an AFRS is the choice of image resolu-
tion. In this chapter, we identify the optimal resolution for an AFRS using the two
most representative ﬁngerprint features, minutiae and pores. We ﬁrst designed a
multi-resolution ﬁngerprint acquisition device to collect ﬁngerprint images at
multiple resolutions and captured ﬁngerprints at various resolutions but at a ﬁxed
image size. We then carried out a theoretical analysis to identify the minimum
required resolution for ﬁngerprint recognition using minutiae and pores. After
experiments on our collected ﬁngerprint images and applying three requirements
for the proportions of minutiae and pores that must be retained in a ﬁngerprint
image, we recommend a reference resolution of 800 dpi. Subsequent tests have
further conﬁrmed the proposed reference resolution.
Keywords High-resolution AFRS • Selecting resolution criteria • Minutiae •
Pores • Fingerprint recognition accuracy
4.1
Introduction
As one of the most popular biometric traits, ﬁngerprints are widely used in personal
authentication, particularly with the availability of a variety of ﬁngerprint acquisi-
tion devices and the advent of thousands of advanced ﬁngerprint recognition
algorithms. Such algorithms make use of distinctive ﬁngerprint features that can
usually be classiﬁed at three levels of detail (Ashbaugh 1999), as shown in Fig. 4.1
and referred to as level 1, level 2, and level 3. Level-1 features are the macro details
of ﬁngerprints, such as singular points and global ridge patterns, e.g., deltas and
cores (indicated by red triangles in Fig. 4.1). They are not very distinctive and are
thus mainly used for ﬁngerprint classiﬁcation rather than recognition. The level-2
features (red rectangles) primarily refer to the Galton features or minutiae, namely,
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_4
65

ridge endings and bifurcations. Level-2 features are the most distinctive and stable
features, which are used in almost all automated ﬁngerprint recognition systems
(AFRSs) (Ashbaugh 1999; Maltoni et al. 2009; Ratha and Bolle 2004) and can
reliably be extracted from low-resolution ﬁngerprint images (500 dpi). A resolu-
tion of 500 dpi is also the standard ﬁngerprint resolution of the Federal Bureau of
Investigation for AFRSs using minutiae (Jain et al. 2007). Level-3 features (red
circles) are often deﬁned as the dimensional attributes of the ridges and include
sweat pores, ridge contours, and ridge edge features, all of which provide quanti-
tative data supporting more accurate and robust ﬁngerprint recognition. Among
these features, pores have most extensively been studied (Jain et al. 2006, 2007;
Stosz and Alyea 1994; Roddy and Stosz 1997; Kryszczuk et al. 2004a, b; Zhao et al.
2008, 2009a, b; CDEFFS 2009; International Biometric Group 2008; Ray et al.
2005; Parsons et al. 2008; Chen 2009) and are considered to be reliably available
only at a resolution higher than 500 dpi.
Resolution is one of the main parameters affecting the quality of a digital
ﬁngerprint image, and so, it has an important role in the design and deployment of
AFRSs and impacts both their cost and recognition performance. Despite this, the
ﬁeld of AFRS does not currently have a well-proven reference resolution or standard
resolution for high-resolution AFRS that can be used interoperably between differ-
ent AFRSs. For example, Stosz and Alyea extracted pores at a resolution of approx-
imately 1270 dpi in the vertical direction and 2400 dpi in the horizontal direction
(1270  2400 dpi) (Stosz and Alyea 1994). Jain et al. chose a resolution of 1000 dpi
based on the 2005 ANSI/NIST ﬁngerprint standard update workshop (Jain et al.
2007). The Committee to Deﬁne an Extended Fingerprint Feature Set (CDEFFS
2009) deﬁned level-3 features at a resolution of 1000 dpi. Zhao et al. proposed some
pore extraction and matching methods at a resolution of 902  1200 dpi (Zhao et al.
2008, 2009a, b). Finally, the International Biometric Group analyzed level-3 fea-
tures at a resolution of 2000 dpi (International Biometric Group 2008).
In this chapter, we take steps toward establishing such a reference resolution,
assuming a ﬁxed image size and making use of the two most representative
ﬁngerprint features, i.e., minutiae and pores, and providing a minimum resolution
for pore extraction that is based on anatomical evidence. The use of a ﬁxed image
Core
Pore
: 1st Level
Features
: 2st Level
Features
: 3st Level
Features
Delta
Minutiae: Ridge Ending
Minutiae: Ridge Bifurcation
Fig. 4.1 Three levels of ﬁngerprint features
66
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

size is determined by the fact that the quality of a digital ﬁngerprint image is mainly
determined by three factors, the resolution, the number of pixels in a ﬁngerprint
image, and the measured area of the ﬁngerprint, with it being possible to uniquely
determine the value of any one given the other two. In analyzing the inﬂuence of
resolution on AFRS, it was thus necessary to ﬁx one of the other two parameters.
Here, we choose to ﬁx the image size. We conducted experiments on a set of
ﬁngerprint images of different resolutions (from 500 to 2000 dpi). By evaluating
these resolutions in terms of the number of minutiae and pores, the results have
shown that 800 dpi would be a good choice for a reference resolution. Finally, we
applied state-of-the-art automated ﬁngerprint recognition algorithms to our col-
lected ﬁngerprint images. Via cross validation experiments, we found the recogni-
tion precision under resolution 700–1000 dpi is one order of magnitude higher than
that under other considered resolutions. The highest recognition accuracy in differ-
ent ﬁngerprint groups is almost always obtained under 800 dpi. These results
validate our proposed resolution from the point of view of automated ﬁngerprint
recognition accuracy.
4.2
Multiresolution Fingerprint Images Collecting
According to our knowledge, there is no data set of multiresolution ﬁngerprint
images publicly available. We therefore collected a multiresolution ﬁngerprint
image database by using our custom-built ﬁngerprint image acquisition device. In
this section, we introduce the ﬁngerprint acquisition device and the established
multiresolution ﬁngerprint image database.
4.2.1
Acquisition Device
A multiresolution ﬁngerprint acquisition device (or sensor) must be cost-effective
but should particularly be able to acquire ﬁngerprint images at multiple resolutions
without any negative impact on the quality of the image (Maltoni et al. 2009). There
are generally three kinds of ﬁngerprint sensors: solid state, ultrasound, and optical
(Maltoni et al. 2009; Ross and Jain 2004). Solid-state sensors are small and
inexpensive but cannot capture high-resolution images (Han et al. 2005). Ultra-
sound sensors can capture high-resolution images but are usually bulky and expen-
sive (Bicz et al., online). Optical sensors can capture a variety of different image
resolutions, varying in a range of sizes and prices. They are easy to implement and
have been found to have a high degree of stability and reliability (Xia and O’Goman
2003). Our system is thus equipped with an optical ﬁngerprint sensor.
While there are also several different ways to implement optical ﬁngerprint
sensors, the oldest and most widely used way (Maltoni et al. 2009) and the way
we have chosen to implement our sensor is frustrated total internal reﬂection
(FTIR). As shown in Fig. 4.2, an FTIR-based ﬁngerprint sensor consists of a light
4.2
Multiresolution Fingerprint Images Collecting
67

source, a glass prism, a lens, and a charge-coupled device (CCD) or complementary
metal-oxide-semiconductor camera. When users put their ﬁngers on the surface of
the glass prism, ridges absorb light, and so, they appear dark, whereas valleys and
the ﬁne details on ridges reﬂect light and thus appear bright. Different resolutions
can be obtained by simply adjusting the distance between the glass prism and the
lens and the distance between the lens and the camera.
4.2.2
Fingerprint Samples
The most commonly used ﬁngers in ﬁngerprint recognition are the thumb, index
ﬁnger, and middle ﬁnger. These are also the ﬁngers that we use for the images used
in our experiments. We collected ﬁngerprint images from both males and females.
This is pertinent because male and female ﬁngers are, on the average, different in
terms of area and ridge width (or pore size). A total of 25 males and 25 females
contributed to our database. Four ﬁngerprint images were captured from each of
their six ﬁngers (i.e., thumb, index, and middle ﬁngers on right and left hands) under
each of the following resolutions: 500, 600, 700, 800, 900, 1000, 1200, 1600, and
2000. As a result, there are totally 1200 ﬁngerprint images for each of the consid-
ered resolutions in the database. Figure 4.3 shows some example ﬁngerprint images
collected from a male and a female.
4.2.3
Implementation of Multiresolution
Three factors among others can affect the quality of a ﬁngerprint image: its
resolution, the measured area of the ﬁngerprint that is captured or sensed, and the
air
contact
ridges and valleys
glass prism
lens
light
CCD or CMOS
optical path
B
A
Fig. 4.2 Operation of an
FTIR-based ﬁngerprint
sensor (Maltoni et al. 2009)
68
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

size of the image (the number of pixels). These factors are essentially not indepen-
dent but related with each other as follows:
H ¼ 25:4  h=r, W ¼ 25:4  w=r
ð4:1Þ
where r denotes the resolution, h and w denote the height and width of the image,
and H and W denote the height and width of the captured area (in millimeters). To
generate ﬁngerprint images of different resolutions, one of the other two parameters
must be ﬁxed. Table 4.1 shows the values of H and W according to (4.1) at different
resolutions when h and w are set as 640 and 480 pixels. It can be seen that, at a ﬁxed
image size, the area captured by the image decreases as the resolution increases.
Different resolutions can easily be obtained by adjusting the distances between the
Fig. 4.3 Example 800-dpi ﬁngerprint images in our established database. (a) From a female. (b)
From a male. From left to right: thumb, index ﬁnger, and middle ﬁnger
Table 4.1 The values of
H and W at various r when
h and w are set as 640 and 480
(h, w) (pixel)
r (dpi)
(H, W) (mm)
(640, 480)
500
(32.5, 24.4)
600
(27.1, 20.3)
700
(23.2, 17.4)
800
(20.3, 15.2)
900
(18.1, 13.5)
1000
(16.3, 12.2)
1200
(13.6, 10.2)
1600
(10.2, 7.6)
2000
(8.1, 6.1)
4.2
Multiresolution Fingerprint Images Collecting
69

glass prism, the lens, and the CCD. Figure 4.4 shows some example ﬁngerprint
images at different resolutions.
It should be noted that the resolution of our device is not identical along the
vertical and horizontal directions. This is because the CCD camera has a vertical
resolution of 1040 lines and a horizontal resolution of 1394 lines. At 500 dpi, this is
not a large difference, and researchers usually ignore it. However, as the resolution
increases, the difference between the vertical and horizontal resolutions becomes
more obvious. For example, at a vertical resolution of 800 dpi, the horizontal
resolution is 1064 dpi, but at a vertical resolution of 1200 dpi, the horizontal
resolution is 1596 dpi. The ratio between the horizontal resolution and vertical
resolution is equal to the one between the horizontal resolution and vertical
resolution of the CCD camera. Thus, given both the vertical or horizontal resolution
of ﬁngerprint images and the parameters of CCD camera, we can calculate the
resolution of ﬁngerprint images along the other direction. For simplicity, in this
chapter, we refer just to the vertical resolution.
Fig. 4.4 Example ﬁngerprint images at different resolutions when using a ﬁxed image size of
640  480 pixels. (a) 500 dpi. (b) 600 dpi. (c) 700 dpi. (d) 800 dpi. (e) 900 dpi. (f) 1000 dpi. (g)
1200 dpi. (h) 1600 dpi. (i) 2000 dpi
70
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

4.3
Selecting Resolution Criteria Using Minutiae and Pores
Generally, people may think that higher recognition accuracy can be achieved by
increasing the resolution. It is true if the whole ﬁngerprint region is covered.
However, in practical AFRSs, the ﬁngerprint image size is usually conﬁned to a
relatively small one for the purpose of miniaturization and reducing the computa-
tional complexity. Until now, the most widely used image size in most chapters
(Roddy and Stosz 1997; Kryszczuk et al. 2004a, b; Jain et al. 2006; Zhao et al. 2008,
2009a, b; CDEFFS 2009; International Biometric Group 2008; Ray et al. 2005;
Parsons et al. 2008; Chen 2009) or in most public ﬁngerprint image databases such
as the ﬁngerprint veriﬁcation competition (FVC) databases (e.g., FVC2000,
FVC2002, FVC2004, and FVC2006) is 640  480 pixels. With a limited image
size, the larger the resolution is, the smaller the captured ﬁngerprint region will
be. Although increasing the ﬁngerprint image resolution can provide more ﬁne
details on ﬁngerprints for ﬁngerprint matching, it would degrade the ﬁngerprint
recognition accuracy if the lost of useful discriminative information (e.g., minutiae)
due to decreased ﬁngerprint areas dominates the newly emerged ﬁngerprint details
(e.g., pores). For instance, the ﬁngerprint images of a ﬁxed size might cover the
whole ﬁngerprint regions at low resolution but capture only few ridges on the
ﬁngers at high resolution (see Fig. 4.4). Thus, in this chapter aiming at a balance
between various ﬁngerprint features (in particular, minutiae and pores) available on
high-resolution ﬁngerprint images, we investigate the ﬁngerprint distinctiveness
and recognition accuracy at different resolutions when a ﬁxed image size is
adopted. It is also worth mentioning that noise caused by the skin condition or
the amount of pressure applied by the ﬁnger (Maltoni et al. 2009) also plays an
important role in the recognition performance of AFRS due to its inﬂuence on the
quality of ﬁngerprint images. However, it is a common issue to ﬁngerprint images
at all resolutions and is thus out of the scope of the resolution selection work in this
chapter.
Since 500-dpi minutiae-based AFRSs were taken as the baseline systems, we
chose the ﬁngerprint image size so that as many minutiae as possible are captured
by the 500-dpiﬁngerprint image, or in other words, it can cover the full ﬁngerprint
region. By experience, we used an initial image size of 640  480 pixels. As can be
seen in Fig. 4.4, this size can actually capture the full ﬁngerprint region at resolu-
tions of 500 and 600 dpi as well. Thus, we cropped the foreground ﬁngerprint
regions on these 500-dpi ﬁngerprint images by using rectangles. The maximum
width and height of these rectangles observed in the database are 380 and 360 pixels,
which were ﬁnally taken as the image size for the ﬁngerprint images captured under
higher resolutions (i.e., 600–2000 dpi in the experiments in this chapter). Such an
image size, which may be comparable with the templates stored in most of existing
minutiae-based AFRSs, will be very helpful to realize the interoperability between
different AFRSs, which is one motivation of this chapter.
In order to utilize the minutiae and pores on ﬁngerprints, it is necessary that we
be able to robustly extract both of these features. Minutiae can be robustly extracted
4.3
Selecting Resolution Criteria Using Minutiae and Pores
71

from images of 500 dpi or above but pore extraction requires higher resolution
images according to investigating most of the chapters about ﬁngerprints’ studies
(Ashbaugh 1999; Maltoni et al. 2009; Ratha and Bolle 2004; Jain et al. 1997, 2006,
2007; Stosz and Alyea 1994; Roddy and Stosz 1997; Kryszczuk et al. 2004a, b;
Zhao et al. 2008, 2009a, b; CDEFFS 2009; International Biometric Group 2008;
Ray et al. 2005; Parsons et al. 2008; Chen 2009; Ross and Jain 2004). It thus became
necessary to ﬁgure out what would be the minimum resolution needed to extract
pore features. Intuitively, such a ﬁgure can be arrived at based on anatomical
evidence, i.e. the possible smallest physical size of pores on ﬁngers. We will discuss
this in detail in Sect. 4.4.1.
We ﬁnally raised three criteria to select the image resolution for high-resolution
AFRSs by considering the following. (1) Given a ﬁxed image size, retain as many
minutiae as possible while pores begin to be available. (2) The number of pores
begins to decrease, and no other useful information but the position of pores will be
conveyed when resolution reaches a certain value. (3) Minutiae are more discrim-
inative than pores if the same number of them is considered. Retain as many
minutiae as possible while also retaining an acceptable number of pores.
We can better understand the rationale for the criteria by considering the images
of an example ﬁnger shown in Fig. 4.5, whose image size is 380  360 pixels and
resolution increases from 500 to 2000 dpi. The minutiae are the features of interest
and are marked with red circles. The availability of pores can also be seen on these
images. One may clearly observe the change of available minutiae and pores across
these ﬁngerprint images of different resolutions. Next, we introduce the three
selection criteria in detail.
Criterion 1 Given a Fixed Image Size, Retain as Many Minutiae as Possible While
Pores Begin to Be Available. A Lower Limit Image Resolution Can Be Obtained.
Most minutiae based AFRSs judge whether two ﬁngerprints are from the same
ﬁnger by counting the number of matched minutiae; basically, the larger the
number of minutiae is, the higher the possibility of making correct judgment will
be. Thus, we should try to retain as many minutiae as possible. Table 4.2 lists the
number of minutiae and pores in an image at different resolutions. As expected, the
number of minutiae decreases as resolution increases. On the other hand, the
number of pores ﬁrst increases and then decreases as resolution increases.
According to the analysis of Stosz and Alyea (1994), there is a minimum resolution
(larger than 500 dpi) for robust pore extraction. As a consequence, criterion 1 is
established to determine the lower limit of resolution.
Criterion 2 The Number of Pores Begins to Decrease, and No Other Useful
Information but the Position of Pores Will Be Conveyed When Resolution Reaches
a Certain Value. An Upper Limit Image Resolution Can Be Obtained.
As can be seen in Fig. 4.5, the size and shape of pores become more visible at
higher resolutions. However, according to Ashbaugh (1999) and Roddy and Stosz
(1997), usually, only the location of pores is reliable discriminative information
for ﬁngerprint recognition; on the contrary, the size and shape of one pore can
signiﬁcantly vary from one impression to another. The two 2000-dpi images in
Fig. 4.6 are from the same ﬁnger but collected at different times. Clearly, the pores’
72
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

size and shape (see the pores marked by red circles) are corrupted by noise or
inﬂuenced by the condition of pores (open or closed). We thus set another criterion
for resolution selection based on the number of pores at different resolutions, which
can offer us the upper limit resolution.
Fig. 4.5 Minutiae and pores on ﬁngerprint images of 380  360 pixels at different resolutions. (a)
500 dpi. (b) 600 dpi. (c) 700 dpi. (d) 800 dpi. (e) 900 dpi. (f) 1000 dpi. (g) 1200 dpi. (h) 1600 dpi.
(i) 2000 dpi
Table 4.2 Number of minutiae and pores in Fig. 4.5 at different resolutions
r (dpi)
500
600
700
800
900
1000
1200
1600
2000
Num_minu
51
46
35
30
20
18
12
6
4
Num_pore
0
85
617
683
710
609
356
172
140
4.3
Selecting Resolution Criteria Using Minutiae and Pores
73

Criterion 3 Minutiae Are More Discriminative than Pores If the Same Number of
Each Is Considered. Retain as Many Minutiae as Possible While Also Retaining an
Acceptable Number of Pores. A Reference Image Resolution Is Then Proposed.
Criteria 1 and 2 put emphasis on the number of minutiae and pores, respectively,
which just offer the lower resolution and upper resolution for high-resolution
AFRSs. However, it is obvious that this will at times also require us to make
some kind of tradeoff between the two. In this tradeoff, the bias will be toward
retaining minutiae because the distribution of minutiae is more random than that of
pores, and so, the number of minutiae in an image will have a greater inﬂuence on
ﬁngerprint recognition. The blue line on Fig. 4.7 links ten adjacent minutiae on a
ﬁngerprint image, while the red line links adjacent pores. We can see that the blue
Fig. 4.6 Two prints of one ﬁnger under 2000 dpi captured at different times
Fig. 4.7 Distribution of a similar number of minutiae and pores on a ﬁngerprint image of
380  360 pixels
74
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

line traverses approximately 1/3 of the entire ﬁngerprint image while the red line is
concentrated in just one area of about 1/100 of the ﬁngerprint image. From this, it
would seem that if one or the other, i.e., minutiae or pores, must be traded off, then
we lose less discriminative power if we bias toward retaining minutiae in the
selection of a suitable resolution. We thus set our last criterion for resolution
selection as retaining as many minutiae as possible while an acceptable number
of pores are available.
Note that all the aforementioned three criteria are about the number of minutiae
and pores with a ﬁxed image size. However, the ridge width, which differs between
different kinds of ﬁngers (e.g., thumb, index ﬁnger, and middle ﬁnger) (Cummins
et al. 1941) and between different genders (female and male) (Acree 1999), also has
some effect on the number of minutiae and pores for a ﬁxed image size and would
consequently affect the selection of resolution. To make the reference resolution we
selected based on the established criteria being universal to all ﬁngers, it is
necessary to study the relationship between the ridge width and the resolution.
An analysis of ridge width on different kinds of ﬁngers (e.g., thumb, index ﬁnger,
and middle ﬁnger) and on ﬁngers from different genders (female and male) is
conducted with respect to the resolution selected based on the established criteria.
Section 4.4.3 will report the analysis result.
4.4
Experiments and Analysis
To get a reference resolution based on our established criteria and to verify it, some
analysis and experiments are organized as follows. First, a theoretical analysis of
the minimum resolution for pore extraction is given. Second, the statistical number
of minutiae and pores manually counted is offered. Third, an analysis of the ridge
width on different kinds of ﬁngers (thumb, index ﬁnger, and middle ﬁnger) and on
ﬁngers of different genders (female and male) is given. Finally, the automated
ﬁngerprint recognition results of different resolution ﬁngerprint images are
provided.
4.4.1
Minimum Resolution Required for Pore Extraction
There is a minimum resolution that is required to be able to extract pores well. In
1994, Stosz and Alyea (1994) automatically extracted pores using a high-resolution
ﬁngerprint sensor. They noted that pores could range in size from 60 to 250 μm in
one dimension and that the smallest detectable pores, i.e., 60 μmin one dimension,
determined the minimum resolution required by a sensor. They assumed a sampling
period half the size of the smallest pore and concluded that the minimum required
are solution of 800 dpi. In a later chapter, in 1997, Roddy and Stosz (1997) talked
4.4
Experiments and Analysis
75

about a range of pore sizes of 88–220 μm. Taking these two ﬁgures into account, in
this chapter, we use the average of these two minimum pore sizes.
To determine the minimum required resolution, we take the size of pores and the
resolution and apply (4.1) to calculate the number of pixels in a pore. Then, based
on the rule that the size of the smallest pores in one dimension can be down sampled
(Stosz and Alyea 1994), we know that the minimum resolution for pore extraction
should guarantee that there are at least 2 pixels of the smallest pores in one
dimension, as illustrated in Fig. 4.8. Table 4.3 shows the minimum values of height
h for different resolutions. We can see that the minimum resolution required for
pore detection is 700 dpi when assuming a sampling period half the size of the
smallest pores.
4.4.2
Resolution Based on the Established Criteria
Given a ﬁxed image size, as resolution increases, the number of minutiae decreases
and pores become more visible. We manually counted the numbers of minutiae and
pores in the 120 ﬁngerprint images at each resolution (500  2000 dpi) at an image
size of 380  360 pixels and then averaged these numbers. Figure 4.9 shows the
relationship between the numbers of minutiae and pores. We have exaggerated the
number of minutiae tenfold for the purpose of display. We can see that the number
of minutiae is monotonically decreasing but within an acceptable range from 500 to
1000 dpi and that a relatively large number of pores (statistical number by counting
manually) are retained at resolutions in the range of 700–1000 dpi. It would appear
that the best choice of resolution for ﬁngerprint recognition is 700 dpi. However,
given that 700 dpi is the minimum resolution for pore extraction, we decided that to
make the system more robust to noise, 800 dpi would be a better choice.
Downsample
1 pixel in one dimension
2 pixels in one dimension
Fig. 4.8 Rule used to choose the minimum resolution for pore extraction
Table 4.3 The minimum value of h of different resolutions
Resolutions (dpi)
500
600
700
800
900
1000
1200
1600
2000
Minimum value of h (pixel)
1.5
1.7
2.0
2.3
2.6
2.9
3.5
4.7
5.8
76
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

4.4.3
Analysis of Ridge Width
Since minutiae and pores are both related to ﬁngerprint ridges, there is some
inﬂuence of ridge width on the number of minutiae and pores. We thus did some
analysis about the ridge width for different groups of ﬁngers. Ridge width has been
studied in Cummins et al. (1941) and Acree (1999). In Cummins et al. (1941), the
ridge width was determined by counting the ridges transversely crossing a line of
1 cm. In the chapter, the authors concluded that ridge width has little to do with the
body weight, stature, hand length, and so on. They also summarized that the ridge
width is different for different ﬁnger seven though they do not greatly differ.
However, they did not discuss the relationship between ridge width and gender,
for the reason that all the samples used in their chapter are from males. The
relationship between ridge width and gender was studied in Acree (1999). Ridge
width in that chapter was decided by the ridge density, which counted the epidermal
ridges on ﬁngerprints with a 5  5 mm square drawn on transparent ﬁlm. The
authors of Acree (1999) concluded that women tend to have a statistically signif-
icant greater ridge density. Getting aware of the variation of ridge width, we also
studied the ridge width on different kinds of ﬁngers (e.g., thumb, index ﬁnger and
middle ﬁnger) and on ﬁngers from different genders (female and male) by using our
collected ﬁngerprint image database at the selected resolution 800 dpi. The ridge
density used in Acree (1999) is adopted here to determine the ridge width. In our
database, there are 150 female ﬁngers, 150 male ﬁngers, 100 thumbs, 100 index
ﬁngers, and 100 middle ﬁngers. Some descriptive statistics of dermal ridge densities
as mentioned in Acree (1999) and the corresponding ridge width represented in
terms of micrometers [calculated by the following (4.2)] and pixels [calculated by
formula (4.1)] are given for different groups of ﬁngers in Table 4.4 The
Fig. 4.9 Average numbers of minutiae and pores in 120 selected images in our database at
different resolutions
4.4
Experiments and Analysis
77

corresponding ridge width represented in terms of micrometers is calculated as
follows:
ridge width ¼ sqrt 52 þ 52


= ridge density  2
ð
Þ
ð4:2Þ
Here, the diagonal length of the 5  5 mm square is considered as the overall
length of all ridge-valley period.
Table 4.4 shows the standard variation (SD), mean value (Mean), minimum
value per person (Minimum), and maximum value per person (Maximum) of the
ridge density, as well as their corresponding ridge width on different groups of
ﬁngers. The results of different kinds of ﬁngers (thumb, index ﬁnger, and middle
ﬁnger) in Table 4.4 show that there is little difference in ridge width between them,
which agrees with the conclusion made in Cummins et al. (1941). The results in
Table 4.4 also show that the ridge width of females is generally smaller than that of
males by 15 μm or 0.5 pixels. However, this difference is not signiﬁcant (i.e., of sub
pixel level). Thus, we conclude that, under a resolution of 800 dpi, the ridge width
had little inﬂuence on the number of minutiae and pores. It makes our proposed
reference resolution universal to all ﬁngers.
4.4.4
Fingerprint Recognition Accuracy
To verify our choice of resolution and its relationship to accurate ﬁngerprint
recognition, we conducted a series of experiments using the fusion strategy
presented in Zhao et al. (2009a, b) by combining the state-of-the-art minutia-
based method proposed in Jain et al. (1997) and the pore-based method proposed
in Zhao et al. (2009a, b), evaluating recognition accuracy according to the equal
error rate (EER). Speciﬁcally, we did cross-validation experiments by dividing all
ﬁngers into three groups according to the types of ﬁngers (i.e., thumb, index ﬁnger,
and middle ﬁnger, respectively), as well as by dividing all ﬁngers into female and
male groups. The recognition results by considering all the ﬁngers included in our
Table 4.4 Descriptive statistics comparisons of ridge density and their corresponding ridge width
on different group of ﬁngers.
Females
Males
Thumb
Index
ﬁnger
Middle
ﬁnger
Number of ﬁngers
150
150
100
100
100
Mean (ridges/25 mm2)
19.13
17.67
18
18.23
18.67
Corresponding ridge width
(μm, pixel)
(185, 5.8)
(200, 6.3)
(196, 6.2)
(194, 6.1)
(189, 6.0)
Minimuma (ridges/25 mm2)
16.83
15.50
16.13
16.00
16.67
Corresponding ridge width
(μm, pixel)
(210, 6.6)
(228, 7.2)
(219, 6.9)
(220, 6.9)
(212, 6.7)
Maximuma (ridges/25 mm2)
22.67
21.67
21.17
21.77
22
78
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

database were also given. The lower the value of EER is, the higher the recognition
accuracy will be. Figure 4.10 shows the EERs obtained at different resolutions on
the six different groups of ﬁngers and the mean EERs by averaging those EERs at
different resolutions. For the thumb, index ﬁnger, and middle ﬁnger groups, the
EERs were obtained from 600 genuine scores (generated from 100 ﬁngers, 4 pic-
tures of each ﬁnger) and 4950 imposter scores (generated from 100 ﬁngers, com-
paring the ﬁrst images of different ﬁngers). For the female and male groups, the
EERs were obtained from 900 genuine scores (generated from 150 ﬁngers, four
pictures of each ﬁnger) and 11,175 imposter scores (generated from 150 ﬁngers,
comparing the ﬁrst images of different ﬁngers). When considering all the ﬁngers,
the EERs were obtained from 1800 genuine scores (generated from 300 ﬁngers,
four pictures of each ﬁnger) and 44,850 imposter scores (generated from 300 ﬁn-
gers, comparing the ﬁrst images of different ﬁngers).
Figure 4.10 shows the EERs on different groups of ﬁnger sat different resolu-
tions by fusing the state-of-the-art minutia based method proposed in Jain et al.
(1997) and the pore-based method proposed in Zhao et al. (2009a, b). Speciﬁcally,
the black line in Fig. 4.10 shows the recognition results when only the males’
ﬁngers in our database are considered. The lowest EER is obtained when resolution
is 700 dpi. The red line in Fig. 4.10 shows the EER values at different resolutions
when only females’ ﬁngers in our database are involved. The lowest EER is
obtained when the resolution is 900 dpi. The gray line that represents the EERs
when only thumbs are considered shows that the lowest EER can be obtained at the
resolution of 700 dpi. The rest of the lines in Fig. 4.10 all show that the lowest EER
is achieved at the resolution of 800 dpi. However, all of the results in Fig. 4.10 show
that a relatively lower EER can be obtained when the resolution is between 700 and
1000 dpi. A resolution of 800 dpi can achieve the lowest EER in most cases and the
lowest mean EER of the sixfold experiment (pink line). This result further conﬁrms
our proposed reference resolution.
Fig. 4.10 EERs obtained at different resolutions on the six different groups of ﬁngers and the
mean EERs by averaging those EERs at different resolutions
4.4
Experiments and Analysis
79

4.5
Summary
This chapter has proposed a method for selecting a reference resolution for use in
high-resolution AFRSs based on minutiae and pores. We have initially found that,
based on anatomical evidence, a minimum resolution of 700 dpi would give good
results, but further analysis based upon an analysis of the number of minutiae and
pores and the ridge width on different kinds of ﬁngers and on ﬁngers of different
genders, as well as tests of comparative accuracy, has led us to recommend a
reference resolution of 800 dpi. While we have regarded this as an advance, we
must point out that the image size also has an important role in high-resolution
AFRSs. In this chapter, we limited images to a size of 380  360 pixels to allow us
to investigate only the impact of resolution. In future work, we will investigate how
to best make the tradeoff between the inﬂuences of resolution and image size within
a certain range on high-resolution AFRS and to ﬁgure out whether there exists a
dynamic resolution to different image sizes for high-resolution AFRSs.
References
Acree M (1999) Is there an gender difference in ﬁngerprint ridge density? Forensic Sci Int
102(1):35–44
Ashbaugh D (1999) Quantitative–qualitative friction ridge analysis: an introduction to basic and
advanced ridgeology. CRC Press, Boca Raton, FL
Bicz W, Banasiak D, Bruciak P, Gumienny S, Gumulinski D, Keysiak A, Kuczynski W, Pluta M,
Rabiej G. Fingerprint structure imaging based on an ultrasound camera [online]. http://www.
optel.com.pl/article/english/article.htm
CDEFFS (2009) Data format for the interchange of extended ﬁngerprint and palmprint features,
version 0.4
Chen Y (2009) Extended feature set and touchless imaging for ﬁngerprint matching. Michigan
State University
Cummins H, Waits W, McQuitty J (1941) The breadths of epidermal ridges on the ﬁnger tips and
palms: a study of variation. Am J Anat 68(1):127–150
Han J, Tan Z, Sato K, Shikida M (2005) Thermal characterization of micro heater arrays on a
polyimide ﬁlm substrate for ﬁngerprint sensing applications. J Micromech Microeng
15(2):282–289
International Biometric Group (2008) Analysis of level 3 features at high resolutions (phase II)
Jain A, Hong L, Bolle R (1997) On-line ﬁngerprint veriﬁcation. IEEE Trans Pattern Anal Mach
Intell 19(4):302–314
Jain A, Chen Y, Demirkus M (2006) Pores and ridges: ﬁngerprint matching using level 3 features.
In: Proceedings of the 18th international conference on pattern recognition, pp 477–480
Jain A, Chen Y, Demirkus M (2007) Pores and ridges: high-resolution ﬁngerprint matching using
level-3 features. IEEE Trans Pattern Anal Mach Intell 29(1):15–27
Kryszczuk K, Drygajlo A, Morier P (2004a) Extraction of level 2 and level 3 features for
fragmentary ﬁngerprints. In: Proceedings of the 2nd COST Action 275 workshop, Vigo,
Spain, pp 83–88
Kryszczuk K, Morier P, Drygajlo A (2004b) Study of the distinctiveness of the level 2 and level
3 features in fragmentary ﬁngerprint comparison. In: Proceedings of the biometrics authenti-
cation, ECCV 2004 international workshop, pp 124–133
80
4
The Resolution for Fingerprint Recognition
www.ebook3000.com

Maltoni D, Maio D, Jain A, Prabhakar S (2009) Handbook of ﬁngerprint recognition. Springer,
New York
Parsons N, Smith J, Thonnes E, Wang L, Wilson R (2008) Rotationally invariant statistics for
examining the evidence from the pores in ﬁngerprints. Law Probab Risk 7(1):1–14
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, New York
Ray M, Meenen P, Adhami R (2005) A novel approach to ﬁngerprint pore extraction. In: Pro-
ceedings of the thirty-seventh southeastern symposium on system theory, pp 282–286
Roddy A, Stosz J (1997) Fingerprint features-statistical analysis and system performance esti-
mates. Proc IEEE 85(9):1390–1421
Ross A, Jain A (2004) Biometric sensor interoperability: a case study in ﬁngerprints. In: Pro-
ceedings of the international ECCV workshop on biometric authentication, LNCS 3087,
pp 134–145
Stosz J, Alyea L (1994) Automated system for ﬁngerprint authentication using pores and ridge
structure. Proc SPIE 2277:210–223
Xia X, O’Gorman L (2003) Innovation in ﬁngerprint capture devices. Pattern Recogn
36(2):361–369
Zhao Q, Zhang L, Zhang D, Luo N, Bao J (2008) Adaptive pore model for ﬁngerprint pore
extraction. In: Proceedings of the 18th international conference on pattern recognition, pp 1–4
Zhao Q, Zhang L, Zhang D, Luo N (2009a) Direct pore matching for ﬁngerprint recognition. In:
ICB 2009, pp 597–606
Zhao Q, Zhang D, Zhang L, Luo N (2009b) High resolution partial ﬁngerprint alignment using
pore-valley descriptors. Pattern Recogn 43(3):1050–1061
References
81

Part II
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

Chapter 5
Finger-Knuckle-Print Veriﬁcation
Abstract Biometric based personal authentication is an effective method for
automatically recognizing, with a high conﬁdence, a person’s identity. By observ-
ing that the texture pattern produced by bending the ﬁnger knuckle is highly
distinctive, in this chapter we present a new biometric authentication system
using ﬁnger-knuckle-print (FKP) imaging. A speciﬁc data acquisition device is
constructed to capture the FKP images, and then an efﬁcient FKP recognition
algorithm is presented to process the acquired data in real time. The local convex
direction map of the FKP image is extracted, based on which a local coordinate
system is established to align the images and a region of interest is cropped for
feature extraction. For matching two FKPs, a feature extraction scheme which
combines orientation and magnitude information extracted by Gabor ﬁltering is
proposed. An FKP database, which consists of 7920 images from 660 different
ﬁngers, is established to verify the efﬁcacy of the proposed system and promising
results are obtained. Compared with the other existing ﬁnger-back surface based
biometric systems, the proposed FKP system achieves much higher recognition rate
and it works in real time. It provides a practical solution to ﬁnger-back surface
based biometric systems and has great potentials for commercial applications.
Keywords Biometrics • Finger-knuckle-print • Personal authentication
5.1
Introduction
Personal authentication is a common concern to both industries and academia due
to its numerous applications such as physical access control, computer security,
banking and law enforcement, etc. Biometrics, which refers to the unique physio-
logical or behavioral characteristics of human beings, can be used to distinguish
between individuals and hence can serve as an ideal solution to this problem. With
the rapid development of computer techniques, in the past three decades researchers
have exhaustively investigated the use of a number of biometric characteristics,
including ﬁngerprint (Jain and Flynn 2007; Maltoni et al. 2003; Ratha and Bolle
2004), face (Delac and Grgic 2007; Wechsler 2006), iris (Daugman 1993, 2004),
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_5
85

retina (Hill 1999; Borgen et al. 2008), palmprint (Guo et al. 2009; Zhang et al. 2003;
Kong and Zhang 2004; Kong et al. 2006; Sun et al. 2005; Huang et al. 2008; Jia
et al. 2008), hand geometry (Jain et al. 1999; Jain and Duta 1999; Sanchez-Reillo
et al. 2000), hand vein (Wang et al. 2008; Kumar and Prathyusha 2008), ﬁnger
surface (Woodard and Flynn 2005a, b; Ravikanth and Kumar 2007; Kumar and
Zhou 2009a, b), inner-knuckle-print (Li et al. 2004; Nanni and Lumini 2009a, b),
voice (Hollien 2002), ear (Burge and Burger 1999), gait (Nixon et al. 2006) and
signature (Plamondona and Loretteb 1989; Liu et al. 1979), etc. Although many
biometric techniques are still under the stage of research and development, some
biometric systems have been used in a large scale; for example, the Hong Kong
government has been using the ﬁngerprint recognition system as the automated
passenger clearance system (e-channel) since 2004 (E-channel System of the Hong
Kong government 2004).
Among various kinds of biometric identiﬁers, hand-based biometrics has been
attracting considerable attention over recent years. Fingerprint (Jain and Flynn
2007; Maltoni et al. 2003; Ratha and Bolle 2004), palmprint (Guo et al. 2009;
Zhang et al. 2003; Kong and Zhang 2004; Kong et al. 2006; Sun et al. 2005; Huang
et al. 2008; Jia et al. 2008), hand geometry (Jain et al. 1999; Jain and Duta 1999;
Sanchez-Reillo et al. 2000), hand vein (Wang et al. 2008; Kumar and Prathyusha
2008), and inner-knuckle-print (Li et al. 2004; Nanni and Lumini 2009a, b) have
been proposed and well investigated in the literature. The popularity of hand-based
biometrics should be attributed to its high user acceptance. In fact, the image pattern
in the ﬁnger knuckle surface is highly unique and thus can serve as a distinctive
biometric identiﬁer. Woodard and Flynn (2005a, b) are among the ﬁrst scholars
who exploit the use of ﬁnger knuckle surface in biometric systems. They set up a
3-D ﬁnger back surface database with the Minolta 900/910 sensor. For feature
extraction, they used the curvature based shape index to represent the ﬁnger back
surface. Woodard’s work makes a good effort to validate the uniqueness of outer
ﬁnger surface as a biometric characteristic. However, their work did not provide a
practical solution to establishing an efﬁcient system using the outer ﬁnger surface
features. The cost, size and weight of the Minolta 900/910 sensor limit the use of it
in a practical biometric system, and the time-consuming 3-D data acquisition and
processing limit its use in real-time applications. In addition, they did not fully
exploit the ﬁnger knuckle texture information in feature extraction. These systems
are not only different in the imaging technologies, but also different in the feature
extraction methods. In the following parts, the imaging technique and feature
extraction will be presented separately.
Later, Kumar and Ravikanth (Ravikanth and Kumar 2007; Kumar and
Ravikanth 2009) proposed another approach to personal authentication by using
2-D ﬁnger-back surface imaging. They developed a system to capture hand-back
images and then extracted the ﬁnger knuckle areas by some preprocessing steps.
The subspace analysis methods such as PCA, LDA and ICA were combined to do
feature extraction and matching. With Kumar’s design, the acquisition device is
doomed to have a large size because nearly the whole hand back area has to be
captured, despite the fact that the ﬁnger knuckle area only occupies a small portion
of the acquired image. Furthermore, subspace analysis methods may be effective
86
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

for face recognition but they may not be able to effectively extract the distinctive
line and junction features from the ﬁnger knuckle surface. In Kumar’s later work
(Kumar and Zhou 2009a, b), they used the robust line orientation code proposed in
(Jia et al. 2008) to extract the orientation of the ﬁnger-back surface images.
In this chapter, a new hand-based biometric technique, namely ﬁnger-knuckle-
print (FKP), is developed for online personal authentication. FKP refers to the
image pattern of the outer surface around the phalangeal joint of one’s ﬁnger, which
is formed by bending slightly the ﬁnger knuckle. A specially designed acquisition
device is constructed to collect FKP images. Unlike the systems in (Woodard and
Flynn 2005a, b) and (Kumar and Ravikanth 2009) which ﬁrst capture the image of
the whole hand and then extract the ﬁnger or ﬁnger knuckle surface areas, the
proposed system captures the image around the ﬁnger knuckle area of a ﬁnger
directly, which largely simpliﬁes the following preprocessing steps. Meanwhile,
with such a design the size of the imaging system can be greatly reduced, which
improves much its applicability. Since the ﬁnger knuckle will be slightly bent when
being imaged in the proposed system, the inherent ﬁnger knuckle print patterns can
be clearly captured and hence the unique features of FKP can be better exploited.
After an FKP image is captured, a region of interest (ROI) needs to be cropped
from the original image for the following feature extraction. An efﬁcient FKP ROI
extraction algorithm is proposed based on the intrinsic characteristics of FKP
images. For matching two FKP ROI images, we propose a new feature extraction
scheme which combines orientation and magnitude information extracted by Gabor
ﬁlters. Experimental results show that it outperforms the other state-of-the-arts
coding based feature extraction methods, such as CompCode (Kong and Zhang
2004), OrdinalCode (Sun et al. 2005), RLOC (Jia et al. 2008; Kumar and Zhou
2009a, b) and BOCV (Guo et al. 2009), in FKP recognition. To evaluate the
performance of the proposed technique, an FKP database was established using
our prototype system, which consists of 7920 images from 660 different ﬁngers.
Experimental results demonstrated that the proposed FKP based authentication
system can verify the personal identity in real time with a high recognition rate.
Compared with the other existing ﬁnger knuckle surface based biometric systems
(Woodard and Flynn 2005a, b; Ravikanth and Kumar 2007; Kumar and Ravikanth
2009), the proposed one performs much better in terms of both the recognition
accuracy and the speed.
5.2
The Finger-Knuckle-Print (FKP) Recognition System
The schematic diagram of our FKP based personal authentication system is shown
in Fig. 5.1. The system is composed of a data acquisition module and a data
processing module. The data acquisition module is composed of a ﬁnger bracket,
a ring LED light source, a lens, a CCD camera and a frame grabber. The captured
FKP image is inputted to the data processing module, which comprises three basic
steps: ROI (region of interest) extraction, feature extraction and coding, and
5.2
The Finger-Knuckle-Print (FKP) Recognition System
87

matching. Figure 5.2 shows the outlook of our FKP image acquisition device whose
overall size is 160 mm  125 mm  100 mm.
A critical issue in data acquisition is to make the data collection environment as
stable and consistent as possible so that variations among images collected from the
same ﬁnger can be reduced to the minimum. In general, a stable image acquisition
process can effectively reduce the complexity of the data processing algorithms and
improve the image recognition accuracy. Meanwhile, we want to put as little
constraint as possible on the users in order for high user friendliness of the system.
With the above considerations, a semi-closed data collection environment is
designed in our system. The LED light source and the CCD camera are enclosed
in a box so that the illumination is nearly constant. One difﬁcult problem is how to
make the gesture of the ﬁnger be nearly constant so that the captured FKP images
from the same ﬁnger are consistent. In our system, the ﬁnger bracket is designed for
this purpose.
CCD
LED
Frame Grabber
Finger Bracket
Triangular Block
Basal Block
Registration 
Database
Input FKP image
ROI sub-image
Imposter
Genuine
ROI Extraction
Feature Extraction and Coding
Data processing
Data acquisition
Matching Score Calculation
Feature map
Fig. 5.1 Structure of the proposed FKP-based personal authentication system. The whole system
is composed of a data acquisition module and a data processing module
Fig. 5.2 (a) The outlook of the developed FKP image acquisition device; (b) The device is being
used to collect FKP samples
88
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

Refer to Fig. 5.1, a basal block and a triangular block are used to ﬁx the position
of the ﬁnger joint. In data acquisition, the user can easily put his/her ﬁnger on the
basal block with the middle phalanx and the proximal phalanx touching the two
slopes of the triangular block. Such a design aims at reducing the spatial position
variations of the ﬁnger in different capturing sessions. The triangular block is also
used to constrain the angle between the proximal phalanx and the middle phalanx to
a certain magnitude so that line features of the ﬁnger knuckle surface can be clearly
imaged.
After the image is captured, it is sent to the data processing module for
preprocessing, feature extraction and matching (refer to Sects 3 and 4 for details).
The size of the acquired FKP images is 768  576 under a resolution about 400 dpi.
Figure 5.3 shows four sample images acquired by the developed device. Two
images in the ﬁrst row are from one ﬁnger and images in the second row are from
another ﬁnger. Example images for the same ﬁnger were captured at two different
collection sessions with an interval of 56 days. We see that by using the developed
system, images from the same ﬁnger but collected at different times are very similar
to each other. Meanwhile, images from different ﬁngers are very different, which
implies that FKP has the potential for personal identiﬁcation.
Fig. 5.3 Sample FKP images acquired by the developed system. (a) and (b) are from one ﬁnger
while (c) and (d) are from another ﬁnger. Images from the same ﬁnger are taken at two different
sessions with an interval of 56 days
5.2
The Finger-Knuckle-Print (FKP) Recognition System
89

5.3
ROI (Region of Interest) Extraction
FKP images collected from different ﬁngers are very different. On the other hand,
for the same ﬁnger, images collected at different collection sessions will also vary
because of the variation of spatial locations of the ﬁnger. Therefore, it is necessary
and critical to align FKP images by adaptively constructing a local coordinate
system for each image. With such a coordinate system, an ROI can be cropped
from the original image in order for reliable feature extraction and matching. In this
section, we will propose an algorithm for the local coordinate system determination
and ROI sub-image extraction.
Because the ﬁnger is always put ﬂatly on the basal block when the FKP image is
captured, the bottom boundary of the ﬁnger is stable in every image and can be
taken as the X-axis of the ROI coordinate system. However, the Y-axis is much
more difﬁcult to determine. Intuitively, we want to locate the Y-axis in the center of
the phalangeal joint so that most of the useful features in the FKP image can be
preserved within the ROI. It can be observed that line features on the two sides of
the phalangeal joint have different convex directions. Taking this fact as a hint, we
propose to code line pixels based on their convex directions and then make use of
the convex direction codes to determine the Y-axis. Figure 5.4 illustrates the main
steps of the coordinate system determination and the ROI extraction. In the follow-
ing, we describe these steps in detail.
Step 1: Image down-sampling
The size of the captured FKP image is 768  576 under a resolution of 400 dpi.
Based on our experiments, it is not necessary to use such a high resolution for
feature extraction and matching. Therefore, we apply a Gaussian smoothing oper-
ation to the original image, and then down-sample the smoothed image to about
150 dpi (see Sect. 5.2 for the discussion of resolution selection). The down-
sampling operation has two advantages. First it can signiﬁcantly reduce the com-
putational cost by reducing the data amount. Second, the Gaussian smoothing will
suppress the noise in the original image, which can beneﬁt the following feature
extraction and matching steps. We denote by ID the down-sampled image and
Fig. 5.4a shows such an image.
Step 2: Determine the X-axis of the coordinate system
Refer to Fig. 5.4b, the bottom boundary of the ﬁnger can be easily extracted by a
Canny edge detector. Actually, this bottom boundary is nearly consistent to all FKP
images because all the ﬁngers are put ﬂatly on the basal block in data acquisition.
By ﬁtting this boundary as a straight line, the X-axis of the local coordinate system
is determined.
Step 3: Crop a sub-image IS from ID
Useful information which can be used for personal identiﬁcation only resides in a
portion of the whole FKP image. Therefore, we ﬁrst crop a sub-image IS from the
original image for the convenience of later processing. The left and right boundaries
of IS are two ﬁxed values evaluated empirically. The top and bottom boundaries are
90
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
X
0
100
200
300
400
500
0
100
200
300
400
500
convexity magnitude
x position in IS
'
0
X
x
=
X
Y
Fig. 5.4 Illustration for the ROI extraction process. (a) ID image which is obtained by a down-
sampling operation after a Gaussian smoothing; (b) X-axis of the coordinate system, which is the
line Y ¼ Y0, ﬁtted from the bottom boundary of the ﬁnger; (c) IS image extracted from ID; (d) IE
image obtained by applying a Canny edge detector on IS; (e) ICD image obtained by applying the
convex direction coding scheme to IE; (f) plot of conMag(x) for a typical FKP image; (g) line
X ¼ x
0
0, where x
0
0 ¼ arg
x
min conMag x
ð Þ
ð
Þ; (h) ROI coordinate system and the rectangle indicates
the area of the ROI sub-image that will be extracted
5.3
ROI (Region of Interest) Extraction
91

estimated according to the boundary of real ﬁngers. Figure 5.4c shows an example
IS image. This roughly cropped sub-image will be used to calculate the Y-axis so
that an accurate ROI image can be cropped. Modulated light projector projects a
continually changing light at the subject. Usually the light source simply cycles its
amplitude in a sinusoidal pattern. A camera detects the reﬂected light and the
changes in the brightness of each pixel of the image and the light phase to get the
distance of the projected light. The structure is shown in Fig. 5.4.
Step 4: Canny edge detection
By applying Canny edge detector to IS, the corresponding edge map IE can be
obtained. See Fig. 5.4(d) for an example.
Step 5: Convex direction coding for IE
Based on the local convexity characteristics of the edge map IE, we can code IE to
get the convex direction coding map ICD. At this step, each pixel on IE will be given
a code to represent the local convex direction of this pixel. The underlying principle
of this coding scheme is as follows. Based on the observation of FKP images, we
abstract an ideal model for “curves” on an FKP image as shown in Fig. 5.5a. In this
model, an FKP “curve” is either convex leftward or convex rightward. We code the
pixels on convex leftward curves as “1”, the pixels on convex rightward curves as
“1”, and the other pixels not on any curves as “0”. Figure 5.5b illustrates the
coding scheme. In our system, we regard the edges obtained in step 4 as “curves”
and this convex direction coding is performed on IE. The pseudo code of this
algorithm is as follows (Tables 5.1 and 5.2):
After convex direction coding, each ICD point is assigned a value 0, 1 or 1.
Figure 5.4e shows an ICD map in false color image format. White pixels on it are the
ones with convexity value “1”; black pixels are the ones with value “1”; and gray
pixels are of value “0”.
Step 6: Determine the Y-axis of the coordinate system
Consider the ideal FKP curve model set up at step 5. For an FKP image, “curves” on
the left part of phalangeal joint are mostly convex leftward and those on the right
part are mostly convex rightward. Meanwhile, “curves” in a small area around the
phalangeal joint do not have obvious convex directions. Based on this observation,
(a)
(b)
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
-1
Fig. 5.5 (a) Ideal model for FKP “curves”; (b) Convex direction coding scheme
92
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

Table 5.1 Algorithm Convex_Direction_Coding (IE)
Input: IE ( m
n
×
binary edge map computed in step 4)
Output: ICD ( m
n
×
convex direction code map)
Begin module
2
E
mid
height of I
y
=
;
for each 
( , )
EI
i j
do
if
( , )
EI
i j
= 0 // it is a background pixel
( , )
0
CD
I
i j =
;
else if
(
1,
1)
1
(
1,
1)
1
E
E
I
i
j
and I
i
j
+
−
=  
 
+
+
=
// it is a  bifurcation 
pixel
( , )
0
CD
I
i j =
;
else if (
(
1,
1)
1
)
(
(
1,
1)
1
)
E
mid
E
mid
I
i
j
and i
y
or I
i
j
and i
y
+
−
=  
 <=
 
 
+
+
=  
 >
( , )
1
CD
I
i j = ;
else if (
(
1,
1)
1
)
(
(
1,
1)
1
)
E
mid
E
mid
I
i
j
and i
y
or I
i
j
and i
y
+
+
=  
 <=
 
 
+
−
=  
 >
( , )
1
CD
I
i j = −;
end if
end for
End module
5.3
ROI (Region of Interest) Extraction
93

at a horizontal position x (x represents the column) of an FKP image, we deﬁne the
“convexity magnitude” as:
conMag x
ð Þ ¼ abs
X
W
ICD
 
!
ð5:1Þ
where W is a window that is symmetrical about the axis X ¼ x and W is of size dh
with h being the height of IS. d is experimentally chosen as 35 in this chapter. The
Table 5.2 Algorithm ImCompCode (IROI)
Input:  
ROI
I
( m
n
×
ROI sub-image)
Output: ImCompCode ( m
n
×
integer matrix)
Begin module
for each 
( , )
ROI
I
x y do
{
}
( , )*
( , ,
)
j
ROI
R
j
R
R
I
x y
G
x y θ
=
=
, where
/ 6,
{0,1,...5}
j
j
j
θ
π
=
=
, 
(
)
(
)
(
)
(
)
max( )
min( )
max
max( ) ,
min( )
abs
R
R
oriMag
abs
R
abs
R
−
=
if  oriMag
T
<
// this pixel does not have a dominant orientation
( , )
6
ImCompCode x y =
;
else 
{ }
( , )
argmin
j
j
ImCompCode x y
R
=
;
end if
end for
End module
94
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

“convexity magnitude” is proposed to measure how strong the dominant
convex direction is in a local area on the FKP image. The characteristic of the
FKP image suggests that conMag(x) will reach a minimum around the center of the
phalangeal joint and this position can be used to set the Y-axis of the coordinate
system. Let
x0
0 ¼ arg
x
min conMag x
ð Þ
ð
Þ
ð5:2Þ
Then X ¼ x0
0 can be set as the Y-axis. Figure 5.4f plots the curve conMag(x) of an
FKP image and Fig. 5.4g shows the vertical line X ¼ x0
0, which is the Y-axis of the
ROI system.
Step 7: Crop the ROI image
Now that we have ﬁxed the X-axis and Y-axis, the local coordinate system can then
be determined. Refer to Fig. 5.4h, with the constructed coordinate system, the ROI
sub-image IROI can be extracted from ID with a ﬁxed size, which is empirically set
as 160  80 in our system.
Figure 5.6 shows some examples of the extracted ROI images. We can see that
the proposed coordinate system construction and ROI extraction method can
effectively align the different FKP images and normalize the area for feature
extraction. Such operations reduce greatly the variations caused by the various
poses of the ﬁnger in data collection.
Fig. 5.6 Sample ROI images extracted by the proposed method. These four images are ROI
images for the sample images shown in Fig. 5.3, respectively
5.3
ROI (Region of Interest) Extraction
95

5.4
FKP Feature Extraction and Matching
The Gabor ﬁltering technique can simultaneously extract the spatial-frequency
information from the original signal (Gabor 1946). Since 1980s, it has been widely
used as an effective tool to fulﬁll the feature extraction job in face, iris, ﬁngerprint
and palmprint systems. In (Nanni and Lumini 2009a, b), Loris and Alessandra
described a Gabor feature selection technique. The Gabor ﬁlter can produce three
types of features—magnitude, phase, and orientation, which can be used separately
or jointly in different systems (Kong 2008). In this chapter, we propose a method
combining the orientation and magnitude features for FKP recognition. Experi-
mental results in Sect. 5 veriﬁes that the proposed scheme performs better in FKP
recognition than the other coding-based methods, such as BOCV (Guo et al. 2009),
CompCode (Kong and Zhang 2004), OrdinalCode (Sun et al. 2005) and RLOC (Jia
et al. 2008; Kumar and Zhou 2009a, b).
The Gabor function has several slightly different forms in the literature and here
we adopt the one proposed by Lee (Lee 1996):
G x; y; ω; θ
ð
Þ ¼
ωﬃﬃﬃﬃﬃ
2π
p
κ
e ω2
8κ2 4x02þy02
ð
Þ eiωx0  eκ2
2


ð5:3Þ
where x0 ¼ (x–x0)cosθ + (y–y0)sinθ, y0 ¼ (x–x0)sinθ + (y–y0)cosθ, (x0, y0) is
the center of the function, ω is the radial frequency in radians per unit length
and θ is the orientation of the Gabor functions in radians. κ is deﬁned by
κ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
2 ln 2
p
2δþ1
2δ1


, where δ is the half-amplitude bandwidth of the frequency
response. ω can be determined by ω ¼ κ/σ, where σ is the standard deviation of
the Gaussian envelop.
Using Gabor ﬁltering, next we propose an improved competitive coding
(ImCompCode) method to exploit the orientation information, and then we propose
a magnitude coding (MagCode) method to exploit magnitude information. Finally,
we fuse these two kinds of features in FKP matching.
5.4.1
Improved Competitive Coding (ImCompCode)
At each pixel IROI(x,y), we extract the orientation information and represent it as an
“orientation code”. This process is similar to the CompCode scheme in (Kong and
Zhang 2004).
With a bank of Gabor ﬁlters, the orientation feature at each pixel IROI(x,y) can be
extracted. In our system, we only use the real part of the Gabor ﬁlter to perform this
job. Mathematically, this orientation coding process can be represented as:
96
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

oriCode x; y
ð
Þ ¼ arg
j
min IROI x; y
ð
Þ∗GR

x; y; θj



ð5:4Þ
where symbol * represents the convolution operation, GR represents the real part of
the Gabor function G, θj ¼ jπ/J, j ¼ {0,. . ., J1}, and J represents the number of
different orientations.
Here we improve the original CompCode scheme. Often on an FKP image, there
are some pixels lying on relatively “plane” areas, i.e. these pixels do not reside on
any lines and consequently do not have a dominate orientation. Accordingly, the
J Gabor ﬁlter responses at such pixels do not have much variation. If we still assign
an orientation code to it, this code may not be stable and will be sensitive to noise,
making the feature representation and matching performance decreased. Therefore,
those “plane” pixels should be removed from orientation coding. We deﬁne the
“orientation magnitude” at a pixel as:
oriMag x; y
ð
Þ ¼
abs max R
ð Þ  min R
ð Þ
ð
Þ
max abs max R
ð Þ
ð
Þ; abs min R
ð Þ
ð
Þ
ð
Þ
ð5:5Þ
where R ¼ {Rj ¼ IROI(x, y) * GR(x, y, θj)}, j ¼ {0,. . ., J1} are the Gabor ﬁltering
responses at this pixel. The “orientation magnitude” oriMag(x, y) can measure how
likely the pixel (x, y) has a dominant orientation. If it is smaller than a threshold, we
reckon that this pixel has no dominant orientation and the corresponding compet-
itive code is assigned as J.
Based on our experimental results, using 6 Gabor ﬁlters of different orientations
is enough. This is in accordance with the conclusion made by Lee in (Lee 1996) that
the simple neural cells are sensitive to speciﬁc orientations with approximate
bandwidths of π/6. Thus, we choose 6 orientations, θj ¼ jπ/6, j ¼ {0,. . ., 5} for
the competition. The pseudo code for our ImCompCode scheme is summarized as
follows and Fig. 5.7a–d are ImCompCode maps for FKP ROI images shown in
Fig. 5.6.
Fig. 5.7 (a)–(d) and (e)–(h) are ImCompCode maps and MagCode maps for the FKP ROI images
shown in Fig. 5.6, respectively
5.4
FKP Feature Extraction and Matching
97

5.4.2
Magnitude Coding (MagCode)
Besides orientation information, we also want to exploit magnitude information
from Gabor ﬁlter responses. The magnitude of the Gabor ﬁlter response at IROI(x, y)
is:
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
IROI x; y
ð
Þ∗GR

x; y; ω; θj


2 þ IROI x; y
ð
Þ∗GI

x; y; ω; θj


2
q
ð5:6Þ
where GR and GI represent the real part and the imaginary part of the Gabor
function G respectively. However, in order to reduce the computational cost,
when generating the magnitude code map, we want to make use of the temporary
results generated from the “orientation coding” process. Thus, we still only use the
real part of the Gabor ﬁlter and deﬁne the magnitude at IROI(x, y) as:
mag x; y
ð
Þ ¼ max
j
abs IROI x; y
ð
Þ∗GR

x; y; θj





ð5:7Þ
Then a localized quantization is applied to mag(x, y) to get the magnitude code.
This process can be expressed as:
magCode x; y
ð
Þ ¼ ceil
mag x; y
ð
Þ  lmin
ð
Þ= lmax  lmin
N
	

	

ð5:8Þ
where N is the number of quantization levels, lmin ¼
min
x;y
ð
Þ2Wm mag x; y
ð
Þ
ð
Þ, and
lmax ¼
max
x;y
ð
Þ2Wm mag x; y
ð
Þ
ð
Þ. Wm is a w  w window centered at (x, y). The resulting
magnitude code is an integer within 1–N. w and N can be tuned by experiments on a
sub-dataset and they are experimentally set as 49 and 8 in this chapter, respectively.
Figure 5.7e–h show magnitude code maps for FKP ROI images presented in
Fig. 5.6.
5.4.3
FKP Matching
Suppose we are comparing two FKP ROI images, P and Q. Let Po and Qo be the two
orientation code maps; and let Pm and Qm be the two magnitude code maps. At ﬁrst,
we will calculate the matching distance between Po and Qo and the matching
distance between Pm and Qm respectively, and then fuse the two matching distances
together as the ﬁnal matching distance between P and Q.
When calculating the matching distance between Po and Qo, we adopt the
angular distance, which is deﬁned as:
98
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

angD P; Q
ð
Þ ¼
P
Rows
y¼1
P
Cols
x¼1
G Po x; y
ð
Þ; Qo

x; y



J=2
ð
Þ  S
ð5:9Þ
where S is the area of the code map, and
G Po x;y
ð
Þ;Qo

x;y



¼
1,Po x;y
ð
Þ¼6 and Qo x;y
ð
Þ6¼6
1,Po x;y
ð
Þ6¼6 and Qo x;y
ð
Þ¼6
0,Po x;y
ð
Þ¼Qo x;y
ð
Þ
min Po x;y
ð
ÞQo

x;y

; Qo

x;y

 Po x;y
ð
Þ6
ð
Þ


, if Po x;y
ð
Þ>Qo x;y
ð
Þ and Po x;y
ð
Þ6¼6
min Qo x;y
ð
ÞPo

x;y

; Po

x;y

 Qo x;y
ð
Þ6
ð
Þ


, if Po x;y
ð
Þ<Qo x;y
ð
Þ and Qo x;y
ð
Þ6¼6
8
>
>
>
>
<
>
>
>
>
:
ð5:10Þ
The matching distance between Pm and Qm is deﬁned as:
magD P; Q
ð
Þ ¼
P
Rows
y¼1
P
Cols
x¼1
abs Pm x; y
ð
Þ  Qm

x; y



N  1
ð
Þ  S
ð5:11Þ
Then, the ﬁnal matching distance between P and Q can be fused from angD and
magD as:
dist P; Q
ð
Þ ¼ 1  λ
ð
Þ  angD P; Q
ð
Þ þ λ  magD P; Q
ð
Þ
ð5:12Þ
where λ is used to control the contribution of magD to dist and it is experimentally
set as 0.15 in our system.
Taking into account the possible translations in the extracted ROI sub-image
(with respect to the one extracted in the enrolment), multiple matches are performed
by translating one set of features in horizontal and vertical directions. And in such
case, S is the area of the overlapping parts of the two code maps. The minimum of
the resulting matching distances is considered to be the ﬁnal distance. The ranges of
the horizontal translation and the vertical translation are empirically set as 8 to
8 and 4 to 4 in this chapter, respectively.
5.5
Experimental Results
5.5.1
Database Establishment
In order to evaluate the proposed FKP-based personal authentication system,
a database was established using the developed FKP image acquisition device
5.5
Experimental Results
99

(refer to Figs. 5.1 and 5.2). FKP images were collected from 165 volunteers,
including 125 males and 40 females. Among them, 143 subjects were 20–30 years
old and the others were 30–50 years old. The database will be available in the
website of Biometrics Research Center, The Hong Kong Polytechnic University.
We collected samples in two separate sessions. In each session, the subject was
asked to provide 6 images for each of the left index ﬁnger, the left middle ﬁnger, the
right index ﬁnger and the right middle ﬁnger. Therefore, 48 images from 4 ﬁngers
were collected from each subject. In total, the database contains 7920 images from
660 different ﬁngers. The average time interval between the ﬁrst and the second
sessions was about 25 days. The maximum and minimum time intervals were
96 days and 14 days respectively. In all of the following experiments, we took
images collected at the ﬁrst session as the gallery set and images collected at the
second session as the probe set. To obtain statistical results, each test image in the
probe set was matched with all the training images in the gallery set. If the test
image and the training image were from the same ﬁnger, the matching between
them was counted as a genuine matching; otherwise it was counted as an imposter
matching.
5.5.2
Selection of the Image Resolution
The resolution of original FKP images acquired in our system is about 400 dpi,
which may not be optimal in terms of the accuracy and efﬁciency of FKP veriﬁca-
tion. In fact, many factors, such as the storage space, the computational cost, the
employed feature extraction and matching method, and the recognition accuracy,
should be considered in selecting a suitable resolution of the FKP images for a more
efﬁcient biometric system. To this end, we conducted a series of experiments to
select the “optimal” resolution and set the selection criterion as: the minimum
resolution with which a satisfying veriﬁcation performance could be obtained. The
experiments were performed on a sub-dataset of the whole FKP database. In this
sub-dataset, there were 120 classes, including 1440 images. With respect to the
feature extraction method, the CompCode was used (Kong and Zhang 2004). We
smoothed the original images by using a Gaussian kernel and then down-sampled
the images to ﬁve lower resolutions: 200 dpi, 170 dpi, 150 dpi, 120 dpi and 100 dpi.
The experimental results are summarized in Table 5.3.
Table 5.3 EERs obtained
under different resolutions
Resolution (dpi)
EER (%)
200
1.73
170
1.41
150
1.36
120
1.71
100
1.92
100
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

Based on the results listed in Table 5.3, it can be seen that 150 dpi is a good
choice. It leads to the lowest EER, while the resolution is much smaller than the
original one (400 dpi). This will reduce the computational cost and speed up the
feature extraction and matching processes signiﬁcantly. Therefore, in all of the
following experiments, we used the FKP images with a resolution 150 dpi.
5.5.3
FKP Veriﬁcation
Veriﬁcation aims to answer the question of “whether the person is the one he/she
claims to be”. In order to show and explain the performance of the proposed system
clearly, 3 experiments were conducted. In each experiment, we evaluated and
compared the performance of ﬁve coding based feature extraction methods:
CompCode (Kong and Zhang 2004), OrdinalCode (Sun et al. 2005), RLOC (Jia
et al. 2008), BOCV (Guo et al. 2009) and the proposed ImCompCode&MagCode.
In OrdinalCode, differences between Gaussians from two orthogonal directions
were used to extract the orientation ﬁelds. The scales of the 2D Gaussian function
along two orthogonal directions in OrdinalCode were set as 1.7 and 4.2 in our
implementation. The RLOC method is based on the modiﬁed ﬁnite Radon trans-
form. It was originally proposed for palmprint recognition (Jia et al. 2008), and was
later adopted for feature extraction of the ﬁnger-back -surface images (Kumar and
Zhou 2009a, b). In our implementation of RLOC, the “line width” was set as 2 and
the kernel size was 16  16. BOCV (Guo et al. 2009) is a recently proposed
method for palmprint recognition which tends to represent multiple orientations
for a local region. The threshold for the binarization used in BOCV was set as 0 in
this chapter. Gabor ﬁlters used in CompCode, BOCV and the proposed
ImCompCode&MagCode were all of the form (5.3), and the parameters were set
as: δ ¼ 3.3 and σ ¼ 4.
Experiment 1
In the ﬁrst experiment, all classes of FKPs were involved. Each image in the probe
set was matched against all the images in the gallery set. Therefore, in this
experiment there were 660 (165  4) classes and 3960 (660  6) images in the
gallery set and the probe set each. The numbers of genuine matchings and imposter
matchings are 23,760 and 7,828,920, respectively. By adjusting the matching
threshold, a ROC (Receiver Operating Characteristic) curve, which is a plot of
Genuine Accept Rate (GAR) against False Accept Rate (FAR) for all possible
thresholds, can be created. The ROC curve can reﬂect the overall performance of a
biometric system. Figure 5.8a shows ROC curves generated by the ﬁve different
FKP recognition schemes and Table 5.4 lists the corresponding EERs, from which
we can see that the proposed ImCompCode&MagCode scheme performs the best
among the ﬁve methods evaluated in terms of EER. Distance distributions of
genuine matchings and imposter matchings obtained by the proposed scheme are
plotted in Fig. 5.8b.
5.5
Experimental Results
101

Experiment 2
As mentioned in Sect. 5.1, our database contains FKPs from four types of ﬁngers,
left index ﬁngers, left middle ﬁngers, right index ﬁngers and right middle ﬁngers.
The aim of this experiment is to evaluate the performance of the proposed
FKP-based personal authentication system on each type of ﬁngers separately. For
(a)
(b)
10-6
10-4
10-2
100
102
75
80
85
90
95
100
False Acceptance Rate(%)
Genuine Acceptance Rate(%)
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&MagCode
0
0.1
0.2
0.3
0.4
0.5
0
0.5
1
1.5
2
2.5
3
3.5
4
Matching Distance
Percentage(%)
Genuine
Imposter
Fig. 5.8 (a) ROC curves
obtained by the ﬁve
recognition methods in
experiment 1. (b) Distance
distributions of genuine
matchings and imposter
matchings with the
proposed scheme in
experiment 1
Table 5.4 EERs obtained by
different methods in
experiment 1
Method
EER (%)
CompCode
1.72
OrdinalCode
3.83
RLOC
1.93
BOCV
1.82
ImCompCode&MagCode
1.48
102
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

each type of FKPs, the gallery and the probe each contains 165 classes and
990 (165  6) sample images, and the numbers of genuine matchings and imposter
matchings are 5940 and 487,080 respectively. Similarly, ﬁve FKP recognition
schemes were evaluated. ROC curves for different ﬁnger types and by different
recognition schemes are shown in Fig. 5.9. Experimental results in terms of EER
are summarized in Table 5.5 for comparison.
The experimental results indicate that in general the right middle and index
ﬁngers perform better than their left counterparts in terms of EER. This is probably
because that the majority of people who provided FKP samples in our database are
right-handed. They would feel more convenience to use our image acquisition
device with their right hand than with the left one, which consequently leads to a
result that the variations of ﬁnger poses of right hand ﬁngers are less severe than left
hand ﬁngers. Remarkable variations of ﬁnger poses would cause severe afﬁne
(a)
(b)
(c)
(d)
10
-4
10
-2
10
0
10
2
84
86
88
90
92
94
96
98
100
False Acceptance Rate(%)
Genuine Acceptance Rate(%)
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&MagCode
10
-4
10
-2
10
0
10
2
75
80
85
90
95
100
False Acceptance Rate(%)
Genuine Acceptance Rate(%)
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&MagCode
10
-4
10
-2
10
0
10
2
80
85
90
95
100
False Acceptance Rate(%)
Genuine Acceptance Rate(%)
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&MagCode
10
-4
10
-2
10
0
10
2
80
85
90
95
100
False Acceptance Rate(%)
Genuine Acceptance Rate(%)
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&MagCode
Fig. 5.9 ROC curves for FKPs from (a) left index ﬁngers, (b) left middle ﬁngers, (c) right index
ﬁngers, and (d) right middle ﬁngers
Table 5.5 EERs (%) by different schemes in experiment 2
Finger type
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&Magcode
Left index
2.06
2.69
2.20
2.45
1.73
Left middle
1.96
4.07
2.27
2.42
1.78
Right index
1.82
3.66
2.07
2.43
1.44
Right middle
1.87
4.15
2.32
2.30
1.64
5.5
Experimental Results
103

transforms and deformations between two FKP images of the same ﬁnger, which
lead to more challenge to FKP recognition.
Experiment 3
The goal of this experiment is to investigate the system’s performance when we
fuse information from 2 or more ﬁngers of a person. In fact, in such a case the
system works as a kind of multi-modal system with a single biometric trait but
multiple units (Ross and Jain 2004). Suppose that we want to fuse information from
the right index FKP and the right middle FKP. Each template in the enrolment
database will be composed by a right index FKP tri and a right middle FKP trm.
When matching, a client’s right index FKP cri and right middle FKP crm will be
matched to tri and trm respectively to get two matching distances, dri and drm. Then
dri and drm will be fused according to some fusion rules to obtain the ﬁnal matching
distance, by which the client’s identity can be identiﬁed.
With respect to fusion rules, in this chapter we simply examined the SUM rule
and the MIN rule as they are easily to be implemented and can well reﬂect the
system’s performance. The SUM rule is deﬁned as:
dsum ¼
X
di
ð5:13Þ
and the MIN rule is deﬁned as:
dmin ¼ min di
ð Þ
ð5:14Þ
where di is the matching distance of the client’s ith ﬁnger.
We tested several different fusion schemes of ﬁngers with the two fusion rules.
Results are presented in Table 5.6, from which it can be easily observed that by
integrating information from more ﬁngers the recognition performance of the
system could be largely improved. We can also ﬁnd that the SUM rule works better
than the MIN rule in our system.
5.5.4
Speed
The FKP recognition software is implemented using Visual C#.Net 2005 on a Dell
Inspiron 530s PC embedded Intel E6550 processor and 2GB of RAM. The execu-
tion time for data preprocessing and ROI extraction is 198 ms. The time for
ImCompCode&MagCode-based feature extraction and matching is 75 ms and
1.2 ms, respectively. Thus, the total execution time for one veriﬁcation operation
is less than 0.5 s in our prototype system, which is fast enough for real-time
applications. We believe that with the optimization of the implementation, the
system’s efﬁciency could be much further improved.
104
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

5.5.5
Discussion
FKP is a new member in the biometrics family compared with other biometric
identiﬁers. As mentioned in the section of Introduction, Woodard and Flynn
(Woodard and Flynn 2005a, b) did some salient work to validate the uniqueness
of features from ﬁnger-back surfaces by using 3D imaging, and Kumar (Ravikanth
and Kumar 2007; Kumar and Ravikanth 2009) integrated 2D ﬁnger knuckle surface
information with the ﬁnger shape information in their system. However, such a
system is doomed to have a relatively large size because they need to collect the
image of the whole hand. Complex preprocessing steps are also needed to extract
the ﬁnger knuckle area which only occupies a small portion of the whole acquired
image. In our system, we make use of a triangular block to control the ﬁnger
freedom. This gadget does not sacriﬁce the user convenience and it is easy to use.
Such a design brings the following merits: (1) the acquisition device could be easily
made to a small size; (2) image around the ﬁnger knuckle area is captured directly,
which largely simpliﬁes the following data preprocessing steps; and (3) since the
ﬁnger knuckle is slightly bent when being captured, the distinctive FKP texture
patterns can be clearly imaged, which makes the proposed FKP system have high
accuracy.
Advantages of the proposed system could be reﬂected by experimental results on
FKP veriﬁcation. For comparison, experimental results in (Woodard and Flynn
2005a, b) and (Kumar and Ravikanth 2009) are extracted from the original chapters
and listed in Table 5.7 and partial experimental results by our system are also
presented. The scale of the dataset used in our experiment is much larger than the
ones mentioned in (Woodard and Flynn 2005a, b) and (Kumar and Ravikanth
2009). Woodard’s result (Woodard and Flynn 2005a, b) and Kumar’s result
Table 5.6 EERs (%) obtained in experiment 3
Fingers in
fusion
CompCode
OrdinalCode
RLOC
BOCV
ImCompCode&
MagCode
S-
rule
M-
rule
S-
rule
M-
rule
S-
rule
M-
rule
S-
rule
M-
rule
S-rule
M-rule
Left index
Left middle
0.33
0.67
0.64
0.72
0.26
0.68
0.41
0.45
0.20
0.63
Right index
Right
middle
0.32
0.39
0.71
0.79
0.34
0.37
0.36
0.47
0.26
0.36
Left index
Right index
0.36
0.67
0.84
0.76
0.42
0.91
0.63
0.63
0.26
0.64
Left middle
Right
middle
0.29
0.33
0.90
0.87
0.33
0.30
0.43
0.42
0.27
0.30
All the four
0
0.03
0.02
0.05
0
0.16
0.01
0.09
0
0.02
Bold value represents best results
5.5
Experimental Results
105

(Kumar and Ravikanth 2009) were obtained by fusing information from three and
four ﬁngers, respectively. It can be clearly seen that the proposed system performs
much better even if we incorporate information from fewer ﬁngers. Particularly, if
four ﬁngers are used, our system could achieve an EER of 0.
It should be noted that although we use a triangular block to control the ﬁnger
freedom in FKP image acquisition, there are still variations for the same ﬁngers at
different collection sessions. Sometimes such variations can result in severe afﬁne
transforms or even non-elastic deformations among intra-class FKPs. And as a
result, feature maps of such FKPs can have large matching distances. Figure 5.10
shows a typical example. The two FKP images in Fig. 5.10 are from the same ﬁnger
but are recognized as different classes in our system. Hence, how to reduce the
effects of afﬁne transforms and deformations will be a direction of our future work.
Fig. 5.10 (a) and (b) are two intra-class FKP images captured by our system. (c) and (d) are their
ROI sub-images. There is an obvious pose variation between the two FKPs and they are recognized
as different classes in our system
Table 5.7 Comparison of experimental results
Method
Gallery
classes
Gallery
samples
Probe
classes
Probe
samples
Finger types for
fusion
EER
(%)
Woodard and
Flynn (2005a, b)
132
660
177
531
r-index, r-middle,
r-ring
5.5
Kumar and
Ravikanth (2009)
105
420
105
210
index, middle, ring,
little
1.39
Ours
165
990
165
990
r-index, r-middle
0.26
Ours
165
990
165
990
r-index, r-middle,
l-index, l-middle
0
106
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

There are two main approaches currently employed in To F technology. The ﬁrst
one utilizes modulated, incoherent light, and is based on a phase measurement. The
second one is based on an optical shutter technology. The main details about the
technology will be showed in the chapters that introduce the 3D face veriﬁcation
system.
5.6
Summary
This chapter presents a new approach to online personal authentication using
ﬁnger-knuckle-print (FKP), which has distinctive line features. A cost-effective
FKP system, including a novel image acquisition device and the associated data
processing algorithms, is developed. A region of interest (ROI) extraction algo-
rithm is proposed to align and extract the FKP sub-image for feature extraction. For
efﬁcient FKP matching, a feature extraction scheme is proposed to exploit both
orientation and magnitude information extracted by Gabor ﬁlters. To evaluate the
performance of the proposed system, an FKP database is established, consisting of
7920 images from 660 different ﬁngers. Extensive experiments are conducted and
promising results demonstrate the efﬁciency and effectiveness of the proposed
technique. Compared with other existing ﬁnger back surface based systems, the
proposed one has merits of high accuracy, high speed, small size and cost-effective.
It has a great potential to be future improved and employed in real commercial
applications. In the future, we will focus on how to deal with afﬁne or even
non-elastic deformations between FKP images from the same ﬁnger to further
improve the system’s performance.
References
Borgen H, Bours P, Wolthusen S (2008) Visible-spectrum biometric retina recognition. In: Pro-
ceedings of the international conference on intelligent information hiding and multimedia
signal processing, pp 1056–1062
Burge M, Burger W (1999) Ear. In: Jain AK, Bolle R, Pankanti S (eds) Biometrics: personal
identiﬁcation in networked society. Kluwer Academic, Boston, pp 273–286
Daugman J (1993) High conﬁdence visual recognition of persons by a test of statistical indepen-
dence. IEEE Trans Pattern Anal Mach Intell 15(11):1148–1161
Daugman J (2004) How iris recognition works. IEEE Trans Circuits Syst Video Technol 14
(1):21–30
Delac K, Grgic M (2007) Face recognition. I-Tech Education and Publishing, Vienna
E-channel (2004) E-channel system of the Hong Kong government. http://www.immd.gov.hk/
ehtml/20041216.htm
Gabor D (1946) Theory of communication. J Inst Electr Eng 93:429–457
Guo Z, Zhang D, Zhang L, Zuo W (2009) Palmprint veriﬁcation using binary orientation
co-occurrence vector. Pattern Recogn Lett 30(13):1219–1227
References
107

Hill R (1999) Retinal identiﬁcation. In: Jain A, Bolle R, Pankati S (eds) Biometrics: personal
identiﬁcation in networked society. Springer, Berlin
Hollien H (2002) Forensic voice identiﬁcation. Academic Press, London
Huang D, Jia W, Zhang D (2008) Palmprint veriﬁcation based on principal lines. Pattern Recogn
41(4):1316–1328
Jain A, Duta N (1999) Deformable matching of hand shapes for veriﬁcation. Proc ICIP
1999:857–861
Jain A, Ross A, Pankanti S (1999) A prototype hand geometry-based veriﬁcation system. In:
Proceedings of the 2nd international conference on audio- and video-based biometric person
authentication, pp 166–171
Jain A, Flynn P, Ross A (2007) Handbook of biometrics. Springer, New York
Jia W, Huang D, Zhang D (2008) Palmprint veriﬁcation based on robust line orientation code.
Pattern Recogn 41(5):1504–1513
Kong A (2008) An evaluation of Gabor orientation as a feature for face recognition. Proc ICPR
2008:1–4
Kong A, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. In: Proceedings
of the ICPR’04, pp 520–523
Kong A, Zhang D, Kamel M (2006) Palmprint identiﬁcation using feature-level fusion. Pattern
Recogn 39(3):478–487
Kumar A, Prathyusha K (2008) Personal authentication using hand vein triangulation. Proc SPIE
Biom Technol Hum Identif 6944:13
Kumar A, Ravikanth C (2009) Personal authentication using ﬁnger knuckle surface. IEEE Trans
Inf Forensics Secur 4(1):98–109
Kumar A, Zhou Y (2009a) Human identiﬁcation using knucklecodes. Proc BTAS 2009:1–6
Kumar A, Zhou Y (2009b) Personal identiﬁcation using ﬁnger knuckle orientation features.
Electron Lett 45(20):1023–1025
Lee T (1996) Image representation using 2D Gabor wavelet. IEEE Trans Pattern Anal Mach Intell
18(10):957–971
Li Q, Qiu Z, Sun D, Wu J (2004) Personal identiﬁcation using knuckleprint. In: Proceedings of
Sino Biometrics, pp 680–689
Liu C, Herbst N, Anthony N (1979) Automatic signature veriﬁcation: system description and ﬁeld
test results. IEEE Trans Syst Man Cybern 9(1):35–38
Maltoni D, Maio D, Jain A, Prabhakar S (2003) Handbook of ﬁngerprint recognition. Springer,
New York
Nanni L, Lumini A (2009a) A multi-matcher system based on knuckle-based features. Neural
Comput Appl 18(1):87–91
Nanni L, Lumini A (2009b) On selecting Gabor features for biometric authentication. Int J Comput
Appl Technol 35(1):23–28
Nixon M, Tan T, Chellappa R (2006) Human identiﬁcation based on Gait. Springer, New York
Plamondona R, Loretteb G (1989) Automatic signature veriﬁcation and writer identiﬁcation- the
state of the art. Pattern Recogn 22(2):107–131
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, New York
Ravikanth C, Kumar A (2007) Biometric authentication using ﬁnger-back surface. Proc CVPR
2007:1–6
Ross A, Jain A (2004) Multimodal biometrics: an overview. In: Proceedings of the 12th European
Signal Processing Conference, pp 1221–1224
Sanchez-Reillo R, Sanchez-Avila C, Gonzalez-Marcos A (2000) Biometric identiﬁcation through
hand geometry measurements. IEEE Trans Pattern Anal Mach Intell 22(10):1168–1171
Sun Z, Tan T, Wang Y, Li S (2005) Ordinal palmprint representation for personal identiﬁcation.
Proc CVPR 2005:279–284
Wang J, Yau W, Suwandy A, Sung E (2008) Personal recognition by fusing palmprint and palm
vein images based on “Lapacianpalm” representation. Pattern Recogn 41(5):1531–1544
108
5
Finger-Knuckle-Print Veriﬁcation
www.ebook3000.com

Wechsler H (2006) Reliable face recognition methods – system design, implementation and
evaluation. Springer, New York
Woodard D, Flynn P (2005a) Finger surface as a biometric identiﬁer. Comput Vis Image Underst
100(3):357–384
Woodard D, Flynn P (2005b) Personal identiﬁcation utilizing ﬁnger surface features. Proc CVPR
2:1030–1036
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
References
109

Chapter 6
Local Features for Finger-Knuckle-Print
Recognition
Abstract Researchers have recently found that the ﬁnger-knuckle-print (FKP),
which refers to the inherent skin patterns of the outer surface around the phalangeal
joint of one’s ﬁnger, has high discriminability, making it an emerging promising
biometric identiﬁer. Effective feature extraction and matching plays a key role in
such an FKP based personal authentication system. This chapter studies image local
features induced by the phase congruency model, which is supported by strong
psychophysical and neurophysiological evidences, for FKP recognition. In the
computation of phase congruency, the local orientation and the local phase can
also be deﬁned and extracted from a local image patch. These three local features
are independent of each other and reﬂect different aspects of the image local
information. We compute efﬁciently the three local features under the computation
framework of phase congruency using a set of quadrature pair ﬁlters. We then
propose to integrate these three local features by score-level fusion to improve the
FKP recognition accuracy. Such kinds of local features can also be naturally
combined with Fourier transform coefﬁcients, which are global features. Experi-
ments are performed on the PolyU FKP database to validate the proposed FKP
recognition scheme.
Keywords Biometrics • Finger-knuckle-print recognition • Phase congruency
6.1
Introduction
The need for reliable automated user authentication techniques has been signiﬁ-
cantly increased in the wake of heightened concerns about security (Jain et al.
2007). Biometrics based methods, which use unique physical or behavioral char-
acteristics of human beings, are drawing increasing attention in both academic
research and industrial applications because of their high accuracy and robust-ness
in the modern e-world. In the past decades, researchers have exhaustively investi-
gated a number of different biometric identiﬁers, including ﬁngerprint, face, iris,
palm print, hand geometry, voice, and gait, etc. (Li 2009).
Among various kinds of biometric identiﬁers, hand-based biometrics attracts
much interest because of their high user acceptance and convenience. Some
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_6
111
www.ebook3000.com

commonly used hand-based biometrics, e.g., ﬁngerprint (Hong et al. 1998; Ross et al.
2003; Maltoni et al. 2003; Zhao et al. 2010), palm print (Han et al. 2003; Hu et al.
2007; Zhang et al. 2003, 2012; Kong and Zhang 2004; Kong et al. 2006, Kong et al.
2009; Jia et al. 2008; Struc and Pavesic 2009; Jain and Feng 2009), hand geometry
(Feng et al. 2011; Duta 2009), and hand vein (Wang et al. 2008; Wilson 2010), have
been well investigated in the literature. Recently, scholars have reported that ﬁnger-
knuckle-print (FKP), the image pattern of skin folds and creases in the outer ﬁnger
knuckle surface, is highly unique and can serve as a distinctive biometric identiﬁer
(Zhang et al. 2009a, b, 2010, 2011; Morales et al. 2011; Woodard and Flynn 2005;
Kumar and Ravikanth 2009; Meraoumia et al. 2011). Compared with ﬁngerprint,
FKP is hard to be abraded since people hold stuffs with the inner side of the hand. In
addition, unlike ﬁngerprint, there is no stigma of criminal investigation associated
with the ﬁnger knuckle surface, so FKP can have a higher user acceptance rate
(Kumar and Ravikanth 2009). Moreover, people rarely leave FKP remains on the
stuff surface, making the loss of private data less possible. Thus, FKP has a great
potential to turn into a widely accepted biometric identiﬁer.
A novel online FKP-based personal authentication system has been established
in our previous works (Zhang et al. 2010), which comprises four major compo-
nents: FKP image acquisition, ROI (region of interest) extraction, feature extrac-
tion, and feature matching. In our design, the ﬁnger knuckle will be slightly bent
when being imaged, and hence the inherent skin patterns can be clearly captured.
As in many pattern classiﬁcation tasks, feature extraction and matching plays a
key role in FKP-based personal authentication system. In (Zhang et al. 2009a, b),
Zhang et al. used the Gabor ﬁlter based competitive coding scheme, which was
originally designed for palmprint recognition (Kong and Zhang 2004), to extract the
local orientation information as FKP features. In (Zhang et al. 2010), Zhang et al.
combined the orientation information and the magnitude information extracted by
Gabor ﬁlters. In (Zhang et al. 2009a, b), the Fourier transform coefﬁcients of the
image were taken as the feature and the band-limited phase-only correlation
technique was employed to calculate the similarity between two FKP images. In
the local-global information combination (LGIC) feature extraction scheme (Zhang
et al. 2011), the local orientation extracted by the Gabor ﬁlters is taken as the local
feature while the Fourier coefﬁcients are taken as a global feature. In (Morales et al.
2011), Morales et al. used a real Gabor ﬁlter to enhance the FKP image and then
used the scale invariant feature transform (SIFT) to extract features; they called the
proposed method as OE-SIFT (orientation enhanced-SIFT).
In our previous methods (Zhang et al. 2009a, b, 2010), real Gabor ﬁlters were
used to extract the local orientation information; such an idea was inspired by the
method “competitive coding” (Kong and Zhang 2004) proposed for palmprint
recognition. Local orientation feature of biometric images can also be deﬁned and
extracted using other different mathematical models. For example, in (Jia et al.
2008), Jia et al. proposed a coding method to extract the local orientation of
palmprints, namely robust line orientation code (RLOC), which is based on a
modiﬁed ﬁnite Radon transform. In addition to the local orientation, the local
phase is also widely used in the biometrics community and it is usually extracted
by using band-pass complex ﬁlters, e.g., Gabor ﬁlters (Gabor 1946; Daugman 1985)
112
6
Local Features for Finger-Knuckle-Print Recognition

and log-Gabor ﬁlters (Field 1987). By making use of the local phase feature
extracted by Gabor ﬁlters, Daugman invented the famous IrisCode (Daugman
1993); inspired by Daugman’s work, Zhang et al. adopted a similar idea to match
palmprint images (Zhang et al. 2003). Actually, according to (Venkatesh and
Owens 1990; Sierra-Va´zquez and Serrano-Pedraza 2010), the local phase reﬂects
the type of local features. However, it is not clear whether such a feature is
signiﬁcant and stable. To address such an issue, we need to know whether the
local phases over scales are consistent. The phase congruency (PC) model
(Morrone et al. 1986; Morrone and Burr 1988; Kovesi 1999; Henriksson et al.
2009) serves as a solution to this issue. Studies of psychophysics and neurophys-
iology have revealed that visually discernable image features coincide with those
points where Fourier waves at different frequencies have congruent phases. PC has
been exploited as features by some biometrics researchers for face recognition
(Gundimada and Asari 2007), iris recognition (Yuan and Shi 2005), and palmprint
recognition (Struc and Pavesic 2009), and it has also been used in some object
recognition applications (Verikas et al. 2012).
In fact, local orientation, local phase, and local phase congruency reﬂect differ-
ent aspects of information embedded in a local image patch. Moreover, they are
independent of each other and none of them can be covered by the others. They can
provide complementary discriminating power to each other for matching biometric
images. Thus, better recognition performance could be expected by combining
these three local features together in some way. However, to the best of our
knowledge, in the biometrics community there is no work reported to deﬁne and
analyze systematically these three local features in a uniﬁed framework, and there is
no attempt trying to integrate these three features to improve the performance of
biometrics systems, either. Based on these considerations, in this chapter, we ﬁrst
deﬁne these three local features under a uniﬁed framework, and then present an
efﬁcient method to compute them using the computation framework of PC. Finally,
we integrate these three local features together for FKP recognition. Experimental
results demonstrate that the integration of the three local features perform s better
than using any of them separately. Moreover, we report the system’s performance
when integrating the three local features with one global feature, the Fourier
transform coefﬁcients, which leads to the best result on our benchmark FKP
database. This work differs from our previous works (Zhang et al. 2010, 2011)
mainly in three aspects. At ﬁrst, besides the local orientation, the local phase and the
local phase congruency are investigated. Secondly, the three local features are
deﬁned, analyzed, and extracted in a uniﬁed framework. And thirdly, we propose
to integrate the three local features together to improve the accuracy of FKP
recognition.
The remainder of this chapter is organized as follows. Section 6.2 deﬁnes and
analyzes the three local features, while Sect. 6.3 presents the extraction and
matching scheme for each local feature. Section 6.4 reports the experimental results
and discussions. Finally, Sect. 6.5 gives the summary.
6.1
Introduction
113
www.ebook3000.com

6.2
Analysis of Local Features
As stated in Sect. 6.1, in literature the three local features (the local orientation, the
local phase, and the phase congruency) are extracted by using different mathemat-
ical models and their relationships are not systematically investigated. In this
section, we will examine these three local features in detail under a uniﬁed
framework.
To ease the following discussions, we ﬁrst introduce the concept of intrinsic
dimension here. The intrinsic dimension is the number of degrees of freedom
necessary to describe a local image structure (Krieger and Zetzsche 1996). A 2D
image patch I can be classiﬁed as a local region, denoted by R, of a speciﬁc intrinsic
dimension. For example, constant areas are of intrinsic dimension zero (i0D) while
straight lines and edges are of intrinsic dimension one (i1D). Mathematically, such
a classiﬁcation can be expressed as (Krieger and Zetzsche 1996)
I 2
i0DR, I xi
ð Þ ¼ I xj
 
, 8xi, xj 2 R
i1DR, I x; y
ð
Þ ¼ g x cos θ þ y sin θ
ð
Þ, 8 x; y
ð
Þ 2 R, I =2 i0DR
i2DR, else
8
<
:
ð6:1Þ
where g is a 1D real-valued function. Examples of i0D, i1D, and i2D signals are
shown in Fig. 6.1.
A point x in an image can be characterized by its “local features”, which are
derived from a local patch centered on it. Before we deﬁne local features we need to
have a model for the signal to be analyzed. In our case, we are dealing with 2D FKP
images, which are actually a special kind of 2D images in that they are abundant of
line-like features. And these line-like features play dominant roles in distinguishing
different individuals. Thus, we assume that FKP images are locally i1D (intrinsic
one dimensional) signals.
Let us consider the one dimensional (1D) real signal ﬁrst. In order to analyze the
local structure of the 1D real signal, analytic signal was proposed in the literature
(Gabor 1946) and it has been corroborated to be quite effective (Granlund and
Knutsson 1995). Analytic representation makes certain attributes of a real signal
more accessible and facilitates the derivation of modulation and demodulation
Fig. 6.1 Signals with different intrinsic dimensions: (a) i0D; (b) i1D; (c) i2D; (d) i2D
114
6
Local Features for Finger-Knuckle-Print Recognition

techniques. Given a 1D real signal f(x), the corresponding analytic signal is deﬁned
as (Gabor 1946; Granlund and Knutsson 1995)
f A x
ð Þ ¼ f x
ð Þ þ if H x
ð Þ
ð6:2Þ
Where fH(x) ¼ f(x)*h(x) , i2 ¼  1 , * denotes the convolution operation, and
h(x) ¼ 1/πx refers to the Hilbert transform kernel in the spatial domain. With such
a complex representation, the local amplitude and the local phase of the 1D analytic
signal are deﬁned as (Gabor 1946; Granlund and Knutsson 1995)
a x
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
f 2 x
ð Þ þ f 2
H x
ð Þ
q
, ϕ x
ð Þ ¼ arctan 2 f H x
ð Þ; f x
ð Þ
ð
Þ
ð6:3Þ
The local amplitude indicates the energetic information of the signal, while the
local phase can be used to distinguish between different local structures and it is
independent of the local amplitude (Venkatesh and Owens 1990). In practice, since
the Hilbert transform operator is an improper integral and difﬁcult to calculate,
researchers usually use a pair of spatial ﬁlters forming a quadrature pair to construct
the analytic signal (Venkatesh and Owens 1990; Morrone and Owens 1987). To this
end, complex Gabor (Gabor 1946; Daugman 1985) or log-Gabor (Field 1987) ﬁlters
are widely used. When the 1D signal is embedded into the 2D space, its orientation
should be considered. Thus, the local amplitude, the local phase, and the local
orientation are three independent measures to characterize a 2D image point.
The local phase reﬂects the type of local structures (Venkatesh and Owens
1990). However, we do not know to what degree it is a signiﬁcant feature. To
address such an issue, we make use of the phase congruency (PC) (Morrone et al.
1986; Morrone and Burr 1988; Kovesi 1999), a dimensionless quantity, to measure
the consistency of the local phases over scales. Based on the physiological and
psychophysical evidence, it is found that visually discernable features coincide with
those points having maximal phase congruency. Such a conclusion has been further
corroborated by some recent studies in neurobiology using functional magnetic
resonance imaging (fMRI) (Henriksson et al. 2009). Phase congruency has an
intriguing property that it is almost invariant to changes in image brightness or
contrast.
Thus, within the local window surrounding an image point x, four features-the
local amplitude, the local phase, the local orientation and the phase congruency-can
be extracted and they reﬂect different aspects of information contained in the local
window. However, we will not use the local amplitude for recognition because it is
not contrast invariant. Hence, the local phase, the local orientation, and the phase
congruency will be used as three local features in this article.
For a real 2D image, these three local features can be deﬁned and extracted using
a set of 2D quadrature ﬁlter pairs, such as 2D complex Gabor or log-Gabor ﬁlters.
Suppose that complex Gabor ﬁlters are adopted, which are deﬁned as
6.2
Analysis of Local Features
115
www.ebook3000.com

G x; y
ð
Þ ¼ exp 1
2
x
02
σ2
x
þ y
02
σ2
y
 
!
 
!
exp i 2π
λ x
0


ð6:4Þ
where x0 ¼ x cos θ + y sin θ, y0 ¼  x sin θ + y cos θ. In Eq. (6.4), λ represents the
wavelength of the sinusoid factor, θ represents the orientation of the normal to
the parallel stripes of the Gabor function, σx and σy are the standard deviations of
the 2D Gaussian envelop. It can be seen from the deﬁnition that a Gabor ﬁlter is
actually a Gaussian envelop modulated by a sinusoidal plane wave. The Gaussian
envelop ensures that the convolution is dominated by the image patch near the
center of the ﬁlter. Therefore, the Gabor ﬁlter is a local operator and can extract
information at a speciﬁc scale and a speciﬁc orientation within a local region.
To deﬁne and extract the local orientation, we make use of the competitive
coding scheme which has been successfully used for palmprint (Kong and Zhang
2004) and FKP (Zhang et al. 2009a, b, 2010) recognition. Competitive coding
scheme assumes that every image pixel resides on a negative “line” and it extracts
the orientation of the line by using a set of real Gabor ﬁlters with different
orientations. Speciﬁcally, the orientation along which the Gabor responses get the
minimum is taken as the feature at this point. Denote by GR (GI) the real (imagi-
nary) part of the Gabor ﬁlter G. With a series of GRs sharing the same parameters,
except the parameter of orientation, the local orientation of the image I at the
position (x, y) can be extracted. Mathematically, the local orientation is deﬁned as
ori x; y
ð
Þ ¼ argmin
j
I x; y
ð
Þ*GR x; y; θj




ð6:5Þ
where θj ¼ jπ/J , j ¼ {0, . . . , J  1}. J represents the number of orientations. It needs
to be noted that theoretically speaking, the local orientation of ideal i1D 2D image
signals can be accurately extracted by the Riesz transform-based monogenic signal
model, which is a 2D extension of the classical 1D analytic signal (Felsberg and
Sommer 2001; Wietzke and Sommer 2010); however, for real 2D images, multi-
dimensional even-symmetric ﬁlters usually perform better for this task (Kong and
Zhang 2004; Zhang et al. 2010).
The extraction of PC using quadrature pair ﬁlters will be presented in Sect. 6.3.1
in detail. Actually, PC is a 1D concept. For 2D images, we can compute PCθj
along
different
orientations
{θj :j j ¼ 0 ~ J  1}.
Then
the
maximum
of
PCθj :j j ¼ 0 eJ  1


can be taken as the PC value at the examined position:
PC2 x; y
ð
Þ ¼ max
i
PCθj x; y
ð
Þ :j j ¼ 0 e J  1


ð6:6Þ
We denote by θm the orientation along which the 1D PC takes the maximum.
Then, we can apply Gabor ﬁltering along θm and deﬁne the local phase as:
phase x; y
ð
Þ ¼ arctan 2 I x; y
ð
Þ*GI x; y; θm
ð
Þ; I x; y
ð
Þ*GR x; y; θm
ð
Þ
ð
Þ
ð6:7Þ
116
6
Local Features for Finger-Knuckle-Print Recognition

6.3
Extraction and Matching of Local Features
In Sect. 6.2, we have deﬁned and analyzed three local features. In practice, for the
reason of computational efﬁciency, we do not compute the three local features
separately. Instead, we present a scheme based on the computational framework of
PC in (Kovesi 1999) to extract those features more efﬁciently. So, in the following
sub-sections, the PC computation will be described ﬁrst.
6.3.1
Step 1: Phase Congruency (PC)
Rather than assume a feature is a point of sharp changes in intensity, the PC model
postulates that features are perceived at points where the Fourier components are
maximal in phase (Morrone et al. 1986; Morrone and Burr 1988; Kovesi 1999).
Phase congruency can be considered as a dimensionless measure for the signiﬁ-
cance of a structure independently of the signal amplitude. The technique to
calculate PC is based on Kovesi’s salient work (Kovesi 1999).
We start from the 1D signal f(x). Denote by M e
n and M0
n the even-symmetric and
odd-symmetric ﬁlters at scale n and they form a quadrature pair. Responses of each
quadrature pair to the signal will form a response vector at position x and on scale n:
en x
ð Þ; on x
ð Þ
½
 ¼ f x
ð Þ*M e
n; f x
ð Þ*M0
n
	

ð6:8Þ
The local amplitude on scale n is given by
An x
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
e2
n x
ð Þ þ o2
n x
ð Þ
q
ð6:9Þ
and the local phase is given by
ϕn x
ð Þ ¼ arctan 2 on x
ð Þ; en x
ð Þ
ð
Þ
ð6:10Þ
These response vectors form the basis of our localized representation of the
signal and the PC can be derived from them.
Let F(x) ¼ ∑nen(x) and H(x) ¼ ∑non(x). Then, the 1-D PC can be computed as
PC x
ð Þ ¼
E x
ð Þ
ε þ P
n An x
ð Þ
ð6:11Þ
where E x
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
F2 x
ð Þ þ H2 x
ð Þ
p
and ε is a small positive constant. We can also
deﬁne the local phase as
Phase x
ð Þ ¼ arctan 2 H x
ð Þ; F x
ð Þ
ð
Þ
ð6:12Þ
Actually, it is the average local phase over n scales.
6.3
Extraction and Matching of Local Features
117
www.ebook3000.com

For 2D images, we have to apply the 1D analysis over several orientations and
combine the results in some way. In such case, 2D ﬁlters with the orientation
selection property can be used, such as the Gabor ﬁlters (Gabor 1946; Daugman
1985) or log-Gabor ﬁlters (Field 1987). Let θj ¼ jπ/J , j ¼ {0, . . . , J  1}, denote the
orientation angle of the ﬁlter, where J is the number of orientations. By modulating
n and θj and convolving with the 2D image, we can get a set of responses at each
image point x as
en,θj x
ð Þ; on,θj x
ð Þ
	

ð6:13Þ
The local amplitude of point x on scale n and along orientation θj is given by
An,θj x
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
en,θj x
ð Þ2 þ on,θj x
ð Þ2
q
ð6:14Þ
The local energy along orientation θj is given by
Eθj x
ð Þ ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
Fθj x
ð Þ2 þ Hθj x
ð Þ2
q
ð6:15Þ
Where Fθj x
ð Þ ¼ P
ne,θj x
ð Þ and Hθj x
ð Þ ¼ P
non,θj x
ð Þ. Then, the phase congru-
ency along orientation θj is computed by
PCθj x
ð Þ ¼
Eθj x
ð Þ
ε þ P
n An,θj x
ð Þ
ð6:16Þ
The average local phase along orientation θj is deﬁned as
Phaseθj x
ð Þ ¼ arctan 2 Hθj x
ð Þ; Fθj x
ð Þ


ð6:17Þ
We deﬁne the 2D PC at x as
PC2 x
ð Þ ¼ max
j
PCθj x
ð Þ
ð6:18Þ
It should be noted that PC2(x) is a real number within 0–1.
6.3.2
Local Feature Extraction and Coding
In this section, we present the extraction and coding algorithm for each local
feature. The local orientation and the local phase can be efﬁciently extracted by
using the intermediate results of the PC computation.
118
6
Local Features for Finger-Knuckle-Print Recognition

Having obtained two raw PC maps of two images, we do not match them
directly. Instead, we quantize them to several levels and then code them into
integers. In practice, such a scheme can have three advantages: (a) it can save a
lot the storage space; (b) for recognition, it works more robustly than using raw PC
maps; and (c) it allows a fast matching of two maps. Therefore, we quantize PC into
L levels and deﬁne the PC code as
pcCode x
ð Þ ¼
PC2 x
ð Þ
1=L


ð6:19Þ
Where bxc is the operator to return the largest integer not bigger than x. It is easy
to see that each pcCode is an integer within 0 ~ L  1.
Although there are other kinds of methods to evaluate the local phase feature and
the local orientation feature, we want to make a full use of the intermediate results
in the process of computing PC in order to reduce the computational cost. It is easy
to see that when calculating PC, we can get responses from a set of even-symmetric
and odd-symmetric quadrature ﬁlters at different scales and different orientations.
We can compute the local orientation and the local phase directly from them. For
the local orientation evaluation, we borrow the idea from the competitive coding
scheme (Kong and Zhang 2004; Zhang et al. 2009a, b, 2010). With the responses
from
the
even-symmetric
ﬁlters
at
a
certain
scale
ς,
e.g.
eζ,θj x
ð Þ : j ¼ 0; . . . ; J  1


, the orientation code at x can be deﬁned as
oriCode x
ð Þ ¼ argmin
j
eζ,θj x
ð Þ


, j ¼ 0, . . . , J  1
ð6:20Þ
Obviously, each orientation code oriCode(x) is an integer within 0 ~ J  1.
Refer to Eq. (6.18), by our deﬁnition the 2D PC is actually the maximum of the
1D PCs along different orientations. We denote by θm the orientation along which
the 1D PC takes the maximum value. Then, we can take the average local phase
along θm as the local phase at x. That is
LP x
ð Þ ¼ Phaseθm x
ð Þ
ð6:21Þ
The range of LP is [0, 2 π]. Once again, we do not need the exact local phase
angle. Instead, we quantize LP into several discrete levels to get the “phase code” as
phaCode x
ð Þ ¼ LP x
ð Þ= 2π=M
ð
Þ
b
c
ð6:22Þ
where M is the number of quantization levels. Thus, each phase code is an integer
within 0~M1.
Finally, for a given image, we can get its three code maps: pcCode, oriCode, and
phaCode. Examples of them are shown in Fig. 6.2.
6.3
Extraction and Matching of Local Features
119
www.ebook3000.com

6.3.3
Matching of Local Feature Maps
Having obtained three code m aps pcCode, oriCode, and phaCode for each image,
the next issue is how to match them for recognition. Since the PC is a dimensionless
measure, we can use the absolute difference to measure the distance between two
pcCode maps. Speciﬁcally, given two PC code maps, pcCode1 and pcCode2, we
deﬁne their normalized matching distance as
pcD ¼
P P abs pcCode1 x
ð Þ  pcCode2 x
ð Þ
ð
Þ
L  1
ð
ÞS
ð6:23Þ
where S is the area of the image.
For comparing two orientation code maps, oriCode1 and oriCode2, we resort to
the normalized angular distance proposed in (Kong and Zhang 2004), which is
deﬁned as
oriD ¼
P P ang oriCode1 x
ð Þ; oriCode2 x
ð Þ
ð
Þ
SJ=2
ang p; q
ð
Þ ¼
min p  q; q  p þ J
ð
Þ, p  q
min q  p; p  q þ J
ð
Þ, p < q

ð6:24Þ
When matching two phase code maps, we use a similar method as matching two
orientation code maps. The matching distance between two phase code maps,
phaCode1 and phaCode2, is given by
phaD ¼
P P ang phaCode1 x
ð Þ; phaCode2 x
ð Þ
ð
Þ
SM=2
ð6:25Þ
In real implementation, it is easy to design “bitwise” representations for pcCode,
oriCode, and phaCode, and accordingly, pcD, oriD, and phaD can be effectively
computed.
(a1)
(b1)
(c1)
(d1)
(a2)
(b2)
(c2)
(d2)
Fig. 6.2 Examples of local feature maps. (a1) and (a2) are the original FKP ROI images; (b1) and
(b2) are the corresponding pcCode maps; (c1) and (c2) are the corresponding oriCode maps;
(d1) and (d2) are the corresponding phaCode maps
120
6
Local Features for Finger-Knuckle-Print Recognition

6.3.4
Integration of Local Features
The three local features reﬂect different aspects of information contained in an
image patch. Thus, we can expect higher recognition accuracy when assembling
information from the three features together. This can be achieved by a score-level
fusion and we refer to this feature integration scheme as local feature integration
(LFI). Suppose that three matching distances pcD, oriD, and phaD have been
calculated by matching the three kinds of local features respectively. These three
distances can be fused together to get the ﬁnal matching distance. There are a
couple of rules for the fusion of matching distances, such as the Simple-Sum
(SS) rule, the MIn-Score (MIS) rule, the MAx-Score (MAS) rule, and the
Matcher-Weighting (MW) rule (Snelick et al. 2005). In our case, pcD, oriD, and
phaD can be considered to be obtained from three different matchers and we adopt
the MW rule. With the MW fusion rule, weights are assigned according to the equal
error rate (EER) obtained on a training dataset by different matchers. Denote by ek
the EER of the matcher k, k ¼ 1 , . . . , 3. Then, the weight wk associated with
matcher k can be calculated as
wk ¼
1=
X
3
j¼1
1
ej
 
!
=ek
ð6:26Þ
where 0  wk  1 and P3
k¼1 wk ¼ 1. It is obvious that the weights are inversely
proportional to the corresponding EERs. Then, the ﬁnal matching distance between
two FKP images using LFI is calculated as
d ¼ w1pcD þ w2oriD þ w3phaD
ð6:27Þ
6.4
Experimental Results and Discussions
6.4.1
FKP Database and the Test Protocol
In our previous work (Zhang et al. 2010), an FKP database was established using
the developed FKP image acquisition device. This database is intended to be a
benchmark to evaluate the performance of various FKP recognition methods, and it
is publicly online available at (PolyU Finger-Knuckle-Print Database 2010). In this
database, FKP images were collected from 165 volunteers, including 125 males and
40 females. Among them, 143 subjects were 20–30 years old and the others were
30–50 years old. We collected samples in two separate sessions. In each session, the
subject was asked to provide six images for each of the left index ﬁnger, the left
middle ﬁnger, the right index ﬁnger, and the right middle ﬁnger. Therefore,
48 images from four ﬁngers were collected from each subject. In total, the database
6.4
Experimental Results and Discussions
121
www.ebook3000.com

contains 7920 images from 660 different ﬁngers. The average time interval between
the ﬁrst and the second sessions was about 25 days. The maximum and minimum
time intervals were 96 days and 14 days, respectively. In all of the following
experiments, we took images collected at the ﬁrst session as the gallery set and
images collected at the second session as the probe set. To obtain statistical results,
each image in the probe set was matched with all the images in the gallery set. If the
two images were from the same ﬁnger, the matching between them was counted as
a genuine matching; otherwise it was counted as an imposter matching.
The equal error rate (EER), which is the point where the false accept rate (FAR)
is equal to the false reject rate (FRR), is used to evaluate the veriﬁcation accuracy.
The decidability index d’ (Daugman 2003) is used to measure how well the genuine
and the imposter distributions are separated. d’ is deﬁned as
d0 ¼
μ1  μ2
j
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1 þ σ2
2


=2
q
ð6:28Þ
where μ1(μ2) is the mean of the genuine (imposter) matching distances and σ1(σ2) is
the standard deviation of the genuine (imposter) matching distances. Besides, by
adjusting the matching threshold, a detection error tradeoff (DET) curve (Martin
et al. 1997), which is a plot of false reject rate (FRR) against false accept rate (FAR)
for all possible thresholds, can be created. The DET curve can reﬂect the overall
veriﬁcation accuracy of a biometric system. Thus, the DET curve obtained by using
each evaluated method will be provided.
6.4.2
Determination of Parameters
In real implementation, with respect to the quadrature pair ﬁlters, we utilized the
log-Gabor ﬁlters whose transfer function in the frequency domain is
G2 ω; θj


¼ exp  log ω=ω0
ð
Þ
ð
Þ2
2σ2
r
 
!
exp  θ  θj

2
2σθ2
 
!
ð6:29Þ
where ω0 is the ﬁlter’s center frequency, σr controls the ﬁlter’s radial bandwidth and
σθ determines the ﬁlter’s angular bandwidth. In the spatial domain, a log-Gabor
ﬁlter has a similar shape with a Gabor ﬁlter (Kovesi 1999). However, compared
with Gabor ﬁlters, log-Gabor ﬁlters have some special advantages (Field 1987;
Kovesi 1999). At ﬁrst, one cannot construct Gabor ﬁlters of arbitrary bandwidth and
still maintain a reasonably small DC component in the even-symmetric ﬁlter, while
log-Gabor ﬁlters, by deﬁnition, has no DC component. Secondly, the transfer
function of the log-Gabor ﬁlter has an extended tail at the high frequency end,
which makes it more capable to encode natural images than ordinary Gabor ﬁlters.
122
6
Local Features for Finger-Knuckle-Print Recognition

Thus, we chose to use log-Gabor ﬁlters to compute local features discussed in Sects.
6.2 and 6.3. Parameters were empirically tuned based on a sub-dataset containing
images from the ﬁrst 300 FKP classes and the tuning criterion was that parameter
values that could lead to a lower EER would be chosen. As a result, parameters
were set as the following:n ¼ 3, J ¼ 6, σθ ¼ 0:44, L ¼ 5, ς ¼ 3, M ¼ 8, ω1
0 ¼ 0:60,
ω2
0 ¼ 0:167, ω3
0 ¼ 0:083,
where ω1
0,
ω2
0
and ω3
0
represent the three center
frequencies of the log-Gabor ﬁlters at three scales. In LFI, the weights assigned
to the local orientation matcher, the local phase matcher, and the phase congruency
matcher are 0.45, 0.25, and 0.30, respectively.
6.4.3
Performance of Local Features
In this experiment, we validate our claim that LFI could provide higher perfor-
mance than using any of the three local features (the local orientation, the local
phase, and the phase congruency) individually. In this experiment, all the classes of
FKPs were involved. Therefore, there were 660 (165  4) classes and 3960
(660  6) images in the gallery set and the probe set each. Each image in the
probe set was matched against all the images in the gallery set. Thus, the numbers
of genuine matchings and imposter matchings were 23,760 and 15,657,840,
respectively.
The veriﬁcation accuracy by using each single feature, the local orientation, the
local phase, or the PC, is given in Table 6.1. The performance of the LFI scheme is
also reported in Table 6.1. The performance of a state-of-the-art FKP recognition
method, Comp-Code (Zhang et al. 2010), is listed in Table 6.1 for comparison. The
DET curves obtained by the evaluated methods are shown in Fig. 6.3.
From the experimental results shown in Table 6.1 and Fig. 6.3, we can have the
following ﬁndings. At ﬁrst, the local orientation can provide higher discriminability
than the other two local features, the local phase and the PC, for the task of FKP
veriﬁcation. Secondly, “local orientation” and “CompCode” have nearly the same
performance because both of them exploit the orientation information. In fact, the
local orientation in this chapter is extracted by using log-Gabor ﬁlters while
CompCode extracts such information by using Gabor ﬁlters. Thus, we can conclude
that Gabor ﬁlters and log-Gabor ﬁlters have very similar performance for orienta-
tion feature extraction. Thirdly, the LFI scheme which integrates all the three local
Table 6.1 Performance of
different FKP veriﬁcation
schemes based on local
feature(s)
Feature type
EER (%)
d0
Local orientation
1.67
4.2847
Local phase
3.01
2.9213
Local phase congruency
2.59
3.3811
CompCode (Zhang et al. 2010)
1.66
4.2989
LFI
1.27
4.3221
6.4
Experimental Results and Discussions
123
www.ebook3000.com

features together performs obviously better than using any of them individually,
which corroborates our claim.
It should be noted that LFI has higher computational cost and needs more storage
space than CompCode (Zhang et al. 2010). At the feature extraction stage, the
major operations involved are convolutions. So, the number of convolutions used
can roughly reﬂect the overall computational complexity of the feature extraction.
For CompCode, 6 convolutions are needed (Zhang et al. 2010). For LFI, in order to
compute the phase congruency, at each speciﬁc scale 6 log-Gabor ﬁltering are
applied and altogether 3 scales are adopted; thus, 6  3 ¼ 18 convolutions are
needed. Therefore, LFI has about 3 times the computational complexity of
CompCode. In CompCode, each feature point is represented by 3-bits. In LFI, for
each feature point, 3-bits are used to represent each local feature, and thus 9-bits are
used to represent a feature point. So, it is easy to see that LFI needs 3 times storage
space compared with CompCode.
6.4.4
Further Discussions
6.4.4.1
Robustness to Small Rotations
From the experimental results reported in Sect. 6.4.3, we can see that the local
orientation has much higher discriminability than the local phase and PC for FKP
recognition, and by incorporating the local phase and PC, the veriﬁcation
Fig. 6.3 DET curves obtained by using various FKP recognition methods based on local feature(s)
124
6
Local Features for Finger-Knuckle-Print Recognition

performance could be much better than using the local orientation feature alone.
The local phase and PC could provide additional discriminative information that is
independent of local orientation for the FKP veriﬁcation task. In addition, the local
phase and PC features are more robust to small rotations than the local orientation.
Due to the imperfection of the imaging device and the ROI extraction algorithm,
there will be small rotations among intra-class images captured at different times,
which will have negative effect to the algorithm s mainly depending on the
orientation information. To validate such a conjecture, we tested the robustness to
small rotations of each local feature in this section.
For this purpose, we selected images from the ﬁrst 400 FKP classes as the dataset
and the experiment protocols were the same as described in Sect. 6.4.1. Let
α ¼ {0, 1, 2, 3, 4, 5}. For each α, by rotating each image in the dataset randomly
by a degree within range [α, α], we can get a new virtual dataset. The veriﬁcation
performances of each local feature in terms of EER on these 6 virtual datasets are
summarized in Table 6.2. For comparison, we also list the results obtained by
CompCode and LFI under the same experimental settings in Table 6.2.
As shown in Table 6.2, when α increases, the EER obtained by using each local
feature increases. However, for different local features, the “accelerations” of the
EER increase are different. We use the following measure to characterize the
“acceleration” of the EER increase:
η ¼ 1
5
X
5
i¼1
eeri  eer0
eer0
ð6:30Þ
where eeri is the EER obtained when α ¼ i. By using this metric η, the “accelera-
tion” of the EER increases by using different local features while α increases can be
compared. For the local orientation, η ¼ 0.2798; for the local phase η ¼ 0.2050; and
for the PC, η ¼ 0.1477. From this experiment, we can clearly see that with the
increase of the rotation degree, the performance decrease of local phase and PC is
much less than local orientation, which indicates that the local phase and PC are
more robust to small rotations than the local orientation. Moreover, for Comp-
Code, η ¼ 0.2866 and for LFI, η ¼ 0.2171, which indicates that the proposed local
feature integration scheme LFI is much more robust to small rotations than the
CompCode scheme which depends on local orientation information only.
Table 6.2 Veriﬁcation performance (measured by EER) of local features on the virtual rotated
datasets
α ¼ 0
α ¼ 1
α ¼ 2
α ¼ 3
α ¼ 4
α ¼ 5
Local orientation
2.03%
2.04%
2.16%
2.40%
2.78%
3.61%
Local phase
3.63%
3.65%
3.87%
4.21%
4.64%
5.50%
Phase congruency
2.83%
2.86%
2.91%
3.23%
3.36%
3.88%
CompCode (Zhang et al. 2010)
2.01%
2.03%
2.15%
2.38%
2.77%
3.60%
LFI
1.52%
1.54%
1.64%
1.77%
1.96%
2.34%
6.4
Experimental Results and Discussions
125
www.ebook3000.com

6.4.4.2
Integrating Local Features with a Global Feature
In our previous work (Zhang et al. 2011), we presented a local-global information
combination (LGIC) scheme for FKP recognition, in which the local orientation
extracted by Gabor ﬁlters was taken as the local feature while the image’s Fourier
transform coefﬁcients were taken as the global feature. Similarity of Fourier
transform coefﬁcients from two images were compared using the phase-only
correlation (POC) technique (Kuglin and Hines 1975; Reddy and Chatterji 1996).
LGIC could achieve the highest veriﬁcation accuracy on our FKP database. In fact,
the local features discussed can also be integrated with the global feature, i.e., the
Fourier transform coefﬁcients, using the same framework as LGIC. We call this
new local–global information combination scheme as LGIC2. Compared with
LGIC, LGIC2 involves two more local features, the local phase and the phase
congruency. We compared the performance of LGIC and LGIC2 under the same
experiment settings as described in Sect. 6.4.3. The results in terms of EER and d’
are summarized in Table 6.3. Besides, the EER reported by another state-of-the-art
method OE-SIFT (Morales et al. 2011) under the same experimental settings is also
listed in Table 6.3 for comparison. DET curves obtained by LGIC and LGIC2 are
shown in Fig. 6.4. Distance distributions of genuine matchings and imposter
matchings obtained by LGIC2 are plotted in Fig. 6.5. From the experimental results,
Table 6.3 FKP veriﬁcation
performance of OE-SIFT,
LGIC and LGIC2
Method
EER (%)
d0
OE-SIFT (Morales et al. 2011)
0.850
–
LGIC (Zhang et al. 2011)
0.402
4.5356
LGIC2
0.358
4.7001
Fig. 6.4 DET curves obtained by LGIC and LGIC2
126
6
Local Features for Finger-Knuckle-Print Recognition

we can see that LGIC2 performs better than LGIC. It once again corroborates our
claim that the local phase and the phase congruency could afford more discrimi-
native information for FKP recognition.
LGIC2 is implemented using Visual C#.Net 2005 on a Dell Inspiron 530s PC
embedded Intel E6550 processor and 2 GB of RAM. Computation time for the key
processes is listed in Table 6.4. The total execution time for one veriﬁcation
operation is less than 0.7 s in our prototype system, which is fast enough for real-
time applications. We believe that with the optimization of the implementation, the
system’s efﬁciency could be much further improved.
It should be noted that though LGIC2 performs the best among all the existing
FKP veriﬁcation methods, it cannot deal with severe intra-class pose variations.
Such variations can result in severe afﬁne transforms or even non-elastic deforma-
tions among intra-class FKP images. In fact, most of the failure cases of LGIC2 can
be attributed to such large-scale intra-class pose variations. Figure 6.6 shows a
typical example. Figure 6.6a, b are two FKP images captured from the same ﬁnger
in different sessions. Figure 6.6c, d are the ROIs extracted from Fig. 6.6a, b,
respectively. It can be seen that there is an obvious pose variation between the
two FKPs. They are recognized as different classes by LGIC2. Hence, in the future,
we will focus on devising high performance FKP recognition algorithms being
Fig. 6.5 Distance distributions of genuine matchings and imposter matchings obtained by LGIC2
Table 6.4 Computation time
for key processes of LGIC2
Operations
Time (ms)
ROI extraction
198
Local feature extraction
405
Local feature matching
0.9
Global feature matching
2.1
6.4
Experimental Results and Discussions
127
www.ebook3000.com

robust to such intra-class pose variations. For example, the idea proposed in
Morales et al.’s work (Morales et al. 2011) can be borrowed.
6.5
Summary
In this chapter, we focused on developing new effective feature extraction and
matching method for FKP recognition. To this end, we analyzed three commonly
used local features, the local orientation, the local phase, and the phase congruency
systematically and presented a method for computing them efﬁciently using the
phase congruency computation framework. Coding and matching algorithm for
each local feature was presented. Extensive experiments were conducted on the
benchmark PolyU FKP database. The experimental results showed that the inte-
gration of all the local features together performs better than using any of them
separately. The algorithm LGIC2, which integrates all the three local features and
one global feature, Fourier transform coefﬁcients, could achieve the best veriﬁca-
tion result on the benchmark FKP database, with the EER 0.358%.
References
Daugman J (1985) Uncertainty relation for resolution in space, spatial frequency, and orientation
optimized by two-dimensional visual cortical ﬁlters. J Opt Soc Am A 2(7):1160–1169
Daugman J (1993) High conﬁdence visual recognition of persons by a test of statistical indepen-
dence. IEEE Trans Pattern Anal Mach Intell 15(11):1148–1161
Fig. 6.6 (a) and (b) are two intra-class FKP images in PolyU FKP dataset (PolyU Finger-
Knuckle-Print Database, 2010); (c) and (d) are their ROI sub-images. There is an obvious pose
variation between the two FKPs and they are recognized as different classes by LGIC2
128
6
Local Features for Finger-Knuckle-Print Recognition

Daugman J (2003) The importance of being random: statistical principles of iris recognition.
Pattern Recogn 36(2):279–291
Duta N (2009) A survey of biometric technology based on hand shape. Pattern Recogn 42
(11):2797–2806
Felsberg M, Sommer G (2001) The monogenic signal. IEEE Trans Signal Process 49
(12):3136–3144
Feng Z, Yang B, Chen Y, Zheng Y, Xu T, Li Y, Zhu D (2011) Features extraction from hand
images based on new detection operators. Pattern Recogn 44(5):1089–1105
Field D (1987) Relations between the statistics of natural images and the response properties of
cortical cells. J Opt Soc Am A 4(12):2379–2394
Gabor D (1946) Theory of communication. J Inst Electr Eng Part III Radio Commun Eng 93
(26):429–441
Granlund G, Knutsson H (1995) Signal processing for computer vision. Springer, Berlin
Gundimada S, Asari V (2007) A novel neighborhood deﬁned feature selection on phase congru-
ency images for recognition of faces with extreme variations. Int J Inf Technol 3(1):25–31
Han C, Cheng H, Lin C, Fan K (2003) Personal authentication using palm-print features. Pattern
Recogn 36(2):371–381
Henriksson L, Hyva¨rinen A, Vanni S (2009) Representation of cross-frequency spatial phase
relationships in human visual cortex. J Neurosci 29(45):14342–14351
Hong L, Wan Y, Jain A (1998) Fingerprint image enhancement: algorithm and performance
evaluation. IEEE Trans Pattern Anal Mach Intell 20(8):777–789
Hu D, Feng G, Zhou Z (2007) Two-dimensional locality preserving projections (2DLPP) with its
application to palmprint recognition. Pattern Recogn 40(1):339–342
Jain A, Feng J (2009) Latent palmprint matching. IEEE Trans Pattern Anal Mach Intell 31
(6):1032–1047
Jain A, Flynn P, Ross A (2007) Handbook of biometrics. Springer Science & Business Media,
New York
Jia W, Huang D, Zhang D (2008) Palmprint veriﬁcation based on robust line orientation code.
Pattern Recogn 41(5):1504–1513
Kong A, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. In: Proceedings
of the 17th international conference on pattern recognition, pp 520–523
Kong A, Zhang D, Kamel M (2006) Palmprint identiﬁcation using feature-level fusion. Pattern
Recogn 39(3):478–487
Kong A, Zhang D, Kamel M (2009) A survey of palmprint recognition. Pattern Recogn 42
(7):1408–1418
Kovesi P (1999) Image features from phase congruency. Videre J Comput Vis Res 1(3):1–26
Krieger G, Zetzsche C (1996) Nonlinear image operators for the evaluation of local intrinsic
dimensionality. IEEE Trans Image Process 5(6):1026–1042
Kuglin C, Hines D (1975), The phase correlation image alignment method. In: Proceedings of
international conference on cybernetics and society, pp 163–165
Kumar A, Ravikanth C (2009) Personal authentication using ﬁnger knuckle surface. IEEE Trans
Inf Forensics Secur 4(1):98–110
Li S (2009) Encyclopedia of biometrics: I-Z, vol 1. Springer Science & Business Media, New York
Maltoni D, Maio D, Jain A, Prabhakar S (2003) Handbook of ﬁngerprint recognition. Springer,
New York
Martin A, Doddington G, Kamm T, Ordowski M, Przybocki M (1997) The DET curve in
assessment of detection task performance. In: European conference on speech communication
& technology, pp 1895–1898
Meraoumia A, Chitroub S, Bouridane A (2011) Palmprint and ﬁnger-knuckle-print for efﬁcient
person recognition based on log-gabor ﬁlter response. Analog Integr Circ Sig Process 69
(1):17–27
Morales A, Travieso C, Ferrer M, Alonso J (2011) Improved ﬁnger-knuckle-print authentication
based on orientation enhancement. Electron Lett 47(6):380–381
References
129
www.ebook3000.com

Morrone M, Burr D (1988) Feature detection in human vision: a phase-dependent energy model.
Proc R Soc Lond B Biol Sci 235(1280):221–245
Morrone M, Owens R (1987) Feature detection from local energy. Pattern Recogn Lett 6
(5):303–313
Morrone M, Ross J, Burr D, Owens R (1986) Mach bands are phase dependent. Nature 324
(6094):250–253
PolyU (2010) Finger-Knuckle-Print Database. http://www.comp.polyu.edu.hk/~biometrics
Reddy B, Chatterji B (1996) An FFT-based technique for translation, rotation, and scale-invariant
image registration. IEEE Trans Image Process 5(8):1266–1271
Ross A, Jain A, Reisman J (2003) A hybrid ﬁngerprint matcher. Pattern Recogn 36(7):1661–1673
Sierra-Va´zquez V, Serrano-Pedraza I (2010) Application of Riesz transforms to the isotropic
AM-PM decomposition of geometrical-optical illusion images. J Opt Soc Am A 27
(4):781–796
Snelick R, Uludag U, Mink A, Indovina M, Jain A (2005) Large-scale evaluation of multimodal
biometric authentication using state-of-the-art systems. IEEE Trans Pattern Anal Mach Intell
27(3):450–455
Struc V, Pavesic N (2009) Phase congruency features for palm-print veriﬁcation. IET Sig Process
3(4):258–268
Venkatesh S, Owens R (1990) On the classiﬁcation of image features. Pattern Recogn Lett 11
(5):339–349
Verikas A, Gelzinis A, Bacauskiene M, Olenina I, Olenin S, Vaiciukynas E (2012) Phase
congruency-based detection of circular objects applied to analysis of phytoplankton images.
Pattern Recogn 45(4):1659–1670
Wang J, Yau W, Suwandy A, Sung E (2008) Person recognition by fusing palmprint and palm vein
images based on “Laplacianpalm” representation. Pattern Recogn 41(5):1514–1527
Wietzke L, Sommer G (2010) The signal multi-vector. J Math Imaging Vision 37(2):132–150
Wilson C (2010) Vein pattern recognition. A privacy-enhancing biometric. CRC, Boca Raton
Woodard D, Flynn P (2005) Finger surface as a biometric identiﬁer. Comput Vis Image Underst
100(3):357–384
Yuan X, Shi P (2005) Iris feature extraction using 2D phase congruency. Third Int Conf Inf
Technol Appl 2:437–441
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhang L, Zhang L, Zhang D (2009a) Finger–knuckle-print: a new biometric identiﬁer. Proceed-
ings of the ICIP 2009, pp 1981–1984
Zhang L, Zhang L, Zhang D (2009b) Finger–knuckle-print veriﬁcation based on band-limited
phase-only correlation. Proc CAIP 2009:141–148
Zhang L, Zhang L, Zhang D, Zhu H (2010) Online ﬁnger-knuckle-print veriﬁcation for personal
authentication. Pattern Recogn 43(7):2560–2571
Zhang L, Zhang L, Zhang D, Zhu H (2011) Ensemble of local and global information for ﬁnger–
knuckle-print recognition. Pattern Recogn 44(9):1990–1998
Zhang D, Zuo W, Yue F (2012) A comparative study of palmprint recognition algorithms. ACM
Comput Surv (CSUR) 44(1):1–37
Zhao Q, Zhang D, Zhang L, Luo N (2010) Adaptive ﬁngerprint pore modeling and extraction.
Pattern Recogn 43(8):2833–2844
130
6
Local Features for Finger-Knuckle-Print Recognition

Chapter 7
Global Information for Finger-Knuckle-Print
Recognition
Abstract Biometrics authentication is an effective method for automatically rec-
ognizing a person’s identity. Recently, it has been found that the ﬁnger–knuckle-
print (FKP), which refers to the inherent skin patterns of the outer surface around
the phalangeal joint of one’s ﬁnger, has high capability to discriminate different
individuals, making it an emerging biometric identiﬁer. In this chapter, based on the
results of psychophysics and neurophysiology studies that both local and global
information is crucial for the image perception, we present an effective FKP
recognition scheme by extracting and assembling local and global features of
FKP images. Speciﬁcally, the orientation information extracted by the Gabor ﬁlters
is coded as the local feature. By increasing the scale of Gabor ﬁlters to inﬁnite,
actually we can get the Fourier transform of the image, and hence the Fourier
transform coefﬁcients of the image can be taken as the global features. Such kinds
of local and global features are naturally linked via the framework of time–
frequency analysis. The proposed scheme exploits both local and global informa-
tion for the FKP veriﬁcation, where global information is also utilized to reﬁne the
alignment of FKP images in matching. The ﬁnal matching distance of two FKPs is a
weighted average of local and global matching distances. The experimental results
conducted on our FKP database demonstrate that the proposed local-global infor-
mation combination scheme could signiﬁcantly improve the recognition accuracy
obtained by either local or global information, and lead to promising performance
of an FKP-based personal authentication system.
Keywords Biometrics
•
Finger-knuckle-print
•
Local-global
information
combination
7.1
Introduction
Recognizing the identity of a person with high conﬁdence is a critical issue in
various applications, such as e-banking, access control, passenger clearance, etc.
The need for reliable user authentication techniques has signiﬁcantly increased in
the wake of heightened concerns about security, and rapid advancement in net-
working, communication and mobility (Jain et al. 2007). Biometrics based
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_7
131
www.ebook3000.com

methods, which use unique physical or behavioral characteristics of human beings,
are of broad interest and have great potentials because of their high accuracy and
convenience to use in the modern e-world. With the rapid development of comput-
ing techniques, in the past several decades researchers have exhaustively investi-
gated the use of a number of biometric characteristics, including ﬁngerprint, face,
iris, palmprint, hand geometry, voice and ear.
Among various kinds of biometric identiﬁers, hand-based biometrics has been
attracting considerable attention over recent years. Fingerprint (Maltoni et al. 2003;
Ratha and Bolle 2004; Gu et al. 2006; Nikam et al. 2007; Aguilar et al. 2006),
palmprint (Zhang et al. 2003; Kong and Zhang 2004; Jia et al. 2008), hand
geometry (Jain et al. 1999; Sanchez-Reillo et al. 2000), hand vein (Wang et al.
2008) and inner-knuckle-print (Li et al. 2004; Nanni and Lumini 2009) have been
proposed and well investigated in the literature. Recently, it has been found that the
image pattern of skin folds and creases in the outer ﬁnger knuckle surface is highly
unique and thus can serve as a distinctive biometric identiﬁer. Compared with
ﬁngerprint, the ﬁnger knuckle surface has some advantages as a biometric identi-
ﬁer. At ﬁrst, it is not easy to be abraded since people usually hold stuffs with the
inner side of the hand. In addition, unlike the use of ﬁngerprint, there is no stigma of
criminal investigation associated with the ﬁnger knuckle surface, so it can have a
high user acceptance (Kumar and Zhou 2009). Thus, the ﬁnger knuckle feature has
a great potential to be widely accepted as a biometric identiﬁer. Some researchers
have already done salient works in this ﬁeld. Woodard and Flynn (Woodard and
Flynn 2005) are among the ﬁrst scholars who exploited the use of ﬁnger knuckle
surface in biometric systems. They set up a 3D ﬁnger back surface database with the
Minolta 900/910 sensor. For feature extraction, they used the curvature based shape
index to represent the ﬁnger back surface. Woodard’s work makes a good effort to
validate the uniqueness of outer ﬁnger surface as a biometric characteristic. How-
ever, the cost, size and weight of the Minolta 900/910 sensor limit the use of it in a
practical biometric system, and the time-consuming 3D data acquisition and
processing limit its use in real-time applications. Later, Kumar and Ravikanth
(Kumar and Ravikanth 2009) proposed a 2D ﬁnger-back surface based personal
authentication system. With respect to the feature extraction, they resorted to some
subspace analysis methods such as PCA, LDA and ICA. With their design, the
acquisition device is doomed to have a large size because nearly the whole hand
back area has to be captured, despite the fact that the ﬁnger knuckle area only
occupies a small portion of the acquired image. Furthermore, subspace analysis
methods may be effective for face recognition but they may not be able to
effectively extract the distinctive line and junction features from the ﬁnger knuckle
surface. In Kumar’s later work (Kumar and Zhou 2009), they adopted the robust
line orientation code (RLOC) (Jia et al. 2008) to extract the local orientation
information of the ﬁnger-back surface images.
In our previous works (Zhang et al. 2009a, b, 2010), a novel online personal
authentication system using ﬁnger–knuckle-print (FKP), which refers to the inher-
ent skin pattern of the outer surface around the phalangeal joint of one’s ﬁnger, has
been established. It comprises four major components: FKP image acquisition, ROI
(region of interest) extraction, feature extraction and feature matching. The
132
7
Global Information for Finger-Knuckle-Print Recognition

proposed FKP imaging system has a small size and it simpliﬁes the preprocessing
steps, such as the ﬁnger segmentation and the ROI extraction. Since the ﬁnger
knuckle will be slightly bent when being imaged, the inherent skin patterns can be
clearly captured and hence the unique FKP features can be better exploited. The
later feature extraction and matching are based on the extracted ROIs. As in any
pattern classiﬁcation task, the feature extraction and matching plays a key role in
our FKP-based personal authentication system. To this end, we have developed a
couple of different methods. In (Zhang et al. 2009a, b), we used the Gabor ﬁlter
based competitive coding scheme, which was originally designed for palmprint
recognition (Kong and Zhang 2004), to extract the local orientation information as
FKP features. In (Zhang et al. 2010), we proposed to combine the orientation
information and the magnitude information extracted by Gabor ﬁlters together. In
(Zhang et al. 2009a, b), the Fourier transform of the whole image was taken as the
feature and the band-limited phase-only correlation technique was employed to
calculate the similarity between two FKP images.
In this chapter, we focus on feature extraction and matching of FKP images.
Based on the area of pixels involved in feature extraction, we can label the features
as “local” or “global” ones. Intuitively, a local feature is a measure computed within
a local patch, encoding the detailed traits within this speciﬁc area; by contrast, a
global feature is a measure derived from all (or most of) the pixels in the image,
reﬂecting some holistic characteristic of the examined image. According to such
deﬁnitions, existing FKP recognition schemes can be classiﬁed into local-based
methods (Kumar and Ravikanth 2009; Woodard and Flynn 2005; Zhang et al.
2009a, b, 2010) and global-based methods (Kumar and Zhou 2009; Zhang et al.
2009a, b). However, few chapters have yet discussed the local–global information
combination for FKP recognition. In the literature of psychophysics and neuro-
physiology, many studies have shown that both local and global information is
crucial for the image perception and recognition of human beings (Su et al. 2009)
and they play different but complementary roles. A global feature reﬂects the
holistic characteristics of the image and is suitable for coarse representation,
while a local feature encodes more detailed information within a speciﬁc local
region and is appropriate for ﬁner representation. Hence, better recognition accu-
racy can be expected if local and global information can be appropriately combined.
Such an idea has already been explored in iris recognition, palmprint recogni-
tion, face recognition and ﬁngerprint recognition. For iris matching, Sun et al. (Sun
et al. 2004, 2005) proposed a “cascade” system in which the ﬁrst stage is a
conventional Daugman-like classiﬁer while the classiﬁer at the second stage uses
“global” features—areas enclosed by zero-crossing boundaries. In (Li et al. 2007),
the authors described a two-level palmprint matching scheme. For coarse-level
ﬁltering, Hough transform is used to extract global features; for ﬁne-level matching,
the local information extracted from the location sand orientations of individual
lines is used. Pan et al. (Pan et al. 2007) also proposed to combine the local and
global features for palmprint recognition. In their work, non-negative matrix
factorization with sparseness constraint and PCA are used to extract local and
global features, respectively. For face recognition, Fang et al. (2002) presented a
7.1
Introduction
133
www.ebook3000.com

method by combining global PCA features and component-based local features
extracted by Haar wavelets. In (Su et al. 2009), Su et al. proposed a hierarchical
ensemble classiﬁer by combining global Fourier features and local Gabor features.
In their method, global features are extracted from the whole face images by
keeping the low-frequency Fourier coefﬁcients while local features are exploited
using Gabor ﬁlters with various scales and orientations. After that, Fisher’s linear
discriminant (FLD) is applied to the global Fourier features and local Gabor
features. In the ﬁngerprint recognition community, the idea of combing local and
global information was also exploited (Gu et al. 2006; Nikam et al. 2007; Aguilar
et al. 2006).
In this chapter, we propose a novel local–global information combination
(LGIC) scheme for FKP recognition. Speciﬁcally, we take the local orientation
information extracted by the Gabor ﬁlters as the local feature because local orien-
tation has been successfully used in palmprint recognition systems (Kong and
Zhang 2004; Jia et al. 2008) and FKP recognition systems (Kumar and Zhou
2009; Zhang et al. 2009a, b, 2010). By increasing the scale of the Gabor ﬁlters,
more and more global information will be involved, yet the characterization of
image local structures will be weakened rapidly. Particularly, if the scale of the
Gabor ﬁlter is increased to inﬁnity, the Gabor transform can be reduced to the
Fourier transform of the whole image. In this case, no local information can be
extracted but we can get the ﬁnest resolution for the global frequency analysis of the
image. Thus, the Fourier transform coefﬁcients are naturally taken as the global
features in this chapter. With the global Fourier features, the alignment between
intra-class FKP ROIs can also be reﬁned. At the matching stage, two matching
distances can be computed by comparing the local features and the global features
separately. Finally, the two matching distances are fused according to some fusion
rule to get the ﬁnal matching distance. Extensive experiments and comparisons are
conducted on our established FKP database (PolyU Finger–Knuckle-Print Database
2010) to validate the efﬁcacy of the proposed LGIC scheme.
7.2
Local Feature Extraction and Matching
Gabor ﬁlters (Gabor 1946) have been widely used as an effective tool to fulﬁll the
feature extraction tasks in many biometrics systems, such as face, iris, ﬁngerprint,
palmprint, etc. The frequency and orientation representations of Gabor ﬁlters are
similar to those of the human visual system (Pinto et al. 2009, 2008). In the spatial
domain, 2D Gabor ﬁlters can be expressed as
G x; y
ð
Þ ¼ exp 1
2
x2
σ2
x
þ y2
σ2
y
 
!
 
!
exp i2π fx0
ð
Þ
ð7:1Þ
134
7
Global Information for Finger-Knuckle-Print Recognition

where x0 ¼ x cos θ + y sin θ, y0 ¼  x sin θ + y cos θ . In Eq. (7.1), f represents the
frequency of the sinusoid factor, y represents the orientation of the normal to the
parallel stripes of the Gabor function, σx and σy are the standard deviations of the
2D Gaussian envelop.
It can be seen from the deﬁnition that a Gabor ﬁlter is actually a Gaussian
envelop modulated by a sinusoidal plane wave. The Gaussian envelop ensures that
the convolution is dominated by the image patch near the center of the ﬁlter.
Thereby, when an image is convolved with a Gabor ﬁlter, the information near
the center of the Gaussian envelop is encoded, and by contrast, the information far
away from the center of the Gaussian envelop will be neglected. Therefore, the
Gabor ﬁlter is a local operator and can extract the information at a speciﬁc scale and
a speciﬁc orientation within a local region. Gabor ﬁlters can have a variety of
different forms with different scales and orientations. Figure 7.1 shows the real part
of the Gabor ﬁlters at 4 scales and along 6 orientations.
With the Gabor ﬁlters, three basic features, magnitude, phase and orientation,
can be extracted (Kong 2008). However, previous studies have shown that the local
orientation information is the most robust and distinctive local feature for palmprint
and FKP recognition (Kong and Zhang 2004; Jia et al. 2008; Kumar and Zhou 2009;
Zhang et al. 2009a, b, 2010). Hence, in this chapter, we only take the local
orientation as the local feature and make use of the Gabor ﬁlter based CompCode
(Kong and Zhang 2004; Zhang et al. 2009a, b) scheme to extract and code it. Such
Fig. 7.1 Real parts of the 24 2D Gabor ﬁlters with four scales and six orientations
7.2
Local Feature Extraction and Matching
135
www.ebook3000.com

an orientation coding based feature extraction method is suitable for images
containing abundant line-like structures and it has the merits of high accuracy,
robustness to illumination variation and fast matching. The working principle of
CompCode and its matching metric are brieﬂy reviewed as follows.
Denote by GR the real part of a Gabor ﬁlter. With a bank of GRs sharing the same
parameters, except the parameter of orientation, the local orientation information of
the image I at the position (x, y) can be extracted and coded. Mathematically, this
competitive coding process can be expressed as
CompCode x; y
ð
Þ ¼ argmin
j
I x; y
ð
Þ*GR x; y; θj




ð7:2Þ
where * stands for the convolution operation, θj ¼ jπ/J, j ¼ {0, . . . , J  1}, and
J represents the number of orientations. Based on our previous studies (Kong and
Zhang 2004; Zhang et al. 2009a, b, 2010), we set J ¼ 6 in this chapter and this is in
accordance with the conclusion made by Lee (Lee 1996) that the simple neural cells
are sensitive to speciﬁc orientations with approximate bandwidths of π/6.
Figure 7.2c, d shows two CompCode maps extracted from the FKP ROI images
in Fig. 7.2a, b, respectively.
In order for real-time recognition, CompCode uses three bits to represent each
orientation. When matching two CompCode maps P and Q, the angular distance
based on the normalized Hamming distance is used (Kong and Zhang 2004):
dL ¼
PRows
Y¼1
PCols
x¼1
P2
i¼0 pb
i x; y
ð
Þ  Qb
i x; y
ð
Þ


3S
ð7:3Þ
where Pb
i Qb
i


is the i th bit plane of P(Q), S is the area of the CompCode map and
 represents the bitwise “exclusive OR” operation.
Fig. 7.2 (a) and (b) are two FKP ROI images; (c) and (d) are the CompCode maps generated from
(a) and (b), respectively
136
7
Global Information for Finger-Knuckle-Print Recognition

7.3
Global Feature Extraction and Matching
7.3.1
From Local to Global
In Sect. 7.2, Gabor transforms are utilized to extract the local orientation informa-
tion. Actually, the Gabor transform can be regarded as a windowed Fourier trans-
form. The corresponding Gabor transform (i.e. ﬁltering) of a function f with respect
to a local window function g is (Zayed 1996)
G f½  w; t
ð
Þ ¼
ðþ1
1
. . .
ðþ1
1
f x
ð Þg x  t
ð
Þei wx
ð
Þdx
ð7:4Þ
where t, w, x 2 ℜn, dx ¼ dx1dx2 . . . dxn, x ¼ (x1, x2, . . . , xn) and w  x ¼ Pn
k¼1 wkxk.
The signal f(t) to be analyzed is deﬁned in the n  D spatial domain. t is the
coordinate variable in the n  D spatial domain and correspondingly, w is the
coordinate variable in the n  D frequency domain. The Gabor transform of f,
G[f](w, t) can give the frequency spectrum of f for a speciﬁed frequency w at a
speciﬁed position t. For the convenience of discussion, we conﬁne ourselves to
the case that n ¼ 1, g is a Gaussian-shaped window and f(x) is of ﬁnite length
[0, T]. Then, the Gabor transform of f is
G f½  ω; x
ð
Þ ¼
ð T
0
e τx
ð
Þ2=2σ2f τð Þeiωτdτ, x 2 0; T
½

ð7:5Þ
The parameter σ controls the size of the local window and the scale of the Gabor
transform. Naturally, when σ goes to inﬁnity, the whole signal f(x), x 2 [0, T], is
involved in calculating G[f](ω, t) and Eq. (7.5) is reduced to
G f½  ω
ð Þ ¼
ð T
0
f τð Þeiωτdτ
ð7:6Þ
It is seen that G[f] does not depend on x anymore, which implies that we lose the
local information in the Gabor transform. Obviously, Eq. (7.6) is the Fourier
transform of f.
The above discussion on the 1D case can be easily extended to the 2D case. For
2D images, by increasing the scale of the Gabor ﬁlters, more and more global
information will be involved, yet the characterization of image local structures will
be rapidly weakened. Particularly, if the scale of the Gabor ﬁlter goes to inﬁnity, the
Gabor transform will degrade to the 2D Fourier transform of the whole image. In
such case, though the local characterization is totally lost, we can get the ﬁnest
frequency resolution for the image analysis. Therefore, in our work the Fourier
transform is selected as the global feature extractor.
7.3
Global Feature Extraction and Matching
137
www.ebook3000.com

7.3.2
Phase-Only Correlation (POC)
Now that the Fourier transform coefﬁcients are used as the global feature, the next
problem is how to measure the similarity of two given Fourier transforms. Phase-
only correlation (POC) is a classical method to this end (Kuglin and Hines 1975). In
the literature, POC based methods have been widely used in image registration
tasks (Reddy and Chatterji 1996). Recently, POC has also been adopted as a
similarity measure in some biometrics systems (Miyazawa et al. 2008; Ito et al.
2004, 2008). Compared with the conventional POC, the band-limited phase-only
correlation (BLPOC) proposed by Ito et al. (Ito et al. 2004) is more effective.
Hence, in this chapter, we use BLPOC to evaluate the displacement parameters
between FKP ROIs and to measure the similarity of the Fourier transforms of the
aligned ROIs. In this subsection, POC will be introduced and in the next subsection
BLPOC will be described.
POC is a kind of effective method to evaluate the translation parameters between
two images in the Fourier domain. Its underlying principle is the translation
property of the Fourier transforms (Bracewell 1965). Let f and g be the two images
that differ only by a displacement (x0, y0) , i.e.
g x; y
ð
Þ ¼ f x  x0; y  y0
ð
Þ
ð7:7Þ
Their corresponding Fourier transforms G(u, v) and F(u, v) will be related by
G u; v
ð
Þ ¼
X
M0
m¼M0
X
N0
n¼N0
g m; n
ð
Þej2π mu=Mþnv=N
ð
Þ ¼ AG u; v
ð
ÞejΦF u;v
ð
Þ
ð7:8Þ
The cross-phase spectrum RGF(u, v) between G(u, v) and F(u, v) is given by
RGF u; v
ð
Þ ¼ G u; v
ð
ÞF* u; v
ð
Þ
G u; v
ð
ÞF* u; v
ð
Þ

 ¼ ej2π ux0þvy0
ð
Þ
ð7:9Þ
where Fn is the complex conjugate of F. By taking inverse Fourier transform of RGF
back to the spatial domain, we will have a Dirac impulse centered on (x0, y0).
In practice, we should consider the ﬁnite discrete representations. Consider
two
M  N
images,
f(m, n)
and
g(m, n),
where
the
index
ranges
are
m ¼  M0 . . . , M0(M0〉0) and n ¼  N0 , . . . , N0(N0〉0) and M ¼ 2M0 + 1 and
N ¼ 2N0 + 1. Denote by F(u, v) and G(u, v) the 2D DFTs of the two images and
they are given by
F u; v
ð
Þ ¼
X
M0
m¼M0
X
N0
n¼N0
f m; n
ð
Þej2π mu=Mþnv=N
ð
Þ ¼ AF u; v
ð
ÞejΦF u;v
ð
Þ
ð7:10Þ
138
7
Global Information for Finger-Knuckle-Print Recognition

G u; v
ð
Þ ¼
X
M0
m¼M0
X
N0
n¼N0
g m; n
ð
Þej2π mu=Mþnv=N
ð
Þ ¼ AG u; v
ð
ÞejΦG u;v
ð
Þ
ð7:11Þ
where u ¼  M0 . . . , M0, v ¼  N0 , . . . , N0, AF(u, v) and AG(u, v) are amplitude
components, and ΦF(u, v) and ΦG(u, v) are phase components. Then, the cross
phase spectrum RGF(u, v) between G(u, v) and F(u, v) is given by
wk ¼
1
X
2
k¼1
1
ek
 
!
ek0  wk  1 P2
k¼1 wk ¼ 1
d0 ¼
μ1  μ2
j
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1 þ σ2
2

=2
q
μ2
ð
Þ σy


fU0=M0 ¼ 0:25
σx ¼ 5:0
σy ¼ 9:0
f ¼ 0:0435
¼ 1:1  103 ¼ 1:1  103
dL
dG
RGF u; v
ð
Þ ¼ G u; v
ð
ÞF* u; v
ð
Þ
G u; v
ð
ÞF* u; v
ð
Þ

 ¼ ej ΦG u;v
ð
ÞΦF u;v
ð
Þ
f
g
ð7:12Þ
The POC function pGF (m, n) is the 2D inverse DFT (IDFT) of RGF (u, v):
pgf m; n
ð
Þ ¼
1
MN
X
M0
m¼M0
X
N0
n¼N0
RGF u; v
ð
Þej2π mu=Mþnv=N
ð
Þ
ð7:13Þ
The peak value of pgf can be calculated as max{pgf (m, n)j m 2 [M0, M0], n 2
[N0, N0]}
If the two images f and g are similar, their POC function pgf will give a distinct
sharp peak. If not, the peak value will drop signiﬁcantly. Thus, the amplitude of the
peak value can be used as a similarity measure, and the location of the peak shows
the translational displacement between the two images.
7.3.3
Band-Limited Phase-Only Correlation (BLPOC)
In the POC-based image matching method, all the frequency components are
involved. However, high frequency components can be prone to noise. To eliminate
noisy high frequency components, Ito et al. (Ito et al. 2004) proposed the band-
limited POC (BLPOC). BLPOC limits the range of the spectrum of the given FKP
image. Suppose that the ranges of the inherent frequency band of FKP texture are
given by u ¼  U0 , . . . , U0 and v ¼  V0 , . . . , V where 0  U0  M0, 0  V0  N0.
7.3
Global Feature Extraction and Matching
139
www.ebook3000.com

Thus, the effective size of spectrum is given by L1 ¼ 2U0 + 1 and L2 ¼ 2V0 + 1.
BLPOC function is deﬁned as
pU0V
gf
m; n
ð
Þ ¼
1
L1L2
X
U0
u¼U0
X
V0
v¼V0
RGF u; v
ð
Þej2π mu=L1þnv=L2
ð
Þ
ð7:14Þ
where m ¼  U0 , . . . , U0 and n ¼  V0 , . . . , V0. From the deﬁnition of BLPOC,
we can see that U0/M0 and V0/N0 can inherent frequency distribution of the FKP
images.
From the deﬁnition of BLPOC, it can be seen that the BLPOC function between
two images f and g can be considered as the POC function between their low-pass
ﬁltered versions. Thus, the BLPOC function can maintain the properties of the POC
function. Speciﬁcally, if two images are similar, their BLPOC function will have a
distinct sharp peak. At the same time, the translational displacement between the
two images can be estimated by the location of the peak. Experiments indicate that
the BLPOC function provides a much higher discrimination capability than the
original POC function in FKP recognition. This can be reﬂected in the matching
examples shown in Fig. 7.3. Figure 7.3a, b are two FKP ROI images from the same
ﬁnger (captured indifferent collection sessions), whose POC function and BLPOC
function are shown in Fig. 7.3c, d, respectively; Fig. 7.3e, f are two FKPROI images
from different ﬁngers, whose POC function and BLPOC function are shown in
Fig. 7.3 Examples of a genuine matching and an imposter matching using POC and BLPOC,
respectively: (a) and (b) are two FKPROI images from the same ﬁnger, (c) is their POC function
and (d) is their BLPOC function; (e) and (f) are two FKPROI images from different ﬁngers, (g) is
their POC function and (h) is their BLPOC function
140
7
Global Information for Finger-Knuckle-Print Recognition

Fig. 7.3g, h, respectively. These examples indicate that in the case of a genuine
matching (a matching performed between a pair of FKP images from the same
ﬁnger), the BLPOC will exhibit a much sharper peak than POC; however, for an
imposter matching (a matching performed between a pair of FKP images from
different ﬁngers), neither BLPOC nor POC will show a distinct sharp peak. Hence,
in this chapter, we adopt the BLPOC to align the displacement between FKP ROI
images and then to measure the similarity between Fourier transforms of the aligned
ROIs.
7.4
Local-Global Information Combination
In this section, we present our local-global information combination (LGIC) based
FKP recognition algorithm. The entire process of our LGIC-based FKP matching is
illustrated in Fig. 7.4. Given two FKPROI images f and g, the following four steps
will be taken to compute their similarity.
Step 1: Translation Alignment by Global Features with BLPOC
Although the FKP image acquisition device and the ROI extraction algorithm can
reduce the geometric transformations between intra-class ROIs much, it is still
inevitable that there is some displacement between intra-class ROIs. This will
weaken the genuine matching scores. In our previous coding-based works (Zhang
et al. 2009a, b, 2010), this problem was addressed by translating one set of features
in horizontal and vertical directions several times and the minimum of the resulting
matching distances was considered to be the ﬁnal matching distance. In this chapter,
we solve this problem in a different way by evaluating the translation parameters
between the two ROIs using the BLPOC function. Then we crop the common
regions, based on which the feature matching is performed. The translation param-
eters (t1, t2) between f and g can be estimated from the peak location of the global
BLPOC of them. Then, we can align f and g based on (t1, t2) and extract the
common regions fc and gc. It should be noted that in our system, we will check
the ratio between the common region area and the area of the original ROI. If area
( fC)/area( f ) < t (or area(gC)/area(g) < t) where t is a threshold, fc and gc will be
simply set as f and g. Generally, this will happen when the two FKP images are from
different ﬁngers, i.e. inter-classes.
Step 2: Local Feature Extraction and Matching
After alignment and common area cropping, two CompCode maps Cf and Cg are
constructed from fc and gc. Then, by Cf matching and Cg, we could get the matching
distance dL. For technical details at this step, please refer to Sect. 7.2.
Step 3: Global Feature Extraction and Matching
We use the peak value of the BLPOC function pU0V0
f CgC between fc and gc to measure
the similarity of their Fourier transforms. Denote by pocS the peak value of pU0V0
f CgC ,
then the matching distance is deﬁned as: dG ¼ 1  pocS.
7.4
Local-Global Information Combination
141
www.ebook3000.com

Step 4: Fusion of Matching Distances
Until now, two matching distances dL and dG have been obtained. These two
distances can be fused together to get the ﬁnal matching distance. There are a
couple of rules for the fusion of matching distances, such as the Simple-Sum
Fig. 7.4 Illustration for the matching distance computation between a pair of FKP ROI images
with LGIC
142
7
Global Information for Finger-Knuckle-Print Recognition

(SS) rule, the MIn-Score (MIS) rule, the MAx-Score(MAS) rule, and the Matcher-
Weighting (MW) rule (Snelick et al. 2005). In our case, dL and dG can be considered
to be obtained from two different matchers, matcher 1 (local feature based matcher)
and matcher 2 (global feature based matcher), and we adopt the MW rule. With the
MW fusion rule, weights are assigned according to the equal error rate (EER)
obtained on a training data set by different matchers. Denote by ek the EER of the
matcher k, k ¼ 1, 2.Then, the weight k associated with matcher k can be calculated
as
wk ¼
1
 X
2
k¼1
1
ek
 
!
ek
ð7:15Þ
where 0  wk  1 and P2
k¼1 wk ¼ 1. It is obvious that the weights are inversely
proportional to the corresponding EERs. Then, the ﬁnal matching distance is
calculated as
d ¼ w1dL þ w2dG
ð7:16Þ
7.5
Experimental Results
7.5.1
FKP Database and Test Protocol
In our previous work (Zhang et al. 2009a, b, 2010), an FKP database was
established using the developed FKP image acquisition device. This database is
intended to be a benchmark to evaluate the performance of various FKP recognition
methods, and it is available at (PolyU Finger–Knuckle-Print Database 2010). In this
database, FKP images were collected from 165 volunteers, including 125 males and
40 females. Among them, 143 subjects were 20–30 years old and the others were
30–50 years old. We collected samples in two separate sessions. In each session, the
subject was asked to provide 6 images for each of the left index ﬁnger, the left
middle ﬁnger, the right index ﬁnger and the right middle ﬁnger. Therefore,
48 images from 4 ﬁngers were collected from each subject. In total, the database
contains 7920 images from 660 different ﬁngers. The average time interval between
the ﬁrst and the second sessions was about 25 days. The maximum and minimum
time intervals were 96 days and 14 days, respectively. In all of the following
experiments, we took images collected at the ﬁrst session as the gallery set and
images collected at the second session as the probe set. To obtain statistical results,
each image in the probe set was matched with all the images in the gallery set. If the
two images were from the same ﬁnger, the matching between them was counted as
a genuine matching; otherwise it was counted as an imposter matching.
7.5
Experimental Results
143
www.ebook3000.com

The EER, which is the point where the false accept rate (FAR) is equal to the
false reject rate (FRR), is used to evaluate the veriﬁcation accuracy. The decidabil-
ity index d0 (Daugman 2003) is used to measure how well the genuine and the
imposter distributions are separated. d0 is deﬁned as
d0 ¼
μ1  μ2
j
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1 þ σ2
2

=2
q
ð7:17Þ
where μ1 (μ2) is the mean of the genuine (imposter) matching distances and σ1(σ2) is
the standard deviation of the genuine (imposter) matching distances.
7.5.2
Determination of Parameters
In real implementation, parameters need to be determined for LGIC. To this end,
we tuned the parameters based on a sub-dataset, which contained the ﬁrst 300 FKP
images. Parameters for the local feature and the global feature we retuned sepa-
rately. Three parameters σx, σy and f need to be tuned for the local feature while two
parameters U0/M0 and V0/N0 need to be tuned for the global feature. The tuning
criterion was that parameter values that could lead to a lower EER would be chosen.
As a result, the parameters used in this chapter were set as: σx ¼ 5.0, σy ¼ 9.0,
f ¼ 0.0435, U0/M0 ¼ 0.25 and V0/N0 ¼ 0.2. Moreover, two fusion weights w1 and
w2 can be calculated using Eq. (7.15).
7.5.3
FKP Veriﬁcation Results
Veriﬁcation aims to answer the question of “whether the person is the one he/she
claims to be”. In this experiment, all the classes of FKPs were involved. Therefore,
there were 660(1654) classes and 3960(6606) images in the gallery set and the
probe set each. Each image in the probe set was matched against all the images in
the gallery set. Thus, the numbers of genuine matchings and imposter matchings
were 23,760 and 15,657,840, respectively. In order to show its superiority, the
proposed LGIC was compared with the other three state-of-the-art FKP veriﬁcation
methods, CompCode (Zhang et al. 2009a, b), BLPOC (Zhang et al. 2009a, b) and
ImCompCode&MagCode (Zhang et al. 2010). Some optimizations have been made
on ROI extraction and matching, so the experimental results for Comp- Code,
ImCompCode&MagCode and BLPOC are better than the previous publications.
The results in terms of the EER and d0 are summarized in Table 7.1. In addition, the
FRRs for each algorithm obtained with a ﬁxed FAR¼1.1  103 are also presented
144
7
Global Information for Finger-Knuckle-Print Recognition

in Table 7.1 for comparison .Furthermore, by adjusting the matching threshold, a
detection error trade off (DET) curve (Martin et al. 1997), which is a plot of false
reject rate (FRR) against false accept rate (FAR) for all possible thresholds, can be
created. The DET curve can reﬂect the overall veriﬁcation accuracy of a biometric
system. Figure 7.5a shows the DET curves generated by the four different FKP
veriﬁcation schemes. Distance distributions of genuine matching sand imposter
matchings obtained by the proposed LGIC scheme are plotted in Fig. 7.5b.
Table 7.2 lists ﬁve typical operating states obtained by using the LGIC scheme.
From the results listed in Table 7.1 and the DET curves shown in Fig. 7.5a, we
can see that the proposed LGIC scheme performs signiﬁcantly better in terms of the
veriﬁcation accuracy than the other state-of-the-art FKP veriﬁcation methods
evaluated, including CompCode, BLPOC and ImCompCode&MagCode. As stated,
the local orientation information or the local magnitude information is used in
CompCode and ImCompCode&MagCode, so they can be classiﬁed as local-based
methods. By contrast, in BLPOC, the Fourier transform of the whole image is taken
as the feature so it is actually a global-based method. Therefore, the experimental
results also corroborate the claim that methods fusing local and global information
together can outperform the methods depending on only a speciﬁc kind of features,
local or global.
7.5.4
Speed
The FKP recognition software is implemented using Visual C#.Net 2005 on a Dell
Inspiron 530s PC embedded Intel E6550 processor and 2 GB of RAM. Computation
time for the key processes is listed in Table 7.3. The execution time for data
preprocessing and ROI extraction is 198 ms. The time for BLPOC-based translation
alignment is about 1.4 ms. The time for competitive coding is 60 ms. The time for
calculating dL and dG is 0.3 and 2.1 ms, respectively. Thus, the total execution time
for one veriﬁcation operation is less than 0.5 s in our prototype system, which is fast
enough for real-time applications. We believe that with the optimization of the
implementation, the system’s efﬁciency could be much further improved.
Table 7.1 Performance comparison of different FKP veriﬁcation schemes
EER(%)
d
0
FRR(%)(when
FAR¼1.1  103)
CompCode (Zhang et al. 2009a, b)
1.658
4.2989
3.4848
BLPOC (Zhang et al. 2009a, b)
1.676
2.4745
8.5939
ImCompCode&MagCode (Zhang et al. 2010)
1.475
4.3224
3.0818
LGIC
0.402
4.5356
0.9680
7.5
Experimental Results
145
www.ebook3000.com

Fig. 7.5 (a) DET curves obtained by the four FKP recognition methods; (b) distance distributions
of genuine matching sand imposter matchings with the proposed scheme LGIC
Table 7.2 Typical operating
states using LGIC
FAR (%)
FRR (%)
0.0515
1.5236
0.1068
0.9680
0.2148
0.6439
0.3396
0.4461
0.5982
0.2819
146
7
Global Information for Finger-Knuckle-Print Recognition

7.6
Summary
In this chapter, a novel local–global information combination (LGIC) based FKP
recognition method was proposed. It is based on the fact that both local and global
features are crucial for the image recognition and perception and they play different
and complementary roles in such a process. In LGIC, the local orientation extracted
by the Gabor ﬁlters based competitive coding scheme was taken as the local feature.
From the perspective of time–frequency analysis, when the scale of the Gabor
transform goes to inﬁnity, it degenerates to the Fourier transform. Thus, the Fourier
transform was naturally taken as the global feature in our work. LGIC exploits both
local and global features for FKP veriﬁcation, where the global features were also
used to reﬁne the alignment of FKP images in matching. Extensive experimental
results conducted on our FKP database indicate that the proposed scheme could
achieve much better performance in terms of EER and the decidability index than
the other state-of- the-art competitors. Speciﬁcally, the EER of LGIC is 0.402% and
it can operate at a low FRR of 1.5236% with a low FAR of 0.0515% on our FKP
database.
References
Aguilar J, Chen Y, Garcia J, Jain A (2006) Incorporating image quality in multi-algorithm
ﬁngerprint veriﬁcation. Proc ICB 3832:213–220
Bracewell R (1965) The fourier transform and its applications. McGraw-Hill, New York
Daugman J (2003) The importance of being random: statistical principles of iris recognition.
Pattern Recogn 36(2):279–291
Fang Y, Tan T, Wang Y (2002) Fusion of global and local features for face veriﬁcation. Proc ICPR
2:382–385
Gabor D (1946) Theory of communication. J Inst Electr Eng 93(1946):429–457
Gu J, Zhou J, Yang C (2006) Fingerprint recognition by combining global structure and local cues.
IEEE Trans Image Process 15(7):1952–1964
Ito K, Nakajima H, Kobayashi K, Aoki T, Higuchi T (2004) A ﬁngerprint matching algorithm
using phase-only correlation. IEICE Trans Fundam E87-A(3):682–691
Ito K, Aoki T, Nakajima H, Kobayashi K, Higuchi T (2008) A palmprint recognition algorithm
using phase-only correlation. IEICE Trans Fundam E91-A(4):1023–1030
Jain A, Ross A, Pankanti S (1999) A prototype hand geometry-based veriﬁcation system. In:
Proceedings of the AVBPA 1999, pp 166–171
Jain A, Flynn P, Ross A (2007) Handbook of biometrics. Springer, New York
Table 7.3 Computation time
for key processes
Operations
Time (ms)
ROI extraction
198
Translation alignment
1.4
Competitive coding
60
Calculation of dL
0.3
Calculation of dG
2.1
References
147
www.ebook3000.com

Jia W, Huang D, Zhang D (2008) Palmprint veriﬁcation based on robust line orientation code.
Pattern Recogn 41(5):1504–1513
Kong A (2008) An evaluation of Gabor orientation as a feature for face recognition. In: Pro-
ceedings of the ICPR 2008, pp 1–4
Kong W, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. Proc ICPR
1:520–523
Kuglin C, Hines D (1975) The phase correlation image alignment method. In: Proceedings of the
international conference on cybernetics and society’75, pp 163–165
Kumar A, Ravikanth C (2009) Personal authentication using ﬁnger knuckle surface. IEEE Trans
Inf Forensics Secur 4(1):98–109
Kumar A, Zhou Y (2009) Personal identiﬁcation using ﬁnger knuckle orientation features.
Electron Lett 45(20):1023–1025
Lee T (1996) Image representation using 2D Gabor wavelet. IEEE Trans Pattern Anal Mach Intell
18(10):957–971
Li Q, Qiu Z, Sun D, Wu J (2004) Personal identiﬁcation using knuckleprint. In: Proceedings of
SinoBiometrics 2004, pp 680–689
Li F, Leung K, Yu X (2007) A two-level matching scheme for speedy and accurate palmprint
identiﬁcation. In: Proceedings of the MMM 2007, pp 323–332
Maltoni D, Maio D, Jain A, Prabhakar S (2003) Handbook of ﬁngerprint recognition. Springer,
New York
Martin A, Doddington G, Kamm T, Ordowski M, Przybocki M (1997) The DET curve in
assessment of detection task performance. In: Proceedings of the Eurospeech 1997, pp
1895–1898
Miyazawa K, Ito K, Aoki T, Kobayashi K, Nakajima H (2008) An effective approach for iris
recognition using phase-based image matching. IEEE Trans Pattern Anal Mach Intell 20
(10):1741–1756
Nanni L, Lumini A (2009) A multi-matcher system based on knuckle-based features. Neural
Comput Appl 18(1):87–91
Nikam S, Goel P, Tapadar R, Agarwal S (2007) Combining Gabor local texture pattern and wavelet
global features for ﬁngerprint matching. In: Proceedings of the ICCIMA 2007, pp 409–416
Pan X, Ruan Q, Wang Y (2007) Palmprint recognition using fusion of local and global features. In:
Proceedings of the ISPACS 2007, pp 642–645
Pinto N, Cox D, Corda B, Doukhan D (2008) Why is real-world object recognition hard?
Establishing honest benchmarks and baselines for object recognition. Proceedings of the
COSYNE 2008
Pinto N, DiCarlo J, Cox D (2009), How far can you get with a modern face recognition test set
using only simple features? In: Proceedings of the CVPR 2009, pp 2591–2598
PolyU Finger–Knuckle-Print Database (2010). http://www.comp.polyu.edu.hk/~biometrics/FKP.
htm
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, New York
Reddy B, Chatterji B (1996) An FFT-based technique for translation, rotation, and scale-invariant
image registration. IEEE Trans Image Process 5(8):1266–1271
Sanchez-Reillo R, Sanchez-Avila C, Gonzalez-Marcos A (2000) Biometric identiﬁcation through
hand geometry measurements. IEEE Trans Pattern Anal Mach Intell 22(10):1168–1171
Snelick R, Uludag U, Mink A, Indovina M, Jain A (2005) Large-scale evaluation of multimodal
biometric authentication using state-of-the-art systems. IEEE Trans Pattern Anal Mach Intell
27(3):450–455
Su Y, Shan S, Chen X, Gao W (2009) Hierarchical ensemble of global and local classiﬁers for face
recognition. IEEE Trans Image Process 18(8):1885–1896
Sun Z, Wang Y, Tan T, Cui J (2004) Cascading statistical and structural classiﬁers for iris
recognition. In: Proceedings of the ICPR 2004, pp 1261–1262
Sun Z, Wang Y, Tan T, Cui J (2005) Improving iris recognition accuracy via cascaded classiﬁers.
IEEE Trans Syst Man Cybern Part C 35(3):435–441
148
7
Global Information for Finger-Knuckle-Print Recognition

Wang J, Yau W, Suwandy A, Sung E (2008) Personal recognition by fusing palmprint and palm
vein images based on “Laplacianpalm” representation. Pattern Recogn 41(5):1531–1544
Woodard D, Flynn P (2005) Finger surface as a biometric identiﬁer. Comput Vis Image Underst
100(3):357–384
Zayed A (1996) Handbook of function and generalized function transformations. CRC Press, Boca
Raton
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhang L, Zhang L, Zhang D (2009a), Finger–knuckle-print: a new biometric identiﬁer. In: Pro-
ceedings of the ICIP 2009, pp 1981–1984
Zhang L, Zhang L, Zhang D (2009b) Finger–knuckle-print veriﬁcation based on band-limited
phase-only correlation. In: Proceedings of the CAIP 2009, pp 141–148
Zhang L, Zhang L, Zhang D, Zhu H (2010) Online ﬁnger–knuckle-print veriﬁcation for personal
authentication. Pattern Recogn 43(7):2560–2571
References
149
www.ebook3000.com

Chapter 8
Finger-Knuckle-Print Veriﬁcation with Score
Level Adaptive Binary Fusion
Abstract Recently, a new biometrics identiﬁer, namely ﬁnger knuckle print
(FKP), has been proposed for personal authentication with very interesting results.
One of the advantages of FKP veriﬁcation lies in its user friendliness in data
collection. However, the user ﬂexibility in positioning ﬁngers also leads to a certain
degree of pose variations in the collected query FKP images. The widely used
Gabor ﬁltering based competitive coding scheme is sensitive to such variations,
resulting in many false rejections. We propose to alleviate this problem by
reconstructing the query sample with a dictionary learned from the template
samples in the gallery set. The reconstructed FKP image can reduce much the
enlarged matching distance caused by ﬁnger pose variations; however, both the
intra-class and inter-class distances will be reduced. We then propose a score level
adaptive binary fusion rule to adaptively fuse the matching distances before and
after reconstruction, aiming to reduce the false rejections without increasing much
the false acceptances. Experimental results on the benchmark PolyU FKP database
show that the proposed method signiﬁcantly improves the FKP veriﬁcation
accuracy.
Keywords Biometrics • Finger-knuckle-print • Reconstruction • Score level fusion
8.1
Introduction
Refer to Chap. 5, although a triangular block is used to control the ﬁnger freedom in
FKP image acquisition, there is still much ﬂexibility for the users to position their
ﬁngers. This is good to increase the user friendliness but also allows much variation
of the ﬁnger pose in query sample collection process. Figure 8.1 shows some
examples. We can see obvious deformations between the two FKP samples due
to the ﬁnger pose variations. Unfortunately, the CompCode and LFI based FKP
recognition methods are sensitive to such variations, resulting in false rejections
and degrading the FKP veriﬁcation performance.
From the above discussions, we can see that the main difﬁculty in FKP recog-
nition is the false rejections caused by ﬁnger-pose-variation in the query samples.
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_8
151

One strategy to solve this problem is to correct the pose deformations by afﬁne
transformations. However, estimating the afﬁne transformation parameters is itself
a very difﬁcult problem, particularly for FKP images where very few distinctive
key points can be extracted. Since our ultimate goal is FKP veriﬁcation but not
pose deformation correction, another strategy is to enhance the FKP matching
process without pose deformation correction. Considering that the ﬁnger pose
variation caused FKP image deformation enlarges the matching distance between
two FKP images from the same person and hence results in false rejections, we
propose a reconstruction based matching scheme to reduce the enlarged matching
distance.
First, a dictionary is learned from the template FKP images, and this dictionary
deﬁnes the subspace of the gallery FKP dataset. For a given query sample which
may have pose variation, we represent it as the linear combination of the atoms in
the learned dictionary. This process actually projects the query sample onto the
subspace spanned by the gallery FKP images. The CompCode scheme can then be
applied to the reconstructed image for feature extraction and matching. Nonethe-
less, the reconstruction of the query sample will not only reduce the intra-class
distance, but also reduce the interclass distance. In other words, it can reduce false
rejections but may also increase the false acceptances. To effectively exploit the
discriminative information of the query sample before and after reconstruction, a
simple yet powerful score level adaptive binary fusion (ABF) rule is proposed to
make the ﬁnal decision by fusing the matching scores before and after reconstruc-
tion. The ABF ensures a good reduction of false rejections without increasing much
the false acceptances, leading to much lower equal error rates than state-of-the-art
methods.
Fig. 8.1 (a) A template FKP sample and (b) two testing FKP samples from the same ﬁnger but
collected at different sessions. Obvious pose variations can be observed from the samples in (b)
152
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

8.2
Competitive Coding Based FKP Veriﬁcation
Gabor ﬁltering has been widely used as an effective feature extraction technique in
face, iris, ﬁngerprint, palmprint, as well as FKP recognition systems. A 2D Gabor
ﬁlter can be mathematically expressed as
G n; m
ð
Þ ¼ exp 1
2
n2
θ
σ2
n
þ m2
θ
σ2
m




 exp i2πfnθ
ð
Þ
ð8:1Þ
where nθ ¼ n  cos θ + m  sin θ, mθ ¼  n  sin θ + m  cos θ, f is the frequency of the
sinusoid factor, θ is the orientation of the normal to the parallel stripes of the Gabor
function, σn and σm are the standard deviations of the 2D Gaussian envelop.
Based on the observation that the FKP images contain abundant line-like
structures, the orientation features can be extracted for FKP image recognition.
Let’s denote by GR the real part of a Gabor ﬁlter, and by IROI an FKP ROI (region
of interest) image. With a bank of Gabor ﬁlters, at each pixel IROI(n, m), the
CompCode scheme (Kong and Zhang 2004; Zhang et al. 2009a, b, c; Zhang et al.
2010) extracts and codes the dominant orientation feature as follows:
CompCode n; m
ð
Þ ¼ argmin
j
IROI n; m
ð
Þ∗GR n; m; θj




ð8:2Þ
where symbol “*” denotes the convolution operation, and θj ¼ jπ/6, j ¼ {0, . . . , 5}.
CompCode(n, m) is assigned the orientation along which the smallest response is
obtained.
In order for real-time recognition, CompCode uses three bits to represent each
orientation (Kong and Zhang 2004). For matching two CompCode maps Pand Q,
the normalized Hamming distance based angular distance is commonly adopted
(Kong and Zhang 2004):
sh ¼
PRows
n¼1
PCols
m¼1
P2
i¼0 Pi n; m
ð
Þ  Qi n; m
ð
Þ
ð
Þ
3S
ð8:3Þ
where S is the area of the code map, Pi(Qi) is the ith bit plane of P(Q), and 
represent the bitwise “exclusive OR” operation. In practice, multiple matches are
performed by translating one of the two feature maps vertically and horizontally,
and the minimum matching distance is regarded as the ﬁnal angular distance.
Nonetheless, the CompCode scheme is sensitive to the variation of FKP image
and then resulting in false rejection. Even a small rotation and misalignment can
lead to an incorrect matching.
8.2
Competitive Coding Based FKP Veriﬁcation
153

8.3
Recognition via Reconstruction
8.3.1
Motivation
The CompCode scheme is simple and fast, and it leads to acceptable accuracy in
FKP veriﬁcation (Zhang et al. 2009a, b, c, 2010). If the query FKP image is well
aligned after ROI extraction, the CompCode scheme can work very well. As we
discussed in the introduction section, however, there can be certain degree of
variations of the ﬁnger pose in the data collection process (refer to Fig. 8.1 please),
which lead to deformations in the FKP images and consequently result in false
rejections because CompCode is sensitive to image deformations.
Figure 8.2 shows the genuine and imposter matching distance distributions of a
typical FKP veriﬁcation system. Based on the curves, it is obvious that we can
divide the matching distance into two parts: an uncertain interval, which is between
thresholds t1 and t2, and a conﬁdence interval, which covers the remaining part.
Generally speaking, if the matching distance of two FKP feature maps falls into the
conﬁdence interval, it can be easily decided if the query sample is a genuine or an
imposter, while most of the false acceptance sand false rejections occur when the
matching distance falls into the uncertain interval [ t1, t2]. On the other hand, the
ﬁnger pose variation is the main cause that increases the intra-class distance (see the
long tail of the blue genuine curve in Fig. 8.2), making the false rejections happen.
The FKP veriﬁcation accuracy can be improved if we could correct the ﬁnger-
pose-variation caused deformations of query samples via afﬁne transformation.
Unfortunately, it is a particularly difﬁcult problem to estimate the afﬁne transfor-
mation parameters for FKP images because very few distinctive key points can be
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0.55
0
0.5
1
1.5
2
2.5
Matching Distance
Percentage(%)
Genuine
Imposter
t2
t1
Confidence  interval
Confidence  interval
Uncertain
interval
Fig. 8.2 The distribution of matching distance of a typical FKP veriﬁcation system
154
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

extracted from them. Therefore, this solution is impractical. In this chapter, we
propose to reduce the pose variation caused false rejections by enhancing the
matching process without pose deformation correction.
Denote by X ¼ [x1, x2, . . . , xk] the set of gallery FKP samples, where xi is a
vectorized FKP sample. We can use the hull of X, denoted by H(X) ¼ {X  w}, where
w ¼ [w1, w2, . . . , wk]T is the vector of weights, to characterize the subspace of
gallery FKP images. If the gallery set is big enough, we can reasonably assume
that each regular FKP image will fall into the hull H(X), i.e., it can be well
represented as the linear combination of the template samples in X. For a query
image y, we can project it into the hull H(X), and rewrite it as by ¼ X  bw, where
weight bw is determined by minimizing the distance between y and its projection bw
(usually some regularization will be imposed on bw), and e is the projection residual.
For a regular query image, there is no much ﬁnger pose variation caused
deformation in it, and the projection residuale will be very small (i.e., kek2 is
very small). In such case, the query sample y can be accurately recognized by
using the efﬁcient CompCode scheme. However, if the ﬁnger pose varies much in
the data acquisition process, the query sample y can have much deformation and
fall outside the hull H(X), i.e., kek2 becomes much bigger. In this case, the
CompCode scheme can fail since it is sensitive to image deformations, which can
be reﬂected by the big values of kek2.
Let’s plot the distribution of kek2 by using the PolyU FKP database (PolyU
Palmprint Database 2006). We use the ﬁrst six samples of each of the 660 classes in
the PolyU FKP database to construct the gallery dataset X, and take the remaining
six samples of each class as the query samples. In projecting each query sample into
the hull H(X), we use the l2-norm regularized least square to compute the weight:
bw ¼ argminw y  X  w
k
k2
2 þ λ w
k k2
2. Then eis computed as
e ¼ y  X  bw. In
Fig. 8.3 we plot the distribution of kek2 by using all the 660  6 ¼ 3960 query
samples. We can see that the distribution has a long tail, which is mainly caused by
those samples with large deformations. Since it is mainly the big residual e that
makes the CompCode feature of query sample ydeviates much from the CompCode
features of gallery samples in X, one intuitive idea is that we can compute the
CompCode of by ¼ X  bw for veriﬁcation.
Note that we use all the gallery samples, not only the samples from the class that
y claims, to compute by. There are two reasons for such a conﬁguration. First, the
number of samples of each class is usually small (e.g., six samples per class in the
gallery set of PolyU FKP database), and thus reconstructing y using only the
samples from one class is not accurate. Second, the query sample y can be an
imposter, i.e., it may come from a class out of the gallery set, or from a class that is
different from the class it claims. Reconstructing y using only the samples of one
class will make the distance from by to this class too small so that false acceptance
will happen. Therefore, all the classes in the gallery set should be involved in
reconstructing y.
8.3
Recognition via Reconstruction
155

8.3.2
Reconstruction with l1-Norm Sparse Regularization
By approximating y with X  w, one solution to w is the least square solution:
bw ¼ argminw y  X  w
k
k2
2. It is easy to see that bw ¼ XTX

1XTy. Though the
least square solution is simple to compute and it ensures the minimal l2-norm
reconstruction residual of by, it is not the best choice for the veriﬁcation purpose.
The least square solution aims to minimize the reconstruction residual, and the
weights w tend to be densely distributed, hence many classes in X will contribute in
reconstructing y. Finally, some discriminative features in y may be smoothed out inby.
In order to preserve the discriminative features of y in by, some regularization
term could be imposed on w. Intuitively, we hope that only a small portion of the
weights in w are signiﬁcant so that only several classes are dominantly involved to
reconstruct y. The l1-norm based sparse representation (or sparse coding) is a very
good choice to this end. In recent years sparse coding has been successfully used in
various image reconstruction and pattern classiﬁcation applications (Cande`s and
Romberg 2005; Kim et al. 2007; Aharon et al. 2006; Rubinstein et al. 2010; Yang
et al. 2010; Wright et al. 2009). It represents a given signal as a sparse linear
combination over a dictionary of atoms. By imposing the l1-norm constraint on w,
we have
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
Percentage(%)
Value of ||e||2
Fig. 8.3 The distribution of kek2
156
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

bw ¼ argminw y  X  w
k
k2
2 þ λ w
k k1
ð8:4Þ
where λ is a positive scalar balancing the reconstruction residual and the sparsity
of w. Equation (8.4) can be solved by many convex optimization algorithms such as
l1-magic (Cande`s and Romberg 2005), l1-ls (Kim et al. 2007), etc.
The sparse coding in Eq. (8.4) still has two problems. First, it is known that the
commonly used l1-minimization solvers such as l1 ls have an empirical complex-
ity of O( z2k1.3) (Kim et al. 2007), where z is the dimension of y and k is the number
of samples in X. In practice, k can be very big so that the sparse coding complexity
is high. Second, the atoms in X are the original gallery FKP images, which may
contain noise and some trivial structures that can be negative to the representation
of y.
To solve the above problems and considering the fact that the FKP images in
X have much redundancy across samples, we can learn a more compact dictionary
D from X, and then use D to code the input FKP image y. Dictionary learning has
been widely used in image processing and pattern recognition (Aharon et al. 2006;
Rubinstein et al. 2010; Yang et al. 2010, 2011; Wang et al. 2012). In this chapter,
we simply adopt the method in (Yang et al. 2010) to learn the dictionaryD. Denote
by D ¼ [d1, d2, . . . , dp], where p  k and each dj is a unit column vector. The
dictionary learning can be formulated as the following minimization problem:
JD,W ¼ argmin
X  DW
k
k2
F þ λ W
k
k
n
o
1
D, W
ð8:5Þ
Equation (8.5) is a joint optimization of dictionary D and the coefﬁcient matrix
W, and it can be solved by optimizing D and W alternatively (Yang et al. 2010).
Once the dictionary D, which has less number of atoms than X, is computed, we use
it to code the input FKP image y as follows:
bw ¼ argmin
w
y  D  w
k
k2
2 þ λ w
k k1
ð8:6Þ
Finally, the image is reconstructed as:
by ¼ D  bw
ð8:7Þ
Let’s show an example of the reconstruction results. We take 1980 images from
the ﬁrst 330 classes in the gallery set of PolyU FKP dataset asX, and learn from it a
dictionary D with 1386 atoms. D is then used to reconstruct the input image y. The
parameter λ is set as 0.1 and 2 in Eqs. (8.5) and (8.6), respectively. The left column
of Fig. 8.5 shows an input FKP image, and the second column shows the
reconstructed image using D. One can see that the reconstructed image is smoother
than the original one because some details as well as variations in the query sample
y, which cannot be represented by the learned dictionary D, are suppressed.
8.3
Recognition via Reconstruction
157

8.3.3
Reconstruction with l2-Norm Regularization
In Eq. (8.6), the l1-norm sparsity constraint is imposed on the coding coefﬁcients to
enforce that only a small portion of the atoms are dominantly used to reconstruct y.
However, l1-minimization is time consuming. Though many fast l1-minimization
solvers such as FISTA (Beck and Teboulle 2009), ALM (Yang et al. 2011) and
Homotopy (Malioutove et al. 2005) have been developed, they may not be fast and
accurate enough for practical use in the application of FKP veriﬁcation, where real
time implementation is expected. One intuitive solution is to relax the strong l1-
regularization to the weaker l2-regularization in Eq. (8.6). The l2-regularization
offers a closed form solution to w, which can be very efﬁciently computed. Though
the resolved coefﬁcient w is not sparse any more, the l2-regularization can still
make w have a small energy. As we will see in the experimental results, the FKP
veriﬁcation accuracy by l2-regularized reconstruction is only a little lower than that
by l1-regularized reconstruction, but the computational complexity is greatly
reduced.
By using l2-norm to regularize w, the coding becomes a regularized least square
problem:
bw ¼ argmin
w
y  D  w
k
k2
2 þ λ w
k k2
2
ð8:8Þ
and a closed form solution can be readily obtained as:
bw ¼ DTD þ λ  1

1DTy
ð8:9Þ
Let P ¼ (DTD + λ  1)1DT. Clearly, Pcan be pre-calculated so that the coding
vector can be obtained by projecting y over P : bw ¼ Py. This process is very fast.
In the third column of Fig. 8.4, we show the reconstructed FKP image by l2-
regularization in Eq. (8.8). The parameter λ is set as 4. (Note that the parameter λ in
l2-regularized coding is usually set bigger than that in l1-regularized coding because
l2-regularization is much weaker than l1-regularization.) For a better illustration, we
also show in the right column of Fig. 8.4 the reconstructed FKP image without
regularization [i.e., set λ ¼ 0 in Eq. (8.8)]. It can be observed that due to the
non-sparse -regularization, more classes are involved in the reconstruction of ,
and thus the reconstructed image is smoother than that by l1-sparse regularization.
Fig. 8.4 From left column to right column: original FKP image; reconstructed FKP image by
l1 -regularization (Eq. 8.6), reconstructed FKP image by l2-regularization (Eq. 8.8), and
reconstructed FKP image without regularization (i.e., let λ ¼ 0)
158
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

This will lead to some lose of the distinctive features in the original FKP image.
Nonetheless, this is the price we should pay for the great reduction in time
complexity. In the section of experimental results, we will see that the reconstructed
image by l2-regularization can still lead to quite competitive veriﬁcation accuracy.
It can also be observed from Fig. 8.4 that the reconstructed image without regular-
ization is the smoothest and more distinctive features are lost. Our experiments also
show that the veriﬁcation accuracy by the reconstructed image without regulariza-
tion is lower than that with, l2-regularization1 which validates that l2-regularization
is very helpful for veriﬁcation. Note that the complexity of reconstruction with l2-
regularization is the same as that without l2-regularization.
8.3.4
Patch Based Reconstruction
In Sects. 8.3.2 and 8.3.3, we stretch the whole FKP image as a vector y for coding
and reconstruction. In coding y over D by Eq. (8.6) or Eq. (8.8), we actually enforce
that all the elements (i.e., all the pixels) in , denoted by yi, i ¼ 1 , 2 , . . . , n, share the
same coding vector w over their corresponding sub dictionary (i.e., the ith row of
D). The good side of such a global coding strategy is that the solution is very stable
because only one global coding vector needs to be solved. The bad side of such a
coding strategy, however, lies in its less ﬂexibility because it does not allow the
different parts of the FKP image to have different coding vectors. Considering the
fact that different portions of a query FKP sample y may have different variations, it
is reasonable to allow them to have different coding vectors so that the reconstruc-
tion can be spatially adaptive. Therefore, we can partition the FKP image into
several patches, reconstruct separately each patch, and then combine them to obtain
the whole reconstructed image.
If we partition the image into too many patches, the size of each patch will
become small and the sub-dictionary corresponding to each patch will tend to be
under-determined (i.e., the sub-dictionary will tend to be a fat matrix). This will
reduce the stability of the coding process, no matter l1 -regularization or l2-regular-
ization is used. Based on our experimental experience and considering the special
pattern of FKP images, we partition the FKP image (size: 110  220) into
6 overlapped patches, as illustrated in Fig. 8.5. Four patches of size 60  45 lie
in the four corners of the FKP images, and two patches of size 60  155 lie in the
middle of the image. The reason that we set two fat rectangle patches in the middle
is based on the observation that the pose variation along vertical direction has
bigger effect than that along horizontal direction on the FKP recognition accuracy.
Therefore, pay more attention to the pose variation along vertical direction in the
partition.
Let’s denote by yj, j ¼ 1 , 2 , . . . , 6, the six patches of an FKP image y. For each
patch, we can learn a dictionary Dj from the training samples by using the same
method described in Sect. 8.3.2. Then for an input query sample y, each patch yj of it
8.3
Recognition via Reconstruction
159

can be reconstructed by Dj. Similarly, both l1-regularization and l2-regularization
can be employed in the reconstruction of yj via
bw ¼ argmin
wj
yj  Dj  wj

2
2 þ λ wj


1
ð8:10Þ
and
bw ¼ argmin
wj
yj  Dj  wj

2
2 þ λ wj

2
2
ð8:11Þ
respectively. After each patch is reconstructed, the whole reconstructed image can
be obtained by combining them. For the overlapped area of neighboring patches, we
simply average the results. Similar to our discussions in Sects. 8.3.2 and 8.3.3, the
l1 -regularization in Eq. (8.10) may preserve more discrimination information, but
the l2-regularization in Eq. (8.11) is much faster.
At last, let’s use an example to illustrate the performance of patched based
reconstruction. Figure 8.6 shows a query sample y and a template image which is
from the same class of y. We reconstruct y with l2-regularization in this example.
Imagebyg is the output of global reconstruction by Eq. (8.8) (we set λ ¼ 1), whilebyp is
the output of patch based reconstruction by Eq. (8.11) (we set λ ¼ 0.1). One can see
that the patch based reconstruction can preserve more distinctive features than the
global reconstruction due to the higher ﬂexibility in coding coefﬁcients of each
patch. After extract the CompCode feature maps (refer to Sect. 8.2 please), the
matching distances between the template image and y, byg and byp are 0.3783, 0.3761
and 0.3631, respectively. One can see that the global reconstruction can reduce the
genuine matching distance, while the patched based reconstruction can further
reduce the matching distance. Though imposter matching distance will also be
reduced after reconstruction, we will see that with an adaptive binary fusion
strategy proposed in next section, more accurate veriﬁcation results can be obtained
by adaptively fusing the matching distances before and after reconstruction. In the
experimental results in Sect. 8.6, we will also see that patch based reconstruction
can lead to better veriﬁcation performance.
Fig. 8.5 The patch
partition of FKP images
160
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

8.4
Veriﬁcation by Binary Score Level Fusion
As can be observed in Figs. 8.4 and 8.6, the reconstruction can reduce the defor-
mation caused matching distance so that the intra-class matching distance can be
reduced. At the same time, however, the inter-class matching distance may also be
reduced. Figure 8.7 shows some examples. One can see that the imposter inter-class
matching distance is also reduced after the query sample is reconstructed (global
reconstruction with l2-regularization is used). What we expected is that the intra
class genuine matching distance can be reduced more than the inter-class imposter
matching distance, but there is no guarantee for such an ideal situation. If we
directly apply the CompCode scheme (Kong and Zhang 2004) to the reconstructed
image for veriﬁcation, incorrect decision can be made. To make this clear, we apply
CompCode to the original images and the reconstructed images, respectively, for
FKP veriﬁcation. The equal error rates (EER) are shown in Table 8.1, from which
we can see that using only the reconstructed images for veriﬁcation leads to worse
performance because some useful texture features are smoothed out in the
reconstructed images. For a more robust and accurate veriﬁcation, the matching
scores (or distances) of both the original image y and the reconstructed image by
should be considered for decision making.
Denote by s1 and s2 the matching distances of y and by to a gallery image,
respectively. We propose to fuse the two distance scores for ﬁnal decision making.
In the following, we ﬁrst brieﬂy review the existing popular score level fusion
Fig. 8.6 A query sample y and the reconstructed images of it with l2-regularization. byg is the
output of global reconstruction while byp is the output of patch based reconstruction. The matching
distances between a gallery image (which is from the same class as y) and y, byg and byp are 0.3783,
0.3761 and 0.3631, respectively. One can see that the global reconstruction can reduce the genuine
matching distance, while the patched based reconstruction can further reduce the matching
distance
8.4
Veriﬁcation by Binary Score Level Fusion
161

methods, and then propose a simple but very effective adaptive fusion method,
namely the adaptive binary fusion rule.
8.4.1
Popular Score Level Fusion Methods
The score level fusion is a kind of combination-based approach, where the
matching scores of individual matchers are integrated to generate a single scalar
score for ﬁnal decision making. Denote by s the fusion result of s1 and s2.Three
commonly used score level fusion methods (Snelick et al. 2005) are the simple-sum
(SS):
s ¼ s1 þ s2
ð8:12Þ
the min-score (MIN):
s ¼ min s1 ; s2
f
g
ð8:13Þ
and the max-score (MAX):
s ¼ max s1 ; s2
f
g
ð8:14Þ
The above three fusion rules do not use any additional information apart from the
matching scores s1 and s2. Another popular method is the matcher weighting
(MW) scheme (Snelick et al. 2005). The fused score is the weighted average of
the two scores:
Fig. 8.7 Examples of matching distances before and after reconstruction. (a) inter-class; (b) intra-
class
Table 8.1 FKP veriﬁcation by using only the original image and only the reconstruction image,
respectively
Images used
Original
Reconstructed
EER
1.65%
2.09%
162
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

s ¼ ω1  s1 þ ω2  s2
ð8:15Þ
where 0  ω1, ω2  1 and ω1 + ω2 ¼ 1. To determine ω1 and ω2, some prior knowl-
edge needs to be known. Often the EERs of the two matchers (i.e., matching by y
and matching byby) are used. Using some training dataset, the two EERs, denoted by
e1 and e2, can be obtained, and the weight can then be calculated as
ωj ¼
1=
X2
j¼1 1=ej


	

	

=ej
ð8:16Þ
It is obvious that the weight is inversely proportional to the corresponding EER.
A higher weight will be assigned to a more reliable (i.e., lower EER) matcher, and
vice versa.
There are also some other score level fusion formulas. For example, in (Kumar
et al. 2010) Kumar et al. proposed the exponential sum rule s ¼ Pn
j¼1 exp sj
 
ωj and
the tan-hyperbolic sum rule s ¼ Pn
j¼1 tanh sj
 
ωj. The Particle Swarm Optimization
(PSO) (Eberhart and Kennedy 2010) is employed to dynamically select the weights
ωj. Such dynamic rules may work better than the SS, MIN, MAX and MW rules
when there are multiple biometric identiﬁers for fusion, but they need to optimize
the fusion rules, weights, and decision thresholds. Overall, the PSO based dynamic
score level fusion is complex and has high computational complexity (Kumar et al.
2010).
8.4.2
Adaptive Binary Fusion
In general, the MW fusion rule works better than SS, MIN and MAX rules.
However, the MW rule has two drawbacks. First, it needs a preset training dataset
to train the weights. Second, once the weights are learned, they are applied to all
query images y and the reconstructed images by. In other words, the MW rule is not
adaptive to the input query image, limiting its performance. This phenomenon can
be seen in Fig. 8.7. In Fig. 8.7b, the reconstructed image is more reliable for
veriﬁcation, so the weight ω2 assigned to this matcher is higher (ω1 < ω2). How-
ever, the situation is opposite in Fig. 8.7a. Because the query sample is an imposter,
it is hoped that the weight ω1 assigned to the original image is higher (ω1 > ω2) so
that the ﬁnal matching distance can be larger. The MW rule cannot meet this
requirement because ω1 and ω2 are ﬁxed.
In this chapter, we propose a new fusion rule, which is adaptive to the query
image and does not need a preset training dataset. The weights ω1 and ω2 are
adaptively determined online based on the input image pair and by. The idea is as
follows. For the query image y which is claimed to belong to class c, we can
calculate its within-class and between class matching distances. Using CompCode,
those matching distances can be computed very fast by Eq. (8.3). So does for the
8.4
Veriﬁcation by Binary Score Level Fusion
163

reconstructed query image by. Then the higher weight is assigned to the matcher
whose within-class and between-class matching distances are better separated.
Denote by μ1 , w and μ1 , b the mean values of the within class and between-class
matching distances of y, respectively, and by σ1 , w and σ1 , b the standard deviations
of the within class and between-class matching distances, respectively. For both
genuine matching and imposter matching, the within class distances are calculated
between the query image and the class which it claims to belong to, and the
between-class distances are calculated between the query image and other classes.
Because of the fast speed of CompCode, this process can be implemented in less
than one second on the PolyU FKP database. For very large scale databases,
computing the between-class distance using the whole dataset can be costly. To
save cost, we can randomly select an enough number of samples from the classes
other than the claimed class to compute a good approximation of the between-class
distance.
The decidability index (Daugman 2003) can be used to measure the separability
of the distributions of within-class and between class matching distances. The
decidability index for y is calculated as follows:
d1 ¼
μ1,w  μ1,b


ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
1,w þ σ2
1,b
	

=2
r
ð8:17Þ
Similarly, we can calculate the decidability index of by and denote it asd2. A
bigger decidability index means that the within-class and between-class matching
scores can be better separated, and hence the matcher is more accurate, vice versa.
Therefore, d1 and d2 can be used to adaptively determine the weights ω1 and ω2 that
are assigned to s1 and s2. The higher the decidability index, the higher the weight.
However, designing an optimal function to map d1 and d2 to ω1 and ω2 is not a
trivial work. In this chapter, we propose to use the simple binary logic operation for
the weight determination. The so-called adaptive binary fusion (ABF) rule is
deﬁned as follows:
Adaptive Binary Fusion (ABF): If d1  d2, ω1 ¼ 1 and ω2 ¼ 0; otherwise, ω1 ¼ 0
and ω2 ¼ 1.
The above proposed ABF rule is a kind of “winner-take all” strategy and is
similar to the notion of using cohort scores for multi-biometric fusion. However,
there are clear differences between them. In cohort score based multi biometric
fusion, the fusion weight is ﬁxed for each biometric identiﬁer once the weights are
learned ofﬂine. All the users of one biometric identiﬁer share the same pre-learned
fusion weight. Instead, in the proposed ABF the fusion weight is adaptively
determined for each user online. The ABF rule is simple but ﬁts our application
very well. The reconstruction of y can only lead to two situations: the reconstructed
image by is either better or worse than y for veriﬁcation. Hence, it is reasonable and
effective to adaptively choose one of them for the ﬁnal decision making. Our
164
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

experimental results in Sect. 8.6 validate the effectiveness of the proposed
ABF rule.
8.5
Summary of the Veriﬁcation Algorithm
In practice, it is not necessary to reconstruct every input query image for veriﬁca-
tion. As shown in Fig. 8.2, if the matching distance of a query image y falls into the
conﬁdent interval, we can directly make the decision; only when the matching
distance falls into the uncertain interval [t1, t2], there construction is needed, and the
ABF rule is applied for the veriﬁcation. The proposed algorithm of reconstruction
based FKP veriﬁcation with ABF is summarized in Table 8.2. The output is the ﬁnal
matching distance s. The ﬁnal decision (accept or reject) is then made by applying a
threshold to s, as in existing FKP veriﬁcation systems (Zhang et al. 2009a, b, c,
2010, 2011, 2012).
8.6
Experimental Results
The PolyU FKP database, which can be freely downloaded at: http://www.comp.
polyu.edu.hk/~biometrics/FKP.htm, was used in our experiments. This database
contains the cropped FKP region of interest (ROI) images of four ﬁngers (the left
index ﬁnger, the left middle ﬁnger, the right index ﬁnger and the right middle
ﬁnger) from 165 persons. Each ﬁnger knuckle was acquired 12 samples in two
separated
sessions
with
6
samples
per
session,
giving
a
total
of
165  4  12 ¼ 7920 samples from 660 (i.e., 165  4) ﬁngers.
In the following experiments, the gallery set is always extracted from the ﬁrst
session while the probe set is extracted from the second session. As in (Zhang et al.
2009a, b, c, 2010, 2011, 2012), each image in the probe set was matched with all the
Table 8.2 Summary of the proposed FKP veriﬁcation algorithm
Input: The gallery dataset X and the trained dictionary D;
The query image y;
The interval [t1, t2].
Output: Matching distance s.
1. Calculate the minimal matching distance, denoted by s1 , from y to the class it claims by using
the CompCode scheme.
2. If s1 is outside [t1, t2], output s ¼ s1 and end the matching process.
3. Otherwise, reconstruct by from y by using the dictionary D.
4. Calculate the minimal matching distance, denoted by s2, from by to the class y claims by using
the CompCode scheme.
5. Calculate the decidability indices d1 and d2, and then fuse s1 and s2 using the ABF rule.
6. Output the fused matching distance s and end the matching process.
8.6
Experimental Results
165

images in the gallery set. If the two images were from the same ﬁnger, a genuine
matching was counted; otherwise, an imposter matching was counted.
8.6.1
Comparison Between Different Fusion Rules
In this section, we verify that the proposed ABF rule is more effective than the
commonly used SS, MIN, MAX and MW rules. In the experiment, the gallery set is
composed of the ﬁrst 165 ﬁngers in the PolyU FKP gallery set, and the probe set is
composed of the ﬁrst 330 ﬁngers in the PolyU FKP probe set. (Other settings of the
gallery and probe sets lead to similar conclusions.) That is, there are 165 classes out
of the gallery set. The gallery set is used to train the dictionary Dto code the samples
in the probe set.
In this experiment the l1-regularization is used in the reconstruction of query
samples. In the dictionary learning (Eq. 8.5) and sparse coding (Eq. 8.6), the
parameter λ is set as 0.1 and 0.5. The number of atoms in the learned dictionary
is 0.7 times the number of samples in the gallery set. The parameters for uncertain
interval setting is t1 ¼ 0.35 and t2 ¼ 0.39 based on our experimental experience. If
the class label of the query sample is out of the gallery set, only the imposter
matching distance will be counted when calculating EER.
The algorithm described in Table 8.2 is used to perform the FKP veriﬁcation
experiments with different fusion rules in step 5. For the MW rule, the whole
dataset is used to train the weights. The EER results by using the different fusion
rules are listed in Table 8.3. We can clearly see that the lowest EER is obtained by
the proposed ABF rule, which works much better than other rules. Even that the
MW rule uses more information with a training dataset, it is only slightly better than
the MIN, SS and MAX rules. In the following experiments, we only report the
results by using the ABF rule.
8.6.2
Experiment Settings and Parameter Selection
We compare the proposed reconstruction based FKP veriﬁcation method with ABF,
denoted by “R-ABF”, with state of-the-art FKP veriﬁcation methods, including the
improved CompCode (ImCompCode) (Zhang et al. 2010), BLPOC (Zhang et al.
2009a, b, c), LGIC (Zhang et al. 2011) and the local feature integration (LFI)
method (Zhang et al. 2012). Considering that the LGIC scheme is a combination of
the CompCode (which employs the image local orientation features) and the
BLPOC (which employs the image global Fourier transform features) methods,
Table 8.3 EER (%) values
by different fusion rules
Fusion rules
SS
MIN
MAX
MW
ABF
EER
2.22
2.07
2.33
1.99
1.10
166
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

for fair comparison we will ﬁrst compare R-ABF with ImCompCode, BLPOC and
LFI, and then couple R-ABF with BLPOC and compare it with the LGIC method.
In the proposed method, the query image reconstruction can be regularized by
either l1-norm or l2-norm, and can be done either globally or patch-by-patch.
Therefore, there are four variants of the proposed R-ABF scheme, denoted by
R-ABF-g-l1, R-ABF-g-l2, R-ABF-p-l1 and R-ABF-p-l2, respectively, where “g” is
for “global” and “p” is for “patch”. In order to evaluate the proposed R-ABF
method more comprehensively, we conduct 3 experiments with different sizes of
the gallery set (165, 330 and 660 ﬁngers, respectively). In all the experiments, the
gallery set is extracted from the FKP images collected in the ﬁrst session, while all
the FKP samples collected in the second session are used as the probe set (all the
660 ﬁngers).
There are some parameters to set in our algorithm. For the parameters in
CompCode, we adopt the settings in the original chapter (Zhang et al. 2009a, b,
c). The uncertain interval is set by letting t1 ¼ 0.35 and t2 ¼ 0.39. The thresholds
t1and t2 can also be automatically determined based on the training set. By using the
“leave-one-out” strategy, each image of one subject is matched with all the other
images in the training set to obtain the distribution of matching distances. The point
where the false acceptance rate equals to the false rejection rate is taken as the
decision threshold t, and the width of the uncertain interval (refer to Fig. 8.2) can be
set as w0. Then t1 and t2 can be determined as: t1 ¼ t  w0/2 and t2 ¼ t + w0/2 . By
using this strategy, the computed t1 and t2 are 0.34 and 0.38, respectively, which are
very close to the manually set thresholds (i.e., 0.35 and 0.39).
In the global reconstruction scheme, we set the parameter λ in dictionary
learning (i.e., in Eq. (8.5)) as 0.1 by experience. The selection of parameter λ in
dictionary learning has small inﬂuence on the ﬁnal veriﬁcation result in terms of
EER. The parameter λ for l1-regularized coding in Eq. (8.6) and -regularized coding
in Eq. (8.8) is related to the number of atoms in dictionary D. We determine it by the
following criterion: λ ¼ 0.5  (n/990)2 in l1-regularized coding and λ ¼ (n/990)2 in
l2-regularized coding, where n is the number of atoms in the dictionary. In patch
based reconstruction, the parameter λ in dictionary learning and coding is set as
1/10 of that in global reconstruction. The partition of patches is discussed in Sect.
8.6.5.
Our method needs to train a dictionary Dwith atom number p  k, where k is the
number of samples in gallery set X. If p is too small, much information contained in
the gallery sample set X can be lost and thus the test sample may not be well
represented by the learned dictionary D. On the other hand, the commonly used l1-
minmization solvers such as lI  lS have an empirical complexity of O(z2p1.3) (Kim
et al. 2007) (z is the dimension of samples). So if p is big, the computational cost
can be high. To balance the computational cost and the representation capability,
we set the ratio of p to k as 0.7 in our method, and this conﬁguration leads to
satisfying experimental results.
8.6
Experimental Results
167

8.6.3
FKP Veriﬁcation Results
1. Experiment 1: In this experiment, only the FKP images of the ﬁrst 165 (out of the
660) ﬁngers (i.e., 165  6 ¼ 990 samples) are used as the gallery set. Hence,
there are 5940 genuine matchings and 3,914,460 imposter matchings, respec-
tively. Figure 8.8a plots the DET (Detection Error Tradeoff) curves, which are
the plots of false rejection rates (FRR) against false acceptance rates (FAR) for
all possible thresholds. Table 8.4 lists the results of competing methods in terms
of EER and decidability index.
It can be seen that the R-ABF methods outperform much ImCompCode, BLPOC
and LFI. This validates that the reconstruction of query sample y can reduce
much the image deformation induced intra-class distance, and the ABF rule can
prevent the less discriminative reconstruction of y from being adopted for ﬁnal
decision making. Among the four variants of R-ABF, the l1-regularized ones
(a)
(b) 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
False Acceptance Rate(%)
False Rejection Rate(%)
ImCompCode
BLPOC
LFI
R-ABF-g-l1
R-ABF-p-l1
R-ABF-g-l2
R-ABF-p-l2
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
False Acceptance Rate(%)
False Rejection Rate(%)
ImCompCode
BLPOC
LFI
R-ABF-g-l1
R-ABF-p-l1
R-ABF-g-l2
R-ABF-p-l2
(c) 
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
False Acceptance Rate(%)
False Rejection Rate(%)
ImCompCode
BLPOC
LFI
R-ABF-g-l1
R-ABF-p-l1
R-ABF-g-l2
R-ABF-p-l2
Fig. 8.8 DET curves by different methods in (a) experiment 1; (b) experiment 2; and (c)
experiment 3
168
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

have higher accuracy than the l2-regularized ones, while the patch based ones
have higher accuracy than the global based ones. Speciﬁcally, R-ABF-p-l1
achieves the lowest EER. This is consistent with our discussions in Sect. 8.3.
Table 8.5 lists the percentage of matchings in which the score from y or the
reconstruction is selected as the ﬁnal matching score. We can see that by for
genuine matchings, about 40%  45% of the matching scores fromby are selected
by the ABF rule.
For imposter matchings, about 28%  35% of the matching scores from by are
adopted.
2. Experiment 2: In the 2nd experiment, 330 classes are involved in the gallery set.
Therefore, the numbers of genuine and imposter matchings are 11,880 and
7,828,920, respectively. Figure 8.8b shows the DET curves by the different
veriﬁcation schemes, while the EER values and decidability indices are listed
in Table 8.4. The percentage of matchings in which the score from the recon-
structionby is selected is listed in Table 8.5. Again, the proposed R-ABF methods
get much better results than ImCompCode, BLPOC and LFI.
3. Experiment 3: At last, all the classes (i.e., all ﬁngers) are involved in the gallery
set, and the numbers of genuine and imposter matchings are 23,760 and
15,657,840, respectively. The DET curves by different veriﬁcation schemes
are illustrated in Fig. 8.8c. Table 8.4 lists the EER values and decidability
indices, and Table 8.5 lists the percentage of matchings in which the score
from the reconstruction by is selected. Similar conclusions to the previous two
experiments can be made.
It can be seen that the EER decreases from Experiment 1 to Experiment 2, and
increases from Experiment 2 to Experiment 3. The reason can be as follows. From
Experiment 1 to Experiment 3, the number of gallery classes is increasing. The
increased number of gallery samples makes the dictionary Dmore capable to
Table 8.4 EER(%) values and decidability indices by the competing methods
Method
Experiment 1
Experiment 2
Experiment 3
EER
(%)
Decidability
index
EER
(%)
Decidability
index
EER
(%)
Decidability
index
ImCompCode (Zhang
et al. 2010)
1.62
4.1478
1.28
4.4518
1.39
4.4302
BLPOC (Zhang et al.
2009a, b, c)
1.44
4.1925
1.35
3.4311
1.44
3.4218
LFI (Zhang et al.
2012)
1.32
4.1951
1.03
4.4821
1.13
4.4625
R-ABF-g-l1
1.24
4.2291
0.84
4.5699
0.94
4.5305
R-ABF-p-l1
1.12
4.2309
0.76
4.6385
0.82
4.2572
R-ABF-g-l2
1.27
4.1948
0.91
4.5014
1.19
4.1882
R-ABF-p-l2
1.28
4.2170
0.82
4.6188
0.92
4.2461
8.6
Experimental Results
169

reconstruct the query sample, but it also makes the veriﬁcation tasks more chal-
lenging. There are only 165 gallery classes in Experiment 1, so the learned
dictionary D may not be representative enough to reconstruct the query sample.
The number of gallery classes is increased to 330 in Experiment 2, and the
representativeness of D is much improved so that the FAR and FRR are decreased
simultaneously. As a consequence, the overall EER in Experiment 2 is reduced.
With 330 gallery classes, the representativeness of learned dictionary D is already
good. Thus, the beneﬁt of using 660 gallery classes in Experiment 3 is not big in
term of learning dictionaryD ; however, the FAR and FRR are increased simulta-
neously due to the increased number of gallery classes, resulting in a bigger EER
than Experiment 2.
8.6.4
Integrating with Global Features
The LGIC scheme (Zhang et al. 2011) combines CompCode, which employs the
image local orientation features, and BLPOC, which employs the global Fourier
transform features. In this section, for fair comparison, we also combine RABF,
which basically employs the image local orientation features, with the BLPOC
method in the same way as that in (Zhang et al. 2011). According to the recon-
struction strategy, we denote the fused methods as R-ABF-g-l1 + BLPOC,
R-ABF-g-l2 + BLPOC, R-ABF-p-l1 + BLPOC and R-ABF-p-l1 + BLPOC, respec-
tively. We compare the performance of LGIC with the proposed methods under the
same experiment settings as described in Sect. 8.6.3. The combination of LFI and
BLPOC is also used for more comprehensive comparison. The DET curves of the
competing methods are shown in Fig. 8.9, and the EER values and decidability
indices are summarized in Table 8.6.
From the experimental results in both Sects. 8.6.3 and 8.6.4, we can see that the
proposed R-ABF scheme leads to state-of-the-art veriﬁcation accuracy, no matter
using only the local orientation feature or using both local and global features.
Speciﬁcally, the R-ABF-p-l1 and R-ABF-p-l1 + BLPOCmethods achieve the best
accuracy, respectively. By relaxing the l1-regularized sparsity constraint in the
reconstruction, the l2-regularized reconstruction can also lead to very competitive
veriﬁcation results but with much less complexity, which is a good solution in
practical FKP recognition systems.
Table 8.5 The percentage (%) of matchings in which the score from y or by is selected
Matching type
Experiment 1
Experiment 2
Experiment 3
ω1 ¼ 1
ω2 ¼ 1
ω1 ¼ 1
ω2 ¼ 1
ω1 ¼ 1
ω2 ¼ 1
Genuine
60
40
55
45
57
43
Imposter
71.59
28.41
65.36
34.61
68.74
31.26
170
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

8.6.5
Discussions
In the proposed method, we learn a dictionary D from the gallery set to reconstruct a
query sample. When a new subject is enrolled, we can update D by solving
Eq. (8.5). However, if the dataset has a large scale, this can be very costly.
Fortunately, it is not necessary to update D for a new enrollment in large-scale
dataset.
The dictionary D in our algorithm is just used for reconstruction, and it is not
used in the classiﬁcation stage. This is very different from the works in (Yang et al.
2010, 2011; Wright et al. 2009), where the atoms in the dictionary have class labels
and they will be used to calculate the class-speciﬁc distances for classiﬁcation. The
role of dictionary Din our work is similar to the dictionaries in image restoration
such as K-SVD (Aharon et al. 2006). There are no class labels of the atoms in D,
and D is a universal dictionary shared by all classes. Once enough gallery classes
are involved in learning the dictionary D, this D will be able to well represent any
FKP image. Therefore, when there are some new enrollments, we actually do not
need to update the dictionary. Kindly note that the online learning algorithms in
(Masip et al. 2009) and (Singh et al. 2010) aim for updating classiﬁers when new
individuals are enrolled. The problem is very different from ours.
(a)  
(b) 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
False Acceptance Rate(%)
False Rejection Rate(%)
LGIC
LFI+BLPOC
R-ABF-g-l1+BLPOC
R-ABF-p-l1+BLPOC
R-ABF-g-l2+BLPOC
R-ABF-p-l2+BLPOC
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
False Acceptance Rate(%)
False Rejection Rate(%)
LGIC
LFI+BLPOC
R-ABF-g-l1+BLPOC
R-ABF-p-l1+BLPOC
R-ABF-g-l2+BLPOC
R-ABF-p-l2+BLPOC
(c) 
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
False Acceptance Rate(%)
False Rejection Rate(%)
LGIC
LFI+BLPOC
R-ABF-g-l1+BLPOC
R-ABF-p-l1+BLPOC
R-ABF-g-l2+BLPOC
R-ABF-p-l2+BLPOC
Fig. 8.9 DET curves by the fused methods in (a) experiment 1; (b) experiment 2; and (c)
experiment 3
8.6
Experimental Results
171

Let us use two experiments to validate the above statement. We use the ﬁrst
600 classes out of the 660 classes in the PolyU FKP database as the gallery set
(6 samples per class in the ﬁrst session) to learn a dictionary, denoted by D1. We
then use all the 660 classes to learn another dictionary, denoted by D2. In the ﬁrst
experiment, we take D1 as the dictionary and take the 60 new classes (6 samples per
class) as the query set, and the EER is 0.8% by the proposed R-ABF-g- l2 method. If
we take as the dictionary D2 and take the 60 same classes as the query set, the EER
is 0.76% by R-ABF-g- l2 In the second experiment, with D1 and using all the
660 classes as the query set (6 samples per class in the second session), the EER
is 1.21% by R-ABF-g- l2, while with D2 and using all the 660 classes as the query
set, the EER is 1.19% by R-ABFg- l2. Clearly, in both the two experiments, the EER
values by D1 and D2 are very close, implying that there is no necessary to further
update the dictionary since D1 is already good in reconstruction.
If we do want to update the dictionary D when new enrollments come, there are
two strategies to save cost. First, we can update D once a batch of new enrollments
is available. Second, we can let the new dictionary be Dnew ¼ [ DoDa], where Do is
the old dictionary and Da includes the new atoms to be added. In this way, we only
need to learn the several new atoms by using dictionary learning algorithms such as
(Yang et al. 2010).
8.7
Summary
This chapter presented a novel reconstruction based ﬁnger knuckle-print (FKP)
veriﬁcation method to reduce the false rejections caused by ﬁnger pose variations in
data collection process. For an input query image whose matching distance falls
into the uncertain interval, we reconstructed a new version of it by using a
dictionary learned from the gallery set. Then a new matching distance can be
Table 8.6 EER (%)values and decidability indices by different methods
Method
Experiment 1
Experiment 2
Experiment 3
EER
(%)
Decidability
index
EER
(%)
Decidability
index
EER
(%)
Decidability
index
LGIC (Zhang
et al.)
0.38
4.3882
0.39
4.6182
0.41
4.4302
LFI+BLPOC
0.33
4.3946
0.35
4.6378
0.35
4.6635
R-ABF-g-
l1 + BLPOC
0.29
4.4197
0.25
4.6886
0.29
4.6940
R-ABF-p-
l1 + BLPOC
0.25
4.4150
0.21
4.7479
0.22
4.4823
R-ABF-g-
l2 + BLPOC
0.29
4.4048
0.29
4.5937
0.33
4.4641
R-ABF-p-
l2 + BLPOC
0.27
4.4076
0.26
4.7354
0.26
4.4764
172
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

obtained. An adaptive binary fusion (ABF) rule was then proposed to fuse the two
matching distances for the ﬁnal decision making. The proposed reconstruction
based FKP veriﬁcation with ABF, denoted by R-ABF, can effectively reduce the
false rejections without increasing much the false acceptances. Our extensive
experimental results demonstrated that the R-ABF can result in much lower equal
error rate than existing state-of-the-art methods.
References
Aharon M, Elad M, Bruckstein A (2006) K-SVD: an algorithm for designing over complete
dictionaries for sparse representation. IEEE Trans Signal Process 54(11):4311–4322
Beck A, Teboulle M (2009) A fast iterative shrinkage-thresholding algorithm for linear inverse
problems. SIAM J Imag Sci 2(1):183–202
Cande`s E, Romberg J (2005) 1-Magic: recovery of sparse signals via convex programming
[Online]. http://www.acm.caltech.edu/l1magic/
Daugman J (2003) The importance of being random: statistical principles of iris recognition.
Pattern Recogn 36(2):279–291
Eberhart R, Kennedy J (2010) Swarm intelligence. Morgan Kaufmann, San Diego, CA
Kim S, Koh K, Lustig M, Boyd S, Gorinevsky D (2007) An interior point method for large-scale
l1-regularized least squares. IEEE J Sel Topics Signal Process 1(4):606–617
Kong W, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. In: Proceedings
of IEEE international conference on pattern recognition, vol 1, pp 520–523
Kumar A, Kanhangad V, Zhang D (2010) A new framework for adaptive multimodal biometrics
management. IEEE Trans Inf Forensics Secur 5(1):92–102
Malioutove D, Cetin M, Willsky A (2005) Homotopy continuation for sparse signal representa-
tion. In: Proceedings of IEEE international conference on acoustics, speech signal processing,
vol 5, pp 733–736
Masip D, Lapedriza A, Vitria J (2009) Boosted online learning for face recognition. IEEE Trans
Syst Man Cybernet B Cybernet 39(2):530–538
Rubinstein R, Bruckstein A, Elad M (2010) Dictionaries for sparse representation modeling. Proc
IEEE 98(6):1045–1057
Singh R, Vatsa M, Ross A, Noore A (2010) Biometric classiﬁer update using online learning: a
case study in near infrared face veriﬁcation. Image Vis Comput 28(7):1098–1105
Snelick R, Uludag U, Mink A, Indovina M, Jain A (2005) Large-scale evaluation of multimodal
biometric authentication using state-of-the-art systems. IEEE Trans Pattern Anal Mach Intell
27(3):450–455
Wang S, Zhang L, Liang Y, Pan Q (2012) Semi-coupled dictionary learning with applications to
image super-resolution and photo-sketch image synthesis. Proc IEEE Conf Comput Vis Pattern
Recogn:2216–2223
Wright J, Yang A, Ganesh A, Sastry S, Ma Y (2009) Robust face recognition via sparse
representation. IEEE Trans Pattern Anal Mach Intell 31(2):210–227
Yang J, Zhang Y (2011) Alternating direction algorithms for_1-problems in compressive sensing.
SIAM J Sci Comput 33(1):250–278
Yang M, Zhang L, Yang J, Zhang D (2010) Metaface learning for sparse representation based face
recognition. In: Proceedings of IEEE international conference on image processing, pp
1601–1604
Yang M, Zhang L, Feng X, Zhang D (2011) Fisher discrimination dictionary learning for sparse
representation. In: Proceedings of IEEE international conference computer vision, pp 543–550
References
173

Zhang D, Lu G, Li W, Zhang L, Luo N (2009a) Palmprint recognition using 3-D information.
IEEE Trans Syst Man Cybern C Appl Rev 39(5):505–519
Zhang L, Zhang L, Zhang D (2009b) Finger-knuckle-print: a new biometric identiﬁer. In: Pro-
ceedings of 16th IEEE international conference on image processing, pp 1981–1984
Zhang L, Zhang L, Zhang D (2009c) Finger-knuckle-print veriﬁcation based on band-limited
phase-only correlation. In: Proceedings of international conference on computer analysis of
images patterns, pp 141–148
Zhang L, Zhang L, Zhang D, Zhu H (2010) Online ﬁnger-knuckle print veriﬁcation for personal
authentication. Pattern Recogn 43(7):2560–2571
Zhang L, Zhang L, Zhang D, Zhu H (2011) Ensemble of local and global information for ﬁnger-
knuckle-print recognition. Pattern Recogn 44(9):1990–1998
Zhang L, Zhang L, Zhang D, Guo Z (2012) Phase congruency induced local features for ﬁnger-
knuckle-print recognition. Pattern Recogn 45(7):2522–2531
174
8
Finger-Knuckle-Print Veriﬁcation with Score Level Adaptive Binary Fusion
www.ebook3000.com

Part III
Other Hand-Based Biometrics

Chapter 9
3D Fingerprint Reconstruction
and Recognition
Abstract The chapter studies a 3D ﬁngerprint reconstruction technique based on
multi-view touchless ﬁngerprint images. This technique offers a solution for 3D
ﬁngerprint image generation and application when only multi-view 2D images are
available. However, the difﬁculties and stresses of 3D ﬁngerprint reconstruction are
the establishment of feature correspondences based on 2D touchless ﬁngerprint
images and the estimation of the ﬁnger shape model. In this chapter, several popular
used features, such as scale invariant feature transformation (SIFT) feature, ridge
feature and minutiae, are employed for correspondences establishment. To extract
these ﬁngerprint features accurately, an improved ﬁngerprint enhancement method
has been proposed by polishing orientation and ridge frequency maps according to
the characteristics of 2D touchless ﬁngerprint images. Therefore, correspondences
can be established by adopting hierarchical ﬁngerprint matching approaches.
Through an analysis of 440 3D point cloud ﬁnger data (220 ﬁngers, 2 pictures
each) collected by a 3D scanning technique, i.e., the structured light illumination
(SLI) method, the ﬁnger shape model is estimated. It is found that the binary
quadratic function is more suitable for the ﬁnger shape model than the other
mixed model tested in this chapter. In our experiments, the reconstruction accuracy
is illustrated by constructing a cylinder. Furthermore, results obtained from differ-
ent ﬁngerprint feature correspondences are analyzed and compared to show which
features are more suitable for 3D ﬁngerprint images generation.
Keywords 3D ﬁngerprint reconstruction • Finger shape model • Finger-print
features correspondences • Orientation map • Frequency map • Touch-less multi-
view imaging
9.1
Introduction
As one of the most widely used biometrics, ﬁngerprints have been investigated for
more than a century (Maltoni et al. 2009). Advanced Automated Fingerprint
Recognition Systems (AFRSs) are available in the market everywhere and most
of them capture ﬁngerprint images by using the touch-based technique, since it is
easy to obtain images with high ridge-valley contrast. However, the touch-based
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_9
177
www.ebook3000.com

imaging technique introduces distortions and inconsistencies to the images due to
the contact of ﬁnger skin with device surface. In addition, the curved 3D ﬁnger
surface ﬂattens into 2D plane during image acquisition, destroying the 3D nature of
ﬁngers. To deal with these problems, 3D ﬁngerprint imaging techniques start to be
considered (Parziale and Diaz-Santana 2006; Hartley 2000; Hernandez et al. 2008;
Blais et al. 1988; Wang et al. 2010; Stockman et al. 1988; Hu and Stockman 1989).
Usually, these techniques capture ﬁngerprint images at a distance and provide the
3D ﬁnger shape feature simultaneously. The advent of these techniques brings new
challenges and opportunities to existing AFRSs.
Currently, there are three kinds of popular 3D imaging techniques: multi-view
reconstruction (Parziale and Diaz-Santana 2006; Hartley 2000) and (Hernandez
et al. 2008), laser scanning (Blais et al. 1988; Rusinkiewicz et al. 2002) and
(Bradley et al. 2002), and structured light scanning (Wang et al. 2010; Stockman
et al. 1988; Hu and Stockman 1989). Among them, the multi-view reconstruction
technique has the advantage of low cost but the disadvantage of low accuracy. Laser
scanning normally achieves high resolution 3D images but costs too much and the
collecting time is long (Blais et al. 1988; Rusinkiewicz et al. 2002; Bradley et al.
2002). As mentioned in (Bradley et al. 2002), the currently available commercial
3D scanning systems cost from $2500 to $240,000USD. The time of scanning a
turtle ﬁgurine (18 cm long) is from 4 to 30 min for different scanners (Rusinkiewicz
et al. 2002). The status (wet or dry) of objects also affects the accuracy of 3D
images due to surface reﬂection. The wetter the surface is, the lower the accuracy
will be (Blais et al. 1988). Different from the multi-view reconstruction and laser
scanning, structured light imaging has high accuracy as well as a moderate cost.
However, it also takes much time to collect 3D data and suffers from the instability
problem such that one needs to keep still when it projects some structured light
patterns to the human ﬁnger (Wang et al. 2010; Stockman et al. 1988; Hu and
Stockman 1989). Thus, it is necessary and important to study the reconstruction
technique based on multi-view 2D ﬁngerprint images when considering the cost,
friendliness, as well as the complexity of device design. It is well known that the 3D
spatial coordinates of an object are available from its two different plane pictures
captured at one time according to binocular stereo vision theory, if some camera
parameters and the corresponding matched pairs are provided (Hartley 2000). In
(Parziale and Diaz-Santana 2006), the authors brieﬂy introduce the 3D reconstruc-
tion method since it is the same as those methods used to reconstruct any other type
of 3D objects. There are several drawbacks with adopting general methods for 3D
ﬁngerprint reconstruction. For instance, it is time-consuming for the reason that the
coordinate of each pixel needs to be calculated. Only the 3D coordinates of
correspondences which represent the same portion of the skin between a pair of
neighbor images can be calculated. 3D visualization of ﬁnger is unavailable, if
correspondences cannot be found between two neighbor images.
To overcome the disadvantages mentioned above, a new 3D ﬁngerprint recon-
struction system using feature correspondences and the prior estimated ﬁnger
model is proposed in this chapter. Comparative little research has been carried
out into touchless ﬁngerprint matching due to the characteristics of touchless
ﬁngerprint imaging, and hardly any work can be found for ﬁnger shape model
178
9
3D Fingerprint Reconstruction and Recognition

analyses. This chapter for the ﬁrst time analyzes touchless ﬁngerprint features for
correspondences establishment and studies the model of human ﬁnger shape. 3D
ﬁngerprints are then reconstructed based on the images captured by a touchless
multi-view ﬁngerprint imaging device designed by us (Liu et al. 2013). Figure 9.1
shows the schematic diagram of our designed acquisition device and an example of
2D ﬁngerprint images. Finally, 3D ﬁngerprint reconstruction results based on
different feature correspondences are given and compared with those based on
manually labeled correspondences. It is concluded that such reconstruction results
are helpful to 3D ﬁngerprint recognition.
The chapter is organized as follows. In Sect. 9.2, the imaging device and the
procedure of the proposed 3D ﬁngerprint reconstruction system are brieﬂy intro-
duced. Sect. 9.3 is devoted to presenting the methods proposed to establish ﬁnger-
print feature correspondences. The approach to estimating the ﬁnger shape model is
described in Sect. 9.4. The feature extraction and matching method are introduced
in Sect. 9.5. Experimental results and the reconstructing error analysis are given in
Sect. 9.4. Section 9.4 concludes the chapter and indicates the future work.
Finger
a
b
LED
Lens
Camera
Fig. 9.1 Device and captured touchless multi-view ﬁngerprint images. (a) Schematic diagram of
our designed touchless multi-view ﬁngerprint acquisition device, (b) images of a ﬁnger captured
by the device (left, frontal, right)
9.1
Introduction
179
www.ebook3000.com

9.2
3D Fingerprint Reconstruction System
Before reconstruction, multi-view ﬁngerprint images need to be provided. The
images used in this chapter are captured by the touchless multi-view ﬁngerprint
acquisition device designed by us. The schematic diagram of the acquisition device
is shown in Fig. 9.1a. One central camera and two side cameras are focused on the
ﬁnger. Four blue LEDs are used to light the ﬁnger and arranged to give uniform
brightness. A hole is designed to place the ﬁnger in a ﬁxed position. All of the three
cameras are JAI CV-A50. The lens focal length is 12 mm and the object-to-lens
distance is set to 91 mm due to the consideration of image quality and device size.
The angle between the central camera and the side cameras is roughly 30. The
image size of each channel is restricted to 576  768 pixels and the resolution of the
image is 400 dpi. The three view images of a ﬁnger captured by the device are
shown in Fig. 9.1b.
According to the theory of binocular stereo vision in computer vision domain
(Hartley 2000), the 3D information of an object can be obtained from its two
different plane pictures captured at one time. As shown in Fig. 9.2a, given two
images and Cr captured at one time, the 3D coordinate of A can be Cl calculated if
some camera parameters (e.g., focal length of the left camera fl, focal length of the
right camera fr, principal point of the left camera Ol, principal point of the right
camera Or) and the matched pair (al(ul, vl)) $ (ar(ur, vr)), where a∗(∗) represents a
2D point in the given images Cr or Cl; u* is the column-axis of the 2D image, and v*
is the row-axis of the 2D image) are provided. Once the shape model and several
calculated 3D coordinates of the 3D object are known, the shape of the 3D object
can be obtained after interpolation. As can be seen in Fig. 9.2b, the triangle in 3D
space is obtained after computing 3D coordinates of three vertices and interpolating
according to a triangle model. Therefore, the reconstruction method is divided into
ﬁve
steps,
including
the
camera
parameters
calculation,
correspondences
Y
O
Z
al(ul, vl)
ul
vr
fr
nr
ur
or
ol
Cl
Ol
Cr
Or
nl
vl
fl
ar(ur, vr)
X
∏A
A(X, Y, Z)
a
b
Fig. 9.2 An illustration of constructing a 3D triangle based on binocular stereo vision. (a) 3D
coordinates calculation on 3D space, (b) 3D triangle reconstruction
180
9
3D Fingerprint Reconstruction and Recognition

establishment, 3D coordinates computation, shape model estimation, and interpo-
lation. The ﬂow chart of the reconstruction system in this chapter is shown in
Fig. 9.3.
Camera calibration is the ﬁrst step for 3D reconstruction. It provides the intrinsic
parameters (Focal Length, Principal Point, Skew, and Distortion) of each camera
and extrinsic parameters (Rotation, Translation) between cameras necessary for
reconstruction. It is usually implemented off-line. In this chapter, the methodology
proposed in (Zhang 2000) and the improved algorithms coded by Bouguet (Bouguet
online) are employed. The free codes can be obtained from the website (Bouguet
online). It can be noted that there are three cameras used in our ﬁngerprint capturing
device. The position of the middle camera is chosen as the reference system,
because the central part of the ﬁngerprint is more likely to be captured by this
camera, where the core and the delta are usually located. The frontal image
captured by the middle camera is also selected as the texture image when the
ﬁnal 3D ﬁngerprint image is generated. To ensure that the frontal view of ﬁnger
is captured by the middle camera of the device, a simple guide is given for users to
correctly use the device.
Correspondences establishment is of great importance to the 3D reconstruction
accuracy. It will be introduced in detail in Sect. 9.3.
Once camera parameters and matched pairs between ﬁngerprint images of
different views are both obtained, the 3D coordinate of each correspondence can
be calculated by using the stereo triangulation method (Bouguet online).
Since it is very hard to identify all of the correspondences which represent the
same portion of the skin between two neighboring ﬁngerprint image pairs, it is very
important to calculate the 3D ﬁnger shape for 3D ﬁngerprint visualization. This
chapter for the ﬁrst time analyzes ﬁnger shape models. The details will be presented
in Sect. 9.4.
Based on the calculated 3D coordinates of limited feature correspondences and
the estimated shape model, a 3D ﬁnger shape can be ﬁnally reconstructed by
interpolation. Here, the classical approach, namely, multiple linear regression
using least squares (Chatterjee and Hadi 1986; Draper and Smith 1981), is adopted
for interpolation because of its simplicity and effectiveness.
Touchless Multi-view
Imaging Device
Camera Parameters
Calculation
3D Coordinates
Computation
Interpolation
Shape Model
Estimation
Correspondences
Establishment
2D Images
Reconstruction
Result
Fig. 9.3 The ﬂow chart of our reconstruction system
9.2
3D Fingerprint Reconstruction System
181
www.ebook3000.com

9.3
Fingerprint Feature Correspondences Establishment
Fingerprints are distinguished by their features. Different ﬁngerprint features can be
observed from different resolution ﬁngerprint images. There are three frequently-used
features for low resolution ﬁngerprint images, namely Scale Invariant Feature Trans-
formation (SIFT) feature, ridge map and minutiae (Choi et al. 2010; Zhang et al. 2011;
Kumar and Zhou 2011; Park et al. 2008; Feng 2008; Malathi and Meena 2010; Jain and
Ross 2002; Shah et al. 2005; Choi et al. 2007). This chapter thus tries to extract such
features and establish correspondences between different views of ﬁngerprint images.
9.3.1
Correspondences Establishment Based on SIFT
SIFT (Lowe 2004) is popular in object recognition and image retrieval, since it is
robust to low quality image. For touchless ﬁngerprint images, they have the charac-
teristic of low ridge–valley contrast. This feature makes true correspondences possi-
ble to be established when minutiae and ridge features cannot be correctly extracted.
Moreover, it is robust to deformation variation and rich in quantity (Park et al. 2008;
Malathi and Meena 2010). Figure 9.4b, d illustrate the extracted 1911 and 1524 SIFT
features, respectively. 108 pairs are matched by using the point wise matching
method to Fig. 9.4a, c, as shown in Fig. 9.4e. From Fig. 9.4e, we can see that there
exist false correspondences and hence reﬁned algorithms are needed to be employed
to select true ones. To this end, the classical RANSAC algorithm, which is insensitive
to initial alignment and outliers (Fishler and Bolles 1981) is utilized. It should be
noted that the TPS model which is popularly used in ﬁngerprint domains (Choi et al.
2010; shah et al. 2005; Ross et al. 2005) is adopted in the RANSAC algorithm due to
the curved surface of ﬁnger and distortions introduced by cameras. Figure 9.4f gives
the ﬁnal selected true correspondences when RANSAC with the TPS model acts on
the initial correspondences of Fig. 9.4e.
9.3.2
Correspondences Establishment Based on Ridge Map
Before establishing correspondences between ridge maps, ridges must be extracted
and recorded. In general, ridge map refers to the thinning image where ridges are
one-pixel-width, and ridge pixels have value 1 and background pixels have value
0. Figure 9.5 shows the ﬂowchart of steps for ridge map extraction. However,
touchless ﬁngerprint images have low ridge–valley contrast and their ridge frequency
increases from center to side, as shown in Fig. 9.4a, c. These make it difﬁcult to extract
the ridge map accurately due to the difﬁculty of ﬁngerprint enhancement. Currently,
there are a number of ﬁngerprint enhancement approaches, such as Gabor ﬁlter-based,
STFT-based, DCT-based and Diffusion ﬁlter-based methods (Hong et al. 1998;
182
9
3D Fingerprint Reconstruction and Recognition

Chikkerur et al. 2007; Jirachaweng and Areekul 2007; Weichert 1999; Chen and Dong
2006; Hao and Yuan 2004; Hastings 2007; Almansa and Lindeberg 2000; Xie and
Wang 2004). Among them, the Gabor ﬁlter based method is the simplest and the most
traditional one. It is ﬁnally adopted in this chapter. Fingerprint images are enhanced by
a bank of Gabor ﬁlters generated from given ﬁngerprint orientation and frequency.
Fig. 9.4 Example of correspondences establishment based on SIFT features. (a) Original frontal
image, (b) extracted SIFT feature from (a), (c) original left-side image, (d) extracted SIFT feature
from (c), (e) initial correspondences established by point wise matching, (f) ﬁnal correspondences
after reﬁning by the RANSAC method
9.3
Fingerprint Feature Correspondences Establishment
183
www.ebook3000.com

Orientation and frequency maps play an important role in the enhancement approach.
This chapter thus tries to improve the orientation map and frequency map so as to
acquire better enhanced results.
As introduced in (Maltoni et al. 2009), the gradient-based ridge orientation
estimation method is the simplest and most intuitive one. It is efﬁcient and popularly
used in ﬁngerprint recognition studies. However, it also has some drawbacks, such as
sensitivity to noise when orientation is estimated at too ﬁne a scale, low accuracy when
smooth factors are used to the orientation map, as shown in Fig. 9.6a (lower rectangle)
and Fig. 9.6b (right rectangle). To keep the estimation accuracy of a good quality area
and correct the orientation where noises exist, a method is proposed to act on original
orientation map to improve the orientation map. The main steps include: (i) part the
original orientation map into eight uniform regions. Small blocks in the uniform
regions represent the wrong estimated orientation results (see Fig. 9.7a, in the red
circles); (ii) sort uniform regions with the same color in a descending manner; such
regions whose size is smaller than the mean size of all regions with the same color are
Fingerprint
image
Pre-process
Enhancement
Post-process
Binarization
Thinning
Ridge
map
Orientation
estimation
Ridge
frequency
estimation
Gabor filtering
ROI extraction
Image
normalization
Fig. 9.5 Flowchart of ridge map extraction
Fig. 9.6 Fingerprint ridge orientation maps. (a) Original orientation map, (b) smoothed orienta-
tion map of (a), (c) improved orientation map by our proposed method
184
9
3D Fingerprint Reconstruction and Recognition

set to zero (see Fig. 9.7b, the dark regions in ROI); (iii) assign values to the points with
zero value set by step (ii) according to the nearest neighbor method. The improved
orientation map is obtained by following these three steps. Figure 9.6c shows the
improved orientation map based on Fig. 9.6a, and Fig. 9.7c gives the partition map
according to Fig. 9.6c. The results show that the estimation accuracy of a good quality
area is kept and the wrong orientation area is corrected (Fig. 9.6c, rectangle).
Frequency maps record the number of ridges per unit length along a hypothetical
segment and orthogonal to the local ridge orientation. The simplest and most popular
ridge frequency estimation method is the x-signatures based method (Maltoni et al.
2009). However, this kind of method does not work with blurry or noisy ﬁngerprint
areas. In this situation, interpolation and ﬁltering is used to post-process the original
estimated frequency map. For touchless ﬁngerprint images, frequency maps are harder
to estimate than touch-based ﬁngerprint images due to the low ridge–valley contrast of
touchless ﬁngerprint images, and simple interpolation or ﬁltering is invalid when the
frequency is wrongly estimated in neighborhoods. By observingtheridges on touchless
ﬁngerprint images, we ﬁnd their frequency increases from the central part to the side
part for horizontal section and decreases from the ﬁngertip to the distal interphalangeal
crease for the vertical section, as shown in Fig. 9.8 (ridge frequency is calculated with
blocks of 32  32 pixels). This phenomenon can be explained from the touchless
capturing technique and the observation of the human ﬁnger. As shown in Eq. (9.1), M
is the optical magniﬁcation. p and q are the lens-to-object and lens-to-image distances,
respectively. For a ﬁx q, a large p will lead to a small magniﬁcation M. Figure 9.9
illustrates three different values of p. It can be seen that the distance from the side parts
to the lens (i.e.,D2 or D3) is larger than the distance from the central part to the lens (i.e.,
D1), which leads to smaller M on the side parts than on the central part. The smaller the
magniﬁcationM is, the larger the ridge frequency will be.Thus,it is larger in thecentral
part of the ridge period than side-view ones for the horizontal section. The vertical
distribution of ridge period increases from the ﬁngertip to the distal interphalangeal
Fig. 9.7 Partition results according to orientation map. (a) Partition result according to original
orientation map, (b) partition result according to our improved orientation map
9.3
Fingerprint Feature Correspondences Establishment
185
www.ebook3000.com

crease, because p increases from the tip to the center part of the ﬁnger and the ridges are
wider near the distal interphalangeal crease than the other parts by observation.
M ¼ q
p
ð9:1Þ
According to the distribution of ridge frequency of touchless ﬁngerprint images,
this chapter proposes to use monotone increasing function (logarithmic function) to
ﬁt the ridge period (1/ridge frequency) map along the vertical direction and
quadratic curve along the horizontal direction. The improved ridge period map is
ﬁnally achieved by ﬁtting original ridge period map with a mixed model of
logarithmic function and quadratic curve.
Once the orientation and ridge frequency maps are calculated, a series of Gabor
ﬁlter can be generated based on them. The enhanced ﬁngerprint image was then
obtained, as shown in Fig. 9.10. After binarizing the enhanced ﬁngerprint image by
simple threshold and morphology approaches, the ﬁnal ridge map is acquired.
Fig. 9.8 Frequency variation of touchless ﬁngerprint images. (a) Original touchless ﬁngerprint
image and (b) corresponding frequency map
D2
D1
D3
Fig. 9.9 Distance between lens and different parts of the ﬁnger
186
9
3D Fingerprint Reconstruction and Recognition

Figure 9.10a, b shows the ridge maps of Fig. 9.4a enhanced by using the original
orientation map and the original ridge frequency map interpolated by mean value of the
frequency map. Figure 9.10c, d shows the enhanced ridge maps of Fig. 9.4a using the
improved orientation and ridge frequency maps. Better results by using the improved
orientation and ridge frequency maps are achieved when comparing Fig. 9.10c, d with
a, b (labeled in rectangles). It should be noticed that the pre-process steps of ROI
extraction and normalization are the same as those proposed in (Liu et al. 2013).
Before correspondences establishment, ridges are recorded at tracing starting
from minutiae where ridges are disconnected. Due to the existence of noise, a ridge
image often has some spurs and breaks. In some cases of insigniﬁcant noise, the
ridge structure can be correctly recovered by removing short ridges or connecting
broken ridges. However, in other cases of strong noise, it is difﬁcult to recover the
correct ridge structure by removing short ridges or connecting broken ridges. In
such cases, we remove all related ridges. Finally, the down sampled ridge point
coordinates of each ridge are recorded in a list.
Coarse alignment of two ridge maps is done by using the global transform model
calculated in Sect. 9.3.1 when SIFT features matched. Ridges in ridge maps are then
matched by adopting the Dynamic Programming (DP) method. As shown in Fig. 9.11
and Table 9.1, {a1, a2, .. . ,a10} represents a ridge line in the template ridge map and
{b1,b2, . . . ,b8} denotes a ridge line in the test ridge map. For any ridge in template and
test ridge maps, the Euclidian distance between each pair of compared ridge lines is
Fig. 9.10 Ridge maps. (a) Ridge map of Fig. 9.4a enhanced by using original orientation and
ridge frequency maps, (b) thinned ridge map of (a), (c) ridge map of Fig. 9.4a enhanced by using
improved orientation and ridge frequency maps, (d) thinned ridge map of (c)
a1
a2
a3
a4
b1
b2
b3
b4
b5
a8
a9
a10
a5
a6
a7
b6
b7
b8
Fig. 9.11 Correspondences establishment between two ridges
9.3
Fingerprint Feature Correspondences Establishment
187
www.ebook3000.com

calculated. The status will be 1 if the distance of a pair of ridge points is smaller than a
threshold (it is set to ﬁve points in this chapter), otherwise, the status will be 0. The DP
method is adopted to ﬁnd matched ridge pairs with the largest number. Coarse ridge
correspondences are then established after DP. RANSAC algorithm introduced in Sect.
9.3.1 isthen adopted to select true ones from the coarse set. Figure 9.12 shows the results
of the established ridge correspondences.
9.3.3
Correspondences Establishment Based on Minutiae
Due to their distinctive ability, minutiae are widely used for ﬁngerprint recognition
and also considered in the chapter. They are extracted from the ridge map calcu-
lated in Sect. 9.3.2. An example of extracted minutiae using the method introduced
in (Jain et al. 1997) is shown in Fig. 9.13.
Since the transformation model is obtained when SIFT correspondences are
established, minutiae sets can be coarsely aligned by the calculated transformation
model. Then, initial minutiae correspondences are established by the nearest neigh-
bor method, and the ﬁnal result is achieved by the RANSAC algorithm with a TPS
model. This kind of minutiae correspondences establishment is demonstrated in
Fig. 9.14.
Table 9.1 Record of status
among ridge points in
Fig. 9.11
a1
a2
a3
a4
a5
a6
a7
a8
a9
a10
b1
0
0
0
0
0
0
0
0
0
0
b2
0
0
0
0
0
0
0
0
0
0
b3
0
0
0
0
1
0
0
0
0
0
b4
0
0
0
0
0
1
1
0
0
0
b5
0
0
0
0
0
0
1
1
0
0
b6
0
0
0
0
0
0
0
1
0
0
b7
0
0
0
0
0
0
0
0
1
0
b8
0
0
0
0
0
0
0
0
0
0
Fig. 9.12 Ridge correspondences establishment. (a) Initial correspondences and (b) ﬁnal corre-
spondences after RANSAC
188
9
3D Fingerprint Reconstruction and Recognition

9.4
Finger Shape Model Estimation
To reconstruct the ﬁnger shape, it is necessary to know the shape model after certain
3D points of the ﬁnger are calculated. Unfortunately, exact model for human’s
ﬁnger shape is not directly available, and hence, it should be estimated. To this end,
we propose to estimate the ﬁnger shape model by analyzing 440 3D point cloud data
collected from human ﬁngers (220 ﬁngers, 2 pictures each) in this chapter. The 3D
point cloud data are deﬁned as the depth information of each point on the ﬁnger.
They are collected by a camera together with a projector using the Structured Light
Illumination (SLI) method (Wang et al. 2010; Zhang et al. 2010). The structure
diagram of the collection device is shown in Fig. 9.15. 13 structured light stripes
generated by a computer are projected onto the ﬁnger surface by using the Liquid
Crystal Display (LCD) projector. The camera then captures the ﬁngerprint images
formed with projected stripes on it. 3D point cloud data, which consists of depth
information of each point on the ﬁnger, can be calculated using transition and phase
expansion techniques (Saldner and Huntley 1997). Since this technique is well
studied and proved to acquire 3D depth information of each point on the ﬁnger with
high accuracy (Wang et al. 2010; Stockman et al. 1988; Hu and Stockman 1989;
Zhang et al. 2009, 2010; Saldner and Huntley 1997), 3D point cloud data obtained
using this technique are taken as the ground truth of the human ﬁnger to build the
database for ﬁnger shape model estimation.
Fig. 9.13 Example of
minutiae extraction result
9.4
Finger Shape Model Estimation
189
www.ebook3000.com

Figure 9.16a displays an example of 3D point cloud data we collected from a
thumb. We randomly selected and drew the horizontal proﬁle and the vertical
proﬁle of the 3D point cloud data, as shown in Fig. 9.17 (thick rugged line). The
horizontal proﬁle is in a parabola-like shape, as shown in Fig. 9.17a, while the
vertical proﬁle can be represented by a quadratic curve or a logarithmic function
(see Fig. 9.17b). Thus, both of the binary quadratic function
f 1 x; y
ð
Þ ¼ Ax2 þ By2 þ Cxy þ Dx þ Ey þ F
ð9:2Þ
Fig. 9.14 Minutiae
correspondences
establishment. (a) Initial
correspondences and (b)
ﬁnal correspondences after
RANSAC
Projector
Camera
Patterns
Finger Surface
Fig. 9.15 Structure
diagram of device used to
capture 3D point cloud data
of human ﬁnger
(Wang et al. 2010)
190
9
3D Fingerprint Reconstruction and Recognition

Fig. 9.16 Example 3D ﬁnger point cloud data and its ﬁtting results by different models. (a) 3D
point cloud data of a thumb, (b) ﬁtting result of (a) by binary quadratic function, (c) ﬁtting result of
(a) by a mixed model with parabola and logarithmic function
0.45
0.35
0.25
0.4
0.3
0.2
0.1
0
1.2
1
0.8
0.6
0.4
0.2
0
0
50
100 150
200
250 300
350
400
–0.2
50
100
150
200
250
Column
a
b
Row
Depth
Depth
300 350
400
450
0.15
0.05
Fig. 9.17 Randomly selected proﬁles of Fig. 9.16a. (a) Horizontal proﬁle, thick rugged line
depicts real data, thin smooth line is ﬁtting by Parabola, (b) vertical proﬁle, thick rugged line
depicts real data, thin smooth lines are ﬁtting by Quadratic Curve (closer to real data) and
logarithmic Function, respectively
9.4
Finger Shape Model Estimation
191
www.ebook3000.com

and the mixed model with parabola and logarithmic function
f 2 x; y
ð
Þ ¼ Ax2 þ By þ C ln y
ð Þ þ D
ð9:3Þ
are chosen to ﬁt all of our collected 440 3D point cloud ﬁnger data by the regression
method (Chatterjee and Hadi 1986) and (Draper and Smith 1981). Note that, in (9.2)
and (9.3), A, B, C, D, E, and F represent the coefﬁcients of the functions, x is the
variable of column-coordinate of the image, and y is the variable of row-coordinate
of the image. Figure 9.16b gives the ﬁtting result of Fig. 9.16a (denoted by V) by the
binary quadratic function (denoted by VEq:2), while Fig. 9.16c gives the ﬁtting result
of Fig. 9.16a by the mixed model (represented by VEq:3). It can be seen that binary
quadratic function is closer to the ﬁnger shape model. Therefore, binary quadratic
function in Eq. (9.2) is ﬁnally adopted in this chapter.
9.5
Curvature Features Extraction and Matching
Since our 3D ﬁngerprint image is reconstructed from multi-view ﬁngerprint images,
there is a one-to-one correspondence between the 3D points and the 2D ﬁngerprint
image pixels. Preprocessing such as ROI extraction and pose correction can be done
in 2D ﬁngerprint images, and implement into 3D situation. The ROI extraction
steps are shown in Fig. 9.18. Since the uncontrolling of ﬁngerprint image collection
(tilted of ﬁnger position, see Fig. 9.18a), pose correction is necessary. We accom-
plished it by simple rotating the original image as follows: (i) Scan each line of ROI
horizontally and ﬁnd the center point (green line in Fig. 9.18b); (ii) Fit such center
points by a line (red line in Fig. 9.18b); (iii) Calculate the angle between the ﬁtted
center line and vertical axis (θ shown in Fig. 9.18b); (iv) Rotate the image
anticlockwise by θ. Figure 9.18c shows the ﬁnal correct 2D ﬁngerprint image and
Fig. 9.18e shows the correct 3D ﬁnger shape of original 3D shape of Fig. 9.18d.
Given a corrected 3D ﬁngerprint image, stable and unique features are expected
to be extracted for the following pattern matching and recognition. 3D depth
information reﬂects the overall structure of human ﬁnger. However, there are
many invalid points in the whole 3D ﬁnger shape due to the structure of human
ﬁnger. Wrinkles and scars in ﬁnger also affect local structure of ﬁnger shape. Thus
we proposed to extract curve-skeleton of ﬁnger shape. As shown in Fig. 9.19,
Different 3D objects almost fully represented by their curve-skeletons.
Since 3D ﬁnger shape model is close to binary quadratic function, proﬁle of
horizontal section can be ﬁtted by parabola and reﬂects the changes of ﬁnger width,
while vertical proﬁle depicts variation tendency of depth from ﬁnger tip to distal
interphalangeal crease. The curve-skeleton of 3D ﬁngerprint image consists of
representative vertical and horizontal lines. Since each horizontal proﬁle is
parabola-like shape, we extracted the extreme value of each ﬁtted parabola line to
form the representative vertical line (blue line in Fig. 9.20a). Three representative
horizontal lines are selected at a certain step length (100). The distal
192
9
3D Fingerprint Reconstruction and Recognition

Fig. 9.18 Position Correction: (a) Original tilted ﬁngerprint image, (b) ROI of (a), (c) Fingerprint
image after pose correction, (d) Original 3D ﬁnger shape, (e) Corrected 3D ﬁnger shape
Fig. 9.19 Examples of curve-skeletons of different 3D objects
9.5
Curvature Features Extraction and Matching
193
www.ebook3000.com

interphalangeal crease is chosen as the base line (green line in Fig. 9.20a).
Figure 9.20b then shows the curve-skeleton we extracted from 3D ﬁnger depth
map. For overall curvatures, they can be easily calculated since our 3D ﬁnger shape
is reconstructed by model ﬁtting. The coefﬁcients of the binary quadratic function
control the maximal horizontal and vertical curvatures of 3D ﬁnger, namely the
parameters of A and B in Eq. (9.4). Thus, these two coefﬁcients of the binary
quadratic function are maintained to represent the maximal horizontal and vertical
curvatures, namely the deﬁned overall curvatures.
f x; y
ð
Þ ¼ Ax2 þ By2 þ Cxy þ Dx þ Ey þ F
ð9:4Þ
From Fig. 9.20b, we can see that curve-skeleton consists of several 3D lines.
Intuitively, the iterative closest point (ICP) algorithm is suitable to solve such
matching problem. ICP method is widely used in many 3D object recognition systems
for matching. In this chapter, we slightly modiﬁed the ICP method to measure the
distances between two sets of points. The algorithm is given below and Fig. 9.21
shows an example of matching two curve skeletons by our modiﬁed ICP method.
ICP algorithm:
1. Input: Medel point set: D1; Test point set D2;
2. Parameters initialization: stop criterion for distance Td ¼ 0.1; initial rotation
matrix R0 ¼ I; initial translation vector T0 ¼ [0 0 0]T;
3. While [new correspondences set found between D1 and D2)]
{[corr, Di] ¼ dsearchn(D1, D2);
Ki ¼ Di > Td;
Discard corr(Ki);
Update Ri, Ti;
D2 ¼ Ri*D2 + Ti;}
4. Output: distance vector D, registered D2, rigid transform parameters: R and T.
Fig. 9.20 Examples of curve-skeleton for 3D ﬁnger: (a) 3D ﬁnger shape, (b) Extracted curve-
skeleton
194
9
3D Fingerprint Reconstruction and Recognition

Fig. 9.21 Example of curve-skeleton matching by icp method: (a) The model 2D ﬁngerprint image, 3D ﬁnger shape, and extracted curve-skeleton feature, (b)
The test 2D ﬁngerprint image, 3D ﬁnger shape, and extracted curve-skeleton feature, (c) Matching result by icp method
9.5
Curvature Features Extraction and Matching
195
www.ebook3000.com

Fig. 9.21 (continued)
196
9
3D Fingerprint Reconstruction and Recognition

9.6
Experimental Results and Analysis
9.6.1
3D Fingerprint Reconstruction System Error Analysis
Reconstruction and system errors are inevitable. To acquire these errors, the
reconstruction of an object with the standard cylinder shape and of radius 10 mm
is given. The example object is shown in Fig. 9.22a. The surface of the object is
wrapped by a grid chapter to facilitate feature extraction. Three 2D pictures (left-
side, frontal, and right-side) of the cylinder are captured by the touchless multi-view
imaging device we designed. Figure 9.22b, c shows two grouped images (left-side
and frontal, right-side and frontal). As mentioned in Sect. 9.2, there are ﬁve main
steps in our reconstruction technique. Camera parameters are ﬁrst calculated
off-line. The corner features of the wrapped grid chapter are then labeled and
their correspondences between grouped images are established manually, as
shown in Fig. 9.22b, c. Figure 9.22d, e illustrates the calculated 3D coordinates
corresponding to the matched pairs shown in Fig. 9.22b, c based on the given
camera parameters and feature correspondences. Shape model estimation is unnec-
essary since the cylinder model is known as a prior knowledge. By using the
calculated 3D coordinates and the known shape model of cylinder, the cylindrical
surface is ﬁnally generated by interpolation based on the multiple linear regressions
using the least squares method (Zhang et al. 2009) and (Chatterjee and Hadi 1986).
Figure 9.22f, g are the reconstructed cylinders shown by a 3D display software
called Image ware 12.1. This software is used for 3D point cloud data display and
analysis. The error maps shown in Fig. 9.22h, i are also obtained by this software.
From Fig. 9.22f, g, we can see that the radius of reconstructed cylinders from 40 3D
points of Fig. 9.22d, e are 9.91 and 9.84 mm compared with the real radius
10 mm. Figure 9.22h, i gives the error maps of 3D points corresponding to
Fig. 9.22d, e when ﬁtted by cylinder shape with radius of 10 mm. The error ranges
are [0.07–0.06 mm] and [0.1–0.06 mm]. The results demonstrate that the
reconstruction error of our device is within 0.2 mm.
9.6.2
Comparison and Analysis of Reconstruction Results
By following the ﬁve steps introduced in Sect. 9.2, reconstructed 3D ﬁngerprint
images can be obtained. Since there are three ﬁngerprint images captured at one
time and the central camera is selected as the reference system, the proposed
reconstruction system consists of two parts (left-side camera and central camera,
right-side camera and central camera) according to binocular stereo vision theory.
In this chapter, these two parts are combined before the fourth steps by normalizing
the calculated depth value of correspondences into [0, 1]. Here, the Min-Max
strategy of normalization is used. This combination is adopted for two reasons.
One is that there are parts of overlapping region between two adjacent ﬁngerprint
9.6
Experimental Results and Analysis
197
www.ebook3000.com

images, and the distribution of correspondences may focus on a small part of
ﬁngerprint images. Larger areas of ﬁngerprint image can be covered by discrete
correspondences through combining two parts of the system. The other is that very
simple to accomplish and the system error of combining two parts before model
Fig. 9.22 Reconstruction accuracy analysis of cylinder shape object. (a) Original cylinder shape
object wrapped with grid chapter, (b) correspondences established between left-side and frontal
images captured by our device, (c) correspondences established between right-side and frontal
images captured by our device, (d) 3D space points corresponding to (b), (e) 3D space points
corresponding to (c), (f) ﬁtting result corresponding to (d), (g) ﬁtting result corresponding to (e),
(h) error map corresponding to (d) when ﬁtting by cylinder shape with radius of 10 mm, (i) error
map corresponding to (e) when ﬁtting by cylinder shape with radius of 10 mm
198
9
3D Fingerprint Reconstruction and Recognition

ﬁtting is alleviated. Table 9.2 shows the reconstruction results based on three
different ﬁngerprint feature correspondences using the example images shown in
Fig. 9.23. The results are different corresponding to different feature matched pairs
due to different numbers and distribution of established ﬁngerprint feature corre-
spondences and the existence of false correspondences.
To investigate which features are more suitable for 3D ﬁngerprint reconstruc-
tion, we also manually labeled ﬁngerprint correspondences, as shown in Fig. 9.24.
The histogram of error map between reconstructed results in Table 9.3 and Fig. 9.24
is shown in Fig. 9.25. The results show that for single feature used, a reconstruction
result based on SIFT features achieves the best result, while the ridge feature-based
is the worst one. When combining with other features, best reconstruction results
can be generated if all three features of correspondences are used. However,
comparable results are obtained by using SIFT and minutiae. Considering the
computational complexity, it is recommended to simply use SIFT and minutiae.
9.6.3
Validation of Estimated Finger Shape Model
The effectiveness of the proposed ﬁnger shape model is validated by analyzing the
ﬁtting errors. Table 9.3 presents the errors measured by the mean distance and the
standard variation between the estimated ﬁnger shape and the original 3D point cloud
Fig. 9.22 (continued)
9.6
Experimental Results and Analysis
199
www.ebook3000.com

Table 9.2 Reconstruction results from different feature correspondences of Fig. 9.23
Results Used
feature
Established correspondences
Reconstructed 3D
ﬁngerprint image
SIFT feature
Minutiae
Ridge feature
Feature
Combination
Reconstructed 3D ﬁngerprint image
SIFT feature and
minutiae
SIFT and ridge
feature
Minutiae and ridge
feature
SIFT feature,
minutiae and ridge
feature
200
9
3D Fingerprint Reconstruction and Recognition

data in Fig. 9.16a. It can be seen that the error between V and ~VEq:9:2 is smaller than the
one between V and ~VEq:9:3. Next, the errors between the 3D point cloud data and their
corresponding ﬁtting results of all 440 ﬁngers we collected are computed. It can be
seen from Fig. 9.26 that the binary quadratic function is more suitable for the ﬁnger
shape model since smaller errors are obtained between the original 3D point cloud data
Fig. 9.23 Example ﬁngerprint images captured by our device (left, middle, right)
Table 9.3 Mean distance and standard variation of error map between estimated ﬁnger shape and
real ﬁnger shape of example images in Fig. 9.16
Index factor ﬁtting mode
function
Mean distance mean
V  ~V


Standard variation
std V  ~V


f1(x, y)
0.024
0.019
f2(x, y)
0.082
0.057
Fig. 9.24 Reconstruction of 3D ﬁnger shape of Fig. 9.23. (a) Manually labeled correspondences
between ﬁngerprint images, (b) reconstructed 3D ﬁnger shape based on (a)
9.6
Experimental Results and Analysis
201
www.ebook3000.com

and their corresponding ﬁtting results by the binary quadratic function. For this reason,
the binary quadratic function is chosen as the ﬁnger shape model in this chapter.
Since the ﬁnal 3D ﬁnger shape is obtained after interpolation according to the
prior estimated ﬁnger shape model, we compared the reconstruction result with the
3D point cloud data of the same ﬁnger to verify the effectiveness of the model.
From the results shown in Fig. 9.27, it can be seen that the proﬁle of the ﬁnger shape
reconstructed from multi-cameras is similar to the 3D point cloud data even though
it is not that accurate. The real distance between the upper left core point and the
lower left delta point is also calculated and shown in Fig. 9.27a, c, the values are
0.357 and 0.386 respectively. As a result, it is concluded that the estimated ﬁnger
shape model is effective even though there is an error between the reconstruction
result and the 3D point cloud data.
Fig. 9.25 Histogram of error maps between reconstructed results in Table 9.2 and Fig. 9.24b. (a)
Histogram of err map between Fig. 9.24b and reconstruction result by using SIFT feature only, (b)
histogram of err map between Fig. 9.24b and reconstruction result by using minutiae only, (c)
histogram of err map between Fig. 9.24b and reconstruction result by using ridge feature only, (d)
histogram of err map between Fig. 9.24b and reconstruction result by using both SIFT feature and
minutiae, (e) histogram of err map between Fig. 9.24b and reconstruction result by using both
SIFT feature and ridge feature, (f) histogram of err map between Fig. 9.24b and reconstruction
result by using both minutiae and ridge feature, (g) histogram of err map between Fig. 9.24b and
reconstruction result by using SIFT feature, minutiae and ridge feature
202
9
3D Fingerprint Reconstruction and Recognition

9.6.4
Reconstruction System Computation Time Analysis
There are six main parts included in our reconstruction system from image acqui-
sition to result generation, as the block diagram shows in Fig. 9.3. The reconstruc-
tion method is implemented by Matlab on Fujitsu notebook embedded with Intel
Core 2 Duo CPU, T9600 (2.80 GHz) processor. For image acquisition, it consumes
no more than 100 ms to capture three views of ﬁngerprint images since the frame
rate of each camera is 30 frames/s. Because both of the camera parameters
calculation and shape model estimation are done off-line, they do not occupy any
time in the whole system. The correspondences establishment step consists of
feature extraction and matching, which consumes considerable time. This time is
variable for different images. The average time statistically calculated in our
database is then used to measure. They are 60.3 and 24.32 s. It takes 0.31 s
to compute the 3D coordinates of feature correspondences. For interpolation, the
code included in the matlab toolbox is employed and the consumption time is
1.21 s. To summarize, it takes 1.5 min to generate a 3D image by using the
proposed system. It is believed, however, this time will be largely reduced once the
code is compiled by C/Cþþ language and the multithread processing technique
is used.
Fig. 9.25 (continued)
9.6
Experimental Results and Analysis
203
www.ebook3000.com

9.6.5
3D Fingerprint Recognition
To study the distinctiveness of curve-skeleton features of human ﬁngers, we show
examples of matching results of different gender and different ﬁngers. As shown in
Table 9.4, examples of curve-skeletons from a female and a male with thumb, index
ﬁnger and little ﬁnger captured at different sessions are given. We then matched
them by ICP method. The percentage of matched points (Pm) and the mean distance
between matched pairs (Mdist) are taken as the match score.
We ﬁrstly matched the curve-skeletons from the same ﬁnger but captured at
different time, as listed in Table 9.5. Results show that the mean distance between
matched pairs are smaller than one and the percentage of matched points are larger
than 70%. Figure 9.28 also shows the matching results of different gender and
ﬁnger types, the match scores are listed in Table 9.6. The results show that big
Fig. 9.26 Errors between the original 3D point cloud data of all 440 ﬁngers we collected and their
corresponding ﬁtting results by different models. (a) Errors represented by the mean distance
between the original 3D point cloud data and their corresponding ﬁtting result by binary quadratic
function, (b) errors represented by the standard variation between the original 3D point cloud data
and their corresponding ﬁtting result by binary quadratic function, (c) errors represented by the
mean distance between the original 3D point cloud data and their corresponding ﬁtting result by
the mixed model, (d) errors represented by the standard variation between the original 3D point
cloud data and their corresponding ﬁtting result by the mixed model
204
9
3D Fingerprint Reconstruction and Recognition

difference existed between different ﬁngers and different genders in curve-skeleton,
since such feature reﬂects the ridge width feature of human ﬁnger and curvatures
are different for human ﬁnger from ﬁnger tip to the distal interphalangeal crease.
Fingerprint
identiﬁcation
experiment
based
on
curve-skeletons
is
then
implemented on our established database. Figure 9.29 shows the ROCs of different
match score indexes. The EERs were obtained from 541 genuine scores and 292,140
imposter scores (generated from 541 ﬁngers, 2 pictures of each ﬁnger). From the
results, we can see that an EER of around 15% can be obtained when matching 3D
ﬁngerprint curve-skeleton features by simple ICP algorithm. The index of mean
distance between matched pairs is better than the percentage of matched points.
Curve-skeleton feature of 3D ﬁngerprint image can be used to distinguish different
ﬁngers even though it is not as accurate as other higher level ﬁngerprint features.
1
0.5
0
0
100
200
300
400
0
100
200
300
400
500
600
1
0.5
0
0
100
200
300
400
500
600
0
200
400
600
800
a
b
c
d
Fig. 9.27 Comparison of 3D ﬁngerprint images from the same ﬁnger but different acquisition
technique. (a) Original ﬁngerprint image captured by the camera when collecting 3D point cloud,
(b) 3D point cloud collected by one camera and a projector using the SLI method, (c) original
ﬁngerprint image captured by our device, (d) reconstructed 3D ﬁngerprint image with labeled
correspondences
9.6
Experimental Results and Analysis
205
www.ebook3000.com

Table 9.4 Examples of extracted curve-skeletons from different gender and different ﬁngers
Finger type
Gender
Male
Female
Original 2D
image
Curve-
skeleton
Original 2D
image
Curve-skeleton
Thumb
Session 1
(a1)
Session
2
(a2)
Index
ﬁnger
Session 1
(b1)
Session
2
(b2)
Little ﬁnger Session 1
(c1)
Session
2
(c2)
206
9
3D Fingerprint Reconstruction and Recognition

9.7
Summary
This chapter investigates a 3D reconstruction technique based on limited feature
correspondences in 2D ﬁngerprint images captured by the designed multi-view
touchless ﬁngerprint imaging device. Speciﬁc to the characteristic of low ridge–
valley contrast of touchless ﬁngerprint images, an improved ﬁngerprint enhance-
ment method is proposed, so as to extract more robust ﬁngerprint features. Then,
three frequently used features, i.e., SIFT feature, ridge feature and minutiae, having
different numbers and various distributions, are considered for correspondences
establishment. Correspondences are ﬁnally established by adopting the hierarchical
ﬁngerprint matching approaches. The ﬁnger shape model in this chapter is esti-
mated by analyzing 3D point cloud ﬁnger data collected by one camera and a
projector using the SLI method. Results show that the binary quadratic function is
more suitable for the ﬁnger shape model compared with another mixed model
proposed in the chapter. By reconstructing a standard cylinder object, it is shown
that it is reasonable and feasible for us to adopt the methodology of the reconstruc-
tion technique, as well as the capturing device. The comparison and analysis of 3D
ﬁngerprint reconstruction results based on different ﬁngerprint feature correspon-
dences illustrates that best reconstruction results can be generated if all three
features of correspondences are used. However, it is recommended to simply use
SIFT and minutiae since comparable results are achieved by using them. The
effectiveness of the estimated ﬁnger shape model is veriﬁed by comparing the
reconstructed 3D ﬁnger shape with the corresponding 3D point cloud ﬁnger data.
This chapter further studied the recognition of the reconstructed 3D ﬁngerprint
image. Some ﬁngerprint features which are coarser than Level 1 features-Curvature
Table 9.5 Matching result of curve-skeletons from the same ﬁnger but different session
Gender
Finger type
Thumb
Index ﬁnger
Little ﬁnger
(a1)–(a2)
(b1)–(b2)
(c1)–(c2)
Male
Pm ¼ 74%; Mdist ¼ 0.20
Pm ¼ 93%; Mdist ¼ 0.39
Pm ¼ 79%; Mdist ¼ 0.25
Female
Pm ¼ 94%; Mdist ¼ 0.72
Pm ¼ 97%; Mdist ¼ 0.09
Pm ¼ 90%; Mdist ¼ 0.32
9.7
Summary
207
www.ebook3000.com

Fingerprint Features, are ﬁrstly deﬁned. These features are then used for assisting
ﬁngerprint recognition. Experimental results show that an EER of ~15% can be
achieved when using 3D curve-skeleton for recognition.
Fig. 9.28 Example of matching results of curve-skeletons from different gender and ﬁnger types
in Table 9.4: (a) Matching result of [(male, thumb)—(male, index ﬁnger)], (b) Matching result of
[(male, thumb)—(male, little ﬁnger)], (c) Matching result of [(male, index ﬁnger)—(male, little
ﬁnger)], (d) Matching result of [(female, thumb)—(female, index ﬁnger)], (e) Matching result of
[(female, thumb)—(female, little ﬁnger)], (f) Matching result of [(female, index ﬁnger)—(female,
little ﬁnger)], (g) Matching result of [(male, thumb)—(female, thumb)], (h) Matching result of
[(male, index ﬁnger)—(female, index ﬁnger)], (i) Matching result of [(male, little ﬁnger)—
(female, little ﬁnger)]
208
9
3D Fingerprint Reconstruction and Recognition

Currently, researchers ﬁnd that 3D ﬁngerprint images provide more attributes for
ﬁngerprint features than 2D ﬁngerprint images. For instance, a minutia feature in
2D ﬁngerprint image is usually represented by its location {x, y} and orientation θ.
While in 3D case, it may be noted by {x, y, z, θ, ϕ}, where x, y and z are the spatial
coordinates. Two angles of orientation of the ridge in 3D space θ and ϕ are
available. Thus, ﬁngerprint recognition with higher security can be achieved by
matching features in 3D space [e.g. 3D minutia matching (Parziale and Niel 2004)].
By observing ﬁngerprint in 3D images, we ﬁnd that the center part of the ﬁnger is
higher than the side parts, and the core point on ﬁngerprints is located at almost the
highest part of the ﬁnger. These characteristics of 3D ﬁngerprint images beneﬁt
alignment when two ﬁngerprint images are compared. Thus, our future work will
investigate the application of such 3D information for ﬁngerprint recognition.
Fig. 9.28 (continued)
Table 9.6 Match scores corresponding to Fig. 9.28
Match score index
Corresponding labels in Fig. 9.28
(a)
(b)
(c)
(d)
(e)
(f)
(g)
(h)
(i)
Pm (%)
57
38
53
55
45
62
50
53
57
Mdist
8.3
13.7
2.9
6.8
14.8
3.1
4.0
4.1
4.6
9.7
Summary
209
www.ebook3000.com

References
Almansa A, Lindeberg T (2000) Fingerprint enhancement by shape adaptation of scale-space
operators with automatic scale selection. IEEE Trans Image Process 9(12):2027–2042
Blais F, Rious M, Beraldin J (1988) Practical considerations for a design of a high precision
3-Dlaserscanner system. Proc SPIE:225–246
Bouguet J. Camera calibration toolbox for Matlab. http://www.visioncaltech.edu/bouguetj/calib_
doc/download/index.html
Bradley B, Chan A, Hayes M (2002) Asimple, low cost, 3D scanning system using the laser light-
sectioning method. In: Proceedings of the IEEE international instrumentation and measure-
ment technology conference, Victoria, Vancouver Island, Canada, pp 299–304
Chatterjee S, Hadi A (1986) Inﬂuential observations, high leverage points, and outliers in linear
regression. Stat Sci 1(3):379–416
Chen H, Dong G (2006) Fingerprint image enhancement by diffusion processes. In: Proceedings of
the13th international conference on image processing, pp 297–300
Chikkerur S, Cartwright A, Govindaraju V (2007) Fingerprint enhancement using STFT analysis.
Pattern Recogn 40(1):198–211
Choi K, Choi H, Lee S, Kim J (2007) Fingerprint image mosaicking by recursive ridge mapping.
IEEE Trans Syst Man Cybern B 37(5):1191–1203
Choi H, Choi K, Kim J (2010) Mosaicing touchless and mirror-reﬂected ﬁngerprint images. IEEE
Trans Inf Forensics Secur 5(1):52–61
Draper N, Smith H (1981) Applied regression analysis, 2nd edn. Wiley, New York
Feng J (2008) Combining minutiae descriptors for ﬁngerprint matching. Pattern Recogn 41
(1):342–352
Fishler M, Bolles R (1981) Random sample consensus: a paradigm for model ﬁtting with
applications to image analysis and automated cartography. Commun ACM 24(6):381–395
Fig. 9.29 ROCs for 3D ﬁngerprint matching by ICP with Curve-Skeleton feature
210
9
3D Fingerprint Reconstruction and Recognition

Hao Y, Yuan C (2004) Fingerprint image enhancement based on nonlinear anisotropic reversed if
fusion equations. In: Proceedings of the 26th annual international conference of the IEEE
engineering in medicine and biology society, pp 1601–1604
Hartley R (2000) Multiple view geometry in computer vision. Cambridge University Press,
Cambridge
Hastings R (2007) Ridge enhancement in ﬁngerprint images using oriented diffusion. In:
DICTA’07 Proceedings of the 9th biennial conference of the Australian Pattern Recognition
Society on digital image computing techniques and applications pp 245–252
Hernandez C, Vogiatzis G, Cipolla R (2008) Multiview photometric stereo. IEEE Trans Pattern
Anal Mach Intell 30(3):548–554
Hong L, Wan Y, Jain A (1998) Fingerprint image enhancement: algorithm and performance
evaluation. IEEE Trans Pattern Anal Mach Intell 20(8):777–789
Hu G, Stockman G (1989) 3-D surface solution using structured light and constraint propagation.
IEEE Trans Pattern Anal Mach Intell 11(4):390–402
Jain A, Ross A (2002) Fingerprint mosaicking. In: Proceedings of the IEEE international confer-
ence on acoustics, speech, and signal processing (ICASSP), Orlando, Florida, 4: pp 4064–4067
Jain A, Hong L, Bolle R (1997) On-line ﬁngerprint veriﬁcation. IEEE Trans Pattern Anal Mach
Intell 19(4):302–314
Jirachaweng S, Areekul V (2007) Fingerprint enhancement based on discrete cosine transform. In:
Proceedings of the international conference on biometrics, LNCS, pp 96–105
Kumar A, Zhou Y (2011) Contactless ﬁngerprint identiﬁcation using level zero features. In:
Proceedings of CVPR 2011, pp 121–126
Liu F, Zhang D, Lu G, Song C (2013) Touchless multi-view ﬁngerprint acquisition and mosaick-
ing. IEEE Trans Instrum Meas 62:2492–2502. doi:10.1109/TIM.2013.2258248
Lowe D (2004) Distinctive image features from scale-invariant key points. Int J Comput Vis 60
(2):91–110
Malathi S, Meena C (2010) Partial ﬁngerprint matching based on SIFT features. Int J Comput Sci
Eng 4(2):1411–1414
Maltoni D, Maio D, Jain A, Prabhakar S (2009) Handbook of ﬁngerprint recognition. Springer,
New York
Park U, Pankanti S, Jain A (2008) Fingerprint veriﬁcation using SIFT features. In: Proceedings of
SPIE 6944, 69440 K-69440K-9
Parziale G, Diaz-Santana E (2006) The surround imager: a multi-camera touchless device to
acquire 3D rolled-equivalent ﬁngerprints. In: Proceedings of international conference on bio-
metrics (ICB), Hong Kong, China, pp 244–250
Parziale G, Niel A (2004) A ﬁngerprint matching using minutiae triangulation. In: Proceedings of
the international conference on biometric authentication (ICBA), LNCS, 3072, pp 241–248
Ross A, Dass S, Jain A (2005) A deformable model for ﬁngerprint matching. Pattern Recogn 38
(1):95–103
Rusinkiewicz S, Holt O, Levoy M (2002) Real-time 3D model acquisition. In: Proceedings of the
29th annual conference on computer graphics and interactive techniques, pp 438–446
Saldner H, Huntley J (1997) Temporal phase unwrapping: application to surface proﬁling of
discontinuous objects. Appl Opt 36(13):2770–2775
Shah S, Ross A, Shah J, Crihalmeanu S (2005) Fingerprint mosaicking using thin plate splines. In:
Proceedings of the biometric consortium conference
Stockman G, Chen S, Hu G, Shrikhande N (1988) Sensing and recognition of rigid objects using
structured light. IEEE Control Syst Mag 8(3):14–22
Wang Y, Hassebrook L, Lau D (2010) Data acquisition and processing of 3-D ﬁngerprints. IEEE
Trans Inf Forensics Secur 5(4):750–760
Weichert J (1999) Coherence-enhancing diffusion ﬁltering. Int J Comput Vis 31(2–3):111–127
Xie M, Wang Z (2004) Fingerprint enhancement based on edge-directed diffusion. In: Proceedings
of the 11th international conference on image processing, pp 274–277
References
211
www.ebook3000.com

Zhang Z (2000) A ﬂexible new technique for camera calibration. IEEE Trans Pattern Anal Mach
Intell 24(11):1330–1334
Zhang D, Lu G, Li W (2009) Palmprint recognition using 3-Dinformation. IEEE Trans Syst Man
Cybern Part C Appl Rev 39(5):505–519
Zhang D, Kanhangad V, Luo N, Kumar A (2010) Robust palmprint veriﬁcation using 2D and 3D
features. Pattern Recogn 43(1):358–368
Zhang D, Liu F, Zhao Q, Lu G, Luo N (2011) Selecting a reference high resolution for ﬁngerprint
recognition using minutiae and pores. IEEE Trans Instrum Meas 60(3):863–871
212
9
3D Fingerprint Reconstruction and Recognition

Chapter 10
Hand Back Skin Texture for Personal
Identiﬁcation
Abstract Human hand back skin texture (HBST) is often consistent for a person and
distinctive from person to person. In this chapter, we study the HBST pattern
recognition problem with applications to personal identiﬁcation and gender classiﬁ-
cation. A specially designed system is developed to capture HBST images, and an
HBST image database was established, which consists of 1920 images from 80 persons
(160 hands). An efﬁcient text on learning based method is then presented to classify
the HBST patterns. First, textons are learned in the space of ﬁlter bank responses from
a set of training images using the l1 -minimization based sparse representation
(SR) technique. Then, under the SR framework, we represent the feature vector at
each pixel over the learned dictionary to construct a representation coefﬁcient histo-
gram. Finally, the coefﬁcient histogram is used as skin texture feature for classiﬁca-
tion. Experiments on personal identiﬁcation and gender classiﬁcation are performed
by using the established HBST database. The results show that HBST can be used to
assist human identiﬁcation and gender classiﬁcation.
Keywords Biometrics • Hand back skin texture • Texton learning • Orientation
map • Sparse representation
10.1
Introduction
Skin, as the outermost part of the human body, is known to provide much useful
information, such as health status (Skin disease atlas 2012; Cula et al. 2004) and
human identity information (Cula et al. 2005). Skin appearance can be viewed as a
kind of texture surface, and skin texture analysis can be used in various applica-
tions. For example, in (Cula et al. 2004), skin texture analysis is applied to
computer-aided diagnosis in dermatology, where the dermatologist can use the
computational texture representation to make an initial diagnosis for the patient.
Meanwhile, biomedical evaluation based on skin texture can provide some tests for
topical skin treatments, which can be used to judge whether these treatments are
effective or not in the early stages. In addition, skin texture analysis can be used to
estimate human skin age (Tanaka et al. 2008; Kim et al. 2009).
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_10
213
www.ebook3000.com

With the rapid development of computer techniques, researchers have investi-
gated the use of various biometric traits, including ﬁngerprints (Jain et al. 2007;
Ratha and Bolle 2004), face (Delac and Grgic 2007; Wechsler 2006), iris (Daugman
1993, 2004), retina (Hill 1999; Borgen et al. 2008), palmprints (Guo et al. 2009; Sun
et al. 2005) and ﬁnger-knuckle-prints (Zhang et al. 2010), etc., for the purposes of
personal authentication. Moreover, face (Bruce et al. 1993; Moghaddam and Yang
2002) and gait (Li et al. 2008) have been used for gender classiﬁcation. In
(Moghaddam and Yang 2002), the authors demonstrated that the SVM classiﬁer
is able to learn and classify gender from a set of hairless low resolution face images
with high classiﬁcation accuracy. For gait-based gender recognition, a number of
combinations of gait components (Li et al. 2008) are extracted to classify gender
with the SVM classiﬁer. Skin texture, as a potential biometric identiﬁer to assist
existing biometric traits, has also received certain attention in the past years. Based
on the locally consistent property of the ﬁngerprint skin tissue, Rowe (2007)
extracted texture features of the ﬁngerprint skin for human identiﬁcation while
reducing the size of the ﬁngerprint sensing area. Cula (Cula et al. 2005, 2004; Cula
and Dana 2001) used the bidirectional texture function, which is analogous to the
bidirectional reﬂectance distribution function, to model skin texture to assist face
recognition. For each skin texture surface, the bidirectional texture function is
sampled in multiple camera views and illumination directions. However, obtaining
accurate bidirectional image measurements of skin texture surface is hard, because
the skin surface is non-planar, non-rigid and can be stretched.
It can be observed that the human hand back skin has a clear and consistent
texture pattern which is uniformly distributed over a large portion of hand back.
Based on our daily life experience, we know that the hand based skin texture
(HBST) pattern is not permanent and it will change over time. For example,
young people will have ﬁner (i.e., smoother and smaller size of micro-cells)
HBST than old people, while females will have much ﬁner HBST than male.
Nonetheless, over a relatively long period, the HBST of a person is stable. Based
on (Tatsumi et al. 1999), the changes in skin associated with age can be visualized
by gloss and wrinkles, and thus some measurements of wrinkles, gloss and density
of microgrooves of skin can be used for age estimation. In (Tatsumi et al. 1999), the
number of pixels in the binary image of the epidermal cross-section is used to
estimate the age. From the curve of measured peripheral length vs. age in (Tatsumi
et al. 1999), one can see that the peripheral length changes little in 1–2 years, which
means that skin texture can remain stable for a relatively long period. These
motivated us to investigate the possibility that the HBST pattern can be used to
aid personal identiﬁcation and gender classiﬁcation. Many biometric identiﬁers
such as ﬁngerprints, faces, iris and palmprints, etc., have been proposed for
human identiﬁcation, and our goal is not to compete with those biometric identi-
ﬁers, but to validate whether HBST has a sufﬁcient level of accuracy so that it can
be helpful to assist the existing biometric authentication techniques. Moreover,
apart from biometric applications, as a speciﬁc kind of texture patterns, the
established HBST dataset can also be used to evaluate the texture feature extraction
214
10
Hand Back Skin Texture for Personal Identiﬁcation

and classiﬁcation algorithms in the community of computer vision and pattern
recognition.
In this chapter, we study the use of HBST for personal identiﬁcation and gender
classiﬁcation. To this end, an HBST imaging device was designed to capture HBST
images. Since HBST is a type of ﬁne scale feature, a high resolution (about 450 dpi)
is set to capture the detailed texture patterns in hand back images. Different from
the method in (Cula et al. 2005), where skin texture is modeled as a 3D texture and
the bidirectional texture function is used to describe the skin appearance, we model
HBST as a kind of 2D appearance texture because the hand back can be approx-
imately viewed as a 2D plane. Therefore, we directly capture the HBST image
using a CCD camera with the ﬁxed position under the ﬁxed illumination direction.
Such a design makes the HBST image acquisition very efﬁcient and feasible for the
purpose of personal identiﬁcation and gender classiﬁcation. In the 3D model (Cula
and Dana 2001, 2004), multiple cameras and multi-illuminations are needed to
collect samples, which makes the imaging system very complex. Compared with
the 3D model, modeling the hand back skin surface with the 2D model makes our
imaging system much easier to design and more convenient to collect samples. In
addition, our goal is to analyze the texture pattern in hand back skin so that 2D
modeling is more suitable.
By using the designed HBST imaging device (please refer to Sect. 10.2 for more
details), an HBST image database is established, which consists of 1920 images
from 80 volunteers (160 hands). A texton learning based method is then proposed
for HBST pattern classiﬁcation. The HBST images are passed through a bank of
ﬁlters, and a set of textons are learned from the ﬁlter responses with the sparse
representation (SR) technique. Then, under the SR framework, the representation
coefﬁcient histograms of HBST images are computed and used for classiﬁcation.
The performance of the proposed method is evaluated by using the established
HBST database in comparison with state-of-the-art texture classiﬁcation schemes,
including the multi-fractal spectrum (Xu et al. 2006), original LBP (Ojala et al.
2004), dominant LBP (Liao et al. 2009), completed LBP (Guo et al. 2010) and k-
means based texton learning method (Varma and Zisserman 2002, 2005). Experi-
mental results demonstrated that HBST could achieve interesting personal identi-
ﬁcation and gender classiﬁcation accuracy, which implies that HBST can be used to
aid existing biometric authentication techniques and improve the overall
performance.
In summary, the major contributions of this work lie in that we developed the
HBST imaging device, established an HBST dataset and proposed a sparse texton
learning based HBST texture classiﬁcation method. We validated that HBST
pattern has potential to do personal identiﬁcation and especially gender classiﬁca-
tion. In addition, as a special type of texture images, the established HBST dataset is
very challenging, and it provides a good platform to evaluate and develop high
performance texture feature extraction and classiﬁcation algorithms.
10.1
Introduction
215
www.ebook3000.com

10.2
Hand Back Skin Texture Imaging System
A schematic diagram of the major components of the developed hand back skin
texture (HBST) imaging system is shown in Fig. 10.1. It is composed of a ring of
LED light source, a lens, an associated CCD camera, and a data acquisition card.
When it works to collect data, the LED light source will illuminate the hand back
skin, and then the CCD camera will capture the HBST image and pass it to the data
acquisition card. The data acquisition card will then transmit the image to the data
processing unit (e.g., the CPU in a PC).
Figure 10.2a illustrates the inner structure of the HBST imaging device and
Fig. 10.2b shows its exterior. A critical issue in HBST data acquisition is how to
Fig. 10.1 The schematic diagram of the developed hand back skin texture imaging system
Fig. 10.2 (a) The inner structure of the developed hand back skin texture imaging system; (b) The
outside view of the imaging system
216
10
Hand Back Skin Texture for Personal Identiﬁcation

make the data collection environment as stable and consistent as possible so that
undesired disturbances (e.g., the background and environmental illumination dis-
turbances) can be reduced. Meanwhile, a stable data collection environment can
effectively reduce the complexity of feature extraction and improve the classiﬁca-
tion accuracy. Speciﬁcally speaking, in our imaging system how to keep the
illumination uniform and constant and how to ﬁx the position of the hand are of
the most importance. To this end, a ring of LED light source (in visible spectrum,
390–780 nm) and a CCD camera are enclosed in a box to keep the illumination
nearly constant. The LEDs are arranged in a circle around the camera to make the
illumination uniform. Referring to Fig. 10.2a, in order to capture the central part
image of the hand back skin texture, two pegs are used to ﬁx the hand, which can
guide the position of index and little ﬁngers with a user friendly interface. This can
also reduce largely the pose variations of the hand in different capturing sessions. In
addition, our design could make the skin texture surface as ﬂat as possible so that
we can model the skin surface as a 2D planar texture image. Note that there are
some differences between our device and the palmprint device (Zhang et al. 2003).
First, in order to capture the micro-structures of HBST, the resolution of the chosen
camera in our device is higher than that in the palmprint device. Second, the light
source is different from that in the palmprint device. In our device, the ring LED is
used while the halogen light source is used in the palmprint device. Finally, the
architecture of the device is different. In our HBST imaging system, we employ the
micro-industrial CCD camera board, LED light source and USB data acquisition
card to collect data. However, in the palmprint device, the commonly used indus-
trial CCD camera, halogen light source and PCI data acquisition card are used to
collect data. Compared to the palmprint device, the size of the HBST imaging
device is much smaller due to the use of micro-industrial CCD camera board and
LED light source. In addition, the cost of HBST device is also lower.
The texture pattern of human hand back skin can only be clearly observed in a
relatively ﬁne scale. In order to capture the HBST image in a high enough
resolution while avoiding the HBST image size to be too big, the focal of the lens
should be carefully designed. In our imaging system, due to the limited distance
between the camera and the hand back, we chose to use a 12 mm focal length lens to
capture the HBST images. Further reduction in the focal length will distort the
captured image. The size of the CCD output image is 576  768 (the raw image is
saved in the 24-bitmap format and we convert it into 8-bit gray level image), and
ﬁnally the HBST image is captured under a resolution of about 450 dpi. In
designing our imaging device, we tested different resolution settings of the HBST
image, and found the resolution of about 450 dpi can satisfy our requirements. If the
resolution of the image is too low, the micro-structures such as wrinkles in the
image cannot be captured clearly. If the resolution of the image is too high, the cost
of the camera will be high and the computational cost will also increase. A
resolution of 450 dpi is good enough to capture clear HBST images at a low cost.
In our HBST imaging system, since we use two pegs to ﬁx the hand position, the
top and bottom boundary of the captured skin texture image can be roughly ﬁxed.
Although the hand back skin can be viewed nearly as a 2D plane in the central part,
10.2
Hand Back Skin Texture Imaging System
217
www.ebook3000.com

the boundary part of the hand back can be quite distorted in the captured HBST
image. In order to reduce the effect of the hand back boundary area on the later
feature extraction and recognition procedures, we crop a sub-image from the
captured raw image by removing the four boundary areas. Referring to Fig. 10.3,
we simply set the top left corner of the HBST image as the origin point, and based
on our experimental experience we crop the central part of size 288  384 from the
original image of size 576  768. Such a sub-image cropping process cannot only
make the feature extraction more stable and accurate, but also reduce computational
cost.
Figure 10.4 shows some example cropped HBST images captured in two
different sessions with a time interval of about 30 days. Figure 10.4a, b are the
left-hand HBST images from one person in the two sessions, while Fig. 10.4c, d are
the right-hand HBST images from the same person. Figure 10.4e, h are the left and
right-hand HBST images from another person. Figure 10.5 shows the HBST images
from one male subject and one female subject. From these HBST example images,
we can have the following observations. (1) First, the left-hand and right-hand
HBST patterns of a person are similar. (2) Second, the HBST patterns captured in
different sessions from the same person are similar. (3) Third, the HBST patterns
from different persons are different, which implies its potential for human identi-
ﬁcation. (4) At last, the HBST patterns of male and female subjects are different,
which makes HBST pattern a good feature for gender classiﬁcation.
10.3
HBST Feature Extraction and Classiﬁcation
Texture classiﬁcation is a classical topic in computer vision and pattern recognition.
Although some well-known texture classiﬁcation methods (Xu et al. 2006; Ojala
et al. 2004; Lazebnik et al. 2005) can obtain good performance on some benchmark
192 
pixels
a
b
192 
pixels
144 pixels
144 pixels
Fig. 10.3 (a) is the raw image (size 576  768) captured by our device and (b) is the sub-image
(size 288  384) cropped from the central part of (a)
218
10
Hand Back Skin Texture for Personal Identiﬁcation

databases such as the UIUC (Lazebnik et al. 2005) and CUReT (Dana et al. 1999)
texture databases, they may not be suitable for HBST patterns due to the special
micro-structure of the hand back skin. The multi-fractals spectrum method
(Xu et al. 2006) and the LBP method in (Ojala et al. 2004) cannot obtain good
results because the features generated by them cannot characterize the appearance
Fig. 10.4 (a) and (b) are the cropped left-hand HBST images of a person collected in two
different sessions, while (c) and (d) are the right-hand HBST images from the same person. (e)
and (f) are the cropped left-hand HBST images from another person, while (g) and (h) are the
right-hand HBST images from this person
10.3
HBST Feature Extraction and Classiﬁcation
219
www.ebook3000.com

of the skin texture well. Since there are no obvious interest points or interest regions
in HBST images, the method in (Lazebnik et al. 2005) cannot detect accurately the
afﬁne invariant regions for a robust statistical description of the skin texture, and
thus it will fail to classify the HBST patterns.
With a more careful look of the HBST images, we can observe some properties of
the HBST patterns. First, there are no clear edges and corner points in the HBST
images. Second, the HBST patterns are made up of some micro-cellular structures.
Third, those micro-structures are generally distributed uniformly across the whole
HBST image. Based on these observations, we choose to learn the micro-structures
(i.e., textons) from the training HBST images, and then use them to describe the query
HBST image for classiﬁcation. Our experimental results in Sect. 10.4 also verify that
the texton learning based method performs well for HBST pattern recognition.
As in (Varma and Zisserman 2002, 2005), the texton learning is performed in the
space of MR8 ﬁlter bank responses. Different from (Varma and Zisserman 2002,
2005), which use the k-means method to learn the textons, in this chapter we
employ the technique of sparse representation (SR) (Donoho 2006; Candes et al.
2006) to learn an over-complete dictionary of textons via the l1 -norm minimization.
And under the SR framework, we extract the SR coefﬁcient histogram as the HBST
feature for recognition. By ﬁltering the training images with the MR8 ﬁlter bank,
for each class of HBST images we can construct a training dataset X ¼ [x1, x2, . . . ,
xn], where xi ¼ 1 , 2 , . . . , n, is an 8-dimensional MR8 ﬁltering response vector at a
pixel of the training sample images of this class. A dictionary of textons, denoted by
D ¼ [d1, d2, . . . , dl], will be trained from the constructed training dataset X, where
dj j ¼ 1 , 2 , . . . , l, is a texton. The number of textons is generally much smaller
than that of the elements in the training dataset, i.e., l << n. In the following
sub-sections, we present in detail the method for HBST feature extraction and
classiﬁcation.
10.3.1
MR8 Filter Bank
The MR8 ﬁlter bank (Varma and Zisserman 2002, 2005) is a nonlinear ﬁlter bank
with 38 ﬁlters but only eight ﬁlter responses. It contains 36 bar and edge ﬁlters,
Fig. 10.5 (a) and (b) are the HBST images from one male and one female, respectively
220
10
Hand Back Skin Texture for Personal Identiﬁcation

which are along six orientations and across three scales, as well as a Gaussian ﬁlter
and a Laplacian of Gaussian ﬁlter on a single scale. In order to obtain rotation
invariance, for the edge and bar ﬁlters, the maximum ﬁltering response along six
orientations is selected for each scale. Moreover, using only the maximum orien-
tation response can reduce the number of responses from 38 to 8. Figure 10.6
illustrates the MR8 ﬁlter bank. The motivation for using the MR8ﬁlter bank is to
extract rotation invariant skin texture features since there will be some rotation
variations in the collected HBST images from the same subject. The MR8 ﬁlter
bank responses are rotation invariant while preserving the distinctive features of the
texture images. In (Varma and Zisserman 2002, 2005), the classical k-means
clustering method is used to learn textons for texture image feature extraction and
classiﬁcation. Speciﬁcally, the textons are determined by solving the following
problem:
When computing the MR8 ﬁlter bank responses, there are some pre-processing
steps to follow in order to reduce some effects on feature extraction. Before
convolving the original HBST image with the MR8 ﬁlter bank, all HBST images
are normalized to have zero mean and unit standard deviation. This normalization
can reduce the variations caused by illumination changes. After computing theMR8
ﬁlter responses, the ﬁlter response xi at pixel i is normalized using the Weber’s law
(Varma and Zisserman 2002): xi log(1 þ L/0.03)/L, where L ¼ kxik2 is the magni-
tude of the ﬁlter response vector xi.
10.3.2
Texton Learning Based on SR
SR reveals the fact that if the input signal is intrinsically sparse in some domain, it
can be sparsely represented over the dictionary which can deﬁne the sparse domain.
For a given signal x 2 Rm, we say that x has a sparse approximation over a
Fig. 10.6 The MR8 ﬁlter bank
10.3
HBST Feature Extraction and Classiﬁcation
221
www.ebook3000.com

dictionary D ¼ [d1, d2, . . . , dl] 2 Rm  l, if we can ﬁnd a linear combination of only
“a few” atoms from D that is “close” to the signal x. Under this assumption, the
sparsest representation of x over D is the solution of the following minimization
problem:
arg min
α
α
k k0s:t: x  Dα
k
k2
2  ε
ð10:1Þ
where α 2 Rl is the sparse representation coefﬁcient vector by coding signal x over
dictionary D such that x  Dα and most of the elements in α are close to zero. The
l0-norm counts the number of non-zero elements in the representation vector α.
Because the l0-norm minimization is an NP hard problem, an alternative way is to
solve the l1 -norm minimization problem:
arg min
α
α
k k1s:t: x  Dα
k
k2
2  ε
ð10:2Þ
In the application of HBST analysis, the signal x is an 8-dimensional feature
vector of MR8 ﬁltering response at a pixel of the HBST image. In order to better
represent the query image for the classiﬁcation propose, the dictionary D needs to
be learned from the training HBST images. For each class of HBST images, we
ﬁlter the training images with the MR8 ﬁlter bank and construct a training data set
X ¼ [x1, x2, . . . , xn], wherexi ¼ 1 , 2 , . . . , n , is the MR8 ﬁltering response vector at
pixel i of the training images of this class. The dictionary D associated with this
class can be learned by optimizing the following objective function:
arg min
D, Λ
Λ
k k1s:t: x  DΛ
k
k2
F  ε
ð10:3Þ
where Λ ¼ [α1, α2, . . . , αn] and kkF is the Frobenius matrix norm. We can rewrite
Eq. (10.3) into an unconstrained optimization problem with a penalty term:
arg min
D, Λ
x  DΛ
k
k2
F þ λ Λ
k k1
ð10:4Þ
The optimization problem in Eq. (10.4) is non-convex. Usually we can have a
local minimum by alternatively optimizing D and Λ; that is, from some initializa-
tion of D, we can solve Λ, and then by ﬁxing Λ, we can update D. Such a procedure
iterates until convergence. In this chapter, we adopt the alternating direction
method in (Yang and Zhang 2011) to solve Λ (when D is ﬁxed) and the Lagrange
dual method (Lee et al. 2006) to update D (when Λ is ﬁxed). After we learn the
dictionary of textons for each class of HBST images, we combine these textons into
one big over-complete dictionary, and use it to extract skin texture features.
In (Varma and Zisserman 2002, 2005), the classical k-means clustering method
is used to learn textons for texture image feature extraction and classiﬁcation.
Speciﬁcally, the textons are determined by solving the following problem:
222
10
Hand Back Skin Texture for Personal Identiﬁcation

arg min
dj
X
l
j¼1
X
xi2Ωj
xi  dj

2
2
ð10:5Þ
The k-means clustering will partition the training set X ¼ [x1, x2, . . . , xn] into
l groups Ω1 , Ω2 , . . . , Ωl, and the texton dj is deﬁned as the mean vector of all the
vectors within αi. The k-means clustering based texton learning method can be
viewed as a special case of the SR based dictionary learning. If we require that αi
has only one non-zero element, which is 1, then the problem in Eq. (10.5) will be
basically the same as the problem in Eq. (10.1). In this case, we use only one texton
to represent the feature vector xi and assign the label of xi to that texton. For an input
vector xi which may lie in the boundary of two or more clusters, the k-means
clustering will randomly assign it to one of the classes. However, such a represen-
tation may not be accurate enough in practice. In contrast, by using SR, xi will be
coded as a linear combination of more than one texton, which can achieve a much
lower reconstruction error due to the less restrictive constraint. In the experiments
in Sect. 10.4, we will see that by using the SR technique to learn the textons and the
associated feature description method in Sect. 10.3.3, the HBST recognition accu-
racy can be much improved.
10.3.3
Feature Extraction and Classiﬁcation
Denote by Dk the texton dictionary for the kth class of HBST, the dictionary for all
c classes of HBST images can be formed by amalgamating the c dictionaries, D ¼
[D1, D2, . . . , DC]. With this dictionary D, each training HBST image can generate a
model by mapping it to the texton dictionary.
In the method of (Varma and Zisserman 2002, 2005), each pixel of a texture
image, is labeled with the element in the dictionary D that is closest to the feature
vector at this pixel. A histogram of texton labels of this image is then formed for
classiﬁcation. Different from this method, under the SR framework, we can con-
struct a histogram of the SR coefﬁcients of a texture image for classiﬁcation. The
representation coefﬁcient vector can be obtained by coding the feature vector xi
over D with the SR technique. However, the computational cost of solving the l1-
norm minimization problem to obtain the SR coefﬁcient is very heavy because of
the highly over-complete dictionary D. To reduce the cost of sparse coding, we can
use only a subset of D to represent xi. Speciﬁcally, we use the closest t textons
(t << z, z is the total number of textons learned from the c classes of HBST images)
to xi in D to form a sub-dictionary for xi. Denote by d i
1, d i
2, . . . , d i
t the t closest
textons to xi, and the sub-dictionary for xi is then Di ¼ d i
1; d i
2; . . . ; d i
t


. The
representation vector of xi over Di, denoted by αi ¼ αi
1; αi
2; . . . ; αi
t


, can then be
computed by solving the following l1 -norm minimization problem:
10.3
HBST Feature Extraction and Classiﬁcation
223
www.ebook3000.com

arg min
αi
xi  Diαi
k
k2
F þ λ αi
k k1
ð10:6Þ
The alternating direction method in (Yang and Zhang 2011) can be used to solve
Eq. (10.6). Since Di is a subset of D, once we have αi, we can easily construct
another representation vector hi over D such that:
Diαi ¼ Dhi
ð10:7Þ
Obviously, most of the entries in hi will be 0, and only the entries corresponding
to the same textons as those in Di will have non-zeros values, which are the same as
those in αi.
Finally, for each pixel at position i, we have a representation vector hi. Hence,
we can form are presentation coefﬁcient histogram, denoted by Hf, for this HBST
image by summing all the vectors of |hi|:
Hf ¼
XN
i¼1 hi
j j
ð10:8Þ
where N is the number of pixels in the HBST image. The Hf can be taken as the ﬁnal
feature descriptor of the HBST image for the classiﬁcation purpose. Figure 10.7
shows the coefﬁcient histograms of some HBST images from different persons.
We denote by Hj, j ¼ 1 , 2 , . . . , J, the histogram of a training texture image.
Similarly, for an input test image Y, we can construct a representation coefﬁcient
histogram for it, denoted by HY. The similarity between Hj and HY can be computed as:
χ2 Hj; HY


¼ 1
2
X Hj  HY

2
Hj þ HY
ð10:9Þ
The test HBST image Y can then be classiﬁed with the nearest neighbor
classiﬁer. That is, it is classiﬁed to the class whose training sample has the shortest
χ2 distance to it.
10.4
Experimental Results
10.4.1
Database Establishment
In order to evaluate the proposed HBST analysis method for personal identiﬁcation
and gender classiﬁcation, we established an HBST image database using the
developed HBST imaging device. Those HBST sample images were collected
from 80 volunteers (160 hands), including 61 males and 19 females whose ages
ranged from 20 to 50 years old.
The samples were collected in two different sessions. In each session, each
person was asked to provide six left-hand and six right-hand HBST images,
224
10
Hand Back Skin Texture for Personal Identiﬁcation

respectively. Therefore, 12 samples from one person were collected in each session.
In total, the database contains 1920 samples from 160 hand backs. The average
interval between the ﬁrst and second session is about 30 days, and the maximum
and minimum intervals are 40 days and 14 days, respectively. In the following
experiments, without speciﬁc instructions, we use the samples collected in the ﬁrst
session as the training set and the samples in the second session as the test set.
Due to the various difﬁculties in data collection (e.g., the funding support, the
recruitment of volunteers, etc.), our established database may not be large and
comprehensive enough to support very strong conclusions. Nonetheless, we believe
30
25
20
15
10
5
0
0
1000
2000
3000
4000
5000
6000
7000
30
25
20
15
10
5
0
0
1000
2000
3000
4000
5000
6000
7000
30
35
25
20
15
10
5
0
0
1000
2000
3000
4000
5000
6000
7000
30
35
25
20
15
10
5
0
0
1000
2000
3000
4000
5000
6000
7000
a
b
c
d
Fig. 10.7 The coefﬁcient histograms of HBST images from different persons. (a) and (b) are the
histograms of the left-hand HBST images from the same person while (c) and (d) are the histo-
grams of the left-hand HBST images from another person
10.4
Experimental Results
225
www.ebook3000.com

that its size is reasonably large to illustrate if HBST patterns can be used to assist
personal identiﬁcation and gender classiﬁcation. We are planning to collect more
samples from more subjects in the following years, making our database more
comprehensive and more balanced in terms of male and female subjects.
10.4.2
Personal Identiﬁcation
In this section we aim to answer the question that whether HBST can be used as a
kind of biometric trait to aid personal identiﬁcation. To this end, we conducted ﬁve
experiments using the proposed texton learning method with SR (TL_SR), and we
compare the proposed TL_SR method to some representative texture classiﬁcation
methods such as the multi-fractal spectrum method (Xu et al. 2006), original LBP
(Ojala et al. 2004), dominant LBP (DLBP) (Liao et al. 2009), completed LBP
(CLBP) (Guo et al. 2010) and the texton learning method using the k-means
clustering (TL_KM) (Varma and Zisserman 2002). For the multi-fractal spectrum
method, the dimension of the multi-fractal spectrum vector is set as 26. In the
original LBP, dominant LBP and completed LBP method, the radius of the neigh-
borhood is set to 2 and the number of sampled points in the neighborhood is set
to 8. For TL_KM, 40 textons are learned for each class of HBST images. In the
proposed TL_SR method, 40 textons are also learned per class. Moreover, in the
stage of feature description, for each descriptor xi, t is set as 100, which means that
100 closest textons to xi in D are chosen to form a sub-dictionary to obtain the SR
coefﬁcient. In the following experiments, we use the classiﬁcation accuracy to
evaluate these HBST classiﬁcation methods. The classiﬁcation accuracy is com-
puted as r ¼ nc/n, where nc is the number of correctly classiﬁed test samples and n is
the number of all test samples.
Experiment 1
In the ﬁrst experiment, all classes of HBST images are involved. The left and right
hand HBST images from the same person are taken as from different classes.
Therefore, in this experiment, there are 160 classes and each class has six training
and six test samples. Since the multi-fractal spectrum vector and the histogram
generated by the original LBP method cannot characterize well the appearance
(e.g., cell-like micro-structures) of skin texture, they lead to poor experimental
results in our task. The multi-fractal spectrum and original LBP methods can only
achieve the classiﬁcation accuracy of 35.65% and 46.52%, respectively. Hence, in
the following experiments, we only compare TL_SR with DLBP, CLBP and
TL_KM.
Table 10.1 shows the classiﬁcation accuracies by the competing methods. We
can see that the TL_SR method that uses the SR coefﬁcient histogram as feature is
superior to the TL_KM method that uses the texton label histogram for HBST
classiﬁcation. Also, the proposed method is better than the CLBP method, which
combines the central pixel, magnitude and sign information of the neighborhood to
completely model the LBP operator.
226
10
Hand Back Skin Texture for Personal Identiﬁcation

The interesting HBST image classiﬁcation accuracies validate that the proposed
HBST identiﬁcation system can well capture the characteristics of skin textures,
allowing good discrimination between different classes. These results also suggest
that human identiﬁcation can be aided by HBST analysis.
Experiment 2
In the second experiment, all HBST images are involved. Different from Experi-
ment 1, here the left and right hand HBST images from the same person are viewed
as from the same class. Therefore, in this experiment there are 80 classes and each
class has 12 training and 12 test samples. The experimental results using the DLBP,
CLBP, TL_KM and TL_SR method are compared in Table 10.2. We can see that
for all methods the classiﬁcation accuracy is increased. This is mainly because the
total number of classes is smaller than that in Experiment 1, and the left hand and
right hand HBST images of one person are similar.
Experiment 3
The aim of this experiment is to evaluate the performance on the left and right hand
HBST separately. For either left hand or right hand HBST images, there are
80 classes and 480 images in the training and test sets, respectively. The classiﬁ-
cation accuracies by different methods are listed in Tables 10.3 and 10.4. From the
experimental results, one can see that the classiﬁcation accuracy on the right-hand
Table 10.1 Classiﬁcation accuracies by competing methods
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
75.56%
84.51%
84.40%
86.81%
For one person, the left hand and right hand HBST images are viewed as from two different
classes. Thus there are 160 classes in this experiment
Table 10.2 Classiﬁcation accuracies by competing methods
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
78.59%
86.29%
88.40%
90.17%
For one person, the left hand and right hand HBST images are viewed as from the same class. Thus
there are 80 classes in this experiment
Table 10.3 Classiﬁcation accuracies on the left-hand HBST images
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
80.38%
85.51%
84.54%
88.60%
Table 10.4 Classiﬁcation accuracies on the right-hand HBST images
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
82.91%
86.44%
85.24%
89.71%
10.4
Experimental Results
227
www.ebook3000.com

HBST images is slightly higher than that on the left-hand HBST. This is probably
because most people who provided their HBST samples to our database are right
handed so that they feel more convenient to use our imaging device with the right
hand. Therefore, compared to the left-hand HBST samples, the right-hand HBST
samples collected in our database have less deformation, which results in a slightly
higher classiﬁcation accuracy for personal identiﬁcation.
Experiment 4
In this experiment, we fuse the left-hand and right-hand HBST for identiﬁcation.
That is, both the left-hand and right-hand HBST samples of a person will be
collected to identify his/her identity. Therefore, there are 480 pairs of left-hand
and right-hand samples in the training set, which are from 80 subjects. In the test set
there are also 480 pairs of HBST samples. For the left-hand and right-hand test
samples, we calculate two distances χ2
l and χ2
r, where χ2
l is the distance between the
left-hand test sample and left-hand training sample, and χ2
r is the distance between
the right-hand test sample and right-hand training sample from the same pair. Then
the two distances can be used by the simple weighted average method. The ﬁnal
distance for classiﬁcation is χ2
f ¼ w  χ2
l þ 1  w
ð
Þ  χ2
r, where the weight w can
be trained from the training dataset using the “leave-one-out” strategy. For the four
competing classiﬁcation methods in our experiment, the weights are 0.4, 0.5, 0.45
and 0.4, respectively. The classiﬁcation accuracies by fusing the left-hand and
right-hand HBST with different methods are listed in Table 10.5. Compared with
the results in Experiments 1–3 (please refer to Tables 10.1, 10.2, 10.3 and 10.4), one
can see that the classiﬁcation accuracy by fusing the left-hand and right-hand HBST
images is much increased, showing that the left-hand and right-hand HBST patterns
have complementary information.
Experiment 5
As we mentioned in the Introduction section, one goal of this work is to investigate
whether hand back skin texture patterns can be used to aid other biometrics
identiﬁers to improve personal identiﬁcation accuracy. Therefore, in this experi-
ment we fuse palmprint and HBST for personal identiﬁcation. Since there are
160 hand backs (80 left hands and 80 right hands) in our HBST dataset, we
randomly extract from the PolyU palmprint database (Zhang et al. 2003) 1920
palmprint images, which belong to160 palms (80 left hands and 80 right hands).
Each palm has 12 samples collected from two separated sessions, 6 samples per
session. We then assume that each hand has six palmprint images and six HBST
images in each session, and use the data from the ﬁrst session for training, and use
the data from the second session for testing.
Table 10.5 Classiﬁcation accuracies by fusing the left hand and right hand HBST
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
85.23%
87.24%
89.03%
92.51%
228
10
Hand Back Skin Texture for Personal Identiﬁcation

We use the competitive code scheme (Kong and Zhang 2004) to extract the
palmprint feature, and use the Hamming distance to measure the similarity between
palmprint features.
We fuse the palmprint and HBST matching distances by the weighted average
method. The ﬁnal distance for classiﬁcation is df ¼ w  dp þ (1 þ w)  dh, where dp
is the distance between palmprint sample sand dh is the distance between HBST
samples. In our experiment, the weight w is set to 0.8 by experience. The classiﬁ-
cation accuracies of palmprint, HBST and the fusion of palmprint and HBST are
listed in Table 10.6. Compared with the identiﬁcation rate by either palmprint or
HBST individually, one can see that the accuracy is improved by fusing palmprint
and HBST matching distances. This validates that HBST can be used to aid the
existing biometric traits for personal identiﬁcation.
10.4.3
Gender Classiﬁcation
As can be seen in Fig. 10.5, the hand back skin appearance differs much from male
to female. In most cases, the HBST surface from female is much smoother than that
from male, and size of micro-cells in female HBST samples is smaller than those
for males. Therefore, it is very interesting to verify that if the HBST patterns are
distinctive enough to distinguish males from females. In this section, we conduct
such experiments for gender classiﬁcation.
In our HBST database, there are 61 males and 19 females. In gender classiﬁca-
tion, there are only two classes: male and female. The samples from all the 61 male
subjects are taken as the samples of the male class, and the samples from all the
19 females are taken as those of the female class. The 960 samples collected from
the ﬁrst session are used as the training set, and the other 960 samples from the
second session are taken as test samples. Table 10.7 shows the results by the DLBP,
CLBP, TL_KM and TL_SR method. One can see that the gender classiﬁcation
accuracy can be higher than 98%, which implies that HBST can be aided to
distinguish males from females.
Moreover, in Table 10.8 we present the numbers of falsely classiﬁed male and
female samples by the proposed TL_SR method. As illustrated in Table 10.8,
among the 732 male test samples, nine samples are incorrectly classiﬁed. Among
Table 10.7 Gender classiﬁcation accuracies by different methods
Method
DLBP (Liao et al.
2009)
CLBP (Guo et al.
2010)
TL_KM (Varma and
Zisserman 2002)
TL_SR
Accuracy
95.46%
97.63%
98.60%
98.65%
Table 10.6 Classiﬁcation accuracies by palmprint, HBST and the fusion of them
Feature
Palmprint
HBST
Fusion
Accuracy
98.65%
86.81%
99.58%
10.4
Experimental Results
229
www.ebook3000.com

the 228 female samples, four samples are falsely classiﬁed. The classiﬁcation error
rates of male and female samples are 1.23% and 1.75%, respectively. Although the
numbers of male and female subjects in our database are not balanced, the classi-
ﬁcation error rate on female samples is only slightly higher than that on male
samples. Certainly, we need to collect more samples and make the dataset more
balanced to further validate this conclusion.
10.4.4
Discussion
Currently, compared with the biometric traits such as ﬁngerprints, iris scans and
palmprints, etc., the personal identiﬁcation accuracy of HBST is much lower than
for those. However, each biometric trait has its pros and cons, and no one can
supersede another one. In practice using two or more biometric traits together will
provide a more robust solution. In this work, our goal is to investigate whether hand
back skin texture patterns can be used to aid personal identiﬁcation and gender
classiﬁcation. Considering that HBST images can be collected when capturing
ﬁngerprint or palmprint images, fusing ﬁngerprint/palmprint and HBST can be a
good way for multi-modal biometrics, as we demonstrated in Sect. 10.4.2.
Furthermore, as a speciﬁc type of texture images, the established HBST dataset
can be used to test texture classiﬁcation algorithms in the community of computer
vision and pattern recognition. Different from the commonly used texture datasets
such as UIUC (Lazebnik et al. 2005), CUReT (Dana et al. 1999) and KTH_TIPS
(Hayman et al. 2004), which are challenging in terms of scaling, viewpoint and
illumination variations, the established HBST dataset is also challenging but in a
very different aspect: the high inter-class similarity. In CUReT, KTH_TIPS and
UIUC, different materials are viewed as different classes. However, in our HBST
dataset, samples are from different persons but they are all from the same material:
skin texture. Although there are no signiﬁcant scale, viewpoint and illumination
changes in the HBST dataset, the high inter-class similarity makes it challenging to
achieve a high classiﬁcation rate. Some classical texture classiﬁcation methods
such as LBP and multi-fractal spectrum, which work well on the CUReT,
KTH_TIPS and UIUC datasets, do not work well on the HBST dataset. This
motivates us to develop more advanced texture classiﬁcation methods.
It should be noted that although HBST analysis can assist personal identiﬁcation
and gender classiﬁcation, there are some factors, such as hairs on skin and humidity
of skin, to affect the performance of personal identiﬁcation and gender
Table 10.8 Numbers and rates of falsely classiﬁed male and female samples by the proposed
TL_SR method
Male
Female
Number
9
4
Rate
1.23%
1.75%
230
10
Hand Back Skin Texture for Personal Identiﬁcation

classiﬁcation. In our established HBST database, most of samples are collected
from oriental people so that there are relatively few hairs on the hand back skin. In
our future work, we will collect more samples from more subjects and investigate
the inﬂuences of these factors on skin texture analysis. In addition, modeling skin
texture over a long period is a challenging problem since there are large variations
between skin textures in different ages. Hence, in the future we will study how to
model skin texture over a long period more effectively to improve the performance
of biometric tasks with skin texture analysis.
10.5
Summary
This chapter studied the problem of using hand back skin texture (HBST) for
assisting personal identiﬁcation and gender classiﬁcation. An effective skin texture
imaging system was developed for capturing HBST images. Moreover, we
employed the sparse representation (SR) technique to learn the dictionary of textons
to model the HBST pattern. Then, based on the learned textons of HBST images,
we extracted the SR coefﬁcient histogram as feature for classiﬁcation. To evaluate
the performance of the proposed system, an HBST database was established,
consisting of 1920 images from 160 hands of 80 persons. Extensive experiments
were conducted and the experimental results showed that human identiﬁcation and
gender classiﬁcation can be aided by HBST analysis with good performance. In the
future, more HBST samples need to be collected to verify the different aspects of
HBST analysis and algorithm development. Meanwhile, some factors (hairs on the
skin, humidity, etc.) will be investigated for HBST analysis.
References
Borgen H, Bours P, Wolthusen S (2008) Visible-spectrum biometric retina recognition. In: Pro-
ceedings of the international conference on intelligent information hiding and multimedia
signal processing, Harbin, China, pp 1056–1062
Bruce V, Burton A, Dench N, Hanna E, Healey P, Mason O, Coombes A, Fright R, Linney A
(1993) Sex discrimination: how do we tell the difference between male and female faces?
Perception 22:131–152
Candes E, Romberg J, Tao T (2006) Stable signal recovery from incomplete and in accurate
measurements. Commun Pure Appl Math 8:1207–1223
Cula O, Dana K (2001) Compact representation of bidirectional texture functions. In: Proceedings
of computer vision and pattern recognition, Saint Petersburg, Russia, pp 1041–1047
Cula O, Dana K (2004) 3D texture recognition using bidirectional feature histograms. Int J
Comput Vis 1:33–60
Cula O, Dana K, Murphy F, Rao B (2004) Bidirectional imaging and modeling of skintexture.
IEEE Trans Biomed Eng 12:2148–2159
Cula O, Dana K, Murphy F, Rao B (2005) Skin texture modeling. Int J Comput Vis 1:97–119
References
231
www.ebook3000.com

Dana K, Ginneken B, Nayar S, Koenderink J (1999) Refelctance and texture of real world surfaces.
ACM Trans Graph 2:1–34
Daugman J (1993) High conﬁdence visual recognition of persons by a test of statistical indepen-
dence. IEEE Trans Pattern Anal Mach Intell 11:1148–1161
Daugman J (2004) How iris recognition works. IEEE Trans Circuits Syst Video Technol 1:21–30
Delac K, Grgic M (2007) Face recognition. I-Tech Education, Vienna
Donoho D (2006) For most large underdetermined systems of linear equations the minimal
l1-normsolution is also the sparsest solution. Commun Pure Appl Math 6:797–829
Guo Z, Zhang D, Zhang L, Zuo W (2009) Palmprint veriﬁcation using binary orientation
co-occurrence vector. Pattern Recogn Lett 30:1219–1227
Guo Z, Zhang L, Zhang D (2010) Completed modeling of local binary pattern operator for texture
classiﬁcation. IEEE Trans Image Process 6:1657–1663
Hayman E, Caputo B, Fritz M, Eklundh O (2004) On the signiﬁcance of real-world conditions for
material classiﬁcation, vol 3024. ECCV 2004, Prague, Czech Republic, pp 253–266
Hill R (1999) Retinal identiﬁcation, in biometrics: personal identiﬁcation in networked society.
Kluwer Academic, Dordrecht
Jain A, Flynn P, Ross A (2007) Handbook of biometrics. Springer, Berlin
Kim K, Choi Y, Hwang E (2009) Wrinkle feature-based skin age estimation scheme. In: Pro-
ceedings of the international conference on multimedia and expo, pp 1222–1225
Kong A, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation, vol 1. In:
International conference on pattern recognition, Cambridge, UK, pp 520–523
Lazebnik S, Schmid C, Ponce J (2005) A sparse texture representation using local afﬁne regions.
IEEE Trans Pattern Anal Mach Intell 2:1265–1278
Lee H, Battle A, Raina R, Ng A (2006), Efﬁcient sparse coding algorithms. In: Proceedings of
NIPS, Vancouver, BC, Canada, pp 801–808
Li X, Maybank S, Yan S, Tao D, Xu D (2008) Gait components and their application to gender
classiﬁcation. IEEE Trans Syst Man Cybern Part C Appl Rev 38:145–155
Liao S, Law M, Chung A (2009) Dominant local binary patterns for texture classiﬁcation. IEEE
Trans Image Process 5:1107–1118
Moghaddam B, Yang M (2002) Learning gender with support faces. IEEE Trans Pattern Anal
Mach Intell 24:707–711
Ojala T, Pietikainen M, Maenpaa T (2004) Multi-resolution gray-scale and rotation invariant
texture classiﬁcation with local binary patterns. IEEE Trans Pattern Anal Mach Intell
7:971–987
Ratha N, Bolle R (2004) Automatic ﬁngerprint recognition systems. Springer, Berlin
Rowe R (2007) Biometrics based on multispectral skin texture. In: Proceedings of ICB, Seoul,
Korea, pp 1144–1153
Skin disease atlas (2012) http://www.dermnet.com. Accessed 20 Feb 2012
Sun Z, Tan T, Wang Y, Li S (2005) Ordinal palmprint representation for personal identiﬁcation.
In: Proceedings of CVPR, San Diego, CA, USA, pp 279–284
Tanaka H, Nakagami G, Sanada H (2008) Quantitative evaluation of elderly skin based on digital
image analysis. Skin Res Technol 2:192–200
Tatsumi S, Noda H, Sugiyama S (1999) Estimation of age by epidermal image processing. Leg
Med 1:266–230
Varma M, Zisserman A (2002), Classifying images of materials: achieving viewpoint and illumi-
nation independence. In: Proceedings of European conference on computer vision, Copenha-
gen, Denmark, pp 255–271
Varma M, Zisserman A (2005) A statistical approach to texture classiﬁcation from single images.
Int J Comput Vis 1:61–81
Wechsler H (2006) Reliable face recognition methods-system design, implementation and eval-
uation. Springer, Berlin
Xu Y, Ji H, Fermuller C (2006) A projective invariant for textures. In: Proceedings of computer
vision and pattern recognition, New York, USA, pp 1932–1939
232
10
Hand Back Skin Texture for Personal Identiﬁcation

Yang J, Zhang Y (2011) Alternating direction algorithms for l1-problems in compressive sensing.
SIAM Sci Comput 1:250–278
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 9:1041–1050
Zhang L, Zhang L, Zhang D, Zhu H (2010) Online ﬁnger-knuckle-print veriﬁcation for personal
authentication. Pattern Recogn 7:2560–2571
References
233
www.ebook3000.com

Chapter 11
Line Scan Palmprint Recognition System
Abstract Biometric recognition systems have been widely used globally. How-
ever, one effective and highly accurate biometric authentication method, palmprint
recognition, has not been popularly applied as it should have been, which could be
due to the lack of small, ﬂexible and user-friendly acquisition systems. To expand
the use of palmprint biometrics, we propose a novel palmprint acquisition system
based on the line-scan image sensor. The proposed system consists of a customized
and highly integrated line-scan sensor, a self-adaptive synchronizing unit, and a
ﬁeld-programmable gate array controller with a cross-platform interface. The
volume of the proposed system is over 94% smaller than the volume of existing
palmprint systems, without compromising its veriﬁcation performance. The veriﬁ-
cation performance of the proposed system was tested on a database of 8000
samples collected from 250 people, and the equal error rate is 0.048%, which is
comparable to the best area camera-based systems.
Keywords Identiﬁcation of persons • Pattern recognition equipment • Image
processing • Image sensors • Optical imaging
11.1
Introduction
BIOMETRICS, led by the fast development of imaging technologies and pattern
recognition algorithms, has been utilized in complex ﬁelds, from physical/logical
access control to justice/law enforcement; from time and attendance to healthcare
biometrics (Biometric Applications 2011; Jain and Feng 2009; Laadjel et al. 2009a,
b; Dai and Zhou 2011). Now, biometrics has been required to be real-time, online,
user-friendly, and ﬂexible for these complicated cross-disciplinary applications
other than traditional high performance and robust requirements. For instance,
hand-held ﬁngerprint capturing devices and iris capturing devices, ﬁngerprint
sensors integrated with laptops, ﬁngerprint sensors embedded in locks, iris sensors
integrated with the steel safe. However, one of the best biometric technology, the
palmprint recognition, has been weak in these new features. Current palmprint
recognition systems have been limiting the palmprint recognition applications.
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_11
235

The ﬁrst palmprint recognition system was invented in 1998 (Shu and Zhang
1998). Since then, numerous systems has been created, such as ﬂatbed scanners
(Goh et al. 2003, 2006; Connie et al. 2005; Han et al. 2003; Lin et al. 2005; Struc
and Pavesic 2008; Badrinath and Gupta 2007; Zheng et al. 2007), web camera
based systems (Chaudhary and Nath 2009; Han et al. 2007a, b; Kumar et al. 2003;
Goh et al. 2008, 2010; Zhu and Zhang 2010), and palmprint systems with the
pegged ﬂat platen surface (Kong et al. 2006, 2009, 2003; Kong and Zhang 2002; Li
and Zhang 2009; Wong et al. 2005; Hao et al. 2008; Wang et al. 2008; Zhang et al.
2010a, b, 2009). These systems achieved good performance. However, limiting by
the imaging structure, the dimensions of the devices of these systems cannot be
reduced further. In addition, the user interaction and applications were limited to
current situation.
Line scan technique would be an ideal solution for an online palmprint recog-
nition system. Using line scan technique, palmprint-capturing devices could save
much space for a comfortable user interface and a ﬂexible structure without
sacriﬁce in image quality and system performance. In a lines can sensor (also called
the linear image sensor, the one dimensional image sensor), pixels array in one line,
which is different from area sensors. Line scan sensors are of high dynamic range
and high resolution with the same cost compared with area image sensors.
In this study, a novel line scan sensor, a Contact Image Sensor (CIS) module in
speciﬁc, based palmprint recognition system was proposed. This system was
simpler and smaller than area sensor based solutions in fundamental imaging
structure. It overcame the dimension problem and the noise sensitive optical path
problem, which were caused by area imaging structure. To solve the motion
feedback problem in the proposed CIS based palmprint system, we designed a
novel synchronizing unit. With this unit, linear CIS module could capture a image
when a hand rolls the rollers. In this chapter, the proposed system including image-
capturing device, extraction of region of interest (ROI), feature extraction is
presented. The veriﬁcation performance of the proposed system was comparable
with current area based ones, which was supported by the experiment results in a
database of 8000 images.
Some operational deﬁnitions used in this chapter are listed below.
Sensor: A sensor means a photoelectric sensor chip, for example, a CCD image
sensor chip or a CMOS sensor chip.
Device: A device refers to the full set of image capturing hardware which consists
of lightings, lens, a camera, a frame grabber and the supportive structure of the
device.
System: A system means all the hardware and software, especially the methods and
algorithms in all palmprint recognition procedures.
This chapter is organized as follows. Section 11.2 reviews existing systems. In
Sect. 11.3, the details of the structure and each component of our line scan
palmprint recognition system are presented. In Sect. 11.4, the system performance
is tested by a veriﬁcation experiment. Comparisons between our system and current
area-based systems are also presented here. Section 11.5 concludes the entire
chapter.
236
11
Line Scan Palmprint Recognition System
www.ebook3000.com

11.2
Existing System
Palmprint systems could be divided into three categories: Flatbed scanners,
Palmprint systems with a digital camera or a web camera and Palmprint systems
with a pegged ﬂat platen surface.
11.2.1
Flatbed Scanner
A ﬂatbed scanner was an off-line palmprint capturing device, which collects
off-line palmprint images in high resolution. It was taken as a palmprint scanner
by many researchers (Chaudhary and Nath 2009; Han et al. 2007a, b; Kumar et al.
2003; Goh et al. 2008, 2010; Zhu and Zhang 2010). The scanning time of a ﬂatbed
scanner was typically 10–20 s for a 300 dpi A4 size scan.
11.2.2
Web Camera Based Systems
Web cameras are fast, ﬂexible, and very compact. They are suitable for real-time
video surveillance applications.
The palmprint authentication systems based on web cameras were examined by
plenty of researchers, mainly Han (Han et al. 2007a, b), Connie’s group (Goh et al.
2008) and Zhu’s group (Zhu and Zhang 2010). They made great contributions in
real-time palmprint tracking and the optimization of lightings, as shown in
Fig. 11.1.
Bchind the
bandpass
filter
Color
camera
Bandpass
Filter
NIR
LED
NIR
camera
a
b
c
Fig. 11.1 Web camera based palmprint capturing devices. (a) Han’s double camera design (Han
et al. 2007a, b), (b) Connie’s environment retrained design (Goh et al. 2008). (c) Zhu’s close
design (Zhu and Zhang 2010)
11.2
Existing System
237

Han et al. (2007a, b) proposed a system based on two web cameras in order to
track a palm by comparing the visible spectrum images and near-infrared images
taken by the two cameras respectively.
Connie’s group (Goh et al. 2008) proposed a web camera based system, in which
a simple case was built to protect the system against environmental light. They used
an effective palm tracking method in this system. This system used a web camera
with a 640*480 resolution to capture hand images. After the hand being tracked in
the images, the hand images were further down-sampled using two-dimensional
(2D) wavelet transform. The ROIs extracted from hand images were normalized to
a 150*150 resolution.
Zhu’s group (Zhu and Zhang 2010) proposed another web camera based system
in 2010, in which the light uniformity was carefully examined. Under the evenly
distributed light, the hand image quality was improved, and the system equal error
rate (EER) was 0.17%.
In web camera based systems, the problem of the calibration of hand pose
variations in three-dimensional (3D) space is challenging (Goh et al. 2010).
11.2.3
Palmprint Systems with Pegged Flat Platen Surface
Palmprint systems with pegged ﬂat platen surface (Wong et al. 2005; Zhang et al.
2003) have been popular in recent research. This design was good in hand stability,
background noise, image quality, and recognition performance. The hand was held
by the pegged ﬂat platen surface in this kind of devices, as Fig. 11.2a, b show. The
Fig. 11.2 (a) PC based area palmprint device, (b) Embedded area palmprint device. Typical
palmprint devices with a pegged ﬂat platen surface
238
11
Line Scan Palmprint Recognition System
www.ebook3000.com

user interface platen of Zhang’s palmprint system covered most of the background
area and the only left parts between ﬁngers were covered by the upper cover.
Currently the best results reported in research articles were mostly achieved in
the database captured by Zhang’s device, and the EERs was summarized to be
distributed from 0.267% to 0.012% (Hao et al. 2008; Wang et al. 2008; Zhang et al.
2010a, b; Guo et al. 2009a, b; Laadjel et al. 2009a, b; Ito et al. 2006; Zuo et al.
2008), which would be generally better than other systems’ EERs, which were
outlined from 0.26% to over 4.73% (Goh et al. 2003, 2006, 2010; Connie et al.
2005; Han et al. 2003, 2007a; Lin et al. 2005; Struc and Pavesic 2008; Badrinath
and Gupta 2007; Kumar et al. 2003).
This palmprint system used an area scan camera. The area scan camera is a
camera with an area scan charge-coupled device (CCD) or complementary metal-
oxide-semiconductor (CMOS) sensor. When an area scan camera captures images,
all pixels of a frame are captured at the same time. Then electrons of pixels transfer
line by line to the output channel and are ampliﬁed and sent to the output port pixel
by pixel.
11.3
Line Scan Palmprint System Design
In order to take advantages of cutting-edge line scan image sensors, we proposed an
online palmprint system based on the line scan image sensor. A synchronizing unit,
which was composed of a roller module, an encoder, and an FPGA, was designed
for our system. It synchronized the motion of hands with the capturing of images of
the line scan image sensor. With this unit, the proposed system captured the
palmprint adaptively to the motion of hands in real-time. Meanwhile, the resolution
and quality of images were similar as current area systems. A control board was
implemented based on an FPGA. The USB interface was utilized. With this
universal interface, the proposed capturing device worked with either a desktop
computer or an embedded platform, which was a major advantage for next gener-
ation real-time online applications. The rest of this section starts with the line scan
imaging scheme, followed by the system framework. After the introduction of
system framework, the three hardware parts: the line scan image sensor, the
synchronizing unit, and the controller board are presented. Then the ROI extraction
is reported. The system is proposed in the last.
11.3.1
Line Scan Imaging Scheme
In a line scan sensor, there is only one line of pixels. Pixels in a line scan sensor can
be extended to enlarge the imaging area. With a larger imaging area, the dynamic
range could be improved. This is different from the area sensors. In are a sensors,
the size of pixels is a trade-off between resolution and dynamic range given a
11.3
Line Scan Palmprint System Design
239

certain kind of pixels. In area cameras, a larger pixel could bring a higher dynamic
range, but lower resolution. This problem does not exist in line scan sensors. Line
scan image sensors could capture high-resolution images without compensating in
dynamic range.
Furthermore, in an imaging system, line scan sensors requires much less optical
space than area image sensors, which is the most important limitation of current
palmprint capturing devices. As Fig. 11.3b shows, a line scan sensor takes a line
image. In a line scan sensor imaging system, the ﬁeld of view is only one pixel
length in the perpendicular direction. Comparing to the angle of view in the length
direction, it is merely one of hundreds or one of thousands in the perpendicular
direction. Therefore, in a line scan imaging system, the optical path, which is
required by the traveling of rays from the object to the sensor, is like a thin plane
in three-dimensional space. On the other hand, the optical path in a traditional area
image sensor is two cone-shaped-space, as Fig. 11.3a shows. Furthermore, in an
imaging system, this optical path is critical to the ﬁeld of view, which is determined
by the dimensions of objects. The ﬁeld of view can be extended by increasing the
angle of view. However, while the angle of view is increasing, the off-axis lights
come into the optical system, and there will be bigger distortion aberration. Because
of the reason above, the place taken by the line scan sensor optical path is much
smaller than the space taken by the area image sensor, which cannot be further
reduced in theory.
Fig. 11.3 (a) Area image sensor based palmprint capturing device imaging scheme. (b) Line scan
image sensor based palmprint device imaging scheme. A comparison of optical path space
between area scan image sensor based palmprint-capturing devices and line scan image sensor
based devices
240
11
Line Scan Palmprint Recognition System
www.ebook3000.com

When building a system based on the line scan sensor, there is a challenge of
motion synchronizing. A line scan image sensor captures images only with the
help of a scanning unit who moves objects or the sensor along vertical direction.
The beneﬁt of this structure is that continuous images of objects could be
captured and a long or large object could be captured by a thin or small sensor.
However, an extra unit, which synchronizes the movement of the sensor and the
object, is required.
This synchronization problem was solved in industrial machine vision systems.
In industrial machine vision systems, line scan image sensors and cameras have
been used for decades. A typical structure of an industrial line scan system
consisted of a line scan camera, a conveyor belt and a motion controller (Kim
et al. 2001; Baykal and Jullien 2004). Objects were moving smoothly on the
conveyor belt when the line scan camera was taking line images. A motion
controller monitored and adjusted the moving speed of the conveyor belt, and
triggered the line scan camera to capture images synchronously.
However, this problem was different in building a lines can palmprint system,
and it was still a challenge. First, in palmprint system, the imaging object is a part of
human body. The palm cannot be moved as a physical subject. The synchronizing
should be driven by the palm of people. Second, the motion of palm is not in a
constant speed without any guide. Typically, the human palm moves in a way of
various speeds and directions unless instructed or guided. Last, when building a
palmprint system, the user interface should be user-friendly. The motion of palm
should not be limited to a rigid way. In summary the motion synchronizing should
be responsive, adaptive and user-friendly.
A line scan camera device in the ﬁngerprint recognition was invented by
Mil’shtein et al. (2008), which is different from the palmprint case. In the ﬁnger-
print line scan device, the camera was rotating around the ﬁnger when capturing.
The camera captured one vertical line of ﬁngerprint image at a time, until 180∘of
the whole ﬁnger surface was scanned. This design captured a cylindrical ﬁngerprint
image. However, it was not suitable to be applied in palmprint. The skin surface of
palm is a bumping plane, and the palmprint is not a cylinder. This structure could
not be used in line scan palmprint systems.
Though a line scan sensor resembles a swipe ﬁngerprint sensor, they are not the
same. A swipe sensor captures a rectangle image, whose aspect ratio is as small as a
bar is. The bar images are matched with each other to assembled the full ﬁngerprint
image. Although swipe sensors are very successful in ﬁngerprint capturing, they are
not suitable for palmprints. First, the palm is not completely ﬂat. The center area of
the palm is too far to be captured by the swipe sensor. The capacitive swipe sensor
has to be touched and the ultrasonic wave based swipe sensor gets true skin layer
ﬁngerprint in less than 1 mm distance. When working with a longer working
distance, the sensor chip generates tremendous heat. Second, the silicon chip is
expensive. The palm is over four times larger as a ﬁngerprint in width. The larger
the chips’ dimension, the lower conformance rate of chips. The swipe sensor
customized for palmprint would not likely be economical.
11.3
Line Scan Palmprint System Design
241

11.3.2
System Framework
The framework of a general optical biometric system was composed of three parts:
the image-capturing device, the computation platform, and the algorithms. The
image-capturing device collects biometric samples. It is composed of a light source,
lens, a camera or a sensor module, a frame grabber or an A/D converter. The
computation platform is for the storage and data process purpose. A desktop
computer is the most common platform. The algorithm is the way to extract ROIs
and features and match encoded palmprints, which is used to identify or verify the
subject.
Biometric applications deployed in hand held or mobile platforms are very
popular (Shen et al. 2012; Han et al. 2007a, b; Jia et al. 2012). The embedded
version of biometric systems is expected other than the desktop ones. A framework,
in which both the desktop computation platforms and the embedded platforms were
compatible with the same image-capturing device, was proposed for our lines can
sensor based system. The framework is illustrated in Fig. 11.4. This device used an
embedded controller, which controlled all the device parts and communicated with
the computation platform through a universal interface. With this structure, the
proposed palmprint capturing device could be used in both desktop applications and
embedded applications with respective drivers.
Plam
Database
Application
Matching
Feature Extraction
ROI Extraction
Driver
USB Interface
Controller Board
Light
Encoder
Light
Rollers and Gears
Synchronizing Unit
Line Scan Image Sensor (CIS Module)
Micro Lens Array
Linear CCD Array
LED
Line Scan Palmprint Capturing Device
Computation Platform(ARM/PC)
Fig. 11.4 The diagram of the system framework
242
11
Line Scan Palmprint Recognition System
www.ebook3000.com

In order to achieve the goal of the proposed system, to get high quality images
for practical online applications, the image capturing speed, the image quality and
the user interface design are the most important parts in palmprint recognition
systems.
The data ﬂow speed of the system is deﬁned by the slowest part. Hence, each part
should be optimized to maximum speed. First, the synchronizing unit should
generate accurate synchronizing pulses according to the movement of the hand to
minimize the lag. Second, the sensor should be fast enough for the real-time image
capturing to minimize the data waiting time in buffer. The driver of the sensor
should generate driver signals at a proper speed and accurate timing to minimize the
lag. In addition, the speed of A/D converter should be faster than the output of the
pixels. A buffer could help to cache the data during waiting for transferring through
the interfaces.
To capture images with proper quality, the sensor, the optical system and the
synchronizing unit should be selected with much care. The sensor resolution should
be large enough to present the palmprint texture. Capturing images of proper
resolution requires that the resolution of the image sensor and optical parts should
be matched. The most challenging part is the synchronizing between the hand
motion and the image capturing. The resolution of vertical and horizontal direction
of the image should be matched accurately. The horizontal resolution of the system
has to fulﬁll the palmprint feature requirements. According to Wong (Wong et al.
2005), the feature of a palmprint is composed of three parts, principal lines,
wrinkles and ridges which lie from 75 dpi to 150 dpi. For online palmprint system,
the resolution should be equal or over 75 dpi, which is analyzed by (Zhang et al.
2003) after the study of (Shu and Zhang 1998; Zhang and Shu 1999). In summary,
100 dpi was a good balance between image quality and speed for palmprint
recognition system. The synchronizing unit is required to maintain a correct
resolution aspect ratio between the horizontal resolution and the vertical resolution.
In order to capture image lines in exactly same density as the horizontal resolution,
which is a built-in parameter of lines can sensor, the synchronizing unit should
generate accurate line image capturing signals. These signals keeps the same
density of line images in vertical as the density of the pixels in the line scan sensor
in the length direction, and it maintains the right aspect ration.
11.3.3
Line Scan Image Sensor-CIS Module
A line scan sensor is a kind of image sensor with a very simple structure (Chang
et al. 2012; Fischer and Radil 2003; Luna et al. 2010; Marino et al. 2007; Watanabe
and Hokari 2006). The photo diodes are simplest arranged in a linear array or a
single line. Applications using line scan sensors are designed with either the camera
or object moving in a direction perpendicular to the row of sensors. They are
applied in the applications in which the object motion is under rigid control, for
example, document scanning. They can be made by CCD or CMOS technology.
11.3
Line Scan Palmprint System Design
243

Instead of traditional line scan CCD or CMOS sensor chip, a new highly
integrated CMOS line scan sensor module, which is called Contact Image Sensor
(CIS), is designed for the proposed system. This CMOS based line scan sensor
module was an integrated module including LED lights, micro lens and several
CMOS line scan sensors. These parts could be integrated in one package, and all the
control signals could be pinned out through one common connector. This module
was customized by us and then was produced in a professional factory. The factory
assured the performance by calibrating the lens, the sensor chip, and the LED lights
in according to the speciﬁcations.
The CIS sensor was customized for high-speed line scan purpose. The basic
scanning resolution was 200 dpi, and could be conﬁgured as 100 dpi or 50 dpi
resolution. The major features are listed in Table 11.1.
Table 11.1 shows that, this sensor was 183 mm long, which is designed to
capture the whole hand. The mediocre hand size is 189 mm for male and 172 mm
for female (Average Hand Size 2012). The sensor size was set 4 mm larger than the
average masculine hand, because the palm was the focus of the proposed system. In
addition, the average hand width, which is also the width of the palm, is 84 mm for
male and 74 mm for female. 183 mm was sufﬁcient for the capturing of human palm
both for male and female. It also covered a little more area of ﬁngers, which was
taken as a stable background in image processing later. This black background was
necessary for the ROI extraction.
The resolution of this CIS sensor could be conﬁgured to 200 dpi, 100 dpi, and
50 dpi. The palmprint features are mostly in from 100 dpi to 150 dpi resolution
according to (Wong et al. 2005). Considering the trade-off of the resolution and the
computation cost, which affected the computation speed greatly, 100 dpi would be
an optimal resolution for the proposed device. This 100 dpi preserved the majority
of the palmprint features and saved lots of storing space and computation cost.
As is shown in Fig. 11.5, this CIS sensor scans at a speed of 45 μs per line in
100 dpi mode. It captures 720 lines of image at a time to get the full hand image
including the wrist part. The image capturing takes 33 dpi. This capturing speed
ensures that the processing time was not delayed by the sensor. However, the entire
processing time largely depended on the movement of hands.
Table 11.1 Line scan image
sensor (CIS module)
characteristic
Items
Value
Scanning width
183 mm
Element density
100 dpi
Scanning speed
45 μs/line
Light sources
Red: λp ¼ 630  15 mm 60 mA
Green: λp ¼ 520  15 mm 60 mA
Blue: λp ¼ 465  15 mm 60 mA
Infra red: λp ¼ 940  15 mm 60 mA
Data output
Three analog parallel output channels
244
11
Line Scan Palmprint Recognition System
www.ebook3000.com

0
1
64
279
0
1
CLK
SI
CNT
SIGI
1
216
432
720
CNT=GND
Note:After 216th , 432th and 720th signal,at least 7 clocks needed.
1
1
1
1 line (45usec)100DPI
217
433
SIG2
SIG3
VLED
LED Light Guide
Rod Lens Array
LEDr
LEDg
LEDb
LEDir
VDD
1
108
109
216
217
360 50dpi
100dpi
200dpi
S/R& switches
S/R& switches
S/R& switches
720
1440
216
217
432
433
865
864
433
432
1
1
GND
SI
CLK
CNT
Vref
SIG1
SIG2
SIG3
Tmax
Tgrn
LEDr/g/b/ir
–
a
b
Fig. 11.5 (a) The timing graph of the CIS module. (CLK—clock timing, SI—starting pulse,
LEDr/g/b/ir—LED integration switches, SIG1—output signal 1, SIG2—output signal 2, SIG3—
output signal 3). (b) The controlling block diagram with three parallel output channels: SIG1,
SIG2 and SIG3. CIS module timing diagram and block diagram
11.3
Line Scan Palmprint System Design
245

After capturing the image, this CIS sensor outputs the data through three parallel
channels. The output structure of this CIS module is shown in Fig.11.5b. The CIS
module outputs the ﬁrst 1–216 pixels in SIG1 pin; then outputs 217–432 pixels in
SIG2 pin; outputs the rest pixels in SIG3 pin. This structure maintained the fastest
output speed, and it accelerated the output in a most convenient way. A typical A/D
converter was working in three parallel channels, because the most typical appli-
cation—video application requires three channels for BGR (Blue, Green and Red)
signals. AD9822 is chosen as the A/D converter of the proposed device. This A/D
converter was designed to work with BGR CCD sensor. There are three parallel
input pins for three BGR channels. It converts three pixels in parallel within a
period at 67 ns, which is over twice faster as the customized CIS module. The
AD9822 A/D converter converts a line, which contains 720 pixels in three channels,
at the speed of 45 μs per line, which is the speed of the CIS module. The delay of
this A/D converting could be neglected compared with the entire reading time.
This CIS module equipped with a Rod Lens Array (RLA).The RLA is an array of
cylindrical rod lens, which are highly polished small diameter rods made of
optically transparent homogeneous materials. They are working as cylindrical
lens for the linear array CMOS sensor pixel by pixel. The typical working distance
of this kind of lens is about several millimeters. In the designed module, it was
3 mm (2/+4 mm), which was set according to our synchronizing unit to ensure the
identical image resolution along vertical and horizontal directions.
As shows in Table 11.1, there were four types of LEDs deployed in our CIS
module, 630 nm for red color, 520 nm for green color, 465 nm for blue color and
940 nm for infrared spectrum. Typical palmprint features lay in the visible spec-
trum, from 380 to 780 nm. They make a hybrid of white color with the color
temperature 6500 K. Recent research (Zhu and Zhang 2010) has discovered that
multi-spectral palmprint images are better than traditional palmprint recognition
using the visible spectrum only. In the proposed CIS module, near infrared LEDs
were deployed with the same intensity as the white light. The combination of
visible and infrared illumination was good for anti-spooﬁng purpose.
11.3.4
Synchronizing Unit
A synchronizing unit is designed to synchronize the motion of the hand with the
CIS module capturing. There are three purposes of this unit. First, it sends synchro-
nizing pulse signals to the CIS sensor. These signals should be distributed in 100 dpi
density, which is the same as the horizontal resolution of the CIS module. Then, it
holds the hand ﬂattened and stretched. If the palm touches the surface of the CIS
sensor, it squeezes and the lines and texture are distorted. At last, it keeps the hand
skin 2–4 mm above the surface. The CIS module only captures objects in a distance
of 1–7 mm. The depth of view of it is very limited.
246
11
Line Scan Palmprint Recognition System
www.ebook3000.com

The synchronizing unit consists of a pair of rollers, a gear set, and a photoelectric
encoder, as is shown in Fig. 11.6. A pair of rollers holds the hand three millimeters
above the surface of CIS module; meanwhile it stretches the palm skin to avoid the
squeeze. First, this distance is optimal working distance of the CIS module. Second,
when a hand is pressing on the rollers, the skin inside the ﬁeld of view between two
contact rollers is actually 1–2 mm lower than the top of rollers. Because it requires
pressure to push the roller, the hand is pressing on the rollers with a moderate force.
Though the force is meant to move the roller by the user, it stretched the skin of the
palmprint, and kept it ﬂattened. Third, the distance between the two rollers is
minimum to avoid too much variation of the hand skin. It is the minimum distance
to keep the CIS module under the rollers top surface. In addition, the two rollers are
made of the same size to maintain a stable motion. They are also synchronized by
the gear set. It means they are rolling at the same angular velocity. The hand motion
on these rollers is both stable and synchronized.
The material of the rollers is copper. Three kinds of metal were tested in the
making of these rollers. Either aluminum alloy or steel was not strong enough for
them. The earlier samples were abraded by sweaty hands within a month. Eventu-
ally the copper rollers have worked for over half a year, and it was still shining
brightly. Tiny ridges and valleys were carved in the cylindrical surface of rollers.
They ensured that the frictional force was strong enough. In addition the rolling of
the rollers was accurately driven by the hand motion.
Then the motion on the rollers is digitalized by the photo electric encoder to send
out synchronizing pulses through the gear set, as illustrated in Fig. 11.7. The image
resolution on the motion direction is deﬁned by the rollers and the gear ratio. The
photoelectric encoder here is industry standard 500 pulses per round. The resolution
SL is deﬁned as Eq. (11.1).
Fig. 11.6 CIS module and the synchronizing unit composed of a pair of rollers, a set of gears and
an optical encoder
Fig. 11.7 The gear set keeps the two rollers synchronized and sends rotations to the encoder
11.3
Line Scan Palmprint System Design
247

SL ¼
25:4  500
π  DR  Rg  Rf
ð11:1Þ
Here dR is the diameter of both rollers; Rg is the gear ratio; Rf is the ﬁlter ratio,
which is set in the controller. In our prototype device, the rollers’ diameter is
10 mm, the gear ratio is 2:1, and Rf is 2:1. Under this condition, the vertical (the
rolling direction) resolution is 100 dpi  1:5 dpi, which is almost the same with the
resolution along the width direction of the CIS module setting. The gears have to be
large enough to transmit power through to the encoder. The gear ratio cannot be 4:1,
in which case the output gear is too small to maintain a stable transmission. A pulse
ﬁlter is set in the controller. It sends out one pulse when receiving two pulses (2: 1).
With this method, the sampling rate is set to 100 dpi as expected.
11.3.5
Controller Board
An FPGA board was built as the controller. It comprised a CIS driver, an A/D
controller, a data buffer and a USB interface. The block diagram of the controller is
shown in Fig. 11.8a.
The FPGA is the core of this board, and it controls the CIS driver, the A/D
controller, and the data buffer. It reads in the motion pulses, and sends out driving
timing pulses to the CIS module. When the CIS module is ready for one line, the
FPGA lets the AD9822 A/D converter start to read in analog pixel signals. Then the
digitalized 8 bit image signals are read and stored in the SDRAM buffer. Finally,
the data are sent out through the USB interface.
Sensor
A/D Convertor
Control signal
Digital signal
Drive signal
Encoder
FPGA
Buffer
Data
Data
PC/ARM
USB FIFO
Command
Reset Logic
Power Supply
Oscillators
Ext. Interfaces
Command
SDRAM
Synepulse
Analog
signal
Fig. 11.8 (a) Scheme block diagram, (b) PCB layout, The controller board of line scan palmprint
capturing device
248
11
Line Scan Palmprint Recognition System
www.ebook3000.com

11.3.6
The Device
Figure 11.9 shows the image-capturing device, which is composed of the line scan
sensor, the synchronizing unit and the controller board. The proposed device could
capture lines can palmprint images with either a desktop computer or an embedded
ARM platform. The only two interfaces connected to it are a 12 V power supply and
a USB mini B plug.
11.3.7
ROI Extraction
The images captured by the proposed device are shown in Fig. 11.10. The position
of hands varies in the images indifferent captures. An automatic ROI extraction
method was implemented. This ROI extraction method was an optimized method
based on Guo’s work (Guo et al. 2009a, b). Because the background of the line scan
palmprint image was pure black, the line scan image was free from noise and
environment light, which were commonly appeared in images captured by area
sensor based devices. The optimization focused on the reduction of the noise
removal part, which affected every step of the ROI extraction.
The extraction of ROI contains ﬁve steps: preprocessing, binarization, contour
extraction, ﬁnding tangent points and computing ROI location.
Fig. 11.9 (a) The 3D design model of the capturing device, (b) A right hand was testing on the
device, The proposed image-capturing device
Fig. 11.10 The captured palmprint images
11.3
Line Scan Palmprint System Design
249

Preprocessing: In the preprocessing step, the raw image was normalized to
remove the noise of the sensor chip. The grayscale value of the raw line scan
image pixel was deﬁned by both the characteristics of the time of the exposure and
the pixel sensor. The noise of the same position of the same object between lines
was caused by the time of the exposure. In order to remove the exposure time
noises, standard white and dark images were captured as reference to normalize the
line scan sensor. An exposure vector was extracted from 50 vertical lines to remove
the exposure time noise. The images were normalized using this exposure vector by
dividing each pixel of the image by it. The noise between pixels in one horizontal
line was caused by the differences of the characteristics of the pixels, which was
caused by dark current. Then dark current noises of the pixels were extracted from
ten images of the standard black objects. As Fig. 11.11a shows, after subtracting the
dark current noise from the image, images were normalized to 0–255 gray-scale.
The preprocessing was implemented in the FPGA. The normalizing was
implemented after capturing each line image.
Binarization: In the second step, fast Otsu method was used to binarize images
with a dynamic threshold. Since the depth of view of proposed system is optimized
to palm skin, there is no environmental interference in the background. As
Fig. 11.11b shows, the background always appears as a black area, even though
there is no back cover above the sensor surface. Comparing with area based images,
extra morphological operations and blob detection can be saved.
Fig. 11.11 (a) Palmprint sample example, (b) Binarization result, (c) Contour extracted, (d)
Tangents points found, (e) ROI located, (f) Extracted ROI. Five-step ROI extraction
250
11
Line Scan Palmprint Recognition System
www.ebook3000.com

Finding Tangent Points: Here the tangent points are deﬁned as the point of
tangency where lowest tangent line meets the ﬁrst and third valleys. The ﬁrst valley
is between the index ﬁnger, and the third valley is between ring ﬁnger and little
ﬁnger. To ﬁnd these two valleys, distances from points on the contour to the hand
center is computed ﬁrst. Local minima, which represent the valley points between
ﬁngers, are found as valley candidates. Second both clockwise and counter clock-
wise directions of hand contour are found to remove local minima of minor noise on
hand shape and repeat recursively to ﬁnd three valley points between ﬁngers. Then
the contour curve segments near the ﬁrst and third valleys are extracted, which is
depicted in Fig. 11.11d. The tangent line and two tangent points are found on these
curve segments.
Computing ROI region: To locate the palmprint ROI region in the hand image, a
coordinate system was set up. The line connecting the two tangent points were
taken as y-axis. The middle point between them was taken as original point. Then in
this coordinate system, the ROI region was obtained in a 128*128 pixel area
between coordinates (50, 64), (178, 64), (50, 64), (178, 64), which is shown
in Fig.11.11e. The ROI was cropped, which is illustrated in Fig. 11.11f. There are
some examples of extracted ROIs shown as Fig. 11.12.
11.4
Experiment and Comparison
In this section, the experiment of the proposed system on a large database is
presented.
11.4.1
Line Scan Palmprint Database
A line scan palmprint sample database was built including 8000 line scan palmprint
samples from 250 people, in order to evaluate the performance of proposed system.
The subjects were volunteers from universities and neighboring communities. In
the database, 189 people are male and the age distribution is from 20 to 63 years
Fig. 11.12 Extracted palmprint ROIs
11.4
Experiment and Comparison
251

old. The samples from both left and right hand were collected in two separate
sessions. Eight samples from each hand were collected in each session. The interval
between the two sessions was 24.5 days in average. When collecting samples, the
subject was asked to roll the rollers of the line scan palmprint-capturing device. A
subject was trained for less than 30 s before the sampling. In this short-time
training, a helper demonstrated the sampling process for two to three times, and
then each subject tried for three to eight times on our line scan palmprint device,
before the sampling in each session. Our database contains 8000 samples from
500 different palms. The resolution of the samples is 720*720 (100 dpi).
11.4.2
Veriﬁcation Experiment
To compare with current real-time online area based palmprint systems (Wong
et al. 2005; Zhu and Zhang 2010; Zhang et al. 2003), a veriﬁcation experiment was
built under the environment similar to area systems.
Figure 11.13 presents the design of the experiment. The experiment was
designed following the area palmprint systems’ convention. This experiment
consisted of four parts: image capturing, ROI extraction, feature extraction, and
matching. Image capturing and the ROI extraction are discussed above. In feature
extraction part, the competitive coding scheme was implemented to extract feature
and to encode the feature of palmprint ROIs. The competitive coding scheme
(CompCode) (Kong and Zhang 2004) was a very effective 2D Gabor based
palmprint feature extraction method. It extracted the orientation information of
Image Capturing
Optimized ROI
Extracation
Feature Extraction
Matching
Competitive Coding
Scheme
Fig. 11.13 Line scan
palmprint system
veriﬁcation experiment
ﬂowchart
252
11
Line Scan Palmprint Recognition System
www.ebook3000.com

palm lines. This method was widely used in area based palmprint systems. This
method was the most promising method in real-time online palmprint feature
extraction study.
In the matching part, the Humming distance was adopted following the best
practice of area-based convention (Zhang et al. 2010a, b), which was proved to be
efﬁcient and effective in palmprint feature matching. With this design of experi-
ment, the line scan palmprint system was tuned to work in the identical setting as
area sensor-based ones. The veriﬁcation accuracy is computed in the following
tests, each palmprint sample is matched with all the other palmprint samples in the
database. A match is counted as a genuine if the two samples are from the same
palm; otherwise, it counted as an impostor. The total number of matchings is
31,996,000 and the number of genuine matchings is 60,000. The EER, which is
the point when false accept rate (FAR) is equal to false reject rate (FRR), is used to
evaluate the accuracy.
As is shown in Table 11.2, the EER is 0.048% when the FAR equals with FRR.
The proposed system achieved a veriﬁcation performance comparable with area
sensor-based designs without compromising in other aspects. Though the perfor-
mance (EER) was not better than the best result in all area counterparts, it was still
comparable. This frame work and structure showed its capability as a promising
palmprint application.
11.4.3
Comparisons with Current Palmprint Systems
A detailed comparison with area sensor-based palmprint system is shown in
Table 11.2. The outer shape of line scan palmprint system is only 22  5  5 cm3.
Compared with current area image sensor based palmprint devices, the proposed
system was much smaller. The size of the proposed system was less than 6% in size
as area based ones. The size of area sensor-based systems could not be reduced
further in area-based design. The area-based design could not reduce the outer
shape smaller than 16  16  20 cm3 in theory considering the space taken by the
optical path and the back cover. In contrast, the outer shape of line scan palmprint
systems could be further reduced greatly after moderate improvement.
In addition, the line scan system improved the speed performance of the
palmprint sampling process. The line scan palmprint system works faster than
area ones when capturing images. The more important is that the line scan
palmprint system captures samples according the user’s movement. The speed of
capturing was self-adaptive to the user.
With all above improvement, the line scan palmprint system didn’t compromise
in veriﬁcation performance. The line scan palmprint system achieved a veriﬁcation
performance comparable with most advanced area sensor-based counterparts.
11.4
Experiment and Comparison
253

Table 11.2 Comparison between the line scan palmprint system and area scan systems
Items
Line scan
palmprint system
System (Wong
et al. 2005)
System
(Zhang et al.
2003)
System (Zhang
et al. 2010a, b)
Type
Line scan
Pegged platen
Pegged platen
Pegged platen
Surface
Surface
Surface
Hand pose
Stretched and roll
Stretched
Stretched
Stretched
Sensor
CIS module
CCD camera
CCD camera
CCD camera
Image resolution
50 dpi/100 dpi/
75 dpi/100 dpi/
75 dpi
<100 dpi
200 dpi
125 dpi/150 dpi
Illumination
Multispectrum
White
White
Multispectrum
LED (Fused)
Fluorescent
Fluorescent
LED
(Alternative)
Lenses
Rod lens array
Industrial
Industrial
Industrial
(Integrated)
Std C mount
Std C mount
Std C mount
A/D converter
Three parallel
PCI 8bit
PCI 8bit
PCI 8bit
Channels 8 bit
Cold start time
40 ms
90 ms
90 ms
90 ms
Image-capturing
time
20–500 ms
40 ms
40 ms
40 ms
Self-adaptive
ROI extraction
time
90 ms
–
538 ms
138 ms
Feature extrac-
tion time
36 ms
–
84 ms
36 ms
Matching time
0:06 ms
–
1:7 ms
0:06 ms
Batch matching
time (s/samples)
1:5/1000
1:5/400
1:1/100
1:5/1000
Database
(Population)
250
235
193
250
Database (Num-
ber of samples)
8000
9400
7752
6000
Veriﬁcation Per-
formance (EER)
0:048%
–
0:6%
0:021 ~ 0:052%
Dimensions
(cm  cm  cm)
22  5  5
32  16  19
32  16  19
34  28  26
Cost without PC
and case
$160
–
–
~$220
Platforms
supported
Desktop and
embedded
Desktop
Desktop
Desktop
Windows and Linux
Windows
Windows
Windows
254
11
Line Scan Palmprint Recognition System
www.ebook3000.com

11.5
Summary
A novel line scan sensor based palmprint recognition system is proposed to improve
the palmprint biometrics. An online line scan image sensor based palmprint system
is made. This line scan palmprint is featured of a customized highly integrated line
scan sensor, a self-adaptive motion feedback, and a cross-platform control board.
The size of the proposed system is less than 6% of the size of current palmprint
systems, without compromising in veriﬁcation performance. The veriﬁcation per-
formance is tested in a database of 8000 samples from 250 people and the EER is
0.048%. This system is proved to be suitable for online palmprint biometric
applications. The future research of line scan palmprint system could be in three
directions. First, using the same structure provided by the proposed system, an even
smaller system could be made with further study on optimization of the mechanical
structure of the current design. Second, the materials and the structure of rollers
should be studied. Self-cleaning materials should be used to keep the surface clean.
More durable materials of the roller can contribute in reducing the size of the
rollers. Third, the motion feedback can be integrated into the line scan module.
With moderate improvements on the synchronizing unit, the ideal size of the
proposed system in mass production should be less than to 15  2  2 cm3 in
theory. We hope that, using the proposed system, the line scan palmprint biometrics
could be used in many space critical and portable situations, where previous area
sensor-based palmprint systems cannot.
References
Average Hand Size (2012) Your resource for hand size, palm sizeand ﬁnger length averages
[Online]. Available http://www.theaveragebody.com/averagehand size.php
Badrinath G, Gupta P (2007) An efﬁcient multi-algorithmic fusion system based on palmprint for
personnel identiﬁcation. In: Proceedings of 15th international conference advanced computing
and communications, Washington, DC, USA, pp 759–764
Baykal I, Jullien G (2004) Self-synchronization of time delay and integration cameras. J Electron
Imaging 13(4):680–687
Biometric Applications (2011) [Online]. Available http://www.ﬁndbiometrics.com/
Chang J, Cheng K, Hsieh C, Chang W, Tsai H, Chiu C (2012) Linear CMOS image sensor with
time-delay integration and interlaced super-resolution pixel. IEEE Sensors J:1–4. doi:10.1109/
ICSENS.2012.6411292
Chaudhary S, Nath R (2009) A multimodal biometric recognition system based on fusion of
palmprint, ﬁngerprint and face. In: Proceedings of 2009 international conference advances in
recent technologies in communication and computing, Washington, DC, USA, pp 596–600
Connie T, Teoh A, Goh M, Ngo D (2005) An automated palmprint recognition system. Image Vis
Comput 23(5):501–515
Dai J, Zhou J (2011) Multifeature-based high-resolution palmprint recognition. IEEE Trans
Pattern Anal Mach Intell 33(5):945–957
Fischer J, Radil T (2003) DSP based measuring line scan CCD camera. In: Proceedings of 2nd
IEEE international workshop intelligent data acquisition and advanced computing systems:
technology and applications, pp 345–348
References
255

Goh M, Connie T, Teoh A, Ngo D (2003) A single sensor hand geometry and palmprint
veriﬁcation system. Proceedings of 2003 ACM SIGMM workshop biometrics methods and
applications (WBMA), pp 100–106
Goh M, Connie T, Teoh A, Ngo D (2006) A fast palm print veriﬁcation system. In: Proceedings of
international conference computer graphics, imaging and visualisation (CGIV), Washington,
DC, USA, pp 168–172
Goh M, Connie T, Teoh A (2008) Touch-less palm print biometrics: novel design and implemen-
tation. Image Vis Comput 26(12):1551–1560
Goh M, Connie T, Teoh A (2010) Design and implementation of a contactless palm print and palm
vein sensor. In: Proceedings of 11th international conference control automation robotics
vision (ICARCV), pp 1268–1273
Guo Z, Zhang D, Zhang L, Zuo W (2009a) Palmprint veriﬁcation using binary orientation
co-occurrence vector. Pattern Recogn Lett 30(13):1219–1227
Guo Z, Zuo W, Zhang L, Zhang D (2009b) Palmprint veriﬁcation using consistent orientation
coding. In: Proceedings of 16th IEEE international conference image process (ICIP), pp
1965–1968
Han C, Cheng H, Lin C, Fan K (2003) Personal authentication using palm-print features. Pattern
Recogn 36(2):371–381
Han Y, Sun Z, Wang F, Tan T (2007a) Palmprint recognition under unconstrained scenes. In:
Proceedings 8th Asian conference on computer vision, Berlin, Springer, pp 1–11
Han Y, Tan T, Sun Z, Hao Y (2007b) Embedded palmprint recognition system on mobile devices,
in advances in biometrics. Lect Notes Comput Sci 4642:1184–1193
Hao Y, Sun Z, Tan T, and Ren C (2008) Multispectral palm image fusion for accurate contact-free
palmprint recognition. In: 15th IEEE international conference on image processing, ICIP 2008,
pp 281–284
Ito K, Aoki T, Nakajima H, Kobayashi K, Higuchi T (2006) A palmprint recognition algorithm
using phase-based image matching. In: Proceedings of IEEE International conference image
processing (ICIP), pp 2669–2672
Jain A, Feng J (2009) Latent palmprint matching. IEEE Trans Pattern Anal Mach Intell 31
(6):1032–1047
Jia W, Hu R, Gui J, Zhao Y, Ren X (2012) Palmprint recognition across different devices. Sensors
12(6):7938–7964
Kim J, Ahn S, Jeon J, and Byun J (2001) A high-speed high resolution vision system for the
inspection of TFT LCD. In: Proceedings of IEEE international symposium on industrial
electronics, vol 1, pp 101–105
Kong A, Zhang D (2002) Palmprint texture analysis based on low resolution images for personal
authentication. In: Proceedings of 16th international conference on pattern recognition (ICPR),
pp 807–810
Kong A, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. In: Proceedings
of 17th international conference on pattern recognition (ICPR), vol 1, pp 520–523
Kong W, Zhang D, Li W (2003) Palmprint feature extraction using2-D Gabor ﬁlters. Pattern
Recogn 36(10):2339–2347
Kong A, Zhang D, Kamel M (2006) Analysis of brute-force break-ins of a palmprint authentication
system. IEEE Trans Syst Man Cybern B Cybern 36(5):1201–1205
Kong A, Zhang D, Kamel M (2009) A survey of palmprint recognition. Pattern Recogn
42(7):1408–1418
Kumar A, Wong D, Shen H, Jain A (2003) Personal veriﬁcation using palmprint and hand
geometry biometric. In: Proceedings of 4th international conference audio- and video-based
biometric person authentication (AVBPA), Berlin, Springer, pp 668–678
Laadjel M, Kurugollu F, Bouridane A, Boussakta S (2009a) Degrade dpartial palmprint recogni-
tion for forensic investigations. In: Proceedings of 16th IEEE international conference image
processing, Piscataway, NJ, USA, pp 1497–1500
256
11
Line Scan Palmprint Recognition System
www.ebook3000.com

Laadjel M, Kurugollu F, Bouridane A, Yan W (2009b) Palmprint recognition based on subspace
analysis of Gabor ﬁlter bank. In: Proceedings of 10th Paciﬁc Rim conference on multimedia:
advances in multimedia information processing (PCM). Berlin, pp 719–730
Li W, Zhang D (2009) Three dimensional palmprint recognition. In: Proceedings of IEEE
international conference on systems man and cybernetics, pp 4847–4852
Lin C, Chuang T, Fan K (2005) Palmprint veriﬁcation using hierarchical decomposition. Pattern
Recogn 38(12):2639–2652
Luna C, Mazo M, Lazaro J, Vazquez J (2010) Calibration of line scan cameras. IEEE Trans
Instrum Meas 59(8):2185–2190
Marino F, Distante A, Mazzeo P, Stella E (2007) A real-time visual inspection system for railway
maintenance: automatic hexagonal-headed bolts detection. IEEE Trans Syst Man Cybern C
Appl Rev 37(3):418–428
Mil’shtein S, Palma J, Liessner C, Baier M, Pillai A, Shendye A (2008) Line scanner for biometric
applications. In: Proceedings of IEEE international conference on technologies for homeland
security (HST), pp 205–208
Shen L, Wu S, Zheng S, Ji Z (2012) Embedded palmprint recognition system using OMAP 3530.
Sensors 12(2):1482–1493
Shu
W,
Zhang
D
(1998)
Automated
personal
identiﬁcation
by
palmprint.
Opt
Eng
37(8):2359–2362
Struc V, Pavesic N (2008) A palmprint veriﬁcation system based on phase congruency features, in
biometrics and identity management. Lect Notes Comput Sci 5372:110–119
Wang J, Yau W, Suwandy A, Sung E (2008) Person recognition by fusing palmprint and palm vein
images based on Laplacianpalm representation. Pattern Recogn 41(5):1514–1527
Watanabe K, Hokari M (2006) Measurement of 3-D loci and attitudes of the golf driver head while
swinging. IEEE Trans Syst Man Cybern A Syst Humans 36(6):1161–1169
Wong M, Zhang D, Kong W, Lu G (2005) Real-time palmprint acquisition system design. IEEE
Proc Vis Image Signal Process 152(5):527–534
Zhang D, Shu W (1999) Two novel characteristics in palmprint veriﬁcation: datum point invari-
ance and line feature matching. Pattern Recogn 32(4):691–702
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhang D, Lu G, Li W, Zhang L, Luo N (2009) Palmprint recognition using 3-D information. IEEE
Trans Syst Man Cybern C Appl Rev 39(5):505–519
Zhang D, Guo Z, Lu G, Zuo W (2010a) An online system of multispectral palmprint veriﬁcation.
IEEE Trans Instrum Meas 59(2):480–490
Zhang D, Kanhangad V, Luo N, Kumar A (2010b) Robust palmprint veriﬁcation using 2D and 3D
features. Pattern Recogn 43(1):358–368
Zheng Y, Shi G, Zhang L, Wang Q, Zhao Y (2007) Research on ofﬂine palmprint image
enhancement. In: Proceedings of IEEE international conference image processing (ICIP),
pp I-541–I-544
Zhu L, Zhang S (2010) Multimodal biometric identiﬁcation system based on ﬁnger geometry,
knuckle print and palm print. Pattern Recogn Lett 31(12):1641–1649
Zuo W, Yue F, Wang K, and Zhang D (2008) Multiscale competitive code for efﬁcient palmprint
recognition. In: Proceedings of 19th international conference on pattern recognition (ICPR),
pp 1–4
References
257

Chapter 12
Door Knob Hand Recognition System
Abstract Biometric applications have been used globally in everyday life. How-
ever, conventional biometrics is created and optimized for high security scenarios.
Being used in daily life by ordinary untrained people is a new challenge. Facing this
challenge, designing a biometric system with prior constraints of ergonomics, we
propose ergonomic biometrics design model, which attains the physiological fac-
tors, the psychological factors, and the conventional security characteristics. With
this model, a novel hand based biometric system, door knob hand recognition
system, is proposed. Door knob hand recognition system has the identical appear-
ance of a conventional door knob, which is an optimum solution in both physio-
logical factors and psychological factors. In this system, a hand image is captured
by door knob imaging scheme, which is a tailored omni-vision imaging structure
and is optimized for this predetermined door knob appearance. Then features are
extracted by local Gabor binary pattern histogram sequence method and classiﬁed
by projective dictionary pair learning. In the experiment on a large data set
including 12,000 images from 200 people, the proposed system achieves compet-
itive recognition performance comparing with conventional biometrics like face
and ﬁngerprint recognition systems, with an equal error rate of 0.091%. This study
shows that a biometric system could be built with a reliable recognition perfor-
mance under the ergonomic constraints.
Keywords Biometrics • Ergonomics • Feature extraction • Image processing •
Machine learning • Optical imaging • Pattern recognition • User centered design
12.1
Introduction
In the last decade, biometrics has expended dramatically and globally. Biometrics
came under the spotlight after the counter-terrorism war began. It has become a
major solution for identity recognition and authentication. Since 2006, America has
been requiring biometric passports for travelers entering the United States under the
visa waiver program (Embassy 2009). Not only in America, biometric technologies
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_12
259
www.ebook3000.com

have been applied in identity documents in 15 countries (Eidan 2013; Jeng and
Chen 2009; Levush 2014).
With the wide spread of biometrics, it has been used not only in boarder control,
forensics and law enforcement agencies (Jain and Kumar 2012), but also in everyday
life, such as, in smart cars (Padmapriya and KalaJames 2012) and smart homes
(Carvalho and Rosa 2010). Applications like these have educated the general public
about the convenience and high-security level of biometric systems. People have
accepted biometric systems as a common, convenient, and secure access control
solution. Therefore, though biometrics has been designed to be used in high-security
applications, there has been a great demand of biometrics in everyday life.
However, biometrics has not been designed in a user-friendly way. In current
biometrics design, ergonomics (human factors) has not been a priority target. In the
biometric system design (Dunstone and Yager 2009), ergonomic study was limited.
When designing a biometric system, only seven elements have been addressed:
universality, distinctiveness, permanence, collectability, performance, acceptabil-
ity, and circumvention (Jain and Kumar 2012; Jain et al. 2004). When testing a
biometric system, only recognition related performance has been considered
(Mansﬁeld and Wayman 2002). Ergonomics, to some extent, has been neglected.
In this chapter, we present ergonomic biometrics design (EBD) model that
considers ergonomics in all aspects of the design in Sect. 12.2. We propose a
door knob hand recognition system (DKHRS) in Sect. 12.3, which is shaped like
a standard door knob, but incorporates a customized imaging device, a robust
feature extraction, and a discriminative classiﬁcation method. When addressing
the imaging problem of this new device in this space-limited and shape-conﬁned
case, we propose a simpliﬁed catadioptric imaging structure-door knob imaging
(DKI) scheme. The DKI scheme captures the surrounding hand skin surface in one
omni-vision image in a cost-efﬁcient structure. In the proposed system, we employ
a local Gabor binary pattern histogram sequence (LGBPHS) method, which
extracts robust histograms of dense local feature from DKHRS images. The fea-
tures are classiﬁed using the dictionaries learned by projective dictionary pair
learning (DPL). Combining DKI scheme, LGBPHS method, and DPL method,
we make the proposed DKHRS effective and efﬁcient under the ergonomic con-
straints. The DKHRS has been used to collect an experimental data set of a
signiﬁcant scale. The experiment result on this data set is promising in Sect. 12.4.
The conclusion and future work is summarized in Sect. 12.5.
12.2
Ergonomic Biometrics Design
12.2.1
Development of Biometric Systems
Biometric systems are identity authentication systems utilizing various biological
and/or behavioral traits, including ﬁngerprint (Vatsa et al. 2009), face (Medioni
et al. 2009), hand/ﬁnger geometry, iris (Kalka et al. 2010; Burge and Bowyer 2013;
260
12
Door Knob Hand Recognition System

Gong et al. 2013), sclera (Zhou et al. 2012; Alkassar et al. 2015), signature, gait,
palmprint (Zhang et al. 2003; Wu et al. 2006), voice pattern, ear (Bustard and Nixon
2010), hand vein, odor, and the DNA information of a person. A biometric system
includes a sample collecting module (device), a feature extraction module, a
database module, and a classiﬁcation module (Jain et al. 2011). A biometric system
provides veriﬁcation and/or identiﬁcation functions (Jain et al. 2011). The design
cycle of a biometric system includes understanding the nature of the application,
choosing biometrics trait, collecting biometric data, choosing features and classiﬁ-
cation algorithm and evaluating the system. During the development of a biometric
system, seven factors: universality, distinctiveness, permanence, collectability,
performance, acceptability, and circumvention are considered important (Jain and
Kumar 2012; Jain et al. 2004, 2011). Another inﬂuential study summarizes that a
biometric system should be assessed by attributes including distinctiveness, stabil-
ity, scalability, usability, inclusiveness, insensitivity, vulnerability, privacy, main-
tenance, health, quality, integration and cost (Dunstone and Yager 2009).
12.2.2
Ergonomics Studies in Biometrics
From the above studies, it is noticed that ergonomics takes a signiﬁcant role in
developing biometric systems. Ergonomics (Human Factors) is to design products
and systems in considering the interaction with people (Schlick 2009). In bio-
metrics, physical ergonomics and cognitive ergonomics are two critical factors.
Physical ergonomics focuses on physical motion related traits including human
anatomical, anthropometric, physiological and biomechanical characteristics. Cog-
nitive ergonomics focuses on human-system interaction related mental activities
including perception, memory, reasoning, and motor response. In biometric sys-
tems, ergonomics presents various signiﬁcant factors. The Collectability/Health
might be categorized to physical ergonomics. The Acceptability/Usability might
fall into the area of cognitive ergonomics. In biometric systems, the ideal ergo-
nomic solution would make the user barely notice the authentication process
(Dunstone and Yager 2009). Also, a biometric system with poor ergonomics
would jeopardize the quality of collected biometric samples (Alonso-Fernandez
et al. 2012). For example, iris systems require ﬁxed height of eyes. When using
these systems, tall or short people encounter frustrations (Dunstone and Yager
2009). Another example is that small ﬁngerprint sensors without guides capture
ﬁngerprints of poor quality (Dunstone and Yager 2009).
Early ergonomics studies in development of biometric systems aimed at user
acceptance (Albrecht 2001; Eschenburg et al. 2005; Elliott et al. 2007), latent
ﬁngerprint examination (Wertheim 2010; Expert 2012), and collectability (Jain
and Kumar 2012; Dunstone and Yager 2009; Jain et al. 2011; Faddis et al. 2011;
Mordini and Tzovaras 2012). According to Albrecht (Albrecht 2001), users accept
natural and everyday motions the most readily, which conforms with ergonomic
principles both in physical and cognitive. The analysis of latent ﬁngerprint is widely
12.2
Ergonomic Biometrics Design
261
www.ebook3000.com

depended on the human judgments because the analysis is semi-automatic
(Wertheim 2010; Expert 2012). In collectability, the physical ergonomics is the
challenge from an engineering perspective to next generation biometrics (Jain and
Kumar 2012; Mordini and Tzovaras 2012). Then, addressing ergonomic issues in a
systematic view, human-biometric sensor interaction (HBSI) model is proposed
(Kukula et al. 2006, 2007).
HBSI model, which is illustrated in Fig. 12.1 focuses on the interactions between
target subjects and the biometric sensors (Kukula et al. 2006, 2007; Kukula 2008).
HBSI model utilizes the metrics from both biometrics and ergonomics to assess the
functionality and performance of biometric systems. Human-sensor intersection
focuses on the physical ergonomics. Human-biometric system intersection repre-
sents the interactions between users and systems, which include sensors, software
and implementations of systems. The aim of this intersection is comprised of three
factors: effectiveness, efﬁciency, and satisfaction. Sensor-biometric system inter-
section addresses the image/sample quality issue.
HBSI model has been used to examine the ergonomics of swipe ﬁngerprint
sensors (Kukula et al. 2006, 2010; Kukula 2008), and hand geometry machine
(Kukula et al. 2007; Elliott et al. 2010). HBSI model provides an adequate and
thorough evaluation framework for biometric systems. Under the assessment of
HBSI model, the functionality and performance of a biometric system can be
characterized. However, HBSI has several restrictions. First, HBSI model, to a
large extent, is an evaluation model. It contributes signiﬁcantly in assessing a
variety of biometric systems, but not in crafting new biometrics. Second, HBSI
model evaluates interactions in the sensor level. In HBSI evaluations, either
ﬁngerprint sensors or hand geometry machines are taken as the elementary subject.
Fig. 12.1 HBSI model
shows the interactions
between human, biometric
sensors, and biometric
systems
262
12
Door Knob Hand Recognition System

The appearance and structure of the sensor constrained the enhancement of ergo-
nomics in biometric systems. Third, in HBSI model, the ergonomics should be
addressed at feature level. It is the biometric feature which deﬁnes the interactions
among human, biometric sensors and biometric systems. The biometric feature
determines the structure of sensors. For example, ﬁngerprints are captured by
semiconductor swipe ﬁngerprint sensors or optical ﬁngerprint sensors; hand geom-
etries are obtained by hand geometry image capturing devices. In addition, the
biometric feature deﬁnes a large part of interactions. For example, the iris system
requires users to stand straight in front of the camera; the swipe ﬁngerprint sensors
require users to swipe with ﬁngers. In order to enhance the functionality and the
performance of biometric systems, ergonomics should be addressed at feature level.
12.2.3
Ergonomic Biometrics Design Model
EBD model is proposed to address the above limitations. EBD model implements
four concepts.
1. Considering ergonomics in the ﬁrst stage—selecting biological and behavioral
characteristics.
2. Considering ergonomics in all developing stages—selecting biological and
behavioral characteristics, designing the sample-collecting device and designing
the feature extraction and classiﬁcation method.
3. Considering both physical and cognitive ergonomics in each stage.
4. Collaborating the recognition performance with ergonomics.
EBD model is illustrated in Fig. 12.2. This model is to provide a guideline for
creating a new ergonomic biometric system. In design theory, Rubin (Rubin and
Chisnell 2008) argue that there are ﬁve reasons why a machine or a system is
difﬁcult to use. The fundamental issue is that the focus has been on the machine or
Fig. 12.2 EBD model considers both physical ergonomics and cognitive ergonomics in all three
stages of biometric system development
12.2
Ergonomic Biometrics Design
263
www.ebook3000.com

the system and not on users during the development. In a biometric system, the
emphasis should be users. The consideration of ergonomics should be as early as in
the ﬁrst design stage and also in the full design process. Conventionally, the
ergonomics analysis is occurred in the implementation stage, which is after the
production of sensors. When the feature and the sensor is determined, the room for
ergonomics is limited. In EBD model, we insist that ergonomics should be included
in the full biometrics system design process. In all three stages: selecting biological
and behavioral characteristics, designing the sample-collecting device and design-
ing the feature extraction and classiﬁcation method, ergonomics should be consid-
ered. Furthermore, both categories of ergonomics should be considered including
the physical ergonomics and the cognitive ergonomics. Though ergonomics is an
essential element in EBD model, recognition performance should not be ignored.
The ergonomics and recognition performance should collaborate with each other
during the design process.
12.3
Door Knob Hand Recognition System
In this section, EBD model applies to the design of DKHRS.
12.3.1
Concept and Framework
The concept of DKHRS comes from the applications of biometric systems. The
biometric systems have been widely employed in the access control scenario (Jain
and Kumar 2012; Dunstone and Yager 2009; Burge and Bowyer 2013; Jain et al.
2008; Jain 2009). However, in the access control scenario, the most accepted device
is the door knob, which can be traced back to 1893 (Russell 1893). Since then, the
door knob has become a standard attachment to the door. Therefore, it occurs to us
that how to remake the door knob by designing a biometric system with the
appearance of the door knob. The appearance of the door knob has been optimized
for human hands for over a hundred years (Russell 1893), which would be an ideal
physical ergonomic design of a biometric device, because it is natural and it has
become a daily motion (Albrecht 2001). With the appearance of the door knob,
users can use DKHRS installed in the door without extra burden of thinking, which
could achieve a better performance in cognitive ergonomics. The function and the
form unite in DKHRS.
The goal of DKHRS is to be user-friendly. The design concept of DKHRS can be
depicted in one sentence, open the door just like it is not locked. To fulﬁll this
requirement, EBD model has been applied in the design process.
In the development of a biometric system using EBD model, both the physical
and cognitive ergonomics are considered in the three designing stages: selecting
biological and behavioral characteristics, designing the sample-collecting device
264
12
Door Knob Hand Recognition System

and designing the feature extraction and classiﬁcation method, as illustrated in
Fig. 12.2.
First, the unique and robust biometric characteristics of DKHRS are the skin
texture of the hand. DKHRS imitates the door knob. When using a traditional door
knob, people hold the door knob with their hands. The hand encloses the door knob.
The hand skin has multiple layers of texture, which have been used in various
biometric systems (Zhang et al. 2003; Kong and Zhang 2002, 2004; Wong et al.
2005). The texture of a hand could be a promising biometric characteristic.
Second, the image capturing device of DKHRS is to capture hand images with
the appearance of a traditional door knob. The appearance of a traditional door knob
is inherited from the historical design experience, which has been physiologically
optimized for the human hand. Because of the long history of the door knob being
used, this appearance is unconsciously perceived as an access control device. With
the standard door knob appearance, DKHRS is identical to the traditional door knob
in appearance and function. However, DKHRS is a biometric system and the
recognition performance is a major factor. A good recognition performance
requires good image quality and rich features. Thus, a novel DKI scheme is
proposed. This imaging scheme is designed to capture quality images under the
constraints of the appearance.
Third, the feature extraction and classiﬁcation method of DKHRS should be fast
and robust to dislocations. The recognition speed of a biometric system is critical to
cognitive user experience. The dislocations of the hand are inescapable in physical
device design. The dislocation problem can also be a challenging issue in cognitive
reception if a user has to try multiple times for successful recognition. In feature
extraction part, LGBPHS method (Zhang et al. 2005; Yan et al. 2011) is adopted to
extract texture feature from the image of the hand. LGBPHS method extracts local
binary patterns from patches in the Gabor surface of the original image and
concatenates all histograms of these patterns to form a feature vector. LGBPHS is
robust to illumination variations and minor dislocations (Zhang et al. 2005; Yan
et al. 2011). However LGBPHS feature is of a very high dimension. Thus, an
efﬁcient and discriminative representation of this feature is required. In classiﬁca-
tion aspect, DPL (Gu et al. 2014) is adopted. DPL learns a synthesis dictionary and
an analysis dictionary to represent the feature. This method converges fast in both
training and testing stages and shows promising performance in face recognition,
object recognition, and action recognition (Gu et al. 2014).
12.3.2
Imaging
The imaging structure is challenging to DKHRS. Considering the physical ergo-
nomic factor, the dimensions of the imaging structure is constrained; the appear-
ance of the imaging structure is predeﬁned as a standard door knob; and the usage of
the imaging structure is to be held with a hand. The imaging structure should be
12.3
Door Knob Hand Recognition System
265
www.ebook3000.com

small and compact, and the hand images around it should be captured. Therefore,
DKHRS cannot use the most popular conventional imaging method.
The most popular used imaging method is the conventional imaging (Kong and
Zhang 2002, 2004; Zhang et al. 2003; Wong et al. 2005; Jain et al. 2008; Dunstone
and Yager 2009; Jain 2009; Jain and Kumar 2012; Burge and Bowyer 2013). This
imaging scheme places a light source and a camera in front of the object. This
structure is simple and stable. It is widely used in most of biometric systems (Kong
and Zhang 2004; Wong et al. 2005; Jain et al. 2008; Dunstone and Yager 2009; Jain
2009; Jain and Kumar 2012; Burge and Bowyer 2013).
However, there are three limitations of the conventional imaging scheme when
this scheme is being applied in DKHRS. First, conventional imaging requires a
large space, which is illustrated in Fig. 12.3. The conventional imaging needs two
clear cone-shaped spaces: one is between the camera and the lens, and the other is
between the lens and the object. Second, in conventional imaging, the camera only
captures objects in front of the lens. The ﬁeld of view captured by this scheme is
proportional to the distance between the lens and the object. Third, a large space
Fig. 12.3 Conventional
imaging scheme requires a
large space and only
captures objects in front of
the lens
266
12
Door Knob Hand Recognition System

and the open light path of the conventional imaging are very sensitive to the
environment light. To address the above limitations and the requirements of the
imaging, the catadioptric scheme is adopted.
The catadioptric scheme is an imaging structure, in which the ray is reﬂected
once or several times to increase the distance and to enlarge the ﬁeld of view (Jones
2009; Zhang and Li 2012 catadioptric). It is widely used in omnidirectional
cameras. In omnidirectional cameras, the catadioptric scheme is used to get a
round image of the environment (Zhang and Li 2012). The omnidirectional cameras
being used in machine vision can be grouped into three categories: cameras that use
special lens, cameras that use a convex mirror and a set of lens, and cameras that use
two mirrors and a set of lens (Jones 2009). Most directional mirrors use these four
types of shapes: conical mirrors, hemispherical mirrors, hyperboloidal mirrors, and
paraboloidal mirrors (Jones 2009; Zhang and Li 2012). The mirrors and the
minimum working distance of the systems restrict the minimum size of omnidirec-
tional cameras (Zhang and Li 2012).
When the catadioptric scheme is being used in the proposed DKHRS, the
advantage of this scheme is that the hand image, which is around the camera and
the door knob device of DKHRS, can be captured in one image. However, there are
still several limitations. First, the catadioptric scheme is designed to capture images
far away from the camera. In the proposed DKHRS, the distance from the hand to
the camera is constrained by the dimension of the door knob. Second, the direc-
tional mirrors of the catadioptric scheme, which are a crucial part of this imaging
scheme, are expensive. The directional mirrors are of a complex curved shape and
tend to be of expensive material. To overcome these limitations, we proposed the
door knob imaging (DKI) scheme.
DKI is a simpliﬁed catadioptric scheme, which is customized for DKHRS. The
simpliﬁcations are as follows. First, the imaging structure is simpliﬁed to capture
the surrounding area near the case of the device only. DKI structure is different
from the catadioptric scheme, in which the object or the environment is at a
relatively long distance. In the proposed device, the hand would be holding the
device and touching the case of the device. Second, the imaging structure is
simpliﬁed to reduce the cost of the device, and to reduce the complexity. In the
catadioptric scheme, the reﬂective mirror is a directional mirror, which reﬂects the
surrounding environment in a linear proportion.
DKI scheme is shown in Fig. 12.4. This simpliﬁed scheme is composed of a door
knob case, a ﬂat reﬂective mirror, a lens of a large angle of view and a camera. The
door knob case is transparent. It holds the hand and enables the capturing of the
hand image. The door knob case is a little larger in the far from the door end than
the near door end, which is identical with a door knob. The ﬂat mirror reﬂects the
image of the hand. The camera and the lens capture images of the reﬂected image of
the surrounding through the transparent case. With DKI scheme, the device can
capture the image of the hand holding the device. Furthermore, to stabilize the
position of the hand, there are two holding pegs ﬁxed on the lower surface of the
door knob case. The image-capturing device of DKHRS, which is made following
DKI scheme, is shown in Fig. 12.5.
12.3
Door Knob Hand Recognition System
267
www.ebook3000.com

The device is made of these components listed below. The door knob is made of
acrylic glass. The acrylic is a kind of highly transparent plastic transparent for over
90% energy and across a large spectrum including visible light and near infrared
light and is also strong, lightweight and easy to process. The ﬂat mirror reﬂects over
95% energy of light across the visible and infrared spectrum with a customized
coating. The camera and the lens together capture the surrounding image in the
mirror. The frame grabber digitizes analog images to digital images for further
processing.
The components are as follows.
1. Acrylic transparent door knob.
2. Over 95% full spectrum reﬂective mirror.
3. White LED board.
4. 6 mm focal length pinhole lens.
5. 1/3 inch mini CCD camera.
6. A USB 2.0 frame grabber.
Fig. 12.4 DKI scheme is
simple and cost-effective
capturing the surrounding
hand texture in one image
Fig. 12.5 Image-capturing
device of DKHRS was
made following the DKI
scheme, which is small,
compact and of a shape
identical to a standard door
knob
268
12
Door Knob Hand Recognition System

12.3.3
Feature Extraction and Classiﬁcation
DKHRS captures hand images surrounding the door knob, as depicted in Fig. 12.6.
A biometric feature should be unique, stable, and persistent. DKHRS captures
the hand image surrounding the door knob, as depicted in Fig. 12.6. The feature of a
DKHRS image is the texture feature on the skin of the hand. Different from a
palmprint image or a ﬁngerprint image, a DKHRS image is an image of a hand in
the holding gesture. Though this hand image is not taken with a ﬂattened standard
pose, the holding gesture of the hand still can be stable and reliable. Meanwhile, this
hand image contains unique texture features. After preprocessing of the raw
DKHRS image, the features of a DKHRS image can be extracted with LGBPHS
method. After the extraction, the features are classiﬁed by DPL.
1. Preprocessing: In the preprocessing, the hand area of a DKHRS image, which is
the region of interest (ROI), is cropped. The preprocessing is simple and straight
forward after a manual calibration of the acquisition device. The calibration
consists of two parts. First, the axes of the door knob and the camera are
calibrated to the same, as shown in Fig. 12.7a. The calibration is performed at
the pixel level with the help of images. Second, the parameters (center and
radius) of the annular hand area are measured after the calibration. Because after
the calibration, the round mirror and the camera image in the mirror shares the
Fig. 12.6 Images captured by DKHRS
12.3
Door Knob Hand Recognition System
269
www.ebook3000.com

same center, the hand skin area is an annular area and can be simply cropped
with two concentric circles, which is depicted in Fig. 12.7b.
2. Feature Extraction using Local Gabor Binary Pattern Histogram Sequence: In
the proposed system, the features are extracted by LGBPHS (Zhang et al. 2005;
Shan et al. 2006) method. LGBPHS method is a combination of local binary
pattern (LBP) method and Gabor wavelet method. LBP was created for texture
feature extraction (Ojala et al. 2002), then was applied in face feature extraction
(Ahonen et al. 2004; Huang et al. 2004; Shen et al. 2008; Zhu et al. 2015; Guo
et al. 2010a, b, c, d; Yan et al. 2011). Gabor wavelet method also achieved a
promising performance in face recognition (Liu and Wechsler 2002; Shan et al.
2006). LGBPHS method combines these two methods. It is robust against
rotation and translation, and against illumination interference.
In the proposed system, a patch-based LGBPHS method is employed, whose
scheme is illustrated as Fig. 12.8b. First, 35 patches of the same size (64 by
64) are cropped from one DKHRS image, as shown in Fig. 12.8a. The center of
these patches is located in the middle of the ring area, 130 pixels away from the
center, to cover most hand skin. Second, each DKHRS patch is ﬁltered by multi-
orientation and multi-scale Gabor ﬁlters to get Gabor magnitude pictures
(GMPs). The Gabor ﬁlters used are as follows:
Fig. 12.7 DKHRS is calibrated before the extraction of the ROI, which is a wide ring area. (a)
Optical axis of the LED light, the knob, the lens, the sensor and the center of the round mirror are
calibrated to the same point. (b) ROI region of a DKHRS image is a ring area cropped by two
circles with the same center, which is stable after the calibration
270
12
Door Knob Hand Recognition System

Ψμ,v zð Þ ¼
kμ,v

2
σ2
e
kμ,v
k
k2 z
k k2
2σ2
eikμ,vz  eσ2
2
h
i
ð12:1Þ
where μ and v are the orientation and scale of the Gabor ﬁltors, z(x, y), k∙k
denotes the norm operator, and the wave vector kμ,v ¼ kveiϕμ, where kv ¼ kmax/λv
and ϕμ ¼ πμ/8. λ is the spacing factor between ﬁlters in the frequency domain.
The patches images convolve with the Gabor ﬁlters. Given a patch f(x, y), the
convolution with a Gabor ﬁlter Ψμ , v(z) is deﬁned as follows:
GΨf x; y; μ; v
ð
Þ ¼ f x; y
ð
Þ∗Ψμ,v zð Þ
ð12:2Þ
where ∗denotes the convolution operator. Five scales v 2 0 ,    , 4 and eight
orientations μ 2 0 ,    , 7 Gabor ﬁlters are used. Convolving each patch with
Fig. 12.8 Patch wise LGBPHS feature extraction method extracts a high dimensional vector from
a DKHRS image. (a) Number of patches perpendicular to the radius direction are extracted for
feature extraction. (b) LGBPHS feature extraction scheme comprises of four stages: localizing
patches, ﬁltering patches with Gabor ﬁlter bank, computing LBP histogram feature for each patch,
concatenates all the histogram vector into one high-dimensional vector
12.3
Door Knob Hand Recognition System
271
www.ebook3000.com

each of the 40 Gabor ﬁlters results in the Gabor feature. Here, one magnitude
value is generated at each pixel position for each Gabor ﬁlter. After the ﬁlter,
40 GMPs are generated.
Third, for each patches, GMPs are encoded by LBP operator. LBP operator labels
the pixels of an image by binarizing the 3  3-neighborhood of each pixel
fp(p ¼ 0, 1,    , 7) with the center value fc and considering the result as a binary
number.
S f p  f c


¼
1, f p  f c
0, f p < f c

ð12:3Þ
Then, by assigning a binomial factor 2p for each S( fp  fc), the LBP pattern at the
pixel is achieved as
LBP ¼ Σ7
p¼0S f p  f c


2p
ð12:4Þ
Which characterizes the spatial structure of the local image texture. We denote
the LBP transform result at position (x, y) of (μ, v)-GMP as Glgbp(x, y, μ, v),
which composes the (μ, v)-LGBP Map.
Fourth, the LBP codes are summarized to histograms. The local feature histogram
summarizes the region property of the LGBP patterns.
At last, all the histograms from all LGBP maps are concatenated into a histogram
sequence. The above process is formulated as follows: The histogram h of an
image f(x, y), with gray levels in the range [0, L  1] could be deﬁned as
hi ¼ Σx,y I f x; y
ð
Þ ¼ i
f
g, i ¼ 0, 1,   , L  1
ð12:5Þ
where i is the ith gray level, hi is the number of pixels in the image with gray
level i and
I A
f g ¼
1, A is true
0, A is false

ð12:6Þ
Assume each LGBP Map is divided into m regions R0 , R1 ,    , Rm  1 (in our
case m ¼ 35). The histogram of rth region of the speciﬁc LGBP Map (from
(μ, v)-GMP) is computed by
Hμ,v,r ¼ hμ,v,r,0; hμ,v,r,1;   ; hμ,v,r,L1


ð12:7Þ
where
Hμ,v,r,i ¼ Σ x;y
ð
Þi2Rr I Glgbp x; y; μ; v
ð
Þ ¼ i


ð12:8Þ
272
12
Door Knob Hand Recognition System

Finally, all the histogram pieces computed from the patches are concatenated to
a high dimensional feature vector, which is the ﬁnal feature vector to represent
the given DKHRS image.
3. Classiﬁcation using Projective Dictionary Pair Learning: In the proposed sys-
tem, DPL (Gu et al. 2014) is used. DPL is dictionary learning method proposed
in 2014, which is inspired by K-SVD (Aharon et al. 2006; Rubinstein et al. 2013)
and LC-KSVD (Jiang et al. 2011, 2013). DPL learns both a synthesis dictionary
and an analysis dictionary in the training stage. Both dictionaries are used in
classiﬁcation. DPL method is fast in both training and testing stages, and the
classiﬁcation accuracy is promising.
Denote by X ¼ [X1,    , Xk,    , XK] a set of p-dimensional training samples
from K classes, where Xk 2 Rp  n is the training sample set of class k, and n is the
number of samples of each class.
DPL method learns a synthesis dictionary D and an analysis dictionary,
denoted by P 2 RmK  p using the following objective function:
P; D
f
g ¼ argminP,D
X
K
k¼1
Xk  DkPkXk
k
k2
F þ λ Pk Xk
k
k2
F
s:t: di
k k2
2  1:
ð12:9Þ
It is different from conventional discriminative dictionary learning that the
sparse code P, X is analytically obtained, instead of computing the time-
consuming l1-norm sparse coding. With this feature, the DPL model is very
efﬁcient, and detailed description and reference can be found in (Gu et al. 2014).
In the optimization of the DPL model, a matrix A is introduced and the model is
relaxed as following:
P∗; A∗; D∗
f
g ¼ argminP,A,D
X
K
k¼1
Xk  DkPkXk
k
k2
F
n
þτ PkXk  Ak
k
k2
Fþλ Pk Xk
k
k2
F
o
s:t: di
k k2
2  1:
ð12:10Þ
where τ is a scalar constant. P and D are initialized as random matrices with unit
Frobenius norm, then A, D, P are updated alternatively.
In the DPL model, the analysis sub-dictionary P∗
k
is trained to produce small
coefﬁcients for samples from classes other than k, and it can only generate
signiﬁcant coding coefﬁcients for samples from class k. Meanwhile, the synthe-
sis sub-dictionary D∗
k is trained to reconstruct the samples of class k from their
projective coefﬁcients P∗
k Xk ; that is, the residual
Xk  D∗
k P∗
k X∗
k

2
F will be
small. On the other hand, since P∗
k Xi, i 6¼ k will be small and D∗
k is not trained to
12.3
Door Knob Hand Recognition System
273
www.ebook3000.com

reconstruct Xi, the residual
Xi  D∗
k P∗
k Xi

2
F
will be much larger than
Xi  D∗
k P∗
k Xk

2
F.
Based on the property above, a matching scheme is designed as follows:
matching y; i
ð
Þ ¼
1,
DiPiy
k
k2  threshold
0,
DiPiy
k
k2 > threshold

ð12:11Þ
The reconstruction residual is taken as a dissimilarity measure.
12.4
Experiments
The major concern of DKHRS, which is designed following EBD model, is the
performance. In DKHRS, the hand is not in a standard gesture and the camera
captures non-conventional and reﬂected image of the hand skin. Though LGBPHS
and DPL methods are reported to be robust against distortions, it is still a myth
whether the system could achieve a competitive performance. There is no doubt
that the recognition performance is the critical criteria to a biometric system.
Experiments have been devised to evaluate the performance of the proposed
system. A large data set was built, and recognition experiments with different
settings are presented.
12.4.1
Data Set
The DKHRS data set including 12,000 DKHRS images from 200 people was built,
in order to evaluate the recognition performance of the proposed system. The
subjects were volunteers from universities and neighboring communities. The
images from both left and right hand were collected. There were two sessions of
data collection. In each session, 15 images were collected for one hand. The interval
between the two sessions was 1 week. When collecting images, the subject was
asked to move away and hold the door knob again after taking each sample. Each
subject was trained for less than 30 s before the collection. In this short-time
training, a helper demonstrated the sampling process, then the subject tried once
or twice with DKHRS device before the data collection. The resolution of the
images is 640  480. The prototype system used in the data collection is shown in
Fig. 12.9. In the experiment, the data set is separated into a training set and a testing
set, which contain equal number of images. For each hand, there are 15 randomly
selected training images, and the other 15 images are used for testing. In the
experiment, the classiﬁcation is to evaluate the similarity between each sample
and each class (hand). Thus, for one testing sample, there is one genuine match to
274
12
Door Knob Hand Recognition System

the class it belongs to and 399 impostor matches to other classes. In total, there are
6000 genuine matches and 2,394,000 impostor matches.
12.4.2
Experimental Results
In the experiment, the recognition process comprises of four parts, as shown in
Fig. 12.10. First, LGBPHS method is applied in all training images in the data set to
extract the feature. Second, the high dimensional features are down-sampled with
PCA. Third, the compressed features are fed to the DPL to learn the two dictionar-
ies. Fourth, testing features, which are produced from the testing images by the
same procedures, are tested using the learned dictionaries. The test result is
evaluated with BioSecure Performance Evaluation Tool (Mansﬁeld and Wayman
2002; Bolle et al. 2000, 2004). In this evaluation tool, three criteria are provided to
evaluate the performance of a biometric system including recognition rate, receiver
operating characteristic (ROC) curve, and equal error rate (EER). The recognition
rate is the rate that right matches divided by all the matches, which is simple and
direct, but is biased because of the imbalance of genuine matches and impostor
matches. ROC curve depicts the overall performance of a biometric system. It
exhibits the percentage of impostor attempts accepted, which is also called false
acceptance rate (FAR), against the percentage of genuine attempts accepted (GAR),
which equals to 1—false rejection rate (FRR). ROC curve does not depend on the
threshold. When FAR curve crosses over FRR curve (FAR ¼ FRR), the rate in the
Fig. 12.9 Prototype system used in the data collection. The DKHRS is installed in a door
simulator, which is a black acrylic box and is positioned on a balance board. This prototype
system collected 12,000 images from 200 volunteers. (a) 3-D design model shows that the DKHRS
is installed on the door. The little end is fastened and the power cord, and control wires are buried
in the door. (b) Appearance of prototype system is a small black door with a transparent door knob.
The prototype is made portable for the convenience of the experiment. The balance board is used
to prevent accidental damage in experiments, because the system is light
12.4
Experiments
275
www.ebook3000.com

crossing point is EER. At EER point, the recognition rate (here it equals to 1—EER)
is not affected by the imbalance of the genuine attempts and the impostor attempts,
because FAR equals to FRR. EER is widely used as a brief criteria of a biometrics.
The testing result is shown in Fig. 12.11, including the recognition rate curve and
the EER curve in Fig. 12.11a and the ROC curve in Fig. 12.11b. Figure 12.11a
shows the recognition rates and EERs when the feature is reduced to different
dimensions. When the feature dimension is higher than 200, the recognition rate is
steadily above 99.5%. EER is lower than 0.15%, when the feature dimension is
higher than 500. When the feature dimension is higher than 800, the EER becomes
steady. When the feature dimension is 850, the proposed system achieves the best
performance with a 0.091% EER. Figure 12.11b shows the ROC curve when the
feature dimension is 850. Even when FAR is as lower as 103, GAR is still above
98%.
Dataset
LGBPHS
PCA
PCA
DPL Traning
D. P
DPL Testing
Testing
Resutls
LGBPHS
Training
Images
Testing
Images
Fig. 12.10 Experiment is
following this ﬂowchart.
Both training and testing
features are extracted using
LGBPHS and down-
sampled using PCA. Then
the dictionaries learned
from the training are used to
test testing features
276
12
Door Knob Hand Recognition System

12.4.3
Comparison with Conventional Biometrics
The recognition rate of DKHRS is over 99%, and its EER can be lower than 0.1%.
Generally speaking, the recognition performance of DKHSR is much better than
hand back skin texture (Xie et al. 2012), gait (Lai et al. 2014) and face recognition
(Gu et al. 2014; LFW 2015); it is even surpass ﬁngerprint recognition [about 1%
EER on STFV-STD-1.0 dataset] and 3D ﬁngerprint [3.4% EER (Liu et al. 2015)];
100.00%
0.50%
0.45%
0.40%
0.35%
0.30%
0.25%
0.20%
0.15%
0.10%
0.05%
0.00%
99.50%
99.00%
98.50%
98.00%
97.50%
100
a
b
98
95
90
80
70
60
50
40
30
20
10
False Acceptance Rate (%)
Genuine Acceptance Rate (%)
10
–5
10
–4
10
–3
10
–2
10
–1
10
0
10
1
100
200
300
400
Recognition Rate
EER
500
600
PCA Dims
700
800
900
1000
Fig. 12.11 Performance evaluating curves of DKHRS. (a) Recognition rates and EERs along
different down-sampled dimensions. (b) ROC curve when the feature dimension is 850
12.4
Experiments
277
www.ebook3000.com

but it is still not as good as iris recognition [<0.003% EER (Daugman 2007)], and
palmprint recognition [EER from 0.062% to 0.012% (Zuo et al. 2008; Guo et al.
2009a, b; Laadjel et al. 2009; Zhang et al. 2010a, b; Li et al. 2012; Qu et al. 2016)].
It should be noted that the data sets, the matching schemes and the experiment
settings are different in these biometric systems. Thus, direct comparison of EERs
may not be precise and fair. However, it still can be found that the performance of
the proposed system is very promising. The reasons could be as follows. First, the
door knob hand recognition is a contact recognition system. The imaging structure
is well protected against environmental light. With this stable environment, the
illumination is stable. DKHRS is free from illumination problem, which is a critical
challenge for face recognition (Medioni et al. 2009). Second, the position of the
hand is stable with the help of the pegs. Thus, the ROI area is stable. A stable ROI
makes the DKHRS free from the ROI localization challenge which widely exists in
many non-contact biometric systems. Third, DKHRS is a hand based biometrics,
then the multiple layers of texture and the large area of skin provides DKHRS a rich
and diverse features.
For the user experience aspect, DKHRS is better than conventional biometrics
both in physical and cognitive aspects. In physical aspect, the appearance of
DKHRS is identical to a common door knob. The dynamics of opening a door
with DKHRS is also identical to opening a door with a common door knob. In the
data collection, 30 s of training makes all volunteers familiar and experienced with
the DKHRS device. In cognitive aspect, a DKHRS device is easy to be recognized
as an access control device. Its appearance is identical to a door knob, which has
been widely used in all occasions in everyday life, and has been accepted and used
for over a century. The function and dynamics of DKHRS is identical to a door
knob. This resemblance relieves the burden of understanding the function and
training of the interactions with this new system. When collecting samples in the
second session, one week after the ﬁrst, 94% of volunteers remembered the
operations of DKHRS.
Comparing with other modern biometrics, for example, keystroke (Liu et al.
2014), and mouse dynamics (Nakkabi et al. 2010). The advantages of DKHRS are
that the performance of DKHRS is very competitive. The limitation of DKHRS is
that the applications can only be with the door in an access control system. The
keystroke and mouse dynamics requires no special devices, which can be used with
a common keyboard or a common mouse. And both keystroke and mouse dynamics
can be used as a continuous biometric system, which can verify the identity of the
subject continuously. However, in access control systems, DKHRS simpliﬁes the
interaction of opening a door, which provides a unique advantage.
12.4.4
Discussion
The best EER that DKHRS achieved is 0.091% in the experiment, which is very
competitive comparing with conventional biometrics. And the ergonomics are well
considered both in physical and in psychological aspects. This is a major challenge
278
12
Door Knob Hand Recognition System

to a hidden assumption that it is only in the standard pose that a biometrics could
achieve a good performance. In DKHRS, the hand is in a holding gesture. In a
holding gesture, the hand is not stretched in a plane. But still, stable and robust
features could be extracted. The reason of this could be two-folded. From the
texture feature point of view, the texture feature of the skin in a curved surface
can be extracted as a stable feature, as long as the curved surface is stable and well
calibrated. From the behavior point of view, the pose, the gesture or even the action
of a person could be persistent after thousands or even millions of repetitions along
a long time period, which is also the foundation of many behavior biometrics. Only
in DKHRS, these two aspects, biological feature and behavioral feature, are com-
bined together, and realized in an ergonomics optimized form.
There are still four limitations of this system. First, the data set covers short term
variances only. For a mature biometrics, a data set consists of samples across years
variances is expected, which is a great challenge to a newly-born biometric system
like DKHRS. Second, the data set in the experiment is lack of diversity of people. In
this data set, all volunteers are Chinese. The color of skin may be a challenge to
DKHRS, which is not tested in the experiment. Third, an auxiliary signal could be
implemented for better cognition. The prototype is a silent system. A green light for
a successful match and a red light for a failure could be very helpful. In addition, a
sound notiﬁcation, which reminds the subject that the door is unlocked could
greatly enhance the user experience. Fourth, the feature extraction and classiﬁca-
tion could be improved. A DKHRS image covers the thumb, the index ﬁnger, the
middle ﬁnger, and the upper half of the palm. Currently, DKHRS uses LGBPHS
and DPL to extract high dimensional dense features and learn discriminative
dictionaries to classify. The essence of the feature is not fully exploited. This
should be studied in the future.
The proposed EBD model is only a concept model, though DKHRS system
developed following this model is promising. In EBD model, the two aspects of
ergonomics are addressed in three stages of designing a biometrics. And DKHRS
shows a potential that designing a biometric system with the constraints of ergo-
nomic concerns may not be deﬁnitely a compromise in security performance.
However, unlike HBSI model, EBD model lacks detailed guides in designing and
testing. This should be further studied.
EBD model exhibits an inspiring angle in designing a new biometric system.
With the fast development of both electronics and algorithms and the increasingly
use of biometrics in everyday life, the users of biometrics have been expanding
from professionals to common people. The requirements of biometrics have also
been shifted from solely high reliability to convenience, ergonomics, good user
acceptance, and good user experience. Before, when biometric systems were only
used in high security scenarios, a special job description could be posted for a
suitable candidate to use a particular biometric system. However, nowadays bio-
metric information has been embedded into ID cards. Biometrics has to be suitable
for a large population. It is either to educate the public of biometrics or to design
ergonomic biometrics. Thus, it is easy to predict that more ergonomic biometric
systems will be invented, and many of them could beneﬁt from EBD model.
12.4
Experiments
279
www.ebook3000.com

12.5
Summary
In this chapter, we propose a novel door knob hand recognition system using
ergonomic biometrics design model, which considers ergonomics in all three
design stages. We invent door knob imaging scheme to capture an omni-vision
hand image. Combining LGBPHS and DPL methods, the proposed system achieves
promising recognition performance. Experimental result shows that the EER could
be as lower as 0.091%. In addition, the proposed system shows that designing a
biometric system with prior constraints of ergonomics does not deﬁnitely means a
worse performance. We expect that the proposed system and model could inspire
more ergonomic biometric systems.
References
Aharon M, Elad M, Bruckstein A (2006) K-SVD: an algorithm for designing overcomplete
dictionaries for sparse representation. IEEE Trans Signal Process 54(11):4311–4322
Ahonen T, Hadid A, Pietika¨inen M (2004) Face recognition with local binary patterns. In:
Pajdla T, Matas J (eds) Computer vision – ECCV 2004 (LNCS 3021). Springer, Heidelberg,
pp 469–481
Albrecht A (2001) Understanding the issues behind user acceptance. Biom Technol Today
9(1):7–8
Alkassar S, Woo W, Dlay S, Chambers J (2015) Robust sclera recognition system with novel
sclera segmentation and validation techniques. IEEE Trans Syst Man Cybernet Syst
47:474–486. doi:10.1109/TSMC.2015.2505649
Alonso-Fernandez F, Fierrez J, Ortega-Garcia J (2012) Quality measures in biometric systems.
IEEE Secur Privacy 10(6):52–62
Biosecure, Biosecure Reference and Evaluation Framework [Online]. http://biosecure.it-sudparis.
eu/AB/index.php?option¼com_content& view¼article&id¼12&Itemid¼15
Bolle R, Pankanti S, Ratha N (2000) Evaluation techniques for biometrics-based authentication
systems (FRR). In: Proceedings 15th international conference on pattern recognition, pp
831–837
Bolle R, Ratha N, Pankanti S (2004) Error analysis of pattern recognition systems – the subsets
bootstrap. Comput Vis Image Underst 93(1):1–33
Burge M, Bowyer K (2013) In: Burge MJ, Bowyer KW (eds) Handbook of iris recognition.
Springer, London
Bustard J, Nixon M (2010) Toward unconstrained ear recognition from two-dimensional images.
IEEE Trans Syst Man Cybernet A Syst Humans 40(3):486–494
Carvalho R, Rosa P (2010) Identiﬁcation system for smart homes using footstep sounds. In:
Proceedings of IEEE international symposium industrial electronics (ISIE), Bari, Italy, pp
1639–1644
Daugman J (2007) New methods in iris recognition. IEEE Trans Syst Man Cybernet B Cybernet
37(5):1167–1175
Dunstone T, Yager N (2009) Biometric system and data analysis – design, evaluation, and data
mining. Springer, New York
Eidan R (2013) Hand biometrics: overview and user perception survey. In: Proceedings of 2nd
international conference on informatics and applications (ICIA), Ło´dz, Poland, pp 252–257
280
12
Door Knob Hand Recognition System

Elliott S, Massie S, Sutton M (2007) The perception of biometric technology: a survey. In:
Proceedings of IEEE workshop on automatic identiﬁcation advanced technologies, Alghero,
Italy, pp 259–264
Elliott S, Senjaya B, Kukula E, Werner J, Wade M (2010) An evaluation of the human biometric
sensor interaction using hand geometry. In: Proceedings of IEEE international Carnahan
conference on security technology (ICCST), San Jose, CA, USA, pp 259–265
Embassy London (2009) Biometric passports and travel to the United States. Visa Services.
[Online]. http://www.usembassy.org.uk/visaservices/?p¼420
Eschenburg F et al (2005) User acceptance: the BioSec approach. Biom Technol Today
13(7):80–10
Expert (2012) Expert Working Group on Human Factors in Latent Print Analysis, Latent print
examination and human factors: improving the practice through a systems approach,
U.S. Department of Commerce, National Institute of Standards and Technology, Gaithersburg,
MD,
USA,
Technical
report,
2012
[Online].
http://www.nist.gov/manuscript-
publicationsearch. cfm?pub_id¼910745
Faddis K, Howard J, Stracener J (2011) Enhancing the usability of human machine interface on the
handheld interagency identiﬁcation detection equipment (HIIDE). In: Proceedings of 21st
international conference on systems engineering (ICSEng), Las Vegas, NV, USA, pp 305–310
Gong Y, Zhang D, Shi P, Yan J (2013) Handheld system design for dual-eye multispectral iris
capture with one camera. IEEE Trans Syst Man Cybernet Syst 43(5):1154–1166
Gu S, Zhang L, Zuo W, Feng X (2014) Projective dictionary pair learning for pattern classiﬁcation.
In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ (eds) Advances in
neural information processing systems 27. Newry, U.K, Curran Association, pp 793–801
Guo Z, Zhang D, Zhang L, Zuo W (2009a) Palmprint veriﬁcation using binary orientation
co-occurrence vector. Pattern Recogn Lett 30(13):1219–1227
Guo Z, Zuo W, Zhang L, Zhang D (2009b) Palmprint veriﬁcation using consistent orientation
coding. In: Proceedings of 16th IEEE international conference on image processing (ICIP),
Cairo, Egypt, pp 1965–1968
Guo Z, Zhang L, Zhang D, Mou X (2010a), Hierarchical multiscale LBP for face and palmprint
recognition. In: Proceedings of 17th IEEE international conference on image processing
(ICIP), Hong Kong, pp 4521–4524
Guo Z, Zhang L, Zhang D, Zhang S (2010b) Rotation invariant texture classiﬁcation using
adaptive LBP with directional statistical features. In: Proceedings of 17th IEEE international
conference on image processing (ICIP), Hong Kong, pp 285–288
Guo Z, Zhang L, Zhang D (2010c) A completed modeling of local binary pattern operator for
texture classiﬁcation. IEEE Trans Image Process 19(6):1657–1663
Guo Z, Zhang L, Zhang D (2010d) Rotation invariant texture classiﬁcation using LBP variance
(LBPV) with global matching. Pattern Recogn 43(3):706–719
Huang X, Li S, Wang Y (2004) Shape localization based on statistical method using extended local
binary pattern. In: Proceedings of IEEE 1st symposium on multi-agent security survivability,
Piscataway, NJ, USA, pp 184–187
Jain A (2009) Next generation biometrics, Department of Computer Science and Engineering,
Michigan State University [Online]. http://www.cse.msu.edu/rgroups/biometrics/Presenta
tions/Next_generation_biometrics_Korea_Dec2010.pdf
Jain A, Kumar A (2012) Biometric recognition: an overview. In: Mordini E, Tzovaras D (eds)
Second generation biometrics: the ethical, legal and social context (The International Library
of Ethics, Law and Technology), vol 11. Springer, Dordrecht, pp 49–79
Jain A, Ross A, Prabhakar S (2004) An introduction to biometric recognition. IEEE Trans Circuits
Syst Video Technol 14(1):4–20
Jain A, Flynn P, Ross A (2008) Handbook of biometrics. Springer, New York
Jain A, Ross A, Nandakumar K (2011) Introduction to biometrics. Springer, New York
References
281
www.ebook3000.com

Jeng A, Chen L (2009) How to enhance the security of e-passport. In: Proceedings of international
conference on machine learning cybernetics, vol 5. Baoding, China, pp 2922–2926
Jiang Z, Lin Z, Davis L (2011) Learning a discriminative dictionary for sparse coding via label
consistent K-SVD. In: Proceedings of IEEE conference on computer vision and pattern
recognition, Colorado Springs, CO, USA, pp 1697–1704
Jiang Z, Lin Z, Davis L (2013) Label consistent K-SVD: Learning a discriminative dictionary for
recognition. IEEE Trans Pattern Anal Mach Intell 35(11):2651–2664
Jones L (2009) Reﬂective and catadioptric objectives. In: Bass M, DeCusatis C, Enoch JM (eds)
Handbook of optics, vol 1: Geometrical and physical optics, polarized light, components and
instruments, 3rd edn. McGraw-Hill, New York, pp 1–29
Kalka N, Zuo J, Schmid N, Cukic B (2010) Estimating and fusing quality factors for iris biometric
images. IEEE Trans Syst Man Cybernet A Syst Humans 40(3):509–524
Kong W, Zhang D (2002) Palmprint texture analysis based on low-resolution images for personal
authentication. In: Proceedings 16th international conference on pattern recognition (ICPR),
vol 3. Quebec City, QC, Canada, pp 807–810
Kong A, Zhang D (2004) Competitive coding scheme for palmprint veriﬁcation. In: Proceedings
of 17th international conference on pattern recognition (ICPR), Cambridge, UK, vol 1, pp
520–523
Kukula E (2008) Design and evaluation of the human-biometric sensor interaction method, Ph.D.
dissertation, The Center for Education and Research in Information Assurance Security,
Purdue University, West Lafayette, IN, USA
Kukula E, Elliott S, Tamer S, Senarith P (2006) Biometrics and manufacturing: a recommendation
of working height to optimize performance of a hand geometry machine, Report BSPA/09-
0001, Biometrics Standards, Performance, and Assurance Laboratory, Purdue University, West
Lafayette, IN, USA
Kukula E, Elliott S, Duffy V (2007) The effects of human interaction on biometric system
performance. In: Duffy VG (ed) Digital human modeling. Springer, Heidelberg, pp 904–914
Kukula E, Sutton M, Elliott S (2010) The human–biometricsensor interaction evaluation method:
Biometric performance and usability measurements. IEEE Trans Instrum Meas 59(4):784–791
Laadjel M, Kurugollu F, Bouridane A, Yan W (2009) Palmprint recognition based on subspace
analysis of Gabor ﬁlter bank. In: Proceedings of 10th Paciﬁc Rim conference multimedia
advances in multimedia information processing (PCM), Bangkok, Thailand, pp 719–730
Lai Z, Xu Y, Jin Z, Zhang D (2014) Human gait recognition via sparse discriminant projection
learning. IEEE Trans Circuits Syst Video Technol 24(10):1651–1662
Levush R (2014) Biometric data retention for passport applicants and holders. Global Legal
Research Center for Law Library of Congress, Washington, DC, USA, Technical Report
[Online]. http://www.loc.gov/law/help/biometric-data-retention/
LFW (2015) Results [Online]. http://vis-www.cs.umass.edu/lfw/results.html
Li W, Zhang D, Lu G, Luo N (2012) A novel 3-D palmprint acquisition system. IEEE Trans Syst
Man Cybernet A Syst Humans 42(2):443–452
Liu C, Wechsler H (2002) Gabor feature based classiﬁcation using the enhanced ﬁsher linear
discriminant model for face recognition. IEEE Trans Image Process 11(4):467–476
Liu J et al (2014) The Beihang keystroke dynamics systems, databases and baselines.
Neurocomputing 144:271–281
Liu F, Zhang D, Shen L (2015) Study on novel curvature features for 3D ﬁngerprint recognition.
Neurocomputing 168:599–608
Mansﬁeld A, Wayman J (2002) Best practices in testing and reporting performance of biometric
devices, Technical report CMSC 14/02. Centre for Mathematical Scientiﬁc and Computing,
National Physical Laboratory, Teddington, UK
Medioni G, Choi J, Kuo C, Fidaleo D (2009) Identifying noncooperative subjects at a distance
using face images and inferred three-dimensional face models. IEEE Trans Syst Man Cybernet
A Syst Humans 39(1):12–24
282
12
Door Knob Hand Recognition System

Mordini E, Tzovaras D (2012) In: Mordini E, Tzovaras D (eds) Second generation biometrics: the
ethical, legal and social context (The International Library of Ethics, Law and Technology),
vol 11. Springer, Dordrecht
Nakkabi Y, Traore I, Ahmed A (2010) Improving mouse dynamics biometric performance using
variance reduction via extractors with separate features. IEEE Trans Syst Man Cybernet A Syst
Humans 40(6):1345–1353
Ojala T, Pietikainen M, Maenpaa T (2002) Multiresolution gray-scale and rotation invariant
texture classiﬁcation with local binary patterns. IEEE Trans Pattern Anal Mach Intell
24(7):971–987
Padmapriya S, KalaJames E (2012) Real time smart car lock security system using face detection
and recognition. In: Proceedings of international conference on computer communication and
informatics (ICCCI), Coimbatore, India, pp 1–6
Qu X, Zhang D, Lu G (2016) A novel line-scan palmprint acquisition system. IEEE Trans Syst
Man Cybernet Syst 46(11):1481–1491
Rubin J, Chisnell D (2008) Handbook of usability testing: how to plan, design, and conduct
effective tests, 2nd edn. Wiley, Indianapolis, IN
Rubinstein R, Peleg T, Elad M (2013) Analysis K-SVD: a dictionary learning algorithm for the
analysis sparse model. IEEE Trans Signal Process 61(3):661–677
Russell H (1893) Design for a door-knob. U.S. Patent D4 114 S
Schlick C (2009) Industrial engineering and ergonomics. Springer, Heidelberg
Shan S, Zhang W, Su Y, Chen X, Gao W (2006) Ensemble of piecewise FDA based on spatial
histograms of local (Gabor) binary patterns for face recognition. In: Proceedings of 18th
international conference on pattern recognition, Hong Kong, pp 606–609
Shen L, Bai L, Auer D (2008) 3D Gabor wavelets for evaluating SPM normalization algorithm.
Med Image Anal 12(3):375–383
Vatsa M, Singh R, Noore A (2009) Uniﬁcation of evidence-theoretic fusion algorithms: a case
study in level-2 and level-3 ﬁngerprint features. IEEE Trans Syst Man Cybernet A Syst
Humans 39(1):47–56
Wertheim K (2010) Human factors in large-scale biometric systems: a study of the human factors
related to errors in semiautomatic ﬁngerprint biometrics. IEEE Syst J 4(2):138–146
Wong M, Zhang D, Kong W, Lu G (2005) Real-time palmprint acquisition system design. IEE
Proc Vis Image Signal Process 152(5):527–534
Wu X, Zhang D, Wang K (2006) Palm line extraction and matching for personal authentication.
IEEE Trans Syst Man Cybernet A Syst Humans 36(5):978–987
Xie J, Zhang L, You J, Zhang D, Qu X (2012) A study of hand back skin texture patterns for
personal identiﬁcation and gender classiﬁcation. Sensors (Basel) 12(7):8691–8709
Yan K, Chen Y, Zhang D (2011) Gabor surface feature for face recognition. In: Proceedings of 1st
Asian conference on pattern recognition (ACPR), Beijing, China, pp 288–292
Zhang B, Li Y (2012) Catadioptric vision system. In: Automatic calibration and reconstruction for
active vision systems (Intelligent systems, control and automation: science and engineering),
Ch 6, pp 117–150
Zhang D, Kong W, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhang W, Shan S, Gao W, Chen X, Zhang H (2005) Local Gabor binary pattern histogram
sequence (LGBPHS): a novel non-statistical model for face representation and recognition.
In: Proceedings of 10th IEEE international conference computer vision, Beijing, China, vol
1, pp 786–791
Zhang D, Guo Z, Lu G, Zuo W (2010a) An online system of multispectral palmprint veriﬁcation.
IEEE Trans Instrum Meas 59(2):480–490
Zhang D, Kanhangad V, Luo N, Kumar A (2010b) Robust palmprint veriﬁcation using 2D and 3D
features. Pattern Recogn 43(1):358–368
Zhou Z, Du E, Thomas N, Delp E (2012) A new human identiﬁcation method: sclera recognition.
IEEE Trans Syst Man Cybernet A Syst Humans 42(3):571–583
References
283
www.ebook3000.com

Zhu Z et al (2015) Three-dimensional Gabor feature extraction for hyperspectral imagery classi-
ﬁcation using a memetic framework. Inf Sci 298:274–287
Zuo W, Yue F, Wang K, Zhang D (2008) Multiscale competitive code for efﬁcient palmprint
recognition. In: Proceedings 19th international conference on pattern recognition (ICPR),
Tampa, FL, USA, pp 1–4
284
12
Door Knob Hand Recognition System

Part IV
Some New Head-Based Biometrics
www.ebook3000.com

Chapter 13
Dynamic Tongueprint Recognition
Abstract Biometrics, which use human physiological or behavioral features for
personal identiﬁcation, currently face the challenge of designing a secure biometric
system that will accept only the legitimate presentation of the biometric identiﬁers
without being fooled by the doctored or spoofed measurements that are input into
the system. More biometric traits are required for improving the performance of
authentication systems. In this chapter, we present a new number for the biometrics
family, i.e. tongueprint, which uses particularly interesting properties of the human
tongue to base a technology for noninvasive biometric assessment. The tongue is a
unique organ which can be stuck out of the mouth for inspection, whose appearance
is amenable to examination with the aid of a machine vision system. Yet it is
otherwise well protected in the mouth and difﬁcult to be forged. Furthermore, the
involuntary squirm of the tongue is not only a convincing proof that the subject is
alive, but also a feature for recognition. That is to say, the tongue can present both
static features and dynamic features for authentication. However, little work has
hitherto been done on the tongue as a biometric identiﬁer. In this work, we make use
of a database of tongue images obtained over a long period to examine the
performance of the tongueprint as a biometric identiﬁer. Our research shows that
tongueprint is a promising candidate for biometric identiﬁcation and worthy of
further research.
Keywords Tongueprint • Veriﬁcation • Identiﬁcation
13.1
Introduction
There has been considerable research in biometrics (Zhang 2000; Jain and Healey
1998) over the last two decades. The list of physiological and behavioral biometric
characteristics that have to date been developed and implemented is long and
includes the face (Li and Juwei 1999; Abate et al. 2007), iris (Daugman 2004;
Bowyer et al. 2007), ﬁngerprint (Ratha et al. 1996), palmprint (Zhang et al. 2000),
hand shape (Sanchez-Reillo et al. 2000), voice (Wan and Renals 2005), signature
(Lee et al. 1996) and gait (Wang et al. 2003). Notwithstanding this great and
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_13
287

increasing variety of biometrics, no biometric has yet been developed that is
perfectly reliable or secure. For example, ﬁngerprints and palmprints are usually
frayed; voice, signatures, hand shapes and iris images are easily forged; face
recognition can be made difﬁcult by occlusions or face-lifts; and biometrics such
as ﬁngerprints, iris and face recognition are susceptible to spooﬁng attacks
(O’Gorman 2003), i.e. the biometric identiﬁers can be copied and used to create
artifacts that can deceive many currently available biometric devices. The great
challenge to biometrics is thus to improve recognition performance and be maxi-
mally resistant to deceptive practices (Jain et al. 2004). To this end, many
researchers have sought to improve reliability and frustrate spoofers by developing
biometrics that are highly individuating; yet at the same time, present a highly
complex, hopefully insuperable challenge to those who wish to defeat them.
In this chapter, we propose a dynamic tongueprint, a biometric identiﬁer that for
the ﬁrst time, makes use of both static features of the biometric as well as dynamic
features. As a biometric identiﬁer, tongueprint has the following properties. To
begin with, the geometric features of the tongue, as well as the cracks and textures
on its surface, are distinctive to each person. By way of illustration, Fig. 13.1a, b
show some different tongue geometries and (c) shows some different types of
cracks and (d) shows some different textures. Second, the tongue is the only internal
organ that can easily be exposed for inspection and it is the exposed portion of the
tongue that carries a great deal of the physiological information including geomet-
ric features, crack features and texture features, which are named as “tongueprint”
synthetically. Third, according to our long time observations, the geometric
features, crack features and texture features of an individual tongue are stable.
Aside from the above three points, there are two unique features of the human
tongue. First, contained in the mouth, the human tongue is protected from the
external environment, unlike for example the ﬁngers that could be damaged easily.
Second, the involuntary squirm of the human tongue not only is a natural and
convincing proof that a subject is alive, but can also be utilized for discriminating
individuals.
The dynamic tongueprint makes use of a dynamic feature, which is the invol-
untary squirm of the human tongue when it is stuck out of the mouth. To date, this
feature has been regarded as mostly a hindrance to the process of capturing
biometric images, although it has been recognized that it does provide a natural
and convincing proof that a subject is alive. However, it has not been used
previously to discriminate individuals. Our authentication system, (Fig. 13.2)
which makes use of both static and dynamic features, contains two modules:
enrollment and recognition. The recognition module operates in two phases: in
the ﬁrst phase, liveness detection, dynamic information about the squirm of the
tongue is used to detect whether the tongue is in fact living; the second phase,
feature extraction, extracts both static physiological features and dynamic squirm
features. The use of dynamic as well as static information not only means that our
proposed system can provide liveness detection, but also has the potential to
improve recognition (Tistarelli et al. 2009). Recent psychophysical and neural
studies (OToole et al. 2002; Knight and Johnston 1997) have described the crucial
288
13
Dynamic Tongueprint Recognition
www.ebook3000.com

role of dynamic information in the human visual recognition process while work by
Haxby et al. (2000) demonstrated that visual perception makes use of a double
architecture of two connected neural activation patterns: one for processing static,
unchanging and invariant features and the other for changing features (Tistarelli
et al. 2009), an architecture reﬂected in this proposed approach.
Fig. 13.1 The distinctiveness of the human tongue patterns. (a) Some different tongue geometries
(front view). (b) Some different tongue geometries (proﬁle view). (c) Some different types of
tongue surface cracks. (d) Some different tongue surface textures
13.1
Introduction
289

13.2
Background
To touch upon the novel concept of dynamic tongueprint as a pattern for recogni-
tion, it is useful to introduce tongue anatomy ﬁrstly. According to the study of
zoology, the invertebrate began to have the tongue structure, such as locusts. And
the structure and functions of the tongue of vertebrates become much more com-
plex, such as snakes or lizards. It is obvious that different animals have different
tongues. In terms of the human being, the tongue normally resides inside the mouth
and mainly composed of muscles (Brand and Isselhard 1998), covered with a
mucous membrane. Two parts of the tongue are separated for easy description,
which are the root and body. Note that this chapter only focuses on the visual
information from the tongue body (see Fig. 13.3 for the anatomic structure of the
tongue).
The tongue is composed of muscle tissues. It is the muscles that move the tongue
and give it the dynamic property that can be used for liveness detection. In addition,
the muscles make up most of its mass and various shapes.
Small nodules of tissue (papillae) cover the upper surface of the tongue. Between
the papillae are the taste buds, which provide taste, heat, pain, and tactile informa-
tion. Note that the various distributions and shape of fungiform papilla show the
various textures on the tongue surface. In addition to taste, the tongue functions in
moving food to aid chewing and swallowing, and it is important in speech.
Figure 13.4 shows a micrograph of the surface of a human tongue featuring both
fungiform and ﬁliform papillae.
Tongue Image Sequence
No
Yes
Dynamic 
Feature Extraction
Static
Feature Extraction
Reject
Dynamic
Feature Extraction
Static
Feature Extraction
Templates
Decision
Liveness 
Detection
Database
Match
Recognition
Enrollment
Tongue Image Sequence
Fig 13.2 The ﬂowchart of tongueprint recognition
290
13
Dynamic Tongueprint Recognition
www.ebook3000.com

Fig. 13.3 Anatomy of the human tongue
Fig. 13.4 A micrograph of a fungiform papilla (the large round structure in the center of the
image) surrounded by hairlike ﬁliform papillae. In this image, the ﬁlliform papillae are “combed”
down so that they are lying side by side
13.2
Background
291

13.3
Tongue Squirm and Its Applications
When a person sticks his or her tongue out, the tongue squirms. Tongue squirm is a
type of physical reaction when the muscles of the tongue are stretching. This
squirming is both continuous and involuntary. From the point of view of biometrics,
this squirm constitutes a very powerful “dynamic signature” which can augment
other information obtained from the tongue and has the potential to dramatically
improve the performance of the tongue as a biometric for individual identiﬁcation.
In the following, we describe how it can be applied to liveness detection and squirm
feature extraction.
13.3.1
Liveness Detection
Notwithstanding its performance on other criteria, any biometric authentication
system will be open to spooﬁng as long as it cannot distinguish between a photo-
graph and biometric features of a living individual (Kollreider et al. 2009). Liveness
detection (or vitality detection as it is sometimes known) seeks to ensure that input
patterns are not from an inanimate object (Jain et al. 2004), determining whether the
biometric data being captured is indeed from a legitimate live user who is physi-
cally present in front of the acquisition device. There has been research (Kollreider
et al. 2005; Moon et al. 2005; Sandstr€om 2004) that counter the threat of physical
spooﬁng of biometric samples and various liveness detection methods have been
implemented in some devices, for example, using perspiration in association with
ﬁngerprints and iris shrink with iris-based applications. However, it is not possible
to claim success in this area as proposed and implemented approaches to date have
either required complex hardware or hard to defeat. However, research continues in
this area (Toth 2005).
Tongue squirm is an especially attractive liveness feature for three reasons. First,
it is an involuntary action. It results from the muscles of the tongue stretching when
the tongue is stuck out of the mouth and cannot be controlled. Moreover, unlike iris
shrinking, it does not need to be externally stimulated (in the case of iris shrinking,
it is by illumination) and so requires no additional hardware. Second, tongue squirm
is random. It is not a performance that any spoofer can learn and imitate. Third,
tongue squirm is a very slight and subtle movement and as such, it does not affect
the global shape of the tongue during the capture process. Finally, tongue squirm is
thus available to be captured in a sequence of continuous images reﬂecting live
action using purely software-based methods rather than additional hardware.
An initial consideration in capturing tongue squirm features is the formidable
task of learning the activity in the original image space. However, because the
captured images of the squirm can be regarded as points in high-dimensional space
and these may generally be expected to lie on a low-dimensional manifold embed-
ded in the high-dimensional image space, we can reduce the size of the task by
292
13
Dynamic Tongueprint Recognition
www.ebook3000.com

analyzing tongue squirm in a low-dimensional subspace rather than the original
image space. The goal thus becomes ﬁnding a compromise between preserving the
global and local structure. It is well known that, of the current dimensionality
reduction methods, the principal component analysis (PCA) (Turk and Pentland
1991) is good at preserving the global structure and locally linear embedding (LLE)
(Roweis and Saul 2000) at preserving local structure. A compromise between these
two goals can be found in Orthogonal Neighborhood Preserving Projections
(ONPP) (Kokiopoulou and Saad 2007), a recently proposed method in image
recognition for feature extraction and dimensionality reduction for visualization.
ONPP is a linear dimensionality reduction technique and tends to preserve not only
the local, but also the global geometry of high dimensional data samples. For this
reason, we use ONPP to embed the tongue squirm into a lower dimensional feature
space.
The tongue image sequence can be represented as a data set I ¼ {I1, I2, . . . , IN} 2
Rm  n, where N is the number of the images. This data set can be transformed into a
matrix X ¼ [x1, x2, . . . , xN] 2 Rh  N, where xi is the vector representation of Ii 2 I
and h ¼ m  n. The key to ONPP is to construct the matrix M ¼ X I  WT


I  W
ð
Þ
XT and solve a generalized eigenvalue problem to create a transform matrix V for
the dimensionality reduction. This mapping can be formulated as:
Y ¼ VTX
i ¼ 1; 2; . . . ; N
ð
Þ
ð13:1Þ
where W is the weight matrix, Y 2 Rd  N contains only the data set mapped into
d-dimensional space (d  h), and the columns of V are the basis of the eigenvectors
associated with the d smallest eigenvalues of M. More details about ONPP can be
found in (Kokiopoulou and Saad 2007).
Figure 13.5 plots the visualizations of the squirm in three-dimensional space
using ONPP. We can see that the image sequence of the living tongue exhibits
random trajectories in three-dimensional subspace. More importantly, images of an
artifact tongue that does not squirm will not exhibit any such random trajectories in
the subspace. (For a description of our related experiments, see Sect. 13.5.4). This is
supported by Eq. (13.1). That is, if the matrix X is composed of the same column
vectors and V is the nonsingular matrix, then the result of mapping Y will be
composed of the same vectors. Hence, only one point is obtained in the subspace.
We propose to use this circumstance as a qualitative proof in our liveness detection
procedure.
The novel framework proposed here is based on the analysis of the low dimen-
sional manifold, as mentioned above. Figure 13.6 is a ﬂowchart for our liveness
detection procedure. The ﬁrst step is to determine whether the input image
sequences are valid against both the front and proﬁle views of the tongue. The
second step is to calculate the dynamic descriptor of the image sequence. The
dynamic descriptor comes from the statistical variant and can be deﬁned as in the
following equation:
13.3
Tongue Squirm and Its Applications
293

D ¼
1
N  1
X
N
i¼1
xi  μ
ð
Þ xi  μ
ð
ÞT


ð13:2Þ
where N is the number of samples assigned to the subject and μ is the mean value of
the samples xi.
Then, the value of the dynamic descriptor is compared with the threshold and a
decision is made based on the result of this comparison.
Fig. 13.5 Mapping the tongue squirm image sequences into 3D subspace: (a) is from the front
view image sequence and (b) is from the proﬁle view image sequence
294
13
Dynamic Tongueprint Recognition
www.ebook3000.com

13.3.2
Squirm Features Extraction
Embedded tongue squirm features can be extracted and visualized using manifold
learning technologies (Kokiopoulou and Saad 2007), with the different trajectories
of the squirms of different subjects being visualized in the three-dimensional
subspace and clustered in different areas of the subspace (illustrated in Fig. 13.7).
The task of enrollment thus requires the collection of feature vectors from a given
subject and the learning of the parameters, i.e. the mean value μ and covariance
matrix Σ, which can be calculated by Eq. 13.3:
μj ¼
X
Nj
i¼1
xi
Nj
Σj ¼
1
Nj  1
X
Nj
i¼1
xi  μj


xi  μj

T
8
>
>
>
>
>
>
>
<
>
>
>
>
>
>
>
:
ð13:3Þ
where Nj is the number of samples assigned to subject j. Once the parameters for
each tongue squirm have been found, determining a probe class is straightforward
by minimum distance method.
Calculate the dynamic 
descriptor 
Tongue image 
sequence
>=Threshold?
No
Image valid?
Yes
No
Live tongue
Fake tongue
Yes
Fig. 13.6 Flowchart for
tongue liveness detection
13.3
Tongue Squirm and Its Applications
295

13.4
Extraction of Static Physiological Features
In this section, we focus on introducing the ways of extracting physiological static
features of an individual tongue, i.e. its geometric features, crack features and
texture features.
Before feature extraction, it is necessary to obtain the contours of the tongue
from the captured tongue image. The accuracy and robustness of the tongue contour
extraction method are crucial for this recognition system. Tongue contour extrac-
tion (sometimes known as tongue segmentation) is made difﬁcult by the fact that
the surface color of the tongue is very similar to that of the ambient tissue. The
literature contains many techniques for solving this problem (Pang et al. 2005). In
this work, we carry out segmentation in the red channel (shown in Fig. 13.8b) of the
original RGB images, since the tongue is basically red and tongue images contrast
more strongly in the red channel. Figure 13.8c shows the contours of the tongue in
proﬁle and front views.
13.4.1
Geometric Features
The geometric features of the tongue can be obtained by a set of control points: P1,
P2, . . ., P11, Ptip and Pm (shown in Fig. 13.9). Note P1 and P2 are obtained from the
corners of the mouth. We deﬁne the part of the tongue that is of interest; the part
below segment LP1,P2 in Fig. 13.9a, from the corners of the mouth and the tip of the
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
-0.2
0
0.2
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Subject A
Subject B
Subject C
Subject D
Subject E
Fig. 13.7 Different tongue squirm trajectories in 3D subspace from ﬁve different subjects
296
13
Dynamic Tongueprint Recognition
www.ebook3000.com

tongue. The following describes the ways we measure (1) the width of the tongue,
(2) the thickness of the tongue, and (3) the curvature of the contour of the tongue to
serve as our measurement vectors:
(1) Width: The width vector W is constructed from the lengths of ﬁve segments.
Four segments (LP3P4,LP5P6LP7P8,LP9P10) are parallel to the segmentLP1,P2. These
four segments are equidistant, according to the following formula:
d LP1,P2; LP3,P4
ð
Þ ¼ d LP3,P4; LP5,P6
ð
Þ ¼ d LP5,P6; LP7,P8
ð
Þ ¼ d LP7,P8; LP9,P10
ð
Þ
ð13:4Þ
where d() represents the distance between two segments.
(2) Thickness: The thickness of the tongue is deﬁned as follows. Take a line
between Ptip and Pm (Fig. 13.9b). The equidistant points on LPmPtip are labeled
Pa1, Pa2, Pa3, Pa4. Crossing these points, we can get the orthogonal lines of the
segment LPmPtip, whose lengths within the proﬁle view contour are used for the
thickness vector T.
(3) Curvature of the contour of the tongue: We measure curvature of the contour of
the tongue by using total curvature functions (TCF) (Pikaz and Dinstein 1995).
Fig. 13.9 The tongue feature model, front (a) and proﬁle (b) views
Fig. 13.8 Tongue contour extraction. (a) The original image; (b) The red channel of image (a); (c)
Extracted tongue contours
13.4
Extraction of Static Physiological Features
297

The total curvature function is an approximate estimation method and deﬁned
for one segment between the two points Q1 and Q2, as illustrated in Fig. 13.10,
where the curvature at Q1 can be formulated as:
C1 ¼ a1 L1 þ L2
ð
Þ
ð13:5Þ
and the curvature at Q2 is formulated as:
C2 ¼ a2 L2 þ L3
ð
Þ
ð13:6Þ
The total curvature value (TC) of the segment L2 between Q1 and Q2 is
formulated as:
TC ¼ L2 * C1  C2
ð
Þ
ð13:7Þ
We then use these TC to build the vector Cur with the curvature values at the
control points P3 , P4 , . . . , P9 , P10 in Fig. 13.9a.
Since the components of these vectors have different dimensions, they have a
large dynamic range. Thus, it is necessary to normalize these components into a
common range. The ﬁnal step is to combine the three measurement vectors, forming
the geometric feature vector.
13.4.2
Crack Features
Cracks are obvious line features which are primarily found on the central part of
tongue surface. To extract this information, we had set up a sub-image of the
segmented tongue image as a region of interest (ROI). This region was selected
using the coordinates system PcornerOPtip and had an area of 256*256 pixels,
corresponding to the rectangular area enclosed by the white line in Fig. 13.11a.
Fig. 13.10 Total curvature measures. L1: length of the segment between Q1 and its predecessor
point; L2: length of the segment between Q1 and Q2; L3: length of the segment between Q2 and its
successor point; a1: interior angle at Q1; a2: interior angle at Q2
298
13
Dynamic Tongueprint Recognition
www.ebook3000.com

To extract the crack features, we applied two-dimensional Gabor ﬁlter (Kong et al.
2003; Jain et al. 1998), which has the following general form (Kong et al. 2003):
G x; y; θ; u; σ
ð
Þ ¼
1
2πσ2 exp x2 þ y2
2σ2


exp 2πi ux cos θ þ uy sin θ
ð
Þ
f
g
ð13:7Þ
where i ¼
ﬃﬃﬃﬃﬃﬃﬃ
1
p
; u is the frequency of the sinusoidal wave, θ controls the orientation
of the function, and σ is the standard deviation of the Gaussian envelope. In order to
make the Gabor ﬁlter more robust against brightness, it is set to zero DC (direct
current) with the application of the following formula (Kong et al. 2003):
Ptip
q – 0°
q – 45°
q – 90°
q – 0°
q – 45°
q – 90°
q – 0°
q – 45°
q – 90°
q – 0°
q – 45°
q – 90°
a
b
d
f
e
g
b’
c
c’
Pcorner
O
Fig. 13.11 (a) Illustrates the obtaining of the ROI; (b) and (c) are two samples of the cracks in the
ROI; and (b’) and (c’) give the results of the histogram equalization from (b) and (c) respectively.
(d) and (f) are respectively the real parts of the Gabor ﬁltered results of (b’) and (c’). (e) and (g) are
the imaginary parts of the Gabor ﬁltered results of (b’) and (c’)
13.4
Extraction of Static Physiological Features
299

G0 x; y; θ; μ; σ
ð
Þ ¼ G x; y; θ; μ; σ
ð
Þ 
X
n
i¼n
X
n
j¼n
G i; j; θ; μ; σ
ð
Þ
2n þ 1
ð
Þ2
ð13:8Þ
where (2n + 1)2 is the size of the ﬁlter. Figure 13.11d–g show some results of this
procedure ﬁltered using the Gable ﬁlter.
An input tongue sub-image I(x, y) is convolved with G0. Then, the sample point
in the ﬁltered image can be coded as two bits, (br, bi) using the following rules:
br ¼ 1
if Re I  G0
½
  0
br ¼ 0
if Re I  G0
½
 < 0
bi ¼ 1
if Im I  G0
½
  0
bi ¼ 0
if Im I  G0
½
 < 0
8
>
>
>
<
>
>
>
:
ð13:9Þ
Using this coding method means that only the phase information in the
sub-images is stored in the crack vector. This feature extraction method was
introduced by Daugman (1993) for use in iris recognition.
13.4.3
Texture Features
The crack features describe the overall attributes of a tongueprint and texture
features provide even more detailed information of a tongueprint. In general,
some tongueprints are very different and can be discriminated by their crack
features. However, some are very similar with cracks or have no cracks. Hence,
additional texture features are required for recognition.
The texture discrimination method used here is based on frequency domain
analysis. Figure 13.12 shows four typical tongue surface textures and their
corresponding frequency domain images.
Texture feature representation is used to describe the features in a concise and
easy-to-compare way. We can represent the frequency domain images using the
polar coordination system (r, θ). The frequency domain image is divided into small
parts by a series of circles which have the same center, as shown in Fig. 13.13a. The
energy in each ring area Ri can be deﬁned as:
Ri ¼
X
π
θ¼0
X
Ki
r¼K i1
ð
Þ
If r; θ
ð
Þ
i ¼ 1, 2, . . . K
ð13:10Þ
where If is the frequency domain images in the polar coordination system, and K is
the number of the pixels in the rings for integration. We call Ri the R features.
300
13
Dynamic Tongueprint Recognition
www.ebook3000.com

Meanwhile, the frequency domain image can be divided by a series of lines that
go through the center of the image, as shown in Fig. 13.13b. The energy in each
fan-shaped part Θi can be deﬁned as:
Θi ¼
X
iπ=M
θ¼π i1
ð
Þ=M
X
L
r¼0
If r; θ
ð
Þ
i ¼ 1, 2, . . . M
ð13:11Þ
where L is the length of the line used for integration and M is the number of
fan-shaped parts, Θi are referred to as Θ features.
Fig. 13.12 Textures from different tongues and their frequency domain images. The upper row is
the original texture image. Below each image is its corresponding frequency domain image
Fig. 13.13 Segmentation of frequency domain images (Wenxin et al. 2002). (a) is used for
R features and (b) is for Θ features
13.4
Extraction of Static Physiological Features
301

13.5
Experimental Results
In this section, we validate the feasibility and effectiveness of a biometric identiﬁer
based on dynamic tongueprint by experiments.
13.5.1
Database
Research on the human tongue as a biometric has been hampered by the lack of a
suitable tongue image database. In this chapter, we present a newly developed
tongue image database. This is the ﬁrst attempt at making a tongue image database
that will be available to the biometric research community. The database collected
tongue images from 174 subjects, 66.4% male and 33.6% female from a variety of
ethnic/racial ancestries. Most of the samples came from the young and middle-
aged. The reasons for this restriction are as follows: ﬁrst, the tongue shapes of the
young and middle-aged are more stable than those of children. Second, the old
people are more difﬁcult for keeping samplings. Table 13.1 shows the composition
of the database.
To ensure that the database is useful for assessing and comparing algorithm
recognition techniques, the tongue images were captured at high-resolution
(1280*1024) in a studio environment to obtain both the textures and shapes of the
tongues. Each subject was required to stick out his or her tongue with the tongue
spread. The front and proﬁle views were captured simultaneously using our special
device. Figure 13.14 shows some examples from three different subjects in the
database.
To evaluate the performance of the novel biometric identiﬁer, we conducted
experiments on the ﬁve types of tongue representations: geometric features, crack
features, texture features, squirm features, and the fusion of physiological static
features and squirm features. To compare the results, we ﬁrst calculated the
templates from the gallery set. We then calculated the matching score of the new
tongue image set (containing front and proﬁle views) against the stored template. In
the experiment with the fusion approach, the overall match score is computed
according to the matching score level fusion method (Kittler et al. 1998).
The geometric features, texture features and squirm features from the gallery
samples were averaged to generate the stored template in each case and the
Table 13.1 Composition of the tongue image database
Sex
Age
Male
Female
20–29
30–39
40–49
Number of samples
115
59
103
49
22
Percentage (%)
66.4
33.6
59.2
28.2
12.6
302
13
Dynamic Tongueprint Recognition
www.ebook3000.com

Euclidean distance was used for discrimination. In contrast, the stored feature
template for the crack features from the gallery samples were directly compared
with the probe features by using the minimum Hamming distance.
13.5.2
Identiﬁcation Experiments
In our identiﬁcation experiments, we used a closed universe model as described by
Phillips et al. (2000). In this model, every subject in the probe set is also in the
gallery set. All templates in the gallery set are assumed to have an identity known at
the enrollment phase. Ninety-two subjects were used for both the probe and gallery
sets. We carried out each experiment three times by randomly choosing the images
for the gallery and probe sets three times. Figure 13.15 shows the average recog-
nition rate for this set of experiments. Each curve represents a different type of
tongue representation. The average rank one recognition rates of the fusion rule is
95%. The average rank one recognition rates of the other four inputs are 89.3%,
79.4%, 72.5% and 71% respectively. The recognition performance obtained in
these experiments was encouraging.
Fig. 13.14 Some examples of three subjects in the tongue image database
13.5
Experimental Results
303

13.5.3
Veriﬁcation Experiments
Our veriﬁcation experiments used an open universe model as described by Phillips
et al. (2000). In this model, a subject in the probe set may or may not be present in
the gallery set. The experiments used a probe set of 136 subjects and a gallery of
142 subjects with 102 subjects being present in both the probe and gallery sets. In
other words, templates from 136 probe subjects were compared with templates of
142 gallery subjects resulting in a total of 19,312 veriﬁcation attempts. Of these
attempts, 102 were genuine and 19,210 were impostors. Again, we carried out each
experiment three times by randomly choosing the images for the gallery and probe
sets three times.
The following gives the results for using each of the ﬁve types of tongue
recognition approaches for veriﬁcation as a receiver operating characteristic
(ROC), which shows the false acceptance rate (FAR) and false rejection rate
(FRR) at each threshold value. An effective method will produce the lowest
possible ﬁgure, but they are actually antagonists, meaning they must be traded off
against each other. For this reason, we quantify the veriﬁcation performance here by
using the equal error rate (EER). Then the average of the experimental results is
illustrated in Fig. 13.16. From Fig. 13.16, we can see that the lowest value of EER is
4.1%, which is obtained by the fusion approach. Moreover, the EER of the other
four inputs are 9.6%, 15.2%, 19.1% and 20.4%.
In summary, in this set of experiments, the fusion approach performed better
than the other approaches utilizing solely geometric features, crack features,
2
4
6
8
10
12
14
16
18
20
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Rank
Recognition Rate
fusion
geometric shape
crack
texture
squirm
Fig. 13.15 Recognition rates of the ﬁve types of tongue representation input
304
13
Dynamic Tongueprint Recognition
www.ebook3000.com

textures features, or squirm features. At the same time, geometric features always
perform better than using crack features, texture features, or squirm features. The
poorer performance of crack features is inherent as cracks on the tongue surface are
obvious in some cases, but not in others. The same applies to texture features, which
furthermore, can also be affected by the image quality. As to squirm features, its
performance as a sole feature is affected by the fact that the squirm data sets can
sometimes overlap in the subspace.
13.5.4
Liveness Detection Experiments
It may be difﬁcult to depict all the kinds of attack methods because it is very
difﬁcult to enumerate all attempts for attacking the system by forge. In this chapter,
the liveness detection method is exploited to resist the main fake approach, i.e.,
using a tongue model to spoof the tongueprint recognition system. In this case, we
assumed the same illumination as the whole process takes place in a studio
environment and we used an apparatus to ﬁx the location of the head. Thus, the
fake does not have a dynamic characteristic, such as a living human tongue. Even if
the model is in motion, due to its 2-D planar structure and static characteristic, the
appearances of images are nearly unchanged. Accordingly, these facts can be used
for liveness detection.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
False Acceptance Rate
False Rejection Rate
fusion
geometric shape
crack
texture
squirm
Fig. 13.16 ROC of the ﬁve types of tongue representation input
13.5
Experimental Results
305

Take the image sequence of the front view for example. The dynamics descrip-
tors are tabulated in Table 13.2. Table 13.2 shows the dynamics descriptors for the
image sequence of the front view. Clearly, the values for the fake and live tongues
are very different. As well, we can easily discriminate them by setting a threshold
value.
Table 13.3 provides the results for a total of 100 live and 100 fake tongue image
sequences used in evaluating our liveness detection system. The False Rate (FR) in
the table is deﬁned as FR ¼ FAT þ FRT
total times  100% where the FAT is the number of
the counterfeit tongueprints that were accepted as tongueprints from living tongues.
Similarly, the FRT shows the number of times that the live tongueprints were
rejected as counterfeit.
Looking at Table 13.3, it is immediately observable that the False Rate (FR) can
be reduced to zero only if we select the correct threshold. In theory, if the threshold
is greater than zero, then FR should be zero. In practice, the results are affected by
the noise so that we cannot obtain an ideal performance.
13.6
Summary
The tongueprint is a promising identiﬁer because the tongue is a well-protected, but
accessible internal organ that is difﬁcult to counterfeit or spoof and its visual
features cannot be reverse engineered. It becomes even more attractive as an
Table 13.2 The values of
dynamics descriptor
Image sequence
Dynamics descriptor
Mean value
Minimum
Maximum
Fake tongue
0.03
0
0.16
Live tongue
1.04
0.67
1.63
Table 13.3 Experimental
results of the proposed
liveness detection method
Threshold
FR
0
0.5
0.005
0.03
0.01
0.02
0.15
0.01
0.2
0
0.4
0
0.7
0.005
0.8
0.05
0.9
0.14
1.1
0.27
2.0
0.5
306
13
Dynamic Tongueprint Recognition
www.ebook3000.com

identiﬁer when we make use of the tongue’s involuntary squirm which not only
provides liveness proof, but also cues for use in recognition.
The contribution of this chapter is not to use some new techniques, but for the
ﬁrst time, make claim that the tongue can be a new member in the biometrics
family. The approach proposed in this chapter makes use of both static and dynamic
features of the human tongue for personal authentication. Furthermore, liveness
detection based on involuntary squirm is another highlight for tongue biometrics. In
experiments based on our unique tongue image database, in every case, the fusion
approach outperformed each of the single tongue representation approaches using
geometric features, crack features, texture features, and squirm features alone. The
dynamic tongueprint identiﬁer using the fusion rule can achieve a 95% recognition
rate in rank one in the identiﬁcation mode and 4.1% for EER in the
veriﬁcation mode.
To sum up the above arguments, we can say that tongueprint is a qualiﬁed
member of the biometrics family. The fact that the dynamic tongueprint is a feasible
biometric identiﬁer leaves a way open for its adoption in a variety of settings, such
as law enforcement or where a system must be fool proof against counterfeiting
with liveness detection, and in conjunction with other biometric modalities (Li and
Juwei 1999; Lin and Anil 1998). Perhaps most obviously, tongue information can
be combined into a feature template with face information, producing more accu-
rate recognition performance. Ultimately, as with some other biometrics, the most
difﬁcult problem for the use of the tongueprint as a biometric identiﬁer may be the
need for the user’s cooperation during the capture procedure.
References
Abate A, Nappi M, Riccio D, Sabatino G (2007) 2D and 3D face recognition: a survey. Pattern
Recogn Lett 28:1885–1906
Bowyer K, Hollingsworth K, Flynn P (2007) Image understanding for iris biometrics: a survey.
Comput Vis Image Underst 110(2):281–307
Brand R, Isselhard D (1998) Anatomy of orofacial structures, 6th edn. St. Louis, Missouri
Daugman J (1993) High conﬁdence visual recognition of persons by a test of statistical indepen-
dence. IEEE Trans Pattern Anal Mach Intell 15:1148–1161
Daugman J (2004) How iris recognition works. IEEE Trans Circuits Syst Video Technol 14:21–30
Haxby J, Hoffman E, Gobbini M (2000) The distributed human neural system for face perception.
Trends Cogn Sci 4:223–233
Jain A, Healey G (1998) A multiscale representation including opponent color features for texture
recognition. IEEE Trans Image Process 7:124–128
Jain A, Bolle R, Pankanti S (1998) Biometrics: personal identiﬁcation in networked society.
Kluwer Academic, Boston
Jain A, Pankanti S, Prabhakar S, Lin H, Ross A (2004) Biometrics: a grand challenge. In:
Proceedings of the 17th international conference on pattern recognition (ICPR), vol 2, pp
935–942
Kittler J, Hatef M, Duin R, Matas J (1998) On combining classiﬁers. IEEE Trans Pattern Anal
Mach Intell 20:226–239
Knight B, Johnston A (1997) The role of movement in face recognition. Vis Cogn 4:265–274
References
307

Kokiopoulou E, Saad Y (2007) Orthogonal neighborhood preserving projections: a projection-
based dimensionality reduction technique. IEEE Trans Pattern Anal Mach Intell 29:2143–2156
Kollreider K, Fronthaler H, Bigun J (2005) Evaluating liveness by face images and the structure
tensor. In: Proceedings of the fourth IEEE workshop on automatic identiﬁcation advanced
technologies, pp 75–80
Kollreider K, Fronthaler H, Bigun J (2009) Non-intrusive liveness detection by face images. Image
Vis Comput 27(3):233–244
Kong W, Zhang D, Li W (2003) Palmprint feature extraction using 2-D Gabor ﬁlters. Pattern
Recogn 36:2339–2347
Lee L, Berger T, Aviczer E (1996) Reliable online human signature veriﬁcation systems. IEEE
Trans Pattern Anal Mach Intell 18:643–647
Li S, Juwei L (1999) Face recognition using the nearest feature line method. IEEE Trans Neural
Netw 10:439–443
Lin H, Anil J (1998) Integrating faces and ﬁngerprints for personal identiﬁcation. IEEE Trans
Pattern Anal Mach Intell 20:1295–1307
Moon Y, Chen J, Chan K, So K, Woo K (2005) Wavelet based ﬁngerprint liveness detection.
Electron Lett 41:1112–1113
O’Gorman L (2003) Comparing passwords, tokens, and biometrics for user authentication. Proc
IEEE 91:2021–2040
OToole A, Roark D, Abdi H (2002) Recognizing moving faces: a psychological and neural
synthesis. Trends Cogn Sci 6:261–266
Pang B, Zhang D, Wang K (2005) The bi-elliptical deformable contour and its application to
automated tongue segmentation in chinese medicine. IEEE Trans Med Imaging 24:946–956
Phillips P, Hyeonjoon M, Rizvi S, Rauss P (2000) The FERET evaluation methodology for face-
recognition algorithms. IEEE Trans Pattern Anal Mach Intell 22:1090–1104
Pikaz A, Dinstein I (1995) Matching of partially occluded planar curves. Pattern Recogn
28:199–209
Ratha N, Karu K, Shaoyun C, Jain A (1996) A real-time matching system for large ﬁngerprint
databases. IEEE Trans Pattern Anal Mach Intell 18:799–813
Roweis S, Saul L (2000) Nonlinear dimensionality reduction by locally linear embedding. Science
290:2323–2326
Sanchez-Reillo R, Sanchez-Avila C, Gonzalez-Marcos A (2000) Biometric identiﬁcation through
hand geometry measurements. IEEE Trans Pattern Anal Mach Intell 22:1168–1171
Sandstr€om M (2004) Liveness detection in ﬁngerprint recognition systems. Department of Elec-
trical Engineering, Link€oping University
Tistarelli M, Bicego M, Grosso E (2009) Dynamic face recognition: from human to machine
vision. Image Vis Comput 27(3):222–232
Toth B (2005) Biometrics liveness detection. Inf Secur Bull 10:291–297
Turk M, Pentland A (1991) Eigenfaces for recognition. J Cogn Neurosci 3:71–86
Wan V, Renals S (2005) Speaker veriﬁcation using sequence discriminant support vector
machines. IEEE Trans Speech Audio Process 13:203–210
Wang L, Tan T, Ning H, Hu W (2003) Silhouette analysis-based gait recognition for human
identiﬁcation. IEEE Trans Pattern Anal Mach Intell 25:1505–1518
Wenxin L, David Z, Zhuoqun X (2002) Palmprint identiﬁcation by Fourier transform. Int J Pattern
Recogn Artif Intell 16:417–432
Zhang D (2000) Automated biometrics – technologies and systems. Kluwer Academic, Boston
Zhang D, Wai-Kin K, You J, Wong M (2000) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25:1041–1050
308
13
Dynamic Tongueprint Recognition
www.ebook3000.com

Chapter 14
Online 3D Ear Recognition
Abstract The three-dimensional shape of the ear has been proven to be a stable
candidate for biometric authentication because of its desirable properties such as
universality, uniqueness, and permanence. In this chapter, a special laser scanner
designed for online three-dimensional ear acquisition was described. Based on the
dataset collected by our scanner, two novel feature classes were deﬁned from a
three-dimensional ear image: the global feature class (empty centers and angles)
and local feature class (points, lines, and areas). These features are extracted and
combined in an optimal way for three-dimensional ear recognition. Using a large
dataset consisting of 2000 samples, the experimental results illustrate the effective-
ness of fusing global and local features, obtaining an equal error rate of 2.2%.
Keywords 3D ear recognition • Feature extraction • Global and local feature
classes
14.1
Introduction
Biometric authentication is of great importance for applications in public security
(Pang et al. 2011; Zhang 2000; Jain et al. 1999). Nowadays, several novel bio-
metrics, including palmprints (Zhang et al. 2003), veins (Zhang et al. 2007), and
ears (Abaza et al. 2013; Abaza and Ross 2010; Hurley et al. 2005), have been
developed to meet the needs of different security requirements.
With advances in three-dimensional (3D) imaging technology, 3D biometric
authentication has drawn increasing attention from researchers. Examples include
3D face (Kakadiaris et al. 2007; Samir et al. 2006), palmprint (Zhang et al. 2009,
2010; Li et al. 2012), and ear recognition (Yan and Bowyer 2007; Chen and Bhanu
2007, 2009; Islam et al. 2011, 2012; Zhou et al. 2012). A 3D ear image is robust to
imaging conditions, and contains surface shape information that is related to
anatomical structure. In addition, it is insensitive to environmental illuminations.
Yan (Yan and Bowyer 2007) utilized both color and depth images to determine the
ear pit for automated 3D ear segmentation. Furthermore, they proposed an
improved Iterative Closest Point (ICP) algorithm for 3D ear point cloud matching.
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_14
309

Chen (Chen and Bhanu 2007) gave a 3D ear recognition method founded on a Local
Surface Patch (LSP) and ICP algorithm. Moreover, they proposed an indexing
approach (Chen and Bhanu 2009) that combines feature embedding and a support
vector machine-based learning technique for ranking their hypotheses. Islam et al.
presented a local 3D features extraction method based on the key point detection
(Islam et al. 2011, 2012). Zhou et al. presented a 3D ear recognition system
combining local and holistic features (Zhou et al. 2012). Zhang et al. introduced a
sparse representation framework into the ﬁeld of 3D ear identiﬁcation (Zhang et al.
2014). Chen and Mu proposed a hybrid multi-keypoint descriptor sparse
representation-based classiﬁcation (MKD-SRC) method to solve one sample per
person problem in ear recognition (Liu et al. 2015).
Even though good results were achieved in these studies, there is no overall
system for online 3D ear recognition. First, most of the current methods use
commercial laser scanners to acquire the 3D range image, for example, the widely
used Minolta VIVID Series (Zhang et al. 2014; Chen et al. 2015). Although these
scanners are general-purpose and high-performance, they are expensive and cum-
bersome. Second, previous 3D ear recognition methods focused on a single aspect,
that is, mostly local features, while global features such as the ear-parotic area
angle, and the ear hole shape have not been discussed or used. Given these
considerations, a laser scanner speciﬁcally designed for 3D ear acquisition and
recognition was ﬁrst developed using the laser-triangulation principle. The scanner
provides 2D intensity images and 3D point-cloud data for subsequent recognition,
and the total scanning and transmission time is less than 2 s. Based on the 3D ear
images collected by our laser scanning device, two feature classes consisting of ﬁve
features were deﬁned. The empty center shape and the angle feature represent the
depth and orientation of a 3D ear, and are treated as global features. The point, line,
and area features describe key points, shapes, and the local area of the 3D ears. They
are treated as local features. By combining these global features with local features,
a hierarchical structure was introduced for 3D ear recognition. The 3D ears are
pre-classiﬁed using global features and then recognized using local features. Thus,
much time can be saved and accuracy can be improved in 3D ear recognition.
Therefore, the 3D ear recognition system achieves both a high efﬁciency and
accuracy.
The purpose of this study was to create a 3D ear recognition system using
equipment that is practical for real applications. The main contents of this
Chapter can be summarized as follows. Firstly, the global and local features
categories in 3D ear are proposed. Secondly, multi-forms of features in 3D ears
have been deﬁned and extracted. Thirdly, multi-features fusion and hierarchical
recognition of 3D ears have been discussed. Finally, a complete solution for 3D ear
authentication has been achieved. The results on the collected 3D ear data show that
the system is efﬁcient and accurate.
310
14
Online 3D Ear Recognition
www.ebook3000.com

14.2
Scanner Design for Online 3D Ear Acquisition
The 3D ear scanner we developed is based on the laser trangulation principle (Liu
et al. 2015). Figure 14.1 illustrates the imaging principle of laser triangulation. In
the reference X-Y-Z coordinates, the 3D coordinates (x, y, z) can be calculated
according to Eq. (14.1).
~p ¼
x
y
z
0
@
1
A ¼
x0 b  d tan θ
ð
Þ
x0 þ f tan θ
y0 b  d tan θ
ð
Þ
x0 þ f tan θ
f 0 b  d tan θ
ð
Þ
x0 þ f tan θ
0
B
B
B
B
B
B
@
1
C
C
C
C
C
C
A
ð14:1Þ
Figure 14.2 illustrates the framework of the 3D ear recognition system. The
system consists of two main parts: hardware and software. To meet the require-
ments of online recognition, the hardware and software should be optimized for
speed and accuracy. At the same time, its portability and cost for real applications
should be considered. The laser scanner developed for 3D ear acquisition is shown
in Fig. 14.3a. Figure 14.3b shows two groups of typical 3D ear samples captured by
our device, where each row is the 3D point cloud from one ear viewed at different
angles.
Table 14.1 provide a performance comparison of our proposed device with the
Minolta Vivid 910 range scanner that is a widely used commercial scanner and has
been used to acquire 3D ear data for UND data set. The acquisition time refers to the
total scanning and transmission time, accuracy refers to the depth precision of the
measurement, dimensions refer to the width, height and length of the scanner, in
addition, the weight and price are also listed. Although the measurement accuracy
of our acquisition system is inferior to that of Vivid 910, it has a higher speed,
Fig. 14.1 Imaging
principle of laser-
triangulation imaging
14.2
Scanner Design for Online 3D Ear Acquisition
311

smaller size, and much lower cost. Moreover, the device could provide original
frames of laser lines that describe the fundamental structure of 3D features. All
these traits make the specially designed device suitable for 3D ear acquisition in
practical biometrics applications.
Fig. 14.2 Framework of the 3D ear recognition system
Fig. 14.3 Proposed 3D ear acquisition system: (a) 3D ear acquisition device and (b) 3D ear
samples viewed at different angles (each row is collected from a single ear)
Table 14.1 Comparison of the scanning device
Acquisition time
(s)
Accuracy
(mm)
Dimensions
(mm)
Weight
(kg)
Price
(USD)
Vivid 910
4
0.1
213  413  271
11
>20,000
Our
scanner
2
0.5
140  200  200
3
<1000
312
14
Online 3D Ear Recognition
www.ebook3000.com

A 3D ear database was established using the developed 3D ear acquisition
device by collecting 3D ears on two separate occasions separated by an interval
of around 1 month. On each occasion, the subject was asked to provide two
samples. The database contains 2000 samples from 500 volunteers consisting of
341 males and 159 females. The volunteers were students and staff of the Shenzhen
Graduate School of Harbin Institute of Technology. The written consents were
obtained from the participants prior to the study. The study was approved by the
Academic Committee of the Department of Computing of Harbin Institute of
Technology, Shenzhen Graduate School, which ensures that research programs
are consistent with academic ethics. The 3D ear acquisition study was discussed
in a meeting of the committee, and written approval was subsequently granted by
the Department Head. Because our research work does not involve patients or
privacy, and all the participants have given written consent to the use of their ear
images for academic purposes, all the data and ﬁgures are fully available from the
ﬁgshare.com and Biometrics Research Center of The Hong Kong Polytechnic
University.
14.3
3D Ear Global and Local Feature Classes
Prior to feature extraction, the 3D ears were normalized using a projection density
method (Huang et al. 2009). After that, a 3D image of the ear is formed as a
normalized posture in uniﬁed X-Y-Z coordinates, where all features are extracted
from the 3D point cloud of the ear.
14.3.1
Global Feature Class
Two global features, empty center and angle, are deﬁned in the proposed system.
Empty Center Feature
In the normalized X-Y coordinates, the boundary points of the ear were ﬁrst
detected (Fig. 14.4a), then the connected areas were labeled (Fig. 14.4b). The
connected areas that are less than a threshold was removed then (Fig. 14.4c). Lastly,
the connected pixels inside the ear were selected as the empty center feature
(Fig. 14.4d).
The template matching technique is used to calculate the distance between two
empty center features. The distance is deﬁned as: D ¼ E1 ⨁E2/E1 [ E2, where E1
and E2 are the empty center features of different samples. To avoid displacement
interference, the test image was shifted by 40 pixels left-right and up-down,
where the minimum distance is taken to be the difference of the two empty center
areas (Fig. 14.5).
Figure 14.6 shows the empty center feature vectors extracted from Sample A,
Sample B, and Sample C. Sample A and Sample B are from the same ear, and
14.3
3D Ear Global and Local Feature Classes
313

Sample C is from a different ear. The distance between Sample A and Sample B is
0.23, and the distance between Sample B and Sample C is 0.56, which indicates that
the empty center feature vectors from the same ear are alike and those from
different ears are dissimilar.
Angle Feature
In Fig. 14.7, there is an angle between the ear and parotic area of a person (Liu et al.
2014). It can be assumed that there is a plane, Afx + Bfy + Cfz + Df ¼ 0, which
represents the 3D points on the parotic region (green circle shown in Fig. 14.8).
Fig. 14.4 Empty center feature extraction
Fig. 14.5 Matching empty center features
314
14
Online 3D Ear Recognition
www.ebook3000.com

And there is another plane, Aex + Bey + Cez + De ¼ 0, represents the 3D points on the
ear edge. Thus, the normal vector of the parotic plane can be obtained as nf ¼ (Af,
Bf, Cf), and the normal vector of the ear plane is ne ¼ (Ae, Be, Ce). The angle θ
between the parotic and ear planes can be deﬁned as follows:
θ1 ¼ arccos
< nf , ne >
nf


2 ne
k
k2
 
!
ð14:2Þ
Where <nf , ne> is the inner product of normal vectors nf and ne. The knfk2 and
knek2 are L2-norms of nf and ne respectively.
Hence,
θ ¼
θ1
if θ1< 90

180
  θ1
otherwise

ð14:3Þ
Fig. 14.6 Discriminating the same ear and different ears using the empty center feature vector
14.3
3D Ear Global and Local Feature Classes
315

14.3.2
Local Feature Class
Three categories of local features in the 3D ear image were deﬁned: point, line, and
area features.
Point Feature
The 3D ear model consists of a number of points in 3D coordinates. Therefore, if
the key points that are stable for the same ear and distinguishable for different ears
could be found, then the 3D ear models would be recognized using these key points.
The aim of key-point detection is to select points on the 3D ear surface that can
be identiﬁed with high repeatability in different models of the same surface. Islam
and Mian proposed a key-point detection and feature extraction method that is
effective on 3D ears (Islam et al. 2011) and faces (Mian et al. 2008). Although the
Fig. 14.7 Angle feature extraction
Fig. 14.8 Detected key-points: The left two samples are from one ear and the other two samples
are from a different ear. The detected key-points are marked by red and blue respectively (Color
ﬁgure online)
316
14
Online 3D Ear Recognition
www.ebook3000.com

core of our key-point detection technique is similar to theirs, the technique is
modiﬁed to make it suitable for the 3D ears data captured by our proposed device.
In addition, the point feature is deﬁned differently.
The input to the algorithm is a point cloud of the ear E ¼ {P1, . . . , Pn}. For each
point P(xi yi zi)T, where i ¼ 1 , . . . , n, a local surface is cropped from the point cloud
using
a
sphere
of
radius
r
centered
at
P
and
recorded
as
SetL ¼
xj yj zj
h
iT
j
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
xj  xi

2 þ
yj  yi

2
þ zj  zi

2
r
< r
(
)
. The principle
component analysis is then applied on the data points SetL. The difference between
the eigenvalues along the ﬁrst two principal axes of the local surface is computed as
d. The value of d indicates the extent of asymmetry around the center point P, which
is zero if SetL is planar or spherical. It is then compared to a threshold t, and if d > t,
the point P(xi yi zi)T is selected as a key-point. At the same time, the angular
separation φ between the third principal axes and the original uniﬁed Z coordinate
was calculated. Let Km ¼ [xm ym zm dm φm]T (where m ¼ 1 , . . . , nk) record the
key-point information. Set Km is used at a later stage of feature extraction. Param-
eters r and t are empirically chosen as r ¼ 5 mm and t ¼ 2 mm.
Figure 14.8 shows examples of key-points detected on four different point
clouds scanned from two individuals. It illustrates that key-points are stable in the
ear data of the same individual, and distinguishable for the ear data of different
individuals.
After key-point detection, features are extracted from set Km (as shown in
Fig. 14.9).
First, the normalized ear was divided into 12 average fan-shaped parts, where
each sector is further divided into four equidistant parts. Each part is marked as Fl,
where l ¼ 1 , . . . , 48. Thus, the x-y-z values of key-point set Km fall within these
48 parts.
Second, for each Fl, the statistical histograms of d and φ were calculated. The
histogram bins of d are set to 2, 3, 4, 5, 6, and 7, and the bins of φ are set to 0, 1,
2, and 3. Next, the number in each bin was counted to obtain a 10-dimensional
vector. If there is no key-point in Fl, the vector was set to [0,0,0,0,0,0,0,0,0,0].
Finally, all 48 vectors were connected to obtain a 480-dimensional vector Vp as
the ﬁnal point feature vector. The difference between two ears is calculated using
the Euclidean distance between their Vp vectors.
Figure 14.10 shows the point feature vectors extracted from different samples.
Sample 1 (S1) and 2 (S2) are from the same ear, and Sample 3 (S3) is from a
different ear. The red curve is the point feature vector of S1, the blue curve is the
point feature vector of S2, and the black curve is the point feature vector of S3. The
distance between S1 and S2 is 33.7, and the distance between S1 and S3 is 127.4. It
can be seen that the point feature vectors from the same ear are very similar, and
those from different ears are dissimilar.
Line Feature
To calculate the line feature, a rectangle was ﬁtted on the normalized ear in the X-Y
coordinates, deﬁne (M + N) lines, V1, . . ., Vm (which divides the rectangle equally in
14.3
3D Ear Global and Local Feature Classes
317

the horizontal direction), and H1, . . ., Hn (which divides the rectangle equally in the
vertical direction), as shown in Fig. 14.11. Next, the 3D points on each line were
obtained and their z values were recorded. Each line was then divided equally and
the z crossing point values were marked as z1, z2, . . ., z10 (or z1, z2, . . ., z20 for V1,
. . ., Vm). These z values were used to form the line feature vector L (V1, . . ., Vm, H1,
. . ., Hn), where the vector is of length (20  m + 10  n).
Figure 14.12 shows the line feature vectors extracted from the same samples as
those in Fig. 14.11. Parameters m ¼ 2 and n ¼ 3 were used in the experiment to test
the discrimination of the line feature. The distance between S1 and S2 using the line
feature is 7.02, and the distance between S1 and S3 is 41.12. It can be seen that the
line feature vectors from the same ear are very close, and the line feature vectors
from different ears are further apart.
Area Feature
In order to compute the area feature, the 3D ear was ﬁtted into a ﬁxed block and
divided into m  n equal areas (see Fig. 14.13). All coordinate points in the area are
deﬁned as (xi, yi, zi) i ¼ 1 , . . . , N, where N is the number of the points in the area.
All the coordinates of these points constitute an N  3 matrix W as follows:
Fig. 14.9 Extraction of the point feature vector
318
14
Online 3D Ear Recognition
www.ebook3000.com

90
80
70
60
50
40
30
20
10
0
90
80
70
60
50
40
30
20
10
0
0
50
100 150 200
Same Ear
Sample 1
Sample 2
Sample 3
Different Ears
250 300
350
400
450
500
| Vp1 – Vp2 | = 33.7
| Vp2 – Vp3 | = 127.4
0
50
100
150
200
250
300
350
400 450
500
Fig. 14.10 Discriminating between the same and different ears using the point feature vector
Fig. 14.11 Extraction of the line feature vector
14.3
3D Ear Global and Local Feature Classes
319

W ¼
x1
y1
z1
. . .
xN
yN
zN
2
4
3
5
ð14:4Þ
Principle component analysis (Pang et al. 2010a, b) is performed on W and the
resulting normal vector is represented as VN(i, j, k).
The average is calculated using
x; y; z
ð
Þ ¼ 1
N
X
N
i¼1
xi; yi; zi
ð
Þ
ð14:5Þ
The scatter matrix is given as S ¼ P
N
i¼1
Wi  
W i
ð
Þ  Wi  
W i
ð
ÞT, the eigenvec-
tors of S are Φ, and the ﬁrst column of Φ is the normal vector VN(i, j, k). It is clear
that VN(i, j, k) can be thought of as the direction of matrix W. In addition, the center
of gravity of W can be represented as VC x; y; z
ð
Þ. As a result, the normal vector VN,
center of gravity VC, and min/max z values VZ are calculated and joined to form a
15
10
5
0
–5
–10
15
20
10
5
5
0
10
15
0
10
20
30
40
Same Ear
Sample 1
Sample 2
Sample 3
Different Ears
50
60
170
| L1 – L2 | = 7.02
| L2 – L3 | = 41.12
0
10
20
30
40
50
60
70
Fig. 14.12 Discriminating between the same and different ears using the line feature vector
320
14
Online 3D Ear Recognition
www.ebook3000.com

vector AN for each area. The area feature subsequently becomes the vector
consisting of all m  n vectors A, (A11, A12, . . ., Amn). Figure 14.14 shows the
area feature vectors extracted from S1, S2, and S3. The distance between S1 and S2
is 6.89, and the distance between S2 and S3 is 27.78, which indicates that the area
feature vectors from the same ear are alike and those from different ears are not
alike.
14.4
Experimental Results and Discussion
The experiments were divided into two parts: feature optimization and veriﬁcation
experiments. As mentioned above, our database contains a total of 2000 different
samples from 500 individual ears. A PC with Intel Core 2 CPU @2.33 GHz and
2 GB memory was used in our experiments.
14.4.1
Feature Optimization
Because the parameters used in the deﬁnition of each local feature may inﬂuence
the length of the feature vector as well as the equal error rate (EER) of the
veriﬁcation experiments, the feature optimization experiments were performed to
determine the most effective values for these parameters.
In our point feature, the number and distribution of the key-points determines the
point feature vector. Hence, threshold t is the parameter that needs to be optimized.
Figure 14.15 shows the different key-points extracted using different thresholds,
while Table 14.2 shows the EER for different thresholds. Considering the time
consumed, the feature optimization experiments were performed on a sub-dataset
Fig. 14.13 Extraction of the area feature vector
14.4
Experimental Results and Discussion
321

that contains 100 different sample ears. From Table 14.2, it can be seen that the best
result is achieved when t ¼ 2.
The line feature vector is determined by the number of horizontal and vertical
lines. Therefore, the line number is the parameter that needs to be optimized here.
Figure 14.16 shows the different lines across the ear. Table 14.3 shows the EER
obtained using different line numbers, where 12 lines obtains the lowest EER.
Because the number of blocks determines the area feature vector, this parameter
is the one that must be optimized. Figure 14.17 shows the different blocks on the ear
and Table 14.4 shows the EER obtained using different block numbers. It can be
seen that the best result is achieved when there are 48 blocks.
14.4.2
Matching Using Local Features
The matching experiments were carried on all 2000 samples, and performed using
the local feature class (point, line, and area features) as well as their feature-level
30
20
10
0
–10
–20
–30
30
20
10
0
–10
–20
–30
0
10
20
30
40
Same Ear
Sample 1
Sample 2
Sample 3
Different Ears
50
60
70
80
90
100
| A1 – A2 | = 6.89
| A2 – A3 | = 27.78
0
10
20
30
40
50
60
70
80
90
100
Fig. 14.14 Discriminating between the same and different ears using the area feature vector
322
14
Online 3D Ear Recognition
www.ebook3000.com

Fig. 14.15 Point feature optimization
Table 14.2 Point features
with different t parameters
t
0
1
2
2.5
2.75
3
No. Points
2009
1320
462
207
136
82
EER (%)
2.5
2.2
2.2
5.3
7.8
17.2
Fig. 14.16 Line feature optimization
14.4
Experimental Results and Discussion
323

fusion. Since all the local features (point, line, and area features) are deﬁned in form
of vectors (VP, VL, VA), the most direct strategy for feature-level fusion is to joint
different vectors into one fusion feature vector. Therefore, the fusion feature
vectors can be described as follows:
VPþL ¼ normalization joint normalization VP
ð
Þ; normalization VL
ð
Þ
ð
Þ
ð
Þ
VPþA ¼ normalization joint normalization VP
ð
Þ; normalization VA
ð
Þ
ð
Þ
ð
Þ
VLþA ¼ normalization joint normalization VL
ð
Þ; normalization VA
ð
Þ
ð
Þ
ð
Þ
Vlocal ¼ normalization joint normalization VPþL
ð
Þ; normalization VA
ð
Þ
ð
Þ
ð
Þ
8
>
>
<
>
>
:
ð14:6Þ
The function normalization normalizes the feature vector into unit vector. The
function joint combines two feature vectors into one fusion feature vector.
Table 14.5 shows the EER results of different local features and their combinations.
It can be seen that the optimal result is achieved when all local features are fused
together.
14.4.3
Recognition with Global Feature Indexing
Different from the weighted fusion method, the global and local features fusion is
implemented in a hierarchical procedure. The 3D ears are pre-classiﬁed using
global features and then recognized using local features. Thus, much time can be
Table 14.3 Line features for
different line numbers
Line No.
5
12
26
Vector length
70
170
370
EER (%)
3.6
3.0
3.2
Fig. 14.17 Area feature optimization
324
14
Online 3D Ear Recognition
www.ebook3000.com

saved and accuracy can be improved in 3D ear recognition. The ﬂowchart of the
overall recognition with global feature indexing is shown in Fig. 14.18a. For a given
ear sample, the procedure is as follows:
1. Extract the global features of the test sample Angle (Gt), Center (Gt).
2. Compare Angle (Gt) with global features Angle (Gi) i ¼ 1, . . ., N of all ear
models (in our experiments, N ¼ 500) to obtain the matching distance Dist
[Angle (Gt), Angle (Gi)].
3. If Dist [Angle (Gt), Angle(Gi)] is smaller than threshold T(β), the ear model is
treated as a matched candidate and place it into a sub-database.
4. Match test ear Gt with the sub-database ears using the empty center feature and
adjust the candidate sub-database Gi accordingly.
5. Extract the local features of VLocal_t and the local features of the ear models in
the candidate sub-database VLocal_i (i ¼ 1,. . .,k), where k is the total number of
ears it contains.
6. Match local features between VLocal_t and VLocal_i to measure the differences
between the test ear and candidate ears (in our experiments, the Euclidean
distance was used).
7. The candidate ear that is closest to the test ear is the recognition result.
Figure 14.18b shows the receiver operating characteristic curve of the results
obtained by combining both global and local features together, where the EER is
2.2%. It can be seen that the fusion of global and local features achieves the smallest
EER of all schemes, and is even better than single feature matching. This is
reasonable, because more information usually leads to more accurate recognition.
14.4.4
Performance Analysis
To better measure the performance of the proposed method, six criteria (database,
acquisition device, feature extraction method, average matching time, EER, and
online properties) were used to compare the proposed method with other 3D ear
recognition methods. The results are shown in Table 14.6.
Table 14.4 Area features
with different block numbers
No. Blocks
12
48
192
Vector length
108
432
1728
EER (%)
5.0
4.3
4.3
Table 14.5 Matching results for different local features
Features
Point
Line
Area
Point + Line
Point + Area
Line + Area
All features
EER (%)
4.7
4.2
5.1
3.3
4.5
3.6
2.8
14.4
Experimental Results and Discussion
325

From Tables 14.1 and 14.6, it can be seen that our 3D ear scanner has a lower
price (approximately 5% that of the Vivid 910), and a smaller size (approximately
25% that of the vivid 910). Meanwhile, the overall recognition time (including
acquisition and recognition time) is less than 2.5 s, and the EER on a database with
2000 samples is 2.2%. So far, our 3D ear recognition system is the only system
offering an overall solution for both 3D ear data acquisition and optimized recog-
nition. Its performance is sufﬁcient to meet the online system requirements for a
real-time application.
14.5
Summary
In this chapter, two novel feature classes, global and local features, were deﬁned
and extracted from 3D ear point clouds. The global feature class includes the empty
center and ear-parotic area angle, whereas the local feature class consists of point,
line, and area features. The experimental results show that all features are stable for
the same ear and distinguishable between different ears. Furthermore, global
features can be used for indexing, while the combination of both global and local
features produces matching results with an EER of 2.2% on our 3D ear database of
2000 samples. Using our own developed scanner and the optimized recognition
method, a real-time 3D ear recognition system is achieved.
Fig. 14.18 Fusion of global
and local features: (a)
ﬂowchart of recognition
with global feature indexing
and (b) receiver operating
characteristic curve of the
global and local feature
fusion
326
14
Online 3D Ear Recognition
www.ebook3000.com

References
Abaza A, Ross A (2010) Towards understanding the symmetry of human ears: a biometric
perspective. In: Fourth IEEE international conference on biometrics: theory applications and
systems (BTAS), pp. 1–7
Abaza A, Ross A, Hebert C, Harrison MAF, Nixon MS (2013) A survey on ear biometrics. ACM
Comput Surv (CSUR) 45(2):22
Burge M, Burger W (2000) Ear biometrics in computer vision. In: 15th International conference on
pattern recognition, pp 822–826
Chen H, Bhanu B (2007) Human ear recognition in 3D. IEEE Trans Pattern Anal Mach Intell 29
(4):718–737
Chen H, Bhanu B (2009) Efﬁcient recognition of highly similar 3D objects in range images. IEEE
Trans Pattern Anal Mach Intell 31(1):172–179
Chen L, Mu Z, Zhang B, Zhang Y (2015) Ear recognition from one sample per person. PLoS One
10(5):e0129505
Choras´ M (2005) Ear biometrics based on geometrical feature extraction. Electro Lett Comput Vis
Image Anal 5(3):84–95
Huang C, Lu G, Liu Y (2009) Coordinate direction normalization using point cloud projection
density for 3D ear. In: Fourth international conference on computer sciences and convergence
information technology, pp 511–515
Hurley DJ, Nixon MS, Carter JN (2005) Force ﬁeld feature extraction for ear biometrics. Comput
Vis Image Underst 98(3):491–512
Iannarelli AV (1989) Ear identiﬁcation. Paramont Publishing, Freemont, CA
Islam SM, Davies R, Bennamoun M, Mian AS (2011) Efﬁcient detection and recognition of 3D
ears. Int J Comput Vis 95(1):52–73
Islam S, Bennamoun M, Owens RA, Davies R (2012) A review of recent advances in 3D ear-and
expression-invariant face biometrics. ACM Comput Surv (CSUR) 44(3):14
Jain A, Bolle R, Pankanti S (1999) Biometrics: personal identiﬁcation in networked society.
Springer Science & Business Media, New York
Kakadiaris IA, Passalis G, Toderici G, Murtuza MN, Lu Y, Karampatziakis N et al (2007) Three-
dimensional face recognition in the presence of facial expressions: an annotated deformable
model approach. IEEE Trans Pattern Anal Mach Intell 29(4):640–649
Li W, Zhang D, Lu G, Luo N (2012) A novel 3-D palmprint acquisition system. IEEE Trans Syst
Man Cybernet A Syst Hum 42(2):443–452
Liu Y, Zhang B, Zhang D (2014) Ear-parotic face angle: a unique feature for 3D ear recognition.
Pattern Recogn Lett 53:9–15
Liu Y, Lu G, Zhang D (2015) An effective 3D ear acquisition system. PLoS One 10(6):e0129439
Table 14.6 Comparison with existing 3D ear recognition methods
Reference
Dataset size
(ID/subjects/
images)
Device
used
Method
applied
Matching
time (s)
Reported
EER
Online
Yan and
Bowyer
UND/415/1386
Vivid 910
ICP
1.5
1.2%
N/A
Chen and
Bhanu
UND-F/302/942
UCR/155/902
Vivid910
Vivid300
LSP
3.7
2.3%–
4.2%
N/A
Islam
et al.
UND-F/302/942
UND-J/415/830
Vivid 910
L3DF + ICP
0.06
2.3%–
4.1%
N/A
Proposed
method
HIT/500/2000
Our
scanner
Global + Local
0.5
2.2%
YES
References
327

Mian AS, Bennamoun M, Owens R (2008) Keypoint detection and local feature matching for
textured 3D face recognition. Int J Comput Vis 79(1):1–12
Pang Y, Wang L, Yuan Y (2010a) Generalized KPCA by adaptive rules in feature space. Int J
Comput Math 87(5):956–968
Pang Y, Li X, Yuan Y (2010b) Robust tensor analysis with 1-norm. IEEE Trans Circuits Syst
Video Technol 20(2):172–178
Pang Y, Yuan Y, Li X, Pan J (2011) Efﬁcient HOG human detection. Signal Process 91
(4):773–781
Purkait R, Singh P (2008) A test of individuality of human external ear pattern: its application in
the ﬁeld of personal identiﬁcation. Forensic Sci Int 178(2):112–118
Samir C, Srivastava A, Daoudi M (2006) Three-dimensional face recognition using shapes of
facial curves. IEEE Trans Pattern Anal Mach Intell 28(11):1858–1863
Yan P, Bowyer KW (2007) Biometric recognition using 3D ear shape. IEEE Trans Pattern Anal
Mach Intell 29(8):1297–1308
Zhang D (2000) Automated biometrics: technologies and systems. Springer Science & Business
Media, Berlin
Zhang D, Kong W-K, You J, Wong M (2003) Online palmprint identiﬁcation. IEEE Trans Pattern
Anal Mach Intell 25(9):1041–1050
Zhang YB, Li Q, You J, Bhattacharya P (2007) Palm vein extraction and matching for personal
authentication. In: International conference on advances in visual information systems.
Springer, Berlin, pp 154–164
Zhang D, Lu G, Li W, Zhang D, Luo N (2009) Palmprint recognition using 3-D information. IEEE
Trans Syst Man Cybernet C Appl Rev 39(5):505–519
Zhang D, Kanhangad V, Luo N, Kumar A (2010) Robust palmprint veriﬁcation using 2D and 3D
features. Pattern Recogn 43(1):358–368
Zhang L, Ding Z, Li H, Shen Y (2014) 3D ear identiﬁcation based on sparse representation. PLoS
One 9(4):e95506
Zhou J, Cadavid S, Abdel-Mottaleb M (2012) An efﬁcient 3-D ear recognition system employing
local and holistic features. IEEE Trans Inf Forensics Secur 7(3):978–991
328
14
Online 3D Ear Recognition
www.ebook3000.com

Chapter 15
Book Review and Future Work
Abstract Traditional biometric technologies, such as ﬁngerprint, face, iris, and
palmprint, have been well studied and introduced in many research books. How-
ever, these technologies have their own advantages and disadvantages, and there is
not one kind of biometric technology can be ﬁt for different applications. Many new
biometric technologies have been developed in recent years, especially for some
new applications. This book describes some new biometric technologies, such as
High resolution ﬁngerprint, Finger-Knuckle-Print, Multi-Spectral Backhand, 3D
ﬁngerprint, Tongueprint, and 3D ear. Many efﬁcient feature extraction, matching
and fusion algorithms are introduced and some potential systems have been devel-
oped in this book. It may serve as a handbook of biometrics authentication and be of
use to researchers and students who wish to understand, participate, and/or develop
a biometrics authentication system. It would also be useful as a reference book for a
graduate course on biometrics.
Keywords Biometrics application • Spooﬁng attack
Traditional biometric technologies, such as ﬁngerprint, face, iris, and palmprint,
have been well studied and introduced in many research books. However, these
technologies have their own advantages and disadvantages, and there is not one
kind of biometric technology can be ﬁt for different applications. Many new
biometric technologies have been developed in recent years, especially for some
new applications.
This book describes some new biometric technologies, such as High resolution
ﬁngerprint, Finger-Knuckle-Print, Hand back skin texture, 3D ﬁngerprint,
Tongueprint, and 3D ear. Many efﬁcient feature extraction, matching and fusion
algorithms are introduced and some potential systems have been developed in this
book. It may serve as a handbook of biometrics authentication and be of use to
researchers and students who wish to understand, participate, and/or develop a
biometrics authentication system. It would also be useful as a reference book for a
graduate course on biometrics.
In this chapter, we ﬁrst recapitulate the contents of this book in Sect. 15.1. Then,
Sect. 15.2 discusses the future of biometrics research.
© Springer International Publishing AG 2018
D. Zhang et al., Advanced Biometrics, DOI 10.1007/978-3-319-61545-5_15
329

15.1
Book Recapitulation
This book includes 15 chapters, and the contents are represented into four main
parts: High Resolution Fingerprint Recognition, Finger-Knuckle-Print Veriﬁcation,
Other Hand-Based Biometrics, and Some New Head-Based Biometrics. These
different techniques have been implemented into different biometric systems,
respectively. The experimental results under different challenging conditions
have shown the superiority of these techniques.
Chapter 1 introduces recent developments in biometrics technologies, some key
concepts in biometrics, and the importance of developing new biometrics systems.
Chapter 2 proposes a new approach to aligning high resolution partial ﬁnger-
prints based on pores, a type of ﬁngerprint ﬁne ridge features, which are abundant
on even small ﬁngerprint areas. Comparing with representative minutia based and
orientation ﬁeld based methods, the experimental results show that the proposed
method can more accurately locate corresponding feature points, estimate the
alignment transformations, and hence signiﬁcantly improve the accuracy of high
resolution partial ﬁngerprint recognition.
Chapter 3 presents a dynamic anisotropic pore model to describe pores more
accurately by using orientation and scale parameters. The ﬁngerprint image is ﬁrst
partitioned into well-deﬁned, ill-posed, and background blocks. According to the
dominant ridge orientation and frequency on each foreground block, a local instan-
tiation of appropriate pore model is obtained. Finally, the pores are extracted by
ﬁltering the block with the adaptively generated pore model. Extensive experiments
are performed on the high resolution ﬁngerprint databases. The results demonstrate
that the proposed method can detect pores more accurately and robustly, and
consequently improve the ﬁngerprint recognition accuracy of pore-based AFRS.
Chapter 4 discusses the optimal resolution for an AFRS using the two most
representative ﬁngerprint features: minutiae and pores. We ﬁrst designed a multi-
resolution ﬁngerprint acquisition device to collect ﬁngerprint images at multiple
resolutions and captured ﬁngerprints at various resolutions but at a ﬁxed image size.
Then, we carried out a theoretical analysis to identify the minimum required
resolution for ﬁngerprint recognition using minutiae and pores.
Chapter 5 presents a new biometric authentication system using ﬁnger-knuckle-
print (FKP) imaging. A speciﬁc data acquisition device is constructed to capture the
FKP images, and then an efﬁcient FKP recognition algorithm is presented to
process the acquired data in real time. The local convex direction map of the FKP
image is extracted, based on which a local coordinate system is established to align
the images and a region of interest is cropped for feature extraction. For matching
two FKPs, a feature extraction scheme which combines orientation and magnitude
information extracted by Gabor ﬁltering is proposed.
Chapter 6 studies image local features induced by the phase congruency model,
which is supported by strong psychophysical and neurophysiological evidences, for
FKP recognition. In the computation of phase congruency, the local orientation and
the local phase can also be deﬁned and extracted from a local image patch. These
330
15
Book Review and Future Work
www.ebook3000.com

three local features are independent of each other and reﬂect different aspects of the
image local information. We compute efﬁciently the three local features under the
computation framework of phase congruency using a set of quadrature pair ﬁlters.
We then propose to integrate these three local features by score-level fusion to
improve the FKP recognition accuracy. Such kinds of local features can also be
naturally combined with Fourier transform coefﬁcients, which are global features.
Chapter 7 explores an effective FKP recognition scheme by extracting and
assembling local and global features of FKP images. Speciﬁcally, the orientation
information extracted by the Gabor ﬁlters is coded as the local feature. By increas-
ing the scale of Gabor ﬁlters to inﬁnite, actually we can get the Fourier transform of
the image, and hence the Fourier transform coefﬁcients of the image can be taken
as the global features. Such kinds of local and global features are naturally linked
via the framework of time–frequency analysis. The proposed scheme exploits both
local and global information for the FKP veriﬁcation, where global information is
also utilized to reﬁne the alignment of FKP images in matching. The ﬁnal matching
distance of two FKPs is a weighted average of local and global matching distances.
Chapter 8 proposes a method to deal with pose variation problem in FKP system
by reconstructing the query sample with a dictionary learned from the template
samples in the gallery set. The reconstructed FKP image can reduce much the
enlarged matching distance caused by ﬁnger pose variations; however, both the
intra-class and inter-class distances will be reduced. We then propose a score level
adaptive binary fusion rule to adaptively fuse the matching distances before and
after reconstruction, aiming to reduce the false rejections without increasing much
the false acceptances.
Chapter 9 introduces several popular features in 3D ﬁngerprint recognition, such
as scale invariant feature transformation (SIFT) feature, ridge feature and minutiae.
To extract these ﬁngerprint features accurately, an improved ﬁngerprint enhance-
ment method has been proposed by polishing orientation and ridge frequency maps
according to the characteristics of 2D touchless ﬁngerprint images. Therefore,
correspondences can be established by adopting hierarchical ﬁngerprint matching
approaches. Through an analysis of 440 3D point cloud ﬁnger data (220 ﬁngers,
2 pictures each) collected by a 3D scanning technique, i.e., the structured light
illumination (SLI) method, the ﬁnger shape model is estimated. It is found that the
binary quadratic function is more suitable for the ﬁnger shape model than the other
mixed model.
Chapter 10 studies the hand back skin texture (HBST) pattern recognition
problem with applications to personal identiﬁcation and gender classiﬁcation. A
specially designed system is developed to capture HBST images, and an HBST
image database was established, which consists of 1920 images from 80 persons
(160 hands). An efﬁcient text on learning based method is then presented to classify
the HBST patterns. First, textons are learned in the space of ﬁlter bank responses
from a set of training images using the l1 -minimization based sparse representation
(SR) technique. Then, under the SR framework, we represent the feature vector at
each pixel over the learned dictionary to construct a representation coefﬁcient
15.1
Book Recapitulation
331

histogram. Finally, the coefﬁcient histogram is used as skin texture feature for
classiﬁcation.
In Chap. 11, we propose a novel palmprint acquisition system based on the line-
scan image sensor. The proposed system consists of a customized and highly
integrated line-scan sensor, a self-adaptive synchronizing unit, and a ﬁeld-
programmable gate array controller with a cross-platform interface. The volume
of the proposed system is over 94% smaller than the existing palmprint systems,
and the veriﬁcation performance of the proposed system is comparable to the best
area camera-based systems.
Chapter 12 explores a novel hand-based biometric system, door knob hand
recognition system (DKHRS), is proposed. DKHRS has the identical appearance
of a conventional door knob, which is an optimum solution in both physiological
factors and psychological factors. In this system, a hand image is captured by door
knob imaging scheme, which is a tailored omnivision imaging structure and is
optimized for this predetermined door knob appearance. Then features are extracted
by local Gabor binary pattern histogram sequence method and classiﬁed by pro-
jective dictionary pair learning.
Chapter 13 presents a new number for the biometrics family, i.e. tongueprint,
which uses particularly interesting properties of the human tongue to base a
technology for noninvasive biometric assessment. The tongue is a unique organ
which can be stuck out of the mouth for inspection, whose appearance is amenable
to examination with the aid of a machine vision system. Yet it is otherwise well
protected in the mouth and difﬁcult to be forged. Furthermore, the involuntary
squirm of the tongue is not only a convincing proof that the subject is alive, but also
a feature for recognition. That is to say, the tongue can present both static features
and dynamic features for authentication. However, little work has hitherto been
done on the tongue as a biometric identiﬁer. In this work, we make use of a database
of tongue images obtained over a long period to examine the performance of the
tongueprint as a biometric identiﬁer. Our research shows that tongueprint is a
promising candidate for biometric identiﬁcation and worthy of further research.
In Chap. 14, in order to expand the use of palmprint biometrics, we propose a
novel palmprint acquisition system based on the line-scan image sensor. The
proposed system consists of a customized and highly integrated line-scan sensor,
a self-adaptive synchronizing unit, and a ﬁeld-programmable gate array controller
with a cross-platform interface. The volume of the proposed system is over 94%
smaller than the existing palmprint systems, and the veriﬁcation performance of the
proposed system is comparable to the best area camera-based systems.
15.2
Future Work
Biometrics systems have been explored and developed in last decades and also
made great achievements. Large numbers of manufacturing and engineering enter-
prises, even education industry, government and military departments, have
332
15
Book Review and Future Work
www.ebook3000.com

implemented biometrics systems as a form of identiﬁcation and access control for
the purpose of security. The world biometric techniques and applications are
undergoing revolutionarily changing, and we expect a great future for biometrical
systems. Our vision for the future of biometrics systems and applications are as
follows:
1. Biometrics systems can completely replace the conventional identiﬁcation
systems. The conventional identiﬁcation systems, knowledge-based and token-
based authentication, nowadays are still the mainstream for personal identiﬁca-
tion. When it comes to their drawbacks and ﬂaws, biometrics can overtake these
traditional systems by its overwhelming beneﬁcial properties. Biometrics sys-
tems can secure your text messages, phonebook contacts and electronic images
on your devices. Since data thieves can be implemented in many ways by
obtaining conventional identiﬁcation patterns, passwords or number codes,
these patterns can only protect a device to a certain degree. However, biometrics
systems can prevent these frauds from occurring in the ﬁrst place. Thus the
ﬂourish of biometrics systems is unpredictable and inevitable. Meanwhile,
signiﬁcant efforts are still required.
2. Multi-biometrics systems can be widely used in identiﬁcation systems. Hybrid
multi-modal biometrics systems can provide more accurate identiﬁcation results.
Multi-biometrics systems can simultaneously capture and utilize more than one
biologic attributes or characteristics, such as both ﬁngerprint and ﬁnger vein
images with the single touch of a ﬁnger for more precise identiﬁcation. The other
application example is to combine ﬁngerprint scanners and voice recognition as
forms of biometric security. It will become popular multi-biometrics techniques
that biometrical hardware systems incorporate state-of-the-art technologies inte-
grating more than one separate sensor into a single unit. However, fundamental
research is still required in this area.
3. Low cost, efﬁcient and effective identiﬁcation performance. Practical identiﬁ-
cation systems should satisfy the requirements which are lightweight, ﬂexible,
secure, efﬁcient, accurate and durable hardware requiring low power require-
ments. Researchers are trying to ﬁnd the way that how to effectively and
efﬁciently represent and recognition biometrics patterns. Future biometrics
systems can quickly and accurately capture biology characteristics regardless
of the conditions or hash physical environment. Moreover, more accurate bio-
metrics systems can be developed for serving for end users, such as recognizing
a person with 99.999% accuracy. It is a tendency that consumers and businesses
will progress more comfortable with using them as well. More research efforts
are needed on how to tactfully design biometrics systems.
4. The thriving use biometrics for healthcare systems. Multi-biometric patient
safety systems can be developed for patient safety. There is a rise in the use of
iris biometrics for patient identiﬁcation in healthcare. This application is to
identify the patients by their biometrics characteristics, and then medical insti-
tutions can promptly retrieve the patients’ medical records. To this end,
healthcare institutions can introduce this technology as a means to help prevent
15.2
Future Work
333

the creation of duplicate medical records and overlays, predigest the registration
process, eliminate medical identity theft and fraud at the point of service.
Moreover, it also can protect patient privacy and heighten patient safety. Thus,
the biometrics system can perform an irreplaceable role in healthcare service.
However, much more research efforts still should be paid in innovative mech-
anisms of biometric systems used in healthcare system.
5. Overcoming external limitations and inﬂuences in biometrics authentication.
Varieties of limitations are still unsolved when biometrics systems perform the
operations of identiﬁcation exploiting any single biometric feature or traits.
External negative inﬂuences on identiﬁcation accuracy are produced by these
limitations such as nonuniversality, noisy sensor data, lack of individuality,
intra-class variations and spooﬁng attacks. Some other factors also should be
pointed out for study in depth. For example, capabilities of ﬁngerprints identi-
ﬁcation systems should adjust to identify these images captured with moist or
dry ﬁngerprints in any type of weather conditions, even extreme in nature. Facial
identiﬁcation systems suffer the negative effects from the variations of poses,
facial expression, disguises, illumination conditions and environments. All these
limitations and inﬂuences can remarkably inﬂuence the identiﬁcation accuracy.
Biometric research and development institutions should pay much close atten-
tion to explore the approaches to overcome these limitations. Although abundant
competing algorithms and techniques have been proposed and applied in bio-
metrics recently, signiﬁcant efforts are still required.
6. High-Resolution and the 3D Biometrics applications. High-resolution biomet-
rics and 3D biometrics are two signiﬁcant and recently-developed biometric
techniques. Typical high-resolution biometric systems include high-resolution
ﬁnger identiﬁcation system and signature authentication system, and so on. A
high-resolution ﬁngerprint identiﬁcation system allows ﬁngerprints to be authen-
ticated at three different levels i.e. pattern level, minutia point level, as well as
the pore and ridge contour level. The algorithm is another key point of this kind
of system. The algorithm developed for the high-resolution biometrics should be
able to adequately exploit the much information provided by the system. The
high-resolution system cooperating with a ﬁne algorithm dedicated to the high-
resolution data is able to produce higher authentication accuracy than the
low-resolution system.
3D biometric techniques acquire 3D-image data of biometric traits and the
corresponding systems are therefore called 3D biometric systems. Examples of
3D biometrics include 3D face and 3D ﬁngerprint. While the high-resolution and
the 3D biometric systems promise higher accuracy, they also suffer from some
problems. First, they usually involve a high device cost. Second, since the
system retains a large quantity of information from the subject, it is necessary
for the system to have a large memory and a high computation performance.
This will continue, particularly with more practical techniques and applications.
7. Exploring new areas of biometric security. Innovation and creativity are the
impetuses of evolution of human civilization. We have already witnessed a
remarkable growth in various biometrics systems and applications over the
334
15
Book Review and Future Work
www.ebook3000.com

past years. Recently, thermal imaging technique, electrocardiogram (or called
heart rhythm) based technique and electroencephalogram signals from imagined
activities are exploited as biometric characteristics for personal identiﬁcation.
The signiﬁcance of an uptick in biometrics applications also reﬂects the require-
ment for more essential types of biometric characteristics. Biometrics researcher
should try their best to ﬁnd other more fraud resistant biological characteristics
and traits for more robust, effective and efﬁcient identiﬁcation systems.
15.2
Future Work
335


www.ebook3000.com

Die-stacking Architecture

Synthesis Lectures on
Computer Architecture
Editor
Margaret Martonosi, Princeton University
Synthesis Lectures on Computer Architecture publishes 50- to 100-page publications on topics
pertaining to the science and art of designing, analyzing, selecting and interconnecting hardware
components to create computers that meet functional, performance and cost goals. ÓÄÄe scope will
largely follow the purview of premier computer architecture conferences, such as ISCA, HPCA,
MICRO, and ASPLOS.
Die-stacking Architecture
Yuan Xie and Jishen Zhao
2015
Power-EÔ¨Écient Computer Architectures: Recent Advances
Magnus Sj√§lander, Margaret Martonosi, and Stefanos Kaxiras
2014
FPGA-Accelerated Simulation of Computer Systems
Hari Angepat, Derek Chiou, Eric S. Chung, and James C. Hoe
2014
A Primer on Hardware Prefetching
Babak FalsaÔ¨Åand ÓÄÄomas F. Wenisch
2014
On-Chip Photonic Interconnects: A Computer Architect‚Äôs Perspective
Christopher J. Nitta, Matthew K. Farrens, and Venkatesh Akella
2013
Optimization and Mathematical Modeling in Computer Architecture
Tony Nowatzki, Michael Ferris, Karthikeyan Sankaralingam, Cristian Estan, Nilay Vaish, and David
Wood
2013
Security Basics for Computer Architects
Ruby B. Lee
2013
www.ebook3000.com

iii
ÓÄÄe Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale
Machines, Second edition
Luiz Andr√© Barroso, Jimmy Clidaras, and Urs H√∂lzle
2013
Shared-Memory Synchronization
Michael L. Scott
2013
Resilient Architecture Design for Voltage Variation
Vijay Janapa Reddi and Meeta Sharma Gupta
2013
Multithreading Architecture
Mario Nemirovsky and Dean M. Tullsen
2013
Performance Analysis and Tuning for General Purpose Graphics Processing Units
(GPGPU)
Hyesoon Kim, Richard Vuduc, Sara Baghsorkhi, Jee Choi, and Wen-mei Hwu
2012
Automatic Parallelization: An Overview of Fundamental Compiler Techniques
Samuel P. MidkiÔ¨Ä
2012
Phase Change Memory: From Devices to Systems
Moinuddin K. Qureshi, Sudhanva Gurumurthi, and Bipin Rajendran
2011
Multi-Core Cache Hierarchies
Rajeev Balasubramonian, Norman P. Jouppi, and Naveen Muralimanohar
2011
A Primer on Memory Consistency and Cache Coherence
Daniel J. Sorin, Mark D. Hill, and David A. Wood
2011
Dynamic Binary ModiÔ¨Åcation: Tools, Techniques, and Applications
Kim Hazelwood
2011
Quantum Computing for Computer Architects, Second Edition
Tzvetan S. Metodi, Arvin I. Faruque, and Frederic T. Chong
2011

iv
High Performance Datacenter Networks: Architectures, Algorithms, and Opportunities
Dennis Abts and John Kim
2011
Processor Microarchitecture: An Implementation Perspective
Antonio Gonz√°lez, Fernando Latorre, and Grigorios Magklis
2010
Transactional Memory, 2nd edition
Tim Harris, James Larus, and Ravi Rajwar
2010
Computer Architecture Performance Evaluation Methods
Lieven Eeckhout
2010
Introduction to ReconÔ¨Ågurable Supercomputing
Marco Lanzagorta, Stephen Bique, and Robert Rosenberg
2009
On-Chip Networks
Natalie Enright Jerger and Li-Shiuan Peh
2009
ÓÄÄe Memory System: You Can‚Äôt Avoid It, You Can‚Äôt Ignore It, You Can‚Äôt Fake It
Bruce Jacob
2009
Fault Tolerant Computer Architecture
Daniel J. Sorin
2009
ÓÄÄe Datacenter as a Computer: An Introduction to the Design of Warehouse-Scale
Machines
Luiz Andr√© Barroso and Urs H√∂lzle
2009
Computer Architecture Techniques for Power-EÔ¨Éciency
Stefanos Kaxiras and Margaret Martonosi
2008
Chip Multiprocessor Architecture: Techniques to Improve ÓÄÄroughput and Latency
Kunle Olukotun, Lance Hammond, and James Laudon
2007
www.ebook3000.com

v
Transactional Memory
James R. Larus and Ravi Rajwar
2006
Quantum Computing for Computer Architects
Tzvetan S. Metodi and Frederic T. Chong
2006

Copyright ¬© 2015 by Morgan & Claypool
All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means‚Äîelectronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews, without the prior permission of the publisher.
Die-stacking Architecture
Yuan Xie and Jishen Zhao
www.morganclaypool.com
ISBN: 9781627057653
paperback
ISBN: 9781627057660
ebook
DOI 10.2200/S00644ED1V01Y201505CAC031
A Publication in the Morgan & Claypool Publishers series
SYNTHESIS LECTURES ON COMPUTER ARCHITECTURE
Lecture #31
Series Editor: Margaret Martonosi, Princeton University
Series ISSN
Print 1935-3235
Electronic 1935-3243
www.ebook3000.com

Die-stacking Architecture
Yuan Xie
University of California, Santa Barbara
Jishen Zhao
University of California, Santa Cruz
SYNTHESIS LECTURES ON COMPUTER ARCHITECTURE #31
C
M
&
cLaypool
Morgan
publishers
&

ABSTRACT
ÓÄÄe emerging three-dimensional (3D) chip architectures, with their intrinsic capability of reduc-
ing the wire length, promise attractive solutions to reduce the delay of interconnects in future
microprocessors. 3D memory stacking enables much higher memory bandwidth for future chip-
multiprocessor design, mitigating the ‚Äúmemory wall‚Äù problem. In addition, heterogenous integra-
tion enabled by 3D technology can also result in innovative designs for future microprocessors.
ÓÄÄis book Ô¨Årst provides a brief introduction to this emerging technology, and then presents a
variety of approaches to designing future 3D microprocessor systems, by leveraging the beneÔ¨Åts
of low latency, high bandwidth, and heterogeneous integration capability which are oÔ¨Äered by
3D technology.
KEYWORDS
emerging technology, die-stacking, 3D integrated circuits, memory architecture,
heterogeneous integration
www.ebook3000.com

ix
Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi
Acknowledgments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xiii
1
3D Integration Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.1
3D Integrated Circuits vs. 3D Packaging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
1.2
DiÔ¨Äerent Process Technologies for 3D ICs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.3
ÓÄÄe Impact of 3D Technology on 3D Microprocessor Partitioning . . . . . . . . . . . 3
2
BeneÔ¨Åts of 3D Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.1
Wire Length Reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.2
Memory Bandwidth Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.3
Heterogenous Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.4
Cost-eÔ¨Äective Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
3
Fine-granularity 3D Processor Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.1
3D Cache Partitioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.1.1 3D Cache Partitioning Strategies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.1.2 Design Exploration using 3DCacti . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
3.2
3D Partitioning for Logic Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4
Coarse-granularity 3D Processor Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4.1
3D Caches Stacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
4.2
3D Main Memory Stacking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4.3
3D On-chip Stacked Memory: Cache or Main Memory? . . . . . . . . . . . . . . . . . 32
4.3.1 On-chip Main Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
4.3.2 3D-stacked LLC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
4.3.3 Dynamic Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
4.4
PicoServer. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
5
3D GPU Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5.1
3D-stacked GPU Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
5.2
3D-stacked GPU Processor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

x
6
3D Network-on-Chip . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
6.1
3D NoC Router Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
6.2
3D NoC Topology Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
6.3
3D Optical NoC Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
6.4
Impact of 3D Technology on NoC Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
7
ÓÄÄermal Analysis and ÓÄÄermal-aware Design. . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.1
ÓÄÄermal Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
7.2
ÓÄÄermal-aware Floorplanning for 3D Processors . . . . . . . . . . . . . . . . . . . . . . . . 63
7.3
ÓÄÄermal-herding: ÓÄÄermal-aware Architecture Design . . . . . . . . . . . . . . . . . . . 69
8
Cost Analysis for 3D ICs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
8.1
3D Cost Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74
8.2
Cost Evaluation for Many-core Microprocessor Designs . . . . . . . . . . . . . . . . . . 86
8.2.1 Cost Evaluation with Homogeneous Partitioning . . . . . . . . . . . . . . . . . . 88
8.2.2 Cost Evaluation with Heterogeneous Partitioning. . . . . . . . . . . . . . . . . . 92
9
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97
Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
Authors‚Äô Biographies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 113
www.ebook3000.com

xi
Preface
ÓÄÄree-dimensional (3D) integration is an emerging technology, where two or more layers of active
devices (e.g., CMOS transistors) are integrated both vertically and horizontally in a single cir-
cuit. With continuous technology scaling, 3D integration is becoming an increasingly attractive
technology in implementing microprocessor systems by oÔ¨Äering much lower power consump-
tion, lower interconnect latency, and higher interconnect bandwidth compared to traditional two-
dimensional (2D) circuit integration.
In particular, 3D integration technologies promise at least four major beneÔ¨Åts toward future
microprocessor design.
‚Ä¢ Reduced interconnect wire length. 3D integration can dramatically reduce interconnect
wire length, especially by reducing global interconnects. ÓÄÄis can directly lead to two ben-
eÔ¨Åts: improved circuit delay and reduced power consumption. Circuit delay reduction can
be a straightforward eÔ¨Äect of the reduced wire length; it can result in substantial system
performance improvement. ÓÄÄe power reduction is a result of reduced parasitic capacitance
due to the shorter wire length and incorporating previously oÔ¨Ä-chip signals to be on-chip;
it can lead to less heat generation and extended battery life.
‚Ä¢ Improved memory bandwidth. 3D integration can improve memory bandwidth by an or-
der of magnitude, because the number of interconnects between processor and the memory
is no longer constrained by oÔ¨Ä-chip pin counts. In the era of big data and multithreading,
applications adopt large memory working sets and multiple threads that simultaneously ac-
cess the memory. Today, memory bandwidth is a fundamental performance bottleneck. 3D
integration can be a critical technology in overcoming the memory bandwidth issue.
‚Ä¢ Enabling heterogeneous integration. 3D integration enables heterogeneous integration,
because circuit layers can be implemented with diÔ¨Äerent and incompatible process tech-
nologies. Such heterogeneous integration can lead to novel architecture designs. For ex-
ample, various incompatible memory technologies, such as SRAM, spin-transfer torque
RAM (STT-RAM), and resistive RAM (ReRAM), can be integrated in a single processor
chip to form a hybrid cache hierarchy [1].
‚Ä¢ Enablingsmallerformfactor. 3D integration enables a much smaller form factor compared
to traditional 2D integration technologies. Due to the addition of a third dimension to
conventional 2D layout, it leads to a higher packing density and smaller footprint. ÓÄÄis
potentially leads to processor designs with lower cost.

xii
PREFACE
Both academia and the semiconductor industry are actively pursuing this technology by de-
veloping eÔ¨Écient architectures in a variety of forms. From the industry prospective, 3D integrated
memory is envisioned to become pervasive in the near future. Intel‚Äôs Xeon Phi processors will de-
liver with 3D integrated DRAMs in 2016 [2]. NVIDIA announced that 3D integrated memory
will be adopted in their new GPU products in 2016 [3]. AMD plans to ship high-bandwidth
memory (HBM) with their GPU products and heterogeneous system architecture (HSA)-based
CPUs in 2015 [4]. From the academia prospective, comprehensive studies have been performed
across all aspects of microprocessor architecture design by employing 3D integration technolo-
gies, such as 3D stacked processor core and cache architectures, 3D integrated memory, and 3D
network-on-chip. Furthermore, a large body of research has studied critical issues and opportuni-
ties raised by adopting 3D integration technologies, such as thermal issues which are imposed by
dense integration of active electronic devices, cost issues which are incurred by extra process and
increased die area, and the opportunity in designing cost-eÔ¨Äective microprocessor architectures.
ÓÄÄis book provides a detailed introduction to architecture design with 3D integration tech-
nologies. ÓÄÄe book will start with presenting the background of 3D integration technologies
(Chapter 1), followed by a detailed analysis of the beneÔ¨Åts oÔ¨Äered by these technologies includ-
ing low latency, high bandwidth, heterogeneous integration capability, and cost eÔ¨Éciency (Chap-
ter 2). ÓÄÄen, it will review various approaches to designing future 3D integrated microprocessors
by leveraging the beneÔ¨Åts of 3D integration (Chapter 3 through Chapter 6). ÓÄÄese approaches
cover all levels of microprocessor systems, including processor cores, caches, main memory, and
on-chip network. Furthermore, this book discusses thermal issues raised by 3D integration and
presents recently proposed thermal-aware architecture designs (Chapter 7). Finally, this book
presents a comprehensive cost model which is built based on detailed cost analysis for fabricating
3D integrated microprocessors (Chapter 8). By utilizing the cost model, the book presents and
compares cost-eÔ¨Äective microprocessor design strategies.
While this book mostly focuses on designing high-performance processors, the concepts
and techniques can also be applied to other market segments such as embedded processors and
exascale high-performance computing (HPC) systems.
ÓÄÄe target audiences for this book are students, researchers, and engineers in IC design and
computer architecture, who are interested in leveraging the beneÔ¨Åts of 3D integration for their
designs and research.
Yuan Xie and Jishen Zhao
June 2015
www.ebook3000.com

xiii
Acknowledgments
Much of the work and ideas presented in this book have evolved over years in working with
our colleagues and graduate students at Pennsylvania State University (in particular Professor
Vijaykrishnan Narayanan, Professor Mary Jane Irwin, Professor Chita Das), and our industry
collaborators including Dr. Gabriel Loh, Dr. Bryan Black, Dr. Norm Jouppi, and Mr. Kerry
Bernstein.
We also thank Prof. Niraj Jha, Prof. Margaret Martonosi, and other reviewers for the com-
ments and feedback to improve the draft.
Yuan Xie and Jishen Zhao
June 2015

www.ebook3000.com

1
C H A P T E R
1
3D Integration Technology
A 3D integrated circuit (3D IC) has two or more active device layers (i.e., CMOS transistor
layers) integrated vertically as a single chip, using various integration methods. ÓÄÄis chapter will
give a brief introduction to diÔ¨Äerent 3D integration technologies, including monolithic 3D ICs
and through-silicon-via (TSV)-based 3D ICs.
1.1
3D INTEGRATED CIRCUITS VS. 3D PACKAGING
Figure 1.1: Illustration of 3D SiP (system-in-package) technology.
PoP memory
Processor
Figure 1.2: Illustration of 3D PoP (package-on-package) technology.
Even though the 3D integrated circuit is an emerging technology, the 3D packaging is
a mature technology and has been widely adopted by industry. ÓÄÄere are two main stream 3D
packaging methods:
‚Ä¢ System-in-package (SiP) stacks separate IC chips together vertically on a substrate in a
single package, with internal wire-bond connections that are bonded to the package. ÓÄÄe
conceptual view of SiP is shown in Figure 1.1.
‚Ä¢ Package-on-package (PoP) stacks separate packages vertically. ÓÄÄe conceptual view of PoP
is shown in Figure 1.2.

2
1. 3D INTEGRATION TECHNOLOGY
ÓÄÄere are many commercial products that use 3D packaging. For example, Apple A8 pro-
cessor (used in iPhone6) integrates a processor package (that has dual-core CPU and quad-core
GPU) with a 1GB of LPDDR3 DRAM package using package-on-package technology. ÓÄÄe
3D chip stacking based on SiP or PoP is packaging technology with the design goal of saving
space. ÓÄÄe chips in the stack communicate with oÔ¨Ä-chip signaling, which usually has limited
connectivity and long delay. It does not require signiÔ¨Åcant change in processor architecture or
design methodologies. On the contrary, the emerging 3D integrated circuits can provide a large
number of connections (and much faster) between two stacked dies, and therefore enables rev-
olutionary improvement in architecture innovation and requires fundamental changes in design
methodologies.
1.2
DIFFERENT PROCESS TECHNOLOGIES FOR 3D ICS
DiÔ¨Äerent from SiP or PoP, which uses oÔ¨Ä-chip signaling for communication between stacked
dies, the emerging 3D integrated technologies develop Ô¨Åne-granularity interconnects between
layers, and therefore provide superior beneÔ¨Åts beyond just packaging for space saving, including
fast interconnects and high-bandwidth communication between layers.
ÓÄÄe 3D integration technologies [5, 6] can be classiÔ¨Åed into one of the two following cat-
egories:
‚Ä¢ Monolithic approach. ÓÄÄis approach involves a sequential device process. ÓÄÄe front-end pro-
cessing (to build the device layer) is repeated on a single wafer to build multiple active device
layers before the back-end processing builds interconnects among devices.
‚Ä¢ Stacking approach, which could be further categorized as wafer-to-wafer, die-to-wafer, or
die-to-die stacking methods. ÓÄÄis approach processes each layer separately, using conven-
tional fabrication techniques. ÓÄÄese multiple layers are then assembled to build up 3D IC,
using bonding technology. Since the stacking approach does not require the change of con-
ventional fabrication process, it is much more practical compared to the monolithic ap-
proach, and become the focus of recent 3D integration research.
Several 3D stacking technologies have been explored recently, including wire bonded, mi-
crobump, contactless (capacitive or inductive), and through-silicon vias (TSV) vertical intercon-
nects. Among all these integration approaches, TSV-based 3D integration has the potential to
oÔ¨Äer the greatest vertical interconnect density, and therefore is the most promising one among
all the vertical interconnect technologies. Figure 1.3 shows a conceptual 2-layer 3D integrated
circuit with TSV and microbump.
3D stacking can be implemented using two major techniques [7]: (1) Face-to-Face (F2F)
bonding: two wafers (dies) are stacked so that the very top metal layers are connected. Note that
the die-to-die interconnects in face-to-face wafer bonding does not go through a thick buried
www.ebook3000.com

1.3. THE IMPACT OF 3D TECHNOLOGY ON 3D MICROPROCESSOR PARTITIONING
3
silicon layer and can be fabricated as microbump. ÓÄÄe connections to C4I/O pads¬π (which are chip
pads used to mount the chip to external circuitry) are formed as TSVs; (2) Face-to-Back (F2B)
bonding: multiple device layers are stacked together with the top metal layer of one die bonded
together with the substrate of the other die, and direct vertical interconnects (which are called
through-silicon vias (TSV)) tunneling through the substrate. In such F2B bonding, TSVs are
used for both between-layer-connections and I/O connections. Figure 1.3 shows a conceptual 2-
layer 3D IC with F2F or F2B bonding, with both TSV connections and microbump connections
between layers.
All TSV-based 3D stacking approaches share the following three common process
steps [7]:
‚Ä¢ TSV formation. ÓÄÄis is the step to fabricate the through-silicon-via on a wafer. It can also
be done in 3 diÔ¨Äerent ways: (1)Via-Ô¨Årst: ÓÄÄe TSVs are built before any CMOS transistors
are fabricated on the wafer; (2)Via-middle: ÓÄÄe TSVs. are built after the CMOS transistors
are fabricated but before the metal layers are fabricated; (3)Via-last: ÓÄÄe TSVs are built
after both CMOS transistors and metal connections among transistors are fabricated. Via-
middle can only be done in a foundry, while the other two approaches can be done outside
a foundry. Currently there are two types of materials: highly conductive copper TSVs, or
smaller Tungsten TSVs.
‚Ä¢ Wafer thinning. Wafer thinning is used to reduce the overheads of TSVs. Since TSVs. need
to maintain a certain aspect ratio for reliability/manufacturability purpose, it is a critical step
to thin the wafer so that we can build small and short TSVs between layers. ÓÄÄe thinner
the wafer, the smaller (and shorter) the TSV is (with the same aspect ratio constraint) [7].
ÓÄÄe wafer thickness could be in the range of 10 m to 100 m and the TSV size is in the
range of 0:2 m to 10 m [8].
‚Ä¢ Aligned wafer and die bonding, which could be wafer-to-wafer (W2W) bonding or die-to-
wafer (D2W) bonding.
1.3
THE IMPACT OF 3D TECHNOLOGY ON 3D
MICROPROCESSOR PARTITIONING
One of the key issues for 3D IC design is what level of logic granularity should be considered in
3D partitioning: designers may perform a Ô¨Åne-granularity partitioning at the logic gate level, or
perform a coarse-granularity partitioning at the core level (such as keeping the processor core to
be a 2D design, with cache stacked on top of the core). Which partitioning strategy should be
adopted is heavily inÔ¨Çuenced by the underlying 3D process technology.
¬πC4 is the acronym of Controlled Collapse Chip Connection, also known as Flip Chip, a method for interconnecting IC chips
to external circuitry with the chip IO pads.

4
1. 3D INTEGRATION TECHNOLOGY
Face-2-Face (F2F)
Face-2-Face (F2B)
substrate
substrate
substrate
substrate
Device layer
Device layer
Device layer
Device layer
Metal layers
Metal layers
Metal layers
Metal layers
Microbump
C4 Pad
TSV (Through-Silicon-Via)
Figure 1.3: Illustration of F2F and F2B 3D bonding.
 6
5
Figure 1.4: Comparison of interconnections at diÔ¨Äerent partitioning granularity [9].
In TSV-based 3D stacking bonding, the dimension of the TSVs. is not expected to scale
at the same rate as feature size because alignment tolerance during bonding poses limitations on
the scaling of the vias. ÓÄÄe TSV size, length, and the pitch density, as well as the bonding method
(face-to-face or face-to-back bonding), can have a signiÔ¨Åcant impact on the 3D IC design. For
example, a relatively large size of TSVs. can hinder partitioning a design at very Ô¨Åne granularity
across multiple device layers, and make the true 3D component design less possible. On the
other hand, monolithic 3D integration provides more Ô¨Çexibility in vertical 3D connection because
the vertical 3D via can potentially scale down with feature size due to the use of local wires for
connections. Availability of such technologies makes it possible to partition the design at a very
Ô¨Åne granularity. Furthermore, face-to-face bonding or SOI-based 3D integration may have a
smaller via pitch size and higher via density than face-to-back bonding or bulk-CMOS-based
integration. Such inÔ¨Çuence of the 3D technology parameters on the microprocessor design must
be thoroughly studied before an appropriate partition strategy is adopted.
With TSV-based 3D stacking, the partitioning strategy is determined by the TSV pitch
size and the via diameter. As shown in Figure 1.3, the TSV goes through the substrate and incurs
www.ebook3000.com

1.3. THE IMPACT OF 3D TECHNOLOGY ON 3D MICROPROCESSOR PARTITIONING
5
Via Diameter
Partition Level
gate
macro
unit
core
0.5
5
0.12
0.01
0.001
1
5
10
20
20
0.5
0.05
0.01
1.2
0.12
12
500
2000
50
2
0.5
2
20
200
8000
Figure 1.5: Area overhead (in relative ratio, via diameter in unit of m) at diÔ¨Äerent partitioning
granularity [9].
area overhead, and therefore the larger the via diameter, the higher the area overhead. For example,
Figure 1.4 shows the number of connections for diÔ¨Äerent partitioning granularity, and Figure 1.5
shows the area overhead for diÔ¨Äerent size of 3D via diameters.¬≤ ÓÄÄey show that for Ô¨Åne granularity
partitioning, there are a lot of connections, and with relative large via diameter, the area overhead
would be very high for Ô¨Åne granularity partitioning. Consequently, for most of the existing 3D
process technology with via diameter usually larger than 1 m, it makes more sense to perform
the 3D partitioning at the unit level or core level, rather than at the gate level that can result in a
large area overhead.
¬≤ÓÄÄese two tables are based on IBM 65nm technology for high performance microprocessor design.

www.ebook3000.com

7
C H A P T E R
2
BeneÔ¨Åts of 3D Integration
ÓÄÄe following subsections will discuss various architecture design approaches that leverage dif-
ferent beneÔ¨Åts that 3D integration technology can oÔ¨Äer, namely, wire length reduction, high
memory bandwidth, heterogeneous integration, and cost reduction. It will also brieÔ¨Çy review 3D
network-on-chip architecture designs.
2.1
WIRE LENGTH REDUCTION
Designers have resorted to technology scaling to improve microprocessor performance. Although
the size and switching speed of transistors beneÔ¨Åt as technology feature sizes continue to shrink,
global interconnect wire delay does not scale accordingly with technologies. ÓÄÄe increasing wire
delays have become one major impediment for performance improvement.
ÓÄÄree-dimensional integrated circuits (3D ICs) are attractive options for overcoming the
barriers in interconnect scaling, thereby oÔ¨Äering an opportunity to continue performance im-
provements using CMOS technology. Compared to a traditional two dimensional chip design,
one of the important beneÔ¨Åts of a 3D chip over a traditional two-dimensional (2D) design is the
reduction on global interconnects. It has been shown that three-dimensional architectures reduce
wiring length by a factor of the square root of the number of layers used [10]. ÓÄÄe reduction of
wire length due to 3D integration can result in two obvious beneÔ¨Åts: latency improvement and
power reduction.
Latency Improvement.
Latency improvement can be achieved due to the reduction of average
interconnect length and the critical path length.
Early work on Ô¨Åne-granularity 3D partitioning of processor components shows that the la-
tency of a 3D component could be reduced. For example, since interconnects dominate the delay
of cache accesses which determines the critical path of a microprocessor, and the regular struc-
ture and long wires in a cache make it one of the best candidates for 3D designs, 3D cache design
is one of the early design examples for Ô¨Åne-granularity 3D partition [5]. Wordline partitioning
and bitline partitioning approaches divide a cache bank into multiple layers and reduce the global
interconnects, resulting in a fast cache access time. Depending on the design constraints, the
3DCacti tool [11] automatically explores the design space for a cache design, and identiÔ¨Åes the
optimal partitioning strategy; the latency reduction can be up to 25% for a two-layer 3D cache.
3D arithmetic-component designs also show latency beneÔ¨Åts. For example, various designs [12‚Äì
15] have shown that the 3D arithmetic unit design can achieve around 6%‚Äì30% delay reduction

8
2. BENEFITS OF 3D INTEGRATION
due to wire length reduction. Such Ô¨Åne-granularity 3D partitioning was also demonstrated by In-
tel [16], showing that by targeting the heavily pipelined wires, the pipeline modiÔ¨Åcations resulted
in approximately 15% improved performance, when the Intel Pentium-4 processor was folded
onto 2-layer 3D implementation.
Note that such Ô¨Åne-granularity design of 3D processor components increases the design
complexity, and the latency improvement varies depending on the partitioning strategies and
the underlying 3D process technologies. For example, for the same Kogge-Stone adder design,
a partitioning based on logic level [12] demonstrates that the delay improvement diminishes
as the number of 3D layers increases; a bit-slicing partitioning [14] strategy would have better
scalability as the bit-width or the number of layers increases. Furthermore, the delay improvement
for such bit-slicing 3D arithmetic units is about 6% when using a bulk-CMOS-based 180nm 3D
process [15], while the improvement could be as much as 20% when using a SOI-based 180nm
3D process technology [14], because the SOI-based process has much smaller and shorter TSVs
(and therefore much smaller TSV delay) compared to the bulk-CMOS-based process.
Power Reduction.
Interconnect power consumption becomes a large portion of the total power
consumption as technology scales. ÓÄÄe reduction of the wire length translates into the power sav-
ing in 3D IC design. For example, 7% to 46% of power reduction for 3D arithmetic units were
demonstrated in [14]. In the 3D Intel Pentium-4 implementation [16], because of the reduction
in long global interconnects, the number of repeaters and repeating latches in the implementa-
tion is reduced by 50%, and the 3D clock network has 50% less metal RC than the 2D design,
resulting in a better skew, jitter, and lower power. Such 3D stacked redesign of Intel Pentium 4
processor improves performance by 15% and reduces power by 15% with a temperature increase
of 14 degrees. After using voltage scaling to lower the leak temperature to be the same as the
baseline 2D design, their 3D Pentium 4 processor still showed a performance improvement of
8%.
2.2
MEMORY BANDWIDTH IMPROVEMENT
It has been shown that circuit limitations and limited instruction level parallelism will diminish
the beneÔ¨Åts of modern superscalar microprocessors by increased architectural complexity, which
leads to the advent of Chip Multiprocessors (CMP) as a viable alternative to the complex super-
scalar architecture. ÓÄÄe integration of multi-core or many-core microarchitecture on a single die
is expected to accentuate the already daunting memory-bandwidth problem. Supplying enough
data to a chip with a massive number of on-die cores will become a major challenge for perfor-
mance scalability. Traditional oÔ¨Ä-chip memory will not suÔ¨Éce due to the I/O pin limitations.
ÓÄÄree-dimensional integration has been envisioned as a solution for future micro-architecture
design (especially for multi-core and many-core architectures), to mitigate the interconnect crisis
and the ‚Äúmemory wall‚Äù problem [17‚Äì19]. It is anticipated that memory stacking on top of logic
would be one of the early commercial uses of 3D technology for future chip-multiprocessor de-
www.ebook3000.com

2.2. MEMORY BANDWIDTH IMPROVEMENT
9
sign, by providing improved memory bandwidth for such multi-core/many-core microprocessors.
In addition, such approaches of memory stacking on top of core layers do not have the design
complexity problem as demonstrated by the Ô¨Åne-granularity design approaches, which require
re-designing all processor components for wire length reduction (as discussed in Sec. 2.1).
Intel [16] explored the memory bandwidth beneÔ¨Åts using a base-line Intel Core2 Duo
processor, which contains two cores. By having memory stacking, the on-die cache capacity is
increased, and the performance is improved by capturing larger working sets, reducing oÔ¨Ä-chip
memory bandwidth requirements. For example, one option is to stack an additional 8MB L2
cache on top of the base-line 2D processor (which contains 4MB L2 cache), and the other op-
tion is to replace the SRAM L2 cache with a denser DRAM L2 cache stacking. ÓÄÄeir study
demonstrated that a 32MB 3D stacked DRAM cache can reduce the cycles per memory access
by 13% on average and as much as 55% with negligible temperature increases.
ÓÄÄe PicoServer project [20] follows a similar approach to stack DRAM on top of multi-core
processors. Instead of using stacked memory as a larger L2 cache (as shown by Intel‚Äôs work [16]),
the fast on-chip 3D stacked DRAM main memory enables wide low-latency buses to the proces-
sor cores and eliminates the need for an L2 cache, whose silicon area is allocated to accommodate
more cores. Increasing the number of cores by removing the L2 cache can help improve the com-
putation throughput, while each core can run at a much lower frequency, and therefore result
in an energy-eÔ¨Écient many core design. For example, the PicoServer design can achieve a 14%
performance improvement and 55% power reduction over a baseline multi-core architecture.
As the number of the cores on a single die increases, such memory stacking becomes more
important to provide enough memory bandwidth for processor cores. Recently, Intel [21] demon-
strated an 80-tile terascale chip with network-on-chip. Each core has a local 256KB SRAM
memory (for data and instruction storage) stacked on top of it. TSVs provide a bandwidth of
12GB/second for each core, with a total about 1TB/second bandwidth for Tera Flop computa-
tion. In this chip, the thin memory die is put on top of the CPU die, and the power and I/O
signals go through memory to CPU.
Since DRAM is stacked on top of the processor cores, the memory organization should
also be optimized to fully take advantages of the beneÔ¨Åts that TSVs oÔ¨Äer [17, 22]. For example,
the numbers of ranks and memory controllers are increased, in order to leverage the memory
bandwidth beneÔ¨Åts. A multiple-entry row buÔ¨Äer cache is implemented to further improve the
performance of the 3D main memory. Comprehensive evaluation shows that a 1.75x speedup
over commodity DRAM organization is achieved [17]. In addition, MSHR design was modiÔ¨Åed
to provide a scalable L2 miss handling before accessing the 3D stacked main memory. A data
structure called the Vector Bloom Filter with dynamic MSHR capacity tuning is proposed. Such
structure provides an additional 17:8% performance improvement. If stacked DRAM is used as
the last-level caches (LLC) in chip multiple processors (CMPs), the DRAM cache sets are or-
ganized into multiple queues [22]. A replacement policy is proposed for the queue-based cache
to provide performance isolation between cores and reduce the lifetimes of dead cache lines. Ap-

10
2. BENEFITS OF 3D INTEGRATION
proaches are also proposed to dynamically adapt the queue size and the policy of advancing data
between queues.
ÓÄÄe latency improvement due to 3D technology can also be demonstrated by such mem-
ory stacking design. For example, Li et al. [23] proposed a 3D chip multiprocessor design using
network-in-memory topology. In this design, instead of partitioning each processor core or mem-
ory bank into multiple layers (as shown in [5, 11]), each core or cache bank remains to be a 2D
design. Communication among cores or cache banks are via the network-on-chip (NoC) topol-
ogy. ÓÄÄe core layer and the L2 cache layer are connected with TSV-based bus. Because of the
short distance between layers, TSVs provide a fast access from one layer to another layer, and
eÔ¨Äectively reduce the cache access time because of the faster access to cache banks through TSVs.
2.3
HETEROGENOUS INTEGRATION
3D integration also provides new opportunities for future architecture design, with a new dimen-
sion of design space exploration. In particular, the heterogenous integration capability enabled by
3D integration gives designers new perspective when designing future CMPs.
3D integration technologies provide feasible and cost-eÔ¨Äective approaches for integrating
architectures composed of heterogeneous technologies to realize future microprocessors targeted
at the ‚ÄúMore than Moore‚Äù technology projected by ITRS. 3D integration supports heterogeneous
stacking because diÔ¨Äerent types of components can be fabricated separately, and layers can be
implemented with diÔ¨Äerent technologies. It is also possible to stack optical device layers or non-
volatile memories (such as magnetic RAM (MRAM) or phase-change memory (PCRAM)) on
top of microprocessors to enable cost-eÔ¨Äective heterogeneous integration. ÓÄÄe addition of new
stacking layers composed of new device technology will provide greater Ô¨Çexibility in meeting the
often conÔ¨Çicting design constraints (such as performance, cost, power, and reliability), and enable
innovative designs in future microprocessors.
Non-volatile Memory Stacking.
Stacking layers of non-volatile memory technologies such as
Magnetic Random Access Memory (MRAM) [24] and Phase Change Random Access Memory
(PRAM) [25] on top of processors can enable a new generation of processor architectures with
unique features. ÓÄÄere are several characteristics of MRAM and PRAM architectures that make
them promising candidates for on-chip memory. In addition to their non-volatility, they have
zero standby power, low access power, and are immune to radiation-induced soft errors. How-
ever, integrating these nonvolatile memories along with a logic core involves additional fabrication
challenges that need to be overcome. For example, the MRAM process requires growing a mag-
netic stack between metal layers. Consequently, it may incur extra cost and additional fabrication
complexity to integrate MRAM with conventional CMOS logic into a single 2D chip. ÓÄÄe ability
to integrate two diÔ¨Äerent wafers developed with diÔ¨Äerent technologies using 3D stacking oÔ¨Äers
an ideal solution to overcome this fabrication challenge and exploit the beneÔ¨Åts of PRAM and
MRAM technologies. For example, Sun et al. [26] demonstrated that the optimized MRAM L2
www.ebook3000.com

2.4. COST-EFFECTIVE ARCHITECTURE
11
cache on top of a multi-core processor can improve performance by 4.91% and reduce power by
73.5% compared to the conventional SRAM L2 cache with the similar area.
Optical Die
Package
Fiber: to off-chip
Processor Core Layer
Non-volatile Memory Layer
DRAM Layer
Fiber: to off-chip
Figure 2.1: An illustration of 3D heterogeneous architecture with non-volatile memory stacking and
optical die stacking.
Optical Device Layer Stacking.
Even though 3D memory stacking can help mitigate the mem-
ory bandwidth problem, when it comes to oÔ¨Ä-chip communication, the pin limitations, the en-
ergy cost of electrical signaling, and the non-scalability of chip-length global wires are still sig-
niÔ¨Åcant bandwidth impediments. Recent developments in silicon nanophotonic technology have
the potential to meet the oÔ¨Ä-chip communication bandwidth requirements at acceptable power
levels. With the heterogeneous integration capability that 3D technology oÔ¨Äers, one can inte-
grate optical die together with CMOS processor dies. For example, HP Labs proposed a Corona
architecture [27], which is a 3D many-core architecture that uses nanophotonic communication
for both inter-core communication and oÔ¨Ä-stack communication to memory or I/O devices. A
photonic crossbar fully interconnects its 256 low-power multithreaded cores at 20 terabyte per
second bandwidth, with much lower power consumption.
Figure 2.1 illustrates such a 3D heterogenous processor architecture, which integrates non-
volatile memories and optical die together through 3D integration technology.
2.4
COST-EFFECTIVE ARCHITECTURE
Increasing integration density has resulted in large die size for microprocessors. With a constant
defect density, a larger die typically has a lower yield. Consequently, partitioning a large 2D mi-
croprocessor to be multiple smaller dies and stacking them together may result in a much higher
yield for the chip, even though 3D stacking incurs extra manufacture cost due to extra steps for 3D
integration and may cause a yield loss during stacking. Depending on the original 2D micropro-
cessor die size, it may be cost-eÔ¨Äective to implement the chip using 3D stacking [28], especially

12
2. BENEFITS OF 3D INTEGRATION
for large microprocessors. ÓÄÄe heterogenous integration capability that 3D provides can also help
reduce the cost.
In addition, as technology feature size scales to reach the physics limits, it has been predicted
that moving to the next technology node is not only diÔ¨Écult but also prohibitively expensive.
3D stacking can potentially provide a cost-eÔ¨Äective integration solution, compared to traditional
technology scaling.
www.ebook3000.com

13
C H A P T E R
3
Fine-granularity 3D Processor
Design
As 3D integration technology emerges, the 3D stacking provides great opportunities of improve-
ments in the microarchitecture. In this chapter, we introduce some recent 3D research in the
architecture level. ÓÄÄese techniques leverage the advantages of 3D and help to improve perfor-
mance, reduce power consumption, etc.
As 3D integration can reduce the wire length, it is straightforward to partition the struc-
ture of a planar processor and stack them to improve the performance. ÓÄÄere are two diÔ¨Äerent
methods: (1) coarse granularity stacking, also known as ‚Äúmemory+logic‚Äù strategy, in which some
on-chip memories are separated from and stacked on the part containing logic components [29‚Äì
33], and (2) Ô¨Åne granularity stacking, in which various function units of the processor are sep-
arated and stacked, and some components are internally partitioned and implemented with 3D
integration [12, 34‚Äì37].
3D stacking enables denser form factor and more cost-eÔ¨Écient integration of heteroge-
neous process technologies. It is possible to stack DRAM or other emerging memories on-chip.
Consequently, the advantages of diÔ¨Äerent memory technologies are leveraged. For example, more
data can be stored on-chip and the static power consumption can be reduced. At the same time,
the high bandwidth provided by 3D integration can be explored for the large capacity on-chip
memory to further improve the performance [25, 30‚Äì33, 38].
In this chapter, we Ô¨Årst describe how to partition and stack the on-chip memory (SRAM
arrays), as they have regular structures. ÓÄÄen, we discuss the beneÔ¨Åts and issues of partitioning
the logic components.
3.1
3D CACHE PARTITIONING
ÓÄÄe regular structure and long wires in a cache make it one of the best candidates for 3D designs.
ÓÄÄis section examines possible partitioning techniques for caches designed using 3D structures
and presents a delay and energy model to explore diÔ¨Äerent options for partitioning a cache across
diÔ¨Äerent device layers.
3.1.1
3D CACHE PARTITIONING STRATEGIES
In this section, we discuss diÔ¨Äerent approaches to partition a cache into multiple device layers.

14
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
ÓÄÄe Ô¨Ånest granularity of partitioning a cache is at the SRAM cell level. At this level of
partitioning, any of the six transistors of a SRAM cell can be assigned to any layer. For example,
the pull-up PMOS transistors can be in one device layer and the access transistors and the pull-
down NMOS transistors can be in another layer. ÓÄÄe beneÔ¨Åts of cell level partitioning include
the reduction in footprint of the cache arrays and, consequently, the routing distance of the global
signals. However, the feasibility of partitioning at this level is constrained by the 3D via size as
compared to the SRAM cell size. Assuming a limitation that the aspect ratio of 3D via size cannot
be scaled less than 1 m  1 m, the 3D via has a comparable size to that of a 2D 6T SRAM cell
in 180 nm technology and is much larger than a single cell in 70 nm technology. Consequently,
when the 3D via size does not scale with feature size (which is the case for wafer-bonding 3D
integration), partitioning at the cell level is not feasible. In contrast, partitioning at the SRAM cell
level is feasible in technologies such as MLBS, because no limitation is imposed on via scaling
with feature size. However, it should be noted that even if the size of a 3D via can be scaled
to as small as a nominal contact in a given technology, the total SRAM cell area reduction (as
compared to a 2D design) due to the use of additional layers is limited, because metal routing and
contacts occupy a signiÔ¨Åcant portion of the 2D SRAM cell area [39]. Consequently, partitioning
at a higher level of granularity is more practical. For example, individual sub-arrays in the 2D
cache can be partitioned across multiple device layers. ÓÄÄe partitioning at this granularity reduces
the footprint of cache arrays and routing lengths of global signals. However, it also changes the
complexity of the peripheral circuits. In this section, we consider two options of partitioning the
sub-array into multiple layers: 3D divided wordline (3DWL) strategy and 3D divided bit line
strategy (3DBL).
3D Divided Wordline (3DWL) In this partitioning strategy, the wordlines in a sub-array
are divided and mapped onto diÔ¨Äerent active device layers (see Fig. 3.3). ÓÄÄe corresponding local
wordline decoder of the original wordline in 2D sub-array is placed on one layer and is used to
feed the wordline drivers on diÔ¨Äerent layers through the 3D vias. We duplicate wordline drivers
for each layer. ÓÄÄe duplication overhead is oÔ¨Äset by the resized drivers for a smaller capacitive
load on the partitioned word line. Further, the delay time of pulling a wordline decreases as the
number of pass transistors connected to a wordline driver is smaller. ÓÄÄe delay calculation of the
3DWL also accounts for the 3D via area utilization. ÓÄÄe area overhead due to 3D vias is small
compared to the number of cells on a word line.
Another beneÔ¨Åt from 3DWL is that the length of the address line from periphery of the
core to the wordline decoder decreases in proportion to the number of device layers. Similarly,
the routing distance between the output of pre-decoder to the local decoder is reduced. ÓÄÄe select
lines for the writes and multiplexer as well as the wires from the output drivers to the periphery
also have shorter length.
3D Divided Bitline (3DBL) ÓÄÄis approach is akin to the 3DWL strategy and applies par-
titioning to the bitlines of a sub-array (see Fig. 3.1). ÓÄÄe bitline length in the sub-array and the
number of the pass transistors connected to a single bitline are reduced. In the 3DBL approach,
www.ebook3000.com

3.1. 3D CACHE PARTITIONING
15
the sense ampliÔ¨Åers can either be duplicated across diÔ¨Äerent device layers or shared among the
partitioned sub-arrays in diÔ¨Äerent layers. ÓÄÄe former approach is more suitable for reducing ac-
cess time while the latter is preferred for reducing number of transistors and leakage. In the latter
approach, the sharing increases complexity of multiplexing of bitlines and reduces performance
as compared to the former. Similar to 3DWL, the length of the global lines are reduced in this
scheme.


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	





	










!"#
$%&'(
*(
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
	





	




	





	










!#
$&'(
*(
	

	

	

	

	

	

	

	





	




	



	



Figure 3.1: Cache with 3D divided bitline partitioning and mapped into two active device layers [40].
3D Cache Delay-Energy Estimator (3DCACTI)
In order to explore the 3D cache design space, a 3D cache delay-energy estimation tool called
3DCacti was developed [40]. ÓÄÄe tool was built on top of the Cacti 3.0 2D cache tool (Cache
Access and Cycle Time) [41]. 3DCacti searches for the optimized conÔ¨Åguration that explores the
best delay, power, and area eÔ¨Éciency trade-oÔ¨Ä, according to the cost function for a given number
of 3D device layers.
In the original Cacti tool [41], several conÔ¨Åguration parameters (see Table 3.1) are used to
divide a cache into sub-arrays to explore delay, energy, and area eÔ¨Éciency trade-oÔ¨Äs. In 3DCacti
implementation, two additional parameters, Nx and Ny, are added to model the intra-subarray
3D partitions. ÓÄÄe additional eÔ¨Äects of varying each parameter other than the impact on length of
global routing signals are listed in Table 3.1. Note that the tag array is optimized independently
of the data array and the conÔ¨Åguration parameters for tag array: Ntwl, Ntbl, and Ntspd are not
listed.

16
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
C = cache size = 32kB
B = block size = 16B
A = associativity = 2
ÀÉÀÄÀÖ
ÀÖÀÄÀÖ
ÀáÀÄÀÖ
ÀâÀÄÀÖ
ÀÑÀÄÀÖ
ÀÜÀÄÀÖ
ÀàÀÄÀÖ
ÀäÀÄÀÖ
ÀÉÀÄÀÖ
ÀÖÀÄÀÖ
ÀáÀÄÀÖ
ÀâÀÄÀÖ
ÀÑÀÄÀÖ
ÀÜÀÄÀÖ
ÀàÀÄÀÖ
ÀäÀÄÀÖ
Ndwl
= 2
Ndbl =
4
Nspd
= 2
Nx:Ny = 2:1
Nx:Ny = 1:2
Original array
256xBLs
128xWLs
128xBLs
128xWLs
128xBLs
32WLs
256xBLs
16WLs
ÀîÃÖÃÖÀ¥Ãå
ÀÉ
ÀÑ
ÀÉ
ÀÖ
Àá
Àâ
ÀÑ
ÀÜ
Àà
Àä
ÀÉ
ÀÖ
Àá
Àâ
ÀÑ
ÀÜ
Àà
Àä
ÀÉÀÄÀÑ
ÀÖÀÄÀÑ
ÀáÀÄÀÑ
ÀâÀÄÀÑ
ÀÑÀÄÀÑ
ÀÜÀÄÀÑ
ÀàÀÄÀÑ
ÀäÀÄÀÑ
128xBLs &16xWLs
256xBLs & 8xWLs
ÀÉÀÄÀÑ
ÀÖÀÄÀÑ
ÀáÀÄÀÑ
ÀâÀÄÀÑ
ÀÑÀÄÀÑ
ÀÜÀÄÀÑ
ÀàÀÄÀÑ
ÀäÀÄÀÑ
Figure 3.2: Cache with 3D divided bitline partitioning and mapped into two active device layers [5].
Figure 3.2 shows an example of how these conÔ¨Åguration parameters used in 3DCacti aÔ¨Äect
the cache structure. ÓÄÄe cell level partitioning approach (using MLBS) is implicitly simulated
using a diÔ¨Äerent cell width and height within Cacti.
Table 3.1: Design parameters for 3DCacti and their impact on cache design
Parameter
DeÔ¨Ånition
EÔ¨Äect on Cache Design
Ndbl
the number of cuts on
1. the bitline length in each sub-array
a cache to divide
2. the number of sense ampliÔ¨Åers
bitlines
3. the size of wordline driver
4. the decoder complexity
5. the multiplexors complexity in
data output path
Ndwl
the number of cuts on
1. the wordline length in each sub-array
a cache to divide
2. the number of wordline drivers
wordlines
3. the decoder complexity
Nspd
the number of sets connected
1. the wordline length in each sub-array
to a wordline
2. the size of wordline drivers
3. the multiplexors complexity in
data output path
Nx
the number of 3D partitions
1. the wordline length in each sub-array
by dividing wordlines
2. the size of wordline driver
Ny
the number of 3D partitions
1. the bitline length in each sub-array
by dividing bitlines
2. the complexity in multiplexors
in data output path
3.1.2
DESIGN EXPLORATION USING 3DCACTI
By using 3DCacti, we can explore various 3D partitioning options of caches to understand their
impact on delay and power at the very early design stage. Note that the data presented in this
section is in 70 nm technology, assuming one read/write port and one bank in each cache, unless
otherwise stated.
First, we explore the best conÔ¨Ågurations for various degrees of 3DWL and 3DBL in terms
of delay. Figure 3.4 and Fig. 3.5 show the access delay and energy consumption per access for
4-way set associative caches of various sizes and diÔ¨Äerent 3D partitioning settings. Remember
that Nx (Ny) in the conÔ¨Åguration refers to the degree of 3DWL (3DBL) partitioning. First, we
www.ebook3000.com

3.1. 3D CACHE PARTITIONING
17


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 




 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 





 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 



	


 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

	



 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 








	


	
	




	
	

	


		

	





	
	











	

	




Figure 3.3: An example showing how each conÔ¨Åguration parameter aÔ¨Äects a cache structure. Each
box is a sub-array associated with an independent decoder [40].
observe that delay reduces as the number of layers increase. From Fig. 3.6, we observe that the
reduction in global wiring length of the decoder is the main reason for delay reduction beneÔ¨Åt.
We also observe that for the 2-layer case, the partitioning of a single cell using MLBS provides
delay reduction beneÔ¨Åts similar to the best intra-subarray partitioning technique as compared to
the 2D design.
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1x1
MLBS
2x1
1x2
4x1
2x2
1x4
8x1
4x2
2x4
1x8
Delay (nS)
64KB
128KB
256KB
512KB
1MB
32,1,32
1,4,4
32,1,32
1,2,8
32,1,16
1,2,2
16,2,32
1,2,32
32,1,16
1,4,8
32,1,16
1,4,1
2D
3D
2 wafers
3D
4 wafers
3D
8 wafers
3D partitioning (Nx*Ny)
32,1,16
1,2,4
markers:
Ndwl,Ndbl,Nspd
Ntwl,Ntbl,Ntspd
32,1,32
1,2,8
32,1,16
1,2,4
32,1,32
1,2,4
32,1,32
1,2,8
Figure 3.4: Access time for diÔ¨Äerent partitioning [40]. Data of caches of associativity=4 are shown.
Another general trend observed for all cache sizes is that partitioning more aggressively
using 3DWL results in faster access time. For example, in the 4-layer case, the conÔ¨Åguration 4x1
(folding wordline into four layers) has an access time which is 16.3% less than that of the 1x4

18
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
0.4
0.6
0.8
1
1.2
1.4
1x1
MLBS
2x1
1x2
4x1
2x2
1x4
8x1
4x2
2x4
1x8
Energy (nJ)
64KB
128KB
256KB
512KB
1MB
2D
3D
2 layers
3D
4 layers
3D
8 layers
3D Partitioning (Nx*Ny)
Figure 3.5: Energy for diÔ¨Äerent partitioning when setting the weightage of delay higher [40]. Data
of caches of associativity=4 are shown.
(folding bitline into four layers) conÔ¨Åguration for a 1MB cache. We observed that the beneÔ¨Åts
from more aggressive 3DWL stem from the longer length of the global wires in the X direction
as compared to the Y direction before 3D partitioning is performed. ÓÄÄe preference for shorter
bitlines for delay minimization in each of the sub-arrays and the resulting wider sub-arrays in
optimal 2D conÔ¨Åguration is the reason for the diÔ¨Äerence in wire lengths along the two directions.
For example, in Fig. 3.8(a), the best sub-array conÔ¨Åguration for the 1MB cache in 2D design
results in the global wire length in the X direction being longer. Consequently, when wordlines
are divided along the third dimension, more signiÔ¨Åcant reduction in critical global wiring lengths
can be achieved. Note that because 3DCacti is exploring partitioning across the dimensions si-
multaneously, some conÔ¨Ågurations can result in 2D conÔ¨Ågurations that have wirelengths greater
in the Y directions (see Fig. 3.8(c)) as in the 1MB cache 1x2 conÔ¨Åguration for two layers. ÓÄÄe
3DBL helps in reducing the global wire length delays by reducing the Y direction length. How-
ever, it is still not as eÔ¨Äective as the corresponding 2x1 conÔ¨Åguration as both the bitline delays in
the core and the routing delays are larger (see Fig. 3.6 and Fig. 3.7). ÓÄÄese trends are diÔ¨Écult to
analyze without the help of a tool to partition across multiple dimensions simultaneously. ÓÄÄe en-
ergy reduction for the corresponding best delay conÔ¨Ågurations tracks the delay reduction in many
cases. For example, the energy of 1MB cache increases when moving from an 8x1 conÔ¨Åguration
www.ebook3000.com

3.1. 3D CACHE PARTITIONING
19
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1x1
MLBS
2x1
1x2
4x1
2x2
1x4
8x1
4x2
2x4
1x8
3D Partitioning (Nx*Ny)
Delay (nS)
output
SA
BL
WL-charge
WL_driver
decoder
predec_driver
Figure 3.6: Access time breakdown of a 1MB cache corresponding to the results shown in
Fig. 3.6 [40].
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1x1
MLBS
2x1
1x2
4x1
2x2
1x4
8x1
4x2
2x4
1x8
3D Partitioning (Nx*Ny)
Energy (nJ)
decoder
WL
BL
SA
leakage
Others (tag array)
Figure 3.7: Energy breakdown of a 1MB cache corresponding to the results shown in Fig. 3.7 [40].
to a 1x8 conÔ¨Åguration. In these cases, the capacitive loading that aÔ¨Äects delay also determines
the energy trends. In some cases, the energy reduces signiÔ¨Åcantly when changing conÔ¨Ågurations
and does not track performance behavior. For example, for the 512KB cache using 8-layers, the
energy reduces when moving from 2*4 to 1x8 conÔ¨Åguration. ÓÄÄis stems from the diÔ¨Äerence in
the number of sense ampliÔ¨Åers activated in these conÔ¨Ågurations, due to the diÔ¨Äerent number of
bitlines in each sub-array in the diÔ¨Äerent conÔ¨Ågurations and the presence of the column decoders

20
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
Figure 3.8: Critical paths in 3DWL and 3DBL for a 1MB cache [40]. Dashed lines represent the
routing of address bits from pre-decoder to local decoder while the solid arrow lines are the routing
paths from the address inputs to predecoders.
after the sense ampliÔ¨Åers. SpeciÔ¨Åcally, the optimum (Ndwl,Ndbl,Nspd) for the 512KB case is
(32,1,16) for the 2*4 case and (32,1,8) for the 1*8 conÔ¨Åguration. Consequently, the number of
sense ampliÔ¨Åers activated per access for 1x8 conÔ¨Åguration is only half as much as that of the 2x4
conÔ¨Åguration, resulting in a smaller energy.
Puttaswamy et al. provided a good study of 3D integrated SRAM components for high-
performance microprocessors [34]. In this paper, they explored various design options of 3D
integrated SRAM arrays with functional block partitioning. ÓÄÄey studied two diÔ¨Äerent types of
SRAM arrays, which are banked SRAM arrays (e.g., caches) and multiported SRAM arrays (e.g.,
register Ô¨Åles).
For the banked SRAM arrays, the paper discussed methods of bank stacking 3D and array
splitting 3D. ÓÄÄe bank stacking is straightforward as the SRAM arrays have already been parti-
www.ebook3000.com

3.1. 3D CACHE PARTITIONING
21
bitline (port 0)
wordline (port 0)
wordline (port 1)
bitline (port 1)
layer 1
layer 0
Figure 3.9: An illustration of 3D stacked two-port SRAM cell [34].
tioned into banks in the 2D case. ÓÄÄe decision for a horizontal or vertical split largely depends on
which dimension has more wire delay. ÓÄÄe array splitting method may help to reduce the lengths
of either wordlines or the bitlines. When the wordline is split, die-to-die vias are required because
the row decoder needs to drive the wordlines on both of the dies, and the column select multi-
plexers have been split across the two dies. For the row stacked SRAM, a row decoder need to be
partitioned across the two dies. If the peripheral circuitry, such as sense ampliÔ¨Åer, is shared for
the two dies, extra multiplexers may be required. In their work, latency and energy results were
evaluated for caches from 16KB to 4MB. ÓÄÄe results show that array-split conÔ¨Ågurations provide
additional latency reductions as compared to the bank-stacked conÔ¨Ågurations. ÓÄÄe 3D organiza-
tion provides more beneÔ¨Åts to the larger arrays because these structures have substantially longer
global wires to route signals between the array edge and the farthest bank. For large-sized (2-4
MB) caches, the 3D organization exhibits the greatest reduction in global bank-level routing la-
tency, since it is the dominant component of latency. On the other hand, moderate-sized (64-512
KB) cache delays are not dominated by the global routing. In these instances, a 3D organization
that targets the intra-SRAM delays provides more beneÔ¨Åt. Similar to the latency reduction, the
3D organization reduces the energy in the most wire-dominated portions of the arrays, namely
the bank-level routing and the bitlines in diÔ¨Äerent conÔ¨Ågurations [34].
For the multiported SRAM, Puttaswamy et al. took the register Ô¨Åles (RF) as an example to
show diÔ¨Äerent 3D stacking strategies. First, a RF can split half of the entries and stack them on
the rest half. ÓÄÄis is called register partitioning (RP). ÓÄÄe bitline and row decoder are halved so
that the access latency is reduced. Second, a bit-partitioned (BP) 3D register Ô¨Åle stacks higher or-
der and lower order bits of the same register across diÔ¨Äerent dies. Such a strategy reduces the load

22
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
on wordline. ÓÄÄird, the large footprint of a multiported SRAM cell provides the opportunity to
allocate one or two die-to-die vias for each cell. Consequently, it is feasible that each die contains
bitlines, wordlines, and access transistors for half of the ports. ÓÄÄis strategy is called port split-
ting (PS), which provides beneÔ¨Åts in reducing area footprint. Figure 3.9 gives an illustration of the
3D structure of the two-port cell. ÓÄÄe area reduction translates into latency and energy savings.
ÓÄÄe register Ô¨Åles with sizes ranging from 16 up to 192 entries were simulated in the work. ÓÄÄe
results show that, for 2-die stacks, the BP design provides the largest latency reduction beneÔ¨Åt
when compared to the corresponding RP and PS designs. ÓÄÄe wordline is heavily loaded by the
two access transistors per bitline column. Splitting the wordline across two dies reduces a major
component of the wire latency. As the number of entries increases, beneÔ¨Åts of latency reduction
from BP designs decrease because the height of the overall structure increases, which makes the
row decoder and bitline/sense-ampliÔ¨Åer delay increasingly critical. ÓÄÄe beneÔ¨Åts of using the other
two strategies, however, increase for the same reason. ÓÄÄe 3D conÔ¨Åguration that minimizes en-
ergy consumption is not necessarily the conÔ¨Åguration that has the lowest latency. For a smaller
number of entries, BP organization requires the least energy. As the number of entries increases
above 64, RP organization provides the most beneÔ¨Åts by eÔ¨Äectively reducing bitline length [34].
When there are more than two stacked layers, the case is more complicated because of many
stacking options. More details can be found in the paper.
3.2
3D PARTITIONING FOR LOGIC BLOCKS
We have discussed how to split a planar SRAM cache and integrate it with 3D stacking. Black
et al. compared the performance, power consumption, and temperatures of splitting logic com-
ponents of a processor and stacking them in two layers. Such a strategy is called ‚Äúlogic+logic‚Äù
stacking. Using logic+logic stacking, a new 3D Ô¨Çoorplan can be developed that requires only 50%
of the original footprint. ÓÄÄe goal is to reduce inter-block interconnect by stacking and reduc-
ing intra-block, or within block interconnect through block splitting [35]. Some components in
pipeline, which are far from each other in the worst case data path of a 2D Ô¨Çoorplan, can be
stacked on each other in diÔ¨Äerent layers so that the pipeline stages can be reduced. An exam-
ple is shown in Fig. 3.10, the path between the Ô¨Årst level data cache (D$) and the data input to
the functional units (F) is drawn illustratively. ÓÄÄe worst case path in planar processor occurs in
Fig. 3.10 (a), when load data must travel from the far edge of the D$ to the farthest functional
unit. Figure 3.10(b) shows that a 3D Ô¨Çoorplan can overlap the D$ and F. In the 3D Ô¨Çoorplan,
the load data only travels to the center of the D$, at which point it is routed to the other die to
the center of the functional units. As a result of stacking that same worst case path contains half
as much routing distance, since the data is only traversing half of the data cache and half of the
functional units [35].
Using logic+logic stacking, 25% of all pipe stages in the microarchitecture are eliminated
with the new 3D Ô¨Çoorplan simply by reducing metal runs, resulting in a 15% performance im-
provement [35]. Although the performance (IPC) increase causes more activity that increases the
www.ebook3000.com

3.2. 3D PARTITIONING FOR LOGIC BLOCKS
23
Figure 3.10: (a) Planar Ô¨Çoorplan of a deeply pipelined microprocessor with the point register read to
execute paths, (b)3D Ô¨Çoorplan of the planar microprocessor in (a) [35].
power, the power per instruction is reduced because of the elimination of pipeline stages. ÓÄÄe
temperature increase, however, is a serious problem because of the doubling of power density in
3D stacked logic. ÓÄÄe ‚Äúworst case‚Äù shows a 26 degree increase if there were no power savings from
the 3D Ô¨Çoorplan and the stacking were to result in a 2x power density. A simple iterative process
of placing blocks, observing the result is a 1.3x power density increase and 14 degree temperature
increase [35].
Balaji et al. also explored design methodologies for processor components in 3D technol-
ogy, in order to maximize the performance and power eÔ¨Éciency of these components [12]. ÓÄÄey
proposed systematic partitioning strategies for custom design of several example components: a)
instruction scheduler, b) Kogge-Stone adder, and c) logarithmic shifter. In addition, they also de-
veloped a 3D ASIC design Ô¨Çow leveraging both widely used 2D CAD tools (Synopsys Design
Compiler, Cadence Silicon Ensemble, and Synopsys Prime Time) and emerging 3D CAD tools
(MIT PR3D and in-house netlist extraction and timing tool).
ÓÄÄe custom designs are implemented with MIT 3D Magic‚Äîa layout tool customized for
3D designs‚Äîtogether with in-house netlist and timing extraction scripts. For the instruction
scheduler, they found that the tag drive latency is a major component of the overall latency and
sensitive to wire delay. ÓÄÄus they proposed to either horizontally or vertically partition the tag
line to reduce the tag drive latency. From the experiment result, horizontal partitioning achieves

24
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
signiÔ¨Åcant latency reduction (44% when moving from 2D to 2-tier 3D implementations) while
vertical partitioning only achieves marginal improvement (4%). For both the KS adder and the
logarithmic shifter, the intensive wiring in the design becomes the limiting factor in performance
and power in advanced technology nodes. In the 3D implementation, the KS adder is partitioned
horizontally and the logarithmic shifter is partitioned vertically. SigniÔ¨Åcant latency reductions
were observed, 20:23%‚Äì32:7% for the KS adder and 13:4% for the logarithmic shifter when the
number of tiers varies. In addition to the custom design, they also experimented the proposed
ASIC design Ô¨Çow with a range of arithmetic units. ÓÄÄe implementation results show that signiÔ¨Å-
cant latency reductions (9:6%‚Äì42:3%) are archived for the 3D designs generated by the proposed
Ô¨Çow. Last but not least, it is observed that moving from one tier to two tier produces the most sig-
niÔ¨Åcant performance improvements, while this improvement saturates when more tiers are used.
Architectural performance impact of the 3D components is evaluated by simulating a 3D pro-
cessor running SPEC2000 benchmarks. ÓÄÄe data path width of the processor is scaled up, since
the 3D components have much lower latency than 2D components with same widths. ÓÄÄerefore,
simulation results show an average IPC speedup of 11% for the 3D processor.
So far, the partitioning of the components in the 3D design are all speciÔ¨Åed and most par-
titions are limited within two layers. In order to further explore the 3D design space, Ma et al.
proposed a microarchitecture-physical codesign framework to handle Ô¨Åne-grain 3D design [42].
First, the components in the design are modeled in multiple silicon layers. ÓÄÄe eÔ¨Äects of diÔ¨Äerent
partitioning approaches are analyzed with respect to area, timing, and power. All these approaches
are considered as the potential design strategies. Note that the number of layers that a compo-
nent can be partitioned into is not Ô¨Åxed. Especially, the single layer design of a component is
also included in the total design space. In addition, the author analyzed the impact of scaling the
sizes of diÔ¨Äerent architectural structures. Having these design alternatives of the components,
an architectural building engine is used to choose the optimized conÔ¨Ågurations among a wide
range of implementations. Some heuristic methods are proposed to speed-up the convergence
and reduce redundant searches on infeasible solutions. At the same time, a thermal-aware pack-
ing engine with temperature simulator tool is employed to optimize the thermal characters of
the entire design so that the hotspot temperatures are below the given thermal thresholds. With
these engines, the framework takes the frequency target, architectural netlist, and a pool of alter-
native block implementations as the inputs and Ô¨Ånds the optimized design solution in terms of
performance, temperature, or both. In the experiments, the author used a superscalar processor
as an example of design space exploration using the framework. ÓÄÄe critical components such as
the issue queue and caches could be partitioned into two to four layers. ÓÄÄe partition methods
included block folding and port partitioning, as introduced before. ÓÄÄe experimental results show
a 36% performance improvement over traditional 2D and 14% over 3D with single-layer unit
implementations.
We have discussed that the 3D stacking aggravates the thermal issues, and a thermal-aware
placement can help alleviate the problem [35]. ÓÄÄere are some architectural level techniques,
www.ebook3000.com

3.2. 3D PARTITIONING FOR LOGIC BLOCKS
25
3
F
1
0
1
5
A
2
2
7
1
1
0
2
1
5
5
3
1
F
0
0
0
0
0
0
0
0
0
0
0
0
(a)
(b)
0-15 Bit
16-31 Bit
32-47 Bit
48-63 Bit
Figure 3.11: ÓÄÄe conceptual view of a 3D stacked register Ô¨Åle: (a) data bits on lower three layers are
all zeros, (b) data in all layers are non-zeros [36].
which can be used to control the thermal hotspots. Puttaswamy et al. proposed thermal herding
techniques for the Ô¨Åne-grain partitioned 3D microarchitecture. For diÔ¨Äerent components in the
processor, various techniques are introduced to reduce 3D power density and locate a majority of
the power on the die (layer) closest to the heat sink.
Assume there are four 3D stacked layers and the processor is 64-bit based. Some 64-bit
based components (e.g., register Ô¨Åles, arithmetic units) are equally partitioned and 16-bit is placed
in each layer, as shown in Fig. 3.11. Such a partition not only reduces the access latency, but also
provides the opportunities for thermal herding. For example, in Fig. 3.11(a) the most signiÔ¨Åcant
48 bits (MSBs) of the data are zero. We don‚Äôt have to load/store these zero bits into register Ô¨Åles, or
process these bits in the arithmetic unit, such as the adder. If we only process the least signiÔ¨Åcant
16 bits (LSBs), the power is reduced. In addition, if the LSBs are located in the layer next to the
heat sink, the temperature is also decreased. For the data shown in Fig. 3.11(b), however, we have
to process all bits since they are non-zeros in four layers.
For some other components, such as instruction scheduler and data caches, entries are par-
titioned and placed in diÔ¨Äerent layers. ÓÄÄe accesses to these components are controlled so that
the entries, which are located in the layer next to the heat sink, are more frequently accessed.
Consequently, the energy is herded toward the heat sink and the temperature is reduced. Extra
hardware, however, is required to achieve these techniques. For example, an extra bit is induced
in the register Ô¨Åle to represent whether the MSBs are zero. ÓÄÄis bit is propagated to other com-
ponents for further controls. Compared to a conventional planar processor, the 3D processor
achieves a 47:9% frequency increase which results in a 47:0% performance improvement, while

26
3. FINE-GRANULARITY 3D PROCESSOR DESIGN
simultaneously reducing total power by 20%. Without ÓÄÄermal Herding techniques, the worst-
case 3D temperature increases by 17 degrees. With ÓÄÄermal Herding techniques, the temperature
increase is only 12 degrees [36].
www.ebook3000.com

27
C H A P T E R
4
Coarse-granularity 3D
Processor Design
In this section, we will focus on the ‚Äúmemory+logic‚Äù strategy in multi-core processors. Memory
of various technologies can be stacked on top of cores as caches or even on-chip main memories.
DiÔ¨Äerent from the research in the previous section, which focuses on optimizations in the Ô¨Åne-
granularity (e.g., wire length reduction), the approaches of this section consider the memories as
a whole structure and explore the high-level improvements, such as access interfaces, replacement
policies, etc.
4.1
3D CACHES STACKING
Black et al. proposed to stack SRAM/DRAM L2 caches on cores based on Intel‚Äôs CoreTM 2 Duo
model. It was assumed that the 4M L2 caches occupy 50% area in a planar two-core processor.
ÓÄÄey proposed three diÔ¨Äerent stacking options without increasing the footprint: (1) stacking extra
8M L2 caches on the original processor; (2) removing 4M SRAM caches and stacking a 32M
DRAM caches on logic (the 2MB DRAM tag is placed in the same layer of logic); (3) remov-
ing 4M SRAM caches and stacking 64M DRAM caches (the 4MB DRAM tag is placed in the
same layer of logic). ÓÄÄese three options are shown in Fig. 4.1 together with the 2D baseline. ÓÄÄe
results showed that stacking DRAM caches could signiÔ¨Åcantly reduce the average memory ac-
cess time and reduce the oÔ¨Ä-chip bandwidth for some benchmarks with a working set larger than
4MB. In addition, the oÔ¨Ä-chip bus power was greatly reduced due to the reduction of oÔ¨Ä-chip
memory access. Since the DRAM has less power than SRAM of the same area, stacking DRAM
cache also reduces the power compared to the case of stacking SRAM caches. ÓÄÄe thermal sim-
ulations also showed that the peak temperature was increased by less than three degrees with the
64MB DRAM caches stacked. It meant that the thermal impact of stacking DRAM was not
signiÔ¨Åcant [35].
While prior approaches have shown the performance beneÔ¨Åts of using 3D stacked DRAM
as a large last-level cache (LLC), Loh proposed new cache management policies for further im-
provements leveraging the hardware organization of DRAM architectures [30]. ÓÄÄe management
scheme organizes the ways a cache set into multiple FIFOs (queues). As shown in Fig. 4.2, each
entry of the queue represents a cache line, and all entries of queues and ‚ÄúLRU‚Äù cache cover a set
of cache line. ÓÄÄe Ô¨Årst level queues and second level queues are similar except that, in multi-core
processors, there is one Ô¨Årst level of queue for each thread, but the second level queue is shared,

28
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
Figure 4.1: Memory stacked options: (a) 4MB baseline; (b) 8MB stacked for a total of 12MB; (c)
32MB of stacked DRAM with no SRAM; (d) 64MB of stacked DRAM [35].
Figure 4.2: Structure of the multi-queue cache management scheme for (a) a single core and (b)
multiple cores [30].
as shown in Fig. 4.2(b). ÓÄÄe data loaded to LLC is Ô¨Årst inserted into these queues before being
placed into cache ways following ‚ÄúLRU‚Äù policies. A ‚Äúu-bit‚Äù is employed to represent whether
the cache line is re-used (hit) during its stay in these queues. When cache lines are evicted from
queues, they are moved to LRU-based cache ways only if they have been re-used. Otherwise,
these cache lines are evicted from queues directly. Under some cache behaviors, these queues can
eÔ¨Äectively prevent useful data from being evicted by inserted data, which are not reused.
ÓÄÄe Ô¨Årst level of queue is core-based so that the cache requests of diÔ¨Äerent cores are placed
into separated queues. Consequently, the cache replacements of diÔ¨Äerent cores are isolated from
each other in Ô¨Årst-level queues. Such a scheme can help to improve the utilization eÔ¨Éciency of
caches because a core with a high access rate can quickly evict the cache lines of another core from
www.ebook3000.com

4.2. 3D MAIN MEMORY STACKING
29
the shared cache without the isolation. ÓÄÄis scheme raises another issue of how to allocate the
size of the Ô¨Årst-level queue for each core. In order to solve this problem, the paper proposed an
adaptive multi-queue (AMQ) method, which leveraged the set-dueling principal [30] to dynam-
ically allocate the size of each Ô¨Årst-level queue. ÓÄÄere are several pre-deÔ¨Åned conÔ¨Ågurations of
queue sizes, and the AMQ can dynamically decide which conÔ¨Ågurations should be used accord-
ing to the real cache access pattern. A stability control mechanism was used to avoid unstable and
rapidly switching across many diÔ¨Äerent conÔ¨Ågurations of Ô¨Årst-level queues. ÓÄÄe results showed
that the AMQ management policy can improve the performance by 27:6% over the baseline of
simply using DRAM as LLC. ÓÄÄis method, however, incurs extra overhead of managing multiple
queues. For example, it requires a head pointer for each queue. In addition, it may not work well
when the cache associativity is not large enough (64 way was used in the paper) because the queue
size needs to be large enough to record the re-use of a cache line. Since the Ô¨Årst level queue is
core-based, it has a scalability limitation for the same reason.
4.2
3D MAIN MEMORY STACKING
In addition to the caches stacking, many studies have been exploring the potential of building
main memory on-chip with the help of 3D integration [31‚Äì33, 43‚Äì45].
ÓÄÄe most straightforward way is to implement a vertical bus across the layers of stacked
DRAM layers to the processor cores [43‚Äì45]. ÓÄÄe topology and overall architecture of the 3D
stacked memory is kept the same as in the traditional oÔ¨Ä-chip memory. Previous work shows that,
simply placing the DRAM provides a 34:7% increase in performance on memory-intensive work-
loads from SPEC2006 [31]. ÓÄÄe beneÔ¨Åts come from reducing wire delay between the memory
controller and main memory, and running the memory controller at a faster speed.
Due to the advantages of dense TSV, the interface between cores and stacked memories
can be implemented as wider buses, compared to that of 2D oÔ¨Ä-chip memory. After increasing
the width of bus from 64-bit (2D FSB width) to 64-byte TSV-based 3D buses, the performance
improvement over 2D case is increased to 71:8% [31]. Increasing the bus width does not fully ex-
ploit 3D stacking because the memory organization is still inherently two-dimensional. Tezzaron
Corporation has announced ‚Äútrue‚Äù 3D DRAMs where the individual bitcell arrays are stacked in a
3D fashion [46]. DiÔ¨Äerent from the prior approaches, these true 3D DRAMs isolate the DRAM
cells from peripheral circuitry. ÓÄÄe DRAM bitcells implemented in a NMOS technology is op-
timized for density, whereas the peripheral circuitry implemented on a CMOS layer optimized
for speed. ÓÄÄe combination of reducing bitline capacitance and using high-speed logic provides
signiÔ¨Åcant improvement in memory access time. Consequently, the true 3D structures result in
a 116:8% performance improvement over the 2D case [31].
ÓÄÄe high bandwidth of 3D stacking also makes it a viable option to increase the number
of ranks and memory controller interfaces (MC) of 3D stacked memories. Note that it is not
practical in oÔ¨Ä-chip due to pin and chip-count limitations [31]. Loh modiÔ¨Åed the banking of
L2 caches to route each set of L2 banks to one and only one MC so that the interleaving of L2

30
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
Figure 4.3: A structure view of 4 MSHR banks, 4 MCs, 16 Ranks [31].
cache occurs on 4KByte boundaries rather than 64 bytes. Such a structure makes L2 cache banks
‚Äúaligned‚Äù to their own MSHR and MC so that the communication is reduced when we have
multiple ranks and MCs. Figure 4.3 shows the structure of the 4-core processor with the aligned
L2 cache banks and corresponding MSHRs and MCs. ÓÄÄe results show that we can have another
1:75X speedup in addition to that provided by the true 3D structure.
Loh‚Äôs work also pointed out that the signiÔ¨Åcant increase in memory system performance
makes the L2 miss handling architecture (MHA) a new bottleneck. ÓÄÄe experiments showed that
simply increasing the capacity of MSHR cannot improve the performance consistently. Conse-
quently, a novel data structure called the Vector Bloom Filter with dynamic MSHR capacity
tuning was proposed to achieve a scalable MSHR. ÓÄÄe VBF-based MSHR is eÔ¨Äectively a hash-
table with linear probing, which speeds up the searching of MSHR. ÓÄÄe experimental results
show that the VBF-based dynamic MSHR can provide a robust, scalable, high-performance
L2 MHA for 3D-stacked memory architecture. Overall, a 23:0% improvement is observed on
memory-intensive workloads of SPEC2006 over the baseline L2 MSHR architecture for the
dual-MC (quad-MC) conÔ¨Åguration [31].
www.ebook3000.com

4.2. 3D MAIN MEMORY STACKING
31
Kgil et al. studied the advantages of 3D stacked main memory, with respect to the energy
eÔ¨Éciency of processors [43]. In their work, the structure of the DRAM-based main memory was
not changed and was stacked directly on the processor. ÓÄÄe bus width of the 3D main memory
was assumed to be up to 1024-bit. With the large bus width, one interesting observation was that
the 3D main memory could achieve a similar bandwidth as the L2 cache. Consequently, the pa-
per pointed out that the L2 cache was no longer necessary and the space could be saved to insert
more processing cores. With more processing cores, the frequency of the cores could be low down
without degrading computing bandwidth. ÓÄÄe energy eÔ¨Éciency could be increased because the
power consumption was reduced as the frequency decreased, especially for applications with high
thread level parallelism. ÓÄÄe paper provided comprehensive comparisons among diÔ¨Äerent conÔ¨Åg-
urations of 2D and 3D processors with similar die areas, with respect to processing bandwidth
and power consumption.
Woo et al. further explored the high bandwidth of 3D stacked main memory by modifying
the structure of the L2 cache and the corresponding interface between cache and 3D stacked
main memory [32]. ÓÄÄe paper Ô¨Årst revisited the impact of cache line size on cache miss rate when
there was no constraint to the bandwidth of main memory. ÓÄÄe experimental results show an
interesting conclusion that most modern applications will beneÔ¨Åt from a smaller L1 cache line
size, but a larger cache line is found to be helpful for a much larger L2 cache. Especially, the
maximum line size, an entire page (4KB), is found to be very eÔ¨Äective in a large cache. ÓÄÄen, the
paper proposed a technique named ‚ÄúSMART-3D‚Äù to employ the beneÔ¨Åts of large cache line size.
ÓÄÄe cache line of L2 cache was still kept as 64B, and read/write from L1 cache to L2 cache was
operated with the traditional 64B bus structures. ÓÄÄe bus between L2 cache and main memory,
however, was designed to be 4KB, and 64 cache lines could be Ô¨Ålled with data from main memory
at the same time. In order to achieve a high parallel data Ô¨Ålling, the L2 cache was divided into
64 subarrays, and one cache line from each subarray was written in parallel. Two cache eviction
policies were proposed so that either one or 64 cache lines could be evicted on demand. Besides the
modiÔ¨Åcation to L2 cache, the 3D stacked DRAM was carefully re-designed since a large number
(32K) of TSVs were required in SMART-3D. ÓÄÄe paper also noticed the potential exacerbation
of the false sharing problem caused by SMART-3D. ÓÄÄe cache coherence policy was modiÔ¨Åed
so that either one or 64 cache lines are fetched for diÔ¨Äerent cases.
ÓÄÄe experimental results show that the performance is improved with the help of SMART-
3D. For the single core, the average speedup of SMART-3D is 2.14x over 2D case for memory-
intensive benchmarks from SPEC2006, which is much higher than that of base 3D stacked
DRAM. In addition, using 1MB L2 and 3D stacked DRAM with SMART-3D achieves 2.31x
speedup for a two-core processor, whereas a 4M L2 cache without SMART-3D only achieves
1.97x over the 2D case. Furthermore, the analysis shows that SMART-3D can even lower the
energy consumption in the L2 cache and 3D DRAM because it reduces the total number of row
buÔ¨Äer misses.

32
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
It is known that the periodical refresh is required for DRAMs to maintain the information
stored in them. Since the read access is self-destructive, the refresh in a DRAM row involves
reading the stored data in each cell and immediately rewriting back to the same cell. ÓÄÄe refresh
incurs considerable power and bandwidth overhead. 3D integration is a promising technique that
beneÔ¨Åts DRAM design particularly from capacity and performance perspectives. Nevertheless,
3D stacked DRAMs potentially further exacerbates power and bandwidth overhead incurred by
the refresh process. Ghosh et al. proposed ‚ÄúSmart Refresh‚Äù [33], a low-cost technique imple-
mented in the memory controller to eliminate the unnecessary periodic refresh processes and
mitigate the energy consumption overhead in DRAMs. By employing a time-out counter in the
memory controller, for each memory row of a DRAM module, the DRAM row that was recently
accessed will not be refreshed during the next periodic refresh operation. ÓÄÄe simulation results
show that the proposed technique can reduce 53% of refresh operations on average with a 2GB
DRAM, and achieves 52:6% energy saving for refresh operations and 12:13% overall energy sav-
ing on average. An average 9:3% energy saving can be achieved for a 64MB 3D DRAM with
64ms refresh rate, and 6:8% energy saving can be achieved for the same DRAM capacity with
32ms refresh rate.
Chen et al. developed an architecture-level modeling tool, CACTI-3DD [47], which es-
timates timing, area, and power of the 3D die-stacked oÔ¨Ä-chip DRAM main memory. ÓÄÄis tool
is designed based on the DRAM main memory model of CACTI [48]; it introduces TSV mod-
els, improves the modeling of 2D oÔ¨Ä-chip DRAM main memory, and includes 3D integration
modeling on top of the original CACTI memory models. CACTI-3DD enables the analysis of
a full spectrum of 3D DRAM design from coarse-grained rank-level 3D stacking to bank-level
3D stacking. It allows memory designers to perform in-depth studies of 3D die-stacked DRAM
main memory, in terms of architecture-level tradeoÔ¨Äs of timing, area, and power. ÓÄÄeir study also
demonstrated the usage of the proposed tool by re-architecting the DRAM dies at a Ô¨Åne granu-
larity under the guidance of modeling results. ÓÄÄe redesigned 3D-stacked DRAM main memory
can achieve signiÔ¨Åcant timing and power improvements compared with coarse-grained baseline.
4.3
3D ON-CHIP STACKED MEMORY: CACHE OR MAIN
MEMORY?
3D/2.5D integration technology allows computer designers to integrate gigabytes of DRAM into
the processor package. Such on-package DRAMs introduce numerous attractive characteristics
to the memory system, such as high memory bandwidth, fast access speed, and large on-chip
memory capacity. Yet one nuisance facing system designers is how to make eÔ¨Écient use of them‚Äî
as a large last-level cache (LLC) or a fast main memory. ÓÄÄe two design options trade-oÔ¨Äamong
access speed, design complexity, and cost. Consequently, the answer is not straightforward. In this
chapter, we introduce existing designs that choose either design options, as well as a combination
of the two.
www.ebook3000.com

4.3. 3D ON-CHIP STACKED MEMORY: CACHE OR MAIN MEMORY?
33
4.3.1
ON-CHIP MAIN MEMORY
Dong et al. [49] observed that adopting the on-chip DRAM as LLC is less feasible than as
a portion of the main memory, due to the non-trivial design eÔ¨Äorts required to accommodate
access to a large cache capacity. Commodity DRAM dies have been optimized for cost and do
not employ specialized tag arrays that automatically determine a cache hit/miss and forward the
request to the corresponding data array. Because the size of a tag array can be a hundred megabytes
or more with a multi-gigabyte cache, storing tags on the processor die requires an impractically
large tag space. ÓÄÄe only alternative is to place the LLC tags inside the on-chip DRAM. However,
doing so will require two DRAM accesses upon each cache hit: one looking up the tag and the
other returning the data. Each cache hit will take approximately 2 of the time for a single access
to the on-package DRAM. ÓÄÄeir experimental results showed that there is almost no beneÔ¨Åt to
enlarge the LLC capacity in terms of the cache miss rate. While accessing the LLC and the main
memory in parallel can help hide the long LLC access latency, there is not suÔ¨Écient oÔ¨Ä-chip
bandwidth to access the oÔ¨Ä-chip memory speculatively and simultaneously with every reference
to an on-chip cache. Furthermore, oÔ¨Ä-chip references consume signiÔ¨Åcantly more power and
should generally be avoided when possible.
To avoid such issues, Dong et al. proposed a heterogeneous main memory architecture,
which leverages the on-chip 3D-stacked DRAM as a fast portion of the main memory. In ad-
dition, the main memory adopts four DDR3 channels connected to traditional oÔ¨Ä-chip DRAM
Dual In-line Memory Modules (DIMMs) to extend the total memory capacity. Figure 4.4 illus-
trates an overview of their heterogeneous main memory architecture. DRAM dies are placed be-
side the processor die using 2.5D integration technology. ÓÄÄe Ô¨Çip-chip system-in-package (SiP)
can provide a die-to-die bandwidth of at least 2 Tbps [50]. To reduce the memory access latency,
the on-chip DRAM is slightly modiÔ¨Åed based on commodity DRAM products to implement a
many-bank structure. Doing so further increased the signal I/O speed by taking the advantage
of the high-speed on-chip interconnects. ÓÄÄe proposed design did not employ a custom tag part
and adopted only a single on-chip DRAM design in order to reduce design cost and maximize
the eÔ¨Äective capacity of the on-package DRAM. ÓÄÄe on-chip memory controller is connected
to oÔ¨Ä-chip DIMMs through the conventional 64-bit DDRx bus and to the on-chip memory
through a customized memory bus. MSBs of physical memory addresses are used to decode the
target location. For example, if 1GB of 32-bit memory space is on-package, Addr≈í31::30¬ç D 00 is
mapped to on-chip memory while Addr≈í31::30¬ç D 01; 10; 11 is mapped to oÔ¨Ä-chip DIMMs.
ÓÄÄeir experiments with all ten workloads in NAS Parallel Benchmark Suite 3.3 [51] showed
directly mapping 1GB on-chip DRAM resources into the main memory space can always achieve
better performance than using the on-chip DRAM resources to add a new L4, when application
memory footprints Ô¨Åt into the on-chip memory. However, with workloads that have a much
larger working set, the performance improvement achieved by such static mapping was trivial. For
example, the performance improvement of DC.B is only 16% and that of FT.C is only 20.7%.
Both of them are less than the ones achieved by using on-chip DRAM as LLC.

34
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
To solve this issue, they proposed to add data migration functionality into the memory con-
troller so that frequently used data can remain on-chip with a higher probability. Compared with
other works on data migration [52‚Äì56]: (1) their data migration was implemented by introducing
another layer of address translation; (2) depending on the data granularity, they proposed either
a pure-hardware implementation or an OS-assisted implementation; and (3) a novel migration
algorithm was used to hide the data migration overhead. ÓÄÄey used the term macro page as the
data migration granularity, and the macro page size can be much larger than the typical 4KB page
size used in most operating systems.
In particular, their data migration algorithm is based on the hottest-coldest swapping
mechanism, which Ô¨Årst monitors the LRU (least recently used) on-chip macro page (the coldest)
and the MRU (most recently used) oÔ¨Ä-chip macro page (the hottest) during the latest period of
execution and then triggers the memory migration if the oÔ¨Ä-chip MRU page is accessed more
frequently than the on-chip LRU page after each monitoring epoch. ÓÄÄe migration algorithm
can be implemented in either a pure-hardware scheme or OS-assisted manner depending on the
migration granularity. ÓÄÄe pure-hardware solution is preferred when the macro page size is rela-
tively large so that the scale of macro page count is controllable within certain hardware resources.
If Ô¨Åner granularity of data migration is required, the number of macro pages becomes too large for
hardware to handle and OS-assisted scheme is used to track the access information of each macro
page. Basically, the functionality of the data migration is achieved by keeping an extra layer of ad-
dress translation that maps the physical address to the actual machine address. ÓÄÄe pure-hardware
scheme keeps the translation table in hardware while the OS-assisted scheme keeps it in software.
ÓÄÄeir evaluation results demonstrate how the heterogeneous main memory can use the
on-package memory eÔ¨Éciently and achieve the eÔ¨Äectiveness of 83% on average.
To manage such a space and move frequently accessed data to fast regions, we propose two
integrated memory controller schemes: a Ô¨Årst technique handles everything in hardware and our
second scheme takes assistance from the operating system.
4.3.2
3D-STACKED LLC
Loh and Hill [57] argue that an intelligent tag design can enable the use of on-chip DRAM
as a large-capacity LLC by eÔ¨Äectively reducing the tag overhead. ÓÄÄey performed a comprehen-
sive analysis on the challenges and ineÔ¨Éciencies found in prior work to implement large caches:
combining tags and data in the DRAM will cause the aforementioned performance and area
challenges; managing the data with large cache lines can lead to fragmentation, false-sharing,
and contention among multiple banks; sub-blocked caches still require a large tag storage space.
ÓÄÄey proposed a new DRAM cache architecture, which supports conventional cache block
sizes (e.g., 64-byte) at low performance overhead. While they adopted a combined tags and data
in the DRAM, they employed two sophisticated mechanisms to accelerate accesses to the DRAM
cache (shown in Fig. 4.5). First, they adopted a compound-access scheduling, which improves the
cache hit latency with a simple modiÔ¨Åcation of the memory controller‚Äôs scheduling algorithm. In
www.ebook3000.com

4.3. 3D ON-CHIP STACKED MEMORY: CACHE OR MAIN MEMORY?
35
Figure 4.4: ÓÄÄe conceptual view of the System-in-Package (SiP) solution with one microprocessor
die and nine DRAM dies connecting oÔ¨Ä-package DIMMs (one on-package DRAM die is for ECC).
their design, a single physical DRAM row holds both tags and data, as illustrated in Fig. 4.5(a).
ÓÄÄerefore, the memory controller can access the data and the corresponding tag with a single com-
pound operation. During access to the row, the memory controller prevents subsequent memory
requests from closing the row by reserving the row buÔ¨Äer. As a result, any updates to the data
and the corresponding tag will also hit in the row buÔ¨Äer. Second, they developed a MissMap
mechanism to avoid the DRAM cache access on a cache miss, by bypassing the DRAM cache
and directing the request immediately to the oÔ¨Ä-chip main memory. ÓÄÄeir solution is to employ
a MissMap data structure (Fig. 4.5(b)) which tracks the cache lines belonging to the same page
stored in the DRAM cache; each MissMap entry has a bit vector, each associated with a cache
line. A zero-bit or no corresponding entry in the MissMap indicates a DRAM cache miss. In
this case, the request will be directly issued to the oÔ¨Ä-chip main memory.
ÓÄÄeir experimental results showed that the compound-access scheduling improves the per-
formance of a DRAM L4 cache by half and provides 92.9% of the performance delivered by the
ideal SRAM-tag conÔ¨Åguration compared to having no DRAM L4 cache. With MissMap, their
design oÔ¨Äers 97.1% of the performance of the ideal SRAM-tag conÔ¨Åguration.
Qureshi and Loh [58] further improved the DRAM cache design with a latency-optimized
cache architecture that is diÔ¨Äerent from conventional cache organizations. ÓÄÄey observed that
Loh and Hill‚Äôs design can substantially increase the DRAM cache access latency by serializing
the tag accesses and the accesses to the MissMap. ÓÄÄeir design leverages DRAM bursts which
do not exist in conventional SRAM caches by streaming tag and data together in a single burst.
ÓÄÄeir design constructs the DRAM cache as direct-mapped cache, and tightly couples the tag and
data into one unit to prevent the tag serialization penalty. Doing this eÔ¨Äectively eliminated the

36
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
Figure 4.5: ÓÄÄe conceptual view of the proposed stacked DRAM cache management mechanisms.
(a) A single physical DRAM row holds both tags and data; (b) MissMap structure used to predict
whether a memory request will hit or miss in the stacked DRAM cache.
delay caused by tag serialization and improved performance by sacriÔ¨Åcing DRAM cache hit rate.
To address the performance penalty introduced by MissMap accesses, they proposed a memory
access predictor, which requires only 96-byte storage per core and provides a performance of 98%
of a perfect predictor. ÓÄÄeir evaluation across various benchmarks in SPEC2006 suite showed that
the proposed design outperforms Loh and Hill‚Äôs work by 24% and the ideal SRAM tag design by
8.9%.
4.3.3
DYNAMIC APPROACH
As shown in above sections, neither of the two design options appears to be substantially more
eÔ¨Écient than the other. One trade-oÔ¨Äbetween the two is main memory capacity and design
complexity: the on-chip main memory extends the main memory capacity, while requiring so-
phisticated OS modiÔ¨Åcations to support page migration between the on-chip and the oÔ¨Ä-chip
memory regions; the 3D-stacked LLC eliminates the need for OS modiÔ¨Åcations, but loses the
beneÔ¨Åt of extending the physical address space of the main memory.
Chou et al. [59] found a sweet spot in between. ÓÄÄey proposed an on-chip memory design,
namely CAMEO, which is managed as an LLC yet exposed to the OS as a portion of the physical
address space. Figure 4.6 illustrates a high-level view of the proposed CAMEO design compared
to previous studies. With CAMEO, the on-chip, 3D-stacked DRAM is visible to the OS and
managed at the cache line granularity (referred to as memory lines in the rest of this chapter).
ÓÄÄe key to exposing the stacked DRAM to the OS is the notion of Congruence Group, which
determines the memory lines that can be mapped to a given location in stacked DRAM. CAMEO
retains the most recently accessed memory lines. When a miss happens at the stacked memory,
CAMEO swaps the data fetched from the oÔ¨Ä-chip memory with an existing memory line in the
stacked DRAM. CAMEO restricts that memory lines in stacked DRAM can only be swapped
with an oÔ¨Ä-chip memory line from the same Congruence Group. ÓÄÄe number of Congruence
Groups is equal to the number of lines in stacked memory.
www.ebook3000.com

4.4. PICOSERVER
37
Figure 4.6: ÓÄÄe proposed CAMEO architecture (c) compared with prior designs of 3D-stacked
DRAM as LLC (a) and a portion of main memory (b) [59].
Such swapping sustains the latency and the bandwidth beneÔ¨Åts of the stacked memory.
However, it may change the physical location of a memory line without the notice of the OS. To
address this issue, the proposed design employs a line location table (LLT) to track the physical
location of all memory lines. CAMEO employs two methods to reduce the storage overhead
yet sustain low memory access latency. First, CAMEO stores the corresponding LLT entries in
stacked DRAM, residing in the same rows of these memory lines. Second, CAMEO adopts a
Line Location Predictor (LLP), a hardware structure less than 1KB per core, to predict whether a
memory line is exclusively stored in oÔ¨Ä-chip DRAMs and identify the potential physical addresses
of the oÔ¨Ä-chip memory lines. With the two method, CAMEO can access a memory line with
the latency of one memory reference, regardless of the location of the memory line. If a memory
line is predicted to be oÔ¨Ä-chip, CAMEO will fetch the predicted physical address in parallel with
LLT accesses.
Experiments conducted across various capacity- and latency-limited benchmarks show that
CAMEO improves system performance by 69%, while employing the stacked DRAM purely as
LLC and main memory only provide performance improvements of 51% and 44%, respectively.
Furthermore, CAMEO achieves 98% the performance of an idealized system which employs
1GB stacked DRAM both as an LLC and main memory capacity extension.
4.4
PICOSERVER
In this section, PicoServer [20], an architecture to reduce power and energy consumption using
3D stacking technology, is introduced.
ÓÄÄe basic idea is to stack on-chip DRAM main memory instead of using stacked memory
as a larger L2 cache. ÓÄÄe on-chip DRAM is connected to the L1 caches of each core through

38
4. COARSE-GRANULARITY 3D PROCESSOR DESIGN
shared bus architecture. It oÔ¨Äers wide low-latency buses to the processor cores and eliminates
the need for an L2 cache, whose silicon area is allocated to accommodate more cores. Increasing
the number of cores can help improve the computation throughput, while each core can run
at a much lower frequency, and therefore result in an energy-eÔ¨Écient many core design. ÓÄÄe
PicoServer is a chip multiprocessor, consists of several single issue in-order processors. Each core
runs at 500MHz, containing an instruction cache and a data cache, which uses a MESI cache
coherence protocol. ÓÄÄe study showed that the majority of the bus traÔ¨Éc is caused by cache miss
traÔ¨Éc instead of cache coherence due to the small cache for each core. ÓÄÄis is one motivation to
stack large on-chip DRAM, which is hundreds of megabytes, using 3D technology.
In PicoServer, a wide shared bus architecture is adopted to provide high memory bandwidth
and to fully take advantage of 3D stacking. A design space is explored by running simulations
varying the bus width on a single shared bus, which ranges from 128 bits to 2048 bits. ÓÄÄe impact
of bus width on the PicoServer is determined by measuring the network performance. ÓÄÄe results
showed that a relatively wide data bus is needed to achieve better performance and satisfy the
outstanding cache miss requests. ÓÄÄe bus traÔ¨Éc increase caused by narrowed bus width will result
in latency increase. Wide bus widths can speedup DMA transfer since more data can be copied
in one transaction. ÓÄÄe simulation also shows that a 1024-bit bus width is reasonable for diÔ¨Äerent
conÔ¨Ågurations including 4, 8, and 12 multiprocessors due to performance saturation at this point.
ÓÄÄe stacked on-chip DRAM contains four layers in order to obtain a total size of 256
MB, which may be enough depending on the workload. More on-chip DRAM capacity can be
obtained with aggressive die stacking. In order to fully take advantage of 3D stacking, it is nec-
essary to modify the conventional DDR2 DRAM interface for PicoServer‚Äôs 3D stacked on-chip
DRAM. In the conventional DDR2 DRAMs, a small pin count is assumed. In addition, address
multiplexing and burst mode transfer are used to compensate the limited number of pins. With
3D stacking, there is no need to address multiplexing so that the additional logic to latch and
mux narrow address/data can be removed. In servers with large network pipes such as PicoServer,
one common problem is how to handle large amount of packets that arrive at each second. In-
terrupt coalescing, a method to coalesce non-critical events to reduce the number of interrupts,
is one solution to solve this problem. However, even with this technique, the number of inter-
rupts received by a low frequency processor in PicoServer is huge. To address this issue, multiple
network interface controllers (NICs) with their interrupt lines are routed to a diÔ¨Äerent processor.
One NIC is inserted for two processors to fully utilize each processor. Such NIC should have
multiple interface IP addresses or an intelligent method to load balancing packets to multiple
processors. In addition, it needs to keep track of network protocol states at the session level.
ÓÄÄermal evaluation showed that the maximum junction temperature increase is about 5 to
10¬∞C in 5-layer PicoServer architecture. ÓÄÄis comes from power and energy reduction caused by
core clock frequency reduction and improvement on high network bandwidth.
www.ebook3000.com

39
C H A P T E R
5
3D GPU Architecture
Graphics processing units (GPUs) are an attractive solution for both graphics and general purpose
workloads, which demand high computational performance. 3D integration is an attractive tech-
nology in developing high-performance, power-eÔ¨Écient GPU systems. Recently, 3D integration
has been explored by both academia and industry as a promising solution to improve GPU per-
formance and address increasingly critical GPU power issues. In this section, we will introduce
recent research and implementation eÔ¨Äorts in 3D GPU system design.
5.1
3D-STACKED GPU MEMORY
3D-stacked GPU memory can provide an order of magnitude more connections between the
GPU and the graphics memory, oÔ¨Äering much higher memory bandwidth than traditional
GDDR memories. Keckler et al. [60] envisioned that 3D stacking is a promising solution to
creating dense DRAM cubes or combining GPU and DRAM stacking with interposers to ac-
commodate high memory bandwidth requirement of parallel computing applications running
on GPUs. Zou et al. [61] demonstrated the beneÔ¨Åts of leveraging 3D integration to develop
in-package nonvolatile memories in CPU/GPU heterogeneous systems. Lee et al. [62] demon-
strated that 3D integration technologies can signiÔ¨Åcantly reduce the power dissipation of buses
in GPUs; the authors developed a GPU design that can achieve up to 21.5% power reduction
compared with the 2D baseline.
GPU exploits extreme multithreading to target high-throughput [63], [64]. For example,
AMD Radeon‚Ñ¢HD 7970 employs 20,480 threads interleaved across 32 compute units [63]. To
accommodate such high-throughput demands, the power consumption of GPU systems con-
tinues to increase. Zhao et al. [65] observed that up to 30% of the total GPU system power is
consumed by its oÔ¨Ä-chip DRAM, i.e., graphics memory. As such, reducing the graphics memory
power is critical to mitigate the power challenges facing GPU systems. To optimize GPU sys-
tem energy eÔ¨Éciency, they proposed a ‚Äú3D+2.5D‚Äù system, where the DRAM itself is 3D stacked
memory with through-silicon vias (TSVs), whereas the DRAM and the GPU processor are in-
tegrated with the interposer solution (2.5D).
ÓÄÄe proposed design explored two key power saving opportunities introduced by such in-
package graphics memory. First, in-package graphics memory has a wide memory interface,
which enables memory power reduction without degrading the peak memory bandwidth. ÓÄÄe
peak memory bandwidth is determined by the bus width and the frequency of the memory
interface. In-package graphics memory has a bus width several times wider than conventional

40
5. 3D GPU ARCHITECTURE
GDDRs. ÓÄÄerefore, we can reduce the memory frequency, and still achieve equivalent or even
higher peak memory bandwidth compared with conventional GDDR memories. One opportu-
nity for memory power reduction is therefore to scale down the memory‚Äôs supply voltage corre-
sponding to the frequency reduction. Second, in-package DRAMs do not require on-die termi-
nation [66]. ÓÄÄerefore, we can further reduce graphics memory power by eliminating the on-die
termination resistors.
Figure 5.1 illustrates the peak memory bandwidth and maximum total power consumption
of 2GB graphics memory with various memory interface conÔ¨Ågurations. ÓÄÄe graphics memo-
ries considered here are electrically similar to GDDR DRAMs except for the wider buses. ÓÄÄe
memory interface conÔ¨Åguration is deÔ¨Åned as a set of parameters (bus width, frequency, Vdd). ÓÄÄe
supply voltage (Vdd) is scaled appropriately to support the given memory interface clock fre-
quency. DRAMs with per-channel bus widths of 16 and 32 bits are evaluated as oÔ¨Ä-chip GDDR5
memory. DRAMs with per-channel bus widths of 64 bits or wider are evaluated as in-package
graphics memory. ÓÄÄe bars in Figs. 5.1(a)‚Äì(e) show the maximum power consumption with sev-
eral conÔ¨Ågurations that adopt various bus widths and clock speeds while maintaining the same
peak memory bandwidth. As illustrated in Figs. 5.1(a)‚Äì(c), the memory power follows U-shaped
bathtub curves at low peak memory bandwidths of 144 GB/s, 180 GB/s, and 288 GB/s. With
wider memory buses, lower frequency allows us to scale down the supply voltage which directly
results in a power reduction. However, the power of I/O output drivers keeps increasing along
with the increase of the bus width. When the bus width is increased to 256 bits, the I/O power
component starts to dominate the total memory power and Ô¨Ånally overwhelms the power ben-
eÔ¨Åts of voltage and frequency (VF) scaling. With peak memory bandwidth of 144 GB/s, 180
GB/s, and 288 GB/s, the optimal bus conÔ¨Åguration is 128-bit wide. With a higher peak memory
bandwidth of 360 GB/s and 720 GB/s (Figs. 5.1(d) and (e)), the memory power will continue to
decrease even when the bus width reaches 256 bits. In this case, the optimal bus width is achieved
by even wider bus conÔ¨Ågurations. In both cases, the optimal bus conÔ¨Ågurations fall in the range
of in-package graphics memory with wide buses.
ÓÄÄe proposed energy-eÔ¨Écient GPU design leverages such power beneÔ¨Åts of in-package
graphics memory. Figure 5.3 depicts an overview of the proposed ‚Äú3D+2.5D‚Äù GPU system ar-
chitecture. DiÔ¨Äerent from conventional GPU systems (shown in Fig. 5.2), which employ oÔ¨Ä-chip
DRAMs as graphics memory, the proposed design integrates GPU processor and DRAMs with
silicon interposer technology. Multiple layers of memory cells can be stacked on top of a logic
layer with TSV technology. ÓÄÄe silicon interposer area required by integrating DRAMs can be
estimated based on DRAM density. DRAM density at 50nm is 27:9 Mb=mm2 [67]. ÓÄÄe density
becomes 43:6 Mb=mm2 with a technology node of 40nm. In this case, we can integrate 6GB of
DRAMs within 281:8 mm2 of silicon interposer area by stacking the DRAMs into four layers of
memory cells and one logic layer.
ÓÄÄermal tolerance can be one concern of processor-memory integration. Zhao et al. stud-
ied the integration of GPU processor and DRAMs with both vertical 3D memory stacking and
www.ebook3000.com

5.1. 3D-STACKED GPU MEMORY
41
Figure5.1: Peak memory bandwidth and maximum total DRAM power consumption of 2GB graph-
ics memory with various interface conÔ¨Ågurations.
silicon interposer technologies. ÓÄÄey performed thermal analysis with a GPU system conÔ¨Ågura-
tion based on NVIDIA Quadro 6000 [64]. ÓÄÄey computed the maximum power consumption of
GPU processors and memory controllers by subtracting the DRAM power from the reported
maximum power consumption of Quadro 6000 [64], resulting in 136W. ÓÄÄe power of 6GB
DRAM is calculated as 68W, based on Hynix‚Äôs GDDR5 memory [68]. ÓÄÄe areas of diÔ¨Äerent
GPU components are obtained from the GPU die photo, which has a 529 mm2 die area. ÓÄÄey
assume that the ambient temperature to be 40 ƒ±C. ÓÄÄey used the HotSpot thermal simulation
tool [69] to conduct the analysis. ÓÄÄe maximum steady-state temperature of the GPU (without
DRAMs) is 71:2 ƒ±C. With 6GB interposer-mounted DRAMs (four layers of memory cells plus
one layer of logic) placed beside the GPU processor as shown in Fig. 5.3, the maximum temper-
ature is 76:6 ƒ±C. ÓÄÄus, it is feasible to employ interposer-based memory integration. Vertically
stacking memories on top of the GPU incurs much greater temperature increases than a silicon
interposer-based approach. By stacking the same DRAMs on top of the GPU processor, the
temperature rises to 83:8 ƒ±C, a 7:2 ƒ±C increase compared to the interposer-mounted DRAMs.
Moreover, the temperature rise can further increase system-wide power consumption due to the
temperature dependence of leakage power. ÓÄÄerefore, the proposed GPU system design employs
2.5D (interposer-based) memory integration.
ÓÄÄe memory interface with Ô¨Åxed bus width and frequency cannot satisfy various mem-
ory utilization requirements of various applications. Even a single application can have variable

42
5. 3D GPU ARCHITECTURE
Figure 5.2: Conventional GPU system with oÔ¨Ä-chip GDDRs.
Figure 5.3: Overview of the proposed ‚Äú3D+2.5D‚Äù GPU system architecture.
memory access patterns during execution. To accommodate the varying graphics memory access
demands, the proposed design developed a reconÔ¨Ågurable memory interface, which can dynami-
cally adapt to the demands of various applications based on dynamically observed memory access
and performance information. To maximize the system energy eÔ¨Éciency, the proposed design
conÔ¨Ågures the memory interface to minimize the DRAM power and maintain the system in-
structions per cycle (IPC) rate. To improve the system throughput under a given power budget,
the proposed design co-optimizes the memory conÔ¨Åguration and the GPU clock frequency by
shifting power saved from the memory interface over the GPU. ÓÄÄe proposed design employs two
reconÔ¨Åguration mechanisms, EOpt and PerfOpt, to optimize the system energy eÔ¨Éciency and
system throughput, respectively. ÓÄÄese reconÔ¨Åguration mechanisms can eÔ¨Äectively accommodate
both memory-intensive and compute-intensive applications.
EOpt: ÓÄÄe proposed design adopts a reconÔ¨Ågurable memory interface that can dynamically de-
tect the various memory access patterns of the two types, and apply appropriate strategies to
achieve their design goals. Both IPC and memory power will be aÔ¨Äected by the change of mem-
ory interface during memory-intensive execution periods: decreasing the memory frequency (and
consequently increasing the memory access latency) results in signiÔ¨Åcant IPC degradation, even
www.ebook3000.com

5.1. 3D-STACKED GPU MEMORY
43
though we provide wider buses to keep the same peak memory bandwidth; the corresponding
IPC typically stays much lower than that of compute-intensive periods, due to the continuous
memory demands that signiÔ¨Åcantly slow down the instruction execution. ÓÄÄerefore, they choose
conÔ¨Ågurations that maintain high memory clock frequencies to minimize the IPC degradation.
Given the memory frequency constraint, the bus width is then conÔ¨Ågured to minimize the mem-
ory power consumption. During compute-intensive execution periods, IPC is stable with various
memory interface conÔ¨Ågurations. During these execution periods, the proposed design adopted
the memory frequency and bus width conÔ¨Åguration that minimizes the memory power.
PerfOpt explores GPU system performance (instruction throughput, i.e., the executed instruc-
tions per second) optimization under a given power budget. During compute-intensive execution
periods, PerfOpt always employs the memory interface conÔ¨Åguration that minimizes the DRAM
power. Any power saved is transferred to scale up the GPU core clock frequency/supply voltage
to improve the system performance. During memory-intensive periods, their design employs two
strategies. First, because the memory interface conÔ¨Åguration directly aÔ¨Äects system performance
during memory-intensive periods, they choose the memory conÔ¨Åguration that delivers the high-
est system performance while staying within the system power budget. Second, sometimes an
application can be relatively memory-intensive while still having signiÔ¨Åcant compute needs as
well. In these cases, reconÔ¨Åguring the memory interface to free up more power for the GPU can
still result in a net performance beneÔ¨Åt despite the reduced raw memory performance. Based on
the predicted beneÔ¨Åt, PerfOpt will choose the better of these two strategies.
Implementation: Figure 5.4 illustrates the hardware design of the proposed reconÔ¨Ågurable mem-
ory interface. ÓÄÄe design made several modiÔ¨Åcations to the interface between the GPU processor
and the 3D die-stacking graphics memories, including adding a central controller, control sig-
nals to the bus drivers, and controls for dynamic VF scaling. ÓÄÄe central controller in Fig. 5.4(a)
is used to collect global information of both GPU performance and memory accesses. A vec-
tor of counters are maintained in the controller, including instruction counter, cycle counter, and
memory access counters, to collect performance and memory access information from either GPU
hardware performance counters or memory controllers. A threshold register vector is used to store
various thresholds and initial values described in reconÔ¨Åguration mechanism. ÓÄÄe calculator mod-
ule calculates the system energy eÔ¨Éciency based on the collected performance information and
the estimated power consumption. ÓÄÄe results are stored in result registers for comparison. Fig-
ures 5.4(b) and (c) illustrate their data bus implementation. ÓÄÄe basic topology of a bi-directional
point-to-point data bus is a set of transmission lines, with transmitter and receiving devices at
both ends of each bit. Control signals of I/O drivers are connected to the central controller. ÓÄÄese
control signals switch the drivers in the transmitters on and oÔ¨Äto change the bus width.
ÓÄÄe experimental results show that even with a static (no reconÔ¨Åguration) in-package
graphics memory solution, the energy eÔ¨Éciency (performance per Watt) of the GPU system can
be improved by up to 21%. Of course, Ô¨Åxing the system bandwidth to be equal to the oÔ¨Ä-chip
solution does not really take advantage of the wide interface provided by the in-package memory.

44
5. 3D GPU ARCHITECTURE
Figure 5.4: Hardware implementation of reconÔ¨Ågurable memory interface. (a) Central controller;
(b) connection between memory controller and 3D die-stacking graphics memory (one channel); (c)
reconÔ¨Ågurable data bus.
By increasing the memory interface clock speed to provide bandwidths of 360 GB/s and 720 GB/s
(note even at these higher bandwidths, the power of in-package memory can still be lower than
the oÔ¨Ä-chip GDDR5 due to the lower clock frequency), performance on the memory-intensive
applications can be brought back up.
With EOpt, system power with almost all applications is reduced, and by an average of up
to 12%. ÓÄÄe overall system performance-per-Watt rate is improved for all the benchmarks. ÓÄÄe
improvement of the non-memory intensive applications (16%) is not as great as for the memory-
intensive applications (44%). ÓÄÄe reason is that the system throughput is signiÔ¨Åcantly improved
with memory-intensive applications, but almost stays the same with non-memory intensive appli-
cations. Overall, EOpt improves system energy eÔ¨Éciency of all baseline conÔ¨Ågurations, including
those peak memory bandwidth conÔ¨Ågurations. Across all low- and high-intensity applications,
the performance-per-Watt improves by 26% on average.
Evaluation with a variety of GPU system power budgets shows that PerfOpt can adjust the
memory power consumption to Ô¨Åt the application memory needs, and that the saved power can be
eÔ¨Äectively redeployed to improve the GPU core performance up to 31%. For non-memory inten-
sive applications, a higher power budget directly leads to more performance improvement. Since
we always conÔ¨Ågure the memory interface to minimize the DRAM power, extra power is avail-
able to increase GPU core clock frequency. ÓÄÄe throughput improvement of memory-intensive
applications is not as signiÔ¨Åcant as the non-memory intensive applications, yet PerfOpt still yields
an average of 8% throughput improvement with these three most memory-intensive applications
under the power budget of 220W, and more improvement with higher power budgets.
www.ebook3000.com

5.2. 3D-STACKED GPU PROCESSOR
45
5.2
3D-STACKED GPU PROCESSOR
One aggressive approach to adopt 3D integration in GPU system design is to stack GPU caches
and cores using 3D technology. For example, Maashri et al. [70] proposed a 3D GPU design with
cache stacking. ÓÄÄe work performed a comprehensive study across performance, cost, power, and
thermal of 3D-stacked GPU system. ÓÄÄe study showed that 3D-stacked GPU system can sustain
low cache access latency, while increasing the cache capacity. It also showed that 3D-stacked GPU
system can achieve up to 45% speedup over the 2D planar architecture without signiÔ¨Åcant increase
of peak temperature.
Future Products.
In-package graphics memory has been considered one of the most promising
and practical solutions for energy-eÔ¨Écient GPU systems. Top GPU vendors, such as NVIDIA
and AMD, have recently invested signiÔ¨Åcant eÔ¨Äort in investigating in-package graphics mem-
ory in their next generation products. For example, NVIDIA recently announced that their next
generation Pascal GPU products will adopt 3D-stacked memory [3]. In the roadmap, NVIDIA
plans to employ stacks of DRAM chips into dense modules with wide interfaces, and integrate
them inside the same package as the GPU processor. Doing so will not only boost GPU system
throughput and energy eÔ¨Éciency by allowing GPU processors to quickly access data from mem-
ory, but also allows the vendor to build more compact GPUs with much larger graphics memory
capacity.

www.ebook3000.com

47
C H A P T E R
6
3D Network-on-Chip
Network-on-chip (NoC) is a general purpose on-chip interconnection network architecture that
is proposed to replace the traditional design-speciÔ¨Åc global on-chip wiring, by using switching
fabrics or routers to connect processor cores or processing elements (PEs). Typically, the PEs
communicate with each other using a packet-switched protocol, as illustrated by Fig. 6.1.
Figure 6.1: For a network-on-chip (NoC) architecture, processing elements (PEs) are connected via
a packet-based network.
A typical 2D NoC consists of a number of Processing Elements (PE) arranged in a grid-like
mesh structure, much like a Manhattan grid. ÓÄÄe PEs are interconnected through an underlying
packet-based network fabric. Each PE interfaces to a network router through a Network Interface
Controller (NIC). Each router is, in turn, connected to four adjacent routers, one in each cardinal
direction. A generic NoC router architecture is illustrated in Fig. 6.2. ÓÄÄe router has P input and
P output channels/ports. As previously mentioned, P D 5 in a typical 2D NoC router, giving rise
to a 5  5 crossbar. ÓÄÄe Routing Computation unit, RC, operates on the header Ô¨Çit (a Ô¨Çit is the
smallest unit of Ô¨Çow control; one packet is composed of a number of Ô¨Çits) of an incoming packet
and, based on the packet‚Äôs destination, dictates the appropriate output Physical Channel/port
(PC) and/or valid Virtual Channels (VC) within the selected output PC. ÓÄÄe routing can be
deterministic or adaptive. ÓÄÄe Virtual channel Allocation unit (VA) arbitrates between all packets
competing for access to the same output VCs and chooses a winner. ÓÄÄe Switch Allocation unit
(SA) arbitrates between all VCs requesting access to the crossbar. ÓÄÄe winning Ô¨Çits can then

48
6. 3D NETWORK-ON-CHIP
traverse the crossbar and move on to their respective output links. Without loss of generality, all
implementations in this work employ two-stage routers.
Figure 6.2: A generic NoC router architecture.
As a communication fabric, network-on-chip takes advantage of 3D integration in two
ways: a) wire-length reduction which allows links to run faster and more power eÔ¨Éciently and
b) the implementation of truly 3D network topologies which improves connectivity and reduces
network diameter, and therefore promotes both performance and power.
Researchers have studied various NoC router design with 3D integration technology. For
example, various design options the NoC router for 3D NoC has been investigated: 1) symmetric
NoC router design with a simple extension to the 2D NoC router; 2) NoC-bus hybrid router
design which leverage the inherent asymmetry in the delays in a 3D architecture between the fast
vertical interconnects and the horizontal interconnects that connect neighboring cores; 3) true 3D
router design with major modiÔ¨Åcation as dimensionally decomposed router [71]; 4) multi-layer
3D NoC router design which partitions a single router to multiple layers to boost the performance
and reduce the power consumption [72]. 3D NoC topology design was also investigated [73, 74].
Even though both 3D integrated circuits and NoCs [75, 76] are proposed as alternatives
for the interconnect scaling demands, the challenges of combining both approaches to design
three-dimensional NOCs have not been addressed until recently [72, 73, 77‚Äì82].
ÓÄÄis chapter gives a brief introduction on the exploration of possible architectural designs
for three-dimensional NoC architectures, and discusses the trade-oÔ¨Äs among various design op-
tions.
www.ebook3000.com

6.1. 3D NOC ROUTER DESIGN
49
Table 6.1: Area and power comparison of the crossbar switches implemented in 90 nm technology
Crossbar Type
Area
Power (500 Mhz)
55 Crossbar
8523 m2
4.21 mW
66 Crossbar
11579 m2
5.06 mW
77 Crossbar
17289 m2
9.41 mW
6.1
3D NOC ROUTER DESIGN
ÓÄÄe NoC router design is critical for NoC architecture. Redesigning a router with the 3D tech-
nology poses interesting design challenges. Given that on-chip networks are severely constrained
in terms of area and power resources, while at the same time they are expected to provide ultra-
low latency, the key issue is to identify a reasonable trade-oÔ¨Äbetween these contradictory design
goals. In this section, we explore the extension of baseline 2D NoC router implementation into
the third dimension, while considering the aforementioned constraints.
Symmetric NoC Router Design.
ÓÄÄe natural and simplest extension to the baseline NoC router
to facilitate a 3D layout is simply adding two additional physical ports to each router; one for Up
and one for Down, along with the associated buÔ¨Äers, arbiters (VC arbiters and Switch Arbiters),
and crossbar extension. We can extend a traditional NoC fabric to the third dimension by simply
adding such routers at each layer (called a symmetric NoC, due to symmetry of routing in all
directions). We call this architecture a 3D Symmetric NoC, since both intra- and inter-layer
movement bear identical characteristics as hop-by-hop traversal. For example, moving from the
bottom layer of a 4-layer chip to the top layer requires three network hops. ÓÄÄis architecture, while
simple to implement, has a few major inherent drawbacks.
‚Ä¢ It wastes the beneÔ¨Åcial attribute of a negligible inter-wafer distance in 3D chips (for exam-
ple, the thickness of a die could be as small as 10s of m). Since traveling in the vertical
dimension is multi-hop, it takes as much time as moving within each layer. Of course, the
average number of hops between a source and a destination does decrease as a result of
folding a 2D design into multiple stacked layers, but inter-layer and intra-layer hops are
indistinguishable. Furthermore, each Ô¨Çit must undergo buÔ¨Äering and arbitration at every
hop, adding to the overall delay in moving up/down the layers.
‚Ä¢ ÓÄÄe addition of two extra ports necessitates a larger 77 crossbar. Crossbars scale upward
very ineÔ¨Éciently, as illustrated in Table 6.1. ÓÄÄis table includes the area and power budgets
of all crossbar types investigated in this section, based on synthesized implementations in
90 nm technology. Clearly, a 77 crossbar incurs signiÔ¨Åcant area and power overhead over
all other architectures. ÓÄÄerefore, the 3D Symmetric NoC implementation is a somewhat
naive extension to the baseline 2D network.

50
6. 3D NETWORK-ON-CHIP
3D NoC-Bus Hybrid Router Design [77].
ÓÄÄere is an inherent asymmetry in the delays in a 3D
architecture between the fast vertical interconnects and the horizontal interconnects that connect
neighboring cores due to diÔ¨Äerences in wire lengths (a few tens of m in the vertical direction
as compared to a few thousands m in the horizontal direction). Consequently, a symmetric
NoC architecture with multi-hop communication in the vertical (inter-layer) dimension is not
desirable.
Given the very small inter-layer distance, single-hop communication is, in fact, feasible.
ÓÄÄis technique revolves around the fact that vertical distance is negligible compared to intra-
layer distances; the bus can provide single-hop traversal between any two layers. ÓÄÄis realization
opens the door to a very popular shared-medium interconnect, the bus. ÓÄÄe NoC router can be
hybridized with a bus link in the vertical dimension to create a 3D NoC-Bus Hybrid structure,
as shown in Fig. 6.3. ÓÄÄis hybrid system provides both performance and area beneÔ¨Åts. Instead
of an unwieldy 77 crossbar, it requires a 66 crossbar (Fig. 6.3), since the bus adds a single
additional port to the generic 2D 55 crossbar. ÓÄÄe additional link forms the interface between
the NoC domain and the bus (vertical) domain. ÓÄÄe bus link has its own dedicated queue, which
is controlled by a central arbiter. Flits from diÔ¨Äerent layers wishing to move up/down should
arbitrate for access to the shared medium.
Communication
Pillar (b-bit 
dTDMA Bus 
spanning all 
layers)
Cache Bank 
or CPU Node
Pillar Node
Layers of 
3D Chip
4 Communication Pillars 
assumed here
n layers
~middle layer
2
Processing
Element
(Cache Bank 
or CPU)
NIC
b bits
Input Buffer
Output Buffer
dTDMA Bus
NoC
Figure 6.3: ÓÄÄe 3D bus-hybrid NoC proposed by Li et al. [29]. (a) ÓÄÄe vertical buses interconnecting
all nodes in a cylinder; (b) the dTDMA bus and the arbiter; (c) the 3D router with the capability for
vertical communication.
Despite the marked beneÔ¨Åts over the 3D Symmetric NoC router, the bus approach also
suÔ¨Äers from a major drawback: it does not allow concurrent communication in the third dimen-
sion. Since the bus is a shared medium, it can only be used by a single Ô¨Çit at any given time.
ÓÄÄis severely increases contention and blocking probability under high network load. ÓÄÄerefore,
while single-hop vertical communication does improve performance in terms of overall latency,
inter-layer bandwidth suÔ¨Äers. More details on the 3D NoC-Bus hybrid architecture can be found
in [77].
www.ebook3000.com

6.1. 3D NOC ROUTER DESIGN
51
True 3D Router Design. Moving beyond the previous options, we can envision a true 3D
crossbar implementation, which enables seamless integration of the vertical links in the overall
router operation. ÓÄÄe traditional deÔ¨Ånition of a crossbar‚Äîin the context of a 2D physical layout‚Äî
is a switch in which each input is connected to each output through a single connection point.
However, extending this deÔ¨Ånition to a physical 3D structure would imply a switch of enormous
complexity and size (given the increased numbers of input- and output-port pairs associated with
the various layers). ÓÄÄerefore, we chose a simpler structure which can accommodate the inter-
connection of an input to an output port through more than one connection point. While such a
conÔ¨Åguration can be viewed as a multi-stage switching network, we still call this structure a cross-
bar for the sake of simplicity. ÓÄÄe vertical links are now embedded in the crossbar and extend to
all layers. ÓÄÄis implies the use of a 55 crossbar, since no additional physical channels need to be
dedicated for inter-layer communication.
As shown in Table 6.1, a 55 crossbar is signiÔ¨Åcantly smaller and less power-hungry than
the 66 crossbar of the 3D NoC-Bus Hybrid and the 77 crossbar of the 3D Symmetric NoC.
Interconnection between the various links in a 3D crossbar would have to be provided by dedicated
connection boxes at each layer. ÓÄÄese connecting points can facilitate linkage between vertical and
horizontal channels, allowing Ô¨Çexible Ô¨Çit traversal within the 3D crossbar.
ÓÄÄe 2D crossbars of all layers are physically fused into one single three-dimensional cross-
bar. Multiple internal paths are present, and a traveling Ô¨Çit goes through a number of switching
points and links between the input and output ports. Moreover, Ô¨Çits re-entering another layer
do not go through an intermediate buÔ¨Äer; instead, they directly connect to the output port of
the destination layer. For example, a Ô¨Çit can move from the western input port of layer 2 to the
northern output port of layer 4 in a single hop.
However, despite this encouraging result, there is an opposite side to the coin which paints
a rather bleak picture. Adding a large number of vertical links in a 3D crossbar to increase NoC
connectivity results in increased path diversity. ÓÄÄis translates into multiple possible paths be-
tween source and destination pairs. While this increased diversity may initially look like a positive
attribute, it actually leads to a dramatic increase in the complexity of the central arbiter, which co-
ordinates inter-layer communication in the 3D crossbar. ÓÄÄe arbiter now needs to decide between
a multitude of possible interconnections, and requires an excessive number of control signals to
enable all these interconnections. A full crossbar with its overwhelming control and coordination
complexity poses a stark contrast to this frugal and highly eÔ¨Écient design methodology. More-
over, the redundancy oÔ¨Äered by the full connectivity is rarely utilized by real-world workloads,
and is, in fact, design overkill [78].
Even if the arbiter functionality can be distributed to multiple smaller arbiters, then the
coordination between these arbiters becomes complex and time-consuming. Alternatively, if dy-
namism is sacriÔ¨Åced in favor of static path assignments, the exploration space is still daunting in
deciding how to eÔ¨Éciently assign those paths to each source-destination pair. Furthermore, a full
3D crossbar implies 25 (i.e., 55) Connection Boxes per layer. A four-layer design would, there-

52
6. 3D NETWORK-ON-CHIP
fore, require 100 CBs! Given that each CB consists of 6 transistors, the whole crossbar structure
would need 600 control signals for the pass transistors alone! Such control and wiring complex-
ity would most certainly dominate the whole operation of the NoC router. Pre-programming
static control sequences for all possible input-output combinations would result in an oversize ta-
ble/index; searching through such table would incur signiÔ¨Åcant delays, as well as area and power
overhead. ÓÄÄe vast number of possible connections hinders the otherwise streamlined function-
ality of the switch. Note that the prevailing tendency in NoC router design is to minimize op-
erational complexity in order to facilitate very short pipeline lengths and very high frequency. A
full crossbar with its overwhelming control and coordination complexity poses a stark contrast
to this frugal and highly eÔ¨Écient design methodology. Moreover, the redundancy oÔ¨Äered by the
full connectivity is rarely utilized by real-world workloads, and is, in fact, design overkill [78].
3D Dimensionally Decomposed NoC Router Design [78].
Given the tight latency and area
constraints in NoC routers, vertical (inter-layer) arbitration should be kept as simple as possible.
Consequently, a true 3D router design, as described in the previous subsection, is not a realistic
option. ÓÄÄe design complexity can be reduced by using a limited amount of inter-layer links. ÓÄÄis
subsection describes a modular 3D decomposable router (called Row-Column-Vertical (RoCoVe)
Router) [78].
In a typical two-dimensional NoC router, the 55 crossbar has Ô¨Åve inputs/outputs corre-
spond to the four cardinal directions and the connection from the local PE. ÓÄÄe crossbar is the
major contributor to the latency and area of a router. It has been shown [83] that through the
use of a preliminary switching process known as Guided Flit Queuing, incoming traÔ¨Éc can be
decomposed into two independent streams: (a) East-West traÔ¨Éc (i.e., packet movement in the
X dimension), and (b) North-South traÔ¨Éc (i.e., packet movement in the Y dimension). Such
segregation of traÔ¨Éc Ô¨Çow allows the use of smaller crossbars and the isolation of the two Ô¨Çows in
two independent router sub-modules, which are called Row Module and Column Module [83].
With the same idea of traÔ¨Éc decomposition, the traÔ¨Éc Ô¨Çow in 3D NoC can be decomposed
into three independent streams: with a third traÔ¨Éc Ô¨Çow in the Z dimension (i.e., inter-layer
communication). An additional module is required to handle all traÔ¨Éc in the third dimension, and
this module is called Vertical Module. In addition, there must be links between Vertical Module
and Row/Column Modules, to allow the movement of packets from the Vertical Module to the
Row Module and Column Module. Consequently, such a dimensionally decomposed approach
allows for a much smaller crossbar design (42), resulting in a much faster and power-eÔ¨Écient
3D NoC router design. ÓÄÄe architectural view of such 3D dimensionally decomposed NoC router
design is shown in Fig. 6.4(b). More details can be found in [78].
Multi-layer 3D NoC Router Design [72].
All the 3D router design options discussed earlier
(symmetric 3D router, 3D NoC-Bus hybrid router, true 3D router, and 3D dimensionally de-
composed router) are based on the assumption that the processing element (PE) (which could
be a processor core or a cache bank) itself is still a 2D design. For a Ô¨Åne-granularity design of
www.ebook3000.com

6.1. 3D NOC ROUTER DESIGN
53
Figure 6.4: Two 3D NoC router design [84]. (a) A true 3D crossbar; (b) the dimensionally decom-
posed (DimDe) architecture.
3D design, one can split a PE across multiple layers. For example, 3D cache design [40] and
3D functional units [5] have been proposed before. Consequently, a PE in the NoC architecture
is possible to be implemented with such Ô¨Åne-granularity approach. Although such a multi-layer
stacking of a PE is considered aggressive in the current technology, it could be possible with
monolithic 3D integration or with very small TSVs.
With such a multi-layer stacking of processing elements in the NoC architecture, it is
necessary to design a multi-layer 3D router that is designed to span across multiple layers of a 3D
chip. Logically, such NoC architecture with multi-layer PEs and multi-layer routers is identical
to the traditional 2D NoC case with the same number of nodes albeit the smaller area of each
PE and router and the shorter distance between routers. Consequently, the design of a multi-
layer router requires no additional functionality as compared to a 2D router and only requires
distribution of the functionality across multiple layers.

54
6. 3D NETWORK-ON-CHIP
In such multi-layer router design, router components are identiÔ¨Åed as separable and non-
separable modules. ÓÄÄe separable modules are the input buÔ¨Äer, the crossbar and the inter-router
links; the non-separable modules are the arbitration and routing logics. To decompose the sep-
arable modules, the input buÔ¨Äer (Fig. 6.5), the crossbar (Fig. 6.6) as well as inter-router links
(Fig. 6.7), are designed as bit-slice modules, such that the data width of each component is re-
duced from W to W=n where n is the number of layers. Bit-slicing the input buÔ¨Äer reduces the
length of the word-line and saves power. In addition, bit-slicing the input buÔ¨Äer allows for se-
lectively switching oÔ¨Äany unused slices of the buÔ¨Äer to save power during run time. ÓÄÄe same
techniques can also be applied for the crossbar and the inter-router links. As a result of 3D parti-
tioning, the area of the router reduces and the available link bandwidth for each router increases.
ÓÄÄis excess bandwidth is leveraged to construct express physical links in this work and is shown
to signiÔ¨Åcantly improve performance. In addition, with reduced crossbar size and link length,
the latencies of both crossbar and link stages decrease and they can be combined into one stage
without violating timing constraints. ÓÄÄe proposed architecture with express links perform best
among the architectures examined (2D NoC, 3D hop-by-hop NoC, the proposed architecture
without express links and with express links). ÓÄÄe latency improvements are up to 51% and 38%,
and the power improvements are up to 42% and 67% for synthetic traÔ¨Écs and real workloads,
respectively [72].
(a) 2DB, 3DB  
(b) 3DM (full) 
(c) 3DM (partial) 
Figure 6.5: Decomposition of the input buÔ¨Äer [85]. (a) ÓÄÄe 2D/3D baseline input buÔ¨Äer; (b) the 3D
decomposed input buÔ¨Äer, and (c) the decomposed buÔ¨Äer with unused portion powered oÔ¨Ä.
6.2
3D NOC TOPOLOGY DESIGN
All the router designs discussed in previous subsections are based on the mesh-based NoC topol-
ogy. ÓÄÄere exist various NoC topologies, such as concentrated mesh or Ô¨Çattened butterÔ¨Çy topol-
ogy, all of which have advantages and disadvantages. By employing diÔ¨Äerent topologies rather
than the mesh topology, the router designs discussed above could also have diÔ¨Äerent variants.
For example, in 2D concentrated mesh topology, the router itself has a radix of 8 (i.e., an 8-port
www.ebook3000.com

6.2. 3D NOC TOPOLOGY DESIGN
55
Figure 6.6: Decomposition of the crossbar [85]: the 2D baseline crossbar (2DB), the 3D decomposed
crossbar (3DM), and the 3D decomposed crossbar with support for express channels (3DM-E), which
sightly increases the crossbar size.
Figure 6.7: Decomposition of the inter-router links [85].
router, with four to local PEs and the others to four cardinal directions). With such topology,
the 3D NoC-bus hybrid approach would result in a 9-port router design. Such high-radix router
designs are power-hungry with degraded performance, even though the hop count between PEs
is reduced. Consequently, a topology-router co-design method for 3D NoC is desirable, so that
the hop count between any two PEs and the radix of the 3D router design is as small as possi-
ble. Xu et al. [73] proposed a 3D NoC topology with low hop count (which is deÔ¨Åned as low
diameter) and low radix router design. ÓÄÄe level 2D mesh is replaced with a network of long links
connecting nodes that are at least m mesh-hops away, where m is a design parameter. In such a
topology, long distance communications can leverage the long physical wire and vertical links to

56
6. 3D NETWORK-ON-CHIP
reach destination, achieving low total hop count, while the radix of the router is kept low. For
application-speciÔ¨Åc NoC architecture, Yan et al. [79] also proposed a 3D-NoC synthesis algo-
rithm that is based on a rip-up and reroute formulation for routing Ô¨Çows and a router merging
procedure for network optimization to reduce the hop count.
6.3
3D OPTICAL NOC DESIGN
Aside from electrical NoC, 3D optical NoCs leveraging heterogeneous technology integration
have also been proposed [86, 87]. ÓÄÄe advancement in silicon photonics have motivated research
on building on-chip optical interconnects with monolithic optical components, e.g., waveguides,
light modulators and detectors, etc. However, building feasible optical on-chip networks requires
massively fabricating these components with electrical devices. Since the optimal processes are
diÔ¨Äerent for optical and electrical devices, this presents an issue for technology tuning. ÓÄÄis prob-
lem is resolved with 3D integration by using diÔ¨Äerent technologies with diÔ¨Äerent layers.
Ye et al. [86] have demonstrated an optical NoC using 3D-stacking, based on the Cygnus
optical router architecture. Cygnus leverages waveguides and micro-ring resonators in the optical
crossbar to switch data between directions. ÓÄÄe micro-ring resonators can be viewed as couplers
that can divert signals from waveguides. ÓÄÄe micro-ring can be switched between the on and oÔ¨Ä
states by electrical signals. In the on state it diverts optical signals while in the oÔ¨Ästate it lets optical
signals pass by. Due to the absent of optical buÔ¨Äering, Cygnus proposes using circuit-switching
and the optical path set-up and tear-down are done with an electrical hop-by-hop network. As
discussed in the previous paragraph, however, it is technologically diÔ¨Écult to integrate the optical
network with the electrical network. A solution surfaces with 3D integration, where the two
networks are fabricated on diÔ¨Äerent layers; control signals and inject/eject links are implemented
by TSVs. Comparing 3D Cygnus with an electrical network having equal size, experiment results
show up to 70% power saving and 50% latency saving. ÓÄÄe drawback of the 3D Cygnus, however,
is the slightly lower maximum throughput due to the overheads of circuit-switching.
Zhang et al. [87] yet propose another 3D optical NoC using packet switching. ÓÄÄis work
is motivated by Corona [88] which adapts token rings for optical interconnects. ÓÄÄe problem
with token rings, however, is the reduced utilization when the network size becomes large due to
the elongated token circulating time. Zhang et al. propose to divide a large network into smaller
regions and implement optical token rings inside and between regions. While this approach re-
duces token circulating time, it introduces a large number of waveguide crossings that severely
aÔ¨Äects optical signal integrity. To address this problem, they leverage the recently demonstrated
‚Äúoptical vias‚Äù that can couple light from one layer to another layer. ÓÄÄey propose to stack cross-
ing waveguides at diÔ¨Äerent layers such that crosstalk and signal loss are largely reduced. As a
result, the proposed 3D NoC architecture outperforms Corona and electrical networks in both
performance and power. For example, the zero-load latency of the proposed 3D optical NoC is
28% lower than Corona, while the throughput is 2.4 times higher. In addition, the proposed 3D
optical NoC consumes 6.5% less power than Corona.
www.ebook3000.com

6.4. IMPACT OF 3D TECHNOLOGY ON NOC DESIGNS
57
6.4
IMPACT OF 3D TECHNOLOGY ON NOC DESIGNS
Since TSV vias contend with active device area, they impose constraints on the number of such
vias per unit area. Consequently, the NoC design should be performed holistically in conjunction
with other system components such as the power supply and clock network that will contend for
the same interconnect resources.
ÓÄÄe 3D integration using TSV (through-silicon-via) can be classiÔ¨Åed into one of the two
following categories; (1) monolithic approach and the (2) stacking approach. ÓÄÄe Ô¨Årst approach in-
volves a sequential device process, where the frontend processing (to build the device layer) is re-
peated on a single wafer to build multiple active device layers before the backend processing builds
interconnects among devices. ÓÄÄe second approach (which could be wafer-to-wafer, die-to-wafer,
or die-to-die stacking) processes each active device layer separately using conventional fabrication
techniques. ÓÄÄese multiple device layers are then assembled to build up 3D ICs using bonding
technology. Dies can be bonded face-to-face (F2F) or face-to-back (F2B). ÓÄÄe microbump in
face-to-face wafer bonding does not go through a thick buried Si layer and can be fabricated with
a higher pitch density. In stacking bonding, the dimension of the TSVs. is not expected to scale
at the same rate as feature size because alignment tolerance and thinned die/wafer height during
bonding poses a limitation on the scaling of the vias.
ÓÄÄe TSV (or micropad) size, length, and the pitch density, as well as the bonding method
(face-to-face or face-to-back bonding, SOI-based 3D or bulk CMOS-based 3D) can have a sig-
niÔ¨Åcant impact on the 3D NoC topology design. For example, a relatively large size of TSVs. can
hinder partitioning a design at very Ô¨Åne granularity across multiple device layers, and make the
true 3D router design less possible. On the other hand, the monolithic 3D integration provides
more Ô¨Çexibility in the vertical 3D connection because the vertical 3D via can potentially scale
down with feature size due to the use of local wires for connection. Availability of such technolo-
gies makes it possible to partition the design at a very Ô¨Åne granularity. Furthermore, face-to-face
bonding or SOI-based 3D integration may have a smaller via pitch size and higher via density
than face-to-back bonding or bulk CMOS based integration. Such inÔ¨Çuence of the 3D tech-
nology parameters on the NoC topology design should be thoroughly studied and suitable NoC
topologies for diÔ¨Äerent 3D technologies should be identiÔ¨Åed with respect to the performance,
power, thermal, and reliability optimizations.

www.ebook3000.com

59
C H A P T E R
7
ÓÄÄermal Analysis and
ÓÄÄermal-aware Design
Power and thermal issues have become the primary concerns in the traditional 2D IC design.
Although emerging 3D technology oÔ¨Äers several beneÔ¨Åts over 2D, the stacking of multiple active
layers in 3D design leads to higher power densities than its 2D counterpart, exacerbating the
thermal issue. ÓÄÄerefore, it is essential to conduct thermal-aware 3D IC designs. ÓÄÄis chapter
presents an overview of thermal modeling for 3D IC and outlines solution schemes to overcome
the thermal challenges.
One of the major concerns in the adoption of 3D technology is the increased power den-
sities that can result from placing one power hungry block over another in the multi-layered 3D
stack. Since the increasing power density and the resulting thermal impact are already major con-
cerns in 2D ICs, the move to 3D ICs could accentuate the thermal problem due to increased
power density, resulting in higher on-chip temperatures. High temperature has adverse impacts
on circuit performance. ÓÄÄe interconnect delay becomes slower while the driving strength of a
transistor decreases with increasing temperature. Leakage power has an exponential dependence
on the temperature and increasing on-chip temperature can even result in thermal runaways. In
addition, at suÔ¨Éciently high temperatures, many failure mechanisms, including electromigration
(EM), stress migration (SM), time-dependent dielectric (gate oxide) breakdown (TDDB), and
thermal cycling (TC), are signiÔ¨Åcantly accelerated, which leads to an overall decrease in reliabil-
ity. Consequently, it is very critical to model the thermal behaviors in 3D ICs and investigate
possible solutions to mitigate thermal problems in order to fully take advantage of the beneÔ¨Åts
that 3D technologies oÔ¨Äer.
7.1
THERMAL ANALYSIS
3D technology oÔ¨Äers several beneÔ¨Åts compared to traditional 2D technology. However, one of
the major concerns in the adoption of 3D technology is the increased power densities that can
result from placing one power hungry block over another in the multi-layered 3D stack. Since
the increasing power density and the resulting thermal impact are already major concerns in 2D
ICs, the move to 3D ICs could accentuate the thermal problem due to increased power density,
resulting in higher on-chip temperatures. High temperature has adverse impacts on circuit per-
formance, such as: (1) ÓÄÄe interconnect delay becomes slower while the driving strength of a tran-
sistor decreases with increasing temperature; (2) leakage power has an exponential dependence

60
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
on the temperature and increasing on-chip temperature can even result in thermal runaways. In
addition, at suÔ¨Éciently high temperatures, many failure mechanisms, including electromigration
(EM), stress migration (SM), time-dependent dielectric (gate oxide) breakdown (TDDB), and
thermal cycling (TC), are signiÔ¨Åcantly accelerated, which leads to an overall decrease in reliabil-
ity. Consequently, it is very critical to model the thermal behaviors in 3D ICs and investigate
possible solutions to mitigate thermal problems in order to fully take advantage of the beneÔ¨Åts
that 3D technologies oÔ¨Äer. ÓÄÄis section surveys several thermal modeling approaches for 3D ICs.
A detailed 3D modeling method is Ô¨Årst introduced, and then compact thermal models are intro-
duced.
3D ÓÄÄermal Analysis based on Finite DiÔ¨Äerence Method or Finite Element Method.
Sapat-
nekar et al. proposed a detailed 3D thermal model [89]. ÓÄÄe heat equation (7.1), which is a
parabolic partial diÔ¨Äerential equation (PDE), deÔ¨Ånes on-chip thermal behavior at the macroscale:
cp
@T .r; t/
@t
D ktr2T .r; t/ C g.r; t/;
(7.1)
where p represents the density of the material (in kg/m3), cp is the heat capacity of the chip
material (in J/(kg K)), T is the temperature (in K), r is the spatial coordinate of the point where
the temperature is determined, t is time (in sec), kt is the thermal conductivity of the material
(in W/(m K)), and g is the power density per unit volume (in W/m3). ÓÄÄe solution of Eq. (7.1)
is the transient thermal response. Since all derivatives with respect to time go to zeroes in the
steady state, steady-state analysis is needed to solve the PDE, which is the well-known Poisson‚Äôs
equation.
A set of boundary conditions must be added in order to get a well-deÔ¨Åned solution to
Eq. (7.1). It typically involves building a package macro model and assuming a constant ambient
temperature is interacted with the model. ÓÄÄere are two methods to discretize the chip and to
form a system of linear equations representing the temperature distribution and power density
distribution: Ô¨Ånite diÔ¨Äerence method (FDM) and Ô¨Ånite element method (FEM). ÓÄÄe diÔ¨Äerence
between them is that FDM discretizes the diÔ¨Äerential operator while FEM discretizes the tem-
perature Ô¨Åeld. Both of them can handle complicated material structures such as non-uniform
interconnects distributions in a chip.
In FDM, a heat transfer theory is adopted, which builds an equivalent thermal circuit
through the thermal-electrical analogy. ÓÄÄe steady-state equation represents the network with
thermal resistors connected between nodes and with thermal current sources mapping to power
sources. ÓÄÄe voltage and the temperature at the nodes can be computed by solving the circuit. ÓÄÄe
ground node is considered as a constant temperature node, typically the ambient temperature.
Since FDM methods are similar to power grid analysis problems, similar solution tech-
niques can be used, such as the multigrid-based approaches. Li et al. [90] proposed multigrid
(MG) techniques for fast chip level thermal steady-state and transient simulation. ÓÄÄis approach
avoids an explicit construction of the matrix problem that is intractable for most full-chip prob-
www.ebook3000.com

7.1. THERMAL ANALYSIS
61
lems. SpeciÔ¨Åc MG treatments are proposed to cope with the strong anisotropy of the full-chip
thermal problem that is created by the vast diÔ¨Äerence inmaterial thermal properties and chip
geometries. Importantly, this work demonstrates that only with careful thermal modeling as-
sumptions and appropriate choices for grid hierarchy, MG operators, and smoothing steps across
grid points, can a full-chip thermal problem be accurately and eÔ¨Éciently analyzed.
ÓÄÄe FEM provides another avenue to solve Poisson‚Äôs equation. In Ô¨Ånite element analysis,
the design space is Ô¨Årst discretized or meshed into elements. DiÔ¨Äerent element shapes can be
used such as tetrahedra and hexahedra. For the on-chip problem, where all heat sources are mod-
eled as being rectangular, a reasonable discretization for the FEM divides the chip into 8-node
rectangular hexahedral elements [91]. ÓÄÄe temperatures at the nodes of the elements constitute
the unknowns that are computed during Ô¨Ånite element analysis, and the temperature within an
element is calculated using an interpolation function that approximates the solution to the heat
equation within the elements.
Compact 3D IC ÓÄÄermal Modeling.
3D IC thermal analysis with FDM or FEM can be very
time consuming and therefore is not suitable to be used for design space exploration where the
thermal analysis has to be performed iteratively. ÓÄÄerefore a compact thermal model for 3D
IC is desirable. For traditional 2D design, one widely used compact thermal model is called
HotSpot [92], which is based on an equivalent circuit of thermal resistances and capacitances
that correspond to microarchitecture components and thermal packages. In a well-known du-
ality between heat transfer and electrical phenomena, heat Ô¨Çow can be described as a ‚Äúcurrent‚Äù
while temperature diÔ¨Äerence is analogous to a ‚Äúvoltage.‚Äù ÓÄÄe temperature diÔ¨Äerence is caused
by heat Ô¨Çow passing through a thermal resistance. It is also necessary to include thermal capaci-
tances for modeling transient behavior to capture the delay before the temperature reaching steady
state due to a change in power consumption. Like electrical RC constants, the thermal RC time
constants characterize the rise and fall times led by the thermal resistances and capacitances. In
the rationale, the current and heat Ô¨Çow are described by exactly the same diÔ¨Äerent equations for
a potential diÔ¨Äerence. ÓÄÄese equivalent circuits are called compact models or dynamic compact
models if thermal capacitors are included. For a microarchitecture unit, the dominant mechanism
to determine the temperature is the heat conduction to the thermal package and to neighboring
units.
In HotSpot, the temperature is tracked at the granularity of individual microarchitectural
units and the equivalent RC circuits have at least one node for each unit. ÓÄÄe thermal model com-
ponent values do not depend on initial temperature or the particular conÔ¨Ågurations. HotSpot is a
simple library that generates the equivalent RC circuit automatically and computes temperature
at the center of each block with power dissipation over any chosen time step.
Based on the original HotSpot model, a 3D thermal estimation tool named HS3D was
introduced [93]. HS3D allows 3D thermal evaluation despite a largely unchanged computational
model and methods in HotSpot. ÓÄÄe inter-layer thermal vias can be approximated by changing
the vertical thermal resistance of the materials. HS3D library allows incompletely speciÔ¨Åed Ô¨Çoor-

62
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
plans as input and ensures accurate thermal modeling of large Ô¨Çoorplan blocks. Many routines
have been recreated for optimizations of loop accesses, cache locality, and memory paging. ÓÄÄese
improvements oÔ¨Äer reduced memory usage and runtime reduction by over three orders when sim-
ulating a large number of Ô¨Çoorplans. To guarantee the correctness and eÔ¨Éciency of HS3D library,
the comparison between this new library and a commercial FEM software was performed. First
a 2D sample device and package is used for the veriÔ¨Åcation. ÓÄÄe diÔ¨Äerence of the average chip
temperatures from HS3D and FEM software is only 0.02 ƒ±C. Multi-layer (3D) device modeling
veriÔ¨Åcation is provided using 10 m thick silicon layers and 2 m thick interlayer material. ÓÄÄe
test case includes two layers with a sample processor in each layer. ÓÄÄe experiment results show
that the average temperature mis-estimation is 3 ƒ±C. In addition, the thermal analysis using FEM
software costs seven minutes while only costs one second HS3D. It indicates that HS3D provides
not only high accuracy but also high performance (low run time). ÓÄÄe extension in HS3D was
integrated into HotSpot in the later versions to support 3D thermal modeling.
(a). Tiles Stack Array
(c). Tile Stack Analysis
(b). Single Tile Stack
Rlateral
Figure 7.1: Resistive thermal model for a 3-D IC [94].
Cong and Zhang derived a closed-form compact thermal model for thermal via planning
in 3D ICs [94]. In their thermal resistive model, a tile structure is imposed on the circuit stack
with each tile the size of a via pitch, as shown in Fig. 7.1(a). Each tile stack contains an array of
tiles, with one tile for each device layer, as shown in Fig. 7.1(b). A tile either contains one via at
the center, or no via at all. A tile stack is modeled as a resistive network, as shown in Fig. 7.1(c).
A voltage source is utilized for the isothermal base, and current sources are present in each silicon
layer to represent heat sources. ÓÄÄe tile stacks are connected by lateral resistances. ÓÄÄe values of
the resistances in the network are determined by a commercial FEM-based thermal simulation
tool.
www.ebook3000.com

7.2. THERMAL-AWARE FLOORPLANNING FOR 3D PROCESSORS
63
To further improve analysis accuracy without losing eÔ¨Éciency, Yang et al. proposed an in-
cremental and adaptive chip-package thermal analysis Ô¨Çow named ISAC [95]. During thermal
analysis, both time complexity and memory usage are linearly or superlinearly related to the num-
ber of thermal elements. ISAC incorporates an eÔ¨Écient technique for adapting thermal element
spatial resolution during thermal analysis. ÓÄÄis technique uses incremental reÔ¨Ånement to generate
a tree of heterogeneous rectangular parallelepipeds that supports fast thermal analysis without loss
of accuracy. Within ISAC, this technique is incorporated with an eÔ¨Écient multigrid numerical
analysis method, yielding a comprehensive steady-state thermal analysis solution.
7.2
THERMAL-AWARE FLOORPLANNING FOR 3D
PROCESSORS
Physical design tools play an important role in 3D design automation since one essential diÔ¨Äerence
between 2D and 3D CAD tools is the physical design. ÓÄÄe physical design tools for 3D IC can
be classiÔ¨Åed into three categories: Ô¨Çoorplanning, placement, and routing. In addition, one major
concern in 3D ICs is the increased power density due to placing one thermal critical block over
another in the 3D stack. Consequently, thermal-aware physical design tools should be developed
to address both thermal and physical tool issues.
ÓÄÄe physical design methodologies for 3D IC depend on the partitioning strategy, which is
determined by the 3D technology process. As discussed in Chapter 1, the partitioning strategy in
3D ICs can be classiÔ¨Åed into two categories: Ô¨Åne-granularity partitioning and coarse-granularity
partitioning. For monolithic 3D approach (such as the MLBS approach) or TSV-based 3D ICs
with extremely small TSVs, it is possible to perform Ô¨Åne-granularity partitioning with the gate-
level 3D placement and routing; on the other hand, with large TSV size (with via diameter larger
than 1 m), it makes more sense to perform coarse-granularity partitioning at the unit level or
core level, to avoid large area overhead. For coarse-granularity partitioning, since each unit or core
is still a 2D design, the physical placement and routing for each unit/core can still be done with
conventional placement and routing EDA tools, and then for coarse-granularity 3D partitioning,
only 3D Ô¨Çoorplanning is needed. Consequently, 3D Ô¨Çoorplanning is the most needed physical
design tool for near term 3D IC design. In this section, we will Ô¨Årst describe in detail a thermal-
aware 3D Ô¨Çoorplanning framework [96], and then give a quick survey on thermal-aware 3D
placement and routing.
3D IC design is fundamentally related to the topological arrangement of logic blocks.
ÓÄÄerefore, physical design tools play an important role in the adoption of 3D technologies. New
placement and routing tools are necessary to optimize 3D instantiations and new design tools are
required to optimize interlayer connections. A major concern in the adoption of 3D architecture
is the increased power densities that can result from placing one computational block over an-
other in the multilayered 3D stack. Since power densities are already a major bottleneck in 2D
architectures, the move to 3D architectures could accentuate the thermal problem. Even though
3D chips could oÔ¨Äer some respite due to reduced interconnect power consumption (as a result

64
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
of the shortening of many long wires), it is imperative to develop thermally aware physical de-
sign tools‚Äîfor example, partition design to place highly loaded, active gates in layers close to the
heat-sink.
Tools for modeling thermal eÔ¨Äects on chip-level placement have been developed. Recently,
Cong et al. [97] proposed a thermal-driven Ô¨Çoorplanning algorithm for 3D ICs. Chu and Wong
[98] proposed using a matrix synthesis problem (MSP) to model the thermal placement problem.
A standard cell placement tool to even thermal distribution has been introduced by Tsai and Kang
[99] with their proposed compact Ô¨Ånite diÔ¨Äerence method (FDM) based temperature modeling.
In [91], thermal eÔ¨Äect was formulated as another force in a force-directed approach to direct
the placement procedure for a thermally even standard cell placement. Another design metric,
reliability, was taken care of in [100] when doing a multi-layer System-on-Package Ô¨Çoorplanning;
thermal issue was neglected, though.
ÓÄÄis section presents a thermal-aware Ô¨Çoorplanner for a 3D microprocessors [96]. ÓÄÄe
Ô¨Çoorplanner is unique in that it accounts for the eÔ¨Äects of the interconnect power consumption
in estimating the peak temperatures. We describe how to estimate temperatures for 3D micro-
processors and show the eÔ¨Äectiveness of the thermal-aware tool in reducing peak temperatures
using one microprocessor design and four MCNC benchmarks.
B-tree Floorplan Model
ÓÄÄe Ô¨Çoorplanner used in this work is based on the B-tree representation, which was proposed
by Chang et al. [101]. A B-tree is an ordered binary tree which can represent a non-slicing
admissible Ô¨Çoorplan. Figure 7.2 shows a B-tree representation and its corresponding Ô¨Çoorplan.
ÓÄÄe root of a B-tree is located at the bottom-left corner. A B-tree of an admissible Ô¨Çoorplan
can be obtained by depth-Ô¨Årst-search procedure in a recursive fashion. Starting from the root, the
left subtree is visited Ô¨Årst then followed by the right subtree, recursively. ÓÄÄe left child of node
ni is the lowest unvisited module in Ri, which denotes the set of modules adjacent to the right
boundary of module bi. On the other hand, the module, representing the right child of ni, is
adjacent to and above module bi. We assume the coordinates of the root node are always (0,0)
residing at bottom-left corner. ÓÄÄe geometric relationship between two modules in a B-tree is
maintained as follows. ÓÄÄe x-coordinate of node nj is xi+wi, if node nj is the left child of node
ni. ÓÄÄat is, bj is located and is adjacent to the right-hand side of bi: For the right child nk of
ni, its x-coordinate is the same as that of ni, with module bk sitting adjacent to and above bi.
While the original B-tree structure was developed and used for the 2D Ô¨Çoorplanning problem,
we extend this model to represent a multi-layer 3D Ô¨Çoorplanning (see Fig. 7.3) and modify the
perturbation function to handle 3D Ô¨Çoorplans in this work.
ÓÄÄere are six perturbation operations used in our algorithm and they are listed below:
(1) Node swap, which swaps two modules.
(2) Rotation, which rotates a module.
www.ebook3000.com

7.2. THERMAL-AWARE FLOORPLANNING FOR 3D PROCESSORS
65
(3) Move, which moves a module.
(4) Resize, which adjusts the aspect-ratio of a soft module.
(5) Interlayer swap, which swaps two modules at diÔ¨Äerent layers.
(6) Interlayer move, which moves a module to a diÔ¨Äerent layer.
ÓÄÄe Ô¨Årst three perturbations are the original moves deÔ¨Åned in [101]. Since these moves
only have inÔ¨Çuence on the Ô¨Çoorplan in single layer, more interlayer moves, (5) and (6) are needed
to explore the 3D Ô¨Çoorplan solution space.
b7
b6
b5
b1
b3
b4
b2
n1
n2
n5
n6
n7
n3
n4
(b)
(a)
Figure 7.2: (a) An example Ô¨Çoorplan; (b) the corresponding B-tree [101].
Figure 7.3: Extending B-tree model to represent 3D Ô¨Çoorplanning [5].

66
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
Simulated Annealing Engine
A simulated annealing engine is used to generate Ô¨Çoorplanning solutions. ÓÄÄe inputs to our Ô¨Çoor-
planning algorithm are the area of all functional modules and interconnects among modules.
However, the actual dimension of each module is unknown a priori except for its area before
placement. ÓÄÄat is, we have to treat them as soft modules. ÓÄÄus, we provide the choice of adjusting
aspect-ratio as one perturbation operation. During simulated process, each module dynamically
adjusts its aspect-ratio to Ô¨Åt closely with the adjacent modules, that is, with no dead space be-
tween two modules. A traditional weighted cost representing optimization costs (area and wire
length) is generated after each perturbation.
DiÔ¨Äerent from 2D Ô¨Çoorplanning, our 3D Ô¨Çoorplanner uses a two-stage approach. ÓÄÄe Ô¨Årst
stage tries to partition the blocks to appropriate layers, and tries to minimize the packed area
diÔ¨Äerence between layers and total wire length using all perturbation operations (one through six
listed in previous subsection). However, because the Ô¨Årst stage tries to balance the packed areas of
the diÔ¨Äerent layers, the Ô¨Çoorplan of some layers may not be compactly packed. ÓÄÄe second stage
is intended to overcome this problem. ÓÄÄus, in the second stage, we start with the partitioning
solution generated by the Ô¨Årst stage and focus on adjusting the Ô¨Çoorplan of each layer simultane-
ously with the Ô¨Årst four operations. At this point, there are no inter-layer operations to disturb
module partition of each layer obtained from stage one.
One problem of 3D Ô¨Çoorplanning is the Ô¨Ånal packed area of each layer must match to
avoid penalties of chip area. For example, if the Ô¨Ånal width of packed modules of layer L1 is
larger than the Ô¨Ånal width of packed modules in layer L2 and the height of L1 is smaller that of
L2, a signiÔ¨Åcant portion of chip area is wasted due to the need for the layer dimensions to match
for manufacturing. To make sure the dimension of each layer will be compatible, we adopt the
concept of dimension deviation dev.F / in [100]. ÓÄÄe goal is to minimize dev.F /, which tells the
deviation of the upper-right corner of a Ô¨Çoorplan from the average Avex, Avey values. ÓÄÄe value,
Avex can be calculated by P ux.fi/=k, where ux.fi/ is the x-coordinate of upper-right corner
of Ô¨Çoorplan i, and k indicates the number of layers. ÓÄÄe value Avey can be obtained in a similar
manner. ÓÄÄus, dev.F / is formulated as P# layers
i
jAvex   ux.fi/j C jAvey   uy.fi/j. ÓÄÄe modiÔ¨Åed
cost function for 3D Ô¨Çoorplanner can be written as
cost D Àõ  area C Àá  wl C   dev.F /
(7.2)
where area, wl are chip area and wire length, respectively.
Temperature Approximation
Although HS3D [93] can be used to provide temperature feedbacks, when evaluating a large
number of solutions during simulated procedure, it is not wise to involve the time-consuming
temperature calculation every time. Other than using the actual temperature values, we have
adopted the power density metric as a thermal-conscious mechanism in our Ô¨Çoorplanner. ÓÄÄe
temperature is heavily dependent on power density based on a general temperature-power equa-
tion: T D P  R D P  .t=k  A/ D .P=A/  .t=k/ D d  .t=k/, where t is the thickness of the
www.ebook3000.com

7.2. THERMAL-AWARE FLOORPLANNING FOR 3D PROCESSORS
67
Table 7.1: Floorplanning results of 2D architecture
Circuit
2D
2D(thermal)
wire
area
peakT
wire
area
peakT
(um)
(mm2)
(um)
(mm2)
Alpha
339672
29.43
114.50
381302
29.68
106.64
xerox
542926
19.69
123.75
543855
19.84
110.45
hp
133202
8.95
119.34
192512
8.98
116.91
ami33
44441
1.21
128.21
51735
1.22
116.97
ami49
846817
37.43
119.42
974286
37.66
108.86
chip, k is the thermal conductivity of the material, R is the thermal resistance, and d is the power
density. ÓÄÄus, we can substitute the temperature and adopt the power density, according to the
equation above, to approximate the 3-tie temperature function, CT D .T   To/=To, proposed in
[97] to reÔ¨Çect the thermal eÔ¨Äect on a chip. As such, the 3-tie power density function is deÔ¨Åned
as P D .Pmax   Pavg//=Pavg, where Pmax is the maximum power density while Pavg is the average
power density of all modules. ÓÄÄe cost function for 2D architecture used in simulated annealing
can be written as
cost D Àõ  area C Àá  wl C   P:
(7.3)
For 3D architectures, we also adopt the same temperature approximation for each layer
as horizontal thermal consideration. However, since there are multiple layers in 3D architec-
ture, the horizontal consideration alone is not enough to capture the coupling eÔ¨Äect of heat.
ÓÄÄe vertical relation among modules also needs to be involved and is deÔ¨Åned as: OP.TPm/ D
P.P m C P mi/  overlap_area, where OP.TPm/ stands for the summation of the power den-
sity of module, P m, and all overlapping module mi with module m and their relative power
densities multiplying their corresponding overlapped area.
ÓÄÄe rationale behind this is that for a module with relatively high power density in one
layer, we want to minimize its accumulated power density from overlapping modules located in
diÔ¨Äerent layers. We can deÔ¨Åne the set of modules to be inspected, so the total overlap power
density is TOP D P OP.TPi/, for all modules in this set. ÓÄÄe cost function for 3D architecture
is thus modiÔ¨Åed as follows:
cost D Àõ  area C Àá  wl C   dev.F / C   P C ƒ±  TOP:
(7.4)
At the end of algorithm execution, the actual temperature proÔ¨Åle is reported by our HS3D tool.
Experiment Results
We implemented the proposed Ô¨Çoorplanning algorithm in C++. ÓÄÄe thermal model is
based on the HS3D. In order to eÔ¨Äectively explore the architecture-level interconnect

68
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
Table 7.2: Floorplanning results of 3D architecture
Circuit
3D
3D(thermal)
wire
area
peakT
wire
area
peakT
(um)
(mm2)
(um)
(mm2)
Alpha
210749
15.49
135.11
240820
15.94
125.47
xerox
297440
9.76
137.51
294203
9.87
127.31
hp
124819
4.45
137.90
110489
4.50
134.39
ami33
27911
0.613
165.61
27410
0.645
155.57
ami49
547491
18.55
137.71
56209
18.71
132.69
power consumption of a modern microprocessor, we need a detailed model which can act
for the current-generation high-performance microprocessor designs. We have used IVM
(http://www.crhc.uiuc.edu/ACS/tools/ivm), a Verilog implementation of an Alpha-like archi-
tecture (denoted as Alpha in the rest of this section) at register-transfer-level, to evaluate the
impacts of both interconnect and module power consumptions at the granularity of functional
module level. A diagram of the processor was shown in Fig. 7.4. Each functional block in Fig. 7.4
represents a module used in our Ô¨Çoorplanner. ÓÄÄe registers between pipeline stages are also mod-
eled but not shown in the Ô¨Ågure.
ÓÄÄe implementation of microprocessor has been mapped to a commercial 160 nm standard
cell library by Design Compiler and placed and routed by First Encounter under 1GHz perfor-
mance requirement. ÓÄÄere are in total 34 functional modules and 168 netlists extracted from the
processor design. ÓÄÄe area and power consumptions from the actual layout served as inputs to our
algorithm. Other than Alpha processor, we have also used MCNC benchmarks to verify our ap-
proach. A similar approach in [99] is used to assign the average power density for each module in
the range of 2.2*104 (W/m2) and 2.4*106 (W/m2). ÓÄÄe total net power is assumed to be 30% of
total power of modules due to lack of information for the MCNC benchmarks, and the total wire
length used to be scaled during Ô¨Çoorplanning is the average number from 100 test runs with the
consideration of area factor alone. ÓÄÄe widely used method of half-perimeter bounding box model
is adopted to estimate the wire length. ÓÄÄroughout the experiments, two-layer 3D architecture
was assumed due to a limited number of functional modules and excessively high power density
beyond two layers; however, our approach is capable of dealing with multiple-layer architecture.
Tables 7.1 and 7.2 show the experiment results of our approach when considering tradi-
tional metrics (area and wire) and thermal eÔ¨Äect. When taking thermal eÔ¨Äect into account, our
thermal-aware Ô¨Çoorplanner can reduce the peak temperature by 7% on average while increasing
wirelength by 18% and providing a comparable chip area as compared to the Ô¨Çoorplan generated
using traditional metrics.
www.ebook3000.com

7.3. THERMAL-HERDING: THERMAL-AWARE ARCHITECTURE DESIGN
69
Figure 7.4: Processor model diagram [96].
When we move to 3D architectures, the peak temperatures increased by 18% (on average)
as compared to the 2D Ô¨Çoorplan due to the increased power density. However, the wire length
and chip area reduced by 32% and 50%, respectively. ÓÄÄe adverse eÔ¨Äect of the increased power
density in the 3D design can be mitigated by our thermal-aware 3D Ô¨Çoorplanner, which lowers
the peak temperature by an average of 8 degrees with little area increase as compared to the 3D
Ô¨Çoorplanner that does not account for thermal behavior. As expected, the chip temperature is
higher when we step from 2D to 3D architecture without thermal consideration. Although the
wire length is reduced when moving to 3D and thus accordingly reduces interconnect power
consumption, the temperature for 3D architecture is still relatively high due to the accumulated
power densities and smaller chip footprints. After applying our thermal-aware Ô¨Çoorplanner, the
peak temperature is lowered to a moderate level through the separation of high power density
modules in diÔ¨Äerent layers.
7.3
THERMAL-HERDING: THERMAL-AWARE
ARCHITECTURE DESIGN
In this section, a family of thermal herding, microarchitecture techniques for controlling Hotspots
in high performance 3D processors, is presented. ÓÄÄermal herding techniques herd or steer the

70
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
switching activity in the 3D processor to the die that is closest to the heat sink and reduce the
total power and power density while still maintaining the performance beneÔ¨Åts. It adopts diÔ¨Äerent
strategies for diÔ¨Äerent components in the processor such as placing the frequently switching 16-
bits on the top die in the signiÔ¨Åcance-partitioned datapath, proposing a 3D-aware allocation
scheme for instruction scheduler, an address memorization approach for the load and store queues,
a partial value encoding for the L1 data cache, and exploiting a form of frequent partial value
locality in target addresses for a branch target buÔ¨Äer.
ÓÄÄermal herding is proposed based on some observations made by prior work that only a
few of the least signiÔ¨Åcant bits are needed in the data used in many integer instructions [102],
i.e., many 64-bit integer values only requires 16 or fewer bits for the representation. In addition,
it is highly predictable for an instruction‚Äôs usage of low-width values. ÓÄÄerefore, the datapath can
be organized by assigning each 16 bits to a separate dies (four dies) in 3D stacking. ÓÄÄe least
signiÔ¨Åcant bits can be placed on the top layer which is closest to the heat sink so that the power
on the other three dies may be saved using the instruction‚Äôs width prediction. A prediction is
made for each instruction whether to use low-width (16bits) or full-width ( 16bits) values. A
simple program counter indexed two-bit saturating counter predictor is used. When the predictor
indicates the data is low-width usage but it is actually a full-width usage then the result is an unsafe
misprediction, which requires pipeline stalls in related stages. Some critical microarchitecture
components are discussed as follows.
Register Files:
Each 64-bit entry in the register Ô¨Åle is partitioned into four layers with the least
signiÔ¨Åcant bits closest to the heat sink. ÓÄÄe width prediction is used to determine gating control
signals for other three layers before the actual register Ô¨Åle access. If low-width instruction is pre-
dicted then only the top layer portion of the register Ô¨Åle is active. In this case, the power density
is similar to that of a 2D register Ô¨Åle design. A width memorization bit is also provided for each
entry in the top layer, indicating whether the remaining three layers contain non-zero values. ÓÄÄe
processor compares this bit to the predicted width when reading it. If the width prediction is low
and the actual width is full, then the processor stalls the previous stages of the register Ô¨Åle and
activates the logic on the other three layers. It also corrects the prediction to prevent further stalls.
Arithmetic Units:
Only 3D integer adder is presented but the concept can be extended to other
arithmetic units. A tree-based adder is partitioned into four layers with the least signiÔ¨Åcant bits
residing on the top layer. ÓÄÄe processor uses predication information to decide if the clock gate
should be activated for other three layers. Two possible unsafe width mispredictions should be
handled properly. One is misprediction on an instruction‚Äôs input operands. In this scenario, since
the arithmetic unit is not fully active at the start of execution, one cycle stall is needed to activate
the upper 48 bits. Another scenario is misprediction on the output, in which the width mispre-
diction is not known at the beginning of the computation. ÓÄÄe instruction needs to be executed
again to guarantee the correctness, causing performance penalty. ÓÄÄerefore, the accuracy of the
width predictor is very important.
www.ebook3000.com

7.3. THERMAL-HERDING: THERMAL-AWARE ARCHITECTURE DESIGN
71
Bypass Network:
ÓÄÄe bypass network does not need additional circuitry to handle the width
prediction since the unsafe misprediction is handled by the arithmetic units. With a correct low-
width prediction, only the drivers/wires on the top layer consume dynamic power. In addition, the
partitioning in 3D reduces the wire length so that the latency and power are reduced accordingly.
Instruction Scheduler:
In the instruction scheduler, there is one reservation station (RS) for each
instruction dispatched but not executed. When an instruction is ready to issue, the instruction
broadcasts its destination identiÔ¨Åer to notify dependent instructions. ÓÄÄe instruction scheduler
is partitioned based on the RS entries so that the length of the broadcast buses is signiÔ¨Åcantly
reduced, resulting in latency and power reduction. A modiÔ¨Åed allocation algorithm is also adopted
to move instructions to the top layer so that the active entries are closer to the heat sink. If there
are no available entries in the top layer, the allocator will check the layer that is next closest to the
heat sink.
Load and Store Queues:
ÓÄÄe load and store queues are used to track the data and addresses of
instructions. ÓÄÄe queues are partitioned in a similar fashion with the main datapath due to its
similarity with register Ô¨Åle. Load and store addresses are normally full-width values but the upper
bits of the addresses do not change frequently. To take advantage of this feature, a partial address
memorization (PAM) scheme is proposed. ÓÄÄe low-order 16 bits of a load or store‚Äôs address is
broadcast on the top layer. In addition, whether the remaining 48 bits are the same as the most
recent store address is also indicated. To summarize, the PAM approach tries to herd the address
broadcasts and comparisons to the top layer.
Data Cache:
ÓÄÄe L1 data cache is organized in a word-partitioned manner due to its similarity
with register Ô¨Åle. Memorization bits are also provided to detect unsafe width mispredictions. In
addition, the deÔ¨Ånition of a ‚Äúlow-width‚Äù value for load and store instructions is broadened to
increase the frequency of low-width values. Two bits instead of one single width memorization
bit are stored to encode the upper 48 bits. Value ‚Äú00‚Äù means the upper 48 bits are all zeros. Value
‚Äú01‚Äù indicates all ones. Value ‚Äú10‚Äù means the upper bits are identical to those of the referencing
address. Value ‚Äú11‚Äù means the upper bits are not encodeable.
Front End:
Data-centric approaches for partitioning the front end are not eÔ¨Äective since no
data values are handled. A register alias table (RAT) is implemented to place the ports of each
instruction on diÔ¨Äerent layers so that unnecessary die-to-tie vias are avoided. ÓÄÄe instruction that
requires the most register name comparisons is placed on the top layer so that most switching
activities are moved to the top layer. For branch predictor based on two-bit saturating counters,
the counters are partitioned into two separate arrays: direction bit array and hysteresis array. ÓÄÄe
more frequently used direction-bit array is place on the layer closer to the heat sink. For the branch
target buÔ¨Äers (BTBs), since most branch targets are located relatively close to the originating
branch, they are organized like the data cache. ÓÄÄe low-order 16 bits are placed on the top layer

72
7. THERMAL ANALYSIS AND THERMAL-AWARE DESIGN
with one extra target memorization bit indicating whether the bits on the other three layers should
be accessed.
3D thermal herding techniques can improve IPC by reducing the pipeline depth and L2
latency but it also can reduce IPC due to width mispredictions. By conducting the experiments
on several benchmarks, the overall performance is improved since the pipeline reduction beneÔ¨Åts
outweigh the performance penalties caused by width mispredictions. ÓÄÄe power consumption
is also reduced due to two reasons: the length of wires is reduced because of 3D stacking and
thermal herding reduces the switching activities because of clock gating. ÓÄÄe thermal experiments
show that thermal herding techniques successfully control power density and mitigate 3D thermal
issues.
Conclusion.
Increasing power density in 3D ICs can result in higher on-chip temperatures,
which can have a negative impact on performance, power, reliability, and the cost of the chip.
Consequently, thermal modeling and thermal-aware design techniques are very critical for future
3D architecture designs. ÓÄÄis chapter presents an overview of thermal modeling for 3D IC and
outlines architecture design schemes to overcome the thermal challenges.
www.ebook3000.com

73
C H A P T E R
8
Cost Analysis for 3D ICs
ÓÄÄe majority of the 3D processor design research has focused on how to take advantage of the
performance, power, smaller form-factor, and heterogeneous integration beneÔ¨Åts oÔ¨Äered by 3D
integration. However, when it comes to the discussion on the adoption of such emerging tech-
nology as a mainstream design approach, it all comes down to the question of the 3D integration
cost. All the advantages of 3D ICs ultimately have to be translated into cost savings when a design
strategy has to be decided. For example, designers may ask themselves questions like,
‚Ä¢ Do all the beneÔ¨Åts of 3D IC design come with a much higher cost? For example, 3D bonding
incurs extra process cost, and the ÓÄÄrough-Silicon Vias (TSVs) may increase the total die
area, which has a negative impact on the cost; however, smaller die sizes in 3D ICs may
result in higher yield than that of a larger 2D die, and reduce the cost.
‚Ä¢ How to do 3D integration in a cost-eÔ¨Äective way? For example, to re-design a small chip may
not gain the cost beneÔ¨Åts of improved yield resulted from 3D integration. In addition, if a
chip is to be implemented in 3D, how many layers of 3D integration would be cost eÔ¨Äective?
and should one use wafer-to-wafer or die-to-wafer stacking [5]?
‚Ä¢ Are there any design options to compensate the extra 3D bonding cost? For example, in a 3D
IC, since some global interconnects are now implemented by TSVs, it may be feasible to
use fewer number of metal layers for each 2D die. In addition, heterogeneous integration
via 3D could also help cost reduction.
Cost analysis for 3D ICs at the early design stage is critical to answer these questions.
Considering 3D integration has both positive and negative eÔ¨Äects on the manufacturing cost,
there are no clear answers yet whether 3D integration is cost-eÔ¨Äective or how to achieve 3D
integration in a cost-eÔ¨Äective way. In order to maximize the positive eÔ¨Äect and compensate the
negative eÔ¨Äect on cost, it becomes critical to analyze the 3D IC cost at the early design stage. ÓÄÄis
early stage cost analysis can help the chip designers make a decision on whether 3D integration
should be used, or which 3D partitioning strategy should be adopted. Cost-eÔ¨Écient design is the
key for the future wide adoption of the emerging 3D IC design, and 3D IC cost analysis needs close
coupling between the 3D IC design and 3D IC process.¬π
¬πIC cost analysis needs a close interaction between designers and foundry. We work closely with our industrial partners to perform 3D IC cost
analysis. However, we cannot disclose absolute numbers for the cost, and therefore in this book, we either use arbitrary units (a.u.) or normalized
value to present the data.

74
8. COST ANALYSIS FOR 3D ICS
In this chapter, we Ô¨Årst propose a 3D IC cost analysis methodology with a complete set of
cost models that include wafer cost, 3D bonding cost, package cost, and cooling cost. Using this
cost analysis methodology along with the existing performance, power, area estimation tools, such
as McPAT [103], we estimate the 3D IC cost in two cases: one is for fully customized ASICs,
the other is for many-core microprocessor designs. During these case studies, we explore some
guidelines showing what a cost-eÔ¨Äective 3D IC fabrication option should be in the future.
Note that 3D integration is not yet a mature technology with very well-developed and
tested cost models; the optimal condition concluded by this work is subject to parameter changes.
However, the major contribution of this work is to provide a cost estimation methodology for
3D ICs. To our best knowledge, this is the Ô¨Årst eÔ¨Äort to model the 3D IC cost with package and
cooling cost included, while the majority of the existing 3D IC research activities mainly focused
on the circuit performance and power consumption.
In this chapter, we propose a complete set of 3D cost models, which not only includes
the wafer cost and the 3D bonding cost, but also includes another two critical cost components,
the package cost and the cooling cost. Both of them should not be ignored because while 3D
integration can potentially reduce the package cost by having a smaller chip footprint, the multiple
3D-stacked dies might increase the cooling cost due to the increased power density. By using the
proposed cost model set, we conduct two case studies on fully customized ASIC and many-core
microprocessor designs, respectively. ÓÄÄe experimental results show that our 3D IC cost model
makes it feasible to estimate the Ô¨Ånal system-level cost of the target chip design at the early
design stage. More importantly, it gives some guidelines to determine what a cost-eÔ¨Äective 3D
fabrication option should be.
8.1
3D COST MODEL
It is estimated that design choices made within the Ô¨Årst 20% of the total design cycle time will
ultimately result in up to 80% of the Ô¨Ånal product cost [104]. Hence, to facilitate the design
decision of using 3D integration from a cost perspective, it is necessary to perform cost analysis at
the early design stage. While it is straightforward to sum up all the incurred costs after production,
predicting the Ô¨Ånal chip cost at the early design stage is a big challenge because most of the
detailed design information is not available at that stage. Taking two diÔ¨Äerent IC design styles
(ASIC design style and many-core microprocessor design style) as examples:
‚Ä¢ ASIC Design style: At the early design stage, probably only the system-level (such as be-
havioral level or RTL level) design speciÔ¨Åcation is available, and a rough gate count can be
estimated from a quick high-level synthesis or logic synthesis tools, or simply by the past
design experience.
‚Ä¢ Microprocessor design style: Using a many-core microprocessor design as an example, the
information available at the early design stage is also very limited. ÓÄÄe design speciÔ¨Åcation
may just include information as follows: (1) ÓÄÄe number of microprocessor cores; (2) the
www.ebook3000.com

8.1. 3D COST MODEL
75
type of microprocessor cores (e.g., in-order or out-of-order); and (3) the number of cache
levels and the cache capacity of each level. All these speciÔ¨Åcations are on the architectural
level. Referring to previous design experience, it is feasible to achieve a rough estimation
on the gate count for logic-intensive cores and the cell count for memory-intensive caches,
respectively.
Consequently, it is very likely that at the early design stage, the cost estimation is simply
based on the design style and a rough gate count for the design as the initial starting point. In
this section, we describe how to translate the logic gate count (or the memory cell count) into
higher level estimations, such as the die area, the metal layer count, and the TSV count, given a
speciÔ¨Åed fabrication process node. ÓÄÄe die area estimator and the metal layer estimator are two
key components in our cost model: (i) larger die area usually causes lower die yield, and thus leads
to higher chip cost; (ii) the fewer metal layers are required, the less the number of fabrication
steps (and fabrication masks) are needed, which reduce the chip cost. It is also important to note
that the 3D partitioning strategy can directly aÔ¨Äect these two numbers: (1) 3D integration can
partition the original 2D design into several smaller dies; (2) TSVs can potentially shorten the
total interconnect length and thus reduce the number of metal layers.
Die Area Estimation.
At the early design stage, the relationship between the die area and the
gate count can be roughly described as follows,
Adie D NgateAgate
(8.1)
where Ngate is the gate count and Agate is an empirical parameter that shows the proportional
relationship between area and gate counts. Based on empirical data from many industrial designs,
in this work, we assume that Agate is equal to 31252, in which  is half of the feature size for
a speciÔ¨Åc technology node. Although this area estimation methodology is straightforward and
highly simpliÔ¨Åed, it accords with the real-world measurement quite well.¬≤
Wire Length Estimation.
As the only inputs at the early design stage are the estimation of
gate counts, it is necessary to further have an estimation on the design complexity, which can
be represented by the total length of wire interconnects. Rent‚Äôs Rule [105] is a well-known and
powerful tool that reveals the trend between the number of signal terminals and the number of
internal gates. Rent‚Äôs Rule can be expressed as follows,
T D kN p
gate
(8.2)
where the parameters k and p are Rent‚Äôs coeÔ¨Écient and exponent and T is the number of signal
terminals. Although Rent‚Äôs Rule is an empirical result based on the observations of previous de-
signs and it is not proper to use it for non-traditional designs, it does provide a useful framework
to compare similar architecture and it is plausible to use it as a part of our 3D IC cost model.
¬≤Note that it is necessary to update Agate along with the technology advance because the gate length is not strictly linearly
scaled.

76
8. COST ANALYSIS FOR 3D ICS
Based on Rent‚Äôs Rule, Donath discovered it can be used to estimate the average wire
length [106] and Davis et al. found it can be used to estimate the wire length distribution in
VLSI chips [107]. As a result, in this work, we use the derivation of Rent‚Äôs Rule to predict the
wire length distribution function i.l/ [107], which has the following forms,
Region I: 1  l  pNgate
i.l/ D Àõk
2  l3
3   2
q
Ngatel2 C 2Ngatel

l2p 4
Region II: pNgate  l < 2pNgate
i.l/ D Àõk
6  
2
q
Ngate   l
3
l2p 4
(8.3)
where l is the interconnect length in units of the gate pitch, Àõ is the fraction of the on-chip
terminals that are sink terminals and is related to the average fanout of a gate (f:o:) as follows,
Àõ D
f:o:
f:o: C 1
(8.4)
and   is calculated as follows,
  D
2Ngate

1   N p 1
gate


 N p
gate
1C2p 22p 1
p.2p 1/.p 1/.2p 3/  1
6p C
2p
Ngate
2p 1
 Ngate
p 1
:
(8.5)
Metal Layer Estimation.
After estimating the die area and the length of wire, we are able to
further predict the number of metal layers that are required to route all the interconnects within
the die area constraint. ÓÄÄe number of required metal layers for routing depends on the complexity
of the interconnects. A simpliÔ¨Åed metal layer estimation can be derived from the average wire
length [104] as follows,
nwire D f:o: N
Rmw

s
Ngate
Adie
(8.6)
where f:o: refers to the average gate fanout, w to the wire pitch,  to the utilization eÔ¨Éciency of
metal layers,
N
Rm to the average wire length, and nwire to the number of required metal layers.
Such a simpliÔ¨Åed model is based on the assumptions that each metal layer has the same
utilization eÔ¨Éciency and the same wire width [104]. However, such assumptions may not be
valid in real design [108]. To improve the estimation of the number of metal layers needed for
feasible routing, a more sophisticated metal layer estimation method is derived from the wire
length distribution rather than the simpliÔ¨Åed average wire length estimation. ÓÄÄe basic Ô¨Çow of
this method is explained as follows,
www.ebook3000.com

8.1. 3D COST MODEL
77
‚Ä¢ Estimate the available routing area of each metal layer with the expression:
Ki D Adiei   2Avi
 Ngatef:o:   I.li/

wi
(8.7)
where i is the metal layer, i is the layer‚Äôs utilization eÔ¨Éciency, wi is the layer‚Äôs wire pitch,
Avi is the layer‚Äôs via blockage area, and function I.l/ is the cumulative integral of the wire
length distribution function i.l/, which is expressed in Eq. 8.3.
‚Ä¢ Assume that shorter interconnects are routed on lower metal layers. Starting from Metal 1, we
route as many interconnects as possible on the current metal layer until the available routing area
is used up. ÓÄÄe interconnects routed on each metal layer can be express as:
L.li/   L.li 1/  Ki
(8.8)
where  D 4=.f:o: C 3/ is a factor accounting for the sharing of wires between interconnects
on the same net [107, 109]. ÓÄÄe function L.l/ is the Ô¨Årst-order moment of i.l/.
‚Ä¢ Repeat the same calculations for each metal layer in a bottom-up manner until all the interconnects
are routed properly.
By applying the estimation methodology introduced above, we can predict the die area
and the number of metal layers at the early design stage where we only have the gate count as
the input. Table 8.1 lists the values of all the related parameters [108, 110]. Figure 8.1 shows an
example which estimates the area and the number of metal layers of 65nm designs with diÔ¨Äerent
scale of gates.
Figure 8.1 also shows an important implication for 3D IC cost reduction: When a large
2D chip is partitioned into multiple smaller dies with 3D stacking, each smaller die requires fewer
number of metal layers to satisfy the interconnect routability requirements. Such metal layer reduction
opportunity can potentially oÔ¨Äset the cost of extra steps caused by 3D integration such as 3D
bonding and test.
TSV Estimation.
ÓÄÄe existence of TSVs in the 3D IC have its eÔ¨Äects on the wafer cost as well.
ÓÄÄese eÔ¨Äects are twofold,
1. In 3D ICs, some global interconnects are now implemented by TSVs, going between
stacked dies. ÓÄÄis could lead to the reduction of the total wire length, and provides op-
portunities for metal layer reduction for each smaller die;
2. On the other hand, 3D stacking with TSVs may increase the total die area, since the silicon
area where TSVs punch through may not be utilized for building devices or 2D metal layer
connections.¬≥
¬≥Based on current TSV fabrication technologies, the diameter of TSVs ranges from 0:2m to 10m [113]. In this work, we
use the TSV diameter of 10m, and assume that the keepout area diameter is 2.5X of TSV diameter, which is 25m.

78
8. COST ANALYSIS FOR 3D ICS
Figure 8.1: An example of early design estimation of the die area and the metal layer under 65nm
process [111]. ÓÄÄis estimation is well correlated with the state-of-the-art microprocessor designs. For
example, Sun SPARC T2 [112] contains about 500 million transistors (roughly equivalent to 125
million gates), with an area of 342mm2 and 11 metal layers).
While the Ô¨Årst eÔ¨Äect is already explained by Fig. 8.1 showing the existence of TSVs lead
to a potential metal layer reduction on the horizontal direction, modeling the second eÔ¨Äort needs
extra eÔ¨Äorts by adding the estimation of the number of TSVs and their associated silicon area
overhead.
To predict the number of required TSVs for a certain partition pattern, a derivation of
Rent‚Äôs Rule [106] describing the relationship between the interconnect count (X) and the gate
count (Ngate) can be used. ÓÄÄis interconnect-gate relationship is formulated as follows,
X D ÀõkNgate

1   N p 1
gate

(8.9)
where Àõ is calculated by Eq. 8.4.
As illustrated in Fig. 8.2, if a planar 2D design is partitioned into two separate dies whose
gate counts are N1 and N2, respectively, by using Eq. 8.9, the interconnect count on each layer,
X1 and X2, can be calculated, respectively, as follows,
X1 D Àõk1N1

1   N p1 1
1

(8.10)
X2 D Àõk2N2

1   N p2 1
2

:
(8.11)
www.ebook3000.com

8.1. 3D COST MODEL
79
Table 8.1: ÓÄÄe parameters used in the metal layer estimation model
Rent‚Äôs exponent for logic, plogic
0.63
Rent‚Äôs coeÔ¨Écient for logic, klogic
0.14
Rent‚Äôs exponent for memory, pmemory
0.12
Rent‚Äôs coeÔ¨Écient for memory, kmemory
6.00
Average gate fanout, f:o:
4
Utilization eÔ¨Éciency of layer 1, 1
9%
Utilization eÔ¨Éciency of layer 2, 2
25%
Utilization eÔ¨Éciency of layer 3, 3
54%
Utilization eÔ¨Éciency of layer 4, 4
38%
Utilization eÔ¨Éciency of other odd layers, 5;7;9;11;:::
60%
Utilization eÔ¨Éciency of other even layers, 6;8;10;12;:::
42%
Wire pitch of lower metal layers, wi.i<9/
.i C 1/F
Wire pitch of higher metal layers, wi.i9/
10F
Blockage area per via on layer i, Avi
.wi C 3F /2
Figure 8.2: ÓÄÄe basic idea of how to estimate the number of TSVs [111].
Irrespective of the partition pattern, the total interconnect count of a certain design always keeps
constant. ÓÄÄus, the number of TSVs can be calculated as follows,
XTSV D Àõk1;2.N1 C N2/
 1   .N1 C N2/p1;2 1
 Àõk1N1

1   N p1 1
1

  Àõk2N2

1   N p2 1
2

(8.12)
where k1;2 and p1;2 are the equivalent Rent‚Äôs coeÔ¨Écient and exponent, and they are derived as
follows [114],
p1;2
D
p1N1 C p2N2
N1 C N2
(8.13)
k1;2
D

kN1
1 kN2
2
1=.N1CN2/
(8.14)

80
8. COST ANALYSIS FOR 3D ICS
KGD Test 
Coverage
3D Chip 
Cost
Wafer/Die 
Yield
Die
Temperature
Package 
Yield
Bonding 
Yield
W2W Stacking
D2W Stacking
Wafer
Test Cost
Cooling 
Cost
Package
Cost
Wafer
 Cost
3D 
Bonding 
Cost
Die Area
I/O
Count
Stacked
Die Test 
Cost
Number 
of TSVs
KGD Test 
Cost
F2F/F2B 
Bonding
Figure 8.3: Overview of the proposed 3D cost model, which includes four components: wafer cost
model, 3D bonding cost model, package cost model, and cooling cost model [111].
where Ni is the gate count on layer i.
ÓÄÄerefore, after adding the area overhead caused by TSVs, we formulate the new TSV-
included area estimation, A3D, as follows:
A3D D Adie C NTSV=dieATSV
(8.15)
where Adie is calculated by die area estimator, NTSV=die is the equivalent number of TSVs on each
die, and ATSV is the size of TSVs (including the keepout area). In practice, the Ô¨Ånal TSV-included
die area, A3D, is used in the wafer cost estimation later, while Adie is used in Eq. 8.7 for metal
layer estimation because that is the actually die area available for routing.
After estimating the die area of each layer, the number of required metal layers, and the
number of TSVs across planar dies in the previous section, a complete set of cost models are
proposed in this section. Our 3D IC cost estimation is composed of four separate parts: wafer
cost model, 3D bonding cost model, package cost model, and cooling cost model. An overview
of this proposed cost model set is illustrated in Fig. 8.3.
www.ebook3000.com

8.1. 3D COST MODEL
81
Wafer Cost Model.
ÓÄÄe wafer cost model estimates the cost of each separate planar die. ÓÄÄis
cost estimation includes all the cost incurred before the 3D bonding process. Before estimating
the die cost, we Ô¨Årst model the wafer cost.
ÓÄÄe wafer cost is determined by several parameters, such as fabrication process type (i.e.,
CMOS logic or DRAM memory), process node (from 180nm down to 22nm), wafer diameter
(200mm or 300mm), and the number of metal layers (polysilicon, aluminum, or copper depending
on the fabrication process type). We model the wafer cost by dividing it into two parts: a Ô¨Åxed
part that is determined by process type, process node, wafer diameter, and the actual fabrication
vendor; a variable part that is mainly contributed to the number of metal layers. We call the Ô¨Åxed
part silicon cost and the variable part metal cost. ÓÄÄerefore, the wafer cost is expressed as follows:
Cwafer D Csilicon C Nmetal  Cmetal
(8.16)
where Cwafer is the wafer cost, Csilicon is the silicon cost (the Ô¨Åxed part), Cmetal is the metal cost
per layer, and Nmetal is the number of metal layers. All the parameters used in this model are from
the ones collected by IC Knowledge LLC [115], which considers several factors including material
cost, labor cost, foundry margin, number of reticles, cost per reticle, and other miscellaneous cost.
Figure 8.4 shows the predicted wafer cost of 90nm, 65nm, and 45nm processes, with 9 or 10 layers
of metal, for three diÔ¨Äerent foundries, respectively.
Besides the number of metal layers aÔ¨Äecting the cost as demonstrated by Eq. 8.16, the die
area is another key factor that aÔ¨Äects the bare die cost since smaller die area implies more dies per
wafer. ÓÄÄe number of dies per wafer is formulated by Eq. 8.17 [116] as follows,
Ndie D   .wafer=2/2
Adie
    wafer
p2  Adie
(8.17)
where Ndie is the number of dies per wafer, wafer is the diameter of the wafer, and Adie is the
die area. After having estimated the wafer cost by Eq. 8.16 and the number of dies per wafer by
Eq. 8.17, the bare die cost can be expressed as follows,
Cdie D Cwafer
Ndie
:
(8.18)
In addition, the die area also relates to the die yield, which later aÔ¨Äects the net die cost. By
assuming rectangular defect density distribution, the relationship between the die area and die
yield can be formulated as follows [117],
Ydie D Ywafer 
 1   e 2AdieD0
2AdieD0
(8.19)
where Ydie and Ywafer are the yields of dies and wafers, respectively, and D0 is the wafer defect
density. ÓÄÄerefore, the net die cost is Cdie=Ydie.

82
8. COST ANALYSIS FOR 3D ICS
Figure 8.4: A batch of data calculated by the wafer cost model [111]. ÓÄÄe wafer cost varies from
diÔ¨Äerent processes, diÔ¨Äerent number of metal layers, diÔ¨Äerent foundries, and some other factors.
3D Bonding Cost Model.
ÓÄÄe 3D bonding cost model estimated the cost incurred during the
process that integrates several planar 2D dies together using the TSV-based technology. ÓÄÄe extra
fabrication steps required by 3D integrations consist of TSV forming, thinning, and bonding.
ÓÄÄere are two ways to build 3D TSVs: laser drilling or etching. Laser drilling is only suitable
for a small number of TSVs (hundreds to thousands) while etching is suitable for a large number
of TSVs. Furthermore, there are two approaches for TSV etching: (1) TSV-Ô¨Årst approach: TSVs
can be formed during the 2D die fabrication process, before the Back-End-of-Line (BEOL)
processes. Such an approach is called TSV-Ô¨Årst approach, and is shown in Fig. 8.5(a); (2) TSV-
last approach: TSVs can also be formed after the completion of 2D fabrications, after the BEOL
processes. Such an approach is called TSV-later approach, and is shown in Fig. 8.5(b). Either
www.ebook3000.com

8.1. 3D COST MODEL
83
Figure 8.5: Fabrication steps for 3D ICs: (a) TSVs are formed before BEOL process, thus TSVs
only punch through the silicon substrate but not the metal layers; (b) TSVs are formed after BEOL
process, thus TSvs. punch through not only the silicon substrate but the metal layers as well [111].
approach has advantages and disadvantages. TSV-Ô¨Årst approach builds TSVs before metal layers,
thus there are no TSVs punching through metal layers and hence no TSV area overhead; TSV-last
approach has TSV area overhead, but it isolates the die fabrication from 3D bonding, which does
not need to change the traditional fabrication process. In order to separate the wafer cost model
and the 3D bonding cost model, we assume TSV-last approach is used in 3D IC fabrication. ÓÄÄe
TSV area overhead caused by TSV-last approach is modeled by Eq. 8.15. ÓÄÄe data of the 3D
bonding cost including the cost of TSV forming, wafer thinning, and wafer (or die) bonding are
obtained from our industry partner, with the assumption that the yield of each 3D process step
is 99%.
Combined with the wafer cost model, the 3D bonding cost model can be used to estimate
the cost of a 3D-stacked chip with multiple dies. In addition, the entire 3D-stacked chip cost de-
pends on some design options. For instance, it depends on whether die-to-wafer (D2W) or wafer-
to-wafer (W2W) bonding is used, and it also depends on whether face-to-face (F2F) or face-to-
back (F2B) bonding is used. If D2W bonding is selected, cost of Known-Good-Die (KGD) test
should also be included [118].
For D2W bonding, the cost of a bare N-layer 3D-stacked chip before packaging is calcu-
lated as follows,
CD2W D
PN
iD1
 Cdiei C CKGDtest

=Ydiei C .N   1/Cbonding
Y N 1
bonding
(8.20)
where CKGDtest is the KGD test cost, which we model as Cwafersort=Ndie, and the wafer sort cost,
Cwafersort, is a constant value for a speciÔ¨Åc process in a speciÔ¨Åc foundry in our model. Note that
the testing cost for 3D IC itself is a complicated problem. Our other study has demonstrated
a more detailed test cost analysis model with the study of design-for-test (DFT) circuitry and
various testing strategies, showing the variants of testing cost estimations [119]. For example,
adding extra DFT circuitry to improve the yield of each die in D2W stacking can help the cost

84
8. COST ANALYSIS FOR 3D ICS
reduction but the increased area may increase the cost. In this book, we adopt the above testing
cost assumptions to simplify the total cost estimation, due to space limitation.
For W2W bonding, the cost is calculated as follows,
CW2W D
PN
iD1 Cdiei C .N   1/Cbonding
 Àò N
iD1Ydiei

Y N 1
bonding
:
(8.21)
In order to support multiple-layer bonding, the default bonding mode is F2B. If F2F mode
is used, there is one more component die that does not need the thinning process, and the thinning
cost of this die is subtracted from the total cost.
Package Cost Model.
ÓÄÄe package cost model estimates the cost incurred during the packaging
process, which can be determined by three factors: package type, package area, and pin count.
As an example, we select the package type to be Ô¨Çip-chip Land Grid Array (fcLGA) in
this book. fcLGA is the common package type of multiprocessors, while other types of packages,
such as fcPGA, PGA, pPGA, etc., are also available in our model. Besides package type, package
area and pin count are the other two key factors that determine the package cost. By using the
early stage estimation methodology as mentioned in Sec. 8.1, the package area can be estimated
from the die area, A3D in Eq. 8.15, and the pin count can be estimated by Rent‚Äôs Rule in Eq. 8.2.
We analyze the actual package cost data and Ô¨Ånd that the number of pins becomes the dominant
factor to the package cost when the die area is much smaller than the total area of pin pads. It is
easy to understand, since there is a base material and production cost per pin which will not be
reduced with die area shrinking.
Figure 8.6 shows the sample data of package cost obtained from IC Knowledge LLC [115].
Based on this set of data, we use curve Ô¨Åtting to derive a package cost function with the parameters
of the die area and the pin count, which can be expressed as follows,
Cpackage D 1Np C 2AÀõ
3D
(8.22)
where Np is the pin count, A3D is the die area. In addition, 1, 2, and Àõ are the coeÔ¨Écients and
exponents, which are 0:00515, 0:141 and 0:35, respectively. By observing Eq. 8.22, we can also
Ô¨Ånd that pin count will dominate the package cost if the die area is suÔ¨Éciently small, since the
order of pin count is higher than that of area. Figure 8.6 also shows the curve Ô¨Åtting result, which
is well aligned to the raw data.
Cooling Cost Model.
Although the combination of the aforementioned wafer cost model, 3D
bonding cost model, and package cost model can already oÔ¨Äer the cost of a 3D-stacked chip after
packaging, we add the cooling cost model because it has been widely noticed that 3D-stacked
chip has higher working temperature than its 2D counterpart and it might need a more powerful
cooling solution that causes extra cost.
Gunther, et al. [120] noticed that the cooling cost is related to the power dissipation of
the system. Furthermore, the system power dissipation is highly related to the chip working tem-
www.ebook3000.com

8.1. 3D COST MODEL
85
2
Figure 8.6: ÓÄÄe package cost depends on both pin count and die area [111].
perature [69]. Our cooling cost estimation is based on the peak steady state temperature of the
targeted 3D-stacked multiprocessor.
ÓÄÄere are many types of cooling solutions, ranging from a simple extruded aluminum
heatsink to an elaborate vapor phase refrigeration. Depending on the cooling mechanisms, cool-
ing solutions used today can be deÔ¨Åned as either convection cooling, phase-change cooling,
thermoelectric cooling (TEC), or liquid cooling [121]. Typical convection cooling solutions are
heatsinks and fans, which are widely adopted for the microprocessor chips in today‚Äôs desktop
computers. Phase-change cooling solutions, such as heatpipes, might be used in laptop comput-
ers. In addition, thermoelectric cooling and liquid cooling are used in some high-end computers.
We collect the cost of all the cooling solutions from the commercial market by searching the data
from Digikey [122] and Heatsink Factory [123]. We Ô¨Ånd that more powerful types of cooling so-
lutions often lead to higher costs as expected. Table 8.2 is a list of typical prices of these cooling
solutions.
In our model, we further assume that cooling cost increases linearly with the rise of chip
temperature, if the same type of cooling solution is adopted. Based on this assumption and the
data listed in Table 8.2, the cooling cost is therefore estimated as follows:
Ccooling D KcT C c
(8.23)

86
8. COST ANALYSIS FOR 3D ICS
Table 8.2: ÓÄÄe cost of various cooling solutions
Cooling Solution
Unit Price $
Heatsink
3   6
Fan
10   40
TEC
15   20
Heatpipe
30   70
Liquid Cooling
75 Table 8.3: ÓÄÄe values of Kc and c in Eq. (8.23), which are related to the chip temperature
Chip Temperature ( ƒ±C)
Kc
c
< 60
0:2
 6
60   90
0:4
 16
90   120
0:2
2
120   150
1:6
 170
150   180
2
 230
where T is the temperature from which the cooling solution can bring down, Kc and c are the
cooling cost parameters, which can be determined by Table 8.3.
Figure 8.7 shows the cost of these Ô¨Åve types of cooling solutions. It can be observed that the
chips with higher steady state temperatures will require more powerful cooling solutions, which
lead to higher costs. It is also illustrated in Fig. 8.7 that the cooling cost is not a global linear
function of the temperature, whereas there are several regions with linear increase of the cost.
Each of the regions is correspondent to one type of cooling solution.
8.2
COST EVALUATION FOR MANY-CORE
MICROPROCESSOR DESIGNS
In the previous section, we evaluated the cost beneÔ¨Åt of 3D fabrication for fully customized ASIC
designs. However, the gate-level Ô¨Åne-granularity 3D partitioning is not feasible in the near future
since it requires all the basic modules (such as adders, multipliers, etc.) to be redesigned in a 3D-
partitioned way. Hence, as a near-term goal, the module-level coarse-granularity 3D partitioning
is regarded as a more realistic way for 3D IC implementation, such as system-on-chip (SoC) style
or many-core microprocessor design style,‚Å¥ where each IP block (such as each core/memory IP)
remains to be a 2D block and the 3D partitioning is performed at the block level. In such design
style, the Rent‚Äôs rule-based TSV estimation is not valid, and the TSV estimation would have to
be carried out based on system-level speciÔ¨Åcation (such as the signal connections between cores
‚Å¥In this book, a ‚Äúmulti-core‚Äù chip refers to eight or less homogeneous cores, whereas a ‚Äúmany-core‚Äù chip has more than eight
homogeneous cores in one microprocessor package.
www.ebook3000.com

8.2. COST EVALUATION FOR MANY-CORE MICROPROCESSOR DESIGNS
87
‘®
Figure 8.7: A plot for the cooling cost model: the cooling cost increases linearly with the chip temper-
ature if the same cooling solution is used; more powerful cooling solutions result in higher costs [111].
and memory). In this section, we demonstrate how to use the remaining parts of the proposed
3D cost model to estimate the 3D many-core microprocessor cost. We use 45nm IBM common
platform cost model in this case study.
Baseline 2D ConÔ¨Åguration.
In order to have a scalable architecture for many-core microproces-
sors, the baseline 2D design used in our study is an NoC-based architecture, in which the process-
ing tile is the basic building block. Each processing tile is composed of one in-order SPARC-like
microprocessor core [112], one L1 cache, one L2 cache, and one router. Figure 8.8 shows the
example of such architecture with 64 tiles connected by an 8-by-8 mesh structure. Table 8.4 lists
the detail of cores, L1, L2 caches, and routers, respectively. ÓÄÄe gate count of the core and the
router modules is obtained from empirical data of existing designs [112], and the cell count of the
cache module by only considering the SRAM cells and ignoring the cost of peripheral circuits.
In this case study, we simply assume logic gates and SRAM cells (6-transistor model) occupy the
same die area. We consider the resulting estimation error to be tolerable since it is an early design
stage estimation. Note that we set the memory cell count (from L1 and L2 caches) to be close
to the logic gate count (from cores and routers) by tuning cache capacities. ÓÄÄis manual tuning is
just for the convenience of our later heterogeneous stacking. Generally, there are no constraints
on the ratio between the logic and the memory modules.
Figure 8.9 illustrates two ways to implement a 3D dual-core microprocessor, and it can be
conceptually extended to the many-core microprocessor case. Generally, the two types of parti-

88
8. COST ANALYSIS FOR 3D ICS
Figure 8.8: Structure of the baseline 2D multiprocessor, which consists of 64 tiles [111].
tioning strategies are: homogeneous and heterogeneous. As shown in Fig. 8.9(a), all the layers after
homogeneous partitioning are identical, while heterogenous partitioning leads to core (logic) layers
and cache (memory) layers as shown in Fig. 8.9(b).
8.2.1
COST EVALUATION WITH HOMOGENEOUS PARTITIONING
We Ô¨Årst explore the homogeneous partitioning strategies, in which all the processing tiles are dis-
tributed into 1, 2, 4, 8, and 16 layers, respectively, without breaking tiles into Ô¨Åner granularity.
Figure 8.10 illustrates the conceptual view of a 4-layer homogeneous stacking. ÓÄÄe role of
TSVs in the homogeneous 3D-stacked chip is to provide the inter-layer NoC connection and
to deliver the power from I/O pins to inner layers. Instead of using Eq. 8.12, the TSV count is
predicted on the basis of NoC bus width. We assume 3D mesh is used and the NoC Ô¨Çit size is
128 bits. ÓÄÄerefore, the number of TSVs between two layers, NTSV, is estimated as,
NTSV D 2  128  Ntiles=layer  50%
(8.24)
www.ebook3000.com

8.2. COST EVALUATION FOR MANY-CORE MICROPROCESSOR DESIGNS
89
Table 8.4: ÓÄÄe conÔ¨Åguration of the SPARC-like core in a tile of the baseline 2D multiprocessor
Processor Cores
Clock frequency
1.2 GHz
Architecture type
in-order
INT pipeline
6 stages
FP pipeline
6 stages
ALU count
1
FPU count
1
Empirical gate count
1.7 million
Routers
Type
4 ports
Empirical gate count
1.0 million
Caches
Cache line size
64B
L1 I-cache capacity
32 KB
L1 D-cache capacity
32 KB
L2 cache capacity
256 KB
Empirical cell count
2.6 million
Figure 8.9: ÓÄÄree ways to implement a 3D dual-core microprocessor [111]
where Ntiles=layer is the number of tiles per layer, 2 means the bi-directional channel between two
tiles, and 50% is added due to our assumption that half TSVs are for power delivery. After cal-
culation, the TSV area overhead is around 3.8%.

90
8. COST ANALYSIS FOR 3D ICS
Figure 8.10: ÓÄÄe conceptual view of a 4-layer homogeneous stacking composed of 4 identical lay-
ers [111].
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
3
5
0
4
0
0
2
D
2
	
L
a
y
e
r
4
	
L
a
y
e
r
8
	
L
a
y
e
r
1
6
	
L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
N
	
1
l
a
y
e
r
s
)
D
i
e
c
o
s
t
(
1
l
a
y
e
r
)
Figure 8.11: ÓÄÄe cost breakdown of a 16-core microprocessor design with diÔ¨Äerent homogeneous
partitioning [111].
In the next step, we use the cost model set to estimate the cost of wafer, 3D bonding, pack-
age, and cooling, respectively. We analyze microprocessor designs of diÔ¨Äerent scales. Figure 8.11
to Fig. 8.14 show the estimated price breakdown of 16-core, 32-core, 64-core, and 128-core mi-
croprocessors when they are fabricated by 1-layer, 2-layer, 4-layer, 8-layer, and 16-layer processes,
respectively. In these Ô¨Ågures, all the prices are divided into Ô¨Åve parts (the Ô¨Årst bar, which repre-
sents 2D planar process, only has three parts as the 2D process only has one die and does not
need the bonding process). From bottom up, they are the cost of one die, the cost of remaining
dies, the bonding cost, the package cost, and the cooling cost.
As the Ô¨Ågures illustrate, the cost of one die is descending when the number of layers is
ascending. ÓÄÄis descending trend is mainly due to the yield improvement brought by the smaller
die size. However, this trend is Ô¨Çattened at the end since the yield improvement is not signiÔ¨Åcant
www.ebook3000.com

8.2. COST EVALUATION FOR MANY-CORE MICROPROCESSOR DESIGNS
91
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
3
5
0
4
0
0
4
5
0
2
D
2

L
a
y
e
r
4

L
a
y
e
r
8

L
a
y
e
r
1
6

L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
N

1
l
a
y
e
r
s
)
D
i
e
c
o
s
t
(
1
l
a
y
e
r
)
Figure 8.12: ÓÄÄe cost breakdown of a 32-core microprocessor design with diÔ¨Äerent homogeneous
partitioning [111].
0
1
0
0
2
0
0
3
0
0
4
0
0
5
0
0
6
0
0
7
0
0
2
D
2
	
L
a
y
e
r
4
	
L
a
y
e
r
8
	
L
a
y
e
r
1
6
	
L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
N
	
1
l
a
y
e
r
s
)
D
i
e
c
o
s
t
(
1
l
a
y
e
r
)
Figure 8.13: ÓÄÄe cost breakdown of a 64-core microprocessor design with diÔ¨Äerent homogeneous
partitioning [111].
any more when the die size is suÔ¨Éciently small. As a result, the cumulative cost of all the dies
does not follow a descending trend. Combined with the 3D bonding cost, which is an ascending
function to the layer count, the wafer cost usually hits its minimum at some middle points. For
example, if only considering the wafer cost and the 3D bonding cost, the optimal number of layers
for 16-core, 32-core, 64-core, and 128-core designs are 2, 4, 4, 8, respectively.
However, the cost before packaging is not the Ô¨Ånal cost. System-wise, the Ô¨Ånal cost needs
to include the package cost and the cooling cost. As mentioned, the package cost is mainly de-
termined by the pin count. ÓÄÄus, the package cost almost remains constant in all the cases since
the pin count does not change from one partitioning to another partitioning. ÓÄÄe cooling cost
mainly depends on the chip temperature. As more layers are vertically stacked, the higher the
chip temperature is, the cooling cost grows with the increase of 3D layers, and this growth is
quite fast because a 3D-stacked chip with more than eight layers can easily reach the peak tem-
perature of more than 150ƒ±C. Actually, the extreme cases in this experiment (such as 16-layer

92
8. COST ANALYSIS FOR 3D ICS
 
‚úÅ
 
 
‚úÇ
 
 
‚úÑ
 
 
‚òé
 
 
‚úÜ
 
 
 
‚úÜ
‚úÅ
 
 
‚úÜ
‚úÇ
 
 
‚úÜ
‚úÑ
 
 
‚úÅ
‚úù
‚úÅ
‚úû
‚úü
‚ú†
‚ú°
‚òõ
‚òû
‚úÇ
‚úû
‚úü
‚ú†
‚ú°
‚òõ
‚òû
‚òé
‚úû
‚úü
‚ú†
‚ú°
‚òõ
‚òû
‚úÜ
‚úÑ
‚úû
‚úü
‚ú†
‚ú°
‚òõ
‚òû
‚úå
‚úç
‚úé
‚úè
‚úë
‚úí
‚úì
‚úî
‚úì
‚úï
‚úñ
‚úó
‚úó
‚úò
‚úô
‚úö
‚úõ
‚úú
‚úó
‚ú¢
‚ú£
‚ú§
‚ú†
‚úú
‚ú•
‚ú†
‚úõ
‚òõ
‚úñ
‚úó
‚ú¢
‚ú£
‚ú¶
‚úó
‚úö
‚úß
‚úô
‚úö
‚úõ
‚úú
‚úó
‚ú¢
‚ú£
‚úù
‚úô
‚òõ
‚úú
‚úó
‚ú¢
‚ú£
‚òÖ
‚ú©
‚úû
‚úÜ
‚úò
‚ú†
‚ú°
‚òõ
‚òû
‚ú¢
‚ú™
‚úù
‚úô
‚òõ
‚úú
‚úó
‚ú¢
‚ú£
‚òÖ
‚úÜ
‚úò
‚ú†
‚ú°
‚òõ
‚òû
‚ú™
Figure 8.14: ÓÄÄe cost breakdown of a 128-core microprocessor design with diÔ¨Äerent homogeneous
partitioning [111].
stacking) is not practical indeed as the underlying chip is severely overheated. In this experiment,
we Ô¨Årst obtain the power data of each component (the core, router, L1 cache, and L2 cache) from
a power estimation tool, McPAT [103]. Combined with the area data achieved from early design
stage estimation, we get the power density data, then feed them into HotSpot [121], which is
a 3D-aware thermal estimation tool, and Ô¨Ånally get the estimated chip temperature and its cor-
responding cost of cooling solutions. Figure 8.11 to Fig. 8.14 also show the cost of packaging
and cooling. As we can Ô¨Ånd, the cooling cost is just a small portion for few layer stacking (such
as 2-layer), but it starts to dominate the total cost for aggressive stacking (such as 8-layer and
16-layer). ÓÄÄerefore, the optimal partitioning solution in terms of the minimum total cost diÔ¨Äers
from the one in terms of the cost only before packaging. Observed from the result, the optimal
number of layers for 16-core, 32-core, 64-core, and 128-core become 1, 2, 2, and 4, respectively
instead. ÓÄÄis result also reveals our motivation for why the package cost and the cooling cost are
included in the decision of optimal 3D stacking layers.
8.2.2
COST EVALUATION WITH HETEROGENEOUS PARTITIONING
Similar to our baseline design illustrated in Fig. 8.8, today‚Äôs high-performance microprocessors
have a large portion of the silicon area occupied by on-chip SRAM or DRAM caches. In addi-
tion, non-volatile memory can also be integrated as on-chip memory [124]. However, diÔ¨Äerent
fabrication processes are not compatible. For example, the CMOS logic module might require
1-poly-9-copper-1-aluminum interconnect layers, while DRAM memory module needs 7-poly-
3-copper, and Ô¨Çash memory module needs 4-poly-1-tungsten-2-aluminum. As a result, integrat-
ing heterogeneous modules in a planar 2D chip would dramatically increase the chip cost. Intel
shows that heterogeneous integration for large 2D System-on-Chip (SoC) would boost the chip
cost by three times [125].
However, fabricating heterogeneous modules separately could be a cost-eÔ¨Äective way for
such systems. For example, if the SRAM cache modules are fabricated separately, as per our es-
www.ebook3000.com

8.2. COST EVALUATION FOR MANY-CORE MICROPROCESSOR DESIGNS
93
Figure 8.15: ÓÄÄe conceptual view of a 4-layer heterogeneous stacking composed of two logic layers
and two memory layers [111].
timation, the required number of metals is only 6 while the logic modules (such as cores and
routers) usually need more than 10 layers of metals. Hence, We reevaluate the many-core micro-
processor cost by using heterogeneous integration, in which the tile is broken into logic parts (i.e.,
core and router) and memory parts (i.e., L1 and L2 caches).
Figure 8.15 illustrates a conceptual view of 4-layer heterogeneous partitioning where logic
modules (cores and routers) and memory modules (L1 and L2 caches) are on separate dies. As
shown in Fig. 8.15, there are two types of TSVs in the heterogeneous stacking chip: one is for
NoC mesh interconnect, which is the same as that in the homogeneous stacking case; the other
is for the interconnect between cores and caches, which is caused by the separation of the logic
and memory modules. ÓÄÄe number of the extra core-to-cache TSVs is calculated as follows,
NextraTSV D

Data C
X
Addressi

 Ntiles=layer
(8.25)
where Data is the cache line size and Addressi is the address width of cache i. Note that the
number of tiles per layer, Ntiles=layer is doubled in the heterogeneous partitioning compared to its
homogeneous counterpart. For our baseline conÔ¨Åguration (Table 8.4), in which IL1 is 32KB, DL2
is 32KB, L2 is 256KB, and cache line size is 64B, Data C P Addressi is 542. As a side-eÔ¨Äect,
the extra core-to-cache TSVs cause the TSV area overhead increasing from 3.8% to 15.7%.
Compared to its homogeneous counterpart, the advantages and disadvantages of partition-
ing an NoC-based many-core microprocessor in the heterogeneous way are,
‚Ä¢ Fabricating logic and memory dies separately reduces the wafer cost;
‚Ä¢ Putting logic dies closer to heat sink reduces the cooling cost;

94
8. COST ANALYSIS FOR 3D ICS
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
3
5
0
2
D
2

L
a
y
e
r
4

L
a
y
e
r
8

L
a
y
e
r
1
6

L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
o
t
h
e
r
d
i
e
s
)
D
i
e
c
o
s
t
(
1
m
e
m
o
r
y
d
i
e
)
D
i
e
c
o
s
t
(
1
l
o
g
i
c
d
i
e
)
Figure 8.16: ÓÄÄe cost breakdown of a 16-core microprocessor design with diÔ¨Äerent heterogeneous
partitioning [111].
0
5
0
1
0
0
1
5
0
2
0
0
2
5
0
3
0
0
3
5
0
4
0
0
4
5
0
2
D
2

L
a
y
e
r
4

L
a
y
e
r
8

L
a
y
e
r
1
6

L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
o
t
h
e
r
d
i
e
s
)
D
i
e
c
o
s
t
(
1
m
e
m
o
r
y
d
i
e
)
D
i
e
c
o
s
t
(
1
l
o
g
i
c
d
i
e
)
Figure 8.17: ÓÄÄe cost breakdown of a 32-core microprocessor design with diÔ¨Äerent heterogeneous
partitioning [111].
‚Ä¢ ÓÄÄe higher TSV area overhead increases the wafer cost and the package cost.
In order to know whether heterogenous integration is more cost-eÔ¨Äective, we repeat the
cost estimation for 16-core, 32-core, 64-core, and 128-core microprocessor designs by using het-
erogenous partitioning. ÓÄÄe price breakdown of heterogeneous stacking is shown in Fig. 8.16 to
Fig. 8.19. Compared to the homogeneous integration cost as illustrated in Fig. 8.11 to Fig. 8.14,
we can observe that the cost of heterogeneous integration is higher than that of homogenous inte-
gration in most cases, which is mainly due to the increased die area caused by higher TSV counts.
However, in some cases, the cost of heterogenous integration can be cheaper compared to its ho-
mogeneous counterparts. Figure 8.20 demonstrates the cost comparison between homogeneous
and heterogeneous integrations for a 64-core microprocessor design using the 45nm 4-layer 3D
process. It can be observed that although the fabrication eÔ¨Éciency of separated logic and memory
www.ebook3000.com

8.2. COST EVALUATION FOR MANY-CORE MICROPROCESSOR DESIGNS
95
Figure 8.18: ÓÄÄe cost breakdown of a 64-core microprocessor design with diÔ¨Äerent heterogeneous
partitioning [111].
0
2
0
0
4
0
0
6
0
0
8
0
0
1
0
0
0
1
2
0
0
1
4
0
0
1
6
0
0
2
D
2

L
a
y
e
r
4

L
a
y
e
r
8

L
a
y
e
r
1
6

L
a
y
e
r
C
o
s
t
(
a
.
u
.
)
C
o
o
l
i
n
g
c
o
s
t
P
a
c
k
a
g
e
C
o
s
t
B
o
n
d
i
n
g
c
o
s
t
D
i
e
c
o
s
t
(
o
t
h
e
r
d
i
e
s
)
D
i
e
c
o
s
t
(
1
m
e
m
o
r
y
d
i
e
)
D
i
e
c
o
s
t
(
1
l
o
g
i
c
d
i
e
)
Figure 8.19: ÓÄÄe cost breakdown of a 128-core microprocessor design with diÔ¨Äerent heterogeneous
partitioning [111].
dies cannot oÔ¨Äset the extra cost caused by the increased die area, the cooling cost is reduced after
putting logic layers, which closer to the heat sink (or other cooling solutions).
While our experiment shows heterogeneous integration has the cost advantages on cheaper
memory layer and lower chip temperature, in reality, some designers might still prefer homoge-
neous integration, which has identical layout on each die. ÓÄÄe reciprocal design symmetry (RDS)
was proposed by Alam, et al. [126] to re-use one mask for multiple 3D-stacked layers. ÓÄÄe RDS
could relieve the design eÔ¨Äort, and thus reduce the design time, which means a cost reduction on
human resources. However, this personnel cost is not included in our cost model set, and it is out
of the scope of this book.

96
8. COST ANALYSIS FOR 3D ICS
Figure 8.20: ÓÄÄe cost comparison between homogeneous and heterogeneous integrations for a 64-
core microprocessor using 45nm 2-layer 3D process [111].
Table 8.5: ÓÄÄe optimal number of 3D layers for many-core microprocessor designs (45nm)
Homogeneous
Heterogeneous
16-core
1
1
32-core
2
2
64-core
2
4
128-core
4
8
We list the optimal partitioning options for 45nm many-core microprocessor designs in
Table 8.5, which shows how our proposed cost estimation methodology helps decide the 3D
partitioning at the early design stage.
Conclusion.
To overcome the barriers in technology scaling, a three-dimensional integrated
circuit (3D IC) is emerging as an attractive option for future microprocessor designs. However,
fabrication cost is one of the important considerations for the wide adoption of the 3D integration.
System-level cost analysis at the early design stage to help the decision making on whether 3D
integration should be used for the application is very critical.
To facilitate the early design stage cost analysis, we propose a set of cost models that in-
clude wafer cost, 3D bonding cost, package cost, and cooling cost. Based on the cost analysis, we
identify the design opportunities for cost reduction in 3D ICs, and provide a few design guide-
lines on cost-eÔ¨Äective 3D IC designs. Our research is complementary to the existing research
on 3D microprocessors that are focused on other design goals, such as performance and power
consumption.
www.ebook3000.com

97
C H A P T E R
9
Conclusion
In this book, we have reviewed the background of emerging 3D integration technologies and
explored various architecture designs that employ 3D integration. 3D integration technolo-
gies promise high performance, low power, low cost, and high density microprocessor archi-
tecture solutions. It is an attractive solution in developing high-performance, energy-eÔ¨Écient,
thermal-aware, and cost-eÔ¨Äective chip-multiprocessors and GPU systems. In particular, with
chip-multiprocessors, 3D integration provides low wire latency in connecting processor cores
and caches. With GPU systems, 3D integration is promising in developing high-bandwidth, low
power graphics memory interface. 3D integration also enlarges the capacity of on-chip memory,
which can be employed as the last-level cache, a portion of main memory, or the combination of
both.
Although 3D integration brings in great beneÔ¨Åts and opportunities in architecture design,
several challenges exist on the way toward its wide adoption in future computer systems. ÓÄÄis
book has discussed two of them‚Äîthermal and cost‚Äîand explored thermal-aware and cost-aware
microprocessor design techniques. Beyond these two challenges, the following two design chal-
lenges are also critical towards the wide adoption of 3D integration technologies.
Design tools and methodologies.
3D integration technologies will not be commercially viable
without the support of electronic design automation (EDA) tools and methodologies. Given par-
ticular design goals, eÔ¨Écient EDA tools and methodologies can help architects and circuit de-
signers make decisions on whether to adopt 3D or 2D integration. ÓÄÄey can also help designers
address design trade-oÔ¨Äs in performance, power, and cost, when 3D integration is adopted. Fur-
thermore, 3D ICs may require new layout rules that might be driven by features on adjacent die;
with larger sizes compared with conventional vias, TSvs. can illustrate signiÔ¨Åcant new layout fea-
ture; power planning in 3D ICs still requires substantial studies due to the complexity introduced
by the third dimension; eÔ¨Écient EDA tools need to take thermal constraints into consideration;
place and route tools need to consider thermal constraints of 3D ICs to avoid hot spots; eÔ¨Écient
analysis tools are required for handling electromagnetic interference concern for 3D ICs. To eÔ¨É-
ciently exploit the beneÔ¨Åts of 3D technologies, design tools and methodologies that support 3D
integration are imperative [6].
Testing methodologies.
In 3D IC design, various testing strategies and integration methods can
aÔ¨Äect system performance, power, and cost dramatically [127]. An obstacle towards the adoption
of 3D technologies is therefore insuÔ¨Écient understanding of 3D testing issues and the lack of

98
9. CONCLUSION
design-for-testability (DFT) techniques for 3D ICs. Without considering test during the design
phase, eÔ¨Äective tests on 3D ICs will become impossible. It is therefore very crucial to study options
in 3D testing methodologies. Both new standards and design tools are required to diagnose issues
properly. 3D IC fabrication incorporates many more intermediate steps than conventional 2D IC
fabrication, such as die stacking and TSV bonding. ÓÄÄese extra steps require wafer tests before
Ô¨Ånal assembly and packaging. However, a wafer test for 3D ICs is challenging in three ways. First,
existing probe technology cannot perform Ô¨Åner pitch and dimensions of TSV tips and is limited
to handling only several hundred probes at a much lower number than required TSV probes.
Second, a wafer test may require creating a known-good die (KGD) stack, which may be at the
risk of damaging due to the contact of the highly thinned wafer by a wafer probe. 3D ICs can also
impose intra-die defects, which may be caused by thermal issues and new manufacturing steps
such as wafer thinning and bonding the top of a TSV to another wafer. Consequently, new fault
models are required to address this issue. While 3D IC testing is a crucial problem, it has remained
largely unexplored in the research community and requires substantial eÔ¨Äorts in exploring them.
www.ebook3000.com

99
Bibliography
[1] J. Zhao, C. Xu, and Y. Xie, ‚ÄúBandwidth-aware reconÔ¨Ågurable cache design with hybrid
memory technologies,‚Äù in Proceedings of the International Conference on Computer-Aided
Design (ICCAD), 2011, pp. 48‚Äì55. DOI: 10.1109/ICCAD.2011.6105304. xi
[2] ‚ÄúMICRON, micron collaborates with Intel to enhance Knights landing with a high per-
formance, on-package memory solution,‚Äù http://investors.micron.com/releasede
tail.cfm?ReleaseID=856057. xii
[3] ‚ÄúNVIDIA,‚Äù http://blogs.nvidia.com/blog/2013/03/19/nvidia-ceo-updates-
nvidias-roadmap/. xii, 45
[4] ‚ÄúAMD, what is heterogeneous system architecture (HSA)?‚Äù http://developer.amd.
com/resources/heterogeneous-computing/what-is-heterogeneous-system-
architecture-hsa/. xii
[5] Y. Xie, G. Loh, B. Black, and K. Bernstein, ‚ÄúDesign space exploration for 3D ar-
chitectures,‚Äù ACM Journal of Emerging Technologies in Compuing Systems, 2006. DOI:
10.1145/1148015.1148016. 2, 7, 10, 16, 53, 65, 73
[6] Y. Xie, J. Cong, and S. Sapatnekar, ÓÅêree-Dimensional Integrated Circuit Design: EDA,
Design and Microarchitectures.
Springer, 2009. DOI: 10.1007/978-1-4419-0784-4. 2, 97
[7] P. Garrou, Handbook of 3D Integration: Technology and Applications using 3D Integrated
Circuits.
Wiley-CVH, 2008, ch. Introduction to 3D Integration. 2, 3
[8] W. R. Davis, J. Wilson, S. Mick, J. Xu, H. Hua, C. Mineo, A. M. Sule, M. Steer, and
P. D. Franzon, ‚ÄúDemystifying 3D ICs: the pros and cons of going vertical,‚Äù IEEE Design
and Test of Computers, vol. 22, no. 6, pp. 498‚Äì 510, 2005. DOI: 10.1109/MDT.2005.136.
3
[9] J. Burns, G. Carpenter, E. Kursun, R. Puri, J. Warnock, and M. Scheuermann, ‚ÄúDesign,
cad and technology challenges for future processors: 3d perspectives,‚Äù in Design Automa-
tion Conference (DAC), 2011 48th ACM/EDAC/IEEE, June 2011, pp. 212‚Äì212. DOI:
10.1145/2024724.2024772. 4, 5
[10] J. Joyner, P. Zarkesh-Ha, and J. Meindl, ‚ÄúA stochastic global net-length distribution for a
three-dimensional system-on-a-chip (3D-SoC),‚Äù in Proc. 14th Annual IEEE International
ASIC/SOC Conference, Sep. 2001. DOI: 10.1109/ASIC.2001.954688. 7

100
BIBLIOGRAPHY
[11] Y.-f. Tsai, F. Wang, Y. Xie, N. Vijaykrishnan, and M. J. Irwin, ‚ÄúDesign space exploration
for three-dimensional cache,‚Äù IEEE Transactions on Very Large Scale Integration Systems,
2008. DOI: 10.1109/ASIC.2001.954688. 7, 10
[12] B. Vaidyanathan, W.-L. Hung, F. Wang, Y. Xie, V. Narayanan, and M. J. Irwin, ‚ÄúAr-
chitecting microprocessor components in 3D design space,‚Äù in Intl. Conf. on VLSI Design,
2007, pp. 103‚Äì108. DOI: 10.1109/VLSID.2007.41. 7, 8, 13, 23
[13] K. Puttaswamy and G. H. Loh, ‚ÄúScalability of 3D-integrated arithmetic units in high-
performance microprocessors,‚Äù in Design Automation Conference, 2007, pp. 622‚Äì625. DOI:
10.1109/DAC.2007.375238.
[14] J. Ouyang, G. Sun, Y. Chen, L. Duan, T. Zhang, Y. Xie, and M. Irwin, ‚ÄúArithmetic unit
design using 180nm TSV-based 3D stacking technology,‚Äù in IEEE International 3D System
Integration Conference, 2009. DOI: 10.1109/3DIC.2009.5306565. 8
[15] R. Egawa, J. Tada, H. Kobayashi, and G. Goto, ‚ÄúEvaluation of Ô¨Åne grain 3D integrated
arithmetic units,‚Äù in IEEE International 3D System Integration Conference, 2009. DOI:
10.1109/3DIC.2009.5306566. 7, 8
[16] B. Black et al., ‚ÄúDie stacking 3D microarchitecture,‚Äù in MICRO, 2006, pp. 469‚Äì479. DOI:
10.1109/MICRO.2006.18. 8, 9
[17] G. H. Loh, ‚Äú3d-stacked memory architectures for multi-core processors,‚Äù in In-
ternational Symposium on Computer Architecture (ISCA), 2008, pp. 453‚Äì464. DOI:
10.1145/1394608.1382159. 8, 9
[18] P. Jacob et al., ‚ÄúMitigating memory wall eÔ¨Äects in high clock rate and multi-core cmos 3D
ICs: Processor memory stacks,‚Äù Proceedings of IEEE, vol. 96, no. 10, 2008.
[19] G. Loh, Y. Xie, and B. Black, ‚ÄúProcessor design in three-dimensional die-stacking tech-
nologies,‚Äù IEEE Micro, vol. 27, no. 3, pp. 31‚Äì48, 2007. DOI: 10.1109/MM.2007.59. 8
[20] T. Kgil, S. D‚ÄôSouza, A. Saidi, N. Binkert, R. Dreslinski, T. Mudge, S. Rein-
hardt, and K. Flautner, ‚ÄúPicoServer: using 3D stacking technology to enable a com-
pact energy eÔ¨Écient chip multiprocessor,‚Äù in ASPLOS, 2006, pp. 117‚Äì128. DOI:
10.1145/1168857.1168873. 9, 37
[21] S. Vangal et al., ‚ÄúAn 80-tile Sub-100-W TeraFLOPS processor in 65-nm CMOS,‚Äù
IEEE Journal of Solid-State Circuits, vol. 43, no. 1, pp. 29‚Äì41, 2008. DOI:
10.1109/JSSC.2007.910957. 9
[22] G. Loh, ‚ÄúExtending the eÔ¨Äectiveness of 3D-stacked dram caches with an adaptive multi-
queue policy,‚Äù in International Symposium on Microarchitecture (MICRO), Dec. 2009. DOI:
10.1145/1669112.1669139. 9
www.ebook3000.com

BIBLIOGRAPHY
101
[23] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, N. Vijaykrishnan, and M. Kandemir, ‚ÄúDesign
and management of 3D chip multiprocessors using network-in-memory,‚Äù in International
Symposium on Computer Architecture (ISCA‚Äô06), 2006. DOI: 10.1145/1150019.1136497.
10
[24] X. Dong, X. Wu, G. Sun, Y. Xie, H. Li, and Y. Chen, ‚ÄúCircuit and microarchitecture
evaluation of 3D stacking Magnetic RAM (MRAM) as a universal memory replacement,‚Äù
in Design Automation Conference, 2008, pp. 554‚Äì559. DOI: 10.1145/1391469.1391610.
10
[25] X. Wu, J. Li, L. Zhang, E. Speight, and Y. Xie, ‚ÄúHybrid cache architec-
ture,‚Äù in International Symposium on Computer Architecture (ISCA), 2009. DOI:
10.1145/1555815.1555761. 10, 13
[26] G. Sun, X. Dong, Y. Xie, J. Li, and Y. Chen, ‚ÄúA novel 3D stacked MRAM cache archi-
tecture for CMPs,‚Äù in International Symposium on High Performance Computer Architecture,
2009. DOI: 10.1109/HPCA.2009.4798259. 10
[27] D. Vantrease, R. Schreiber, M. Monchiero, M. McLaren, N. P. Jouppi, M. Fiorentino,
A. Davis, N. Binkert, R. G. Beausoleil, and J. H. Ahn, ‚ÄúCorona: System implications of
emerging nanophotonic technology,‚Äù in Proceedings of the 35th International Symposium on
Computer Architecture, 2008, pp. 153‚Äì164. DOI: 10.1109/ISCA.2008.35. 11
[28] X. Dong and Y. Xie, ‚ÄúCost analysis and system-level design exploration for 3D ICs,‚Äù in Asia
and South PaciÔ¨Åc Design Automation Conference, 2009. DOI: 10.1145/1509633.1509700.
11
[29] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, V. Narayanan, and M. Kandemir,
‚ÄúDesign and management of 3D chip multiprocessors using network-in-memory,‚Äù in
ISCA ‚Äô06: Proceedings of the 33rd annual international symposium on Computer Architec-
ture.
Washington, DC, USA: IEEE Computer Society, 2006, pp. 130‚Äì141. DOI:
10.1145/1150019.1136497. 13, 50
[30] G. H. Loh, ‚ÄúExtending the eÔ¨Äectiveness of 3D-stacked DRAM caches with an adaptive
multi-queue policy,‚Äù in MICRO 42: Proceedings of the 42nd Annual IEEE/ACM Interna-
tional Symposium on Microarchitecture.
New York, NY, USA: ACM, 2009, pp. 201‚Äì212.
DOI: 10.1145/1669112.1669139. 13, 27, 28, 29
[31] G. H. Loh, ‚Äú3D-stacked memory architectures for multi-core processors,‚Äù in ISCA
‚Äô08: Proceedings of the 35th Annual International Symposium on Computer Architec-
ture.
Washington, DC, USA: IEEE Computer Society, 2008, pp. 453‚Äì464. DOI:
10.1145/1394608.1382159. 29, 30

102
BIBLIOGRAPHY
[32] D. H. Woo, N. H. Seong, D. L. Lewis, and H.-H. S. Lee, ‚ÄúAn optimized 3D-
stacked memory architecture by exploiting excessive, high-density TSV bandwidth,‚Äù in
HPCA ‚Äô10: Proceedings of the 2007 IEEE 16th International Symposium on High Perfor-
mance Computer Architecture.
Bangalore, Indian: IEEE Computer Society, 2010. DOI:
10.1109/HPCA.2010.5416628. 31
[33] M. Ghosh and H.-H. S. Lee, ‚ÄúSmart Refresh: An enhanced memory controller design for
reducing energy in conventional and 3D die-stacked DRAMs,‚Äù in MICRO 40: Proceed-
ings of the 40th Annual IEEE/ACM International Symposium on Microarchitecture.
Wash-
ington, DC, USA: IEEE Computer Society, 2007, pp. 134‚Äì145. DOI: 10.1109/MI-
CRO.2007.38. 13, 29, 32
[34] K. Puttaswamy and G. H. Loh, ‚Äú3D-integrated SRAM components for high-performance
microprocessors,‚Äù IEEE Trans. Comput., vol. 58, no. 10, pp. 1369‚Äì1381, 2009. DOI:
10.1109/TC.2009.92. 13, 20, 21, 22
[35] B. Black, M. Annavaram, N. Brekelbaum, J. DeVale, L. Jiang, G. H. Loh, D. McCaule,
P. Morrow, D. W. Nelson, D. Pantuso, P. Reed, J. Rupley, S. Shankar, J. Shen, and
C. Webb, ‚ÄúDie stacking (3D) microarchitecture,‚Äù in MICRO 39: Proceedings of the 39th An-
nual IEEE/ACM International Symposium on Microarchitecture.
Washington, DC, USA:
IEEE Computer Society, 2006, pp. 469‚Äì479. DOI: 10.1109/MICRO.2006.18. 22, 23,
24, 27, 28
[36] K. Puttaswamy and G. H. Loh, ‚ÄúÓÄÄermal herding: Microarchitecture techniques for con-
trolling hotspots in high-performance 3D-integrated processors,‚Äù in HPCA ‚Äô07: Proceed-
ings of the 2007 IEEE 13th International Symposium on High Performance Computer Archi-
tecture.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 193‚Äì204. DOI:
10.1109/ICCD.2004.1347939. 25, 26
[37] B. Black, D. W. Nelson, C. Webb, and N. Samra, ‚Äú3D processing technology and its impact
on iA32 microprocessors,‚Äù in ICCD ‚Äô04: Proceedings of the IEEE International Conference on
Computer Design.
Washington, DC, USA: IEEE Computer Society, 2004, pp. 316‚Äì318.
13
[38] G. Sun, X. Dong, Y. Xie et al., ‚ÄúA novel architecture of the 3D stacked MRAM L2 cache
for CMPs,‚Äù in Proceedings of the International Symposium on High Performance Computer
Architecture, 2009, pp. 239‚Äì249. DOI: 10.1109/HPCA.2009.4798259. 13
[39] K. Zhang, U. Bhattacharya, Z. Chen, F. Hamzaoglu, D. Murray, N. Vallepalli, B. Z.
Y. Wang, and M. Bohr, ‚ÄúA sram design on 65nm cmos technology with integrated leakage
reduction scheme,‚Äù in VLSI Technology Digest of Technical Papers, 2004. DOI: 10.1109/VL-
SIC.2004.1346592. 14
www.ebook3000.com

BIBLIOGRAPHY
103
[40] Y. Tsai, Y. Xie, V. Narayanan, and M. J. Irwin, ‚ÄúÓÄÄree-dimensional cache design explo-
ration using 3DCacti,‚Äù Proceedings of the IEEE International Conference on Computer Design
(ICCD 2005), pp. 519‚Äì524, 2005. DOI: 10.1109/ICCD.2005.108. 15, 17, 18, 19, 20, 53
[41] P. Shivakumar et al., ‚ÄúCacti 3.0: An Integrated Cache Timing, Power, and Area Model,‚Äù
Western Research Lab Research Report, 2001. 15
[42] Y. Ma, Y. Liu, E. Kursun, G. Reinman, and J. Cong, ‚ÄúInvestigating the eÔ¨Äects of Ô¨Åne-grain
three-dimensional integration on microarchitecture design,‚Äù J. Emerg. Technol. Comput.
Syst., vol. 4, pp. 17:1‚Äì17:30, November 2008. [Online]. Available: http://doi.acm.or
g/10.1145/1412587.1412590 DOI: 10.1145/1324177.1324180. 24
[43] T. Kgil, A. Saidi, N. Binkert, S. Reinhardt, K. Flautner, and T. Mudge, ‚ÄúPicoserver: Using
3D stacking technology to build energy eÔ¨Écient servers,‚Äù J. Emerg. Technol. Comput. Syst.,
vol. 4, no. 4, pp. 1‚Äì34, 2008. DOI: 10.1145/1412587.1412589. 29, 31
[44] C. Liu, I. Ganusov, M. Burtscher, and S. Tiwari, ‚ÄúBridging the processor-memory per-
formance gap with 3D IC technology,‚Äù Design Test of Computers, IEEE, vol. 22, no. 6, pp.
556 ‚Äì 564, nov. 2005. DOI: 10.1109/MDT.2005.134.
[45] G. L. Loi, B. Agrawal, N. Srivastava, S.-C. Lin, T. Sherwood, and K. Banerjee, ‚ÄúA
thermally-aware performance analysis of vertically integrated (3-D) processor-memory hi-
erarchy,‚Äù in DAC ‚Äô06: Proceedings of the 43rd annual Design Automation Conference.
New
York, NY, USA: ACM, 2006, pp. 991‚Äì996. DOI: 10.1145/1146909.1147160. 29
[46] T.
Semiconductors,
‚ÄúLeo
FaStack
1Gb
DDR
SDRAM
datasheet,‚Äù
in
http://www.tezzaron.com/memory/TSC Leo.htm, 2005. 29
[47] K. Chen, S. Li, N. Muralimanohar, J. H. Ahn, J. Brockman, and N. Jouppi, ‚ÄúCACTI-
3DD: Architecture-level modeling for 3D die-stacked DRAM main memory,‚Äù in Pro-
ceedings of the Design, Automation Test in Europe Conference Exhibition, 2012, pp. 33‚Äì38.
DOI: 10.1109/DATE.2012.6176428. 32
[48] P. Shivakumar, , and N. Jouppi, ‚ÄúCacti 3.0: An Integrated Cache Timing, Power, and Area
Model,‚Äù Western Research Lab Research Report, 2001/2. 32
[49] X. Dong, Y. Xie, N. Muralimanohar, and N. P. Jouppi, ‚ÄúSimple but eÔ¨Äective heterogeneous
main memory with on-chip memory controller support,‚Äù in Proceedings of the International
Conference for High Performance Computing, 2010, pp. 1‚Äì11. DOI: 10.1109/SC.2010.50.
33
[50] ‚ÄúItrs 2009.‚Äù [Online]. Available: http://www.itrs.net 33
[51] ‚ÄúNAS parallel benchmarks,‚Äù http://www.nas.nasa.gov/publications/npb.html. 33

104
BIBLIOGRAPHY
[52] B. M. Beckmann, M. R. Marty, and D. A. Wood, ‚ÄúASR: Adaptive selective replication for
CMP caches,‚Äù in Proceedings of the 39th Annual IEEE/ACM International Symposium on
Microarchitecture, 2006, pp. 443‚Äì454. DOI: 10.1109/MICRO.2006.10. 34
[53] Z. Chishti, M. D. Powell, and T. N. Vijaykumar, ‚ÄúOptimizing replication, com-
munication, and capacity allocation in CMPs,‚Äù in Proceedings of the 32Nd An-
nual International Symposium on Computer Architecture, 2005, pp. 357‚Äì368. DOI:
10.1145/1080695.1070001.
[54] J. Huh, C. Kim, H. ShaÔ¨Å, L. Zhang, D. Burger, and S. W. Keckler, ‚ÄúA NUCA substrate
for Ô¨Çexible CMP cache sharing,‚Äù in Proceedings of the 19th Annual International Conference
on Supercomputing, 2005, pp. 31‚Äì40. DOI: 10.1145/1088149.1088154.
[55] N. RaÔ¨Åque, W.-T. Lim, and M. ÓÄÄottethodi, ‚ÄúArchitectural support for operating
system-driven CMP cache management,‚Äù in Proceedings of the 15th International Con-
ference on Parallel Architectures and Compilation Techniques, 2006, pp. 2‚Äì12. DOI:
10.1145/1152154.1152160.
[56] S. Cho and L. Jin, ‚ÄúManaging distributed, shared L2 caches through OS-level page allo-
cation,‚Äù in Proceedings of the 39th Annual IEEE/ACM International Symposium on Microar-
chitecture, 2006, pp. 455‚Äì468. DOI: 10.1109/MICRO.2006.31. 34
[57] G. Loh and M. Hill, ‚ÄúSupporting very large DRAM caches with compound-access
scheduling and MissMap,‚Äù pp. 70‚Äì78, May 2012. DOI: 10.1109/MM.2012.25. 34
[58] M. K. Qureshi and G. H. Loh, ‚ÄúFundamental latency trade-oÔ¨Äin architecting DRAM
caches: Outperforming impractical SRAM-tags with a simple and practical design,‚Äù in
Proceedings of the 45th Annual IEEE/ACM International Symposium on Microarchitecture,
ser. MICRO-45, 2012, pp. 235‚Äì246. DOI: 10.1109/MICRO.2012.30. 35
[59] C. Chou, A. Jaleel, and M. K. Qureshi, ‚ÄúCAMEO: A two-level memory organization with
capacity of main memory and Ô¨Çexibility of hardware-managed cache,‚Äù in Proceedings of the
47th Annual IEEE/ACM International Symposium on Microarchitecture, ser. MICRO-47,
2014. DOI: 10.1109/MICRO.2014.63. 36, 37
[60] S. Keckler, W. Dally, B. Khailany, M. Garland, and D. Glasco, ‚ÄúGPUs and the fu-
ture of parallel computing,‚Äù Micro, IEEE, vol. 31, no. 5, pp. 7‚Äì17, Sept 2011. DOI:
10.1109/MM.2011.89. 39
[61] Q. Zou, M. Poremba, R. He, W. Yang, J. Zhao, and Y. Xie, ‚ÄúHeterogeneous architecture
design with emerging 3d and non-volatile memory technologies,‚Äù in Design Automation
Conference (ASP-DAC), 2015 20th Asia and South PaciÔ¨Åc, Jan 2015, pp. 785‚Äì790. DOI:
10.1109/ASPDAC.2015.7059106. 39
www.ebook3000.com

BIBLIOGRAPHY
105
[62] Y.-J. Lee and S. K. Lim, ‚ÄúOn gpu bus power reduction with 3d ic technologies,‚Äù in Pro-
ceedings of the Conference on Design, Automation & Test in Europe, ser. DATE ‚Äô14, 2014,
pp. 175:1‚Äì175:6. DOI: 10.7873/DATE.2014.188. 39
[63] AMD, ‚ÄúAmd radeon‚Ñ¢hd 7970 graphics,‚Äù 2012. [Online]. Available: http://www.amd.
com/us/products/desktop/graphics/7000/7970/Pages/radeon-7970.aspx 39
[64] NVIDIA, ‚ÄúQuadro 6000 - workstation graphics card for 3d design, styling, visualization,
cad, and more,‚Äù 2010. [Online]. Available: http://www.nvidia.com/object/produc
t-quadro-6000-us.html 39, 41
[65] J. Zhao, G. Sun, G. Loh, and Y. Xie, ‚ÄúOptimizing GPU energy eÔ¨Éciency with 3D die-
stacking graphics memory and reconÔ¨Ågurable memory interface,‚Äù ACM Transactions on
Architecture and Code Optimization (TACO), vol. 10, no. 4, pp. 24:1‚Äì24:25, 2013. DOI:
10.1145/2541228.2555301. 39
[66] E. Vick, S. Goodwin, G. Cunnigham, and D. S. Temple, ‚ÄúVias-last process technology
for thick 2.5d si interposers,‚Äù in 3D Systems Integration Conference, 2012, pp. 1‚Äì4. DOI:
10.1109/3DIC.2012.6262990. 40
[67] G. H. Loh, ‚Äú3d-stacked memory architectures for multi-core processors,‚Äù in Proceed-
ings of the International Symposium on Computer Architecture, 2008, pp. 453‚Äì464. DOI:
10.1145/1394608.1382159. 40
[68] Hynix, ‚ÄúHynix gddr5 sgram datasheet,‚Äù 2009. [Online]. Available: http://www.hynix.
com/products/graphics/ 41
[69] K. Skadron, M. R. Stan, K. Sankaranarayanan, W. Huang, S. Velusamy, and
T. D., ‚ÄúTemperature-aware microarchitecture: Modeling and implementation,‚Äù ACM
Trans. on Architecture and Code Optimization, vol. 1, no. 1, pp. 94‚Äì125, 2004. DOI:
10.1145/980152.980157. 41, 85
[70] A. Al Maashri, G. Sun, X. Dong, V. Narayanan, and Y. Xie, ‚Äú3d gpu architec-
ture using cache stacking: Performance, cost, power and thermal analysis,‚Äù in Pro-
ceedings of the International Conferenece on Computer Design, 2009, pp. 254‚Äì259. DOI:
10.1109/ICCD.2009.5413147. 45
[71] J. Kim, C. Nicopoulos, D. Park, R. Das, Y. Xie, N. Vijaykrishnan, and C. Das, ‚ÄúA novel
dimensionally-decomposed router for on-chip communication in 3D architectures,‚Äù in
Proceedings of the Annual International Symposium on Computer Architecture, 2007. DOI:
10.1145/1273440.1250680. 48
[72] P. Dongkook, S. Eachempati, R. Das, A. K. Mishra, Y. Xie, N. Vijaykrishnan, and
C. R. Das, ‚ÄúMIRA: A multi-layered on-chip interconnect router architecture,‚Äù in

106
BIBLIOGRAPHY
International Symposium on Computer Architecture (ISCA), 2008, pp. 251‚Äì261. DOI:
10.1109/ISCA.2008.13. 48, 52, 54
[73] Y. Xu et al., ‚ÄúA low-radix and low-diameter 3D interconnection network de-
sign,‚Äù
in
Intl. Symp. on High Performance Computer Architecture,
2009.
DOI:
10.1109/HPCA.2009.4798234. 48, 55
[74] L. Carloni, P. Pande, and Y. Xie, ‚ÄúNetworks-on-chip in emerging intercoonect
paradigms: Advantages and challenges,‚Äù in Intl. Symp. on Networks-on-chips, 2009. DOI:
10.1109/NOCS.2009.5071456. 48
[75] A. Jantsch and H. Tenhunen, Networks on Chip.
Kluwer Academic Publishers, 2003. 48
[76] G. De Micheli and L. Benini, Networks on Chips. San Francisco, CA: Morgan Kaupmann,
2006. 48
[77] F. Li, C. Nicopoulos, T. Richardson, Y. Xie, V. Narayanan, and M. Kandemir, ‚ÄúDe-
sign and management of 3D chip multiprocessors using network-in-memory,‚Äù in Pro-
ceedings of International Symposium on Computer Architecture, 2006, pp. 130‚Äì141. DOI:
10.1145/1150019.1136497. 48, 50
[78] J. Kim, C. Nicopoulos, D. Park, R. Das, Y. Xie, N. Vijaykrishnan, and C. Das, ‚ÄúA
novel dimensionally-decomposed router for on-chip communication in 3D architec-
tures,‚Äù in Proceedings of International Symposium on Computer Architecture, 2007. DOI:
10.1145/1273440.1250680. 51, 52
[79] S. Yan and B. Lin, ‚ÄúDesign of application-speciÔ¨Åc 3D networks-on-chip architectures,‚Äù
in Proceedings of International Conference of Computer Design, 2008, pp. 142‚Äì149. DOI:
10.1109/ICCD.2008.4751853. 56
[80] I. Loi, F. Angiolini, and L. Benini, ‚ÄúDeveloping mesochronous synchronizers to enable
3D NoCs,‚Äù in Proceedings of Design, Automation and Test in Europe Conference, 2008, pp.
1414‚Äì1419. DOI: 10.1109/DATE.2008.4484872.
[81] I. Loi, S. Mitra, T. H. Lee, S. Fujita, and L. Benini, ‚ÄúA low-overhead fault tolerance
scheme for tsv-based 3D network on chip links,‚Äù in Proceedings of International Conference
on Computer-Aided Design, 2008, pp. 598‚Äì602. DOI: 10.1145/1509456.1509589.
[82] V. F. Pavlidis and E. G. Friedman, ‚Äú3-D topologies for networks-on-chip,‚Äù IEEE Trans-
actions on Very Large Scale Integration (VLSI) Systems, vol. 15, no. 10, pp. 1081‚Äì1090, 2007.
DOI: 10.1109/TVLSI.2007.893649. 48
[83] J. Kim, C. Nicopoulos, D. Park, V. Narayanan, M. S. Yousif, and C. Das, ‚ÄúA grace-
fully degrading and energy-eÔ¨Écient modular router architecture for on-chip networks,‚Äù
www.ebook3000.com

BIBLIOGRAPHY
107
in Proceedings of International Symposium on Computer Architecture, 2006, pp. 4‚Äì15. DOI:
10.1145/1150019.1136487. 52
[84] J. Kim, C. Nicopoulos, D. Park, R. Das, Y. Xie, V. Narayanan, M. S. Yousif, and C. R.
Das, ‚ÄúA novel dimensionally-decomposed router for on-chip communication in 3D ar-
chitectures,‚Äù SIGARCH Comput. Archit. News, vol. 35, no. 2, pp. 138‚Äì149, 2007. DOI:
10.1145/1273440.1250680. 53
[85] D. Park, S. Eachempati, R. Das, A. K. Mishra, Y. Xie, N. Vijaykrishnan, and C. R. Das,
‚ÄúMira: A multi-layered on-chip interconnect router architecture,‚Äù in ISCA ‚Äô08: Proceedings
of the 35th Annual International Symposium on Computer Architecture.
Washington, DC,
USA: IEEE Computer Society, 2008, pp. 251‚Äì261. DOI: 10.1145/1394608.1382143. 54,
55
[86] Y. Ye, L. Duan, J. Xu, J. Ouyang, M. K. Hung, and Y. Xie, ‚Äú3D optical networks-on-
chip (NoC) for multiprocessor systems-on-chip (MPSoC),‚Äù sep. 2009, pp. 1 ‚Äì6. DOI:
10.1109/3DIC.2009.5306588. 56
[87] X. Zhang and A. Louri, ‚ÄúA multilayer nanophotonic interconnection network for
on-chip many-core communications,‚Äù in DAC ‚Äô10: Proceedings of the 47th Design Au-
tomation Conference.
New York, NY, USA: ACM, 2010, pp. 156‚Äì161. DOI:
10.1145/1837274.1837314. 56
[88] D. Vantrease, R. Schreiber, M. Monchiero, M. McLaren, N. P. Jouppi, M. Fiorentino,
A. Davis, N. Binkert, R. G. Beausoleil, and J. H. Ahn, ‚ÄúCorona: System implications of
emerging nanophotonic technology,‚Äù in ISCA ‚Äô08: Proceedings of the 35th Annual Interna-
tional Symposium on Computer Architecture.
Washington, DC, USA: IEEE Computer
Society, 2008, pp. 153‚Äì164. DOI: 10.1145/1394608.1382135. 56
[89] S. Sapatnekar, ‚ÄúAddressing ÓÄÄermal and Power Delivery Bottlenecks in 3D Circuits,‚Äù in
Proceedings of the Asia and South PaciÔ¨Åc Design Automation Conference, 2009., 2009, pp.
423‚Äì428. DOI: 10.1145/1509633.1509738. 60
[90] P. Li, L. T. Pileggi, M. Asheghi, and R. Chandra, ‚ÄúIC thermal simulation and
modeling via eÔ¨Écient multigrid-based approaches,‚Äù IEEE Transactions on Computer-
Aided Design of Integrated Circuits and Systems, vol. 25, pp. 1763‚Äì1776, 2006. DOI:
10.1109/TCAD.2005.858276. 60
[91] B. Goplen and S. Sapatnekar, ‚ÄúEÔ¨Écient ÓÄÄermal Placement of Standard Cells in
3D ICs using a Force Directed Approach,‚Äù Proceedings of ICCAD, 2003. DOI:
10.1145/996070.1009873. 61, 64

108
BIBLIOGRAPHY
[92] W. Huang, S. Ghosh, and et al., ‚ÄúHotspot: a compact thermal modeling methodol-
ogy for early-stage VLSI design,‚Äù TVLSI, vol. 14, no. 5, pp. 501‚Äì513, 2006. DOI:
10.1109/TVLSI.2006.876103. 61
[93] G. M. Link and N. Vijaykrishnan, ‚ÄúÓÄÄermal trends in emerging technologies,‚Äù 2006, pp.
625‚Äì632. DOI: 10.1109/ISQED.2006.136. 61, 66
[94] J. Cong and Y. Zhang, ‚ÄúÓÄÄermal-driven multilevel routing for 3D ICs,‚Äù in Proc. Asia and
South PaciÔ¨Åc Design Automation Conf the ASP-DAC 2005, vol. 1, 2005, pp. 121‚Äì126. DOI:
10.1109/ASPDAC.2005.1466143. 62
[95] Y. Yang, Z. Gu, C. Zhu, R. P. Dick, and L. Shang, ‚ÄúISAC: Integrated space-and-
time-adaptive chip-package thermal analysis,‚Äù vol. 26, no. 1, pp. 86‚Äì99, 2007. DOI:
10.1109/TCAD.2006.882589. 63
[96] W. Hung, G. Link, Y. Xie, V. Narayanan, and M.J.Irwin, ‚ÄúInterconnect and thermal-
aware Ô¨Çoorplanning for 3D microprocessors,‚Äù in Proceedings of the International Symposium
of Quality Electronic Devices, 2006. DOI: 10.1109/ISQED.2006.77. 63, 64, 69
[97] J. Cong, J. Wei, and Y. Zhang, ‚ÄúA thermal-driven Ô¨Çoorplanning algorithm for 3D
ics,‚Äù in Proc. of International Conference Computer-Aided Design (ICCAD), 2004. DOI:
10.1145/1112239.1112292. 64, 67
[98] C. Chu and D. Wong, ‚ÄúA matrix synthesis approach to thermal placement,‚Äù in Proceedings
of the ISPD, 1997. DOI: 10.1145/267665.267708. 64
[99] C. Tsai and S. Kang, ‚ÄúCell-level placement for improving substrate thermal distributio,‚Äù
in IEEE Trans. on Computer-Aided Design of Integrated Circuits and System, 2000. DOI:
10.1109/43.828554. 64, 68
[100] P. Shiu and S. K. Lim, ‚ÄúMulti-layer Ô¨Çoorplanning for reliable system-on-package,‚Äù in Proc.
of IEEE International Symposium on Circuits and Systems (ISCAS), 2004. DOI: 10.1109/IS-
CAS.2004.1329460. 64, 66
[101] Y. Chang, Y. Chang, G.-M. Wu, and S.-W. Wu, ‚ÄúB*-trees: a new representation for non-
slicing Ô¨Çoorplans ,‚Äù Proceedings of the Annual ACM/IEEE Design Automation Conference,
2000. DOI: 10.1109/DAC.2000.855354. 64, 65
[102] K. Puttaswamy and G. Loh, ‚ÄúÓÄÄermal herding: Microarchitecture techniques for control-
ling hotspots in high-performance 3D-integrated processors,‚Äù in High Performance Com-
puter Architecture, 2007. HPCA 2007. IEEE 13th International Symposium on, Feb. 2007,
pp. 193‚Äì204. DOI: 10.1109/HPCA.2007.346197. 70
www.ebook3000.com

BIBLIOGRAPHY
109
[103] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and N. P. Jouppi, ‚ÄúMc-
pat: An integrated power, area, and timing modeling framework for multicore and many-
core architectures,‚Äù in Proceedings of the International Symposium on Microarchitecture, 2009.
DOI: 10.1145/1669112.1669172. 74, 92
[104] R. Weerasekera, L.-R. Zheng, D. Pamunuwa, and H. Tenhunen, ‚ÄúExtending Systems-on-
Chip to the third dimension: performance, cost and technological tradeoÔ¨Äs,‚Äù in Proceed-
ings of the International Conference on Computer-Aided Design, 2007, pp. 212‚Äì219. DOI:
10.1145/1326073.1326117. 74, 76
[105] B. S. Landman and R. L. Russo, ‚ÄúOn a pin versus block relationship for partitions of logic
graphs,‚Äù IEEE Transancations on Computers, vol. 20, no. 12, pp. 1469‚Äì1479, 1971. DOI:
10.1109/T-C.1971.223159. 75
[106] W. E. Donath, ‚ÄúPlacement and average interconnection lengths of computer logic,‚Äù
IEEE Transactions on Circuits and Systems, vol. 26, no. 4, pp. 272‚Äì277, 1979. DOI:
10.1109/TCS.1979.1084635. 76, 78
[107] J. A. Davis, V. K. De, and J. D. Meindl, ‚ÄúA stochastic wire-length distribution for gigas-
cale integration (GSI)¬°¬™Part I: derivation and validation,‚Äù IEEE Transactions on Electron
Devices, vol. 45, no. 3, pp. 580‚Äì589, 1998. DOI: 10.1109/16.661219. 76, 77
[108] A. B. Kahng, S. Mantik, and D. Stroobandt, ‚ÄúToward accurate models of achievable
routing,‚Äù IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,
vol. 20, no. 5, pp. 648‚Äì659, 2001. DOI: 10.1109/43.920697. 76, 77
[109] P. Chong and R. K. Brayton, ‚ÄúEstimating and optimizing routing utilization in DSM
design,‚Äù in Proceedings of the Workshop on System-Level Interconnect Prediction, 1999, pp.
97‚Äì102. 77
[110] H. B. Bakoglu, Circuits, Interconnections, and Packaging for VLSI.
Addison-Wesley, 1990.
77
[111] X. Dong, J. Zhao, and Y. Xie, ‚ÄúFabrication cost analysis and cost-aware design
space exploration for 3-D ICs,‚Äù Computer-Aided Design of Integrated Circuits and Sys-
tems, IEEE Transactions on, vol. 29, no. 12, pp. 1959 ‚Äì1972, dec. 2010. DOI:
10.1109/TCAD.2010.2062811. 78, 79, 80, 82, 83, 85, 87, 88, 89, 90, 91, 92, 93, 94,
95, 96
[112] M. Tremblay and S. Chaudhry, ‚ÄúA third-generation 65nm 16-core 32-thread llus 32-scout-
thread CMT SPARC(R) processor,‚Äù in Proceedings of the International Solid State Circuit
Conference, 2008, pp. 82‚Äì83. DOI: 10.1109/ISSCC.2008.4523067. 78, 87

110
BIBLIOGRAPHY
[113] G. Loh, Y. Xie, and B. Black, ‚ÄúProcessor design in three-dimensional die-stacking tech-
nologies,‚Äù IEEE Micro, vol. 27, no. 3, pp. 31‚Äì48, 2007. DOI: 10.1109/MM.2007.59. 77
[114] P. Zarkesh-Ha, J. Davis, W. Loh, and J. Meindl, ‚ÄúOn a pin versus gate relationship for
heterogeneous systems: heterogeneous Rent‚Äôs rule,‚Äù in Proceedings of the IEEE Custom In-
tegrated Circuits Conference, 1998, pp. 93‚Äì96. DOI: 10.1109/CICC.1998.694914. 79
[115] IC Knowledge LLC., ‚ÄúIC Cost Model, 2009 Revision 0906,‚Äù 2009. 81, 84
[116] J. Rabaey, A. Chandrakasan, and B. Nikolic, ‚ÄúDigital Integrated Circuits,‚Äù Prentice-Hall,
2003. 81
[117] B. Murphy, ‚ÄúCost-size optima of monolithic integrated circuits,‚Äù Proceedings of the IEEE,
vol. 52, no. 12, pp. 1537‚Äì1545, dec. 1964. DOI: 10.1109/PROC.1964.3442. 81
[118] L. Smith, G. Smith, S. Hosali, and S. Arkalgud, ‚Äú3D: it all comes down to cost,‚Äù in Pro-
ceedings of RTI Conference of 3D Architecture for Semiconductors and Packaging, 2007. 83
[119] Y. Chen, D. Niu, X. Dong, Y. Xie, and K. Chakrabarty, ‚ÄúTesting Cost Analsysis in three-
dimensional (3D) integration technology,‚Äù in Technical Report, CSE Department, Penn
State, http://www.cse.psu.edu/yuanxie/3d.html, 2010. 83
[120] S. Gunther, F. Binns, D. M. Carmean, and J. C. Hall, ‚ÄúManaging the impact of increasing
microprocessor power consumption,‚Äù Intel Technology Journal, vol. 5, no. 1, pp. 1‚Äì9, 2001.
84
[121] W. Huang, K. Skadron, S. Gurumurthi et al., ‚ÄúDiÔ¨Äerentiating the roles of IR measure-
ment and simulation for power and temperature-aware design,‚Äù in Proceedings of the Inter-
national Symposium on Performance Analysis of Systems and Software, 2009, pp. 1‚Äì10. DOI:
10.1109/ISPASS.2009.4919633. 85, 92
[122] Digikey, 2009, www.digikey.com. 85
[123] Heatsink Factory, 2009, www.heatsinkfactory.com. 85
[124] X. Dong, X. Wu, G. Sun et al., ‚ÄúCircuit and microarchitecture evaluation of 3D stacking
Magnetic RAM (MRAM) as a universal memory replacement,‚Äù in Proceedings of the Design
Automation Conference, 2008, pp. 554‚Äì559. DOI: 10.1145/1391469.1391610. 92
[125] S. Borkar, ‚Äú3D technology: a system perspective,‚Äù in Proceedings of the International 3D-
System Integration Conference, 2008, pp. 1‚Äì14. 92
[126] S. M. Alam, R. E. Jones, S. Pozder, and A. Jain, ‚ÄúDie/wafer stacking with reciprocal design
symmetry (RDS) for mask reuse in three-dimensional (3D) integration technology,‚Äù in
Proceedings of the International Symposium on Quality of Electronic Design, 2009, pp. 569‚Äì
575. DOI: 10.1109/ISQED.2009.4810357. 95
www.ebook3000.com

BIBLIOGRAPHY
111
[127] ‚ÄúCadence, 3D ICs with TSVs ‚Äì design challenges and requirements,‚Äù http://www.cade
nce.com/rl/resources/white_papers/3dic_wp.pdf. 97

www.ebook3000.com

113
Authors‚Äô Biographies
YUAN XIE
Yuan Xie received his B.S. degree in electronic engineering from Tsinghua University, Beijing,
in 1997, and his M.S. and Ph.D. degrees in electrical engineering from Princeton University in
1999 and 2002, respectively. He is currently a Professor in the Electrical and Computer Engi-
neering department at the University of California at Santa Barbara. Before joining UCSB in Fall
2014, he was with the Pennsylvania State University from 2003 to 2014, and with IBM Micro-
electronic Division‚Äôs Worldwide Design Center from 2002 to 2003. Prof. Xie is a recipient of the
National Science Foundation Early Faculty (CAREER) award, the SRC Inventor Recognition
Award, IBM Faculty Award, and several Best Paper Award and Best Paper Award Nominations
at IEEE/ACM conferences. He has published more than 100 research papers in journals and ref-
ereed conference proceedings, in the area of EDA, computer architecture, VLSI circuit designs,
and embedded systems. His current research projects include: three-dimensional integrated cir-
cuits (3D ICs) design, EDA, and architecture; emerging memory technologies; low power and
thermal-aware design; reliable circuits and architectures; and embedded system synthesis. He is
currently Associate Editor for ACM Journal of Emerging Technologies in Computing Systems
(JETC), IEEE Transactions on Very Large Scale Integration Systems (TVLSI), IEEE Trans-
actions on Computer Aided Design of Integrated Circuits (TCAD), IEEE Design and Test of
Computers, IET Computers and Digital Techniques (IET CDT). He is a Fellow of IEEE.
JISHEN ZHAO
Jishen Zhao received her B.S. and M.S. degrees from Zhejiang University, and Ph.D. degree
from Pennsylvania State University. She is currently an Assistant Professor at the University of
California, Santa Cruz. Her research interests include a broad range of computer architecture
topics with an emphasis on memory systems, high-performance computing, and energy eÔ¨Éciency.
She is also interested in electronic design automation and VLSI design for three-dimensional
integrated circuits and nonvolatile memories.


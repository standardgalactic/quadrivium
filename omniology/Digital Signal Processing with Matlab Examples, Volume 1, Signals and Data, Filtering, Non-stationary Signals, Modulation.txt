Jose Maria Giron-Sierra
Digital Signal Processing
with Matlab Examples,
Volume 1
Signals and Data, Filtering, Non-stationary
Signals, Modulation
123

Jose Maria Giron-Sierra
Systems Engineering and Automatic Control
Universidad Complutense de Madrid
Madrid
Spain
ISSN 1860-4862
ISSN 1860-4870
(electronic)
Signals and Communication Technology
ISBN 978-981-10-2533-4
ISBN 978-981-10-2534-1
(eBook)
DOI 10.1007/978-981-10-2534-1
Library of Congress Control Number: 2016951678
MATLAB® is a registered trademark of The MathWorks, Inc., and is used with permission. The
MathWorks does not warrant the accuracy of the text or exercises in this book. This book’s use or
discussion of MATLAB software or related products does not constitute endorsement or sponsorship by
the MathWorks of a particular pedagogical approach or particular use of the MATLAB software.
© Springer Science+Business Media Singapore 2017
This work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part
of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations,
recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and transmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, express or implied, with respect to the material contained herein or
for any errors or omissions that may have been made.
Printed on acid-free paper
This Springer imprint is published by Springer Nature
The registered company is Springer Nature Singapore Pte Ltd.
The registered company address is: 152 Beach Road, #22-06/08 Gateway East, Singapore 189721, Singapore

Preface
In our contemporary world, the digital processing of signals and data is certainly
very important. Most people are now using it, as it is involved in mobile com-
munications, TV and radio, GPS, medical instruments, transportation and trafﬁc,
and a long et cetera. Also, most branches of technical development and research are
intrinsically connected to signals and data processing.
This book is the ﬁrst of a trilogy. Our desire is to provide a concise and relatively
complete exposition of signal processing topics, and a guide for personal practical
exploration based on MATLAB programs. It has been said by many experts on
learning that readings could be forgotten, but experiments leave a mark in our
minds and help to gain signiﬁcant insights.
The books include MATLAB programs to illustrate each of the main steps of the
theory. The code has been embedded in the text; with the purpose of showing how
to put into practice the ideas and methods being proposed.
It seems opportune to say some words on the author’s experience in the ﬁeld of
signal and data processing. Since 30 years ago I belong to the Faculty of Physics,
University Complutense of Madrid, Spain. My research concerns automatic control
and robotics, with applications in autonomous vehicles, maritime drones, chemical
processes, satellites, and others. During the research a variety of sensors has been
used, for the measurement of gases, pH, forces, light, magnetic ﬁelds, evoked
human potentials, etc. More complex sensors, like GPS or cameras, have also been
employed. Currently I teach Biomedical Digital Signal Processing, and Digital
Signal Processing for New Technologies.
The motivation for elaborating this book is related to our interaction with stu-
dents and young researchers. Our teaching includes theory classes and laboratory
exercises. It was noticed in laboratory that the use of MATLAB and its Signal
Processing Toolbox has noticeable initial difﬁculties for the students, if they had to
start from scratch. Therefore, we began to provide them some simple programs that
give them initial success, and graphical results. A good start encourages the stu-
dents for further study steps, and helps to develop a more ambitious teaching.
This book is divided into three parts. The ﬁrst part introduces periodic and
non-periodic signals. The second part is devoted to ﬁltering, which is an important
vii

and most usual application. The third part contemplates topics that could be con-
sidered as advanced; one tries to analyze, with several purposes, what happens with
signals and data from real life, like for example fatigue of structures, earthquakes,
electro-encephalograms, animal songs, etc. Therefore, the third part focuses on
non-stationary signals. The last chapter is devoted to modulation, which implies the
intentional use of non-stationary signals, and so this chapter belongs to the third
part.
The book has two appendices. The ﬁrst appendix is devoted to the Fourier
transform, other transforms, and sampling fundamentals. The second contains long
programs, which are put here in order to make the chapters more readable.
Concerning the MATLAB programs, the programming style is purposively
simple and illustrative. We tried to avoid coding ways that could be more optimized
but may result in obscuring the ideas behind. The programs work in the diverse
MATLAB versions, with perhaps some possible changes for some functions (in this
case, MATLAB itself suggests appropriate changes).
There are traditional books that could be used for consultation. The chapters
include references to these books and pertinent scientiﬁc papers. Most of the papers
are available from Internet. By the way, we have to show our gratitude to the public
information available from Internet, from web sites, academic institutions, ency-
clopedia, etc. All chapters have a ﬁnal section with some convenient Internet links.
The reader is invited to typeset the programs included in the book, for it would
help for catching coding details. Anyway, all programs are available from the book
web page: www.dacya.ucm.es/giron/SPBook1/Programs.
Please, send feedback and suggestions for further improvements and support.
Acknowledgments: Thanks to my university, my colleagues and my students.
I have to mention in particular the help I received, along friendly discussions on
some signal processing topics, from Juan F. Jimenez and Segundo Esteban, two
members of my department. Since this book required a lot of time taken from
nights, weekends and holidays, I have to sincerely express my gratitude to my
family.
Madrid, Spain
Jose Maria Giron-Sierra
viii
Preface

Contents
Part I
Signals and Data
1
Periodic Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Signal Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.3
Generation of Periodic Signals . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3.1
Sinusoidal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3.2
Square . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.3.3
Sawtooth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.4
Hearing the Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.5
Operations with Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.5.1
Adding Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.5.2
Multiplication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
1.6
Harmonics. Fourier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.6.1
Odd Signals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.6.2
Even Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.6.3
Half Wave Symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.6.4
Pulse Train . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.7
Sampling Frequency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.8
Suggested Experiments and Exercises. . . . . . . . . . . . . . . . . . . . . .
25
1.9
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.9.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.9.2
Web Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2
Statistical Aspects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
2.2
Random Signals and Probability Density Distributions. . . . . . . . .
30
2.2.1
Basic Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.2.2
Random Signal with Uniform PD . . . . . . . . . . . . . . . . . .
31
ix

2.2.3
Random Signal with Normal (Gaussian) PDF . . . . . . . . .
33
2.2.4
Random Signal with Log-Normal PDF . . . . . . . . . . . . . .
36
2.3
Expectations and Moments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.3.1
Expected Values, and Moments. . . . . . . . . . . . . . . . . . . .
39
2.3.2
Mean, Variance, Etc. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.3.3
Transforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.3.4
White Noise . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.4
Power Spectra. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.4.1
Basic Concept . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.2
Example of Power Spectral Density of a Random
Variable. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.4.3
Detecting a Sinusoidal Signal Buried in Noise . . . . . . . .
44
2.4.4
Hearing Random Signals . . . . . . . . . . . . . . . . . . . . . . . . .
46
2.5
More Types of PDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
2.5.1
Distributions Related with the Gamma Function . . . . . . .
47
2.5.2
Weibull and Rayleigh PDFs . . . . . . . . . . . . . . . . . . . . . .
53
2.5.3
Multivariate Gaussian PDFs . . . . . . . . . . . . . . . . . . . . . .
55
2.5.4
Discrete Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.6
Distribution Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.6.1
Probability Plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.6.2
Histogram . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.6.3
Likelihood . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.6.4
The Method of Moments. . . . . . . . . . . . . . . . . . . . . . . . .
65
2.6.5
Mixture of Gaussians. . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.6.6
Kernel Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
2.7
Monte Carlo Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.7.1
Monte Carlo Integration . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.7.2
Generation of Random Data with a Desired PDF . . . . . .
80
2.8
Central Limit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
2.9
Bayes’ Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
89
2.9.1
Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . . .
90
2.9.2
Bayes’ Rule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
2.9.3
Bayesian Networks. Graphical Models . . . . . . . . . . . . . .
94
2.10
Markov Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96
2.10.1
Markov Chain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
2.10.2
Markov Chain Monte Carlo (MCMC). . . . . . . . . . . . . . .
99
2.10.3
Hidden Markov Chain (HMM) . . . . . . . . . . . . . . . . . . . .
103
2.11
MATLAB Tools for Distributions . . . . . . . . . . . . . . . . . . . . . . . .
106
2.12
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
2.12.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
2.12.2
Web Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
110
x
Contents

Part II
Filtering
3
Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
117
3.2
Examples About Transfer Functions . . . . . . . . . . . . . . . . . . . . . . .
117
3.2.1
A Basic Low-Pass Electronic Filter . . . . . . . . . . . . . . . . .
117
3.2.2
A Basic Resonant Electronic Filter . . . . . . . . . . . . . . . . .
119
3.3
Response of Continuous Linear Systems . . . . . . . . . . . . . . . . . . .
120
3.3.1
Frequency Response . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
3.3.2
Time Domain Response . . . . . . . . . . . . . . . . . . . . . . . . .
126
3.4
Response of Discrete Linear Systems . . . . . . . . . . . . . . . . . . . . . .
133
3.5
Random Signals Through Linear Systems . . . . . . . . . . . . . . . . . .
135
3.6
State Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
138
3.7
State Space Gauss–Markov Model . . . . . . . . . . . . . . . . . . . . . . . .
141
3.7.1
A Scalar State Space Case. . . . . . . . . . . . . . . . . . . . . . . .
141
3.7.2
General State Space Case . . . . . . . . . . . . . . . . . . . . . . . .
153
3.8
Time-Series Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
3.8.1
The Discrete Transfer Function in Terms of the
Backshift Operator. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155
3.8.2
Considering Random Variables . . . . . . . . . . . . . . . . . . . .
158
3.9
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
3.9.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
3.9.2
Web Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183
4
Analog Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
4.2
Basic First Order Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
4.3
A Basic Way for Filter Design . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
4.4
Causality and the Ideal Band-Pass Filter. . . . . . . . . . . . . . . . . . . .
197
4.5
Three Approximations to the Ideal Low-Pass Filter . . . . . . . . . . .
198
4.5.1
Butterworth Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199
4.5.2
Chebyshev Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
4.5.3
Elliptic Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
211
4.5.4
Comparison of Filters . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
4.5.5
Details of the MATLAB Signal Processing Toolbox. . . .
217
4.6
Considering Phases and Delays . . . . . . . . . . . . . . . . . . . . . . . . . .
221
4.6.1
Bessel Filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
4.6.2
Comparison of Filter Phases and Group Velocities . . . . .
224
4.7
Some Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
4.7.1
Recovering a Signal Buried in Noise. . . . . . . . . . . . . . . .
229
4.7.2
Adding and Extracting Signals . . . . . . . . . . . . . . . . . . . .
230
4.7.3
Near Cut-off Frequency. . . . . . . . . . . . . . . . . . . . . . . . . .
231
Contents
xi

4.8
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
4.8.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
4.8.2
Web Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
5
Digital Filters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
239
5.2
From Analog Filters to Digital Filters. . . . . . . . . . . . . . . . . . . . . .
241
5.3
FIR Digital Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
5.3.1
Duality and Brickwall Shapes . . . . . . . . . . . . . . . . . . . . .
246
5.3.2
Truncation and Time-Shifting . . . . . . . . . . . . . . . . . . . . .
250
5.3.3
Windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
253
5.3.4
Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
266
5.3.5
Other FIR Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
5.3.6
Details of FIR Filters in the MATLAB Signal
Processing Toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
281
5.4
IIR Digital Filters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
5.4.1
Classical Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
282
5.4.2
Direct Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
285
5.4.3
Details of IIR Filters in the MATLAB Signal
Processing Toolbox . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
297
5.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
5.5.1
Adding and Extracting Signals . . . . . . . . . . . . . . . . . . . .
302
5.5.2
Modelling a Piano Note . . . . . . . . . . . . . . . . . . . . . . . . .
303
5.6
A Quick Introduction to the FDATool . . . . . . . . . . . . . . . . . . . . .
305
5.7
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
5.7.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
5.7.2
Web Sites . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
309
Part III
Non-stationary Signals
6
Signal Changes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
313
6.2
Changes in Sinusoidal Signals . . . . . . . . . . . . . . . . . . . . . . . . . . .
314
6.2.1
Changes in Amplitude. . . . . . . . . . . . . . . . . . . . . . . . . . .
314
6.2.2
Changes in Frequency . . . . . . . . . . . . . . . . . . . . . . . . . . .
317
6.3
Two Analytical Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
320
6.3.1
Cepstral Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
6.3.2
Chirp Z-Transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
328
6.4
Some Signal Phenomena . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
6.4.1
Spectrum Shifts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
6.4.2
Changes in Spectrum Shape . . . . . . . . . . . . . . . . . . . . . .
335
6.4.3
Musical Instruments . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
xii
Contents

6.4.4
Changes in Signal Energy . . . . . . . . . . . . . . . . . . . . . . . .
343
6.4.5
Repetitions, Rhythm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
6.5
Some Complex Sounds. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
6.5.1
Animal Sounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
349
6.5.2
Music . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
6.6
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
6.6.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
352
6.6.2
Internet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
353
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
354
7
Time-Frequency Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
7.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
357
7.2
Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
7.3
Ambiguity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
362
7.4
Transforms for Time-Frequency Studies . . . . . . . . . . . . . . . . . . . .
363
7.4.1
The Short-Time Fourier Transform . . . . . . . . . . . . . . . . .
364
7.4.2
The Gabor Expansion . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
7.4.3
The Continuous Wavelet Transform . . . . . . . . . . . . . . . .
372
7.5
Time-Frequency Distributions. . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
7.5.1
Densities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
375
7.5.2
The Wigner Distribution . . . . . . . . . . . . . . . . . . . . . . . . .
376
7.5.3
The SAF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
381
7.5.4
From Wigner to SAF and Vice-Versa . . . . . . . . . . . . . . .
386
7.5.5
About Interferences . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
390
7.5.6
Smoothing of the Wigner Distribution. . . . . . . . . . . . . . .
397
7.6
Signal Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
399
7.6.1
Types of Representations. . . . . . . . . . . . . . . . . . . . . . . . .
399
7.6.2
Analysis Approaches . . . . . . . . . . . . . . . . . . . . . . . . . . . .
400
7.6.3
Basic Time-Frequency Operators. . . . . . . . . . . . . . . . . . .
402
7.6.4
Geometric Transformations . . . . . . . . . . . . . . . . . . . . . . .
404
7.6.5
Some Important Types of Matrices . . . . . . . . . . . . . . . . .
406
7.6.6
Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
407
7.6.7
Covariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
410
7.7
The Cohen’s Class and the Afﬁne Class. . . . . . . . . . . . . . . . . . . .
412
7.7.1
The Cohen’s Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
412
7.7.2
The Afﬁne Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
416
7.7.3
Classiﬁcation of TFRs . . . . . . . . . . . . . . . . . . . . . . . . . . .
417
7.8
Linear Canonical Transformation . . . . . . . . . . . . . . . . . . . . . . . . .
418
7.8.1
Particular Cases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
419
7.8.2
Decomposition of the LCT . . . . . . . . . . . . . . . . . . . . . . .
421
7.8.3
Effect on the Wigner Distribution . . . . . . . . . . . . . . . . . .
422
7.8.4
Comments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
422
7.8.5
Example of Fractional Fourier Transform . . . . . . . . . . . .
423
Contents
xiii

7.9
Adaptation and Decomposition for Better Signal
Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
426
7.9.1
The Chirplet Transform. . . . . . . . . . . . . . . . . . . . . . . . . .
426
7.9.2
Unitary Equivalence Principle . . . . . . . . . . . . . . . . . . . . .
437
7.9.3
The Reassignment Method . . . . . . . . . . . . . . . . . . . . . . .
444
7.10
Other Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
447
7.10.1
The Modiﬁed S-Transform . . . . . . . . . . . . . . . . . . . . . . .
448
7.10.2
The Fan-Chirp-Transform . . . . . . . . . . . . . . . . . . . . . . . .
451
7.10.3
The Mellin Transform . . . . . . . . . . . . . . . . . . . . . . . . . . .
452
7.10.4
The Empirical Mode Decomposition
and Hilbert–Huang Transform . . . . . . . . . . . . . . . . . . . . .
455
7.10.5
More Transforms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
465
7.11
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
467
7.11.1
Fractional Fourier Transform of a Rectangular
Signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
468
7.11.2
Filtered Wigner Analysis of Nature Chirps . . . . . . . . . . .
469
7.11.3
Wavelet Analysis of Lung and Heart Sounds . . . . . . . . .
471
7.11.4
Fan-Chirp Transform of Some Animal Songs . . . . . . . . .
474
7.11.5
Modiﬁed S-Transform Analysis of Some Cases . . . . . . .
478
7.12
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
7.12.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
484
7.12.2
Internet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
485
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
486
8
Modulation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
495
8.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
495
8.2
Modulation and Demodulation of Sinusoidal Signals . . . . . . . . . .
496
8.2.1
Amplitude Modulation and Demodulation. . . . . . . . . . . .
496
8.2.2
Frequency Modulation and Demodulation . . . . . . . . . . . .
507
8.2.3
Digital Modulation of Sine Signals . . . . . . . . . . . . . . . . .
512
8.2.4
Details of the MATLAB Signal Processing
Toolbox. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
517
8.3
Modulation and Demodulation of Pulses . . . . . . . . . . . . . . . . . . .
518
8.3.1
Sampling. Demodulation of modulated pulses. . . . . . . . .
518
8.3.2
Modulation of Pulses. . . . . . . . . . . . . . . . . . . . . . . . . . . .
519
8.3.3
Coding. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
522
8.3.4
Inter-symbol Interference. . . . . . . . . . . . . . . . . . . . . . . . .
522
8.4
Transmission Media. Multiplexing . . . . . . . . . . . . . . . . . . . . . . . .
523
8.4.1
Frequency Domain Multiplexing . . . . . . . . . . . . . . . . . . .
523
8.4.2
Time Domain Multiplexing . . . . . . . . . . . . . . . . . . . . . . .
523
8.5
Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
8.5.1
Communication and Noise . . . . . . . . . . . . . . . . . . . . . . .
525
8.5.2
Cepstrum of Analog AM Modulation . . . . . . . . . . . . . . .
529
xiv
Contents

8.6
Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
530
8.6.1
MATLAB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
530
8.6.2
Internet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
531
Appendix A: Transforms and Sampling . . . . . . . . . . . . . . . . . . . . . . . . . .
533
Appendix B: Long Programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
557
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
617
Contents
xv

List of Figures
Figure 1.1
A square signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
Figure 1.2
The sampled square signal . . . . . . . . . . . . . . . . . . . . . . . .
5
Figure 1.3
Sinusoidal signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
Figure 1.4
Sine (solid) and cosine (dashed) signals. . . . . . . . . . . . . . .
6
Figure 1.5
Square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
Figure 1.6
Sawtooth signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
Figure 1.7
Different sawtooth signals . . . . . . . . . . . . . . . . . . . . . . . .
9
Figure 1.8
The sinusoidal audio signal . . . . . . . . . . . . . . . . . . . . . . .
10
Figure 1.9
Sum of three sinusoidal signals. . . . . . . . . . . . . . . . . . . . .
12
Figure 1.10
Multiplication of two sinusoidal signals . . . . . . . . . . . . . . .
13
Figure 1.11
Example of odd signal (sawtooth signal, 3 periods) . . . . . . .
14
Figure 1.12
Amplitude of the first ten harmonics of the sawtooth
signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
Figure 1.13
Example of even signal (rectified sine signal, 3 periods) . . .
16
Figure 1.14
Amplitude of the first ten harmonics of the rectified
sine signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
Figure 1.15
Example of signal with half-wave symmetry
(triangular signal, 3 periods). . . . . . . . . . . . . . . . . . . . . . .
18
Figure 1.16
Amplitude of the first ten harmonics of the triangular
signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
Figure 1.17
A pulse train signal (3 periods). . . . . . . . . . . . . . . . . . . . .
20
Figure 1.18
Amplitude of the first 50 harmonics of the pulse train
signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
Figure 1.19
The sinc(t) function. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
Figure 1.20
Effect of a low sampling frequency
(sampled sine signal) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
Figure 1.21
Aliasing example (3 Hz sine signal) . . . . . . . . . . . . . . . . .
24
Figure 2.1
A probability density function . . . . . . . . . . . . . . . . . . . . .
30
Figure 2.2
A random signal with uniform PDF. . . . . . . . . . . . . . . . . .
31
Figure 2.3
Uniform PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
Figure 2.4
Histogram of a random signal with uniform PDF . . . . . . . .
33
xvii

Figure 2.5
A random signal with normal PDF . . . . . . . . . . . . . . . . . .
34
Figure 2.6
Normal PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
Figure 2.7
Histogram of a random signal with normal PDF . . . . . . . . .
35
Figure 2.8
A random signal with log-normal PDF . . . . . . . . . . . . . . .
37
Figure 2.9
Log-normal PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
Figure 2.10
Histogram of a random signal with log-normal PDF . . . . . .
38
Figure 2.11
Mean, median and mode marked on a PDF . . . . . . . . . . . .
40
Figure 2.12
PSD of a random signal with log-normal PDF . . . . . . . . . .
44
Figure 2.13
The buried sinusoidal signal . . . . . . . . . . . . . . . . . . . . . . .
44
Figure 2.14
The sine+noise signal . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
Figure 2.15
PSD of the sine+noise signal . . . . . . . . . . . . . . . . . . . . . .
46
Figure 2.16
The random signal with normal PDF to be heared. . . . . . . .
47
Figure 2.17
The gamma function . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
Figure 2.18
Gamma-type PDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
Figure 2.19
Example of chi-square PDF . . . . . . . . . . . . . . . . . . . . . . .
50
Figure 2.20
Example of beta PDF . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
Figure 2.21
Example of Student’s t PDF. . . . . . . . . . . . . . . . . . . . . . .
52
Figure 2.22
Example of Weibull PDF. . . . . . . . . . . . . . . . . . . . . . . . .
53
Figure 2.23
Example of Rayleigh PDF . . . . . . . . . . . . . . . . . . . . . . . .
54
Figure 2.24
Example of bivariate Gaussian PDF . . . . . . . . . . . . . . . . .
56
Figure 2.25
Example of bivariate Gaussian PDF . . . . . . . . . . . . . . . . .
56
Figure 2.26
Example of binomial PDF . . . . . . . . . . . . . . . . . . . . . . . .
58
Figure 2.27
Example of Poisson PDF . . . . . . . . . . . . . . . . . . . . . . . . .
59
Figure 2.28
Example of geometric PDF . . . . . . . . . . . . . . . . . . . . . . .
59
Figure 2.29
Normal probability plot . . . . . . . . . . . . . . . . . . . . . . . . . .
61
Figure 2.30
Weibull probability plot . . . . . . . . . . . . . . . . . . . . . . . . . .
62
Figure 2.31
The log-likelihood of the Gaussian distribution,
using 20, or 60, or 200, or 500 data values . . . . . . . . . . . .
64
Figure 2.32
Bimodal distribution and mixture of Gaussians . . . . . . . . . .
68
Figure 2.33
Bimodal distribution example . . . . . . . . . . . . . . . . . . . . . .
69
Figure 2.34
Parzen estimation of PDF. . . . . . . . . . . . . . . . . . . . . . . . .
70
Figure 2.35
Kernel-based estimation of PDF . . . . . . . . . . . . . . . . . . . .
71
Figure 2.36
Area covered by a curve . . . . . . . . . . . . . . . . . . . . . . . . .
73
Figure 2.37
Same as previous figure, but with random points . . . . . . . .
73
Figure 2.38
Example of integral of the product gðxÞ  pðxÞ. . . . . . . . . . .
75
Figure 2.39
Example of un-importance sampling . . . . . . . . . . . . . . . . .
78
Figure 2.40
Example of f(x), and p(x) for importance sampling . . . . . . .
78
Figure 2.41
The inversion procedure. . . . . . . . . . . . . . . . . . . . . . . . . .
81
Figure 2.42
Example of desired distribution function and PDF. . . . . . . .
81
Figure 2.43
Histogram of random data generated using analytical
inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
Figure 2.44
Generation of random data with a certain PDF . . . . . . . . . .
83
Figure 2.45
Histogram of random data generated using numerical
inversion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
xviii
List of Figures

Figure 2.46
Example of desired f(x) PDF and proposal g(x) PDF. . . . . .
85
Figure 2.47
Histogram of random data generated using the rejection
method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
Figure 2.48
Histogram of sum of signals. . . . . . . . . . . . . . . . . . . . . . .
89
Figure 2.49
The PDFs of two variables A and B . . . . . . . . . . . . . . . . .
92
Figure 2.50
The two products of interest . . . . . . . . . . . . . . . . . . . . . . .
93
Figure 2.51
The product of the two PDFs . . . . . . . . . . . . . . . . . . . . . .
93
Figure 2.52
A simple Bayes network . . . . . . . . . . . . . . . . . . . . . . . . .
94
Figure 2.53
Another example of Bayes network. . . . . . . . . . . . . . . . . .
95
Figure 2.54
Two parents in a Bayes network . . . . . . . . . . . . . . . . . . . .
95
Figure 2.55
An example of Markov chain FSM . . . . . . . . . . . . . . . . . .
97
Figure 2.56
Example of Markov Chain result. . . . . . . . . . . . . . . . . . . .
100
Figure 2.57
The case considered in the Metropolis example. . . . . . . . . .
102
Figure 2.58
Histogram of random data generated by the Metropolis
algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
102
Figure 2.59
An example of HMM (speech generator) . . . . . . . . . . . . . .
104
Figure 2.60
Results of HMM example . . . . . . . . . . . . . . . . . . . . . . . .
104
Figure 2.61
Another HMM example. . . . . . . . . . . . . . . . . . . . . . . . . .
105
Figure 2.62
An experiment with the HMM example . . . . . . . . . . . . . . .
105
Figure 2.63
A generic HMM path . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
Figure 2.64
Initial disttool screen . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106
Figure 2.65
Initial randtool screen . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
Figure 2.66
Example of box plot . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108
Figure 3.1
Example of electronic filter . . . . . . . . . . . . . . . . . . . . . . .
118
Figure 3.2
Solution of equation (3.1) . . . . . . . . . . . . . . . . . . . . . . . .
118
Figure 3.3
Another example of electronic filter. . . . . . . . . . . . . . . . . .
119
Figure 3.4
Visualization of poles and zeros of G(s)
on the complex plane . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
Figure 3.5
Comparison of sinusoidal input and output. . . . . . . . . . . . .
122
Figure 3.6
Visualization of the frequency response of example A . . . . .
123
Figure 3.7
Alternative visualization of the frequency response
of example A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
Figure 3.8
Visualization of the frequency response of example B . . . . .
124
Figure 3.9
Alternative visualization of the frequency response
of example B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Figure 3.10
Step response of filter example A . . . . . . . . . . . . . . . . . . .
127
Figure 3.11
Step response of filter example B . . . . . . . . . . . . . . . . . . .
128
Figure 3.12
Three cases of zero-pole maps . . . . . . . . . . . . . . . . . . . . .
129
Figure 3.13
Response of example A to sinusoidal input . . . . . . . . . . . .
130
Figure 3.14
Response of example B to sinusoidal input . . . . . . . . . . . .
131
Figure 3.15
Response of example B to a square signal . . . . . . . . . . . . .
132
Figure 3.16
Response of example B to a high-frequency
square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133
List of Figures
xix

Figure 3.17
Step response of a discrete system. . . . . . . . . . . . . . . . . . .
134
Figure 3.18
Example of zero-pole map for a discrete system . . . . . . . . .
135
Figure 3.19
PSD of u(t), and PSD of y(t) . . . . . . . . . . . . . . . . . . . . . .
136
Figure 3.20
Real and estimated frequency responses of G(s) . . . . . . . . .
137
Figure 3.21
State space model diagram . . . . . . . . . . . . . . . . . . . . . . . .
138
Figure 3.22
A two-tank system example . . . . . . . . . . . . . . . . . . . . . . .
139
Figure 3.23
System state evolution . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
Figure 3.24
Autonomous behaviour of the system . . . . . . . . . . . . . . . .
142
Figure 3.25
Forced response of the system, unit step input . . . . . . . . . .
142
Figure 3.26
Propagation of initial state perturbations. . . . . . . . . . . . . . .
144
Figure 3.27
Evolution of mean and variance of the state
for the previous example . . . . . . . . . . . . . . . . . . . . . . . . .
145
Figure 3.28
Influence of process perturbations . . . . . . . . . . . . . . . . . . .
146
Figure 3.29
Evolution of mean and variance of the state
for the example with process noise . . . . . . . . . . . . . . . . . .
147
Figure 3.30
Histograms of x(2), x(3) and process noise. . . . . . . . . . . . .
148
Figure 3.31
A beta PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
Figure 3.32
Influence of process perturbations . . . . . . . . . . . . . . . . . . .
149
Figure 3.33
Histograms of process noise and x(10). . . . . . . . . . . . . . . .
150
Figure 3.34
Influence of the process and the measurement noises. . . . . .
151
Figure 3.35
Histograms of y(10) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151
Figure 3.36
Influence of the process and the measurement noises. . . . . .
152
Figure 3.37
Histograms of y(10) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
Figure 3.38
Behaviour of a DARMA model example . . . . . . . . . . . . . .
156
Figure 3.39
Semi-annual Sunspot activity . . . . . . . . . . . . . . . . . . . . . .
160
Figure 3.40
Periodogram of semi-annual Sunspot activity . . . . . . . . . . .
161
Figure 3.41
Behaviour of an MA process . . . . . . . . . . . . . . . . . . . . . .
163
Figure 3.42
Roots of the lag polynomial . . . . . . . . . . . . . . . . . . . . . . .
164
Figure 3.43
Covariances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164
Figure 3.44
Behaviour of an AR process. . . . . . . . . . . . . . . . . . . . . . .
165
Figure 3.45
Roots of the lag polynomial . . . . . . . . . . . . . . . . . . . . . . .
166
Figure 3.46
Covariances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
Figure 3.47
Drift and trend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
Figure 3.48
Behaviour of an ARMA model. . . . . . . . . . . . . . . . . . . . .
171
Figure 3.49
Weekly toothpaste sales: (left) data, (right) increments. . . . .
173
Figure 3.50
An AR(1) process simulation . . . . . . . . . . . . . . . . . . . . . .
174
Figure 3.51
Quarterly US personal consumption expenditure:
(left) data, (right) increments . . . . . . . . . . . . . . . . . . . . . .
175
Figure 3.52
Gold prices (monthly): (left) data, (center) log(data),
(right) increments of log(data) . . . . . . . . . . . . . . . . . . . . .
177
Figure 3.53
Monthly beer production in Australia . . . . . . . . . . . . . . . .
178
Figure 3.54
Classical decomposition: (top) beer production,
(below) linear drift, (next below) cosine component,
(bottom) residual. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
xx
List of Figures

Figure 3.55
Cosine fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
180
Figure 4.1
A first order low-pass filter . . . . . . . . . . . . . . . . . . . . . . .
186
Figure 4.2
Frequency response of the first order low-pass filter . . . . . .
186
Figure 4.3
A first order high-pass filter . . . . . . . . . . . . . . . . . . . . . . .
187
Figure 4.4
Frequency response of the first order high-pass filter . . . . . .
187
Figure 4.5
Response of the first order low-pass filter
to a square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
188
Figure 4.6
Response of the first order high–pass filter
to a square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Figure 4.7
A desired low-pass frequency response . . . . . . . . . . . . . . .
191
Figure 4.8
A desired high-pass frequency response . . . . . . . . . . . . . . .
191
Figure 4.9
A desired band-pass frequency response. . . . . . . . . . . . . . .
192
Figure 4.10
A desired band-stop frequency response. . . . . . . . . . . . . . .
192
Figure 4.11
A desired frequency response: approximation and reality . . .
193
Figure 4.12
A filter structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194
Figure 4.13
Foster forms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
195
Figure 4.14
Cauer forms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
Figure 4.15
Radio tuning with an ideal filter . . . . . . . . . . . . . . . . . . . .
198
Figure 4.16
Specification of a low-pass filter . . . . . . . . . . . . . . . . . . . .
199
Figure 4.17
Frequency response of 5th order Butterworth filter. . . . . . . .
200
Figure 4.18
Poles of 5th order Butterworth filter. . . . . . . . . . . . . . . . . .
201
Figure 4.19
Step response of 5th order Butterworth filter . . . . . . . . . . . .
202
Figure 4.20
Effect of n on the frequency response of the Butterworth
filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202
Figure 4.21
Response of 5th order Butterworth filter
to a square wave. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203
Figure 4.22
Frequency response of 5th order Chebyshev 1 filter . . . . . . .
204
Figure 4.23
Poles of 5th order Chebyshev 1 filter . . . . . . . . . . . . . . . . .
205
Figure 4.24
Step response of 5th order Chebyshev 1 filter . . . . . . . . . . .
205
Figure 4.25
Effect of n on the frequency response of the Chebyshev
1 filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
Figure 4.26
Response of 5th order Chebyshev 1 filter
to a square wave. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
206
Figure 4.27
Frequency response of 5th order Chebyshev 2 filter . . . . . . .
208
Figure 4.28
Poles and zeros of 5th order Chebyshev 2 filter. . . . . . . . . .
209
Figure 4.29
Step response of 5th order Chebyshev 2 filter . . . . . . . . . . .
209
Figure 4.30
Effect of n on the frequency response
of the Chebyshev 2 filter . . . . . . . . . . . . . . . . . . . . . . . . .
210
Figure 4.31
Response of 5th order Chebyshev 2 filter
to a square wave. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
210
Figure 4.32
Frequency response of 5th order elliptic filter . . . . . . . . . . .
212
Figure 4.33
Poles of 5th order elliptic filter . . . . . . . . . . . . . . . . . . . . .
212
Figure 4.34
Step response of 5th order elliptiic filter . . . . . . . . . . . . . . .
213
List of Figures
xxi

Figure 4.35
Effect of n on the frequency response of the
elliptic filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
Figure 4.36
Response of 5th order elliptic filter to a square wave . . . . . .
213
Figure 4.37
Comparison of the frequency response of the four filters . . .
215
Figure 4.38
Comparison of the step response of the four filters . . . . . . .
217
Figure 4.39
Frequency response of a band-pass 5th order Butterworth
filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
Figure 4.40
Frequency response of a high-pass 5th order Butterworth
filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
Figure 4.41
Frequency response of a band-stop 5th order Butterworth
filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
Figure 4.42
Frequency response of 5th order Bessel filter . . . . . . . . . . .
222
Figure 4.43
Poles of 5th order Bessel filter . . . . . . . . . . . . . . . . . . . . .
222
Figure 4.44
Step response of 5th order Bessel filter. . . . . . . . . . . . . . . .
223
Figure 4.45
Effect of n on the frequency response of the Bessel filter. . .
223
Figure 4.46
Response of 5th order Bessel filter to a square wave . . . . . .
224
Figure 4.47
Effect of n on the frequency response phase
of the Bessel filter. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
Figure 4.48
Frequency response phases of the five 5th filters . . . . . . . . .
225
Figure 4.49
Polar plot of frequency response of 5th Butterworth,
Chebyshev 1 and Bessel filters . . . . . . . . . . . . . . . . . . . . .
226
Figure 4.50
Polar plot of frequency response of 5th Chebyshev 2,
elliptic and Butterworth filters . . . . . . . . . . . . . . . . . . . . .
227
Figure 4.51
Zoom in of the polar plot of frequency response
of 5th Chebyshev 2, elliptic and Butterworth filters . . . . . . .
228
Figure 4.52
Comparison of the group delay of the five 5th order
filters. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229
Figure 4.53
Recovering a signal buried in noise. . . . . . . . . . . . . . . . . .
230
Figure 4.54
Extracting components from a compound signal . . . . . . . . .
231
Figure 4.55
Original and reconstructed signals . . . . . . . . . . . . . . . . . . .
232
Figure 4.56
The three filtered square signals when using
5th Butterworth filter . . . . . . . . . . . . . . . . . . . . . . . . . . . .
232
Figure 4.57
The three filtered square signals when
using 5th Chebyshev 1 filter . . . . . . . . . . . . . . . . . . . . . . .
233
Figure 4.58
The three filtered square signals when using
5th Chebyshev 2 filter . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
Figure 4.59
The three filtered square signals when
using 5th elliptic filter . . . . . . . . . . . . . . . . . . . . . . . . . . .
234
Figure 4.60
The three filtered square signals when
using 5th Bessel filter. . . . . . . . . . . . . . . . . . . . . . . . . . . .
235
Figure 5.1
Frequency response of the analog filter example . . . . . . . . .
242
Figure 5.2
Frequency response of the digital filter obtained
with bilinear transformation and fs ¼ 1200 Hz . . . . . . . . . .
243
xxii
List of Figures

Figure 5.3
Frequency response of the digital filter obtained
with bilinear transformation and fs ¼ 100 Hz . . . . . . . . . . .
244
Figure 5.4
Frequency response of the digital filter obtained
with the impulse invariance method and fs ¼ 1200 Hz . . . .
244
Figure 5.5
Frequency response of the digital filter obtained
with the impulse invariance method and fs ¼ 100 Hz . . . . .
245
Figure 5.6
The sinc(x) function . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247
Figure 5.7
Frequency response of filter with h(n) ¼ ones(7,1) . . . . . . .
249
Figure 5.8
Frequency response of filter with h(n) ¼ ones(33,1) . . . . . .
250
Figure 5.9
h(n) truncation and time-shift . . . . . . . . . . . . . . . . . . . . . .
251
Figure 5.10
Frequency response of truncated causal filter . . . . . . . . . . .
252
Figure 5.11
Triangular window and frequency response
of the windowed FIR filter. . . . . . . . . . . . . . . . . . . . . . . .
255
Figure 5.12
Hamming window and frequency response
of the windowed FIR filter. . . . . . . . . . . . . . . . . . . . . . . .
257
Figure 5.13
Comparison of Hamming filter frequency response Hf(ωÞ
for several orders N of the ﬁlter . . . . . . . . . . . . . . . . . . . .
258
Figure 5.14
Comparison of hwðnÞ of Hanning, Hamming
and Blackman windows . . . . . . . . . . . . . . . . . . . . . . . . . .
258
Figure 5.15
Comparison of Hf (ωÞ of Hanning, Hamming
and Blackman windowed ﬁlters . . . . . . . . . . . . . . . . . . . .
260
Figure 5.16
Comparison of HwðωÞ of Hanning, Hamming
and Blackman windows . . . . . . . . . . . . . . . . . . . . . . . . . .
260
Figure 5.17
Detail of the stop-band in the Hanning, Hamming
and Blackman windows . . . . . . . . . . . . . . . . . . . . . . . . . .
261
Figure 5.18
Comparison of hwðnÞ of Kaiser window for several
values of β. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
Figure 5.19
Frequency response of the Kaiser windowed filter
for several values of β. . . . . . . . . . . . . . . . . . . . . . . . . . .
264
Figure 5.20
Detail of the stop-and in the Kaiser window
for several values of β. . . . . . . . . . . . . . . . . . . . . . . . . . .
265
Figure 5.21
A typical frequency response of a digital filter . . . . . . . . . .
266
Figure 5.22
Boxcar window and frequency response of the
corresponding windowed FIR filter . . . . . . . . . . . . . . . . . .
268
Figure 5.23
Chebyshev window and frequency response
of the corresponding windowed FIR filter . . . . . . . . . . . . .
269
Figure 5.24
Impulse response h(n) and frequency response H(ωÞ
of a 50th Parks–McClelland FIR ﬁlter . . . . . . . . . . . . . . . .
271
Figure 5.25
Impulse response h(n) and frequency response H(ωÞ
of a 50th least-squares FIR ﬁlter . . . . . . . . . . . . . . . . . . . .
273
Figure 5.26
Frequency responses H(ωÞ of a 50th raised cosine
FIR ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Figure 5.27
Impulse responses h(n) of a 50th raised cosine FIR filter . . .
275
List of Figures
xxiii

Figure 5.28
Train of impulses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
Figure 5.29
Response of the 50th raised cosine FIR filter
to the train of impulses . . . . . . . . . . . . . . . . . . . . . . . . . .
277
Figure 5.30
Response of the 50th raised cosine FIR filter
with 2 fu cut-off frequency to the train of impulses . . . . . . .
278
Figure 5.31
FIR filter coefficients to be applied, according
with the Savitzky–Golay strategy . . . . . . . . . . . . . . . . . . .
279
Figure 5.32
On top, the pure sawtooth signal. Below,
the sawtooth þ noise input signal and the response
of the Savitzky–Golay ﬁlter . . . . . . . . . . . . . . . . . . . . . . .
280
Figure 5.33
Block diagram of an IFIR filter. . . . . . . . . . . . . . . . . . . . .
280
Figure 5.34
Comparison of frequency response
of the four digital filters. . . . . . . . . . . . . . . . . . . . . . . . . .
283
Figure 5.35
Comparison of impulse response
of the four digital filters. . . . . . . . . . . . . . . . . . . . . . . . . .
284
Figure 5.36
Comparison of pole-zero maps of the four digital filters. . . .
286
Figure 5.37
Frequency response amplitude of a Yule–Walker filter
approximating desired frequency response amplitude
(dotted line) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287
Figure 5.38
Pole-zero map of the obtained IIR filter . . . . . . . . . . . . . . .
288
Figure 5.39
Phase of the frequency response of the obtained
IIR filter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
Figure 5.40
Amplitude and phase of the frequency response
of the obtained IIR filter approximating desired
amplitude and phase . . . . . . . . . . . . . . . . . . . . . . . . . . . .
289
Figure 5.41
Pole-zero map of the obtained IIR filter . . . . . . . . . . . . . . .
291
Figure 5.42
Frequency response amplitude and impulse response
of a digital all-pole IIR filter . . . . . . . . . . . . . . . . . . . . . .
292
Figure 5.43
Comparison of all-pole IIR desired and modelled
response, for the arburg() case . . . . . . . . . . . . . . . . . . . . .
293
Figure 5.44
Comparison of all-pole IIR desired and modelled
responses, for the four methods. . . . . . . . . . . . . . . . . . . . .
294
Figure 5.45
Comparison of recursive IIR desired and modelled
response, for the prony() case . . . . . . . . . . . . . . . . . . . . . .
296
Figure 5.46
Comparison of recursive IIR desired and modelled
response, for the stmcb() case. . . . . . . . . . . . . . . . . . . . . .
297
Figure 5.47
Frequency response and pole-zero map of maxflat()
filter example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Figure 5.48
Comparison of filter() and filtfilt() effects. . . . . . . . . . . . . .
299
Figure 5.49
Input (with x-marks) and output of Hilbert filter . . . . . . . . .
301
Figure 5.50
Input (with x-marks) and output of a differentiator . . . . . . .
301
Figure 5.51
Extracting components from a compound signal . . . . . . . . .
303
Figure 5.52
Original and reconstructed signals . . . . . . . . . . . . . . . . . . .
304
Figure 5.53
IIR models of G6 and C6 piano notes . . . . . . . . . . . . . . . .
304
xxiv
List of Figures

Figure 5.54
FDATool initial screen . . . . . . . . . . . . . . . . . . . . . . . . . .
306
Figure 5.55
Result of the defaults FIR filter design. . . . . . . . . . . . . . . .
307
Figure 5.56
Prepare for export to simulink model. . . . . . . . . . . . . . . . .
307
Figure 6.1
Sine signal with decay. . . . . . . . . . . . . . . . . . . . . . . . . . .
314
Figure 6.2
Audio sine signal with decay . . . . . . . . . . . . . . . . . . . . . .
315
Figure 6.3
Envelope of the sine signal with decay . . . . . . . . . . . . . . .
316
Figure 6.4
Sine signal with frequency variation . . . . . . . . . . . . . . . . .
317
Figure 6.5
Audio sine signal with frequency variation. . . . . . . . . . . . .
318
Figure 6.6
Sine signal with frequency variation . . . . . . . . . . . . . . . . .
319
Figure 6.7
Spectrogram of the sine signal with frequency variation. . . .
320
Figure 6.8
Coloured noise signals; the signal below is a delayed
version of the signal on top . . . . . . . . . . . . . . . . . . . . . . .
322
Figure 6.9
Fourier transforms of main signal and composite signal . . . .
323
Figure 6.10
The composite signal and its real cepstrum. . . . . . . . . . . . .
324
Figure 6.11
The ‘I’ vowel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
Figure 6.12
Spectrum of the ‘I’ signal . . . . . . . . . . . . . . . . . . . . . . . .
327
Figure 6.13
Cepstrum of the ‘I’ signal . . . . . . . . . . . . . . . . . . . . . . . .
328
Figure 6.14
Spectrogram of a quadratic chirp signal . . . . . . . . . . . . . . .
328
Figure 6.15
The result of adding 10 and 11 Hz sine signals. . . . . . . . . .
330
Figure 6.16
DFT and chirp-z transform of the composite signal . . . . . . .
331
Figure 6.17
Car Doppler signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
333
Figure 6.18
Spectrum shift due to Doppler effect . . . . . . . . . . . . . . . . .
333
Figure 6.19
Spectrogram of the car Doppler signal . . . . . . . . . . . . . . . .
334
Figure 6.20
Spectrogram of siren signal . . . . . . . . . . . . . . . . . . . . . . .
335
Figure 6.21
Transformer sound . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
Figure 6.22
Spectral density of the transformer signal. . . . . . . . . . . . . .
337
Figure 6.23
A quack signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
Figure 6.24
Changes in the spectrum shape along the quack . . . . . . . . .
339
Figure 6.25
Triangle signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
Figure 6.26
Spectrogram of the Big Ben chime . . . . . . . . . . . . . . . . . .
341
Figure 6.27
Spectrogram of a harp phrase . . . . . . . . . . . . . . . . . . . . . .
342
Figure 6.28
A synthesised audio signal with ADSR envelope . . . . . . . .
343
Figure 6.29
Earthquake vertical acceleration record . . . . . . . . . . . . . . .
344
Figure 6.30
Spectrum of the main part of quake signal . . . . . . . . . . . . .
345
Figure 6.31
A periodic impulse train and its autocovariance . . . . . . . . .
346
Figure 6.32
An ECG record and its covariance . . . . . . . . . . . . . . . . . .
347
Figure 6.33
ECG record showing problems . . . . . . . . . . . . . . . . . . . . .
348
Figure 6.34
Spectrogram of elephant trumpeting . . . . . . . . . . . . . . . . .
349
Figure 6.35
The mooing of a cow signal. . . . . . . . . . . . . . . . . . . . . . .
350
Figure 6.36
Spectrogram of cow sound. . . . . . . . . . . . . . . . . . . . . . . .
351
Figure 7.1
GMP signal and spectrum . . . . . . . . . . . . . . . . . . . . . . . .
361
Figure 7.2
Sine signal for STFT testing. . . . . . . . . . . . . . . . . . . . . . .
365
Figure 7.3
STFT of 1-sine signal and time fine precision. . . . . . . . . . .
366
Figure 7.4
STFT of 1-sine signal and frequency fine precision . . . . . . .
367
List of Figures
xxv

Figure 7.5
Dennis Gabor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
368
Figure 7.6
Idea of the Gabor expansion. . . . . . . . . . . . . . . . . . . . . . .
369
Figure 7.7
Visualization of the Gabor logons . . . . . . . . . . . . . . . . . . .
370
Figure 7.8
Scalogram of square signal. . . . . . . . . . . . . . . . . . . . . . . .
373
Figure 7.9
Eugene Wigner. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
376
Figure 7.10
A test signal composed of two different GMPs . . . . . . . . . .
379
Figure 7.11
Wigner distribution of the previous test signal . . . . . . . . . .
380
Figure 7.12
Frequency marginal of the Wigner distribution
(2 GMP signal). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
Figure 7.13
Time marginal of the Wigner distribution
(2 GMP signal). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
382
Figure 7.14
SAF of the previous test signal. . . . . . . . . . . . . . . . . . . . .
384
Figure 7.15
Frequency autocorrelation (2 GMP signal) . . . . . . . . . . . . .
386
Figure 7.16
Time autocorrelation (2 GMP signal). . . . . . . . . . . . . . . . .
386
Figure 7.17
SAF obtained from Wigner distribution (2 GMP signal). . . .
387
Figure 7.18
Wigner distribution obtained from SAF,
two GMPs signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
389
Figure 7.19
Signal with 2 sine components . . . . . . . . . . . . . . . . . . . . .
390
Figure 7.20
Wigner distribution of the signal with 2 sine components. . .
391
Figure 7.21
Frequency marginal of the Wigner distribution
(2 sine signal). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
Figure 7.22
Time marginal of the Wigner distribution (2 sine signal) . . .
392
Figure 7.23
Quadratic chirp signal . . . . . . . . . . . . . . . . . . . . . . . . . . .
392
Figure 7.24
Wigner distribution of the quadratic chirp signal . . . . . . . . .
393
Figure 7.25
Kernel and filtered SAF for 2 GMP signal . . . . . . . . . . . . .
394
Figure 7.26
Filtered Wigner distribution for 2 GMP signal . . . . . . . . . .
394
Figure 7.27
Kernel and filtered SAF for chirp signal . . . . . . . . . . . . . .
396
Figure 7.28
Filtered Wigner distribution for chirp signal . . . . . . . . . . . .
397
Figure 7.29
Analysis in the TF plane using inner product . . . . . . . . . . .
401
Figure 7.30
Frequency shift þ time shift of the analysis function . . . . .
401
Figure 7.31
Scaling þ time shift of the analysis function . . . . . . . . . . .
402
Figure 7.32
Tiling of the TF plane corresponding to the first
alternative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
402
Figure 7.33
Tiling of the TF plane corresponding to the second
alternative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
403
Figure 7.34
Fourier relationships between kernels (Cohen’s class) . . . . .
413
Figure 7.35
The Choi–Williams kernel . . . . . . . . . . . . . . . . . . . . . . . .
415
Figure 7.36
Fractional Fourier transform of one cosine cycle . . . . . . . . .
423
Figure 7.37
Fractional Fourier transform of a chirp which rate
corresponds to the transform exponent. . . . . . . . . . . . . . . .
425
Figure 7.38
An oblique perspective . . . . . . . . . . . . . . . . . . . . . . . . . .
427
Figure 7.39
Four affine transformations of the rectangular tiling. . . . . . .
428
xxvi
List of Figures

Figure 7.40
Two more affine transformations, and two perspective
projections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
Figure 7.41
Wigner distribution of prolate signal . . . . . . . . . . . . . . . . .
429
Figure 7.42
Wigner distribution of prolate function with frequency
shear . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
430
Figure 7.43
Example of Gaussian chirplet atom . . . . . . . . . . . . . . . . . .
433
Figure 7.44
Wigner distribution of a Gaussian chirplet atom . . . . . . . . .
433
Figure 7.45
Example of dopplerlet . . . . . . . . . . . . . . . . . . . . . . . . . . .
434
Figure 7.46
Example of warblet . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
435
Figure 7.47
Instantaneous frequency of the warblet example . . . . . . . . .
436
Figure 7.48
The modulated signal . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
Figure 7.49
Wigner distribution of the original signal . . . . . . . . . . . . . .
440
Figure 7.50
Wigner distribution of the warped signal . . . . . . . . . . . . . .
440
Figure 7.51
The time-warping relationship. . . . . . . . . . . . . . . . . . . . . .
442
Figure 7.52
The frequency conversion factor . . . . . . . . . . . . . . . . . . . .
442
Figure 7.53
Unwarped Wigner distribution . . . . . . . . . . . . . . . . . . . . .
444
Figure 7.54
Re-assigning idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
445
Figure 7.55
A re-assigned spectrogram example. . . . . . . . . . . . . . . . . .
445
Figure 7.56
Original spectrogram of the siren signal . . . . . . . . . . . . . . .
447
Figure 7.57
Reassigned spectrogram of the siren signal. . . . . . . . . . . . .
447
Figure 7.58
Spectrogram of a linear chirp . . . . . . . . . . . . . . . . . . . . . .
449
Figure 7.59
Modified S-transform of the linear chirp . . . . . . . . . . . . . .
449
Figure 7.60
Geometry corresponding to the Fan-Chirp transform . . . . . .
452
Figure 7.61
Spectrogram of a linear chirp . . . . . . . . . . . . . . . . . . . . . .
452
Figure 7.62
Fan-Chirp transform of the linear chirp . . . . . . . . . . . . . . .
453
Figure 7.63
A signal, its 1st IMF, and its 1st residue. . . . . . . . . . . . . . .
456
Figure 7.64
The 1st residue, the 2nd IMF, and the 2nd residue . . . . . . . .
456
Figure 7.65
The upper and lower envelopes . . . . . . . . . . . . . . . . . . . .
457
Figure 7.66
The mean of the two envelopes . . . . . . . . . . . . . . . . . . . .
457
Figure 7.67
The signal and the mean of envelopes . . . . . . . . . . . . . . . .
458
Figure 7.68
The first IMF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
458
Figure 7.69
The electrocardiogram . . . . . . . . . . . . . . . . . . . . . . . . . . .
461
Figure 7.70
The first five IMFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
461
Figure 7.71
The 6th to 10th IMFs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
Figure 7.72
The first five IMFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
464
Figure 7.73
The Hilbert spectrum. . . . . . . . . . . . . . . . . . . . . . . . . . . .
464
Figure 7.74
Fractional Fourier transforms of a rectangular signal,
using different values of the exponent . . . . . . . . . . . . . . . .
468
Figure 7.75
The fractional Fourier transform of the rectangle
becomes close to the sinc signal for a ¼ 0.99. . . . . . . . . . .
469
Figure 7.76
Wigner analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
469
Figure 7.77
Bat chirp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
470
Figure 7.78
Bird tweet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
471
Figure 7.79
Sound of normal respiration (10 s) . . . . . . . . . . . . . . . . . .
472
List of Figures
xxvii

Figure 7.80
Scalogram of a signal segment . . . . . . . . . . . . . . . . . . . . .
473
Figure 7.81
Sound of respiration with crackles (10 s) . . . . . . . . . . . . . .
473
Figure 7.82
Scalogram of a signal segment . . . . . . . . . . . . . . . . . . . . .
474
Figure 7.83
Scalogram of heart sound (2 beats) . . . . . . . . . . . . . . . . . .
475
Figure 7.84
Spectrogram of the quack. . . . . . . . . . . . . . . . . . . . . . . . .
475
Figure 7.85
Fan-Chirp transform of the quack . . . . . . . . . . . . . . . . . . .
476
Figure 7.86
Spectrogram of the dog bark . . . . . . . . . . . . . . . . . . . . . .
476
Figure 7.87
Fan-Chirp transform of the dog bark . . . . . . . . . . . . . . . . .
477
Figure 7.88
Respiration with wheezing, 3 levels of detail . . . . . . . . . . .
478
Figure 7.89
Spectrogram of the signal segment with wheezing. . . . . . . .
479
Figure 7.90
Modified S-transform of the signal segment
with wheezing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
479
Figure 7.91
Spectrogram of whale song (divided into 2 parts) . . . . . . . .
480
Figure 7.92
Modified S-transform of whale song
(divided into 2 parts) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
480
Figure 7.93
The Earthquake signal at two detail levels . . . . . . . . . . . . .
482
Figure 7.94
Spectrogram of the signal segment . . . . . . . . . . . . . . . . . .
482
Figure 7.95
Modified S-transform of the signal segment . . . . . . . . . . . .
483
Figure 7.96
Extraction of a T-F region of interest. . . . . . . . . . . . . . . . .
483
Figure 7.97
The extracted signal segment at two levels of detail . . . . . .
494
Figure 8.1
The Brant Rock radio tower, and Mr. R. Fessenden . . . . . .
496
Figure 8.2
Radio transmission can be done using modulation
and demodulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
496
Figure 8.3
AM modulation diagram . . . . . . . . . . . . . . . . . . . . . . . . .
497
Figure 8.4
Amplitude modulation of sine signal . . . . . . . . . . . . . . . . .
498
Figure 8.5
Amplitude modulation of audio sine signal. . . . . . . . . . . . .
499
Figure 8.6
Spectral density and amplitude modulation. . . . . . . . . . . . .
501
Figure 8.7
Diode demodulation of DSB amplitude modulated signal. . .
503
Figure 8.8
Suppressed carrier amplitude modulation of sine signal . . . .
504
Figure 8.9
SSB amplitude modulation with a sawtooth signal . . . . . . .
506
Figure 8.10
Mr. Armstrong, his wife and a superheterodyne radio . . . . .
508
Figure 8.11
Frequency modulation of sine signal . . . . . . . . . . . . . . . . .
508
Figure 8.12
Frequency of FM modulated signal in audio example . . . . .
509
Figure 8.13
Spectral density of frequency modulated signal. . . . . . . . . .
511
Figure 8.14
Pulse modulation of sine signal. . . . . . . . . . . . . . . . . . . . .
513
Figure 8.15
ASK, FSK and PSK modulation of sine signal . . . . . . . . . .
513
Figure 8.16
The four basic pieces of 4_PSK modulation . . . . . . . . . . . .
514
Figure 8.17
Example of 4-PSK modulation . . . . . . . . . . . . . . . . . . . . .
515
Figure 8.18
Amplitude modulation of sine signal . . . . . . . . . . . . . . . . .
517
Figure 8.19
C.E. Shannon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
519
Figure 8.20
Modulation of pulses. . . . . . . . . . . . . . . . . . . . . . . . . . . .
519
Figure 8.21
Analog modulation of pulses . . . . . . . . . . . . . . . . . . . . . .
520
Figure 8.22
Demodulation of pulses . . . . . . . . . . . . . . . . . . . . . . . . . .
521
Figure 8.23
Frequency division multiplexing50 . . . . . . . . . . . . . . . . . .
524
xxviii
List of Figures

Figure 8.24
Time division multiplexing. . . . . . . . . . . . . . . . . . . . . . . .
524
Figure 8.25
Noisy communication . . . . . . . . . . . . . . . . . . . . . . . . . . .
525
Figure 8.26
AM communication in the presence of noise . . . . . . . . . . .
526
Figure 8.27
PWM communication in the presence of noise . . . . . . . . . .
528
Figure 8.28
PTM communication in the presence of noise. . . . . . . . . . .
528
Figure 8.29
Cepstrum of AM modulated signal, compared
with added signals on top . . . . . . . . . . . . . . . . . . . . . . . .
529
Figure A.1
Square wave. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
Figure A.2
Single pulse . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
538
Figure A.3
Single triangle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
539
Figure A.4
Basic diagram. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
543
Figure A.5
The N = 4 DFT computation procedure . . . . . . . . . . . . . . .
543
Figure A.6
The N=8 DFT decomposition into 3 stages . . . . . . . . . . . .
544
Figure A.7
The N = 8 DFT computation procedure . . . . . . . . . . . . . . .
544
Figure B.1
Weather prediction model with three states (Fig. 2.55). . . . .
558
Figure B.2
Simple HMM model of speech (Fig. 2.59) . . . . . . . . . . . . .
559
Figure B.3
Comparison of the group delay of the five
5th order ﬁlters (Fig. 4.52) . . . . . . . . . . . . . . . . . . . . . . . .
561
Figure B.4
Recovering a signal buried in noise (Fig. 4.53). . . . . . . . . .
563
Figure B.5
Extracting components from a compound signal
(Fig. 4.54) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
564
Figure B.6
Original and reconstructed signals (Fig. 4.55) . . . . . . . . . . .
565
Figure B.7
Comparison of pole-zero maps of the four digital filters
(Fig. 5.36) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
566
Figure B.8
Extracting components from a compound signal
(Fig. 5.51) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
568
Figure B.9
Original and reconstructed signals (Fig. 5.52) . . . . . . . . . . .
568
Figure B.10
Wigner distribution of a 2 sine signal (Fig. 7.20) . . . . . . . .
570
Figure B.11
Wigner distribution of a chirp signal (Fig. 7.24) . . . . . . . . .
572
Figure B.12
Kernel and filtered SAF for chirp signal (Fig. 7.27). . . . . . .
573
Figure B.13
Filtered Wigner distribution for chirp signal (Fig. 7.28) . . . .
573
Figure B.14
Fractional Fourier transform of a chirp which rate
corresponds to the transform exponent. . . . . . . . . . . . . . . .
575
Figure B.15
Wigner distribution of a Gaussian chirplet atom
(Fig. 7.44) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
577
Figure B.16
Wigner distribution of the original signal (Fig. 7.49) . . . . . .
578
Figure B.17
Original spectrogram of the siren signal (Fig. 7.56) . . . . . . .
579
Figure B.18
Reassigned spectrogram of the siren signal (Fig. 7.57). . . . .
579
Figure B.19
Spectrogram of a linear chirp (Fig. 7.61) . . . . . . . . . . . . . .
582
Figure B.20
Fan-Chirp transform of the linear chirp (Fig. 7.62) . . . . . . .
582
Figure B.21
The first five IMFs (Fig. 7.72) . . . . . . . . . . . . . . . . . . . . .
584
Figure B.22
The Hilbert spectrum (Fig. 7.63). . . . . . . . . . . . . . . . . . . .
584
Figure B.23
Fractional Fourier transforms of a rectangular signal,
using different values of the exponent (Fig. 7.74) . . . . . . . .
587
List of Figures
xxix

Figure B.24
The fractional Fourier transform of the rectangle
becomes close to the sinc signal for a ¼ 0.99 (Fig. 7.75). . .
587
Figure B.25
Wigner analysis (Fig. 7.6) . . . . . . . . . . . . . . . . . . . . . . . .
589
Figure B.26
Bat chirp (Fig. 7.7) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
592
Figure B.27
Bird tweet (Fig. 7.78) . . . . . . . . . . . . . . . . . . . . . . . . . . .
593
Figure B.28
Sound of normal respiration (10 s) (Fig. 7.79) . . . . . . . . . .
594
Figure B.29
Scalogram of a signal segment (Fig. 7.80) . . . . . . . . . . . . .
595
Figure B.30
Sound of respiration with crackles (10 s) (Fig. 7.81) . . . . . .
596
Figure B.31
Scalogram of a signal segment (Fig. 7.82) . . . . . . . . . . . . .
597
Figure B.32
Scalogram of heart sound (2 beats) (Fig. 7.83) . . . . . . . . . .
598
Figure B.33
Spectrogram of the quack (Fig. 7.84). . . . . . . . . . . . . . . . .
600
Figure B.34
Fan-Chirp transform of the quack (Fig. 7.85) . . . . . . . . . . .
600
Figure B.35
Spectrogram of the dog bark (Fig. 7.86) . . . . . . . . . . . . . .
602
Figure B.36
Fan-Chirp transform of the dog bark (Fig. 7.87) . . . . . . . . .
602
Figure B.37
Respiration with wheezing, 3 levels of detail (Fig. 7.88) . . .
604
Figure B.38
Spectrogram of the signal segment with wheezing
(Fig. 7.89) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
604
Figure B.39
Modified S-transform of the signal segment
with wheezing (Fig. 7.90) . . . . . . . . . . . . . . . . . . . . . . . .
605
Figure B.40
Spectrogram of whale song (divided into 2 parts)
(Fig. 7.91) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
606
Figure B.41
Modified S-transform of whale song
(divided into 2 parts) (Fig. 7.92) . . . . . . . . . . . . . . . . . . . .
606
Figure B.42
The Earthquake signal at two detail levels (Fig. 7.93) . . . . .
608
Figure B.43
Spectrogram of the signal segment (Fig. 7.94) . . . . . . . . . .
608
Figure B.44
Modified S-transform of the signal segment (Fig. 7.95) . . . .
609
Figure B.45
Extraction of a TF region of interest (Fig. 7.96) . . . . . . . . .
611
Figure B.46
The extracted signal segment at two levels of detail
(Fig. 7.97) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
611
Figure B.47
ASK, FSK and PSK modulation of sine signal
(Fig. 8.15) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
613
xxx
List of Figures

Listings
1.1
Square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.2
Sine signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
Sine and cosine signals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.4
Square signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.5
Sawtooth signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.6
Sawtooth signals. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
1.7
Sine audio signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
1.8
Sum of sines signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.9
Multiplication of sines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
1.10
Sawtooth signal to be analyzed. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
1.11
Fourier transform of sawtooth signal . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.12
Rectiﬁed signal to be analyzed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
1.13
Fourier transform of rectiﬁed signal . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.14
Triangular signal to be analyzed. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
1.15
Fourier transform of triangular signal . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.16
Pulse train signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.17
Fourier transform of pulse train signal . . . . . . . . . . . . . . . . . . . . . . . .
20
1.18
Sinc function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.19
Sine signal and low sampling frequency. . . . . . . . . . . . . . . . . . . . . . .
23
1.20
Sine signal and aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
2.1
Random signal with uniform PDF . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.2
Uniform PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.3
Histogram of a random signal with uniform PDF. . . . . . . . . . . . . . . .
33
2.4
Random signal with normal PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.5
Normal PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
2.6
Histogram of a random signal with normal PDF . . . . . . . . . . . . . . . .
36
2.7
Random signal with log-normal PDF . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.8
Log-normal PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.9
Histogram of a random signal with log-normal PDF . . . . . . . . . . . . .
38
2.10
A skewed PDF with mean, median and mode . . . . . . . . . . . . . . . . . .
40
xxxi

2.11
Power spectral density (PSD) of random signal
with log-normal PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
2.12
The sine+noise signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.13
Power spectral density (PSD) of a signal+noise . . . . . . . . . . . . . . . . .
45
2.14
See and hear a random signal with normal PDF. . . . . . . . . . . . . . . . .
46
2.15
Gamma function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
2.16
Gamma-type PDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
2.17
Chi-square PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
2.18
beta PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
2.19
Student’s PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
52
2.20
Weibull PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
2.21
Rayleigh PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
2.22
Bivariate normal PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
2.23
Binomial PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
2.24
Poisson PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58
2.25
Geometric PDF. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.26
Normal probability plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60
2.27
Weibull probability plot . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
61
2.28
Likelihood example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
2.29
Mixture of 2 Gaussians. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67
2.30
Histogram of Bimodal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
2.31
Kernel method example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
2.32
Curve and area . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
2.33
Monte Carlo points, and area approximation . . . . . . . . . . . . . . . . . . .
73
2.34
Integration as expected value . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
2.35
Integration as expected value: Importance sampling . . . . . . . . . . . . . .
78
2.36
Generation of random data with a desired PDF . . . . . . . . . . . . . . . . .
81
2.37
Numerical inversion of a function . . . . . . . . . . . . . . . . . . . . . . . . . . .
83
2.38
Generation of random data with a desired PDF . . . . . . . . . . . . . . . . .
84
2.39
Generation of random data with a desired PDF . . . . . . . . . . . . . . . . .
85
2.40
Central limit of wav sounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
88
2.41
Two overlapped PDFs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
2.42
Generation of random data with a desired PDF . . . . . . . . . . . . . . . . .
101
2.43
Example of box plots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
3.1
Pole-zero map of G(s) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
3.2
Frequency response of example A . . . . . . . . . . . . . . . . . . . . . . . . . . .
122
3.3
Frequency response of example A . . . . . . . . . . . . . . . . . . . . . . . . . . .
123
3.4
Frequency response of example B . . . . . . . . . . . . . . . . . . . . . . . . . . .
124
3.5
Frequency response of example B . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
3.6
Step response of example A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
3.7
Step response of example B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
127
3.8
Pole-zero maps of three G(s) cases. . . . . . . . . . . . . . . . . . . . . . . . . . .
129
3.9
Time-domain response to sine, example A . . . . . . . . . . . . . . . . . . . . .
130
3.10
Time-domain response to sine, example B . . . . . . . . . . . . . . . . . . . . .
131
xxxii
Listings

3.11
Time-domain response to square signal, example B . . . . . . . . . . . . . .
132
3.12
Step response of a discrete system . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
3.13
Pole-zero map of a discrete system. . . . . . . . . . . . . . . . . . . . . . . . . . .
134
3.14
PSDs of random u(t) and output y(t) . . . . . . . . . . . . . . . . . . . . . . . . .
136
3.15
Estimate of the transfer function from random input u(t)
and output y(t) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137
3.16
System example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
3.17
Deterministic state evolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
3.18
Deterministic state evolution. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
143
3.19
Propagation of uncertainty on initial state. . . . . . . . . . . . . . . . . . . . . .
143
3.20
Evolution of state mean and variance . . . . . . . . . . . . . . . . . . . . . . . . .
144
3.21
Inﬂuence of Gaussian process noise . . . . . . . . . . . . . . . . . . . . . . . . . .
145
3.22
Evolution of state mean and variance . . . . . . . . . . . . . . . . . . . . . . . . .
146
3.23
Propagation of Gaussian state noise: details of states
x(2) and x(3) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147
3.24
A beta PDF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149
3.25
Propagation of non-Gaussian process noise . . . . . . . . . . . . . . . . . . . .
149
3.26
Inﬂuence of Gaussian process noise and measurement noise . . . . . . .
151
3.27
Inﬂuence of Gaussian process noise . . . . . . . . . . . . . . . . . . . . . . . . . .
152
3.28
Example of DARMA behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156
3.29
Example of DARMA behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
157
3.30
Periodogram of Sunspots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160
3.31
Example of MA behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162
3.32
Example of AR behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
3.33
Drift and trend . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167
3.34
Example of ARMA behaviour . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
3.35
Example of weekly toothpaste sales . . . . . . . . . . . . . . . . . . . . . . . . . .
173
3.36
AR(1) model of weekly toothpaste sales. . . . . . . . . . . . . . . . . . . . . . .
174
3.37
Quarterly US personal consumption expediture . . . . . . . . . . . . . . . . .
175
3.38
Gold prices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
3.39
Australian Beer production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179
4.1
Frequency response of high-pass ﬁlter . . . . . . . . . . . . . . . . . . . . . . . .
188
4.2
Response to square signal, low-pass ﬁlter. . . . . . . . . . . . . . . . . . . . . .
188
4.3
Response to square signal, high-pass ﬁlter . . . . . . . . . . . . . . . . . . . . .
190
4.4
Abstract design, desired ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193
4.5
Frequency response of Butterworth ﬁlter . . . . . . . . . . . . . . . . . . . . . .
200
4.6
Pole-zero map of Butterworth ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . .
201
4.7
Step response of Butterworth ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . .
201
4.8
Comparison of frequency response of Butterworth ﬁlters . . . . . . . . . .
202
4.9
Response of Butterworth ﬁlter to square signal . . . . . . . . . . . . . . . . .
203
4.10
FrB . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
4.11
FrC1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204
4.12
FrC2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209
4.13
Pole-zero map of Chebyshev 2 ﬁlter. . . . . . . . . . . . . . . . . . . . . . . . . .
210
Listings
xxxiii

4.14
FrE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
4.15
Pole-zero map of elliptic ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
4.16
Comparison of frequency response of the 4 ﬁlters . . . . . . . . . . . . . . .
215
4.17
Comparison of step response of the 4 ﬁlters. . . . . . . . . . . . . . . . . . . .
216
4.18
Frequency response of band-pass Butterworth ﬁlter . . . . . . . . . . . . . .
218
4.19
Frequency response of high-pass Butterworth ﬁlter . . . . . . . . . . . . . .
218
4.20
Frequency response of band-stop Butterworth ﬁlter . . . . . . . . . . . . . .
219
4.21
FrT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
4.22
Comparison of frequency response phase of Bessel ﬁlters . . . . . . . . .
223
4.23
Frequency response phases of the ﬁve ﬁlters . . . . . . . . . . . . . . . . . . .
225
4.24
Comparison of frequency response phase of Butterworth,
Chebyshev 1 and Bessel ﬁlters in polar plane . . . . . . . . . . . . . . . . . .
226
4.25
Comparison of frequency response phase of Butterworth,
Chebishev 2 and elliptic ﬁlters in polar plane. . . . . . . . . . . . . . . . . . .
227
4.26
Response of Butterworth ﬁlter to square signal near cut-off . . . . . . . .
231
5.1
Frequency response of analog ﬁlter example . . . . . . . . . . . . . . . . . . .
241
5.2
Bilinear transformation from analog ﬁlter to digital ﬁlter . . . . . . . . . .
242
5.3
Invariant impulse transformation from analog ﬁlter to digital
ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
245
5.4
The sinc function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246
5.5
h(n) = 7 ones, H(w) of the digital ﬁlter . . . . . . . . . . . . . . . . . . . . . . .
248
5.6
h(n) = 13 ones, H(w) of the digital ﬁlter . . . . . . . . . . . . . . . . . . . . . .
249
5.7
Truncation and time shifting. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252
5.8
Frequency response of the truncated ﬁlter. . . . . . . . . . . . . . . . . . . . . .
253
5.9
hw(n) of triangular window, and frequency response Hf(w)
of windowed ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
255
5.10
hw(n) of Hamming window, and frequency response Hf(w)
of windowed ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256
5.11
Comparison of hw(n) of Hanning, Hamming and Blackman
windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
5.12
Comparison of Hf(w) of Hanning, Hamming and Blackman
windowed ﬁlters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
259
5.13
Comparison of Hw(w) of Hanning, Hamming and Blackman
windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
260
5.14
Comparison of stop-band of Hw(w) of Hanning, Hamming
and Blackman windows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
5.15
Comparison of hw(n) of Kaiser window. . . . . . . . . . . . . . . . . . . . . . .
263
5.16
Comparison of Hf(w) of Kaiser window. . . . . . . . . . . . . . . . . . . . . . .
264
5.17
Comparison of Hw(w) of Kaiser window . . . . . . . . . . . . . . . . . . . . . .
265
5.18
hw(n) of boxcar window, and frequency response Hf(w)
of windowed ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
5.19
hw(n) of Chebyshev window, and frequency response Hf(w)
of windowed ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
269
5.20
h(n) and Hf(w) of (Remez) Parks-McClellan ﬁlter . . . . . . . . . . . . . . .
271
xxxiv
Listings

5.21
h(n) and Hf(w) of least-squares error ﬁlter . . . . . . . . . . . . . . . . . . . . .
272
5.22
Frequency response of raised cosine ﬁlter. . . . . . . . . . . . . . . . . . . . . .
274
5.23
Impulse response of raised cosine ﬁlter . . . . . . . . . . . . . . . . . . . . . . .
275
5.24
Train of impulses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
5.25
Response of the raised cosine ﬁlter to the train of impulses . . . . . . . .
276
5.26
FIR ﬁlter coefﬁcients with Savitzky-Golay ﬁlter. . . . . . . . . . . . . . . . .
278
5.27
Frequency response of sgolay ﬁlter. . . . . . . . . . . . . . . . . . . . . . . . . . .
279
5.28
Comparison of frequency response of the 4 digital ﬁlters . . . . . . . . . .
283
5.29
Comparison of impulse response of the 4 digital ﬁlters . . . . . . . . . . .
284
5.30
Frequency response of IIR yulewalk ﬁlter . . . . . . . . . . . . . . . . . . . . .
287
5.31
Pole-zero map of IIR yulewalk ﬁlter. . . . . . . . . . . . . . . . . . . . . . . . . .
287
5.32
Phase of the frequency response of IIR yulewalk ﬁlter. . . . . . . . . . . .
288
5.33
Frequency response of IIR invfreqz ﬁlter . . . . . . . . . . . . . . . . . . . . . .
289
5.34
Pole-zero map of IIR invfreqz ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . .
290
5.35
Reference IIR ﬁlter: frequency and impulse responses . . . . . . . . . . . .
291
5.36
IIR from impulse response, using arburg . . . . . . . . . . . . . . . . . . . . . .
292
5.37
IIR from impulse response, the four methods . . . . . . . . . . . . . . . . . . .
293
5.38
IIR from impulse response, using prony . . . . . . . . . . . . . . . . . . . . . . .
295
5.39
IIR from impulse response, using stmcb. . . . . . . . . . . . . . . . . . . . . . .
296
5.40
Frequency response of IIR maxﬂat ﬁlter. . . . . . . . . . . . . . . . . . . . . . .
298
5.41
Comparing ﬁlter() with ﬁltﬁlt() . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
299
5.42
Effect of Hilbert ﬁlter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
300
5.43
Effect of a differentiator . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
302
5.44
Piano note modelling, using stmcb . . . . . . . . . . . . . . . . . . . . . . . . . . .
305
6.1
Sine signal with decay . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
314
6.2
Sound of a sine signal with decay . . . . . . . . . . . . . . . . . . . . . . . . . . .
315
6.3
Hilbert and the envelope of sine signal with decay. . . . . . . . . . . . . . .
316
6.4
Sine signal with frequency variation. . . . . . . . . . . . . . . . . . . . . . . . . .
317
6.5
Sound of sine signal with frequency variation . . . . . . . . . . . . . . . . . .
318
6.6
Hilbert and the frequency of sine signal with frequency variation . . .
319
6.7
Spectrogram of sine signal with frequency variation. . . . . . . . . . . . . .
320
6.8
Coloured noise and echo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
321
6.9
Spectra of input and composite signals. . . . . . . . . . . . . . . . . . . . . . . .
323
6.10
Composite signal and cepstrum . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
6.11
Hear and plot vowel signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
326
6.12
Analyse vowel with spectrum. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
6.13
Analyse vowel with cepstrum. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327
6.14
Spectrogram of a chirp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
329
6.15
Two added sines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
330
6.16
Chirp-z transform of a signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
331
6.17
Hear and see car doppler WAV . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
332
6.18
Spectral densities of Doppler signal begin and end. . . . . . . . . . . . . . .
333
6.19
Spectrogram of car doppler signal . . . . . . . . . . . . . . . . . . . . . . . . . . .
334
6.20
Spectrogram of siren signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
335
Listings
xxxv

6.21
Hear and see transformer signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
336
6.22
Spectral density of transformer signal. . . . . . . . . . . . . . . . . . . . . . . . .
337
6.23
Hear and see quack WAV . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
338
6.24
Spectral densities of quack signal begin and end . . . . . . . . . . . . . . . .
339
6.25
Triangle signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
340
6.26
Spectrogram of Big-Ben signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
6.27
Spectrogram of harp signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
341
6.28
ADSR synthesis of audio sine signal . . . . . . . . . . . . . . . . . . . . . . . . .
342
6.29
Read quake data ﬁle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
344
6.30
Spectrum of central quake signal . . . . . . . . . . . . . . . . . . . . . . . . . . . .
345
6.31
Impulse train autocovariance. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
346
6.32
Read ECG data ﬁle and compute autocovariance . . . . . . . . . . . . . . . .
347
6.33
Read ECG data ﬁle and compute autocovariance . . . . . . . . . . . . . . . .
348
6.34
Spectrogram of Elephant signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
6.35
Hear and see cow WAV. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
6.36
Spectrogram of cow signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
351
7.1
GMP signal and spectrum. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
361
7.2
STFT of 1-sine signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
366
7.3
STFT of 1-sine signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
367
7.4
Plot of 5x6 large logons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
371
7.5
Signal analysis by continuous wavelet transform . . . . . . . . . . . . . . . .
374
7.6
2 GMPs signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
7.7
Wigner distribution of 2 GMPs signal . . . . . . . . . . . . . . . . . . . . . . . .
380
7.8
SAF of 2 GMPs signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
384
7.9
SAF from Wigner distribution of 2 GMPs signal . . . . . . . . . . . . . . . .
387
7.10
Wigner distribution from SAF, of 2 GMPs signal . . . . . . . . . . . . . . .
388
7.11
Quadratic chirp signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
391
7.12
WD of 2 GMPs signal, with no interference. . . . . . . . . . . . . . . . . . . .
395
7.13
The Choi-Williams kernel. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
415
7.14
Fractional Fourier transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
423
7.15
Wigner distribution of prolate signal. . . . . . . . . . . . . . . . . . . . . . . . . .
429
7.16
Wigner distribution of prolate signal with frequency shear. . . . . . . . .
431
7.17
Gaussian chirplet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
432
7.18
Dopplerlet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
435
7.19
Warblet. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
436
7.20
Modulated signal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
439
7.21
Wigner distribution of a warped modulated signal . . . . . . . . . . . . . . .
440
7.22
Unwarping the Wigner distribution of the warped signal . . . . . . . . . .
442
7.23
Modiﬁed S-transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
449
7.24
EMD example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
462
8.1
Amplitude modulation of sine signal . . . . . . . . . . . . . . . . . . . . . . . . .
498
8.2
Amplitude modulation of audio sine signal . . . . . . . . . . . . . . . . . . . .
499
8.3
Amplitude modulation of sine signal and spectra . . . . . . . . . . . . . . . .
500
8.4
Demodulation of a DSB signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
502
xxxvi
Listings

8.5
Suppressed carrier amplitude modulation of sine signal . . . . . . . . . . .
504
8.6
SSB amplitude modulation of sine signal . . . . . . . . . . . . . . . . . . . . . .
506
8.7
Frequency modulation of sine signal . . . . . . . . . . . . . . . . . . . . . . . . .
509
8.8
Frequency modulation of audio sine signal. . . . . . . . . . . . . . . . . . . . .
510
8.9
Spectra of frequency modulated signals . . . . . . . . . . . . . . . . . . . . . . .
511
8.10
Pieces of 4-PSK . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
514
8.11
A message via 4-PSK. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
516
8.12
Amplitude modulation of sine signal, using modulate() . . . . . . . . . . .
518
8.13
Modulation of pulses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
520
8.14
Demodulation of pulses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
521
8.15
AM and noise in the communication channel. . . . . . . . . . . . . . . . . . .
526
8.16
PWM and noise in the communication channel . . . . . . . . . . . . . . . . .
527
8.17
PTM and noise in the communication channel . . . . . . . . . . . . . . . . . .
528
8.18
Analyze AM modulation with cepstrum . . . . . . . . . . . . . . . . . . . . . . .
530
B.1
Example of Markov Chain (weather prediction) . . . . . . . . . . . . . . . . .
557
B.2
Example of HMM (synthetic speech) . . . . . . . . . . . . . . . . . . . . . . . . .
559
B.3
Comparison of group delay of 5 ﬁlters . . . . . . . . . . . . . . . . . . . . . . . .
561
B.4
Recovering a sinusoid buried in noise . . . . . . . . . . . . . . . . . . . . . . . .
562
B.5
Adding and recovering experiment . . . . . . . . . . . . . . . . . . . . . . . . . . .
564
B.6
Comparison of pole-zero maps of the 4 digital ﬁlters . . . . . . . . . . . . .
567
B.7
Adding and recovering with ﬁltﬁlt experiment . . . . . . . . . . . . . . . . . .
569
B.8
Wigner distribution of a 2-sine signal. . . . . . . . . . . . . . . . . . . . . . . . .
570
B.9
Wigner distribution of a chirp signal . . . . . . . . . . . . . . . . . . . . . . . . .
571
B.10
WD of chirp signal, with no interference . . . . . . . . . . . . . . . . . . . . . .
574
B.11
Fractional Fourier transform (chirp signal) . . . . . . . . . . . . . . . . . . . . .
575
B.12
Wigner distribution of Gaussian chirplet. . . . . . . . . . . . . . . . . . . . . . .
577
B.13
Wigner distribution of a modulated signal . . . . . . . . . . . . . . . . . . . . .
578
B.14
Reassigned STFT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
580
B.15
Short Time Fan-Chirp transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
581
B.16
EMD and Hilbert Spectrum example . . . . . . . . . . . . . . . . . . . . . . . . .
585
B.17
Fractional Fourier transform of a rectangle signal. . . . . . . . . . . . . . . .
588
B.18
Wigner analysis of FFR of rectangle signal . . . . . . . . . . . . . . . . . . . .
590
B.19
Filtered (mask) WD of Bat signal. . . . . . . . . . . . . . . . . . . . . . . . . . . .
591
B.20
Filtered (mask) WD of Bird signal . . . . . . . . . . . . . . . . . . . . . . . . . . .
593
B.21
Signal analysis by Morlet continuous wavelet transform . . . . . . . . . .
594
B.22
Signal analysis by Morlet continuous wavelet transform . . . . . . . . . .
596
B.23
Signal analysis by Morlet continuous wavelet transform . . . . . . . . . .
598
B.24
Short Time Fan-Chirp transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
600
B.25
Short Time Fan-Chirp transform. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
602
B.26
Modiﬁed S-transform . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
604
B.27
Modiﬁed S-transform of Whale signal . . . . . . . . . . . . . . . . . . . . . . . .
607
B.28
Modiﬁed S-transform (Earthquake). . . . . . . . . . . . . . . . . . . . . . . . . . .
608
B.29
Inversion of Modiﬁed S-transform (Earthquake). . . . . . . . . . . . . . . . .
610
B.30
Pulse modulations of sine signal. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
613
Listings
xxxvii

Part I
Signals and Data

Chapter 1
Periodic Signals
1.1
Introduction
This chapter is devoted to initial fundamental concepts of signal processing. Periodic
signals provide a convenient context for this purpose.
The chapter is also an introduction to the Signal Processing Toolbox. A number
of ﬁgures and MATLAB programs have been included to illustrate the concepts and
functions being introduced. This methodology is continued in the next chapters of
the book.
Some examples include sound output, which contributes for a more intuitive study
of periodic signals.
Of course, a main reference for this chapter and the rest of the book is the Docu-
mentation that accompanies the MATLAB Signal Processing Toolbox. Other more
speciﬁc references are indicated when opportune in the different sections of the
chapter.
The most important contents to be considered in the next pages refers to the Fourier
transform and to sampling criteria. Both are introduced by way of examples. The
Appendix A of the book contains a more formal exposition of these topics, including
bibliography.
The two ﬁnal sections of this chapter include Internet addresses of interesting
resources, and a list of literature references. The book [3] provides a convenient
background for this chapter.
1.2
Signal Representation
Suppose you have a signal generator so you have the capability of generating a square
wave with 1 Hz frequency. You adjust the generator, so the low level of the signal is
0 v. and the high level is 1 v. Figure 1.1 shows 3s of such signal:
The signal in Fig.1.1 repeats three consecutive times the same pattern. It is a
periodic signal with period T = 1 s.
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_1
3

4
1
Periodic Signals
Fig. 1.1 A square signal
0
0.5
1
1.5
2
2.5
3
-0.5
0
0.5
1
1.5
seconds
Suppose you also have a computer with a data acquisition channel, so it is possible
to get samples of the 1 Hz square signal. For instance, let us take 10 samples per
second. In this case, you get from 3s of signal, a data set like the following:
A = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0];
From this data set, with 30 numbers, it is possible to reproduce the signal, provided
information about time between samples is given. In order to have such information,
another parallel set of 30 sampling times could be recorded when you get the data;
or just keep in memory or paper what the sampling frequency was (or the total time
of signal that was sampled).
Figure 1.2 plots the data set A versus 30 equally spaced time intervals along the
3s.
The MATLAB code to generate Fig.1.2 is the following:
Program 1.1 Square signal
% Square signal
A=[1,1,1,1,1,0,0,0,0,0,1,1,1,1,1,0,0,0,0,0,...
1,1,1,1,1,0,0,0,0,0];
fs=10; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set (30 values)
plot(t,A,'*'); %plots figure
axis([0 3 -0.5 1.5]);
xlabel('sec.'); title('square wave samples');
MATLAB handles signals using data sets (vectors). Usually the Signal Processing
Toolbox routines ask for the signal samples data set, and for information about the
sampling frequency.

1.3 Generation of Periodic Signals
5
Fig. 1.2 The sampled
square signal
0
0.5
1
1.5
2
2.5
3
-0.5
0
0.5
1
1.5
seconds
1.3
Generation of Periodic Signals
Let us use the periodic signals included in the Signal Processing Toolbox.
1.3.1
Sinusoidal
The following MATLAB program generates a sinusoidal signal with 1s period. It is
based on the use of the sin() function.
Program 1.2 Sine signal
% Sine signal
fy=1;
%signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
fs=60; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set, 180 values
y=sin(wy*t); %signal data set
plot(t,y,'k'); %plots figure
axis([0 3 -1.5 1.5]);
xlabel('seconds'); title('sine signal');
Figure 1.3 shows the results of the Program 1.2, it is a 180 points data set covering
3s of the 1 Hz signal.
In the next program, both sine and cosine signals, with the same 1 Hz frequency,
are generated.

6
1
Periodic Signals
Fig. 1.3 Sinusoidal signal
0
0.5
1
1.5
2
2.5
3
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 1.4 Sine (solid) and
cosine (dashed) signals
0
0.5
1
1.5
2
2.5
3
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Program 1.3 Sine and cosine signals
% Sine and cosine signals
fy=1;
%signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
fs=60; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set
ys=sin(wy*t); %signal data set
plot(t,ys,'k'); hold on; %plots figure
axis([0 3 -1.5 1.5]);
xlabel('seconds');
yc=cos(wy*t); %signal data set
plot(t,yc,'--k'); %plots figure

1.3 Generation of Periodic Signals
7
axis([0 3 -1.5 1.5]);
xlabel('seconds'); title('sine (solid) and cosine (dashed)');
Figure 1.4 shows the results of Program 1.3, two superimposed (using hold on)
signals. Notice the 90◦phase difference between both signals.
As shall be seen shortly, periodic signals can be decomposed into a sum of sine
and cosine signals.
1.3.2
Square
The following MATLAB program generates 0.03s of a square signal with 0.01s
period (100Hz frequency). The signal generation is based on the use of the square()
function. A feature of this function is that it is possible to specify the duty cycle (the
percent of the period in which the signal is positive), in order to generate rectangular
signals.
Program 1.4 Square signal
% Square signal
fy=100; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
duy=0.03; %signal duration in seconds
fs=20000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(duy-tiv); %time intervals set
y=square(wy*t); %signal data set
plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('square signal');
Figure 1.5 shows the results of Program 1.4.
Fig. 1.5 Square signal
0
0.005
0.01
0.015
0.02
0.025
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds

8
1
Periodic Signals
1.3.3
Sawtooth
The following MATLAB program generates 0.03s of a sawtooth signal with 0.01 s
period (100Hz frequency). The signal generation is based on the use of the sawtooth()
function.
Program 1.5 Sawtooth signal
% Sawtooth signal
fy=100; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
duy=0.03; %signal duration in seconds
fs=20000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(duy-tiv); %time intervals set
y=sawtooth(wy*t); %signal data set
plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('sawtooth signal');
Figure 1.6 shows the results of Program 1.5.
The shape of the sawtooth signal can be modiﬁed using a feature (a width speciﬁ-
cation) of this function. Figure 1.7 shows four examples. Look into the sentences with
the sawtooth() function in the Program 1.6, to see the width speciﬁcations applied in
the examples.
Fig. 1.6 Sawtooth signal
0
0.005
0.01
0.015
0.02
0.025
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds

1.3 Generation of Periodic Signals
9
0
0.01
0.02
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
0
0.01
0.02
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
0
0.01
0.02
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
0
0.01
0.02
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 1.7 Different sawtooth signals
Program 1.6 Sawtooth signals
% Sawtooth signals
fy=100; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
duy=0.03; %signal duration in seconds
fs=20000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(duy-tiv); %time intervals set
y=sawtooth(wy*t,0.1); %signal data set (width 0.1)
subplot(2,2,1); plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('sawtooth signal');
y=sawtooth(wy*t,0.3); %signal data set (width 0.3)
subplot(2,2,2); plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('sawtooth signal');
y=sawtooth(wy*t,0.5); %signal data set (width 0.5)
subplot(2,2,3); plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('sawtooth signal');
y=sawtooth(wy*t,0.9); %signal data set (width 0.9)
subplot(2,2,4); plot(t,y,'k'); %plots figure
axis([0 duy -1.5 1.5]);
xlabel('seconds'); title('sawtooth signal');

10
1
Periodic Signals
1.4
Hearing the Signals
Many computers have loudspeakers, or a connector for headsets. Using MATLAB it
is possible to hear the signals under study, provided the frequencies of these signals
are in the audio range. Frequencies in the 100–1000Hz range give comfortable sound.
The Program 1.7 generates a 300Hz sinusoidal signal along 5s. There is a sen-
tence with the function sound() that sends this signal, with sufﬁcient power, to the
loudspeaker.
Program 1.7 Sine audio signal
% Sine signal sound
fy=300; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
fs=6000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(5-tiv); %time intervals set (5 seconds)
y=sin(wy*t); %signal data set
sound(y,fs); %sound
t=0:tiv:(0.01-tiv); %time intervals set (0.01 second)
y=sin(wy*t); %signal data set
plot(t,y,'k'); %plots figure
axis([0 0.01 -1.5 1.5]);
xlabel('seconds'); title('sine signal');
Figure 1.8 shows the sinusoidal audio signal.
0
0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009 0.01
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 1.8 The sinusoidal audio signal

1.5 Operations with Signals
11
1.5
Operations with Signals
We can add, subtract, multiply, etc. signals. It is interesting to deal, in this section,
with some basic examples.
1.5.1
Adding Signals
In Program 1.8 we add three sinusoidal signals. This is done in the sentence with the
following expression:
y = 0.64 sin(ωyt) + 0.21 sin(3ωyt) + 0.12 sin(5ωyt)
(1.1)
Notice that the amplitude and frequency of the three sinusoids are the following:
signal
amplitude frequency
1st harmonic
0.64
ω (300 Hz)
3rd harmonic
0.21
3 ω (900 Hz)
5th harmonic
0.12
5 ω (1500 Hz)
The Program 1.8 is as follows:
Program 1.8 Sum of sines signal
% Sum of sines signal
fy=300; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
fs=6000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(5-tiv); %time intervals set (5 seconds)
%signal data set:
y=0.64*sin(wy*t)+0.21*sin(3*wy*t)+0.12*sin(5*wy*t);
sound(y,fs); %sound
t=0:tiv:(0.01-tiv); %time intervals set (0.01 second)
%signal data set:
y=0.6*sin(wy*t)+0.3*sin(3*wy*t)+0.2*sin(5*wy*t);
plot(t,y,'k');
%plots figure
axis([0 0.01 -1.5 1.5]);
xlabel('seconds'); title('sum of sines signal');
Figure 1.9 shows the result of adding the three sinusoidal signal. It is a periodic
signal with frequency 300Hz. It looks similar to a square signal.
Fourier series are sums of sinusoidal harmonics. In the case of Program 1.8 we
are taking the ﬁrst three non-zero harmonics of the series corresponding to a square

12
1
Periodic Signals
0
0.001 0.002 0.003 0.004 0.005 0.006 0.007 0.008 0.009
0.01
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 1.9 Sum of three sinusoidal signals
signal. By adding more non-zero harmonics we can improve the approximation to
the square signal.
The Program 1.8 includes the sound() function so it is possible to hear the 300Hz
square-like signal and compare it with the smoother sound of the pure 300Hz sinu-
soidal signal (Program 1.7).
The reader is invited to obtain other waveforms by changing the expression of the
signal y in Program 1.8. This is one of the typical activities when using a music syn-
thesizer [6]. A few web sites on sound synthesis have been included in the Resources
section at the end of the chapter.
1.5.2
Multiplication
Figure 1.10 shows the result of multiplying two sinusoidal signals. One of the signals
has 70Hz frequency and the other has 2 Hz frequency. Both signals have amplitude 1.
The result obtained is a modulated signal. Chapter 8 of this book is devoted to
modulation, and a similar example will be seen in more detail.
Program 1.9 has been used to generate the Fig.1.10. It includes a sentence with
a sound() function. When hearing the modulated signal, notice the tremolo effect
caused by the 2Hz signal. The Internet address of a tutorial on sound amplitude
modulation has been included in the Resources section.

1.5 Operations with Signals
13
Fig. 1.10 Multiplication of
two sinusoidal signals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Program 1.9 Multiplication of sines
% Multiplication of sines signal
fx=70;
%signal frequency in Hz
wx=2*pi*fx; %signal frequency in rad/s
fz=2; %signal frequency in Hz
wz=2*pi*fz; %signal frequency in rad/s
fs=6000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(8-tiv); %time intervals set (8 seconds)
y=sin(wx*t).*sin(wz*t); %signal data set
sound(y,fs); %sound
t=0:tiv:(1-tiv); %time intervals set (1 second)
y=sin(wx*t).*sin(wz*t); %signal data set
plot(t,y,'k'); %plots figure
axis([0 1 -1.5 1.5]);
xlabel('seconds'); title('multiplication of sines signal');
1.6
Harmonics. Fourier
Periodic signals can be decomposed into sums of sine and cosine signals, according
with the following expression, which is a Fourier series:
y(t) = a0 +
∞

n=1
an cos (n · w0 t) +
∞

n=1
bn sin(n · w0 t)
(1.2)
where ω0 is the frequency (rad/s) of the periodic signal.

14
1
Periodic Signals
Fig. 1.11 Example of odd
signal (sawtooth signal, 3
periods)
0
0.5
1
1.5
2
2.5
3
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds
Several types of periodic signals can be distinguished, considering symmetries.
Let us explore some of these types.
1.6.1
Odd Signals
Odd signals have the following symmetry:
y(−t) = −y(t)
(1.3)
The sine signal is an odd signal. All other odd signals can be obtained with:
y(t) =
∞

n=1
bn sin (n · w0 t)
(1.4)
Let us consider the example shown in Fig.1.11. It is a sawtooth signal (which is an
odd signal).
Figure 1.11 has been obtained with the Program 1.10 (very similar to
Program 1.5)
Program 1.10 Sawtooth signal to be analyzed
%sawtooth signal to be analyzed
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz

1.6 Harmonics. Fourier
15
tiv=1/fs; %time interval between samples;
t=0:tiv:((3*Ty)-tiv); %time intervals set (3 periods)
y3=sawtooth(wy*t); %signal data set
plot(t,y3'k');
xlabel('seconds'); title('sawtooth signal (3 periods)');
By using the function fft() (fast Fourier transform) we can obtain the values of
coefﬁcients bi in Eq. (1.4). Let us apply this function for the y(t) sawtooth signal.
Program 1.11 obtains the ﬁrst ten coefﬁcients. Notice that it is enough to pass to fft()
one signal period.
Program 1.11 Fourier transform of sawtooth signal
%Fourier Transform of sawtooth signal
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(Ty-tiv); %time intervals set
y=sawtooth(wy*t); %signal data set
fou=fft(y,fs); %Fourier Transform (set of complex numbers)
hmag=imag(fou); bh=hmag/N; %get set of harmonic amplitudes
stem(0:9,bh(1:10)); %plot of first 10 harmonics
axis([0 10 0 1]);
xlabel('Hz'); title('sawtooth signal harmonics');
Figure 1.12 shows the values of the coefﬁcients bi versus frequencies 0, ω0, 2ω0,
3ω0, etc. in Hz.
Fig. 1.12 Amplitude of the
ﬁrst ten harmonics of the
sawtooth signal
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Hz

16
1
Periodic Signals
Fig. 1.13 Example of even
signal (rectiﬁed sine signal, 3
periods)
0
0.5
1
1.5
2
2.5
3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
seconds
1.6.2
Even Signals
Even signals have the following symmetry:
y(−t) = y(t)
(1.5)
The cosine signal is an even signal. All other even signals can be obtained with:
y(t) = a0 +
∞

n=1
an cos (n · w0 t)
(1.6)
where a0 is the average value of the signal.
Consider the signal shown in Fig.1.13. It is a rectiﬁed sine signal (which is an
even signal). This signal is found, for instance, in AC to DC electronics conversion
for power supply.
The signal in Fig. 1.13 has been obtained with the Program 1.12.
Program 1.12 Rectiﬁed signal to be analyzed
%rectified sine signal to be analyzed
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:((3*Ty)-tiv); %time intervals set (3 periods)
y3=abs(sin(wy*t)); %signal data set
plot(t,y3'k');
xlabel('seconds'); title('rectified sine signal (3 periods)');

1.6 Harmonics. Fourier
17
Now, let us apply fft() to obtain the an coefﬁcients in Eq.(1.6). This is done with
the Program 1.13.
Program 1.13 Fourier transform of rectiﬁed signal
%Fourier Transfom of rectified sine signal
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(Ty-tiv); %time intervals set
y=abs(sin(wy*t)); %signal data set
fou=fft(y,fs); %Fourier Transform (set of complex numbers)
hmag=real(fou); ah=hmag/N; %get set of harmonic amplitudes
stem(0:9,ah(1:10)); hold on; %plot of first 10 harmonics
plot([0 10],[0 0],'k');
xlabel('Hz'); title('rectified sine signal harmonics');
Figure 1.14 shows the values of the coefﬁcients ai versus frequencies 0, ω0, 2ω0,
3ω0, etc. in Hz.
Since the rectiﬁed sine has a non-zero average value, Fig. 1.14 shows a non-zero
value of a0.
Fig. 1.14 Amplitude of the
ﬁrst ten harmonics of the
rectiﬁed sine signal
0
1
2
3
4
5
6
7
8
9
10
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Hz

18
1
Periodic Signals
1.6.3
Half Wave Symmetry
A signal has half-wave symmetry if:
y

t + T0
2

= −y(t)
(1.7)
For this kind of signals, the harmonics for frequencies 0, 2ω0, 4ω0, 8ω0, etc. have
zero amplitude.
Consider the signal shown in Fig.1.15. It is a triangular signal, which has half-
wave symmetry. It is also an even signal (it will have only cosine harmonics).
The signal in Fig.1.15 has been obtained with the Program 1.14.
Program 1.14 Triangular signal to be analyzed
%triangular signal to be analyzed
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:((3*Ty)-tiv); %time intervals set (3 periods)
y3=-sawtooth(wy*t,0.5); %signal data set
plot(t,y3'k');
xlabel('seconds'); title('triangular signal (3 periods)');
Let us apply fft() to obtain the an coefﬁcients corresponding to the triangular
signal. This is done with the Program 1.15.
Fig. 1.15 Example of signal
with half-wave symmetry
(triangular signal, 3 periods)
0
0.5
1
1.5
2
2.5
3
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds

1.6 Harmonics. Fourier
19
Fig. 1.16 Amplitude of the
ﬁrst ten harmonics of the
triangular signal
0
1
2
3
4
5
6
7
8
9
10
-0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Hz
Program 1.15 Fourier transform of triangular signal
%Fourier Transfom of triangular signal
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(Ty-tiv); %time intervals set
y=-sawtooth(wy*t,0.5); %signal data set
fou=fft(y,fs); %Fourier Transform (set of complex numbers)
hmag=real(fou); ah=hmag/N; %get set of harmonic amplitudes
stem(0:9,ah(1:10)); hold on; %plot of first 10 harmonics
plot([0 10],[0 0],'k');
xlabel('Hz'); title('triangular signal harmonics');
Figure 1.16 shows the values of the coefﬁcients ai versus frequencies 0, ω0, 2ω0,
3ω0, etc. in Hz.
Notice that amplitudes of harmonics with frequencies 0, 2ω0, 4ω0, 6ω0, etc. are
zero.
1.6.4
Pulse Train
Consider the case of a pulse train signal, as shown in Fig.1.17.
The signal in Fig.1.17 has been obtained with the Program 1.16.

20
1
Periodic Signals
Fig. 1.17 A pulse train
signal (3 periods)
0
0.5
1
1.5
2
2.5
3
0
0.2
0.4
0.6
0.8
1
seconds
Program 1.16 Pulse train signal
%pulse train signal to be analyzed
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:((3*Ty)-tiv); %time intervals set (3 periods)
W=20;
%signal first part:
y1=zeros(256,1); y1(1:W)=1; y1((256-W):256)=1;
yt=cat(1,y1,y1,y1); %signal to be plotted
plot(t,yt'k');
xlabel('seconds'); title('pulse train signal (3 periods)');
The width of the high level parts of the signal can be modiﬁed by changing the
value of W in Program 1.16.
Again, let us apply fft() to obtain the an coefﬁcients corresponding to the pulse
train signal, which is an even signal (Program 1.17).
Program 1.17 Fourier transform of pulse train signal
%Fourier Transform of pulse train signal
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256; W=20;
fs=N*fy; %sampling frequency in Hz
y1=zeros(256,1); y1(1:W)=1; y1((256-W):256)=1;
%signal period
fou=fft(y1,fs); %Fourier Transform (set of complex numbers)
hmag=real(fou); ah=hmag/N; %get set of harmonic amplitudes

1.6 Harmonics. Fourier
21
stem(0:49,ah(1:50)); hold on; %plot of first 50 harmonics
plot([0 50],[0 0],'k');
xlabel('Hz'); title('pulse train signal harmonics');
Figure 1.18 shows the values of the coefﬁcients ai versus frequencies 0, ω0, 2ω0,
3ω0, etc. in Hz.
For a certain value nc, it will happen that all coefﬁcients an with n>nc will have
an absolute value less than a0/100. Let us take this value nc as a practical limit of
the number of harmonics that deserve to be considered. Also, let us take wnc as the
maximum frequency of interest for the study of the signal.
If you decrease the value of W in Program 1.17, making the pulses to narrow, you
will notice an increase of nc and of wnc. From the point of view of data transmission
it means that the narrower the pulses to be transmitted through a channel (for instance
a wire), the larger the channel bandwidth must be.
Look at the data points in Fig.1.18. They draw a peculiar curve that we shall meet
again in other parts of the book. It corresponds to the sinc() function:
sinc(x) = sin(x)
x
(1.8)
Figure 1.19 depicts part of the sinc(t) function. It is usual to represent this function
with respect to negative and positive values of time.
Fig. 1.18 Amplitude of the
ﬁrst 50 harmonics of the
pulse train signal
0
5
10
15
20
25
30
35
40
45
50
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Hz

22
1
Periodic Signals
Fig. 1.19 The sinc(t)
function
-6
-4
-2
0
2
4
6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
seconds
Figure 1.19 has been obtained with Program 1.18.
Program 1.18 Sinc function
%sinc function
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
Ty=1/fy; %signal period in seconds
N=256;
fs=N*fy; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
%time intervals set (12 periods):
t=-((6*Ty)-tiv):tiv:((6*Ty)-tiv);
y=sinc(t); %signal data set
plot(t,y'k'); hold on;
plot([0 0],[-0.4 1.2],'k');
xlabel('seconds'); title('sinc function');
1.7
Sampling Frequency
Notice than all signals in Sect.1.3. have been generated with a sampling frequency
that is higher enough with respect to the signal frequency. Let us see what happens
lowering the sampling frequency. Next program generates a 1Hz sinusoidal signal
using a 7Hz sampling frequency.

1.7 Sampling Frequency
23
Program 1.19 Sine signal and low sampling frequency
% Sine signal and low sampling frequency
fy=1; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
fs=7; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set
y=sin(wy*t); %signal data set
plot(t,y,'-kd'); %plots figure
axis([0 3 -1.5 1.5]);
xlabel('seconds'); title('sine signal');
Figure 1.20 shows the results of Program 1.19. Notice how the signal looks dis-
torted with respect to a pure sine.
It is interesting to play with the Program 1.19, changing the sampling frequency
and looking at the results. In particular, there may appear peculiar effects if the
sampling frequency is equal or lower than two times the signal frequency.
For example let us consider the case of the sampling frequency and the signal
frequency being equal. In this case the signal data points draw a horizontal line. It
seems that there is a simple constant (DC) signal. This can happen, and has happened,
in reality: you measure at a certain sampling frequency, you believe from sampled
data that there is a constant signal, but it is not true (what you see on the computer
screen is an impostor, a signal “alias”) [2, 5].
So it is important to determine the frequency of the signal you are sampling.
Fig. 1.20 Effect of a low
sampling frequency
(sampled sine signal)
0
0.5
1
1.5
2
2.5
3
-1.5
-1
-0.5
0
0.5
1
1.5
seconds

24
1
Periodic Signals
0
0.5
1
1.5
2
2.5
3
-1.5
-1
-0.5
0
0.5
1
1.5
fs=100
0
0.5
1
1.5
2
2.5
3
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
fs=4
Fig. 1.21 Aliasing example (3 Hz sine signal)
Figure 1.21 shows what happens when a 3Hz sinusoidal signal is sampled at 4Hz
sampling frequency. This ﬁgure is obtained with Program 1.20.
Program 1.20 Sine signal and aliasing
% Sine signal and aliasing
fy=3; %signal frequency in Hz
wy=2*pi*fy; %signal frequency in rad/s
% good sampling frequency
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set
y=sin(wy*t); %signal data set
subplot(2,1,1); plot(t,y,'k'); %plots figure
axis([0 3 -1.5 1.5]);
title('3Hz sine signal');
ylabel('fs=100');
% too slow sampling frequency
fs=4; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(3-tiv); %time intervals set
y=sin(wy*t); %signal data set
subplot(2,1,2); plot(t,y,'-kd'); %plots figure
axis([0 3 -1.5 1.5]);
xlabel('seconds');
ylabel('fs=4');

1.7 Sampling Frequency
25
Figure 1.21 shows on top the 3Hz sinusoidal signal. The plot below shows the
data obtained sampling the sinusoidal signal: by joining the data points it seems that
the data obtained correspond to a triangular signal. Again this is a mistake, we see
on screen an “alias” of the real sinusoidal signal.
In the case of sinusoidal signals, the sampling frequency should be higher than
two times the signal frequency [1, 7]. If this is the case of a non-sinusoidal periodic
signal, the highest frequency of interest wnc for this signal must be determined (see
the case of the pulse train in Sect. 1.6.4.) and the sampling frequency should be
higher than two times this frequency (Shannon’s theorem) [4].
1.8
Suggested Experiments and Exercises
In this brief section some signal processing experiments/exercises are suggested.
(a) Synthesizer
Based on Fourier series, a kind of simple synthesizer can be developed. The GUI for
this synthesizer could be a series of sliders, corresponding to even and odd harmonics.
It would be interesting to use as reference the frequencies of piano notes.
An opportune experiment would be to try only even harmonics, or only odd
harmonics, and hear the result.
There are databases of music instrument sounds, available from Internet, that
could be analysed using fft( ). Once the analysis was done, it would be interesting to
try to reproduce the same harmonics contents using the synthesizer. We are not yet
speaking of sound envelopes, but some curiosity about would be expected to arise.
(b) Saturation
Imagine you inject a sine signal into a device with saturation. For instance, the case
of connecting a humble, small loudspeaker to a large signal, which normally results
in much distortion and not very much sound.
What happens when there is saturation? The output will not be a pure sinusoid,
some odd harmonics appear. A basic exercise would be to analyse with fft( ) the
saturated signal.
Suppose the saturated signal is applied to a resistor. It would be interesting to
evaluate, with some graphics, the proportion of power that goes to distortion.
Saturation is a major problem in industrial scenarios, and in communication sys-
tems.
(c) Aliasing
Although one example has been already considered in the chapter, it would be con-
venient to do some more experiments about aliasing. For instance, to conduct a
systematic study of sampling frequencies that leads to aliasing in the form of other
types of signals. The original signal to be sampled could be a sinusoid, squares,
triangles, etc.
A typical aliasing effect happens when you see on a movie wheels that seem to
turn backwards while the car moves forward.
(d) Fourier transform of a signal made of sincs

26
1
Periodic Signals
This is a simple exercise that may lead to interesting observations. The idea is to
create a signal by concatenating a series of sinc functions, and then apply fft( ) to this
signal.
(e) Analysis of a modulated signal
The modulated signals obtained by multiplication have interesting harmonics con-
tents. It would be opportune to apply fft( ) and discuss the result.
One experiment could be to take a sound ﬁle and try to modulate it with a sinusoid,
and see what happens.
1.9
Resources
1.9.1
MATLAB
1.9.1.1
Tutorial Texts
• Partial List of On-line Matlab Tutorials (Duke Univ.):
http://people.duke.edu/~hpgavin/matlab.html/
• Getting Started with MATLAB (MathWorks):
http://www.mathworks.com/help/pdf_doc/matlab/getstart.pdf
• Signal Processing Toolbox Examples (MathWorks):
http://es.mathworks.com/help/signal/examples.html
• Grifﬁths, D.F. (1996). An Introduction to Matlab (U. Dundee.).
http://www.exercicescorriges.com/i_42456.pdf/
• Houcque, D. (2005). Introduction to MATLAB for Engineering Students (North-
western Univ.).
http://www.cse.cuhk.edu.hk/~cslui/CSCI1050/matlab_notes2.pdf/
• Overman, E. (2015). A MATLAB Tutorial (Ohio State Univ.).
https://people.math.osu.edu/overman.2/matlab.pdf
• Kalechman, M. (2009). Practical Matlab Basics for Engineers (City Univ. New
York):
http://read.pudn.com/downloads161/ebook/731301/Practical-Matlab-Basics-for-
Engineers.pdf
• MATLAB Tutorial (GA Tech):
http://users.ece.gatech.edu/bonnie/book/TUTORIAL/tutorial.html/
• YAGTOM: Yet Another Guide TO Matlab (U. Britsh Columbia):
http://ubcmatlabguide.github.io/
• MATLAB Hints and Tricks (Columbia U.):
http://www.ee.columbia.edu/~marios/matlab/matlab_tricks.html
• Some Useful Matlab Tips (K. Murphy, U. British Columbia):
http://www.cs.ubc.ca/~murphyk/Software/matlab_tips.html

1.9 Resources
27
• MATLAB Hints Index (Rensselaer I.):
http://www.rpi.edu/dept/acs/rpinfo/common/Computing/Consulting/Software/
MATLAB/Hints/matlab.html
• High-Quality Graphics in Matlab:
http://dgleich.github.io/hq-matlab-ﬁgs/
• High-Quality Figures in Matlab (Univ. Utah.):
https://www.che.utah.edu/department_documents/Projects_Lab/Projects_Lab_
Handbook/MatlabPlots.pdf
• How to Make Pretty Figures with Matlab (D. Varagnolo):
http://staff.www.ltu.se/~damvar/Matlab/HowToMakePrettyFiguresWithMatlab.
pdf
1.9.1.2
Toolboxes
• Signal Processing Toolbox MATLAB:
http://es.mathworks.com/products/signal/
• Data Visualization Toolbox for MATLAB:
http://www.datatool.com/prod02.htm
• WFDB Toolbox (Physiologic signals):
http://physionet.org/physiotools/matlab/wfdb-app-matlab/
1.9.1.3
MATLAB Code
• Matlab Signal Processing Examples:
http://eleceng.dit.ie/dorran/matlab/resources/MatlabSignalProcessingExamples.
pdf
• Digital Signal Processing Demonstrations (Purdue Univ.):
https://engineering.purdue.edu/VISE/ee438/demos/Demos.html
• Matlab Signal Processing, Plotting and Recording Notes,
J. Vignola (Catholic Univ. America):
http://faculty.cua.edu/vignola/Vignola_CUA/ME_560_ﬁles/Matlabsignal.proce
ssing-plotingand-recording-knotes.pdf
• Lecture Support Material and Code (Cardiff Univ.):
http://www.cs.cf.ac.uk/Dave/CM0268/Lecture_Examples/
• Audio Processing in Matlab (McGill Univ.):
http://www.music.mcgill.ca/~gary/307/week1/matlab.html
• Sound Processing in Matlab (U. Dayton):
http://homepages.udayton.edu/~hardierc/ece203/sound.htm
• Matlab Implementation of some Reverberation Algorithms (U. Zaragoza):
http://www.cps.unizar.es/~fbeltran/matlab_ﬁles.html
• Sound Processing in Matlab:
http://alex.bikfalvi.com/research/advanced_matlab_boxplot/

28
1
Periodic Signals
1.9.2
Web Sites
• Educational Matlab GUIs (GA Tech):
http://users.ece.gatech.edu/mcclella/matlabGUIs/
• Signal Processing FIRST (Book site, demos):
http://www.rose-hulman.edu/DSPFirst/visible3/contents/index.htm/
• OnLine Demos (Book site):
http://users.ece.gatech.edu/bonnie/book/applets.html
• Signal Processing Tools (U. Maryland):
http://terpconnect.umd.edu/~toh/spectrum/SignalProcessingTools.html
• Music Analysis and Synthesis (M.R. Petersen):
http://amath.colorado.edu/pub/matlab/music/
• Music Signal Processing (J. Fessler):
http://web.eecs.umich.edu/~fessler/course/100/
• Sound based on sinewaves (D. Ellis):
http://labrosa.ee.columbia.edu/matlab/sinemodel/
• Tutorial on sound amplitude modulation:
https://docs.cycling74.com/max5/tutorials/msp-tut/mspchapter09.html
References
1. P. Cheung, Sampling & Discrete Signals. Lecture presentation, Imperial College Lon-
don
(2011).
http://www.ee.ic.ac.uk/pcheung/teaching/ee2_signals/Lecture2013-Sampling&
discrete-signals.pdf
2. M. Handley, Audio Basics. Lecture presentation, University College London (2002). www0.cs.
ucl.ac.uk/teaching/Z24/02-audio.pdf
3. J.H. McClellan, R.W. Schafer, M.A. Yoder, Signal Processing First (Prentice Hall, Upper Saddle
River, 2003)
4. T.K. Moon, Sampling. Lecture Notes, UtahState Univ. (2006). http://ocw.usu.edu/Electrical_
and_Computer_Engineering/Signals_and_Systems/lecture6.pdf
5. B.A.
Olshausen.
Aliasing.
Lecture
Notes,
UC
Berkeley
(2000).
redwood.berkeley.edu/bruno/npb261/aliasing.pdf
6. H.D. Pﬁster. Fourier Series Synthesizer. Lecture Notes, Duke Univ. (2013). http://pﬁster.ee.
duke.edu/courses/ecen314/project1.pdf
7. J. Schesser. Sampling and Aliasing. Lecture presentation, New Jersey Institute of Technology
(2009). https://web.njit.edu/~joelsd/Fundamentals/coursework/BME310computingcw6.pdf

Chapter 2
Statistical Aspects
2.1
Introduction
Practical signal processing frequently involves statistical aspects. If you are using a
sensor to measure say temperature, or light, or pressure, or anything else, you usually
get a signal with noise in it, and then there is a problem of noise removal. The almost
immediate idea could be to apply averaging; however our advice is ﬁrst try to know
better about the noise you have. There are many other contexts where the data you get
suffer from interference, lack of precision, variations along time, etc. For example,
suppose you want to measure the period of a pendulum using a watch: the scientiﬁc
procedure is to repeat the measurements (the values obtained will be different for
each measurement), get a data set, and then statistically process this set.
Let us imagine an example that encloses the main sources of randomness in the
signals to be processed. Suppose that with a radar on the sea coast you want to
determine the position of a ﬂoating body you just detected. There will be three main
problems:
• The radar signals are contaminated with electromagnetic noise.
• There are resolution limitations in the measurements.
• The ﬂoating body is moving because of the waves.
The best you can do in this example is to get a good estimate, in statistical terms.
In this chapter some aspects of probability and statistics, particularly relevant for
signal processing, are selected. First several kinds of probability density distributions
are considered, and then parameters to characterize random signals are introduced,
[101]. The last sections are devoted to matters that, nowadays, are subject of increas-
ing attention, like for instance Bayes’ rule and Markov processes.
In view of these topics it is opportune to consider two random events A and B.
They occur with probabilities P(A) and P(B) respectively. The two random events
are independent if the probability of having A and B is P(A, B) = P(A)P(B). A
typical example is playing with two dice. Another important concept is conditional
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_2
29

30
2
Statistical Aspects
probability.TheexpressionP(A|C)readsastheprobabilityofAgivenC.Forinstance,
the probability of raining in July.
Some of the functions used in this chapter belong to the MATLAB Statistics
Toolbox. This will be indicated with (*ST); for example weibpdf() (*ST) says the
function weibpdf() belongs to the MATLAB Statistics Toolbox.
2.2
Random Signals and Probability Density Distributions
The chief objective of this section is to introduce probability density distributions
and functions, selecting three illustrative cases: the uniform, the normal and the
log-normal distributions. The normal distribution is, in particular, a very important
case.
2.2.1
Basic Concepts
Suppose there is a continuous random variable y(t), the distribution function Fy(v)
of this variable is the following:
Fy(v) = P (y(t) ≤v), −∞< v < ∞
(2.1)
where P() is the probability of.
The probability density function of y(t) is:
Fig. 2.1 A probability
density function
-1.5
-1
-0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
values

2.2 Random Signals and Probability Density Distributions
31
fy(v) = d Fy(v)
d v
(2.2)
A well-known example of probability distribution function, the so-called normal
distribution, has a bell shaped probability density function as shown in Fig.2.1. In
this ﬁgure a shaded zone has been painted corresponding to an interval [a, b] of the
values that y(t) can have. The probability of y(t) value to fall into this interval is
given by the area of the shaded zone.
The abbreviation “PDF” will be used in this book to denote “Probability Density
Function”.
2.2.2
Random Signal with Uniform PD
A random signal taking equiprobable values in successive instants has a uniform
PDF. For example, the sequence of values that would be obtained recording the ﬁnal
angles (0◦.. 360◦) where a roulette wheel stops along several runs in gambling days.
Figure2.2 shows a random signal with uniform PDF. It has been obtained using
the rand() function provided by MATLAB. Notice that the values are from 0 to 1.
This signal can be easily modiﬁed by adding a constant and/or multiplying by a
constant: the result will also have a uniform PDF.
The following program has been used to generate Fig.2.2.
Fig. 2.2 A random signal
with uniform PDF
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
seconds

32
2
Statistical Aspects
Program 2.1 Random signal with uniform PDF
% Random signal with uniform PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (200 values)
N=length(t); %number of data points
y=rand(N,1); %random signal data set
plot(t,y,'-k'); %plots figure
axis([0 2 0 1.2]);
xlabel('seconds');
title('random signal with uniform PDF');
The uniform PDF graphical representation is just a horizontal line between two
limits, as shown in Fig.2.3. This ﬁgure has been generated by Program 2.2, which
uses unifpdf() (*ST) for a uniform PDF between values 0 and 1 (other values can be
speciﬁed in the unifpdf() function parenthesis).
Program 2.2 Uniform PDF
% Uniform PDF
v=0:0.01:1; %values set
ypdf=unifpdf(v,0,1); %uniform PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([-0.5 1.5 0 1.1]);
xlabel('values'); title('uniform PDF');
plot([0 0],[0 1],'--k');
plot([1 1],[0 1],'--k');
An interesting way to check the quality of the MATLAB random variable gener-
ation functions is by plotting a histogram of the signal values along time. For this
Fig. 2.3 Uniform PDF
-0.5
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
values

2.2 Random Signals and Probability Density Distributions
33
Fig. 2.4 Histogram of a
random signal with uniform
PDF
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
0
50
100
150
200
250
values
purpose relatively large signal data sets should be generated. MATLAB provides the
hist() function to obtain the histogram.
Figure2.4 shows the result for the rand() function. As it can be seen in Program
2.3, which has been used to generate the ﬁgure, a signal data set of 10,000 values
has been generated and then classiﬁed into data bins 0.02 wide, from 0 to 1 signal
values. The colour of the bars has been chosen to be cyan for a better view in press.
In general, Fig.2.4 shows a passable approximation to a uniform PDF.
Program 2.3 Histogram of a random signal with uniform PDF
% Histogram of a random signal with uniform PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(100-tiv); %time intervals set (10000 values)
N=length(t); %number of data points
y=rand(N,1); %random signal data set
v=0:0.02:1; %value intervals set
hist(y,v); colormap(cool); %plots histogram
xlabel('values');
title('Histogram of random signal with uniform PDF');
2.2.3
Random Signal with Normal (Gaussian) PDF
The normal PDF has the following mathematical expression:
fy(v) = e−(v−μ)2/2σ2
σ
√
2π
, σ > 0, −∞< μ < ∞, −∞< v < ∞
(2.3)
where μ is the mean and σ is the standard deviation of the random variable.

34
2
Statistical Aspects
Fig. 2.5 A random signal
with normal PDF
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-3
-2
-1
0
1
2
3
seconds
As said before, the normal distribution is very important, both from theoretical and
practical points of view (see [78] for historical details). Most noise and perturbation
models employed in systems or automatic control theory are of Gaussian nature. The
practical reason is provided by the central limit theorem, which in words says that if
a phenomenon is the accumulation of many small additive random effects, it tends
to a normal distribution. For instance, the number of travels per day of an elevator.
Figure2.5 shows a random signal with normal PDF. It has been obtained using the
randn() function provided by MATLAB (notice the slight name difference compared
to rand(), which corresponds to uniform PDF).
The values of the signal in Fig.2.5 have positive and negative values. The ﬁgure
has been obtained with Program 2.4, using the randn() function, which generates a
signal with mean zero and variance one.
Program 2.4 Random signal with normal PDF
% Random signal with normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (200 values)
N=length(t); %number of data points
y=randn(N,1); %random signal data set
plot(t,y,'-k'); %plots figure
axis([0 2 -3 3]);
xlabel('seconds');
title('random signal with normal PDF');
The normal PDF has the shape of a bell. The larger the standard deviation, the
wider is the bell. Figure2.6 shows a PDF example, obtained with Program 2.5, for
a mean zero and a standard deviation one. The program uses the normpdf() (*ST)
function to obtain the PDF.

2.2 Random Signals and Probability Density Distributions
35
Fig. 2.6 Normal PDF
-3
-2
-1
0
1
2
3
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
values
Fig. 2.7 Histogram of a
random signal with normal
PDF
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
50
100
150
200
250
300
350
400
450
values
Program 2.5 Normal PDF
% Normal PDF
v=-3:0.01:3; %values set
mu=0; sigma=1; %random variable parameters
ypdf=normpdf(v,mu,sigma); %normal PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([-3 3 0 0.5]);
xlabel('values'); title('normal PDF');
Like in the previous case—the uniform PDF—a histogram of the signal generated
by the randn() function has been obtained, with 10,000 signal data values, data bins
0.1 wide. Figure2.7 shows the result: a fairly good approximation to the normal PDF.

36
2
Statistical Aspects
Program 2.6 Histogram of a random signal with normal PDF
% Histogram of a random signal with normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(100-tiv); %time intervals set (10000 values)
N=length(t); %number of data points
y=randn(N,1); %random signal data set
v=-4:0.1:4; %value intervals set
hist(y,v); colormap(cool); %plots histogram
xlabel('values');
title('Histogram of random signal with normal PDF');
2.2.4
Random Signal with Log-Normal PDF
A random variable y is log-normally distributed if log(y) has a normal distribution.
The log-normal PDF has the following mathematical expression:
fy(v) = e−(log(v)−μ)2/2σ2
v σ
√
2π
, σ > 0, −∞< μ < ∞, −∞< v < ∞
(2.4)
where μ is the mean and σ is the standard deviation of the random variable.
The log-normal distribution is related the multiplicative product of many small
independent factors. It is observed for instance in environment, microbiology, human
medicine, social sciences, or economics contexts, [49]. For example, the case of latent
periods (time from infection to ﬁrst symptoms) of infectious diseases.
Figure2.8 shows a random signal with log-normal PDF. It has been obtained, using
the lognrnd() (*ST) function, with the Program 2.7. A mean zero and a standard
deviation one has been speciﬁed inside the parenthesis of lognrnd(); other mean
and standard deviation values can be explored. Notice that signal values are always
positive.
Program 2.7 Random signal with log-normal PDF
% Random signal with log-normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (200 values)
N=length(t); %number of data points
mu=0; sigma=1; %random signal parameters
y=lognrnd(mu,sigma,N,1); %random signal data set
plot(t,y,'-k'); %plots figure
axis([0 2 0 12]);
xlabel('seconds'); title('random signal with log-normal PDF');

2.2 Random Signals and Probability Density Distributions
37
Fig. 2.8 A random signal
with log-normal PDF
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
2
4
6
8
10
12
seconds
Fig. 2.9 Log-normal PDF
0
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
values
Figure2.9 shows a log-normal PDF, as obtained by Program 2.8 using lognpdf()
(*ST). Notice that the PDF is skewed, corresponding to the fact that the signal exhibit
large peaks, as can be seen in Fig.2.8.
Program 2.8 Log-normal PDF
% Log-normal PDF
v=-3:0.01:6; %values set
mu=0; sigma=1; %random variable parameters
ypdf=lognpdf(v,mu,sigma); %log-normal PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 6 0 0.7]);
xlabel('values'); title('log-normal PDF');

38
2
Statistical Aspects
Fig. 2.10 Histogram of a
random signal with
log-normal PDF
0
1
2
3
4
5
6
7
8
0
100
200
300
400
500
600
700
values
Again a histogram of the signal has been obtained, for the case of log-normal
PDF, with 10,000 signal data values and data bins 0.1 wide. Figure2.10 shows the
result, as obtained by Program 2.9.
Program 2.9 Histogram of a random signal with log-normal PDF
% Histogram of a random signal with log-normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(100-tiv); %time intervals set (10000 values)
N=length(t); %number of data points
mu=0; sigma=1; %random signal parameters
y=lognrnd(mu,sigma,N,1); %random signal data set
v=0:0.1:12; %value intervals set
hist(y,v); colormap(cool); %plots histogram
axis([0 8 0 700]);
xlabel('values');
title('Histogram of random signal with log-normal PDF');
2.3
Expectations and Moments
Let us take from descriptive statistics some important concepts concerning the char-
acterization of random signals. Some comments and examples are added for a better
insight.

2.3 Expectations and Moments
39
2.3.1
Expected Values, and Moments
Consider the random variable y, with fy(v) as PDF. The expected value of y is:
E(y) =
∞

−∞
v fy(v) dv
(2.5)
E(y) is said to exist if the integral converges absolutely.
Let g(y) be a function of y, then the expected value of g(y) is:
E(g(y)) =
∞

−∞
g(v) fy(v) dv
(2.6)
The moments about the origin for the variable y are given by:
μ′
′k = E(yk), k = 1, 2, 3 . . .
(2.7)
For k = 1: μ′
1 = μ (μ denotes the mean of y)
The moments about the mean, or central moments, for the variable y are given by:
μ′k = E((y −μ)k), k = 1, 2, 3 . . .
(2.8)
2.3.2
Mean, Variance, Etc.
Figure2.11 shows a skewed PDF where the mean, the median and the mode values are
marked (Program 2.10). In symmetrical PDFs these three values would be coincident.
The mean μ of the variable y is the expected value of y (Eq.2.5). It is also called
the average value of y. Using a mass analogy, it may be regarded as the center of
mass of the distribution.
A median y0 of the variable y is any point that divides the mass of the distribution
into two equal parts, that is:
P (y ≤y0) = 1
2
(2.9)
A point vi such that:
fy(vi) > fy(vi + ε) and fy(vi) > fy(vi −ε)
(2.10)
(where ε is an arbitrarily small positive quantity) is called a mode of y.
A mode is a value of y corresponding to a peak of the PDF. When the PDF has
only one peak, the distribution is said to be unimodal.

40
2
Statistical Aspects
Fig. 2.11 Mean, median and
mode marked on a PDF
0
1
2
3
4
5
6
7
8
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
values
mean 
median 
mode 
In measurement tasks, depending on the PDF of the signal being obtained it would
be advisable to consider the mean, or the median, or the mode or modes, as the value
of interest. In particular, while the mean of a variable y may not exist, the median
will exist.
Program 2.10 A skewed PDF with mean, median and mode
% A skewed PDF with mean, median and mode
v=0:0.01:8; %values set
alpha=2;; %random variable parameter
ypdf=raylpdf(v,alpha); %Rayleigh PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 8 0 0.4]);
xlabel('values'); title('a skewed PDF');
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(20-tiv); %time intervals set (2000 values)
N=length(t); %number of data points
y=raylrnd(alpha,N,1); %random signal data set
mu=mean(y); %mean of y
vo=median(y); %median of y
[pky,pki]=max(ypdf); %peak of the PDF
plot([mu mu],[0 0.33],'--k'); %mean
plot([vo vo],[0 0.37],':k'); %median
plot([v(pki) v(pki)],[0 pky],'-.k'); %mode
The autocorrelation of y(t) is deﬁned as:
R(t1 , t2) = E(y(t1) y(t2))
(2.11)

2.3 Expectations and Moments
41
The autocorrelation is related with the extent of predictability about the future behav-
iour of y(t) taking into account its past.
The value of R(t1, t2) for t1 = t2 is the average power of the signal y(t). It is also
the second moment of y(t) about the origin.
The autocovariance of y(t) is deﬁned as:
C(t1 , t2) = E((y(t1) −μ) (y(t2) −μ))
(2.12)
The value of C(t1, t2) for t1 = t2 is the variance of the signal y(t). It is also the
second moment of y(t) about the mean. The positive square root of the variance is
the standard deviation σ.
The variance is related with how large is the range of values of y(t).
The random signal y(t) is called strict-sense stationary if all its statistical prop-
erties are invariant to a shift of the time origin.
MATLAB offers the following functions: mean(), median(), var(), std() (for stan-
dard deviation).
2.3.3
Transforms
There are several transforms that help to beautifully deduce important results. For
instance the following generating functions:
• Generating function:
g(υ) = E(υy)
(2.13)
• Moment generating function:
Γ (υ) = E(eυ·y)
(2.14)
As in many other contexts, it is convenient to consider the following transforms:
• Laplace transform:
E(e−s·y)
(2.15)
• Fourier transform:
E(e−j υ·y)
(2.16)

42
2
Statistical Aspects
Finally:
• Characteristic function:
ϕy(υ) = E(ej υ·y) =
∞

−∞
ej υ·yfy(v) dv
(2.17)
Notice the relationship of the characteristic function and the Fourier transform of the
PDF.
Ifthecharacteristicfunctionsoftworandomvariablesagree,thenthetwovariables
have the same distribution.
Consider the sum of independent random variables:
z =

yi
(2.18)
Then, the PDF of the sum is the convolution (denoted with an asterisk) of the PDFs
of each variable:
fz = fy1 ∗fy2 ∗. . . ∗fyn
(2.19)
And the characteristic function is the product:
ϕz = ϕy1 · ϕy2 · · · ϕyn
(2.20)
2.3.4
White Noise
White noise is a signal y(t) whose autocorrelation is given by:
R(t1 , t2) = I(t1) δ(t1 −t2)
(2.21)
where δ() is 1 for t1 = t2 and zero elsewhere.
The white noise is important for system identiﬁcation purposes, and for noise
modelling.
2.4
Power Spectra
Up to this point only the time and the values domains have been considered. Now
let us have a look to the frequency dimension. Power spectra allow us to get an idea
of the frequencies contents of random signals.

2.4 Power Spectra
43
2.4.1
Basic Concept
The power spectrum of a stationary random variable y(t) is the Fourier transform of
its autocorrelation:
Sy(ω) =
∞

−∞
R(τ) e−j ωt dτ
(2.22)
The area of Sy(ω) equals the average power of y(t):
E(y2) =
1
2π
∞

−∞
Sy(ω) dω
(2.23)
2.4.2
Example of Power Spectral Density
of a Random Variable
Figure2.12 shows the power spectral density (PSD for short) of a random signal
y(t) with log-normal PDF. The PSD has units of power per unit frequency interval
(for example, if y(t) is in volts, the PSD is in watts per hertz). The PSD curve in
Fig.2.12 is expressed in decibels. This ﬁgure has been generated by the Program
2.11, which uses the pwelch() function to compute and plot the PSD. The name of
the function refers to the method of Welch to obtain the PSD by repeated application
of the Fourier Transform.
The PSD curve in Fig.2.12 is in general ﬂat, except for a clear peak at 0Hz. This
peak is due to the non-zero DC level of the signal y(t). In order to suppress this peak,
the mean of y(t) should be obtained and then subtracted to y(t).
Program 2.11 Power spectral density (PSD) of random signal with log-normal PDF
% Power spectral density (PSD) of random signal
% with log-normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(40.96-tiv); %time intervals set (4096 values)
N=length(t); %number of data points
mu=0; sigma=1; %random signal parameters
y=lognrnd(mu,sigma,N,1); %random signal data set
nfft=256; %length of FFT
window=hanning(256); %window function
numoverlap=128; %number of samples overlap
pwelch(y,window,numoverlap,nfft,fs);
title('PSD of random signal with log-normal PDF');

44
2
Statistical Aspects
Fig. 2.12 PSD of a random
signal with log-normal PDF
0
5
10
15
20
25
30
35
40
45
50
-15
-10
-5
0
5
10
Frequency (Hz)
Power Spectral Density (dB/Hz)
Fig. 2.13 The buried
sinusoidal signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-3
-2
-1
0
1
2
3
seconds
2.4.3
Detecting a Sinusoidal Signal Buried in Noise
Let us consider the following signal:
y(t) = sin (15 · 2 · π) + yn(t)
(2.24)
where yn(t) is a zero-mean random signal with normal PDF.
Figure2.13 shows the 15Hz sinusoidal signal, and Fig.2.14 shows the signal y(t).
Program 2.12 has been used to generate Fig.2.14.

2.4 Power Spectra
45
Fig. 2.14 The sine+noise
signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-3
-2
-1
0
1
2
3
seconds
Program 2.12 The sine+noise signal
% The sine+noise signal
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (200 values)
N=length(t); %number of data points
yr=randn(N,1); %random signal data set
ys=sin(15*2*pi*t); %sinusoidal signal (15 Hz)
y=ys+yr'; %the signal+noise
plot(t,y,'k'); %plots sine+noise
axis([0 2 -3 3]);
xlabel('seconds'); title('sine+noise signal');
Since we fabricated the signal y(t) we already know there is a sinusoidal signal
buried into y(t). However it seems difﬁcult to notice this in Fig.2.14.
We can use the PSD to detect the sinusoidal signal. Figure2.15 shows the result.
Thereisapeakon15Hzthatrevealstheexistenceofaburied15Hzsignal.Figure2.15
has been generated with Program 2.13.
Program 2.13 Power spectral density (PSD) of a signal+noise
% Power spectral density (PSD) of a signal+noise
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(40.96-tiv); %time intervals set (4096 values)
N=length(t); %number of data points
yr=randn(N,1); %random signal data set
ys=sin(15*2*pi*t); %sinusoidal signal (15 Hz)
y=ys+yr'; %the signal+noise
nfft=256; %length of FFT
window=hanning(256); %window function

46
2
Statistical Aspects
Fig. 2.15 PSD of the
sine+noise signal
0
5
10
15
20
25
30
35
40
45
50
-25
-20
-15
-10
-5
0
Frequency (Hz)
Power Spectral Density (dB/Hz)
numoverlap=128; %number of samples overlap
pwelch(y,window,numoverlap,nfft,fs);
title('PSD of a sine+noise signal');
2.4.4
Hearing Random Signals
Like in Sect.1.4, we can use MATLAB to hear random signals. Humans are usually
good to distinguish subtle details in the sounds.
Figure2.16 shows 1s of a random signal with normal PDF, generated by Program
2.14. This program also includes some lines to let us hear 5s of the same signal.
In case you wished to hear any other of the random signals considered in this
chapter, the advice is to conﬁne the signal into an amplitude range −1 < y < 1. It is
good to plot the signal to see what to do: for instance compute its mean and subtract
it to the signal, and then multiply by an opportune constant.
Program 2.14 See and hear a random signal with normal PDF
% See and hear a random signal with normal PDF
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (200 values)
N=length(t); %number of data points
y=randn(N,1); %random signal data set
plot(t,y,'-k'); %plots figure
axis([0 2 -3 3]);
xlabel('seconds');
title('random signal with normal PDF');
fs=6000; %sampling frequency in Hz

2.4 Power Spectra
47
Fig. 2.16 The random
signal with normal PDF to be
heared
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-3
-2
-1
0
1
2
3
seconds
tiv=1/fs; %time interval between samples;
t=0:tiv:(5-tiv); %time intervals set (5 seconds)
N=length(t);
y=randn(N,1); %random signal data set
sound(y,fs); %sound
2.5
More Types of PDFs
There are many types of PDFs. This section is devoted to add some signiﬁcant types
of PDFs to the three types already presented in Sect.2.2, [40, 71, 96].
2.5.1
Distributions Related with the Gamma Function
The gamma function has the following expression:
Γ (α) =
∞

0
vα−1 e−v dv
(2.25)
Figure2.17, obtained with the Program 2.15, depicts the gamma function.

48
2
Statistical Aspects
Fig. 2.17 The gamma
function
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1
2
3
4
5
6
7
8
9
10
values
Program 2.15 Gamma function
% Gamma function
v=0.1:0.01:1;
ygam=gamma(v);
plot(v,ygam,'k'); hold on;
xlabel('values'); title('gamma function');
2.5.1.1
The Gamma PDF
The gamma PDF has the following mathematical expression:
fy(v) =

vα−1 e−v/β
βα Γ (α)
α, β > 0, 0 ≤v ≤∞
0
elsewhere
(2.26)
The gamma distribution corresponds to positively skewed data, such as movement
data and electrical measurements. The parameter α is called the rate parameter, and
β is called the scale parameter. Figure2.18 shows three gamma PDFs, corresponding
to β = 1 and three different values of α. The ﬁgure has been generated with Program
2.16, which uses the gampdf() (*ST) function.
For large values of α the gamma distribution closely approximates a normal PDF.
If y2 has gamma PDF with α = 3/2 and β = 2α, then y has a Maxwell–Boltzmann
PDF.

2.5 More Types of PDFs
49
Fig. 2.18 Gamma-type
PDFs
0
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2
values
α=1 
α=2 
α=4 
Program 2.16 Gamma-type PDFs
% Gamma-type PDFs
v=0:0.01:10; %values set
alpha=1; beta=1; %random variable parameters
ypdf=gampdf(v,alpha,beta); %gamma-type PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 10 0 1.2]);
alpha=2; beta=1; %random variable parameters
ypdf=gampdf(v,alpha,beta); %gamma-type PDF
plot(v,ypdf,'--k'); hold on; %plots figure
alpha=4; beta=1; %random variable parameters
ypdf=gampdf(v,alpha,beta); %gamma-type PDF
plot(v,ypdf,':k'); hold on; %plots figure
xlabel('values'); title('gamma-type PDFs');
2.5.1.2
The Exponential PDF
Taking as reference the gamma PDF, the density function for the special case α = 1
is called the exponential PDF, thus having the following expression:
fy(v) = e−v/β
β
β > 0, v ≥0
(2.27)
This PDF is one of the curves in Fig.2.18. The time intervals between successive
random events follow an exponential distribution; this is the case, for example, of
life-times of electronic devices.

50
2
Statistical Aspects
Fig. 2.19 Example of
chi-square PDF
0
2
4
6
8
10
12
14
16
0
0.05
0.1
0.15
0.2
0.25
0.3
values
2.5.1.3
The Chi-Square PDF
The gamma PDF with parameters α = υ/2 and β = 2 is called a chi-square PDF.
The parameter υ is called the “number of degrees of freedom” associated with the
chi-square random variable. Figure2.19 shows an example of chi-square PDF, for
υ = 3. This ﬁgure has been generated with Program 2.17, which uses the chi2pdf()
(*ST) function.
The sum of υ independent y2 variables with y having normal PDF is a chi-square
signal with υ degrees of freedom.
Program 2.17 Chi-square PDF
% Chi-square PDF
v=-3:0.01:16; %values set
nu=3; %random variable parameter ("degrees of freedom")
ypdf=chi2pdf(v,nu); %chi-square PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 16 0 0.3]);
xlabel('values'); title('chi-square PDF');
2.5.1.4
The Beta PDF
The beta PDF is deﬁned on a [0..1] interval, according with the following mathemat-
ical expression:
fy(v) =

vα−1 (1−v)β−1
B(α,β)
α, β > 0, 0 ≤v ≤1
0
elsewhere
(2.28)

2.5 More Types of PDFs
51
Fig. 2.20 Example of beta
PDF
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
values
where:
B(α, β) = Γ (α) Γ (β)
Γ (α + β)
(2.29)
The parameter α is the ﬁrst shape parameter and β is the second shape parameter.
Figure2.20 shows an example of beta PDF, corresponding to α = 5 and β = 3.
The ﬁgure has been generated with Program 2.18, which uses the betapdf() (*ST)
function.
The beta distribution is used in Bayesian statistics. Events which are constrained
to be within an interval deﬁned by a minimum and a maximum correspond to beta
distributions; for instance time to completion of a task in project management or in
control systems.
For α = β = 1 the beta distribution is identical to the uniform distribution.
y1/(y1 + y2) has a beta PDF if y1 and y2 are independent and have gamma PDF.
Program 2.18 beta PDF
% beta PDF
v=0:0.01:1; %values set
alpha=5; beta=3; %random variable parameters
ypdf=betapdf(v,alpha,beta); %beta PDF
plot(v,ypdf,'k'); %plots figure
axis([0 1 0 2.5]);
xlabel('values'); title('beta PDF');

52
2
Statistical Aspects
Fig. 2.21 Example of
Student’s t PDF
-5
-4
-3
-2
-1
0
1
2
3
4
5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
values
2.5.1.5
The Student’s t PDF
The Student’s t PDF has the following mathematical expression:
fy(v) =
Γ ((υ + 1)/2)
√υ π (υ/2) (1 + v2
υ )(υ+1)/2
(2.30)
Theparameterυ isthenumberof“degreesoffreedom”ofthedistribution.Figure2.21
shows an example of Student’s t PDF, corresponding to υ = 3. The ﬁgure has been
generated with Program 2.19, which uses the tpdf() (*ST) function.
The parameter υ is also the size of random samples of a normal variable; the larger
the degrees of freedom, the closer is the PDF to the normal PDF.
Program 2.19 Student’s PDF
% Student's PDF
v=-5:0.01:5; %values set
nu=3; %random variable parameter ("degrees of freedom")
ypdf=tpdf(v,nu); %Student's PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([-5 5 0 0.4]);
xlabel('values'); title('Student's PDF');

2.5 More Types of PDFs
53
Fig. 2.22 Example of
Weibull PDF
0
0.5
1
1.5
2
2.5
0
0.2
0.4
0.6
0.8
1
1.2
values
2.5.2
Weibull and Rayleigh PDFs
2.5.2.1
The Weibull PDF
The Weibull PDF has the following mathematical expression:
fy(v) = m vm−1e−vm/α
α
α, m > 0, 0 ≤v < ∞
(2.31)
The parameter α is the scale parameter and m is the shape parameter. The Weibull
distribution is frequently used in reliability studies for time to failure modelling,
[2, 103]. If the failure rate decreases over time then m < 1, if it is constant m = 1,
and if it increases m > 1. When m < 1 it suggests that defective items fail early;
when m = 1 the failing comes from random events; when m > 1 there is “wear out”.
Figure2.22 shows an example of Weibull PDF, corresponding to α = 1 and m = 3
The ﬁgure has been generated with Program 2.20, which uses the weibpdf() (*ST)
function.
For α = 1 and m = 1 the Weibull distribution is identical to the exponential dis-
tribution. For m = 3 the Weibull distribution is similar to the normal distribution.
Program 2.20 Weibull PDF
% Weibull PDF
v=0:0.01:2.5; %values set
alpha=1; m=3; %random variable parameters
ypdf=weibpdf(v,alpha,m); %Weibull PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 2.5 0 1.2]);
xlabel('values'); title('Weibull PDF');

54
2
Statistical Aspects
Fig. 2.23 Example of
Rayleigh PDF
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
values
2.5.2.2
The Rayleigh PDF
The Rayleigh PDF is a particular case of the Weibull PDF, having the following
mathematical expression:
fy(v) = ve−v2/2β2
β2
β > 0, 0 ≤v < ∞
(2.32)
If y1 and y2 are independent random signals with normal PDF and equal variance,
then

y2
1 + y2
2
(2.33)
has Rayleigh PDF.
For example the distance of darts from the target in a dart-throwing game has a
Rayleigh distribution. Complex numbers with real and imaginary parts being inde-
pendent random numbers with normal distribution are also examples of Rayleigh dis-
tributions. Figure2.23 shows an example of Rayleigh PDF, corresponding to β = 1.
The ﬁgure has been generated with Program 2.21, which uses the raylpdf() (*ST)
function.
If y has Rayleigh distribution then y2 is chi-square with 2 degrees of freedom.
Rayleigh distributions are considered in image noise modelling and restoration,
[39, 53, 97], wind energy forecasting, [18, 56], reliability studies, [38], etc.
Program 2.21 Rayleigh PDF
% Rayleigh PDF
v=0:0.01:5; %values set
beta=1; %random variable parameter

2.5 More Types of PDFs
55
ypdf=raylpdf(v,beta); %Rayleigh PDF
plot(v,ypdf,'k'); hold on; %plots figure
axis([0 5 0 0.7]);
xlabel('values'); title('Rayleigh PDF');
2.5.3
Multivariate Gaussian PDFs
The multidimensional version of the Gaussian PDF is the following:
f (
⇀x) =
1
(2 π)n/2 √|S|
· exp

−1
2 (
⇀x −
⇀μx)T · S · (
⇀x −
⇀μx)

(2.34)
where n is the dimension, and S is the covariance matrix:
S =
⎛
⎜⎜⎝
σ2
1
σ12
σ1n
σ21
σ2
2
σ2n
−
−
−
−
σn1
σn2
σ2
n
⎞
⎟⎟⎠
(2.35)
In case of two dimensions, the covariance matrix is:
S =
σ2
1
σ12
σ21
σ2
2

(2.36)
Hence, the bivariate Gaussian PDF is:
f (
⇀x) =
1
(2 π) √|S|
· exp

−1
2
σ2
1 σ2
2
|S|
· Q

(2.37)
where Q:
Q =
(x1 −μ1)2
σ2
1
−2 σ12
(x1 −μ1)(x2 −μ2)
σ2
1 σ2
2
+ (x2 −μ2)2
σ2
2

(2.38)
Figure2.24 depicts in 3D an example of bivariate Gaussian PDF. The ﬁgure has been
generated with the Program 2.22, which also generates the Fig.2.25.
Figure2.25 shows the probability density information (the same information given
by Fig.2.24) via contour plot, and it clearly highlights that the contours are inclined
ellipses: the inclination is due to the cross terms in the covariance matrix.

56
2
Statistical Aspects
Fig. 2.24 Example of
bivariate Gaussian PDF
Fig. 2.25 Example of
bivariate Gaussian PDF
1
1.5
2
2.5
3
3.5
4
4.5
5
1
1.5
2
2.5
3
3.5
4
4.5
5
Program 2.22 Bivariate normal PDF
% Bivariate normal PDF
x1=0:0.02:6;
x2=0:0.02:6;
N=length(x1);
%the PDF
mu1=3; mu2=3;
C=[0.4 0.1;
0.1 0.6];
D=det(C);
K=1/(2*pi*sqrt(D)); Q=(C(1,1)*C(2,2))/(2*D);
ypdf=zeros(N,N); %space for the PDF
for ni=1:N,
for nj=1:N,
aux1=(((x1(ni)-mu1)^2)/C(1,1))+...
+(((x2(nj)-mu2)^2)/C(2,2))...

2.5 More Types of PDFs
57
-(((x1(ni)-mu1).*(x2(nj)-mu2)/C(1,2)*C(2,1)));
ypdf(ni,nj)= K*exp(-Q*aux1);
end;
end;
%display
figure(1)
mesh(x1,x2,ypdf);
title('Bivariate Gaussian: 3D view');
figure(2)
contour(x1,x2,ypdf);
axis([1 5 1 5]);
title('Bivariate Gaussian PDF: top view');
2.5.4
Discrete Distributions
Discrete distributions, [46], are related to counting discrete events. Although we
continue using the term PDF, it should be considered as a discrete version.
2.5.4.1
The Binomial PDF
If an event occurs with probability q, and we make n trials, then the number of times
m that it occurs is:
m =
n
j

qj (1 −q)n−j
(2.39)
An example of binomial PDF is given in Fig.2.26 generated with Program 2.23,
which uses the binomial() (*ST) function.
Program 2.23 Binomial PDF
% Binomial PDF
n=20;
ypdf=zeros(1,n);
for k=1:n,
ypdf(k)=binomial(n,k);
end;
stem(ypdf,'k'); %plots figure
xlabel('values'); title('Binomial PDF');

58
2
Statistical Aspects
Fig. 2.26 Example of
binomial PDF
0
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2 x 10
5
values
2.5.4.2
The Poisson PDF
Suppose there is a particular event you are counting along a given time interval T.
It is known that the expected count is λ. For instance you know that on average the
event occurs 5 times every minute, and T = 20 min; then λ = 5 × 20 = 100.
The Poisson PDF has the following mathematical expression:
f (k) = λk e−λ
k !
(2.40)
This expression gives the probability that the actual count is k (integer values).
The Poisson distribution is of practical importance. It is used to predict the number
of telephone calls, access to a web page, failures of a production chain, performance
of a communication channel or a computer network, etc.
Figure2.27 shows an example of Poisson PDF. It has been generated with the
Program 2.24, which uses the poisspdf() (*ST) function.
Program 2.24 Poisson PDF
% Poisson PDF
lambda=20;
N=50;
ypdf=zeros(1,N);
for nn=1:N,
ypdf(nn)=poisspdf(nn,lambda);
end;
stem(ypdf,'k'); %plots figure
axis([0 N 0 0.1]);
xlabel('values'); title('Poisson PDF');

2.5 More Types of PDFs
59
Fig. 2.27 Example of
Poisson PDF
0
5
10
15
20
25
30
35
40
45
50
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
values
Fig. 2.28 Example of
geometric PDF
-1
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
values
2.5.4.3
The Geometric PDF
The geometric distribution is a discrete analog of the exponential distribution. As
an example, k could be the number of consecutive heads when repeatedly ﬂipping a
coin. The probability of heads in each attempt is p (might be 0.5).
The geometric PDF has the following mathematical expression:
f (k) = (1 −p)k · p
0 < p ≤1
(2.41)
Figure2.28, which has been generated with the Program 2.25, shows an example of
geometric PDF. The program uses the geopdf() (*ST) function.

60
2
Statistical Aspects
Program 2.25 Geometric PDF
% Geometric PDF
P=0.5;
N=10;
ypdf=zeros(1,N);
for nn=0:N,
ypdf(nn+1)=geopdf(nn,P);
end;
stem(0:N,ypdf,'k'); %plots figure
axis([-1 10 0 0.6]);
xlabel('values'); title('Geometric PDF');
2.6
Distribution Estimation
Given a data set, we would like to estimate its probability distribution. This section
presents some of the available methods for this purpose, [6, 55, 83] (see also [1, 62]
for the Weibull distribution). In general, one tries reasonable distribution alternatives
(hypotheses), until getting a satisfactory solution.
2.6.1
Probability Plots
There are some graphical representation methods that help to determine an appro-
priate distribution ﬁtting for a given random signal.
2.6.1.1
Normal Probability
Let us generate with the simple Program 2.26, a random signal with normal PDF,
and let us use the function normplot() (*ST) to plot the signal data set in a special
way. Figure2.29 shows the result. The signal data, represented with plus signs, look
grouped along a straight line: this fact conﬁrms that the signal approximately has a
normal PDF.
Program 2.26 Normal probability plot
% Normal probability plot
N=200; %200 values
y=randn(N,1); %random signal with normal PDF
normplot(y); % the normal probability plot

2.6 Distribution Estimation
61
-3
-2
-1
0
1
2
0.003
0.01 
0.02 
0.05 
0.10 
0.25 
0.50 
0.75 
0.90 
0.95 
0.98 
0.99 
0.997
Data
Probability
Fig. 2.29 Normal probability plot
2.6.1.2
Weibull Probability
Like in the case of the normal PDF, let us now generate with a few MALAB lines,
the Program 2.27, a random signal with Weibull PDF and then plot in a special way
the signal data using the function weibplot() (*ST). Figure2.30 shows the result.
Again the signal data, represented with plus signs, look grouped along a straight
line, conﬁrming that the signal has a Weibull PDF.
Program 2.27 Weibull probability plot
% Weibull probability plot
N=200; %200 values
y=weibrnd(2,0.5,N,1); %random signal with Weibull PDF
weibplot(y); % the Weibull probability plot
2.6.2
Histogram
Most times, the ﬁrst thing to do is to look at the histogram of the random data, since
it gives a lot of fundamental information.
Supposing there seems to be a good distribution PDF candidate to ﬁt the data, it
is convenient ﬁrst to normalize the histogram. Recall that the area covered by a PDF
is one, and so the area of the normalized histogram must be one.

62
2
Statistical Aspects
10
-6
10
-4
10
-2
10
0
0.003
0.01 
0.02 
0.05 
0.10 
0.25 
0.50 
0.75 
0.90 
0.96 
0.99 
Data
Probability
Fig. 2.30 Weibull probability plot
In order to normalize the histogram, it must be divided by the following factor:
r = N h
(2.42)
where N is the number of data, and h is the width of each histogram bin.
The normalized histogram is called the density histogram.
A typical problem is to decide how many bins to use for the histogram. There are
several published rules. One of them, the Normal Reference Rule is the following:
h =

24 σ3 √
3
n
1/3
≈3.5 · σ · N−1/3
(2.43)
For skewed distributions, Scott proposed the following correction factor:
h ≈3.5 · s · N−1/3
(2.44)
s =
21/3 σ
exp( 5 σ2
4 ) (σ2 + 2)

(exp(σ2) −1)
(2.45)

2.6 Distribution Estimation
63
2.6.3
Likelihood
Suppose you have a set of data
⇀x = (x1, x2, . . . , xn) with a certain PDF. This PDF
is characterized by a parameter set
⇀
θ = (θ1, θ2, . . . , θk). For instance, in the case
of a Gaussian PDF, the parameters are μ and σ.
Let us express the PDF as f (
⇀x|
⇀
θ).
We are interested in ﬁnding the PDF that is most likely to have produced the data.
We deﬁne the ‘likelihood function’ by reversing the roles of
⇀x and
⇀
θ:
L(
⇀
θ) = f (
⇀x |
⇀
θ)
(2.46)
The problem is: given the data, ﬁnd the PDF parameters.
The maximum likelihood estimate (MLE) of
⇀
θ is that value of
⇀
θ that maximises
L(
⇀
θ), [57, 98].
Supposing the random data are mutually independent, the likelihood function can
be expressed as a product:
L(
⇀
θ) = f (x1 |
⇀
θ) · f (x2 |
⇀
θ) · · · f (xn |
⇀
θ)
(2.47)
It is usual, in this context, to use natural logarithms. The log-likelihood function is:
l(
⇀
θ) =

i
log (f (xi |
⇀
θ))
(2.48)
For instance, the log-likelihood function corresponding to the Gaussian distribution
is:
l(μ, σ) = −n
2 log(2πσ2) −
1
2 σ2

i
(xi −μ)2
(2.49)
Figure2.31 shows examples of the log-likelihood function of the Gaussian distribu-
tion. We supposed a constant value of the variance, equal to one. The ﬁgure has been
generated with the Program 2.28, which explores 100 different values of the PDF
parameter μ (the mean). The function values have been computed using N random
data generated with the randn() MATLAB function, using μ = 5. Four values of N
have been chosen. Notice that as the number of data increases the curve is sharper.
The peak of the curve, the maximum, corresponds to the mean equal to 5.
Program 2.28 Likelihood example
% Likelihood example
sig2=1;
%constant
r=1/(2*sig2);
Lh=zeros(4,101); %reserve space

64
2
Statistical Aspects
0
5
10
-2000
-1500
-1000
-500
0
0
5
10
-2000
-1500
-1000
-500
0
0
5
10
-2000
-1500
-1000
-500
0
0
5
10
-2000
-1500
-1000
-500
0
N=20 
N=60 
N=200 
N=500 
Fig. 2.31 The log-likelihood of the Gaussian distribution, using 20, or 60, or 200, or 500 data
values
for ni=1:4,
switch ni
case 1, N=20;
case 2, N=60;
case 3, N=200;
case 4, N=500;
end;
%N is number of data
%data generation with normal distribution,
% mean=5, sigma=1
x=5+randn(1,N);
K=(-N/2)*log(2*pi*sig2);
aux=0;
for nm=1:101,
mu=(nm-1)/10; %mean
aux=(x-mu).^2;
Lh(ni,nm)=K-(r*sum(aux)); %Log-Likelihood
end;
end;
%display
ex=0:0.1:10;
figure(1)
for ni=1:4,
subplot(2,2,ni),
plot(ex,Lh(ni,:),'k');
axis([0 10 -2000 0]);
end;

2.6 Distribution Estimation
65
The maximum of the log-likelihood can be analytically determined, using
derivatives, [36]. For instance, in the case of the Gaussian PDF:
∂l(
⇀
θ)
∂μ
= 0 →μ = 1
n
n

i=1
xi
(2.50)
∂l(
⇀
θ)
∂σ
= 0 →σ2 = 1
n
n

i=1
(xi −μ)2
(2.51)
The log-likelihood of the gamma PDF is:
l(α, β) =

i

(α −1) log xi −xi
β −α log β −log Γ (α)

(2.52)
The log-likelihood of the exponential PDF is:
l(α, β) =

i

−xi
β −log β

(2.53)
The log-likelihood of the Weibull PDF is:
l(α, m) =

i

m + (m −1) log xi −xm
i
α −log α

(2.54)
The log-likelihood of the Poisson PDF is:
l(λ) =

i
(ki log λ −λ −log (ki!))
(2.55)
The log-likelihood of the geometric PDF is:
l(p) =

i
(ki log (1 −p) + log p)
(2.56)
2.6.4
The Method of Moments
Let us recall from (2.7) the deﬁnition of moment:
μ′
′k = E(yk), k = 1, 2, 3 . . .
(2.57)

66
2
Statistical Aspects
As in the last sub-section, suppose you have a set of data
⇀y = (y1, y2, . . . , yn) with
a certain PDF. Based on these data, an estimate of the moments can be obtained:
ˆμ′k = 1
n

i
(yk
i )
(2.58)
Assume that the PDF parameters,
⇀
θ = (θ1, θ2, . . . , θk), can be written as functions
of the moments. For instance, θ1 = h (μ1, μ2, μ3).
Now, the idea for the estimation of the parameters is just to use the estimated
moments. Continuing with the example: ˆθ1 = h ( ˆμ1, ˆμ2, ˆμ3).
Honouring its name, the moment generating function can be used to actually
generate moments, [37]:
dΓ
dυ (0) = E(y);
d2Γ
dυ2 (0) = E(y2); . . .;
dnΓ
dυn (0) = E(yn)
(2.59)
For instance, in the case of the Poisson distribution the moment generating function
is:
Γ (υ) =

k
eυ k λk
k ! e−λ = e−λ eλ exp(υ) = Q
(2.60)
Taking derivatives:
dΓ
dυ = λ eυ Q
(2.61)
d2Γ
dυ2 = λ eυQ + λ2 e2υQ
(2.62)
The evaluation at 0 gives:
E(y) = λ
(2.63)
E(y2) = λ + λ2
(2.64)
Clearly, the estimation of the ﬁrst moment, using the data, is enough for the estimation
of λ.
Consider another example: the gamma distribution. The moment generating func-
tion is:
Γ (υ) =

1/β
(1/β) −1
α
(2.65)
Taking derivatives and evaluating at 0:
dΓ
dυ (0) = E(y) = α β
(2.66)

2.6 Distribution Estimation
67
d2Γ
dυ2 (0) = E(y2) = α (α + 1) β2
(2.67)
Using the estimated ﬁrst and second moments, we have two equations and the values
of α and β can be obtained.
2.6.5
Mixture of Gaussians
A popular way to approximate the PDF of a given random data set, is by using a
mixture of well-know PDFs. It can be written as follows:
ˆf (x) =

k
pk fk(x)
(2.68)
where ˆf (x) is the estimated PDF, fk(x) are PDFs (the components of the mixture),
and pk sets the proportions of the mixture (the sum of the pk is one).
Depending on the a priori knowledge on the data, different types of PDFs could
be combined: Weibull, beta, Rayleigh, etc.
Nowadays, the use of Gaussians is predominant for many applications, [92]. They
are universal approximators of continuous densities given enough Gaussian compo-
nents.
The use of mixtures is most appropriate for multi-modal PDFs. This is the case
chosen for the next example, treated with the Program 2.29. It is a simple example
with a bimodal PDF (two peaks).
As it can be seen in the Program 2.29, the random data have been generated by
interleaving data from two Gaussian distribution, according with the proportions
deﬁned by p.
Figure2.32 shows the density histogram (the normalized histogram) of the gener-
ated data. The ﬁgure also shows the shape of the estimated PDF, which is a mixture
of two Gaussian PDFs.
Program 2.29 Mixture of 2 Gaussians
% Mixture of 2 Gaussians
v=-6:0.02:10; %value set
mu1=0; sigma1=1.5; %parameters of Gaussian 1
mu2=5; sigma2=1; %"""Gaussian 2
ypdf1=normpdf(v,mu1,sigma1); %PDF1
ypdf2=normpdf(v,mu2,sigma2); %PDF2
p=0.4; %mix parameter
%mixed Gaussian PDF
ypdf=(p*ypdf1)+((1-p)*ypdf2);
%random data generation
N=5000;
y=zeros(1,N); %reserve space
for nn=1:N,
r=rand(1); %uniform PDF

68
2
Statistical Aspects
Fig. 2.32 Bimodal
distribution and mixture of
Gaussians
-6
-4
-2
0
2
4
6
8
10
0
0.05
0.1
0.15
0.2
0.25
values
if r<p,
y(nn)=mu1+(sigma1*randn(1)); %PDF1
else
y(nn)=mu2+(sigma2*randn(1)); %PDF2
end;
end;
%histogram normalization
nB=100; %number of bins
h=16/100; %bin width
k=N*h;
%display
figure(1)
[nh,xh]=hist(y,100);
plot(xh,nh/k,'k'); hold on; %density histogram
plot(v,ypdf,'r'); %multi-modal PDF
xlabel('values');
title('Mix of 2 Gaussians: histogram and PDF');
An example of bimodal distribution is shown in Fig.2.33. It corresponds to waiting
times (minutes) between successive eruptions of the Old Faithful geyser at Yellow-
stone National Park. See the Resources section for the web address of data. The
ﬁgure with the histogram has been generated using the Program 2.30.
Program 2.30 Histogram of Bimodal distribution
%Histogram of Bimodal distribution
% Geyser eruption data (time between eruptions)
%read data
fer=0;
while fer==0,
fid2=fopen('Geyser1.txt','r');
if fid2==-1, disp('read error')

2.6 Distribution Estimation
69
Fig. 2.33 Bimodal
distribution example
40
50
60
70
80
90
100
110
0
5
10
15
20
25
30
35
else
y1=fscanf(fid2,'%f \r\n'); fer=1;
end;
end;
fclose('all');
%display
hist(y1,30); colormap('cool');
title('Time between Geyser eruptions');
2.6.6
Kernel Methods
Again, suppose you have a set of data
⇀x = (x1, x2, . . . , xn) with a certain PDF. It
was suggested by Parzen (1962) to use the following estimation of the PDF:
ˆf (x) =
1
n
n

i=1
K(x −xi)
(2.69)
where K() is the Parzen window, which is a rectangular window:
K(u) =
 1
2h for |u| < h
0
otherwise
(2.70)

70
2
Statistical Aspects
Fig. 2.34 Parzen estimation
of PDF
xxxxxx
xx
xx
xxxxxx
xx
xx
xx
xx
The idea of the Parzen estimation is represented in the Fig.2.34 for an example
having only a few data. It is similar to the histogram. A rectangle of height 1/2h and
width 2h is placed over each datum; heights are added in overlapping zones.
The idea has been extended and reﬁned, choosing other functions—kernel
functions—for K(), [81, 102].
A popular choice is the Gaussian PDF:
K(u) =
1
√
2πh
exp(−u2/2h2)
(2.71)
Some other choices of kernels are the following:
• Triangular:
K(u) = 1 −
u
h
 for
u
h
 < 1; 0 otherwise
(2.72)
• Biweight:
K(u) = 15
16 (1 −(u/h)2)2 for
u
h
 < 1; 0 otherwise
(2.73)
• Epanechnikov:
K(u) = 0.75 · (1 −0.2 · (u/h)2)
√
5
for
u
h
 <
√
5; 0 otherwise
(2.74)
The main problem with the kernel methods is to choose an adequate value for the
bandwidth h.
Figure2.35 shows an example of PDF estimation using a series of Gaussian PDFs
with the same bandwidth. The number of Gaussians is given by the number of data
points.

2.6 Distribution Estimation
71
Fig. 2.35 Kernel-based
estimation of PDF
-4
-2
0
2
4
6
8
10
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
values
Program 2.31 Kernel method example
% Kernel method example, using Gaussian kernel
v=-4:0.02:10; %set of values
L=length(v); %number of values
% random data:
X=[0.1, 0.25, 1, 1.6, 2.1, 3, 4, 5.2, 5.9, 6.5];
N=length(X); %number of data points
Kpdf=zeros(N,L); % reserve space
h=1; %bandwidth
q=1/(sqrt(2*pi)*h); %constant
for np=1:N,
for nv=1:L,
Kpdf(np,nv)=(q/N)*exp((-(v(nv)-X(np))^2)/(2*(h)^2));
end;
end;
%total PDF
ypdf=sum(Kpdf);
%display
figure(1)
for np=1:N,
plot(v,Kpdf(np,:),'k'); hold on; %PDF components
end;
plot(v,ypdf,'r'); %total PDF
plot(X,zeros(1,N),'bd'); %the data
axis([-4 10 0 0.18]);
xlabel('values');title('PDF estimation with Kernel method');

72
2
Statistical Aspects
2.7
Monte Carlo Methods
Random variables could conveniently be used for several computation and evaluation
purposes. An illustrative example is given in the next subsection about Monte Carlo
integration. The other subsections extend and apply the basic ideas.
Before going into next topics it is convenient to rewrite a small modiﬁcation of
Eq.(2.6), to obtain the expected value of a function g(x):
E(g(x)) =
∞

−∞
g(v) fx(v) dv
(2.75)
Although the Monte Carlo methods will be introduced here using one-dimensional
examples, the real advantage of the methods take place in multi-dimensional prob-
lems where deterministic numerical approximations stumble upon combinatorial
explosion, [47].
ThenameMonteCarlowassuggestedbyNicolasMetropolisin1949.Thisnameis
linked with gambling, Monaco and all that. Statistical simulation has some similarity
with it. A little more history on Monte Carlo methods, together with an illustrative
tutorial, is given by [50]; a frequently cited introduction is [52].
2.7.1
Monte Carlo Integration
The Monte Carlo integration methodology has proved to be effective in difﬁcult or
complicated cases. Let us introduce a series of approaches in this context, [3, 65,
73].
2.7.1.1
A Basic Method
Consider the following example, as represented in Fig.2.36. There is a curve, given
by a certain function g(x), with x between 0 and 10. It is asked to determine the area
A covered by the curve.
Program 2.32 Curve and area
% Curve and area
%the curve
x=0:0.1:10;
y=0.5+(0.3*sin(0.8*x));
plot(x,y,'k'); hold on;
axis([0 10 0 1]);
for vx=1:1:10,
nx=vx*10;
plot([vx vx],[0 y(nx+1)],'g','linewidth',2);

2.7 Monte Carlo Methods
73
Fig. 2.36 Area covered by a
curve
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
y
Fig. 2.37 Same as previous
ﬁgure, but with random
points
0
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
y
end;
xlabel('x'); ylabel('y');
title('area covered by a curve');
Let us generate with the simple Program 2.33 a series of random points on the
x−y plane, with x uniformly random between 0 and 10, and y uniformly random
between 0 and 1. Figure2.37 shows these points on the same plane as Fig.2.36.
Program 2.33 Monte Carlo points, and area approximation
% Monte Carlo points, and area approximation
%the curve
x=0:0.1:10;
y=0.5+(0.3*sin(0.8*x));
%the random points

74
2
Statistical Aspects
N=500; %number of points
px=10*rand(1,N); %uniforma distribution
py=rand(1,N); %"""
plot(x,y,'k'); hold on;
plot(px,py,'b.');
axis([0 10 0 1]);
xlabel('x');ylabel('y');
title('curve and random points');
%area calculation
na=0; %counter of accepted points
for nn=1:N,
xnn=px(nn); ynn=0.5+(0.3*sin(0.8*xnn));
if py(nn)<ynn, na=na+1; end; %point accepted
end;
%print computed area
%the plot rectangle area is 10
A=(10*na)/N
Denote the area of the plane (10 × 1 = 10) as S. The total number of random
points is N. Count the na points inside A.
Then, one can approximate the area A as follows:
A
S
∼= na
N →A ∼= na
N · S
(2.76)
This is an example of Monte Carlo integration. Notice that we have accepted nb
points, and rejected the rest of the points. Notice that the last part of Program 2.33
provides an implementation of Monte Carlo integration. The last sentence prints the
area computation result.
2.7.1.2
Using Expected Values
Suppose one has a certain function q(x) such that:
q(x) ≥0, x ∈(a, b)
 b
a q(x) dx = M < ∞
(2.77)
Now, let:
p(x) = q(x)
M
(2.78)
Then p(x) satisﬁes the conditions for being a PDF. The value M could be obtained
using the basic integration method just explained.
If we have to integrate the following:
y =
 b
a
g(x) q(x) dx
(2.79)

2.7 Monte Carlo Methods
75
Fig. 2.38 Example of
integral of the product
g(x) · p(x)
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0
0.2
0.4
0.6
0.8
1
1.2
x
p(x) 
g(x) 
This is equivalent to:
y =
 b
a
M · g(x) p(x) dx = M · E(g(x))
(2.80)
(recall expression (2.26) with the expected value)
According to the common practice for the computation of expected values, the
integral can be approximated with:
y ≈M · 1
n
n

i=1
g(xi)
(2.81)
One draws a set of xi samples from the p(x) PDF, and then computes the sum of
g(xi).
An example of the integration technique is presented in the Fig.2.38. To simplify
the example, a p(x) has been chosen that directly can be represented with the exp-
pdf() (*ST) function; that is, p(x) is an exponential function. Likewise, the function
random(‘exp’,..) has been used to generate samples from p(x) as PDF. This can be
seen in the Program 2.34, which generates the ﬁgure. The other function g(x) has
been chosen as a fragment of sinusoidal signal. The ﬁgure includes a plot of the
product p(x) g(x), adding some vertical lines to visualize the area which should be
the result of the integral.

76
2
Statistical Aspects
Program 2.34 Integration as expected value
% Integration as expected value
% integral of g(x)*p(x), where p(x) can be taken as a PDF
% the integrand functions
x=0:(pi/100):(1.5*pi); %domain of the integral
g=(0.8*sin(x)).^2; % the function g(x)
mu=1; %parameter of the exponential distribution
p=exppdf(x,mu); %the function p(x) (exponential PDF)
%Deterministic approximation of the integral
aux=abs(g.*p);
disp('deterministic integral result:');
DS=sum(aux)*(pi/100) %print result
%display of the integrand functions
figure(1)
plot(x,g,'k'); hold on;
plot(x,p,'r');
plot(x,aux,'b');
for vx=10:10:151, %mark the integral area
l=(vx*pi)/100;
plot([l l],[0 aux(vx)],'g','linewidth',2);
end;
axis([0 1.5*pi 0 1.2]);
title('Integral of the product g(x)p(x)');
xlabel('x');
%Monte Carlo Integration------------------------------------
%draw N samples from p(x) as PDF
N=3000; %number of samples
x=random('exp',mu,1,N); %the samples
%evaluate g(x) at the samples
nv=0; %counter of valid data points
L=1.5*pi; %limit of the integral
for nn=1:N,
if x(nn)<=L, g(nn)=(0.8*sin(x(nn)))^2; nv=nv+1;
else
g(nn)=0; %the value of x is outside integral domain
end;
end;
%integral
disp('Monte Carlo integral result:');
S=(sum(g)/nv) %print result
TheProgram2.34alsocomputeswithadeterministicsimpleapproachtheintegral.
For comparison purposes, both the deterministic and the Monte Carlo results are
printed when executing the program. Notice that the program includes a protection
against trying to operate outside the integral domain.
Coming now to a simpler case:
y =
 b
a
f (x) dx
(2.82)
Let us take:

2.7 Monte Carlo Methods
77
g(x) = f (x)
p(x)
(2.83)
Therefore:
y =
 b
a
g(x) p(x) dx
(2.84)
Which can be approximated as follows:
y ≈· 1
n
n

i=1
g(xi)
(2.85)
2.7.1.3
Importance Sampling
In the previous approximations a certain p(x) has been used. It is an arbitrary PDF.
Several alternatives have been proposed for choosing a p(x) in order to speed up the
convergence (some literature refers to it as variance reduction).
A key observation is that in:
y ≈· 1
n
n

i=1
f (xi)
p(xi)
(2.86)
it is convenient that p(x) ≈f (x) in order to avoid negligible terms. If this is done,
most samples of f (x) will be taken where f (x) is larger, and so it is termed as
importance sampling, [19].
Let us put an example of not using importance sampling. The example is repre-
sented in Fig.2.39. A uniform PDF is chosen for p(x) along a wide range. What we
see on this ﬁgure is that a large part of p(x) is of no use since there are many samples
with f (xi) = 0.
Figure2.39 is useful also for noticing a possible problem. If p(x)was narrowed so
it ﬁts inside f (x) there would be samples where:
f (xi)
p(xi) ≈∞
(2.87)
This should be avoided. In general, the advice is to use p(x) with long tails.
A better option for p(x) is shown in Fig.2.40. It is clear that p(x) is similar to the
f (x) to be integrated, and that it covers the tails of f (x). The case has been treated
with the Program 2.35. Notice that the program includes a protection against division
by zero. In this example, the function p(x) corresponds to a beta PDF, so one can use
MATLAB (*ST) functions. The program also prints, for comparison, the results of
the deterministic and the importance sampling integrations.

78
2
Statistical Aspects
Fig. 2.39 Example of
un-importance sampling
0
1
2
3
4
5
6
7
8
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
f(x) 
p(x) 
Fig. 2.40 Example of f(x),
and p(x) for importance
sampling
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
x
f(x) 
p(x) 
Program 2.35 Integration as expected value: Importance sampling
% Integration as expected value: Importance sampling
% integral of f(x), an appropriate p(x) PDF is taken
% the integrand functions
x=0:(pi/100):(0.25*pi); %domain of f(x)
xx=0:(pi/100):1; %domain of p(x)
f=25*(x.^3).*cos(2*x); %the function f(x)
alpha=4; beta=3; %parameters of the PDF
p=betapdf(xx,alpha,beta); %the function p(x) (beta PDF)
%Deterministic approximation of the integral
disp('deterministic integral result:');
DS=sum(f)*(pi/100) %print result

2.7 Monte Carlo Methods
79
%display of f(x) and p(x)
figure(1)
plot(x,f,'k'); hold on;
plot(xx,p,'r');
axis([0 1 0 2.5]);
title('importance sampling: f(x) and p(x)');
xlabel('x');
%Monte Carlo Integration------------------------------------
%draw N samples from p(x) PDF
N=2000; %number of samples
x=random('beta',alpha,beta,1,N); %the samples
%evaluate g(x) at the samples
nv=0; %counter of valid data points
g=0; %initial value
for nn=1:N,
if x(nn)>0, %avoid division by zero
if x(nn)<=(0.25*pi), %values inside f() domain
f=25*(x(nn).^3).*cos(2*x(nn)); %evaluate f() at xi
else
f=0;
end;
p=betapdf(x(nn),alpha,beta);
g=g+(f/p); %adding
nv=nv+1;
end;
end;
%integral
disp('Monte Carlo integral result:');
S=(g/nv) %print result
Let us consider again the integration of:
y =
 b
a
g(x) p(x) dx
(2.88)
It can be written as:
y =
 b
a
g(x) p(x)
h(x)h(x) dx
(2.89)
Denote:
w(x) = p(x)
h(x)
(2.90)
as ‘weight function’.
Then, the approximation is:
y ≈· 1
n
n

i=1
(g(xi) · w(xi))
(2.91)

80
2
Statistical Aspects
The function h(x) is a proposed PDF, as close as possible to p(x), and the samples
xi are drawn from the h(x) PDF.
2.7.2
Generation of Random Data with a Desired PDF
Several PDFs, provided by MATLAB, have been presented in this chapter. With the
function random() (*ST) it is possible to select a PDF among a set of alternatives,
and then use the function to generate random numbers from the selected PDF.
In order to be open for more options, it is convenient to study how to generate
random numbers from any desired PDF, [27, 91].
In this subsection, two methods will be introduced: the ﬁrst is based on inversion
of the distribution function, [66]; the second is based on rejection. Other methods
will be described later on, in the section on Markov processes.
2.7.2.1
Inversion Sampling
For easier description, let us denote distribution functions as F(x) (recall Sect.2.2.1).
The value of a distribution function is in the range 0..1 and increases or keep cosntant
as x increases.
In our case, we wish to obtain a set of samples obeying to a desired distribution
F(x). Denote as F−1 the inverse of F.
Let us draw a set of samples U with values between 0 and 1 from a uniform PDF.
Then, the set of samples:
Z = F−1(U)
(2.92)
obeys to the desired distribution
Figure2.41 depicts the idea of sample generation.
Therefore the procedure is: (a) draw a sample yi from uniform PDF; (b) compute
zi = F−1(yi); and go back to (a), until sufﬁcient data have been obtained.
For example, it is desired to generate a set of samples with a sinusoidal distrib-
ution function as depicted on the left part of Fig.2.42. The corresponding PDF (the
derivative) is depicted on the right part of this ﬁgure.
In this example, it is easy to analytically obtain the inverse of the distribution
function: the inverse of sin(x) is arc sin(x). Moreover, MATLAB actually provides
the asin() function.
The inversion procedure is implemented with the Program 2.36. It generates the
set of samples with the desired distribution, and displays Figs.2.42 and 2.43. This
last ﬁgure is an histogram of the generated data.

2.7 Monte Carlo Methods
81
Fig. 2.41 The inversion
procedure
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
series U
series Z
F
Fig. 2.42 Example of
desired distribution function
and PDF
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
desired
 distribution function
0
0.5
1
1.5
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
desired PDF
Program 2.36 Generation of random data with a desired PDF
% Generation of random data with a desired PDF
% using analytic inversion
% example of desired distribution function
x=0:(pi/100):(pi/2);
F=sin(x); %an always growing curve
pf=cos(x); %PDF=derivative of F
% generation of random data
N=2000; %number of data
y=rand(1,N); %uniform distribution
% random data generation:
z=asin(y); %the inverse of F
figure(1)

82
2
Statistical Aspects
Fig. 2.43 Histogram of
random data generated using
analytical inversion
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
20
40
60
80
100
120
x
subplot(1,2,1)
plot(x,F,'k');
xlabel('x'); title('desired distribution function');
subplot(1,2,2)
plot(x,pf,'k');
xlabel('x'); title('desired PDF');
figure(2)
hist(z,30); colormap('cool');
xlabel('x');title('histogram of the generated data');
Here is a set of analytical inverses of distribution functions.
Exponential:
PDF
F
F−1
e−x , x > 0 (1 −e−x) log( 1
U )
Weibull (simple version):
PDF
F
F−1
m xm−1 · e−xm, x > 0 (1 −e−xm)

log( 1
U )
1/m
Cauchy:
PDF
F
F−1
1
π(1+x2) ( 1
2 + 1
π arctan x) tan(π U)

2.7 Monte Carlo Methods
83
Pareto:
PDF
F
F−1
a
xa+1 , a > 0, x > 1 (1 −
a
xa ) (
1
U1/a )
In case of difﬁculty with the analytical inversion, it is still possible to numerically
obtain the inversion. Program 2.37 gives an example of it, continuing with the pre-
vious example. Figure2.44 compares the numerical result with the analytical result:
they are essentially the same.
Program 2.37 Numerical inversion of a function
% Numerical inversion of a function
% example of F=sin(x), in the growing interval
M=1001;
y=0:0.001:1; %F between 0 and 1
x=zeros(1,M);
%incremental inversion
aux=0; dax=0.001*pi;
for ni=1:M,
while y(ni)>sin(aux),
aux=aux+dax;
end;
x(ni)=aux;
end;
plot(y,asin(y),'gx'); hold on; %analytical inversion
plot(y,x,'k'); %result of numerical inversion
xlabel('y'); ylabel('x');
title('numerical and analytical inversion of F');
To complete the example, the Program 2.38 obtains a set of samples with the
desired PDF, using numerical inversion. Figure2.45 shows the histogram.
Fig. 2.44 Generation of
random data with a certain
PDF
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
y
x

84
2
Statistical Aspects
Fig. 2.45 Histogram of
random data generated using
numerical inversion
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
20
40
60
80
100
120
z
Program 2.38 Generation of random data with a desired PDF
% Generation of random data with a desired PDF
% using numerical inversion
%first: a table with the inversion of F
M=1001;
y=0:0.001:1; %F between 0 and 1
x=zeros(1,M);
%incremental inversion
aux=0; dax=0.001*pi;
for ni=1:M,
while y(ni)>sin(aux),
aux=aux+dax;
end;
x(ni)=aux;
end;
%second: generate uniform random data
N=2000; %number of data
ur=rand(1,N); %uniform distribution
%third: use inversion table
z=zeros(1,N);
for nn=1:N,
%compute position in the table:
pr=1+round(ur(nn)*1000);
z(nn)=x(pr); %read output table
end;
%display histogram of generated data
hist(z,30); colormap('cool');
xlabel('z');
title('histogram of the generated data');

2.7 Monte Carlo Methods
85
Fig. 2.46 Example of
desired f(x) PDF and
proposal g(x) PDF
0
1
2
3
4
5
6
7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
x
c g(x) 
f(x) 
--
pi
2.7.2.2
Rejection Sampling
The desired PDF is f (x). A proposal g(x) PDF is chosen, such that:
f (x)
g(x) ≤c for all x
(2.93)
where c is a positive constant. Figure2.46 shows an example, and illustrates the
procedure explained below.
The procedure is: (a) generate a sample vi from the g(x) PDF; (b) generate a
sample ui from uniform PDF on (0, 1); (c) if:
pi = ui · c · g(vi) < f (vi)
(2.94)
then accept vi, else reject; (d) back to (a) until sufﬁcient data have been obtained,
[82].
Program 2.39 provides an implementation of the rejection procedure, with the
same desired sin() PDF as before. Figure 2.47 shows an histogram of the generated
data.
Program 2.39 Generation of random data with a desired PDF
% Generation of random data with a desired PDF
% Using rejection method
% example of desired PDF
x=0:(pi/100):pi;
dpf=0.5*sin(x); % desired PDF
% example of proposal PDF
xp=0:(pi/100):pi+3;
ppf=raylpdf(xp,1.5);

86
2
Statistical Aspects
Fig. 2.47 Histogram of
random data generated using
the rejection method
0
0.5
1
1.5
2
2.5
3
3.5
0
20
40
60
80
100
120
x
%factor
c=1.5;
% generation of random data
N=2000; %number of data
z=zeros(1,N); %space for data to be generated
for nn=1:N,
accept=0;
while accept==0,
v=raylrnd(1.5,1,1); %Rayleigh distribution
u=rand(1,1); %uniform distribution
if v<=pi, %v must be inside dpf domain
P=u*c*raylpdf(v,1.5);
L=0.5*sin(v);
if P<L, z(nn)=v; accept=1; end; %accept
end;
end;
end;
figure(1)
plot(x,dpf,'k'); hold on;
plot(xp,c*ppf,'r');
xlabel('x'); title('desired PDF and proposal PDF');
figure(2)
hist(z,30); colormap('cool');
xlabel('x');title('histogram of the generated data');

2.7 Monte Carlo Methods
87
2.7.2.3
Other Methods
There are a number of transformations that can be used to generate random variables
with a desired PDF.
For instance, the method of Box and Müller obtains a pair of Gaussian variables
as follows, [13]:
X =

log
 1
U1

· cos(2π U2)
(2.95)
Y =

log
 1
U1

· sin(2π U2)
(2.96)
where U1 and U2 are independent uniform [0, 1] random variables.
The random cosine is also used by other methods, which are called polar methods,
[33, 80]. A symmetric beta distribution (with α = β) is obtained using:
X = 1
2 (1 +

1 −UV
1
· cos(2π U2))
(2.97)
where: V =
2
2α−1.
The Student’s t distribution can be obtained using:
X =

a( U−2/a
1
−1) · cos(2π U2)
(2.98)
2.8
Central Limit
There are two main alternative formulations of the Central Limit Theorem (CLT). The
ﬁrst alternative is related to the distribution of means; while the second alternative
is related to sums of random data sets. In both cases, one has several random data
sets of size n: X1, X2, . . . , XK. The data sets are independent with equal distribution.
The variances of the data set are ﬁnite. Suppose K tends to inﬁnity, then:
1. Take the means μ1, μ2, . . . , μK of each data set. CLT establishes that these means
form a random data set with normal (Gaussian) distribution.
2. The sum of X1, X2, . . . , XK is also a random data set with normal (Gaussian)
distribution.
It does not matter what the distribution of the data sets X1, X2, . . . , XK is.
The reader is invited to repeatedly convolve any PDF with itself (recall 2.3.3,
about the characteristic function), the result always tend to a Gaussian PDF. Perhaps
the most dramatic example is when you use a uniform PDF for this exercise.

88
2
Statistical Aspects
In the case of products of positive random data sets, the logarithm will tend to a
normal distribution, and the product itself will tend to a log-normal distribution.
For the interested reader it is recommended to examine the topic of stable distri-
butions, [12, 61]. Particular cases of stable distributions are the normal distribution,
the Cauchy distribution and the Lévy distribution. If the random data sets have not
ﬁnite variance (this can be observed on the PDF tails), the sum may still tend to a
stable distribution.
There are some variants of the CLT, [79]. In particular, the Lyapunov CLT and the
Lindeberg CLT require the random data sets to be independent but not necessarily
to have the same PDF.
Consider again X1, X2, . . . , XK; each data set has a mean μiand variance σ2
i .
Deﬁne s2
n = 
i
σ2
i and Yi = Xi −μi. If there exists δ > 0 such that:
lim
n→∞
1
s2+δ
n
n

i=1
E(|Yi|2+δ ) = 0
(2.99)
then the sum of Yi/sn tends to a normal distribution. This is the Lyapunov CLT.
The Lindenberg CLT is similar, [41], but using as condition that for every ε > 0:
lim
n→∞
1
s2n
n

i=1
E(|Yi|2 I(| Yi| ≥ε sn)) = 0
(2.100)
where I() is the indicator function.
Both are sufﬁcient conditions. The Lyapunov condition is stronger than the Lin-
denberg condition.
In the next chapters some sound ﬁles will be used for several purposes. These
sounds are quite different: music, animal sounds, sirens… The Program 2.40 just
reads a set of 8 sounds, and adds the corresponding data. Figure2.48 shows a his-
togram of the result: it exhibits a Gaussian shape, as predicted by the central limit
theorem. The ﬁnal sentence of the Program let you hear the accumulated signal.
Program 2.40 Central limit of wav sounds
%Central limit of wav sounds
%read a set of sound files
[y1,fs]=wavread('srn01.wav'); %read wav file
[y2,fs]=wavread('srn02.wav'); %read wav file
[y3,fs]=wavread('srn04.wav'); %read wav file
[y4,fs]=wavread('srn06.wav'); %read wav file
[y5,fs]=wavread('log35.wav'); %read wav file
[y6,fs]=wavread('ORIENT.wav'); %read wav file
[y7,fs]=wavread('elephant1.wav'); %read wav file
[y8,fs]=wavread('harp1.wav'); %read wav file
%Note: all signals have in this example fs=16000
N=25000; %clip signals to this length
y=zeros(8,N); %signal set
y(1,:)=y1(1:N)'; y(2,:)=y2(1:N)';

2.8 Central Limit
89
Fig. 2.48 Histogram of sum
of signals
-15
-10
-5
0
5
10
15
0
500
1000
1500
2000
2500
3000
y(3,:)=y3(1:N)'; y(4,:)=y4(1:N)';
y(5,:)=y5(1:N)'; y(6,:)=y6(1:N)';
y(7,:)=y7(1:N)'; y(8,:)=y8(1:N)';
%normalization
for nn=1:8,
s=y(nn,:); s=s-mean(s); %zero mean
vr=var(s); s=s/sqrt(vr); %variance=1
y(nn,:)=s;
end;
%sum of signals
S=sum(y);
%histogram
figure(1)
hist(S,30); colormap('cool');
title('histogram of the sum of signals');
%sound of the sum
soundsc(S,fs);
2.9
Bayes’ Rule
According with the Stanford Encyclopedia of Philosophy (web site cited in the
Resources section), “the most important fact about conditional probabilities is
undoubtedly Bayes’ Theorem, whose signiﬁcance was ﬁrst appreciated by the British
cleric Thomas Bayes (1764)”.

90
2
Statistical Aspects
Nowadays, the recognition given to the Bayes approach is rapidly extending in
several methodologies and ﬁelds of activity, like estimation, modelling, decision
taking, etc.
A reference book on Bayesian Theory is [10]. In addition, [30] provides a detailed
history of how the Bayesian methodology has evolved.
This section has three parts, following a logical order. First, the concept of con-
ditional probability is introduced. Then, the Bayes’ rule is enounced, and illustrated
with the help of some ﬁgures. Finally, a brief introduction of Bayesian networks is
made.
2.9.1
Conditional Probability
Let us introduce the concept of conditional probability using an example.
There was a factory producing hundreds of a certain device. The products were
tested before going to the market.
Each device could be ‘good’ or ‘bad’ (it works well, or not). The situation is that
2% of the devices are bad. Then, there are two probabilities:
P(good) = 0.98 ;
P(bad) = 0.02
(2.101)
The test says ‘accept’ or ‘reject’. Sometimes the test is erroneous:
• In the case of good devices there are two probabilities:
P(accept |good) = 0.99 ;
P(reject |good) = 0.01
(2.102)
• In the case of bad devices there are two probabilities:
P(accept |bad) = 0.03 ;
P(reject |bad) = 0.97
(2.103)
Conditional probability is expressed as P (A|B): the conditional probability of A
given B.
Unconditional probability P(A) of the event A, is the probability of A regardless
of what happens with B. The unconditional probability is also denoted as ‘prior’ or
‘marginal’ probability, and also ‘a priori’.
The conditional probability P (A|B) could also be denoted as ‘posterior’, or ‘a
posteriori’ probability.
Notice in the example of the device that good or bad refers to the state of the device,
and accept or reject pertains to measurements. This point of view is important in the
context of Kalman ﬁlters.

2.9 Bayes’ Rule
91
The joint probability is the probability of having both A and B events together.
The joint probability is denoted as P (A∩B) (or P (AB), or P(A, B)). Recall that two
events A and B are independent if:
P(A ∩B) = P(A) P(B)
(2.104)
2.9.2
Bayes’ Rule
The conditional probability and the joint probability are related by the formula:
P(A|B) = P(A ∩B)
P(B)
(2.105)
Likewise:
P(B|A) = P(A ∩B)
P(A)
(2.106)
Combining the two equations:
P(A|B) P(B) = P(A ∩B) = P(B|A) P(A)
(2.107)
Therefore:
P(A|B) = P(B|A) P(A)
P(B)
(2.108)
This is the famous Bayes’ rule.
Let us return to the devices example. There will be four cases:
(a) P(accept ∩good) = P(accept| good) P(good) = 0.9702
(b) P(accept ∩bad) = P(accept| bad) P(bad) = 0.0006
(c) P(reject ∩good) = P(reject| good) P(good) = 0.0098
(d) P(reject ∩bad) = P(reject| bad) P(bad) = 0.0194
The rejected devices are:
P(reject) = P(reject ∩good ) + P(reject ∩bad ) = 0.0292
(2.109)
And the probability of a rejected device to be a good device:
P(good|reject) = P(good ∩reject)
P(reject)
= 0.0098
0.0292 = 0.335
(2.110)
Now, let us introduce conditional PDFs. Given two random variables x1 and x2, the
conditional PDF of x1 given x2 is:

92
2
Statistical Aspects
Fig. 2.49 The PDFs of two
variables A and B
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
x
y
A (beta) 
B (normal) 
f (x1|x2) = f (x1 ∩x2)
f (x2)
(2.111)
The Bayes’ rule can be generalized to conditional PDFs.
f (x1|x2) = f (x2|x1) f (x1)
f (x2)
(2.112)
Here is the Chapman-Kolmogorov equation about the product of two conditional
PDFs:
f (x1|(x2 ∩x3 ∩x4)) f ((x2 ∩x3) |x4) = f ((x1 ∩x2 ∩x3)|x4)
(2.113)
In order to illustrate the Bayes’ rule in the PDF context, a simple example is now
presented. Figure2.49 shows the case: two random variables A and B. The variable
A has a beta PDF; the variable B a normal PDF. There is a region where both PDFs
overlap.
The two products of interest for the Bayes’ rule are shown in Fig.2.50. In both
cases, the product is plotted: it is a low hill at the bottom. Clearly, the result of the two
products is the same. Notice that the conditional probability f(A|B) is zero outside
the domain of B, and similarly with f(B|A) and A.
It is also clear that the result of the products considered in Fig.2.50 can also be
obtained by simple product of the two PDFs, as it is shown in Fig.2.51. The joint
PDF, f (A, B) is also equal to this product.

2.9 Bayes’ Rule
93
Fig. 2.50 The two products
of interest
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
f(A|B) * f(B)
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
f(B|A) * f(A)
Fig. 2.51 The product of the
two PDFs
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
2
2.5
f(A,B)=f(A)*f(B)
Program 2.41 Two overlapped PDFs
% Two overlapped PDFs
x=0:0.01:2.5;
%densities
alpha=5; beta=3;
Apdf=betapdf(x,alpha, beta); %beta PDF
mu=1.3; sigma=0.3;
Bpdf=normpdf(x,mu,sigma); %normal pdf
%product of PDFs at intersection zone
piz=Apdf(50:100).*Bpdf(50:100);
%display
figure(1)
plot(x,Apdf,'r'); hold on;
plot(x,Bpdf,'b');
title('Two random variables A and B: their PDFs')

94
2
Statistical Aspects
xlabel('x'); ylabel('y');
figure(2)
subplot(2,1,1)
plot(x,Bpdf,'b'); hold on;
plot(x(50:100),Apdf(50:100),'r')
plot([x(50) x(50)],[0 Apdf(50)],'r--');
plot(x(50:100),piz,'k');
title('f(A|B) * f(B)');
subplot(2,1,2)
plot(x,Apdf,'r'); hold on;
plot(x(50:100),Bpdf(50:100),'b')
plot([x(100) x(100)],[0 Bpdf(100)],'b--');
plot(x(50:100),piz,'k');
title('f(B|A) * f(A)');
figure(3)
join=Apdf.*Bpdf;
plot(x(50:100),join(50:100),'k'); hold on;
plot(x,Apdf,'r'); hold on;
plot(x,Bpdf,'b');
title('f(A,B)=f(A)*f(B)')
More details on the Bayes’ rule can be found in [64]. Some examples of applica-
tions are given in [11]. A more extensive exposition on Bayesian probability topics
is [16].
2.9.3
Bayesian Networks. Graphical Models
One convenient way for the study of probabilistic situations is offered by the Bayesian
networks, [8, 21, 89]. These are graphical models that represent probabilistic rela-
tionships among a set of variables.
A very simple model is shown in Fig.2.52. It refers to two random variables A
and B. We would say that A is a parent of B, B is a child of A.
According with Fig.2.52 one has:
P(A ∩B) = P(A) P(B|A)
(2.114)
Both the Bayes network of Fig.2.52 and Eq.(2.35) can represent the four cases (a,
b, c, d) of the devices example. For instance, the case a) is:
Fig. 2.52 A simple Bayes
network
A
B

2.9 Bayes’ Rule
95
Fig. 2.53 Another example
of Bayes network
A
B
C
Fig. 2.54 Two parents in a
Bayes network
A
B
C
P(A = good ∩B = accept) = P(A = good) P(B = accept|A = good) (2.115)
In some cases, the variables A and B can take only two values: true or false (good
or bad, etc.). In other cases, the variables can take any value.
Figure2.53 represents another situation. In this ﬁgure, C is independent of A and
B; while B depends on A.
Concerning Fig.2.53, one has:
P(A ∩B ∩C) = P(A) P(B|A) P(C)
(2.116)
A typical example of the situation depicted in Fig.2.54 is that A and C are the
results of ﬂips of two coins (two possible values: T or H). B is true if the values of
A and C coincide.
Now:
P(A ∩B ∩C) = P(B|A ∩C) P(A) P(C)
(2.117)
Suppose that, in the example of the coins, we know that B is true (we got evidence
on this), then:
P((A = H ∩C = H) | B = true) = 1
2
(2.118)
On the other hand:
P(A = H | B = true) P(C = H | B = true) = 1
2 · 1
2 = 1
4
(2.119)

96
2
Statistical Aspects
Therefore, once we know B is true, A and C are not independent (Eqs.(2.38) and
(2.39) give different results).
The literature related with Bayesian networks (BN) is becoming particularly
extensive. In general, BN are used for the study of scenarios with several alter-
natives, like medical diagnosis, planning and decision making, etc. The description
of a situation in terms of a BN would be quite useful for forward or backward infer-
ence; and also for illustrating the complexity and the internal structure of the problem
at hand.
Most cited books are [59] on BN and decision graphs, [43] on BN and Bayesian
Artiﬁcial Intelligence, or [58] on learning BN. This last subject, learning, has deep
interest for a number of reasons, being one of them the possibility of automatic
construction of BN by learning mechanisms, instead of direct human work.
With respect to learning BN, [34] offers a tutorial, [26] presents some BN learning
approaches, and [54] treats in academic detail learning from data.
Arelatedtopicis‘beliefnetworks’.Representativereferencesare[22]forclassiﬁer
systems, and the tutorial of [45].
There are many published applications, like some papers connecting BN and GIS.
The acronym GIS means Geographic Information System, which, for instance, could
be related with the prediction of ﬂooding or avalanches, etc. Examples of this kind
of applications are [86] on BN and GIS based decision systems, and [87] on BN,
GIS and planning in marine pollution scenarios.
Other illustrative applications are, [17] for meteorology, [5, 51, 60] for medical
diagnosis and prediction, [99] for risk analysis and maintenance, [68] for natural
resources management, and [32] for ﬁnancial analysis. See the book [70] for more
types of applications.
2.10
Markov Process
A stochastic process (or random process) with state space S, is a collection of indexed
random variables. Usually the index is time. The state space could be discrete or
continuous. Likewise, the index (time) could be discrete or continuous. Hence, there
are four general types of stochastic processes. There are many books on stochastic
processes, like [63, 67]. In addition, there are also brief academic introductions, like
[14, 44].
Consider any state Si of the stochastic process. The next state could be any of the
states belonging to S. There are state transition probabilities. The Markov property
is that these probabilities only depend on the present state, and not on past states.
A Markov chain is a discrete-state random process with the Markov property. The
chain could be discrete-time or continuous time.
The ﬁrst part of this section focuses on Markov chains, [42]. The section then
continues with the generation of random data using Markov chain Monte Carlo
(MCMC).

2.10 Markov Process
97
2.10.1
Markov Chain
In a Markov chain the transition probability from Sn to Sn+1 is:
P(Sn+1 | Sn, Sn−1, . . . , Sn−m) = P(Sn+1 | Sn)
(2.120)
which is the Markov property.
The transition probabilities could be written as a table. For instance, in a process
with three states A, B and C:
after
A
B
C
A
0.65
0.20
0.15
before
B
0.3
0.24
0.46
C
0.52
0.12
0.36
Notice that row sums are equal to 1.
Also, the transition probabilities could be written in matrix form:
T =
⎡
⎣
0.65 0.20 0.15
0.30 0.24 0.46
0.52 0.12 0.36
⎤
⎦
(2.121)
In writing these numbers we are supposing the Markov chain is time-homogeneous,
that is: the probabilities keep constant along time.
A graphical expression of the Markov chain could be done as a stochastic ﬁnite
state machine (FSM). For instance, continuing with the example (Fig.2.55):
The process starts with an initial probability vector:
⇀X0 = [x1(0), x2(0), , xk(0)]
Fig. 2.55 An example of
Markov chain FSM
A
B
C
0.20
0.20
0.46
0.34
0.65
0.12
0.52
0.15
0.36
A
B
C
0.20
0.20
0.46
0.34
0.65
0.12
0.52
0.15
0.36

98
2
Statistical Aspects
For instance, it could be [0.25, 0.40, 0.35] for the three states example we are
considering (therefore, the initial probabilities are: P(A) = 0.25, P(B) = 0.40,
P(C) = 0.35).
The probability vector after one transition is:
⇀X1 =
⇀X0 T
(2.122)
And, after n transitions:
⇀X1 =
⇀X0 T n
(2.123)
There are many applications of this framework. Like for instance the study of certain
system evolutions: market preferences, voting, population components (the case of
several types of trees in a forest), transportation options, etc. We recommend [94],
as it describes the ﬁve greatest applications of Markov Chains, including Shannon’s
information theory, web searching (Google), computer performance evaluation, etc.
Another interesting text is [35], with a connection between Markov chains and game
theory.
A transition matrix is regular if for some k, all the entries tij ∈T k are positive;
that is: 0 < tij < 1. For example:
T =
⎡
⎣
0.35 0.65 0
0
0.30 0.7
0.75 0.25 0
⎤
⎦
(2.124)
T 2 =
⎡
⎣
0.1225 0.4225 0.4550
0.5250 0.2650 0.2100
0.2625 0.5625 0.1750
⎤
⎦
(2.125)
For k = 2, all entries are positive; the matrix is regular.
If the matrix T is regular, then for any initial probability vector
⇀z, it happens that
as the number of transitions increases, the probability vector tends to a unique vector
⇀V :
⇀z T n =
⇀V
(2.126)
This vector
⇀V is denoted as the equilibrium vector, or the ﬁxed vector, or the steady
state vector. This last name refers to the fact that:
⇀V T =
⇀V
(2.127)
Therefore, the studies on system evolutions may well end with a constant,
equilibrium population.

2.10 Markov Process
99
It is important to study the eigenvalues of the transition matrix. If the transition
matrix is regular, the largest eigenvalue is 1, and the rest of eigenvalues are |λ1| < 1.
Let us express a 3 × 3 transition matrix in diagonal form:
T =
⇀v
⎡
⎣
1 0 0
0 λ2
0
0 0 λ3
⎤
⎦⇀v
−1
(2.128)
Then, as n increases:
T n =
⇀v
⎡
⎣
1 0 0
0 λn
2
0
0 0 λn
3
⎤
⎦⇀v
−1 →
⇀v
⎡
⎣
1 0 0
0 0 0
0 0 0
⎤
⎦⇀v
−1
(2.129)
Therefore, the matrix T n converges to a constant matrix we shall denote as Te.
Notice that Eq.(2.127) gives the left eigenvector corresponding to an eigenvalue
equal to 1. This eigenvector is
⇀V . All the rows of matrix Te are equal to
⇀V . For
instance, if
⇀V = [0.13 , 0.42, 0.45], then:
Te =
⎡
⎣
0.13 0.42 0.45
0.13 0.42 0.45
0.13 0.42 0.45
⎤
⎦
(2.130)
Another type of Markov chain is the absorbing Markov chain. One or more of
the diagonal entries tii of T is equal to 1, so the transition matrix is not regular. The
states corresponding to such entries are absorbing states. Once the process enters in
an absorbing state, it is not possible to leave.
Program B.1, which has been included in the Appendix for long programs, con-
siders a simple weather prediction model with three states: Clouds (‘C’), Rain (‘R’),
or Sun (‘S’). We take the same values depicted in Fig.2.55. The program departs
from a vector of initial probabilities, and depicts in Fig.2.56 the transitions between
states.
The Program B.1 also prints the series of consecutive states as a string of charac-
ters, like: CCSRCRR…
2.10.2
Markov Chain Monte Carlo (MCMC)
Let us consider again the generation of random data with a desired PDF, p(x). The
MCMC methods do use a Markov chain that converges to a stationary distribution
with the desired p(x). Therefore, once the chain has converged, the chain is used to
get draws from p(x), although they would be correlated.
This convergence occurs regardless of the starting point. Usually, one throws out
a certain number of initial draws. This is known as the ‘burn-in’ of the algorithm

100
2
Statistical Aspects
Fig. 2.56 Example of
Markov Chain result
0
10
20
30
40
50
60
1
1.5
2
2.5
3
n
The research has already provided a number of methods for driving the chain to
the desired PDF. See for instance the handbook [15]. The key contribution to start
all this activity was the Metropolis algorithm, which is recognized as one of the ten
most inﬂuential algorithms proposed in the 20th century [7]. According with [28],
one could tell of a MCMC revolution. A brief history of this revolution is reported
in [77], and with more extension in [75].
As background literature on MCMC, a brief introduction is [95], while a more
extended introduction is [4]. Details of the rationale behind MCMC can be found in
[23]. More extended texts are [9, 84, 93]. In addition, the academic literature from
[76, 90] includes MATLAB programs.
2.10.2.1
Metropolis Algorithm
The goal is to draw samples from some p(x) PDF, where p(x) = f (x) / K, and K is
not known.
According with the algorithm introduced by Nicholas Metropolis in 1953, a pro-
posal distribution (also called jumping distribution) q(y|x) is chosen. This distribution
corresponds to the transition probabilities of a Markov chain.
The generation of samples starts from an initial value x0,
(a) Draw a sample from q(y|x) →x1
(b) Compute:
α = p(x1)
p(x0) = f (x1)
f (x0)
(2.131)
(the constant K cancels out)

2.10 Markov Process
101
(c) If α >1 accept x1 as new sample;
else, with probability α accept x1,
else reject it and take x1 = x0 as new sample
(d) Back to (a) until sufﬁcient number of samples has been obtained.
Unlike rejection sampling, when a sample is rejected we do not try again until
one is accepted, we just let x1 = x0 and continue with the next time step.
The Metropolis algorithm uses a symmetric proposal distribution:
q(y|x) = q(x|y)
(2.132)
2.10.2.2
Metropolis–Hastings Algorithm
Hastings generalized in 1970 the Metropolis algorithm, taking an arbitrary q(y|x),
possibly non-symmetric, and using the following acceptance probability:
α = min
f (x1) q(x1|x0)
f (x0) q(x0|x1), 1

(2.133)
(the Metropolis algorithm takes: α = min( f (x1)
f (x0) , 1))
See [23], and references therein, for different implementation strategies for the
Metropolis–Hastings algorithm.
2.10.2.3
Example
Let us consider for example a desired PDF with a half-sine shape. We choose a
Gaussian PDF for the proposal distribution. Figure 2.57 depicts the scenario with the
desired (D) PDF and the proposal (P) PDF.
Program 2.42 provides an implementation of the Metropolis algorithm for the
example just described. Two ﬁgures are generated. The ﬁrst is Fig.2.57 with the
desired and the proposal PDFs. The second is Fig.2.58 that shows the histogram of
draws obtained by the Metropolis algorithm, which agrees with the desired PDF.
Program 2.42 Generation of random data with a desired PDF
% Generation of random data with a desired PDF
% Using MCMC
% Metropolis algorithm
% example of desired PDF
x=0:(pi/100):pi;
dpf=0.5*sin(x); % desired PDF
% example of proposal PDF (normal)
xp=-4:(pi/100):4;
sigma=0.6; %deviation
q=normpdf(xp,0,sigma);

102
2
Statistical Aspects
Fig. 2.57 The case
considered in the Metropolis
example
-2
-1
0
1
2
3
4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
x
P
D
Fig. 2.58 Histogram of
random data generated by
the Metropolis algorithm
0
0.5
1
1.5
2
2.5
3
3.5
0
50
100
150
200
250
x
% generation of random data
N=5000; %number of data
z=zeros(1,N); %space for data to be generated
x0=pi/2; %initial value
for nn=1:N,
inr=0;
while inr==0, %new value proposal (Markovian transition)
x1=x0+(sigma*randn(1)); %normal distribution (symmetric)
if (x1<pi) & (x1>0),
inr=1; %x1 is valid (is inside dpf domain)
end;
end;
f1=0.5*sin(x1);

2.10 Markov Process
103
f0=0.5*sin(x0);
alpha=f1/f0;
if alpha>=1
z(nn)=x1; %accept
else
aux=rand(1);
if aux<alpha,
z(nn)=x1; %accept
else
z(nn)=x0;
end;
end;
x0=x1;
end;
nz=z(1000:5000); %eliminate initial data
figure(1)
plot(x,dpf,'k'); hold on;
plot(xp,q,'r');
axis([-2 4, 0 0.8]);
xlabel('x'); title('desired PDF and proposal PDF');
figure(2)
hist(nz,30); colrmap('cool');
xlabel('x');title('histogram of the generated data');
TheMATLABStatisticsToolboxincludesthemhsample()andslicesample()func-
tions for using MCMC.
2.10.3
Hidden Markov Chain (HMM)
Consider the case of a Markov chain where states emit certain observable variables.
Figure 2.59 shows a simplistic model of speech, the Markov chain has two states:
(1) wovel and (2) consonant. When the process is in state 1, it could emit one of the
three consonants W, H, or T, or a spacing _. When the process is in state 2, it could
emit one of the four wovels A, E, O, U. The emission of any observable is made with
an assigned probability.
Hence, in this example, there is a ‘hidden Markov chain’ (HMM) with two states,
and eight observables. The fundamental reference on HMM is [72]. Brief introduc-
tions are given in [29, 85].
This kind of process is being useful for the study of languages, genetics (bioin-
formatics), and other important ﬁelds, [24, 31, 48, 100].
Program B.2 implements the HMM process depicted in the previous diagram
(Fig.2.59). The results of one experiment running this program is shown in Fig.2.60,
with two subplots. The subplot on top corresponds to the hidden Markov chain
transitions. The other subplot goes through four possible states (1, 2, 3, 4), according
with the obervables emitted during the experiment.
The Program B.2 has been included in the Appendix for long programs. The
program also prints the ‘synthetic speech’ generated by the HMM. It is possible, from
public domain, to obtain data on transition probabilities of real human languages.

104
2
Statistical Aspects
1
2
H
0.7
0.5
0.2
0.5
0.3
0.2
0.3
0.3
0.2
consonant
wovel
T
O
W
_
E
A
U
0.2
0.3
0.3
Fig. 2.59 An example of HMM (speech generator)
0
10
20
30
40
50
60
1
1.5
2
2.5
HMM (synthetic speech): the hidden states
n
0
10
20
30
40
50
60
1
2
3
4
5
the emissions
n
Fig. 2.60 Results of HMM example
In order to give a more complete idea of HMM, Fig.2.61 shows another diagram.
It is about the habits of someone, as the day is sunny or there is rain. Notice that for
instance, this person like to walk under the Sun, and also (not so much) to walk in
the rain.
The description of an HMM can be made with the transition matrix of the hidden
Markov chain and with a matrix of observation probabilities. Continuing with the
example, this last matrix corresponds to the following table:
Observations
(1)Swim (2)Walk
(3)Shop (4)TV
States 1 0.2
0.5
0.3
0
2 0
0.2
0.4
0.4

2.10 Markov Process
105
Fig. 2.61 Another HMM
example
1
2
Swim
0.5
0.4
0.4
0.6
0.5
0.4
Sun
Rain
TV
Walk
Shop
0.2
0.5
0.3
0.2
1
2
Swim
0.5
0.4
0.4
0.6
0.5
0.4
Sun
Rain
TV
Walk
Shop
0.2
0.5
0.3
0.2
Fig. 2.62 An experiment
with the HMM example
(S1=1)
(O1=2)
(S4=1)
(O2=3)
(O3=3)
(O4=4)
(S2=1)             (S3=2)
Fig. 2.63 A generic HMM
path
S1
O1
S2
S3
S4
O2
O3
O4
S1
O1
S2
S3
S4
O2
O3
O4
Figure2.62 shows an example of experiment running the HMM. The process
advances through the Markov chain states, and it is observed by a sequence of
emissions.
For example, suppose you are studying a series of archaeological strata, from
a series of observables you might be interested in guessing if there were climate
changes along certain epochs.
Figure2.63 depicts a more abstract diagram showing the general behavior of the
HMM along time. When you give values to states and observables, you describe a
particular path of the process.
An interesting set of HMM application examples is given in [74]. Other published
applications are, [88] on video background modeling, [20] on folk music classiﬁca-
tion, and [25] on classiﬁcation of continuous heart sound signals.

106
2
Statistical Aspects
2.11
MATLAB Tools for Distributions
The MATLAB Statistics Toolbox provides an interactive graph of PDF for many
probability distributions. In response to the MATLAB prompt, the user writes:
disttool
And the screen shown in Fig.2.64 will appear.
The user may select one of the many types of distributions included in the tool,
and choose the visualization of the CDF or the PDF. Distribution parameters can be
changed in order to observe their effects.
Another tool of interest is:
randtool
In response to this, the screen shown in Fig.2.65 will appear.
The randtool will obtain samples of the PDF selected by the user, and visualize
the corresponding histogram.
In other order of things, it is convenient to mention the interest of the MATLAB
function boxplot( ) for the display of statistical box plots. See the web site (https://
plot.ly/matlab/box-plots/) for interesting examples.
Fig. 2.64 Initial disttool screen

2.11 MATLAB Tools for Distributions
107
Fig. 2.65 Initial randtool screen
Figure2.66 shows an example of box plot. The ﬁgure has been generated with the
Program 2.43, which also contains the data being visualized.
Each box is used to indicate the position of the upper and lower quartiles. There
is a crossbar inside the box that indicates the median. The extrema of the distribution
are indicated with dashed lines and markers. See [69] for more details.
Program 2.43 Example of box plots
%Example of box plots
data=[1 5 8 3;
3 2 1 5;
5 4 8 1;
9 12 1 3;
14 0 2 2;
7 9 1 3];
median(data) %median of each column
mean(data) %mean of each column
std (data) % standard deviation of each column
figure(1)
boxplot(data)
title('box plot example')

108
2
Statistical Aspects
Fig. 2.66 Example of box
plot
1
2
3
4
0
2
4
6
8
10
12
14
Values
Column Number
2.12
Resources
2.12.1
MATLAB
2.12.1.1
Toolboxes
• Exploratory Data Analysis Toolbox (EDA):
http://cda.psych.uiuc.edu/martinez/edatoolbox/Docs/Contents.htm
• Bayes Net Toolbox:
https://code.google.com/p/bnt/
• Bayes Net Toolbox for Student Modeling:
http://www.cs.cmu.edu/~listen/BNT-SM/
• Markov Decision Processes (MDP) Toolbox:
http://www7.inra.fr/mia/T/MDPtoolbox/
• MCMC Toolbox for Matlab:
http://helios.fmi.ﬁ/~lainema/mcmc/
• MCMC Methods for MLP and GP and Stuff (Aalto Univ.):
http://becs.aalto.ﬁ/en/research/bayes/mcmcstuff/
• Hidden Markov Model (HMM) Matlab Toolbox:
http://nuweb.neu.edu/bbarbiellini/CBIO3580/HW7.html
• Mendel HMM Toolbox for Matlab:
http://www.math.uit.no/bi/hmm/
• Stochastic Processes Toolkit for Risk Magement:
http://www.damianobrigo.it/toolboxweb.pdf
• CompEcon Toolbox for Matlab (economics and ﬁnance):
http://www4.ncsu.edu/~pfackler/compecon/toolbox.html

2.12 Resources
109
2.12.1.2
Links to Toolboxes
• Van Horn (Bayesian statistical inference):
http://ksvanhorn.com/bayes/free-bayes-software.html
• Graphical Models (Bayesian):
http://fuzzy.cs.uni-magdeburg.de/books/gm/tools.html
• Tools (Bayesian matters):
http://www.cs.iit.edu/~mbilgic/classes/fall10/cs595/tools.html
2.12.1.3
Matlab Code
• Educational MATLAB GUIs (demos):
http://users.ece.gatech.edu/mcclella/matlabGUIs/
• Advanced Box Plot for Matlab (Alex Bikfalvi):
http://alex.bikfalvi.com/research/advanced_matlab_boxplot/
• Bayesian Statistics:
http://www2.isye.gatech.edu/~brani/isyebayes/programs.html
• Matlab listings for Markov chains (Renato Feres):
http://www.math.wustl.edu/~feres/Math450Lect04.pdf
• Matlab Code: Tutorial 1: Creating a Bayesian Network:
https://dslpitt.org/genie/wiki/Matlab_Code:_Tutorial_1:_Creating_a_Bayesian
_Network
• CGBayesNets (Gaussian Bayesian networks):
http://www.cgbayesnets.com/
• Monte Carlo Methods (G. Gordon):
http://www.cs.cmu.edu/~ggordon/MCMC/
• MCMC (Kevin Murphy):
http://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/mcmc.pdf
• Handbook of Monte Carlo Methods (D.P. Kroese et al.):
http://www.maths.uq.edu.au/~kroese/montecarlohandbook/
2.12.2
Web Sites
• Stanford Encyclopedia of Philosophy (Bayes’ Theorem):
http://plato.stanford.edu/entries/bayes-theorem/
• STAT 504 PennState Univ.:
https://onlinecourses.science.psu.edu/stat504
• scikit-learn:
http://scikit-learn.org/dev/index.html
• Graphics_Examples
(sample
data
for
graphics
demonstrations):
https://people.sc.fsu.edu/~jburkardt/m_src/graphics_examples/graphics
_examples.html

110
2
Statistical Aspects
• Probvis; K. Potter (visualization of distribution functions.):
https://people.sc.fsu.edu/~jburkardt/
• John Burkardt (Matlab codes, examples, etc.):
https://people.sc.fsu.edu/~jburkardt/
• Bayes Nets:
http://www.bayesnets.com/
• The Gaussian Processes Web Site:
http://www.gaussianprocess.org/
• Belief Networks:
https://www.cis.upenn.edu/~ungar/KDD/belief-nets.html
References
1. M.A. Al-Fawzan, Methods for Estimating the Parameters of TheWeibull Distribution (King
Abdulaziz City for Science and Technology, Saudi Arabia, 2000). http://interstat.statjournals.
net/YEAR/2000/articles/0010001.pdf
2. S.J. Almalki, S. Nadarajah, Modiﬁcations of the Weibull distribution: A review. Reliab. Eng.
Syst. Saf. 124, 32–55 (2014)
3. E. Anderson, Monte Carlo Methods and Importance Sampling. Lecture Notes, UC Berkeley
(1999). http://ib.berkeley.edu/labs/slatkin/eriq/classes/guest_lect/mc_lecture_notes.pdf
4. C. Andrieu, N. De Freitas, A. Doucet, M.I. Jordan, An introduction to MCMC for machine
learning. Mach. Learn. 50(1–2), 5–43 (2003)
5. G. Arroyo-Figueroa, L.E. Suear, A temporal Bayesian network for diagnosis and prediction,
in Proceedings 15th Conference Uncertainty in Artiﬁcial Intelligence (Morgan Kaufmann
Publishers Inc, 1999), pp. 13–20
6. A. Assenza, M. Valle, M. Verleysen, A comparative study of various probability density
estimation methods for data analysis. Int. J. Comput. Intell. Syst. 1(2), 188–201 (2008)
7. I. Beichl, F. Sullivan, The Metropolis algorithm. Comput. Sci. Eng. 2(1), 65–69 (2000)
8. I. Ben-Gal, Bayesian networks, in Encyclopedia of Statistics in Quality & Reliability, ed. by
F. Faltin, R. Kenett, F. Ruggeri (Wiley, Chichester, 2007)
9. M. Bergomi, C. Pedrazzoli, Bayesian Statistics: Computational Aspects. Lecture Notes, ETH
Zurich (2008). http://www.rw.ethz.ch/dmath/research/groups/sfs-old/teaching/lectures/FS_/
seminar/8.pdf
10. J.M. Bernardo, A.F.M. Smith, Bayesian Theory (Wiley, New York, 2000)
11. G. Bohling, Applications of Bayes’ Theorem. Lecture Notes, Kansas Geological Surveys
(2005). http://people.ku.edu/~gbohling/cpe940/BayesOverheads.pdf
12. S. Borak, W. Härdle, R. Weron, Stable distributions, in Statistical Tools for Finance and
Insurance (2005), pp. 21–44
13. G.E. Box, M.E. Muller, A note on the generation of random normal deviates. Ann. Math. Stat.
29, 610–611 (1958)
14. L. Breuer. Introduction to Stochastic Processes. Lecture Notes, Univ. Kent (2014). https://
www.kent.ac.uk/smsas/personal/lb209/ﬁles/notes1.pdf
15. S. Brooks, A. Gelman, G.L. Jones, X.-L. Meng, Handbook of Markov Chain Mote Carlo
(Chapman and Hall/CRC, Boca Raton, 2011)
16. H. Bruyninckx, Bayesian Probability. Lecture Notes, KU Leuven, Belgium (2002). http://
www.stats.org.uk/bayesian/Bruyninckx.pdf
17. R. Cano, C. Sordo, J.M. Gutiérrez, Applications of Bayesian networks in meteorology, in
Advances in Bayesian Networks, ed. by J.A. Gamez, et al. (Springer, Berlin, 2004), pp. 309–
328

References
111
18. A.N. Celik, A statistical analysis of wind power density based on the Weibull and Rayleigh
models at the southern region of Turkey. Renew. Energy 29(4), 593–604 (2004)
19. V. Cevher, Importance Sampling. Lecture Notes, Rice University (2008). http://www.ece.rice.
edu/~vc3/elec633/ImportanceSampling.pdf
20. W. Chai, B. Vercoe, Folk music classiﬁcation using hidden Markov models, in Proceedings
of International Conference on Artiﬁcial Intelligence, vol. 6, no. 4 (2001)
21. E. Charniak, Bayesian networks without tears. AI Mag. 12(4), 50–63 (1991)
22. J. Cheng, R. Greiner, Learning bayesian belief network classiﬁers: algorithms and system, in
Advances in Artiﬁcial Intelligence, ed. by M. Stroulia (Springer, Berlin, 2001), pp. 141–151
23. S. Chib, E. Greenberg, Understanding the Metropolis-Hastings algorithm. Am. Stat. 49(4),
327–335 (1995)
24. K.H. Choo, J.C. Tong, L. Zhang, Recent applications of hidden Markov models in computa-
tional biology. Genomics Proteomics Bioinf. 2(2), 84–96 (2004)
25. Y.J. Chung, Classiﬁcation of continuous heart sound signals using the ergodic hidden Markov
model. Pattern Recogn. Image Anal. 563–570 (2007)
26. R. Daly, Q. Shen, S. Aitken, Learning Bayesian networks: approaches and issues. Knowl.
Eng. Rev. 26(2), 99–157 (2011)
27. L. Devroye, Sample-based non-uniform random variate generation, in Proceedings ACM 18th
Winter Conference on Simulation (1986), pp. 260–265
28. P. Diaconis, The Markov chain Monte Carlo revolution. Bull. Am. Math. Soc. 46(2), 179–205
(2009)
29. S.R. Eddy, What is a hidden Markov model? Nat. Biotechnol. 22(10), 1315–1316 (2004)
30. S.E. Fienberg, When did Bayesian inference become “Bayesian”? Bayesian Anal. 1(1), 1–40
(2006)
31. M. Gales, S. Young, The application of hidden Markov models in speech recognition. Found.
Trends Sig. Process. 1(3), 195–304 (2008)
32. J. Gemela, Financial analysis using Bayesian networks. Appl. Stoch. Models Bus. Ind. 17(1),
57–67 (2001)
33. M. Haugh, Generating Random Variables and Stochastic Processes. Lecture Notes, Columbia
Univ (2010). http://www.columbia.edu/~mh2078/MCS_Generate_RVars.pdf
34. D. Heckerman, A Tutorial on Learning with Bayesian Networks (Springer, Netherlands, 1998)
35. C.C. Heckman, Matrix Applications: Markov Chains and Game Theory. Lecture Notes, Ari-
zona State Univ (2015). https://math.la.asu.edu/~checkman/MatrixApps.pdf
36. S. Holmes, Maximum Likelihood Estimation. Lecture Notes, Stanford Univ. (2001). http://
statweb.stanford.edu/~susan/courses/s200/lect11.pdf
37. S. Holmes, The Methods of Moments. Lecture Notes, Stanford Univ. (2001). http://statweb.
stanford.edu/~susan/courses/s200/lect8.pdf
38. R.J. Hoppenstein, Statistical Reliability Analysis on Rayleigh Probability Distributions
(2000). www.rfdesign.com
39. S. Intajag, S. Chitwong, Speckle noise estimation with generalized gamma distribution, in
Proceedings of IEEE International Joint Conference SICE-ICASE (2006), pp. 1164–1167
40. D. Joyce, Common Probability Distributions. Lecture Notes, Clark University (2006)
41. K. Knight. Central Limit Theorems. Lecture Notes, Univ. Toronto (2010). www.utstat.toronto.
edu/keith/eco2402/clt.pdf
42. T. Konstantopoulos, Markov Chains and Random Walks. Lecture Notes (2009). http://159.
226.43.108/~wangchao/maa/mcrw.pdf
43. K.B. Korb, A.E. Nicholson, Bayesian Artiﬁcial Intelligence (CRC Press, Boca Raton, 2010)
44. M. Kozdron, The Deﬁnition of a Stochastic Process. Lecture Notes, Univ. Regina (2006).
http://stat.math.uregina.ca/~kozdron/Teaching/Regina/862Winter06/Handouts/revised_
lecture1.pdf
45. M.L. Krieg, A tutorial on Bayesian belief networks. Technical Report DSTO-TN-0403 (2001).
http://dspace.dsto.defence.gov.au/dspace/handle/1947/3537
46. D.P. Kroese, A Short Introduction to Probability. Lecture Notes, Univ. Queensland (2009).
http://www.maths.uq.edu.au/~kroese/asitp.pdf

112
2
Statistical Aspects
47. D.P. Kroese, T. Brereton, T. Taimre, Z.I. Botev, Why the Monte Carlo method is so important
today. Wiley Interdisciplinary Reviews: Computational Statistics 6(6), 386–392 (2014)
48. A. Krogh, M. Brown, I.S. Mian, K. Sjölander, D. Haussler, Hidden Markov models in com-
putational biology: applications to protein modeling. J. Mol. Biol. 235(5), 1501–1531 (1994)
49. E. Limpert, W.A. Stahel, M. Abbt, Log-normal distributions across the sciences: keys and
clues. Bioscience 51(5), 341–352 (2001)
50. R. Linna, Monte Carlo Methods I. Lecture Notes, Aalto University (2012). http://www.lce.
hut.ﬁ/teaching/S-114.1100/lect_9.pdf
51. P. Lucas, Bayesian Networks in Medicine: A Model-based Approach to Medical Decision
Making. Lecture Notes, Univ. Aberdeen (2001). http://cs.ru.nl/~peterl/eunite.pdf
52. D.J. MacKay, Introduction to Monte Carlo methods, in Learning in Graphical Models, ed.
by M.I. Jordan (Springer, Berlin, 1998), pp. 175–204
53. V. Manian, Image Processing: Image Restoration. Lecture Notes, Univ. Puerto Rico (2009).
www.ece.uprm.edu/~manian/chapter5IP.pdf
54. D. Margaritis, Learning Bayesian Network Model Structure from Data. Ph.D. thesis, US Army
(2003)
55. W.L. Martinez, A.R. Martines, Computational Statistics Handbook with MATLAB (Chapman
& Hall/CRC, Boca Raton, 2007)
56. G.M. Masters, Renewable and Efﬁcient Electric Power Systems (Wiley, New York, 2013)
57. I.J. Myung, Tutorial on maximum likelihood estimation. J. Math. Psychol. 47(1), 90–100
(2003)
58. R.E. Neapolitan, Learning Bayesian Networks, vol. 38 (Prentice Hall, Upper Saddle River,
2004)
59. T.D. Nielsen, F.V. Jensen, Bayesian Networks and Decision Graphs (Springer Science &
Business Media, New York, 2009)
60. D. Nikovski, Constructing bayesian networks for medical diagnosis from incomplete and
partially correct statistics. IEEE T. Knowl. Data Eng. 12(4), 509–516 (2000)
61. J.P. Nolan, Stable Distributions, Chap1. Lecture Notes, American University (2014). http://
academic2.american.edu/~jpnolan/stable/chap1.pdf
62. F.N. Nwobi, C.A. Ugomma, A comparison of methods for the estimation of Weibull distrib-
ution parameters. Adv. Method. Stat./Metodoloski zvezki 11(1), 65–78 (2014)
63. P. Olofsson, M. Andersson, Probability, Statistics, and Stochastic Processes (Wiley, Chich-
ester, 2012)
64. B.A. Olshausen, Bayesian Probability Theory. Lecture Notes, UC. Berkeley (2004). http://
redwood.berkeley.edu/bruno/npb163/bayes.pdf
65. A.B. Owen, Monte Carlo Theory, Methods and Examples. Lecture Notes, Stanford Univ.
(2013). Book in progress. http://statweb.stanford.edu/~owen/mc/
66. C. Pacati, General Sampling Methods. Lecture Notes, Univ. Siena (2014). http://www.econ-
pol.unisi.it/ﬁneng/gensampl_doc.pdf
67. M. Pinsky, S. Karlin, An Introduction to Stochastic Modeling (Academic Press, Cambridge,
2010)
68. C.A. Pollino, C. Henderson, Bayesian Networks: A Guide for Their Application in Natural
Resource Management and Policy. Technical Report 14, Landscape Logicpp. (2010)
69. K. Potter, Methods for Presenting Statistical Information: The Box Plot. Lecture Notes, Univ.
Utah (2006). http://www.kristipotter.com/publications/potter-MPSI.pdf
70. O. Pourret, P. Naïm, B. Marcot (eds.), Bayesian Networks: A Practical Guide to Applications,
vol. 73 (Wiley, Chichester, 2008)
71. R forge distributions Core Team. A guide on probability distributions. Technical report (2009).
http://dutangc.free.fr/pub/prob/probdistr-main.pdf
72. L. Rabiner, A tutorial on hidden markov models and selected applications in speech recogni-
tion. Proc. IEEE 77(2), 257–286 (1989)
73. R. Ramamoorthi, Monte Carlo Integration. Lecture Notes, U.C. Berkeley (2009). https://inst.
eecs.berkeley.edu/~cs/fa09/lectures/scribe-lecture4.pdf

References
113
74. N. Ramanathan, Applications of Hidden Markov Models. Lecture Notes, University of Mary-
land (2006). http://www.cs.umd.edu/~djacobs/CMSC828/ApplicationsHMMs.pdf
75. M. Richey, The evolution of Markov chain Monte Carlo methods. Am. Math. Mon. 117(5),
383–413 (2010)
76. B.D. Ripley, Computer-Intensive Statistics. Lecture Notes, Oxford Univ (2008). http://www.
stats.ox.ac.uk/~ripley/APTS2012/APTS-CIS-lects.pdf
77. C. Robert, G. Casella, A short history of Markov chain Monte Carlo: Subjective recollections
from incomplete data. Stat. Sci. 26(1), 102–115 (2011)
78. S. Stahl, The evolution of the normal distribution. Math. Mag. 79(2), 96–113 (2006)
79. F.W. Scholz, Central Limit Theorems and Proofs. Lecture Notes, Univ. Washington (2011).
http://www.stat.washington.edu/fritz/DATAFILES394_/CLT.pdf
80. R. Seydel, Tools for Computational Finance (Springer, Berlin, 2012)
81. S.J. Sheather, Density estimation. Stat. Sci. 19(4), 588–597 (2004)
82. K. Sigman, Acceptance-Rejection Method. Lecture Notes, Columbia University (2007). www.
columbia.edu/~ks20/4703-Sigman-Notes-ARM.pdf
83. B.W. Silverman, Density Estimation for Statistics and Data Analysis (2002). https://ned.ipac.
caltech.edu/level5/March02/Silverman/paper.pdf
84. M. Sköld, Computer Intensive Statistical Methods. Lecture Notes, Lund University (2006)
85. M. Stamp, A Revealing Introduction to Hidden Markov Models. Lecture Notes,
San Jose State University (2012). http://gcat.davidson.edu/mediawiki-1.19.1/images/2/23/
HiddenMarkovModels.pdf
86. A. Stassopoulou, M. Petrou, J. Kittler, Application of a Bayesian network in a GIS based
decision making system. Int. J. Geogr. Inf. Sci. 12(1), 23–46 (1998)
87. V. Stelzenmüller, J. Lee, E. Garnacho, S.I. Rogers, Assessment of a Bayesian belief network-
GIS framework as a practical tool to support marine planning. Mar. Pollut. Bull. 60(10),
1743–1754 (2010)
88. B. Stenger, V. Ramesh, N. Paragios, F. Coetzee, J.M. Buhmann, Topology free hidden Markov
models: Application to background modeling, in Proceedings of IEEE International Confer-
ence on Computer Vision, vol. 1 (2001), pp. 294–301
89. T.A. Stephenson, An introduction to bayesian network theory and usage. Technical report,
IDIAP Research, Switzerland (2000). http://publications.idiap.ch/downloads/reports/2000/
rr00-03df
90. M. Steyvers, Computational Statistics with MATLAB. Lecture Notes, UC Irvine (2011). http://
www.cidlab.com/205c/205C_v4.pdf
91. D.B. Thomas, W. Luk, P.H. Leong, J.D. Villasenor, Gaussian random number generators.
ACM Comput. Surv. (CSUR) 39(4), 11 (2007)
92. C. Tomasi, Estimating Gaussian Mixture Models with EM. Lecture Notes, Duke University
(2004). https://www.cs.duke.edu/courses/spring04/cps196.1/handouts/EM/tomasiEM.pdf
93. B. Vidakovic, MCMC Methodology. Lecture Notes, Georgia Tech (2014). http://www2.isye.
gatech.edu/~brani/isyebayes/bank/handout10.pdf
94. P.VonHilgers,A.N.Langville,TheﬁvegreatestapplicationsofMarkovchains,inProceedings
of the Markov Anniversary Meeting (Boston Press, Boston, 2006)
95. R. Waagepetersen, A quick introduction to Markov chains and Markov chain Monte Carlo
(revised version). Aalborg Univ. (2007). http://people.math.aau.dk/~rw/Papers/mcmc_intro.
pdf
96. C. Walk, Hand-book of statistical distributions for experimentalists. Technical report, Uni-
versity of Stockholm (2007). Internal Report
97. G. Wang, Q. Dong, Z. Pan, X. Zhao, J. Yang, C. Liu, Active contour model for ultrasound
images with Rayleigh distribution. Mathematical Problems in Engineering, ID 295320 (2014)
98. J.C. Watkins, Maximum Likelihood Estimation. Lecture Notes, University of Arizona (2011).
http://math.arizona.edu/~jwatkins/o-mle.pdf
99. P. Weber, G. Medina-Oliva, C. Simon, B. Iung, Overview on Bayesian networks applications
for dependability, risk analysis and maintenance areas. Eng. Appl. Artif. Intell. 25(4), 671–682
(2012)

114
2
Statistical Aspects
100. B.J. Yoon, Hidden Markov models and their applications in biological sequence analysis.
Curr. Genomics 10(6), 402–415 (2009)
101. G.A. Young, M2S1 Lecture Notes. Lecture Notes, Imperial College London (2011). www2.
imperial.ac.uk/~ayoung/m2s1/M2S1.PDF
102. A.Z. Zambom, R. Dias, A Review of Kernel Density Estimation with Applications to Econo-
metrics (2012). arXiv preprint arXiv:1212.2812
103. L. Zhang, Applied Statistics I. Lecture Notes, University of Utah (2008). http://www.math.
utah.edu/~lzhang/teaching/3070summer/DailyUpdates/jul1/lecture_jul1.pdf

Part II
Filtering

Chapter 3
Linear Systems
3.1
Introduction
Many times signal ﬁltering is done with linear ﬁlters, which are an important instance
of linear systems. We are going to study ﬁrst the linear systems, with special emphasis
in system response to input signals. The next two chapters will be devoted to different
types of linear ﬁlters.
Linear systems are those systems that satisfy the properties of superposition and
scaling, [12].
The transfer function is a popular alternative for the representation of linear sys-
tems, [5, 26]. Another way of representation is by using state-space equations, which
can easily deal with multivariable systems, [8, 20, 23]. In cases where a statisti-
cal approach is opportune, the time-series framework provides a convenient sys-
tem representation, [11, 25, 27]. Therefore, we have three main system modelling
approaches that will be considered in this chapter.
Some of the MATLAB functions that will be used in our examples belong to the
Control System Toolbox. We shall indicate this with (*C).
3.2
Examples About Transfer Functions
3.2.1
A Basic Low-Pass Electronic Filter
A basic example of electronic ﬁlter is shown in Fig.3.1. This example will be denoted
as “example A”.
The behaviour of the circuit can be easily analyzed in the time domain: ﬁrst we
write the following differential equation:
R C d Vo
dt
= Vi −Vo
(3.1)
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_3
117

118
3
Linear Systems
Fig. 3.1 Example of
electronic ﬁlter
R
C
Vi
Vo
R
C
Vi
Vo
Fig. 3.2 Solution of
equation (3.1)
seconds
Vo
0
1
2
3
4
5
6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
From: U(1)
To: Y(1)
then we ﬁnd the analytical solution of (3.1)
Vo(t) = Vi (1 −e−t/RC)
(3.2)
Figure3.2 plots this solution: it is an exponential curve that approaches asymptoti-
cally the input value Vi, with a time constant Ta = RC. The curve depicts the typical
process of capacitor charging.
By using the Laplace transform (see Appendix A), Eq.(3.1) can be transformed
to:
R C s Vo(s) = Vi(s) −Vo(s)
(3.3)
From this equation in the complex s variable, we can obtain the transfer function (an
output/input relationship) of the electronic ﬁlter:
G(s) = Vo(s)
Vi(s) =
1
1 + RCs
(3.4)
The solution given by (3.2) can also be obtained departing from (3.4) and using tables
of Laplace transforms.
FortheanalysisofmorecomplexcircuitsitisconvenienttouseLaplacetransforms
of the impedances of the components:

3.2 Examples About Transfer Functions
119
• The impedance of R (resistor) is R
• The impedance of L (inductance) is Ls
• The impedance of C (capacitor) is 1/(Cs)
For instance, continuing with the same example we can write:
Vi(s) = R I(s) + ( 1/(Cs)) I(s)
(3.5)
Vo(s) = ( 1/(Cs)) I(s)
(3.6)
Now, from (3.5) and (3.6), we obtain the transfer function:
G(s) = Vo(s)
Vi(s) =
1/Cs
R + 1/Cs =
1
1 + RCs
(3.7)
In the next section it will be shown that this transfer function corresponds to a low-
pass ﬁlter.
3.2.2
A Basic Resonant Electronic Filter
Another basic example of electronic ﬁlter is depicted in Fig.3.3. It includes an induc-
tance L and a capacitor C; the combinations of these two components are the heart of
many oscillator circuits and put on scene the resonance phenomenon. This example
will be denoted as “example B”.
Let us establish the transfer function of the electronic ﬁlter; ﬁrst we write the
following equations:
Vi(s) = Ls I(s) + ( 1/(Cs)) I(s) + R I(s)
(3.8)
Vo(s) = R I(s)
(3.9)
Fig. 3.3 Another example
of electronic ﬁlter
R
C
Vi
Vo
L
R
C
Vi
Vo
L

120
3
Linear Systems
Like in the previous example, now, from (3.8) and (3.9), we obtain the transfer
function:
G(s) = Vo(s)
Vi(s) =
R
Ls + 1/Cs + R =
RCs
LCs2 + RCs + 1
(3.10)
In the next section it will be shown that this transfer function corresponds to a band-
pass ﬁlter.
3.3
Response of Continuous Linear Systems
In this section the response of continuous linear systems to different types of inputs
is considered. The transfer function provides a straight way to study this main issue.
The output Y(s) of the system with transfer function G(s) to any input U(s), is simply
given by:
Y(s) = G(s) U(s)
(3.11)
The section is divided into two parts: ﬁrst we look at the frequency domain, and
then we look at the time domain. Before that, it is opportune to note that a transfer
function can be expressed in function of the numerator and denominator roots as
follows:
G(s) = K (s −z1) (s −z2) ... (s −zm)
(s −p1) (s −p2) ... (s −pn)
(3.12)
where z1, z2…zm are the zeros of G(s), and p1, p2, …, pn are the poles of G(s).
Suppose for example that you are handling the following transfer function:
G(s) =
s2 + 5s + 3
s3 + 3s + 25
(3.13)
In order to use MATLAB for the study of this G(s), the ﬁrst step is to specify this
transfer function in MATLAB terms:
G = tf ([1 5 3], [1 0 3 25]);
We used the tf() (*C) function, specifying into brackets the numerator and the
denominator of G(s) (notice, in these brackets, the spaces between numbers).
Now we can write a MATLAB program (Program 3.1) to plot on the complex
plane, Fig.3.4, the poles and zeros of G(s). The program uses the pzmap() (*C)
function: poles are represented with x, zeros are represented with small circles.

3.3 Response of Continuous Linear Systems
121
Fig. 3.4 Visualization of
poles and zeros of G(s) on
the complex plane
Real Axis
Imag Axis
-5
-4
-3
-2
-1
0
1
2
-3
-2
-1
0
1
2
3
Program 3.1 Pole-zero map of G(s)
% Pole-zero map of G(s)
G=tf([1 5 3],[1 0 3 25]); %the transfer function G(s)
pzmap(G); %pole-zero map on the complex plane
title(‘pole-zero map’);
3.3.1
Frequency Response
Let us apply a sinusoidal input u = sin (ωt) to the system with transfer function
G(s). It can be shown that, once the steady-state has been reached, the output of the
system would be:
y = |G(jω)| · sin(ω t + ϕ)
(3.14)
where G(jω) is obtained from G(s) by changing s by jω in the expression of G(s).
The phase φ is given by:
tg ϕ = Im G(jω)
Re G(jω)
(3.15)
In words: the output of G(s) is another sinusoidal signal with the same frequency as
the sinusoidal input; input and output having a difference in phase. Amplitude and
phase of the output is determined by G(jω).
The complex function G(jω) is called the “frequency response” of the system.
Figure3.5 shows on top a sinusoidal input to a linear system G(s), and below
the corresponding output. We marked with an arrow the delay between sine peaks,

122
3
Linear Systems
0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995
1
-1
-0.5
0
0.5
1
input
0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995
1
-0.01
-0.005
0
0.005
0.01
output
seconds
Fig. 3.5 Comparison of sinusoidal input and output
which corresponds to the phase lag of the output signal with respect to the input
signal. Notice that amplitudes of input and output are different.
There are two traditional ways to visualize the frequency response of a system:
1. Two plots: one with 20 log |G(jω| versus log ω, the other with φ versus log ω.
2. Im G(jω) versus Re G(jω) (a polar plot).
The alternative (a) is called “the Bode plot”, in honour of H.W. Bode. The magni-
tude 20 log |G(jω)| is in decibels (dB). Figure3.6 shows, according with alternative
(a), the frequency response of the ﬁlter example A, with transfer function (3.4) and
R = 1, C = 0.1. The ﬁgure has been obtained with the Program 3.2. The curve in
decibels is the ampliﬁcation of the system in function of frequency. It decreases at
high frequencies. This curve shows that the circuit in Fig.3.1 is a low-pass ﬁlter.
Program 3.2 Frequency response of example A
% Frequency response of example A
R=1; C=0.1; %values of the components
num=[1]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
w=logspace(-1,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
AG=20*log10(abs(G)); %take decibels
FI=angle(G); %take phases (rad/s)
subplot(2,1,1); semilogx(w,AG,’k’); %plots decibels
grid;
ylabel(’dB’); title(’frequency response of example A’)
subplot(2,1,2); semilogx(w,FI,’k’); %plots phases
grid;
ylabel(’rad.’); xlabel(’rad/s’)

3.3 Response of Continuous Linear Systems
123
10
-1
10
0
10
1
10
2
-25
-20
-15
-10
-5
0
dB
10
-1
10
0
10
1
10
2
-1.5
-1
-0.5
0
rad.
rad/s
Fig. 3.6 Visualization of the frequency response of example A
Figure3.7 has been obtained with the Program 3.3 and depicts the frequency
response of the same ﬁlter example A, according with alternative (b). The amplitude
of the output, which is given by |G(jω)|, can be measured for any particular frequency
ωx as the distance from the corresponding curve point G(jωx) to the origin. It can be
seen again that |G(jω)|, decreases as ω increases (the curve tends to the origin).
Program 3.3 Frequency response of example A
% Frequency response of example A
R=1; C=0.1; %values of the components
num=[1]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
w=logspace(-1,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
FI=angle(G); %take phases (rad.)
polar(FI,abs(G)); %plots frequency response
title(’frequency response of example A’)
Now let us study the frequency response of example B with R = 0.5, L = C = 0.1.
Figure3.8 has been generated by the Program 3.4 and shows this frequency response,
using the ﬁrst representation alternative.

124
3
Linear Systems
  0.2
  0.4
  0.6
  0.8
  1
30
210
60
240
90
270
120
300
150
330
180
0
Fig. 3.7 Alternative visualization of the frequency response of example A
10
-1
10
0
10
1
10
2
10
3
-50
-40
-30
-20
-10
0
dB
10
-1
10
0
10
1
10
2
10
3
-2
-1
0
1
2
rad.
rad/s
Fig. 3.8 Visualization of the frequency response of example B
Program 3.4 Frequency response of example B
% Frequency response of example B
R=0.5; C=0.1; L=0.1; %values of the components
num=[R*C 0]; % transfer function numerator;
den=[L*C R*C 1]; %transfer function denominator

3.3 Response of Continuous Linear Systems
125
Fig. 3.9 Alternative
visualization of the
frequency response of
example B
  0.2
  0.4
  0.6
  0.8
  1
30
210
60
240
90
270
120
300
150
330
180
0
w=logspace(-1,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
AG=20*log10(abs(G)); %take decibels
FI=angle(G); %take phases (rad)
subplot(2,1,1); semilogx(w,AG,’k’); %plots decibels
grid;
ylabel(’dB’); title(’frequency response of example B’)
subplot(2,1,2); semilogx(w,FI,’k’); %plots phases
grid;
ylabel(’rad.’); xlabel(’rad/s’)
The other frequency response representation alternative, for the example B, is
shown in Fig.3.9, which has been obtained with the Program 3.5.
Program 3.5 Frequency response of example B
% Frequency response of example B
R=0.5; C=0.1; L=0.1; %values of the components
num=[R*C 0]; % transfer function numerator;
den=[L*C R*C 1]; %transfer function denominator
w=logspace(-1,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
FI=angle(G); %take phases (rad.)
polar(FI,abs(G)); %plots frequency response
title(’frequency response of example B’)

126
3
Linear Systems
Figures3.8 and 3.9 show that the example B corresponds to a band-pass ﬁlter.
There is a maximum peak value of |G(jω)|, for a frequency (Hz) given by:
ωr =
1
2π
√
L C
(3.16)
At this frequency ωr the circuit enters into resonance with the input. The ampliﬁcation
of the ﬁlter rapidly decreases for frequencies up and down ωr: the pass band is narrow.
Notice that φ = 0 at ωr: that corresponds to a pure resistive behaviour (the circuit
behaves at frequency ωr as a resistor).
3.3.2
Time Domain Response
3.3.2.1
Convolution
The y(t) time domain response of a linear system to any input u(t) (with u(t) = 0
for t < 0), is given by:
y(t) =
 t
0
g(τ) u(t −τ) dτ
(3.17)
The expression in (3.17) is called the “convolution” of u and g,[4]. The function g(t)
is the “impulse response” of the system, being the response of the system to an input
δ(t). The function δ(t) is zero everywhere except at t = 0 and is such that:
 a
−a
δ(t) f (t) dt = f (0), for all a > 0
(3.18)
where f (t) is any continuous function.
The Laplace transform of the impulse response g(t) of a linear system is G(s),
the transfer function of the system.
3.3.2.2
Step Response
A typical input test signal is the unit step: u(t) = 0 for t < 0, u(t) = 1 for t ≥0. This
signal is rich in harmonics and is able to excite most interesting aspects of systems
dynamical behaviour.
Let us apply the MATLAB step() (*C) function. This function computes and
displays the y(t) response of the system with transfer function G(s), when an unit
step is applied to the system input.

3.3 Response of Continuous Linear Systems
127
Fig. 3.10 Step response of
ﬁlter example A
Time (sec.)
Amplitude
0
0.1
0.2
0.3
0.4
0.5
0.6
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
From: U(1)
To: Y(1)
Figure3.10 shows the step response of the ﬁlter example A (with R = 1, C = 0.1),
the ﬁgure has been generated by the Program 3.6.
Program 3.6 Step response of example A
% Step response of example A
R=1; C=0.1; %values of the components
num=[1]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
step(G,’k’); %step response of G
title(’step response of example A’)
Now, let us see the step response of the ﬁlter example B (with R = 1, L = C = 0.1).
This response is shown in Fig.3.11, which has been generated by the Program 3.7.
Program 3.7 Step response of example B
% Step response of example B
R=0.5; C=0.1; L=0.1; %values of the components
num=[R*C 0]; % transfer function numerator;
den=[L*C R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
step(G,’k’); %step response of G
title(’step response of example B’)

128
3
Linear Systems
Fig. 3.11 Step response of
ﬁlter example B
Time (sec.)
Amplitude
0
0.5
1
1.5
2
2.5
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
From: U(1)
To: Y(1)
3.3.2.3
Stability
In general the system response to a step has a transient, immediately following the
step, and after some time it may converge to a steady-state value. If it happens –
that the response reaches a steady-state value- the system is “stable”. If the response
becomes larger and larger, with no convergence to a steady-state value, the system is
“unstable”. There is a third possibility: the response becomes a sustained sinusoidal
oscillation; in this case the system is “marginally stable”.
The Laplace transform of a unit step is U(s) = 1/s. In consequence, the step
response of a system G(s) is given by:
Y(s) = G(s)
s
(3.19)
In cases similar to example A, with one pole, the stability can be clearly analyzed.
Let us consider a system with G(s) = −a/(s−a); its step response is:
Y(s) =
−a
s (s −a)
(3.20)
Using partial fraction expansion:
Y(s) = 1
s −
1
s −a
(3.21)
And directly using a table of Laplace transforms, we can obtain the time domain step
response:
y(t) = (1 −eat)
(3.22)

3.3 Response of Continuous Linear Systems
129
stable G(s)
-1
0
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
marginally 
stable G(s)
-1
0
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
unstable G(s)
-1
0
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Fig. 3.12 Three cases of zero-pole maps
In Eq.(3.22) if the pole a < 0, then y(t) converges to 1, so we can say that the system
G(s) is stable. However, if the pole a > 0, then y(t) goes to −∞(does not converge
to a steady-state value), so we can say that the system G(s) is unstable.
In general there will be real and/or complex poles. Using pzmap() we can plot
these poles in the complex plane. A system G(s) is stable if none of its poles is on
the right hand semiplane. Figure3.12, obtained with Program 3.8, shows three cases
of zero-pole maps, with no zeros and two complex poles. It corresponds to:
G(s) =
1
(s + a + jb) (s + a −jb)
(3.23)
The case 1 has the two poles on the left hand semiplane (a < 0; the system G(s)
is stable). The case 2 has the two poles on the imaginary axis; it can be checked that
the step response is a steady sinusoidal oscillation (a = 0; in this case, the system
G(s) is marginally stable). The case 3 has the two poles on the right hand semiplane
(a > 0; the system G(s) is unstable).
Program 3.8 Pole-zero maps of three G(s) cases
% Pole-zero maps of three G(s) cases
% on the complex plane
G1=tf([1],[1 1 1]); %the transfer function G1(s)
subplot(1,3,1); pzmap(G1);

130
3
Linear Systems
Fig. 3.13 Response of
example A to sinusoidal
input
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-0.1
-0.05
0
0.05
0.1
0.15
seconds
xlabel(’stable G(s)’);
G2=tf([1],[1 0 1]); %the transfer function G2(s)
subplot(1,3,2); pzmap(G2);
xlabel(’marginally stable G(s)’);
G3=tf([1],[1 -1 1]); %the transfer function G3(s)
subplot(1,3,3); pzmap(G3);
xlabel(’unstable G(s)’);
3.3.2.4
Response to Any Input Signal
The lsim() (*C) function is able to compute and plot the output of the system G(s)
in response to any input signal. Let us exploit this powerful function to visualize in
the time domain some interesting aspects of the response of linear systems.
If a system has energy storage, such a circuit having capacitors and inductances,
there will be charging and discharging processes taking some time to be achieved:
that is the cause of transients in the response of a system to changes in the input. Let
us show this with two examples.
Figure3.13, which has been generated with the Program 3.9, shows the response
of the system example A to a sinusoidal input. During the ﬁrst 0.5s, the envelop of
the output curve follows an exponential decay until it reaches the horizontal: this is
an initial transient stage of the response.
Program 3.9 Time-domain response to sine, example A
% Time-domain response to sine, example A
R=1; C=0.1; %values of the components
num=[1]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function

3.3 Response of Continuous Linear Systems
131
Fig. 3.14 Response of
example B to sinusoidal
input
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
seconds
% Input sine signal
fu=20; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=2000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(1-tiv); %time intervals set (1 second)
u=sin(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
plot(t,y,’k’); %plots output signal
xlabel(’seconds’);
title(’time-domain response to sine, example A’);
Now, let us consider the same experiment with system example B. Figure3.14,
which has been generated with the Program 3.10, shows the response of example
B to a sinusoidal input. Again it can be noticed an initial transient that makes the
envelop of the output curve to be oscillatory during the ﬁrst 1.4s.
Program 3.10 Time-domain response to sine, example B
% Time-domain response to sine, example B
R=0.5; C=0.1; L=0.1; %values of the components
num=[R*C 0]; % transfer function numerator;
den=[L*C R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
% Input sine signal
fu=20; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=2000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (2 seconds)
u=sin(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
plot(t,y,’k’); %plots output signal
xlabel(’seconds’);
title(’time-domain response to sine, example B’);

132
3
Linear Systems
0
2
4
6
8
10
12
14
16
18
20
-1
-0.5
0
0.5
1
input
0
2
4
6
8
10
12
14
16
18
20
-1
-0.5
0
0.5
1
output
seconds
Fig. 3.15 Response of example B to a square signal
Another interesting aspect can be visualized by using a square wave as the input
signal. This signal is like a chain of successive steps being applied to the system
input. If enough time is allowed between the square signal transitions, the complete
transient can develop, so the output would be a chain of step responses. Figure3.15,
generated by the Program 3.11, shows the result of applying a low-frequency square
signal input to the system example B.
Program 3.11 Time-domain response to square signal, example B
% Time-domain response to square signal, example B
R=0.5; C=0.1; L=0.1; %values of the components
num=[R*C 0]; % transfer function numerator;
den=[L*C R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
% Input square signal
fu=0.2; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=2000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(20-tiv); %time intervals set (20 second)
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
subplot(2,1,1); plot(t,u,’k’); %plots input signal
axis([0 20 -1.2 1.2]);
ylabel(’input’);
title(’time-domain response to square, example B’);
subplot(2,1,2); plot(t,y,’k’); %plots output signal
ylabel(’output’); xlabel(’seconds’);

3.3 Response of Continuous Linear Systems
133
Fig. 3.16 Response of
example B to a
high-frequency square signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
0.25
seconds
Using the same Program 3.11, with a change in the signal frequency fu (from 0.2
Hz to 10 Hz), we obtain the Fig.3.16. There is a substantial change in the signal
look: now it becomes similar to Fig.3.14, with the difference that now the shape of
the output signal is triangular.
Figure3.16 can be explained by considering the Fourier decomposition of signals
into sinusoidal harmonics. In Fig.3.8, which depicts the frequency response of ﬁlter
example B, there is a peak at 10 rad/s (1.6 Hz). What happens with Fig.3.16 is that at
frequencies well higher than 1.6 Hz the ﬁlter attenuates high-frequency harmonics
of the square signal, making its shape to tend to the signal fundamental sinusoidal
harmonic.
3.4
Response of Discrete Linear Systems
Filters made with digital processors or computers are discrete systems.
The “z transform” provides a way to still be using transfer functions, [6], so the
output of a discrete linear system with “discrete transfer function” G(z) is given by:
Y(z) = G(z) U(z)
(3.24)
where U(z) is the z transform of the input signal, and Y(z) is the z transform of the
system output signal.

134
3
Linear Systems
Fig. 3.17 Step response of a
discrete system
Time (sec.)
Amplitude
0
0.2
0.4
0.6
0.8
1
1.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
From: U(1)
To: Y(1)
The function freqz() can be used to compute the frequency response of a discrete
system.
In the time domain it is possible to apply a simple algorithm to obtain the system
response to any input, since it is given by the following discrete convolution:
y(nT) =
m=+∞

m=−∞
g(mT) u(nT −mT)
(3.25)
Figure3.17 has been obtained with the Program 3.12 and shows an example of step
response, for the system G(z) = 1/(z −0.5).
Program 3.12 Step response of a discrete system
% Step response of a discrete system
%transfer function numerator and denominator:
num=[1]; den=[1 -0.5];
Ts=0.1; %sampling period
G=tf(num,den,Ts); %transfer function
step(G,’k’); %step response of G
title(’step response of discrete system’)
In the case of discrete systems, G(z) is stable if none of its poles is outside a unit
circle in the complex plane. When using pzmap() for discrete system, you can add
zgrid() to visualize the unit circle on the zero-pole map. Figure3.18 shows an example
of zero-pole map and the grid with the unit circle; the ﬁgure has been generated with
the Program 3.13.
Program 3.13 Pole-zero map of a discrete system
% Pole-zero map of a discrete system
%transfer function numerator and denominator:
num=[1 -0.5]; den=[1 0.55 0.75];

3.4 Response of Discrete Linear Systems
135
Fig. 3.18 Example of
zero-pole map for a discrete
system
Real Axis
Imag Axis
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Ts=0.1; %sampling period
G=tf(num,den,Ts); %transfer function
pzmap(G); %pole-zero map
zgrid
If any pole is outside the unit circle, the system is unstable. If all poles are inside
the unit circle, the system is stable. In case any or all poles were on the unit circle,
the system is marginally stable.
3.5
Random Signals Through Linear Systems
The cross-correlation of two random signals x(t) and y(t) is deﬁned as:
Rxy(t1 , t2) = E(x(t1)y∗(t2))
(3.26)
If a random signal u(t) is applied to the input of a linear system G(s), the output of
the system is another random signal y(t) which is correlated with the input u(t). If
u(t) is stationary white noise, then:
Ruy(τ) = k · g(τ)
(3.27)
This result is convenient for system control purposes, since in certain situations the
ﬁrst problem is to experimentally determine the transfer function G(s) (or the impulse
response g(t)) of the plant to be controlled.
The power spectrum of y(t) and the power spectrum of u(t) are related by:
Sy(ω) = Su(ω) |G(ω)|2
(3.28)

136
3
Linear Systems
0
10
20
30
40
50
60
70
80
90
100
-26
-24
-22
-20
-18
-16
Frequency (Hz)
input
0
10
20
30
40
50
60
70
80
90
100
-40
-35
-30
-25
-20
-15
Frequency (Hz)
output
Fig. 3.19 PSD of u(t), and PSD of y(t)
Figure3.19 compares the PSD of y(t) and the PSD of u(t), for G(s) = 100/(s+100).
The ﬁgure has been made with the Program 3.14.
Program 3.14 PSDs of random u(t) and output y(t)
%PSDs of random u(t) and output y(t)
G=tf([100],[1 100]); %the linear system
fs=200; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(8-tiv); %time intervals set (800 values)
N=length(t); %number of data points
u=randn(N,1); %random input signal data set
[y,ty]=lsim(G,u,t); %random output signal data set
nfft=256; %length of FFT
window=hanning(256); %window function
numoverlap=128; %number of samples overlap
%PSD of the input:
subplot(2,1,1); pwelch(u,nfft,fs,window,numoverlap);
title(’PSDs of input and output’); ylabel(’input’);
%PSD of the output:
subplot(2,1,2); pwelch(y,nfft,fs,window,numoverlap);
title(’’); ylabel(’output’);
Another interesting relationship between input and output is the following:
Suy(ω) = G(ω) Su(ω)
(3.29)
where Suy (ω) is the cross power spectrum, deﬁned as:

3.5 Random Signals Through Linear Systems
137
0
10
20
30
40
50
60
70
80
90
100
0
0.2
0.4
0.6
0.8
1
the system
Hz
0
10
20
30
40
50
60
70
80
90
100
0.2
0.4
0.6
0.8
1
1.2
Hz
the estimate
Fig. 3.20 Real and estimated frequency responses of G(s)
Suy(ω) =
∞

−∞
Ruy(τ) e−j ωt dτ
(3.30)
From Eq.(3.29) we can obtain the transfer function of the system:
G(ω) = Suy(ω)
Su(ω)
(3.31)
Program 3.15 applies the tfe() function to obtain an estimate of G(s) from y(t) and
u(t). This program generates the Fig.3.20, which compares the frequency response
of G(s) = 100/(s + 100) with the estimated frequency response of G(s).
Program 3.15 Estimate of the transfer function from random input u(t) and output y(t)
%Estimate of the transfer function from random input u(t)
and output y(t)
num=100; den=[1 100];
G=tf(num,den); %the linear system
fs=200; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(8-tiv); %time intervals set (1600 values)
N=length(t); %number of data points
u=randn(N,1); %random input signal data set
[y,ty]=lsim(G,u,t); %random output signal data set
nfft=256; %length of FFT
window=hanning(256); %window function
numoverlap=128; %number of samples overlap
%Frequency response of the system G(s):

138
3
Linear Systems
Hz=0:0.1:100; w=2*pi*Hz; %frequencies
G=freqs(num,den,w); %frequency response of the system
%plot of frequency response of the system:
subplot(2,1,1); plot(Hz,abs(G));
title(’Real and estimated frequency response of G(s)’);
ylabel(’the system’); xlabel(’Hz’)
%Frequency response of the transfer function estimate:
%frequency response estimate:
[GE,FE]=tfe(u,y,nfft,fs,window,numoverlap);
%plot of estimated frequency response of the system
subplot(2,1,2); plot(FE,abs(GE));
title(’’); xlabel(’Hz’); ylabel(’the estimate’);
3.6
State Variables
As said in the introduction, it is also possible to represent linear systems in terms of
state variables, which are able to describe multiple input-multiple output (MIMO)
systems. The system itself has one or more state variables.
Actually two types of philosophies are found in system representation, so one
could speak of external or internal views. The transfer function is an example of
external representation: the system is considered as a black box, and we use only a
relationship between output and input. In the case of internal representations the box
is opened, and the internal states of the system are taken into account.
The next equations are an example of state space model. There is a vector of states
¯x(t), a vector of inputs ¯u(t) and a vector of outputs ¯y(t):
˙¯x(t) = A ¯x(t) + B ¯u(t)
¯y(t) = C ¯x(t) + D ¯u(t)
(3.32)
In the state space model, A, B, C and D are matrices. MATLAB has been originally
developed to deal with matrices and vectors.
Figure3.21 depicts a diagram corresponding to a state space model.
There is a discrete time version of the state model, as follows:
¯x(n + 1) = a ¯x(n) + b ¯u(n)
¯y(n) = c ¯x(n) + d ¯u(n)
(3.33)
Fig. 3.21 State space model
diagram
x(n)
y(n)
u(n)
−
−
−

3.6 State Variables
139
Fig. 3.22 A two-tank
system example
h1
h2
R1
R2
u
h1
h2
R1
R2
u
With simple iteration loops, it is easy to develop MATLAB programs to see what is
the behaviour of the system described by such a model.
Consider an example. It is a two tank system, as depicted in Fig.3.22. Both tanks
communicate through a pipe with resistance R1. The input is liquid that falls into
tank1, the output is liquid that leaves tank2 through a pipe with resistance R2.
Next equations are a simplistic model of the system, which is enough for illustra-
tion purposes.
A1
dh1
dt
=
1
R1 (h2 −h1 ) + u(t)
A2
dh2
dt
= −1
R1 (h2 −h1 ) −
1
R2 h2
(3.34)
where A1 = 1, A2 = 1, R1 = 0.5, R2 = 0.4.
From these equations, we can write the state space model using the following
matrices:
A =
−
1
R1A1
1
R1A1
1
R1A2
1
A2 ( 1
R1 + 1
R2 )

B =
 1
A1
0

C = (0 1)
(3.35)
The states are h1(t) and h2(t).
In many cases, it is convenient to study the behaviour of a system let alone from an
initial state not at the origin. There is no input (input equal to zero). This is denoted as
autonomous behaviour. For example, let us see how the two tank system goes empty,
starting from some initial state. Figure3.23 shows the evolution of h1(t) and h2(t).
This ﬁgure has been generated with the Program 3.16, which includes a conversion
from continuous state model to discrete time model. The reader is invited to change
initial states and see what happens.

140
3
Linear Systems
Fig. 3.23 System state
evolution
0
5
10
15
20
25
30
35
40
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
sampling periods
Program 3.16 System example
%system example
%state space system model (2 tank system):
A1=1; A2=1; R1=0.5; R2=0.4;
A=[-1/(R1*A1) 1/(R1*A1); 1/(R1*A2) -(1/A2)*((1/R1)+(1/R2))];
B=[1/A1; 0]; C=[0 1]; D=0;
Ts=0.1; %sampling period
csys=ss(A,B,C,D); %setting the continuous time model
dsys=c2d(csys,Ts,’zoh’); %getting the discrete-time model
[a,b,c,d]=ssdata(dsys); %retrieves discrete-time model matrices
% system simulation preparation
Nf=40; %simulation horizon
x1=zeros(1,Nf); % for x1(n) record
x2=zeros(1,Nf); % for x2(n) record
x=[1;0]; % state vector with initial tank levels
u=0.1; %constant input
%behaviour of the system after initial state
% with constant input u
for nn=1:Nf,
x1(nn)=x(1); x2(nn)=x(2); %recording the state
xn=(a*x)+(b*u); %next system state
x=xn; %state actualization
end;
% display of states evolution
figure(1)
plot([0 Nf],[0 0],’b’); hold on; %horizontal axis
plot([0 0],[-0.2 1.2],’b’); %vertical axis
plot(x1,’r-x’); %plots x1
plot(x2,’b-x’); %plots x2
xlabel(’sampling periods’);
title(’system states’);
The standard procedure for matrix diagonalization uses eigenvalues and eigen-
vectors. MATLAB provides the function eig() to obtain these elements. It can be
shown that the eigenvalues of the matrix A of the state space model, are equal to the

3.6 State Variables
141
poles of the system. Therefore, the stability of the system is linked to the eigenvalues.
In particular, in the case of discrete time systems, the module of all eigenvalues of
the matrix a must be ≤1 for the system to be stable.
The MATLAB function tf2ss() obtains from the transfer function of a system an
equivalent state space representation (it gives the four matrices). The reverse is also
possible with the function ss2tf().
3.7
State Space Gauss–Markov Model
Usually, in real ﬁltering applications there are noise and perturbations. Depending
on the case, it would be recommended to study in which way the ﬁlter responds to
these elements. The examples studied in this section would show that the perturbed
behaviour may have increasing variance along transients, which can become an issue.
It should be said that there are special ﬁlter designs that take into account noise
and perturbations, in order to adapt internal parameters for optimal behaviour. A
representative example of this is the Kalman ﬁlter, which is out of the scope of this
chapter. In any case, the simple examples to be studied next, do have illuminating
aspects of interest for sophisticated ﬁlter designs.
The section is divided into two subsections. In the ﬁrst subsection, the simplest
state space model is used to gain insight. This model is a scalar model with only one
state variable. The second subsection generalizes the results to more dimensions, and
the Gauss–Markov model is introduced.
Along the next pages, the study will focus on discrete-time state variables. Model
parameters are constant.
3.7.1
A Scalar State Space Case
Consider the following scalar state space model:
x(n + 1) = a · x(n) + b · u(n)
y(n) = c · x(n)
(3.36)
with |a| ≤1.
3.7.1.1
Autonomous and Forced Responses
Let us ﬁrst see the autonomous response, putting u(t) to zero and taking a nonzero
initial state. Figure3.24 shows the behaviour of the system, for a = 0.6 and x(0) = 5.
The ﬁgure has been generated with the Program 3.17. The reader may explore the
consequences of choosing values |a| ≥1, which implies an unstable situation.

142
3
Linear Systems
Fig. 3.24 Autonomous
behaviour of the system
1
2
3
4
5
6
7
8
9
10
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
samples
state
Fig. 3.25 Forced response
of the system, unit step input
1
2
3
4
5
6
7
8
9
10
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
samples
state
Program 3.17 Deterministic state evolution
% Deterministic state evolution
%example of state dynamics model
% xn=0.6 xo
%initial state
X0=5;
Ns=10; %number of samples along time
X=zeros(1,Ns); %reserve space
X(1)=X0;
for nn=2:Ns,
X(nn)=0.6*X(nn-1);
end;
figure(1)
plot(X,’k*-’); hold on;
xlabel(’samples’); ylabel(’state’);
title(’Autonomous behaviour’);

3.7 State Space Gauss–Markov Model
143
Now, let us apply a step input, setting u(t) to one and taking x(0) = 0 as initial
state. The value of b is 0.7. Figure3.25, obtained with the Program 3.18, shows the
result:
Program 3.18 Deterministic state evolution
% Deterministic state evolution
%example of state dynamics model
% xn=0.6 xo + 0.7 u
%initial state
X0=0;
Ns=10; %number of samples along time
X=zeros(1,Ns); %reserve space
u=1; %step input
X(1)=X0;
for nn=2:Ns,
X(nn)=0.6*X(nn-1)+ 0.7 *u;
end;
figure(1)
plot(X,’k*-’); hold on;
xlabel(’samples’); ylabel(’state’);
title(’Step response’);
3.7.1.2
Perturbations on the Initial State
Suppose that there are perturbations on the initial state, or that there is some uncer-
tainty about its value. Assume also that this situation can be represented with a
random state with a Gaussian distribution. It is intriguing to see how it inﬂuences
the autonomous behaviour of the system; or, in other words, how the perturbation
propagates along time.
The Program 3.19 has been prepared for this case, using a Gaussian perturbation
and propagating 500 trajectories. The result is shown in Fig.3.26.
Program 3.19 Propagation of uncertainty on initial state
% Propagation of uncertainty on initial state
%example of state dynamics model
% xn=0.6 xo
%initial state (with some Gaussian uncertainty)
X0=5+(0.5*randn(500,1));
Ns=10; %number of samples along time
X=zeros(500,Ns); %reserve space
X(:,1)=X0;
for nn=2:Ns,
X(:,nn)=0.6*X(:,nn-1);
end;
figure(1)
for np=1:500,
plot(X(np,:),’g-’); hold on;
end;
xlabel(’samples’); ylabel(’state’);
title(’Autonomous behaviour, uncertain initial state’);

144
3
Linear Systems
Fig. 3.26 Propagation of
initial state perturbations
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
samples
state
The evolution of mean and variance of the states, in absence of input, is given by:
μx(n + 1) = a μx(n)
(3.37)
varx(n + 1) = a2 varx(n)
(3.38)
Figure3.27, which has been generated with the Program 3.20, shows the evolution
of the mean and variance of state for the example previously considered.
Program 3.20 Evolution of state mean and variance
% Evolution of state mean and variance
%initial state
X0=5;
Ns=10; %number of samples along time
mX=zeros(1,Ns); %reserve space
varX=zeros(1,Ns); %"""
mX(1)=X0;
varX(1)=0.5;
for nn=2:Ns,
mX(nn)=0.6*mX(nn-1);
varX(nn)=(0.6^2)*varX(nn-1);
end;
figure(1)
subplot(2,1,1)
plot(mX,’k*-’);
xlabel(’samples’); ylabel(’state mean’);
title(’Autonomous behaviour’);
subplot(2,1,2)
plot(varX,’k*-’);
xlabel(’samples’); ylabel(’state variance’);

3.7 State Space Gauss–Markov Model
145
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
samples
state mean
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
samples
state variance
Fig. 3.27 Evolution of mean and variance of the state for the previous example
3.7.1.3
State Perturbations Along Time
The states of a system may be subject to perturbations, like for instance a ship moving
in rough seas. Additive perturbations can be included in the system equations as
follows:
x(n + 1) = a · x(n) + p · w(n)
(3.39)
where w(n) represents the perturbation, also denoted as ‘process noise’.
Continuing with the example, let us include some process noise. Figure3.28 shows
the autonomous behaviour of the system in the presence of this noise, using Gaussian
noise and the propagation of 1000 trajectories. The ﬁgure has been generated with
the Program 3.21.
Program 3.21 Inﬂuence of Gaussian process noise
% Influence of Gaussian process noise
%example of state dynamics model
% xn=0.6 xo + 0.5 w;
%autonomous behaviour with process noise---------------
Ns=10; %number of samples along time
X=zeros(1000,Ns); %reserve space
for np=1:1000,
X(np,1)=5; %initial state
for nn=2:Ns,
X(np,nn)= 0.6*X(np,nn-1)+ 0.5*randn(1); %with process noise
end;
end;
figure(1) %trajectories

146
3
Linear Systems
Fig. 3.28 Inﬂuence of
process perturbations
1
2
3
4
5
6
7
8
9
10
-3
-2
-1
0
1
2
3
4
5
samples
state
nf=1:Ns;
for ns=1:1000,
plot(nf,X(ns,nf),’g-’); hold on;
end;
title(’Autonomous behaviour, with process noise’);
xlabel(’samples’); ylabel(’state’);
The evolution of mean and variance of the states in the presence of state noise,
and with no input, is given by:
μx(n + 1) = a μx(n)
(3.40)
varx(n + 1) = a2 varx(n) + p2 varw(n)
(3.41)
Figure3.29, which has been generated with the Program 3.22, shows the evolution
of the mean and variance of state for the example with state noise and a ﬁxed initial
state (no uncertainty here). It has been assumed that the state noise has constant
variance along time.
Program 3.22 Evolution of state mean and variance
% Evolution of state mean and variance
% in the presence of process noise
%initial state
X0=5;
Ns=10; %number of samples along time
mX=zeros(1,Ns); %reserve space
varX=zeros(1,Ns); %"""
mX(1)=X0;
varX(1)=0;
varW=1;
for nn=2:Ns,

3.7 State Space Gauss–Markov Model
147
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
samples
state mean
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
samples
state variance
Fig. 3.29 Evolution of mean and variance of the state for the example with process noise
mX(nn)=0.6*mX(nn-1);
varX(nn)=(0.6^2)*varX(nn-1)+(0.5^2)*varW;
end;
figure(1)
subplot(2,1,1)
plot(mX,’k*-’);
xlabel(’samples’); ylabel(’state mean’);
title(’Autonomous behaviour’);
subplot(2,1,2)
plot(varX,’k*-’);
xlabel(’samples’); ylabel(’state variance’);
It is interesting to observe in more detail what happens in the transition from one
to the next state. In particular, let us show the histograms of states x(2) and x(3), and
the histogram of the process noise. This has been depicted in Fig.3.30.
Program 3.23 Propagation of Gaussian state noise: details of states x(2) and x(3)
% Propagation of Gaussian state noise:
% details of states X2 and X3
% example of state dynamics model
% xn=0.6 xo + 0.5 w;
%autonomous behaviour with process noise---------------
Ns=10; %number of samples along time
X=zeros(5000,Ns); %reserve space
for np=1:5000,
X(np,1)=5; %initial state
for nn=2:Ns,
X(np,nn)= 0.6*X(np,nn-1)+ 0.5*randn(1); %with process noise
end;
end;

148
3
Linear Systems
Fig. 3.30 Histograms of
x(2), x(3) and process noise
-2
-1
0
1
2
3
4
5
0
50
100
150
200
250
300
350
400
450
w
x(2) 
x(3) 
Fig. 3.31 A beta PDF
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.5
1
1.5
2
2.5
3
3.5
values
[Y2,V2]=hist(X(:,2),40);
[Y3,V3]=hist(X(:,3),40);
R=0.5*randn(5000,1);
[YR,VR]=hist(R,40);
figure(1)
plot(V2,Y2,’k’); hold on;
plot(VR,YR,’b’);
plot(V3,Y3,’r’)
title(’transition from X(2) to X(3)’);
The process noise may be non-Gaussian. Let us brieﬂy study this case, using
the same example but changing the process noise. Instead of a zero mean Gaussian
noise, a beta distribution is chosen as a non-zero mean asymmetrical distribution.
Figure3.31 depicts the PDF of the beta distribution selected for our example

3.7 State Space Gauss–Markov Model
149
Fig. 3.32 Inﬂuence of
process perturbations
1
2
3
4
5
6
7
8
9
10
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
samples
state
Program 3.24 A beta PDF
% A beta PDF
v=0:0.01:1; %values set
alpha=7; beta=2; %random variable parameters
ypdf=betapdf(v,alpha,beta); %beta PDF
plot(v,ypdf,’k’); %plots figure
xlabel(’values’); title(’beta PDF’);
The inﬂuence of the non-Gaussian process noise is shown in Fig.3.32. It has been
generated with the Program 3.25.
Notice that x(10) has non-zero mean. Figure3.33, also generated with the Program
3.25, compares two histograms. The histogram on top corresponds to the process
noise. The other histogram corresponds to state x(10). As a consequence of the
central limit, this last histogram should tend to be Gaussian. This is because along
the system trajectory, random variables (state x(n) and noise) are iteratively added.
Program 3.25 Propagation of non-Gaussian process noise
% Propagation of non-Gaussian process noise
% state evolution
% histograms of noise and X(10)
%example of state dynamics model
% xn=0.6 xo + 0.5 w;
%autonomous behaviour with process noise---------------
Ns=10; %number of samples along time
X=zeros(500,Ns); %reserve space
for np=1:500,
X(np,1)=5; %initial state
for nn=2:Ns,
%with beta process noise:
X(np,nn)= 0.6*X(np,nn-1)+ 0.5*random(’beta’,7,2,1,1);
end;
end;
R=0.5*random(’beta’,7,2,500,1);
figure(1) %trajectories

150
3
Linear Systems
Fig. 3.33 Histograms of
process noise and x(10)
0
0.5
1
1.5
0
10
20
30
40
histogram of process noise
0
0.5
1
1.5
0
10
20
30
40
histogram of x(10)
nf=1:Ns;
for ns=1:500,
plot(nf,X(ns,nf),’g-’); hold on;
end;
title(’Autonomous behaviour, with beta process noise’);
xlabel(’samples’); ylabel(’state’);
figure(2)
subplot(2,1,1)
hist(R,40); title(’histogram of process noise’);
axis([0 1.5 0 40]);
subplot(2,1,2)
hist(X(:,10),40); title(’histogram of x(10)’)
axis([0 1.5 0 40]);
3.7.1.4
Measurement Noise Along Time
The outputs of the system may be subject to additive measurement noise. This situ-
ation can be modeled with the following equation:
y(n) = c · x(n) + v(n)
(3.42)
where v(n) is the ‘measurement noise’.
As a ﬁrst case, let us consider that both the process noise and the measurement
noise are Gaussian. An example of this has been implemented with the Program 3.26,
which generates Figs.3.34 and 3.35. These ﬁgures correspond to the system output
y. The histogram has approximately a Gaussian shape, with zero mean.

3.7 State Space Gauss–Markov Model
151
Fig. 3.34 Inﬂuence of the
process and the measurement
noises
1
2
3
4
5
6
7
8
9
10
-4
-2
0
2
4
6
8
10
samples
output
Fig. 3.35 Histograms of
y(10)
-4
-3
-2
-1
0
1
2
3
4
0
10
20
30
40
50
60
70
Program 3.26 Inﬂuence of Gaussian process noise and measurement noise
% Influence of Gaussian process noise and measurement noise
%example of state dynamics model
% xn=0.6 xo + 0.5 w;
% yn=1.2 xn + 0.8 v;
%autonomous behaviour with process noise---------------
Ns=10; %number of samples along time
X=zeros(1000,Ns); %reserve space
Y=zeros(1000,Ns); %"""
for np=1:1000,
X(np,1)=5; %initial state
for nn=2:Ns,
X(np,nn)= 0.6*X(np,nn-1)+ 0.5*randn(1); %with process noise
end;
for nn=1:Ns,
Y(np,nn)=1.2*X(np,nn) + 0.8*randn(1); %with meas. noise

152
3
Linear Systems
Fig. 3.36 Inﬂuence of the
process and the measurement
noises
1
2
3
4
5
6
7
8
9
10
-2
-1
0
1
2
3
4
5
6
7
8
samples
output
end;
end;
figure(1) %trajectories
nf=1:Ns;
for ns=1:1000,
plot(nf,Y(ns,nf),’g-’); hold on;
end;
title(’System output for autonomous behaviour’);
xlabel(’samples’); ylabel(’output’);
figure(2) %histogram
hist(Y(:,Ns),40);
title(’Histogram of final measurement’);
As a second case, let us consider a non-Gaussian measurement noise, while the
process noise continues being Gaussian.
Program 3.27 uses a strong measurement noise with beta distribution. The effects
are visible in Figs.3.36 and 3.37. The histogram corresponding to y has now a beta-
alike shape (it mixes one Gaussian PDF and one beta PDF), and has non-zero mean
(an important fact for certain applications).
Program 3.27 Inﬂuence of Gaussian process noise
% Influence of Gaussian process noise
% and non-Gaussian measurement noise
%example of state dynamics model
% xn=0.6 xo + 0.5 w;
% yn=1.2 xn + 1.8 v;
%autonomous behaviour with process noise---------------
Ns=10; %number of samples along time
X=zeros(1000,Ns); %reserve space
Y=zeros(1000,Ns); %"""
for np=1:1000,
X(np,1)=5; %initial state
for nn=2:Ns,

3.7 State Space Gauss–Markov Model
153
Fig. 3.37 Histograms of
y(10)
-2
-1
0
1
2
3
4
5
0
10
20
30
40
50
60
70
80
X(np,nn)= 0.6*X(np,nn-1)+ 0.5*randn(1); %with process noise
end;
for nn=1:Ns,
%with beta measurement noise:
Y(np,nn)=1.2*X(np,nn) + 0.8*random(’beta’,7,2,1,1);
end;
end;
figure(1) %trajectories
nf=1:Ns;
for ns=1:1000,
plot(nf,Y(ns,nf),’g-’); hold on;
end;
title(’System output for autonomous behaviour’);
xlabel(’samples’); ylabel(’output’);
figure(2) %histogram
hist(Y(:,Ns),40);
title(’Histogram of final measurement’);
3.7.2
General State Space Case
The general state space model including process noise and measurement noise is the
following:
¯x(n + 1) = A ¯x(n) + B ¯u(n) +
¯w(n)
(3.43)
¯y(n) = C ¯x(n) + ¯v(n)
(3.44)
where
⇀w(n) is the process noise,
⇀v(n) is the measurement noise, and the capital letters
represent matrices.

154
3
Linear Systems
Under certain conditions (Gaussian noises, etc.), this state space model belongs
to the family of Gauss–Markov models, [13, 22]. Its Markovian nature is clear: the
next state only depends on the present state.
3.7.2.1
Propagation of Mean and Variance
The propagation of mean and variance of the states obey to the following equations,
[10]:
µx(n + 1) = A µx(n) + B u(n)
(3.45)
Σx(n + 1) = A Σx(n) AT + Σw(n)
(3.46)
And the propagation of mean and variance of the output is given by:
µy(n) = C µx(n)
(3.47)
Σy(n) = C Σx(n) CT + Sv(n)
(3.48)
3.7.2.2
An Important Lemma
Suppose a set of Gaussian random variables. Let us take the following partition:
x =
x1
x2

(3.49)
With:
µx =
 µx1
µx2

;
Sx =
S11S12
S21S22

(3.50)
Then the conditional distribution of
⇀x1(n), given a
⇀x2(n) =
⇀x
∗
2(n) is Gaussian with:
mean = µx1 + S12 S−1
22 (x2 −µx2)
(3.51)
cov = Σ11 −Σ12 Σ−1
22 Σ21
(3.52)
This is an important lemma for the development of adaptive ﬁlters, [10], and in
particular for the already mentioned Kalman ﬁlter, [21].

3.8 Time-Series Models
155
3.8
Time-Series Models
Time-series models are popular in economy; in particular for prediction purposes,
[14, 15]. Typically they belong to a statistical context. The archetypical time-series
model is the ARMA model, where AR means auto-regressive, and MA means
moving-average. Historically, the fundamental book that conﬁrmed the interest of
time-series models is [3] (modernized version).
This section is a short introduction to time-series models, departing from time
domain expressions and the corresponding z-transforms.
3.8.1
The Discrete Transfer Function in Terms
of the Backshift Operator
Suppose a discrete linear system with one input u and one output y. Let us write a
general expression relating output to input:
y(t) + a1 y(t −1) + a2 y(t −2) + · · · + an y(t −n) =
= b0u(t) + b1 u(t −1) + b2 u(t −2) + · · · + bm u(t −m)
(3.53)
Now, using the z-transform:
(1 + a1 z−1 + a2 z−2 + · · · + an z−n) Y(z) =
= (b0 + b1 z−1 + b2 z−2 + · · · + bn z−m) U(z)
(3.54)
The equation can be written in a shorter way:
A(z−1) Y(z) = B(z−1) U(z)
(3.55)
The discrete transfer function of the system is:
G(z) = Y(z)
U(z) = B(z−1)
A(z−1)
(3.56)
Now, let us introduce the (time-domain) ‘backshift operator’ q−k:
q−k y(t) = y(t −k)
(3.57)
Using this operator, the same model (3.55) can be written as:
A(q−1) y(t) = B(q−1) u(t)
(3.58)
where:

156
3
Linear Systems
Fig. 3.38 Behaviour of a
DARMA model example
0
5
10
15
20
25
0
0.2
0.4
0.6
0.8
1
1.2
1.4
n
A(q−1) = 1 + a1 q−1 + a2 q−2 + · · · + an q−n
and,
B(q−1) = b0 + b1 q−1 + b2 q−2 + · · · + bm q−m
In the book [10], expression (3.58) is called the DARMA model, where D stands
for ‘deterministic’. This is not a ﬁrmly recognized name: other authors use DARMA
to mean a Discrete ARMA model, others use DARMA to mean a de-seasonalized
ARMA model, etc.
A simple example of DARMA model has been considered in the Program 3.28.
The model combines a second order A(q−1) and a ﬁrst order B(q−1). The chosen
input is a constant value u(t) = 1; the reader is invited to change this input to
any other alternative, like for instance a sinusoidal function. The program is a basic
implementation of the algorithm implicit in the DARMA model. Figure3.38 shows
the behaviour of y(t) in response to the constant input u(t) = 1.
Program 3.28 Example of DARMA behaviour
%Example of DARMA behaviour
%coeffs. of polynomials A and B
a1=0.5; a2=0.7; b0=1; b1=0.7;
%variable initial values
y=0; y1=0; y2=0; u=0; u1=0;
Ni=24; %number of iterations
ry=zeros(1,Ni); %for storage of y values
%iterations
for nn=1:Ni,
u=1; %test value (edit)
y=(b0*u+b1*u1)-(a1*y1+a2*y2); %according with the model
ry(nn)=y;
y2=y1; y1=y; u1=u; %memory update
end;

3.8 Time-Series Models
157
figure(1)
stem(ry,’k’);
xlabel(’n’);title(’evolution of model output’);
From the point of view of digital ﬁlters, which will be studied in Chap.5, the
expression (3.58) corresponds to an IIR ﬁlter unless A(q−1) = 1, in which case one
has:
y(t) = B(q−1) u(t)
(3.59)
that corresponds to a FIR ﬁlter.
Actually, the behaviour of the DARMA model can be studied using the ﬁlter ()
function of the MATLAB SPT. The reader can easily conﬁrm that the Program 3.29
obtains the same results of the Program 3.28 already listed. Evidently, the Program
3.29 is shorter, because it uses ﬁlter(). More details on this function are presented in
the next chapter.
Program 3.29 Example of DARMA behaviour
%Example of DARMA behaviour
% now using filter()
%coeffs. of polynomials A and B
a=[1 0.5 0.7]; b=[1 0.7];
Ni=24;
u=ones(24,1); %test value
y=filter(b,a,u);
figure(1)
stem(y,’k’);
title(’evolution of model output’);
xlabel(’n’);
Specialists in control systems frequently use the backshift operator q−k. Other
authors, mostly statisticians, prefer to use the ‘lag operator’ Lk, which is equivalent
to the backshift operator. The polynomial:
h(L) = 1 + h1 L + h2 L2 + · · · + hn Ln
would be called a ‘lag polynomial’.
Of course, linear discrete-time models can be written in several ways: using dif-
ference equations, or backshift operator, or lag operator, etc.
Since two types of mentalities have been just mentioned, linked to control systems
or to statistics, it seems convenient to make a remark on zeros of polynomials. If you
consider the following polynomial:
p(z) = 1 + β1 z + β2 z2 + · · · + βnzn
(3.60)
the polynomial will have a series of zeros: λ1, λ2, . . . , λn.
Supposing that the polynomial p(z) was the denominator of a discrete transfer
function, these zeros must be inside the unit circle for the transfer function to be
stable.

158
3
Linear Systems
Take now the following polynomial:
r(z−1) = 1 + β1 z−1 + β2 z−2 + · · · + βnz−n
(3.61)
the zeros of this polynomial will be: η1, η2, . . . , ηn. These zeros correspond to values
of z−1 that make r(z−1) be zero, and will be the reciprocals of λ1, λ2, . . . , λn.
Supposing again that r(z−1) was the denominator of a transfer function, the zeros
η1, η2, . . . , ηn must be outside the unit circle for the transfer function to be stable.
This remark is particularly relevant when one uses the backshift or the lag operator.
3.8.2
Considering Random Variables
Typically time-series models are used in scenarios with random variables. For exam-
ple in ﬁnancial or marketing studies, weather forecasting, earthquake prediction,
electroencephalography, etc.
The following model considers random variables:
A(q−1) y(t) = B(q−1) u(t) + C(q−1) e(t)
(3.62)
where e(t) is white noise.
This is an ARMAX model. The X refers to exogenous inputs. In this case, the
exogenous input is u(t).
Notice that y(t) would be a random variable.
Important particular cases are the following:
• AR (auto-regressive) model: A(q−1) y(t) = e(t)
(3.63)
• MA (moving-average) model: y(t) = C(q−1) e(t) (3.64)
The ARMA model is a combination:
A(q−1) y(t) = C(q−1) e(t)
(3.63)
If there is a pure delay d:
A(q−1) y(t) = q−dB(q−1) u(t) + C(q−1) e(t)
(3.64)
3.8.2.1
Stationary Time-Series. Wold’s Decomposition
Let us now introduce a main reason for the ARMA models to be so relevant; in
particular for a certain class of processes.
A process is said to be ‘covariance-stationary’, or ‘weakly-stationary’, if the ﬁrst
and second moments are time invariant. In other words: the values of the mean, the

3.8 Time-Series Models
159
variance, and all autocovariances (Cov(yt, yt−k) = γk, ∀t, ∀k) do not depend on
time t.
A stationary process {xt, t = 1, 2, . . .} is deterministic if xt can be predicted
with zero error based on the entire past xt−1, xt−2, . . .. Notice that xt could be a
random variable, [24].
The Wold decomposition theorem states that any covariance stationary process
can be decomposed into two mutually uncorrelated processes:
• One is a MA process
• The other is a deterministic process.
In mathematical terms, the covariance stationary process xt can be written as:
xt = dt +

j
cjet−j
(3.65)
One of the implications of this theorem is that any purely non-deterministic covari-
ance stationary process can be arbitrarily well approximated by an ARMA process,
[24]. See the review of [18] for more aspects of the Wold’s decomposition.
3.8.2.2
Frequency Domain Study
The deﬁnition of ‘spectral density’ of a time-series with autocovariances satisfying

k
|γk| < ∞, is the following:
f (ω) =
∞

k=−∞
γke−j2π ω k
(3.66)
The spectral density is the Fourier transform of the autocovariance function. It pro-
vides a frequency domain approach for the study of time-series.
Another approach for frequency domain studies is based on the Discrete Fourier
Transform applied to the time-series. A popular way of graphical representation
of frequency components is the ‘periodogram’, which plots the already mentioned
power spectral density (PSD) of the time-series. The MATLAB SPT provides the
function periodogram().
A typical example of time-series data is Sunspot activity. There is a web page that
provides data on this (see the Resources section). Figure3.39 shows the smoothed
number of Sunspots along 459 periods, each period being six months.
Figure3.40 shows the periodogram of the Sunspot data. A simple pre-processing
of the data has being done by differentiation. There is a peak around a frequency of
0.05, which corresponds to a peak of Sunspot activity every 10–11 years approxi-
mately. Both Figs.3.39 and 3.40 have been obtained with the Program 3.30.

160
3
Linear Systems
Program 3.30 Periodogram of Sunspots
%Periodogram of Sunspots
% Read data file
%
fer=0;
while fer==0,
fid2=fopen(’sunspots.txt’,’r’);
if fid2==-1, disp(’read error’)
else
Ssp=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
% differenced data
x=diff(Ssp);
N=length(x);
M=N/2;
figure(1)
plot(Ssp,’k’); %plots Sunspot data
title(’Sunspots’); xlabel(’index’);
figure(2)
P=periodogram(x);
Pn=P/(2*sqrt(N));
freq=(0:M)/N;
plot(freq,Pn(1:M+1),’k’)
title(’Periodogram’);
xlabel(’freq’);
Fig. 3.39 Semi-annual
Sunspot activity
0
50
100
150
200
250
300
350
400
450
500
0
20
40
60
80
100
120
140
160
180
200
index

3.8 Time-Series Models
161
Fig. 3.40 Periodogram of
semi-annual Sunspot activity
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
10
20
30
40
50
60
freq
3.8.2.3
Details of the MA Model
A ﬁrst remark, [16], is that in the case of the MA model, y(t) is a ﬁltered version of
white noise.
The simplest MA model is MA(1) (ﬁrst order model), with the following expres-
sion:
y(t) = e(t) + c1e(t −1)
(3.67)
This process is the sum of two stationary processes, and therefore is stationary for any
value of the parameter c1. The mean of the process is 0. The variance is (1 + c2
1)σ2
e.
The autocovariances are:
γ0 = (1 + c2
1)σ2
e
γ1 = c1 σ2
e
γ−1 = c1 σ2
e
γk = 0 , |k| > 1
The spectral density function is:
f (ω) = σ2
e|1 + c1 exp(j 2π ω)|2
(3.68)
For a MA model of order q, MA(q), we have again a stationary process (the sum of
stationary processes). The mean is again 0. The variance is (1+c2
1+ c2
2+· · ·+c2
q)σ2
e;
and the autocovariances:

162
3
Linear Systems
γm
⎧
⎪⎪⎨
⎪⎪⎩
(1 + c2
1 + c2
2 + · · · + c2
q)σ2
e , m = 0
σ2
e
q−m

k=0
ck ck+|m| , |m| ≤q
0, |m| > q
(3.69)
The model MA(∞) is stationary if the coefﬁcients are absolute summable, that is:
the sum of their absolute values converges.
The spectral density function is:
f (ω) = σ2
e|1 + c1 exp(j 2π ω) + · · · + cq exp(j 2π qω)|2
(3.70)
It can be inferred from (3.69), [16], that the MA process cannot have sharp peaks,
unless q is large.
A characteristic of MA processes is that covariances become zero for |m| > q.
For this reason, MA models are regarded as having short memory.
Coming back to the MA(1) model, it is possible, by successive substitutions [16],
to derive the following expression:
∞

j=0
(c1)jy(t −j) = e(t)
(3.71)
If |c1| < 1 this expression converges. In this case, an AR model has been obtained,
being equivalent to the MA(1) model. It is said that the MA model has been inverted
(so an equivalent AR model was derived).
Notice that the zero of the lag polynomial (right-hand side of MA(1) model) is
−1/c1, which is outside the unit circle. This is a general result that can be shown
for MA(q): the condition for a MA(q) model to be invertible is to have all the lag
polynomial zeros outside the unit circle.
Next three ﬁgures correspond to an example of MA(3) model. The model has
been simulated with the Program 3.31, which generates these ﬁgures.
Figure3.41 shows the behaviour of y(t) along 200 samples. It is evident that y(t)
is a random variable.
Figure3.42 depicts the roots of the lag polynomial. The three roots are outside of
the unit circle, so the MA process is invertible.
The two plots included in Fig.3.43 show the covariances of the MA process.
The plot on top includes all covariances corresponding to 200 samples. The plot
at the bottom is a zoom on a few covariances around the index 0. Notice how the
covariances corresponding to 0, ±1, ±2, and 1±3, are non-zero.
Program 3.31 Example of MA behaviour
%Example of MA behaviour
%coeffs. of polynomial C
c0=1; c1=0.8; c2=0.5; c3=0.3;
%variable initial values
y=0; e1=0; e2=0; e3=0;

3.8 Time-Series Models
163
Ni=200; %number of iterations
ry=zeros(1,Ni); %for storage of y values
ee=randn(1,Ni); %vector of random values
%iterations
for nn=1:Ni,
e=ee(nn);
y=(c0*e+c1*e1+c2*e2+c3*e3); %according with MA model
ry(nn)=y;
e3=e2; e2=e1; e1=e; %memory update
end;
figure(1)
plot(ry,’k’);
title(’evolution of model output’);
xlabel(’n’);
LP=[c3 c2 c1 c0]; %lag polynomial
R=roots(LP); %roots of the lag polynomial
figure(2)
plot(0,0,’y.’); hold on
line([-2 1.5],[0 0]); line([0 0],[-2 2]); %axes
m=0:(pi/100):2*pi; plot(cos(m),sin(m),’k’); %circle
plot(real(R),imag(R),’kx’,’MarkerSize’,12); %roots
axis([-2 1.5,-2 2]);
title(’Roots of lag polynomial, and the unit circle’)
figure(3)
subplot(2,1,1)
[cv,lags]=xcov(ry,’biased’);
plot(lags,cv,’k’);
title(’covariances’)
subplot(2,1,2)
stem(lags(Ni-6:Ni+6),cv(Ni-6:Ni+6),’k’); hold on;
plot([-6 6],[0 0],’k’);
title(’zoom around index 0’);
Fig. 3.41 Behaviour of an
MA process
0
20
40
60
80
100
120
140
160
180
200
-4
-3
-2
-1
0
1
2
3
4
n

164
3
Linear Systems
-2
-1.5
-1
-0.5
0
0.5
1
1.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Fig. 3.42 Roots of the lag polynomial
-200
-150
-100
-50
0
50
100
150
200
-0.5
0
0.5
1
1.5
2
covariances
-6
-4
-2
0
2
4
6
-0.5
0
0.5
1
1.5
2
zoom around index 0
Fig. 3.43 Covariances
3.8.2.4
Details of the AR Model
The simplest AR model is AR(1), with the following expression:
y(t) + a1 y(t −1) = e(t)
(3.72)
This process is stationary if |a1| < 1.

3.8 Time-Series Models
165
Fig. 3.44 Behaviour of an
AR process
0
20
40
60
80
100
120
140
160
180
200
-4
-3
-2
-1
0
1
2
3
4
n
The zero of the lag polynomial (left-hand side of the AR(1) model) is −1/a1.
For the process to be stationary, this zero must be outside the unit circle. It is also
a condition for the AR model to be invertible, so an equivalent MA model can be
obtained.
In general, the lag polynomial zeros must be outside the unit circle for the AR(q)
model to be invertible.
The spectral density function of an AR(q) process is:
f (ω) = σ2
e
1
|A(ej 2πω)|2
(3.73)
According with this equation, the AR process could have sharp peaks, [16].
The AR(q) processes usually are a mixture of exponents, which corresponds to
real zeros, and sinusoids, due to complex zeros. Usually, AR processes have many
non-zero autocovariances that decay with the lag, and so they are regarded as long
memoryprocesses.Becauseofthesinusoids,ARprocessesmayhaveaquasi-periodic
character.
As in the case of MA(3), a series of three ﬁgures have been obtained, correspond-
ing now to an example of AR(3) model. Notice that we selected the same model
parameters as in MA(3), The model has been simulated with the Program 3.32.
Figure3.44 shows the behaviour of y(t) along 200 samples. Compared with
Fig.3.41, there are much more sharp oscillations of y(t).
Figure3.45 conﬁrms that the roots of the lag polynomial are the same as before.
The AR(3) process would be invertible, and stable.
The two plots included in Fig.3.46 show the covariances at two levels of detail.

166
3
Linear Systems
-2
-1.5
-1
-0.5
0
0.5
1
1.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
Fig. 3.45 Roots of the lag polynomial
-200
-150
-100
-50
0
50
100
150
200
-1
0
1
2
covariances
-6
-4
-2
0
2
4
6
-1
0
1
2
zoom around index 0
Fig. 3.46 Covariances
Program 3.32 Example of AR behaviour
%Example of AR behaviour
%coeffs. of polynomial A
a1=0.8; a2=0.5; a3=0.3;
%variable initial values
y=0; y1=0; y2=0; y3=0;
Ni=200; %number of iterations
ry=zeros(1,Ni); %for storage of y values
ee=randn(1,Ni); %vector of random values
%iterations

3.8 Time-Series Models
167
for nn=1:Ni,
e=ee(nn);
y=e-(a1*y1+a2*y2+a3*y3); %according with AR model
ry(nn)=y;
y3=y2; y2=y1; y1=y; %memory update
end;
figure(1)
plot(ry,’k’);
title(’evolution of model output’);
xlabel(’n’);
LP=[a3 a2 a1 1]; %lag polynomial
R=roots(LP); %roots of the lag polynomial
figure(2)
plot(0,0,’y.’); hold on
line([-2 1.5],[0 0]); line([0 0],[-2 2]); %axes
m=0:(pi/100):2*pi; plot(cos(m),sin(m),’k’); %circle
plot(real(R),imag(R),’kx’,’MarkerSize’,12); %roots
axis([-2 1.5,-2 2]);
title(’Roots of lag polynomial, and the unit circle’)
figure(3)
subplot(2,1,1)
[cv,lags]=xcov(ry,’biased’);
plot(lags,cv,’k’);
title(’covariances’)
subplot(2,1,2)
stem(lags(Ni-6:Ni+6),cv(Ni-6:Ni+6),’k’); hold on;
plot([-6 6],[0 0],’k’);
title(’zoom around index 0’);
Many time series data grow or decrease along time. For example the rise of prices
due to persistent inﬂation.
A very simple case can be illustrated with the following model:
y(t) = k + y(t −1)
(3.74)
The constant term will cause a linear growth if k > 0, or linear decreasing if
k < 0. This phenomenon is called ‘drift’.
Another simple case is the following:
y(t) = (k · t) + y(t −1)
(3.75)
Now one has a quadratic growth or decrease, depending on the sign of k. In this case
we have a ‘trend’. The trend can be also k · t2, or any other function of time.
The two plots in Fig.3.47 depict on top the effects of drift, and at the bottom the
effects of a k · t trend.
Program 3.33 Drift and trend
% Drift and trend
%drift
y1=0; y1_old=0;
ry1=zeros(500,1);
for nn=1:500,

168
3
Linear Systems
0
50
100
150
200
250
300
350
400
450
500
0
50
100
150
200
250
n
drift
0
50
100
150
200
250
300
350
400
450
500
0
500
1000
1500
n
trend
Fig. 3.47 Drift and trend
ry1(nn)=y1;
y1=0.5+y1_old;
y1_old=y1;
end;
%trend
y2=0; y2_old=0; t=0;
ry2=zeros(500,1);
for nn=1:500,
ry2(nn)=y2;
y2=(0.5*t)+y2_old;
y2_old=y2;
t=t+0.02;
end;
figure(1)
subplot(2,1,1)
plot(ry1,’k’);
xlabel(’n’);
ylabel(’drift’);
subplot(2,1,2)
plot(ry2,’k’);
xlabel(’n’);
ylabel(’trend’);
3.8.2.5
Yule–Walker Equations
Given a real process, one wants to establish an AR model for this process. The
question is how to estimate its coefﬁcients.

3.8 Time-Series Models
169
Consider a zero-mean AR(q) model:
y(t) + a1y(t −1) + a2y(t −2) + · · · + aqy(t −q) = e(t)
(3.76)
If one multiplies both sides by y(t −k), and takes expectations:
E

(y(t) + a1y(t −1) + a2y(t −2) + · · · + aqy(t −q)) · y(t −k)

=
= E (e(t) · y(t −k))
(3.77)
The result for k = 0 would be:
γ0 + a1 γ1 + a2 γ2 . . . + aqγq = σ2
e
(3.78)
and for k > 0:
γk + a1 γk−1 + a2 γk−2 . . . + aqγk−q = 0
(3.79)
Since γp = γ−p, the set of equations can be expressed as follows:
⎡
⎢⎢⎢⎢⎢⎣
γ0 γ1 γ2 . . . γq
γ1 γ0 γ1 . . . γq−1
γ2 γ1 γ0 . . . γq−2
...
...
...
...
γq γq−1 γq−2 . . . γ0
⎤
⎥⎥⎥⎥⎥⎦
⎡
⎢⎢⎢⎢⎢⎣
1
a1
a2
...
aq
⎤
⎥⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎢⎢⎣
σ2
e
0
0
...
0
⎤
⎥⎥⎥⎥⎥⎦
(3.80)
These are called the ‘Yule–Walker equations’, published by G.U. Yule and Sir G.
Walker in 1931. The equations can be used to obtain the values of a1, a2, . . . , aq
from the autocovariances γ0, γ1, γ2, . . . , γq, or vice-versa. A recursive algorithm for
problems with a Toeplitz matrix, such is the case with the Yule–Walker equations,
was introduced in [29].
For an AR(1) model, the Yule–Walker equations are:
γ0 + a1 γ1 = σ2
e , k = 0
(3.81)
γ1 + a1 γ0 = 0 , k = 1
(3.82)
Therefore, [16]:
γ0 =
σ2
e
1 −a2
1
; γ1 = (−a1) ·
σ2
e
1 −a2
1
(3.83)
For an AR(2), the Yule–Walker equations are, [19]:

170
3
Linear Systems
⎡
⎣
γ0 γ1 γ2
γ1 γ0 γ1
γ2 γ1 γ0
⎤
⎦
⎡
⎣
1
a1
a2
⎤
⎦=
⎡
⎣
a2 a1 1
0 0
0 a2 a1 1 0
0 0
a2 a1 1
⎤
⎦
⎡
⎢⎢⎢⎢⎣
γ2
γ1
γ0
γ1
γ2
⎤
⎥⎥⎥⎥⎦
=
=
⎡
⎣
1
a1
a2
a1 1 + a2 0
a2
a1
1
⎤
⎦
⎡
⎣
γ0
γ1
γ2
⎤
⎦=
⎡
⎣
σ2
e
0
0
⎤
⎦
(3.84)
Later on, in the chapter of digital ﬁlters, we will meet again the Yule–Walker method,
and a related MATLAB function.
3.8.2.6
Details of the ARMA Model
The ARMA model is a straight combination of AR and MA models. It is a more
parsimonious model compared to the AR model, which means that it requires less
parameters for modeling.
A theorem establishes that the ARMA model is stationary provided the zeros of
the AR lag polynomial lie outside the unit circle.
According with [1], one of the reasons in favor of the ARMA model is that
summing AR processes results in an ARMA process.
The spectral density function of an ARMA process is:
f (ω) = σ2
e
|C(ej 2πω)|2
|A(ej 2πω)|2
(3.85)
The Yule–Walker equations are:
γk + a1 γk−1 + a2 γk−2 . . . + aqγk−q =
⎧
⎨
⎩
σ2
e
p
j=k
cjηj−k , k = 0, . . . , p
0 ,
k > p
(3.86)
where the ARMA model combines an AR(q) model and a MA(p) model; and the η
are the coefﬁcients of C(z)/A(z).
Figure3.48 shows the behaviour of the output y(t) of an ARMA model along 200
samples. The ﬁgure has been generated with the Program 3.34, which implements a
simple simulation based on the model.
Program 3.34 Example of ARMA behaviour
%Example of ARMA behaviour
% ARMA model coeefs
%coeffs. of polynomial A
a1=0.05; a2=0.1;
%variable initial values
y=0.1; y1=0; y2=0;

3.8 Time-Series Models
171
Fig. 3.48 Behaviour of an
ARMA model
0
20
40
60
80
100
120
140
160
180
200
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
n
%coeffs. of polynomial C
c0=0.6; c1=0.4; c2=0.3;
%variable initial values
e1=0; e2=0;
Ni=200; %number of iterations
ry=zeros(1,Ni); %for storage of y values
ee=randn(1,Ni); %vector of random values
%iterations
for nn=1:Ni,
e=ee(nn);
% ARMA model:
y=(c0*e+c1*e1+c2*e2)-(a1*y1+a2*y2);
ry(nn)=y;
% memory update:
y2=y1; y1=y;
e2=e1; e1=e;
end;
figure(1)
plot(ry,’k’),
xlabel(’n’);
title(’evolution of model output’);
3.8.2.7
Unit Roots. ARIMA Model
Consider the following simple process:
y(t) = y(t −1) + e(t)
(3.87)
Although this equation looks quite innocent, it has been studied by several famous
scientists. The process described by this equation is a ‘random walk’. It happens that
the variance of y(t) is:

172
3
Linear Systems
V ar(y(t)) = (t −t0) σ2
e
(3.88)
where t0 is the time when the process started. As you can see, the variance increases
along time. That means that the random walk is a non-stationary process.
Notice that the lag polynomial corresponding to the random walk has one zero,
which lies on the unit circumference. This is called a unit root.
Denote as Δy(t) = y(t) −y(t −1). The operator Δ is the single lag difference
operator. Using this operator, the random walk can be described as follows:
Δy(t) = e(t)
(3.89)
which is a stationary process.
In a more general situation, one may have the following model (where L is the
lag operator):
g(L) y(t) = h(L) e(t)
(3.90)
where g(L) has a unit root. Then the model can be factorized as follows:
g∗(L) )(1 −L) y(t) = h(L) e(t)
(3.91)
Or, equivalently:
g∗(L) )Δy(t) = h(L) e(t)
(3.92)
This last equation is an ARMA model. And Eq.(3.87) is an ARIMA model: an autore-
gressive integrated moving average model; it describes a non-stationary process.
In general, ARIMA models have a number of unit roots, so they can be factorized
as follows:
g∗(L) )(1 −L)d y(t) = h(L) e(t)
(3.93)
An ARMA model can be obtained from (3.89) by using d times the differentiation.
Indeed, there is much more to be said about time-series processes. Some books
and Toolboxes on this topic have been cited in the Resources section.
3.8.2.8
Examples
Let us consider for a ﬁrst example the following time-series data: the weekly sales
of Ultra-Shine toothpaste in units of 1000 tubes. The case is studied in [2] and some
other academic literature (for instance [28]).
As shown in the left hand side of Fig.3.49, the data are not stationary. By simple
differencing,thedataaretransformedtoaprocessthatcouldberegardedasstationary.
In other words, we are supposing that there is one unit root. These differenced data
are shown o the right hand side of Fig.3.49. The ﬁgure has been generated with the
Program 3.35.

3.8 Time-Series Models
173
0
50
100
200
300
400
500
600
700
800
900
1000
1100
weeks
Toothpaste
0
50
100
0
2
4
6
8
10
12
14
16
18
weeks
differences
Fig. 3.49 Weekly toothpaste sales: (left) data, (right) increments
Program 3.35 Example of weekly toothpaste sales
%Example of weekly toothpaste sales
% Display of data
% read data file
fer=0;
while fer==0,
fid2=fopen(’Tpaste.txt’,’r’);
if fid2==-1, disp(’read error’)
else
TP=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
x=diff(TP); %data differencing
figure(1)
subplot(1,2,1)
plot(TP,’k’),
xlabel(’weeks’); ylabel(’Toothpaste’)
title(’weekly toothpaste sales’);
subplot(1,2,2)
plot(x,’k’)
xlabel(’weeks’); ylabel(’differences’);
It can be noticed that the differenced data have non-zero mean. Then, it is conve-
nient to subtract the mean, and also to divide by the standard deviation, to obtain a
normalized data series. The cited literature recommends to choose a simple AR(1)
model for this series.

174
3
Linear Systems
Fig. 3.50 An AR(1) process
simulation
0
10
20
30
40
50
60
70
80
90
100
-4
-3
-2
-1
0
1
2
3
n
Then, we apply the aryule( ) function, which belongs to the MATLAB System
Identiﬁcation Toolbox, to estimate just one parameter: the one free coefﬁcient of
the AR(1) model. Program 3.36 implements this approach, and uses the model to
generate simulated model outputs. Figure3.50 shows an example of the results of
the simulation. Each time one runs the program, one obtains a different plot, since
ARMA models are of stochastic nature.
Program 3.36 AR(1) model of weekly toothpaste sales
% AR(1) model of weekly toothpaste sales
% read data file
fer=0;
while fer==0,
fid2=fopen(’Tpaste.txt’,’r’);
if fid2==-1, disp(’read error’)
else
TP=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
x=diff(TP); %data differencing
D=std(x);
M=mean(x);
Nx=(x-M)/D; %normalized data
A=aryule(Nx,1); %AR(1) parameter estimation
%AR model simulation
%coeffs. of polynomial A
a1=A(2);
%variable initial values
y=0; y1=0;
Ni=100; %number of iterations
ry=zeros(1,Ni); %for storage of y values
ee=randn(1,Ni); %vector of random values
%iterations

3.8 Time-Series Models
175
for nn=1:Ni,
e=ee(nn);
y=e-(a1*y1); %according with AR model
ry(nn)=y;
y1=y; %memory update
end;
figure(1)
plot(ry,’k’);
title(’evolution of model output’); xlabel(’n’);
A second example is the US quarterly personal consumption expenditure, in bil-
lions of US dollars. We focus on the percentage change from trimester to trimester.
The model recommended in OTexts (web address in the Resources section) for this
data series is a MA(3).
Figure3.51 shows on the left the original data, and on the right the corresponding
percentage change. This ﬁgure has been generated with the Program 3.37. The ﬁnal
part of the program uses the armax( ) function from the MATLAB System Identiﬁ-
cation Toolbox, for estimating the MA(3) model parameters. The last sentence of the
program is intended for printing on screen the model parameter estimation result.
Program 3.37 Quarterly US personal consumption expediture
% Quarterly US personal consumption expediture
% Display of data
% read data file
fer=0;
while fer==0,
fid2=fopen(’consum.txt’,’r’);
0
100
200
0
2000
4000
6000
8000
10000
12000
14000
trimester
Consumption
0
100
200
-3
-2
-1
0
1
2
3
4
5
6
7
8
trimester
percentage change
Fig. 3.51 Quarterly US personal consumption expenditure: (left) data, (right) increments

176
3
Linear Systems
if fid2==-1, disp(’read error’)
else
CM=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
x=diff(CM); %data differencing
L=length(CM)-1;
nn=1:L;
d(nn)=x(nn)./CM(nn);
p=d*100; %percentage change
figure(1)
subplot(1,2,1)
plot(CM,’k’),
xlabel(’trimester’); ylabel(’Consumption’)
title(’US personal consumption’);
axis([0 L+1 0 14000]);
subplot(1,2,2)
plot(p,’k’)
xlabel(’trimester’)
ylabel(’percentage change’);
axis([0 L+1 -3 8]);
% MA(3) parameter estimation:
D=std(p);
M=mean(p);
Np=((p-M)/D)’; %column format
model=armax(Np,[0 3]);
% extract info from model structure
[A,B,C,D,F,LAM,T]=th2poly(model);
% print vector of MA(3) coeffs:
C
The third example is monthly gold prices. An interval of time, from 2001-1-1 to
2012-12-1, has been selected. The leftmost plot in Fig.3.52 shows the price data in
US dollars per ounce.
By taking natural logarithms, it can be noticed that the price evolution approx-
imately follows an exponential growth. This is conﬁrmed by the central plot in
Fig.3.52, which shows the logarithm of the gold price data, being almost a straight
line.
Again, one uses differences to obtain the data to be modeled by an ARMA model.
In this case, the differences of the logarithm of the data (called ‘returns’ in ﬁnance)
are computed; the rightmost plot in Fig.3.38 shows the result.
After generating Fig.3.52, the Program 3.38 continues with a last part devoted
to the estimation of an ARMA model for the returns. As recommended by [7], an
ARMA(7 10) model was chosen. The result of parameter estimation is printed on
screen when executing the program.
Program 3.38 Gold prices
% Gold prices
% Display of data
% read data file
fer=0;

3.8 Time-Series Models
177
0
50
100
200
400
600
800
1000
1200
1400
1600
1800
month
price
0
50
100
5.5
6
6.5
7
7.5
month
logarithm of price
0
50
100
-0.1
-0.05
0
0.05
0.1
0.15
month
returns
Fig. 3.52 Gold prices (monthly): (left) data, (center) log(data), (right) increments of log(data)
while fer==0,
fid2=fopen(’gold.txt’,’r’);
if fid2==-1, disp(’read error’)
else
GL=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
lgGL=log(GL); %logarithm of the data
x=diff(lgGL); %differencing of log(data)
L=length(x);
figure(1)
subplot(1,3,1)
plot(GL,’k’),
xlabel(’month’)
ylabel(’price’)
title(’Gold price’);
axis([0 L+1 200 1900]);
subplot(1,3,2)
plot(lgGL,’k’)
xlabel(’month’)
ylabel(’logarithm of price’);
axis([0 L+1 5.5 7.5]);
subplot(1,3,3)
plot(x,’k’)
xlabel(’month’)

178
3
Linear Systems
ylabel(’returns’);
axis([0 L -0.15 0.15]);
% MA(3) parameter estimation:
D=std(x);
M=mean(x);
Nx=((x-M)/D);
model=armax(Nx,[7 10]);
% extract info from model structure
[A,B,C,D,F,LAM,T]=th2poly(model);
% print vector of ARMA(7 10) coeffs:
A
C
It is not a surprise that the monthly demand of ice cream is higher in summer.
Likewise, coats demand is higher in winter; etc. Seasons have inﬂuence on many time
series data sets. Actually, much literature is devoted to seasonal ARIMA models.
A possible way of attack for obtaining a model is classical additive decomposition.
The data series is decomposed into three data series: an appropriate regression curve,
a seasonal oscillating curve, and a random data remainder.
For example, consider the monthly production of beer in Australia. Figure3.53
shows the evolution of this production from 1956-01 to 1980-12., in millions of litres.
the peaks correspond to summer, the valleys to winter.
The classical decomposition has been implemented with the Program 3.39. As
a ﬁrst step, it ﬁts a straight line to the data, since an approximate linear growth of
the mean production can be visually noticed in Fig.3.53. Then, the line is subtracted
from the data. Let us denote the result as the data series sB.
The ﬁtting of the line is obtained with the MATLAB polyﬁt() function. Higher
degree polynomials could also be ﬁtted.
The second step is to ﬁt a cosine periodic curve to sB. The period of the cosine is
assumed to be 12 months. For the ﬁtting of cosine amplitude and phase, a searching
optimization procedure has been used, by means of the MATLAB fminbnd() function.
Fig. 3.53 Monthly beer
production in Australia
0
50
100
150
200
250
300
60
80
100
120
140
160
180
200
220
months
production

3.8 Time-Series Models
179
0
50
100
150
200
250
300
0
100
200
month
Beer prduction
0
50
100
150
200
250
300
0
100
200
month
growth line
0
50
100
150
200
250
300
-50
0
50
month
cosine component
0
50
100
150
200
250
300
-50
0
50
month
random component
Fig. 3.54 Classical decomposition: (top) beer production, (below) linear drift, (next below) cosine
component, (bottom) residual
The random remainder is obtained by subtracting the cosine periodic curve from
sB.
Figure3.54 shows the components obtained by the decomposition. The plot on
top corresponds to the beer production data. Immediately below is the plot of the
ﬁtted straight line. Below this, another plot shows the ﬁtted cosine periodic curve.
The plot at the bottom shows the random data remainder.
Figure3.55 shows in more detail how is the ﬁtting of the cosine curve to sB. All
three Figs. 3.53, 3.54 and 3.55, have been obtained with the Program 3.39.
The last part of the program put the focus on the estimation of an ARMA model
of the random remainder data.
Program 3.39 Australian Beer production
% Australian Beer production
% Display of data

180
3
Linear Systems
Fig. 3.55 Cosine ﬁtting
0
50
100
150
200
250
300
-60
-40
-20
0
20
40
60
month
cos( ) fitting
% read data file
fer=0;
while fer==0,
fid2=fopen(’beer.txt’,’r’);
if fid2==-1, disp(’read error’)
else
BR=fscanf(fid2,’%f \r\n’);
fer=1;
end;
end;
fclose(’all’);
N=length(BR);
t=(1:N)’;
% estimate growth line
[r,s]=polyfit(t,BR,1);
gl=r(2)+r(1).*t;
%subtract the line
sB=BR-gl;
%fit a sinusoid
y=sB;
f=0.2288;
%function to be minimised by x:
ferror=inline(’sum(abs(y-x*cos(f+(t*2*pi/12))))’);
%find x for minimum error;
[ox ferrorx]=fminbnd(ferror,5,40,[],f,t,y);
ye=ox*cos(f+(t*2*pi/12)); %estimated cos( )
%subtract the sinusoid
nB=sB-ye;
% display -----------
figure(1)
plot(BR,’k’);
xlabel(’months’); ylabel(’production’);
title(’Australia beer production’);
figure(2)
subplot(4,1,1)
plot(BR,’k’),
xlabel(’month’); ylabel(’Beer production’)

3.8 Time-Series Models
181
title(’Beer production’);
axis([0 N 0 250]);
subplot(4,1,2)
plot(gl,’k’)
xlabel(’month’); ylabel(’growth line’);
axis([0 N 0 250]);
subplot(4,1,3)
plot(ye,’k’)
xlabel(’month’); ylabel(’cosine component’);
axis([0 N -50 50]);
subplot(4,1,4)
plot(nB,’k’)
xlabel(’month’); ylabel(’random component’);
axis([0 N -50 50]);
figure(3)
plot(sB,’k’); hold on; plot(ye,’r’);
xlabel(’month’); ylabel(’cos( ) fitting’);
title(’cos( ) fitting’);
% ARMA parameter estimation:
D=std(nB);
M=mean(nB);
Ny=((nB-M)/D);
model=armax(Ny,[1 12]);
% extract info from model structure
[A,B,C,D,F,LAM,T]=th2poly(model);
% print vector of ARMA(1 12) coeffs:
A
C
Seasonal data are treated in a number of ways, including additive or multiplicative
decompositions. One could think of something similar to the product of two signals
(as depicted in Fig.1.10), where one acts as the envelope of the other, the envelope
corresponding to seasonality.
By the way, in the case of the sunspot data, [9] proposes an ARMA(3, 2) model.
However, there is a lot of discussion on good models for these data. In particular,
[17] alludes to a nonlinear oscillation mechanism in the Sun that could justify the
observed behaviour.
3.9
Resources
3.9.1
MATLAB
3.9.1.1
Toolboxes
• The Large Time/frequency Analysis Toolbox (LTFAT):
http://ltfat.sourceforge.net/
• Control System Toolbox:
http://es.mathworks.com/products/control/

182
3
Linear Systems
• LTPDA Toolbox:
http://www.lisa.aei-hannover.de/ltpda/usermanual/ug/whatis.html/
• ARMASA Toolbox (ARMA modeling):
http://www.mathworks.com/matlabcentral/linkexchange/links/792-armasa
-toolbox
• MATLAB Econometrics Toolbox (time-series):
http://es.mathworks.com/products/econometrics/
• State Space Models Toolbox, SSM (time-series):
http://sourceforge.net/projects/ssmodels/
• Econometrics Toolbox, J.P. LeSage (time-series):
http://www.spatial-econometrics.com/
• E4 Toolbox (time-series):
http://pendientedemigracion.ucm.es/info/icae/e4/
3.9.1.2
Links to Toolboxes
• Statistical and Financial Econometrics Toolboxes (time series):
https://matlab11.wordpress.com/toolboxes/
• Kevin Sheppard page (time series):
https://www.kevinsheppard.com/Main_Page/
3.9.1.3
Matlab Code
• Educational MATLAB GUIs (demos):
http://users.ece.gatech.edu/mcclella/matlabGUIs/
• John Loomis, Convolution Demo:
http://www.johnloomis.org/ece202/notes/conv/
• Convolution Demo 1:
http://www.mathworks.com/matlabcentral/ﬁleexchange/48662-convolution
-demo-1
• Animated Convolution:
http://www.mathworks.com/matlabcentral/ﬁleexchange/4616-animated
-convolution
• Andrew Patton (time-series):
http://public.econ.duke.edu/ap172/code.html

3.9 Resources
183
3.9.2
Web Sites
• S. Boyd, Convolution listening demo:
https://web.stanford.edu/boyd/ee102/conv_demo.html
• Statwiki, Google site (books and data sets):
https://sites.google.com/a/crlstatistics.net/crlstatwiki/statwiki-home
• STAT 510 (Time-series Analysis, PennState):
https://onlinecourses.science.psu.edu/stat510
• OTexts (Forecastig tutorial):
https://www.otextsorg/
• Statsoft STATISTICA (time-series tutorials)
http://www.statsoft.com/Textbook/Time-Series-Analysis
• Vincent Arel-Bundock (data sets):
http://arelbundock.com/
• Time Series Data Library (data sets):
https://datamarket.com/data/list/?q=provider%3Atsdl
• Some time-series data sets (DUKE):
https://stat.duke.edu/mw/ts_data_sets.html
• Links to time-series data sets:
http://www.stats.uwo.ca/faculty/aim/epubs/datasets/default.htm
• FRED, Economic Research (economic data sets):
https://research.stlouisfed.org/fred2/release?rid=53
• Economic time-series data sets:
http://www.economagic.com/
• dataokfn.org (Gold prices):
http://data.okfn.org/data/core/gold-prices
• Climate data sets
http://climate.geog.udel.edu/climate/html_pages/download.html
• SILSO (Sunspot data ﬁles):
http://www.sidc.be/silso/home
References
1. A.M. Alonso, C. Garcia-Martos, Time Series Analysis, Lecture Presentation (Univer-
sity of Carlos III, Madrid, Spain, 2012). http://www.etsii.upm.es/ingor/estadistica/Carol/
TSAtema4petten.pdf
2. H. Bowerman, Forecasting and Time Series (South Western College Publishing, 2004)
3. G.E.P. Box, G.M. Jenkins, Time-Series Analysis: Forecasting and Control (Wiley, New Jersey,
2008)

184
3
Linear Systems
4. S. Boyd, Transfer Functions and Convolution, Lecture Presentation (Stanford University, Cal-
ifornia, 2002). https://web.stanford.edu/boyd/ee102/tf.pdf
5. J.H. Braslavsky, Mathematical Description of Systems, Lecture Notes, Lec.2 (University
of Newcastle, Newcastle, 2003). http://www.eng.newcastle.edu.au/jhb519/teaching/elec4410/
lectures/Lec02.pdf
6. M. Cannon, Discrete Systems Analysis, Lecture Presentation (Oxford University, Oxford,
2014). http://www.eng.ox.ac.uk/conmrc/dcs/dcs-lec2.pdf
7. R. Davis, V.K. Dedu, F. Bonye, Modeling and forecasting of gold prices on ﬁnancial markets.
Am. Int. J. Contemp. Res. 4(3), 107–113 (2014)
8. B.
Demirel,
State-Space
Representations
of
Transfer
Function
Systems,
Lecture
Notes (KTH, Sweden, 2013). https://people.kth.se/demirel/State_Space_Representation_of_
Transfer_Function_Systems.pdf
9. P. Faber, Sunspot Activity Modeling, Time Series Student Project (2010). http://tempforum.
neas-seminars.com/Attachment4364.aspx
10. G.C. Goodwin, K.S. Sin, Adaptive Filtering Prediction and Control (Dover, New York, 2009)
11. J. Grandell, Time Series Analysis, Lecture Notes (KTH, Sweden, 2000). http://www.math.kth.
se/matstat/gru/sf2943/ts.pdf
12. D. Heeger, Signals, Linear Systems, and Convolution, Lecture Notes (New York University,
New York, 2000). http://www.cns.nyu.edu/david/handouts/convolution.pdf
13. J. Huang, J.A. Bagnell, Gauss-Markov Models, Lecture Notes (Carnegie-Mellon Univer-
sity, Pittsburgh, 2000). http://www.cs.cmu.edu/./16831-f12/notes/F11/16831_gaussMarkov_
jonHuang.pdf
14. G. Kitagawa, Introduction to Time Series Modeling (Chapman and Hall, CRC, 2010)
15. H. Madsen, Time-series Analysis (Chapman & Hall, CRC, 2008)
16. H.J. Newton, ARMA Models, Lecture Notes (Texas A&M University, Texas, 2014). https://
www.stat.tamu.edu/jnewton/stat626/topics/lectures/topic11.pdf
17. M. Paluš, D. Novotna, Sunspot cycle: a driven nonlinear oscillator? Phys. Rev. Lett. 83(17),
1–4 (1999)
18. A. Papoulis, Predictable processes and wold’s decomposition: a review. IEEE Trans. Acoust.
Speech Signal Process. 33(4), 933–938 (1985)
19. D.S.G. Pollock, Lectures in the City, Lecture Notes (The University of London, London, 2007).
http://www.le.ac.uk/users/dsgp1/COURSES/BANKERS/PROBANK.HTM
20. D. Rowell, State-space Representation of LTI Systems, Lecture Notes (MIT Press, Cambridge,
2002). http://www.web.mit.edu/2.14/www/Handouts/StateSpace.pdf
21. H. Sandberg, Kalman Filtering, Lecture Presentation (Caltech, 2006). https://www.cds.caltech.
edu/murray/wiki/images/4/46/L_Kalman.pdf
22. N. Shimkin, Derivations of the Discrete-Time Kalman Filter, Lecture Notes (Technion, Israel,
2009). http://webee.technion.ac.il/people/shimkin/Estimation09/ch4_KFderiv.pdf
23. R. Smith, System Theory: Controllability, Observabiliy, Stability; Poles and Zeros, Lecture pre-
sentation (ETH Zurich, Zurich, 2014). http://control.ee.ethz.ch/ifa_cs2/RS2_lecture7.small.
pdf
24. U. Triacca, The Wold Decomposition Theorem, Lecture presentation (University of dell’Aquila,
Italy, 2000). www.phdeconomics.sssup.it/documents/Lesson11.pdf
25. R.S. Tsay, ARMA Models (University Chicago: Booth, Chicago, 2008). http://faculty.
chicagobooth.edu/ruey.tsay/teaching/uts/lecpdf
26. Transfer Functions and Bode Plots, Lecture Notes (Georgia Institute of Technology, Atlanta,
2005). http://users.ece.gatech.edu/mleach/ece3040/notes/bode.pdf
27. R. Weber, Time Series, Lecture Notes (University of Cambridge, Cambridge, 2000). http://
www.statslab.cam.ac.uk/rrw1/timeseries/t.pdf
28. W.-C.Yu,ForecastingMethods,LectureNotes(WinonaStateUniversity,Winona,2011).http://
course1.winona.edu/bdeppa/FIN335/Handouts/Ch5 Computing Handout in JMP and R.docx
29. E.Y. Zhang, X.F. Zhu, The recursive algorithms of Yule–Walker equation in generalized sta-
tionary prediction. Adv. Mater. Res. 756, 3070–3073 (2013)

Chapter 4
Analog Filters
4.1
Introduction
Signal ﬁltering is one of the main applications of signal processing, so textbooks usu-
ally include important chapters on this functionality. There are also books speciﬁcally
devoted to analog ﬁlters, like [4, 9, 13].
Filters are used for many purposes. For example, the case of radio tuning: a narrow
band-pass ﬁlter is used to select just one among the many radio stations that you can
ﬁnd across a large range of electromagnetic frequencies. In other applications, the
desire could be to let pass low or high frequencies, or, complementary, to attenuate
high or low frequencies. To avoid interferences notch ﬁlters, which reject a certain
frequency band, are used. There are cases that require a combination of attenuations
in certain frequency bands and ampliﬁcation in other frequency bands.
The ﬁltering desires can be speciﬁed in several ways. For instance in terms of
stable transfer functions. Alternatively, ideal (non feasible) ﬁltering can be initially
stated, and then approximated in a certain manner.
Filters can be implemented in several contexts, such as mechanic systems,
hydraulic systems, etc., [2, 10, 11]. Before computers were born, the electronic
circuits context was the main protagonist of ﬁlter developments, [4]. The synthesis
of circuits that implement desired ﬁlter transfer functions is a large traditional topic,
with many successful results.
Now that ﬁlters can be implemented using computers, it is still important to have
previous ideas from the classical approach with analog circuits. This is the purpose
of this chapter, which paves the way for the next chapter on digital ﬁlters.
4.2
Basic First Order Filters
The term “ﬁrst order” refers to having a transfer function with just one pole. A basic
ﬁrst order ﬁlter was already considered in the chapter before. This was a low-pass
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_4
185

186
4
Analog Filters
ﬁlter, made with a simple R-C circuit. Let us include again a diagram of the circuit,
Fig.4.1.
As reﬂected in Fig.4.1 the idea is to short-circuit the high frequency signals trough
the capacitor, so these signals are eliminated: only low frequencies would pass. Recall
that the transfer function of this circuit is:
G(s) = Vo(s)
Vi(s) =
1
1 + RCs
(4.1)
Figure4.2 shows the frequency response of the low-pass ﬁlter. Two straight lines
were added, which represent a simple manual approximation to the real frequency
response. The intersection of these lines determines a certain frequency ωc, which is
called a “corner frequency”. This allows for a simpliﬁed view of the ﬁlter: frequencies
below the corner frequency will pass; frequencies over the corner frequency will be
attenuated. The corner frequency coincides with the transfer function pole; at this
frequency the manual approximation and the real frequency response differ by 6 dB.
Figure4.3 shows the circuit of a high-pass ﬁlter. The idea is to provide a direct
way for the high frequency signal to cross the circuit, through the capacitor, while
low frequency signals are attenuated by this capacitor.
Fig. 4.1 A ﬁrst order
low-pass ﬁlter
R
C
Vi
Vo
Fig. 4.2 Frequency
response of the ﬁrst order
low-pass ﬁlter
10
-1
10
0
10
1
10
2
-30
-25
-20
-15
-10
-5
0
5
10
dB
rad/s

4.2 Basic First Order Filters
187
Fig. 4.3 A ﬁrst order
high-pass ﬁlter
R
C
Vi
Vo
The transfer function of the Fig.4.2 circuit is deduced as follows:
Vi(s) = R I(s) + ( 1/(Cs)) I(s)
(4.2)
Vo(s) = R I(s)
(4.3)
G(s) = Vo(s)
Vi(s) =
R
R + 1/Cs =
Cs
1 + RCs
(4.4)
Figure4.4, generated by Program 4.1, shows the frequency response of the high-pass
ﬁlter with R = 1 and C = 0.1. Like before, two straight lines were added as a simple
manual approximation. Again, the intersection of these lines determines a corner
frequency ωc. Now the simpliﬁed view of the ﬁlter is: frequencies over the corner
frequency will pass; frequencies below the corner frequency will be attenuated.
Fig. 4.4 Frequency
response of the ﬁrst order
high-pass ﬁlter
10
-1
10
0
10
1
10
2
-30
-25
-20
-15
-10
-5
0
5
10
dB
rad/s

188
4
Analog Filters
Program 4.1 Frequency response of high-pass ﬁlter
% Frequency response of high-pass filter
R=1; C=0.1; %values of the components
num=[C 0]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
w=logspace(-1,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
AG=20*log10(abs(G)); %take decibels
semilogx(w,AG,'k'); %plots decibels
axis([0.1 100 -30 10]);
grid;
ylabel('dB'); xlabel('rad/s');
title('frequency response of high-pass filter');
There is another interesting point of view that deserves a comment, on the basis
of the two ﬁlters just seen. In Fig.4.3 the slope of the inclined straight line is
–20dB/decade, this corresponds to high frequencies. At high frequencies the trans-
fer function (4.1) tends to 1/RCs, which corresponds to an integrator. In Fig.4.4 the
slope of the inclined straight line is 20dB/decade, and that corresponds to deriva-
tion. Integration or derivation behaviour can be observed looking at the steady-state
responses to a square signal, provided the frequency of this signal falls into the range
of the inclined slope.
Let us take the low-pass ﬁlter with R = 1 and C = 0.1, and a square signal with a
frequency over ωc = 1/RC = 10rad/s. Using the Program 4.2 we compute and plot,
in Fig.4.5, the response of the ﬁlter to this square signal.
The area integral of the squares are triangles. The signal in Fig.4.5 is almost
triangular, reﬂecting the integration done by the ﬁlter at low frequencies. From other
perspective, it can be noticed that the signal shows a repeated pattern: a partial charge
and then discharge of the capacitor.
Fig. 4.5 Response of the
ﬁrst order low-pass ﬁlter to a
square signal
1.5
1.55
1.6
1.65
1.7
1.75
1.8
1.85
1.9
1.95
2
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
seconds

4.2 Basic First Order Filters
189
Program 4.2 Response to square signal, low-pass ﬁlter
% Response to square signal, low-pass filter
R=1; C=0.1;%values of the components
num=[1]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
% Input square signal
fu=7; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=2000; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(2-tiv); %time intervals set (2 seconds)
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
%plots last 1/2 second of output signal:
plot(t(3001:4000),y(3001:4000),'k');
xlabel('seconds');
title('response to square signal, low-pass filter');
Now, let us take the high-pass ﬁlter with R = 1 and C = 0.1, and a square signal
with a frequency below ωc = 1/RC = 10rad/s. The response of the ﬁlter to this
square signal is shown in Fig.4.6, which has been generated by the Program 4.3.
The derivative of an ideal square signal should have very large spikes, since if
the transitions from one to other amplitude were instantaneous, the corresponding
derivatives were inﬁnite. The spikes in Fig.4.6 show the derivative action of the ﬁlter
at low frequencies.
Fig. 4.6 Response of the
ﬁrst order high–pass ﬁlter to
a square signal
20
25
30
35
40
45
50
55
60
-3
-2
-1
0
1
2
3
seconds

190
4
Analog Filters
Program 4.3 Response to square signal, high-pass ﬁlter
% Response to square signal, high-pass filter
R=1; C=0.1;%values of the components
num=[C 0]; % transfer function numerator;
den=[R*C 1]; %transfer function denominator
G=tf(num,den); %transfer function
% Input square signal
fu=0.1; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(60-tiv); %time intervals set (60 seconds)
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
%plots last 40 seconds of output signal:
plot(t(2001:6000),y(2001:6000),'k');
axis([20 60 -3 3]);
xlabel('seconds');
title('response to square signal, low-pass filter');
Circuits having only R, L, C components are passive. Their transfer functions
are always stable. Passive circuits do not amplify; in consequence their frequency
response amplitude is always ≤0dB (0dB corresponds to gain = 1). The examples
considered in this section were passive circuits.
4.3
A Basic Way for Filter Design
The manual approximation just introduced with the examples in the previous section,
can be extended for more complex frequency responses and may be useful for an
initial speciﬁcation of the desired transfer function. This manual approximation is
acceptable for well separated poles and zeros.
Readers familiar with the Bode diagram may already know how to approximate
frequency responses by straight lines. In any case, it is interesting to consider some
examples pertaining to ﬁlters.
Suppose we want a low-pass ﬁlter, with a corner frequency ωa = 10. Let us draw
a manual approximation as in Fig.4.7. The procedure for manual approximation is to
proceed from left to right (from low frequency to high frequency), when you arrive to
a pole you start a new straight line adding –20dB/decade to the slope of the previous
straight line, when you arrive to a zero you do the same but adding +20dB/decade
to the slope.
Looking at Fig.4.7 the corresponding transfer function is:
G(s) =
10
s + 10
(4.5)
This transfer function has a pole p = −10 (the desired corner frequency is ωa = 10).

4.3 A Basic Way for Filter Design
191
Fig. 4.7 A desired low-pass
frequency response
10
-1
10
0
10
1
10
2
-20
-15
-10
-5
0
5
10
dB
rad/s
wa 
Fig. 4.8 A desired high-pass
frequency response
10
0
10
1
10
2
10
3
10
4
-20
-15
-10
-5
0
5
10
dB
rad/s
wa 
Now we want a high-pass ﬁlter, with a corner frequency ωa = 100. We draw a
manual approximation as in Fig.4.8.
Looking at Fig.4.8 the corresponding transfer function is:
G(s) =
s
s + 100
(4.6)
This transfer function has a zero at the origin and a pole p = −100 (the desired corner
frequency is ωa = 100). The zero at the origin causes the initial leftmost straight
line to have 20dB/decade slope.
Suppose we want a band-pass ﬁlter, with the band Δω between ωa = 10 and
ωb = 100. Figure4.9 shows a manual approximation.

192
4
Analog Filters
Fig. 4.9 A desired
band-pass frequency
response
10
0
10
1
10
2
10
3
-20
-15
-10
-5
0
5
10
dB
rad/s
wa 
wb 
Fig. 4.10 A desired
band-stop frequency
response
10
0
10
1
10
2
10
3
-20
-15
-10
-5
0
5
dB
rad/s
wa 
wb 
Now, looking at Fig.4.9 we can write the corresponding transfer function:
G(s) =
100 s
(s + 10) (s + 100)
(4.7)
Since we want to reject a band Δω between ωa = 10 and ωb = 100, we want a
band-stop ﬁlter (a notch ﬁlter). Figure4.10 shows a manual approximation.
Based on Fig.4.10 we write the corresponding transfer function:
G(s) =
(s + 25) (s + 40)
(s + 10) (s + 100)
(4.8)

4.3 A Basic Way for Filter Design
193
Fig. 4.11 A desired
frequency response:
approximation and reality
10
-2
10
-1
10
0
10
1
10
2
10
3
-25
-20
-15
-10
-5
0
5
dB
rad/s
The values of the zeros in this last example can be modiﬁed, depending on how much
attenuation is needed in the stop band. Likewise, it could be convenient to modify
the values of the poles to expand their mutual distance.
Now let us check our approach considering a more complex example, as repre-
sented in Fig.4.11. From the ﬁgure we obtain the corresponding transfer function:
G(s) = (s + 0.1) (s + 25) (s + 40)
(s + 1) (s + 10) (s + 100)
(4.9)
We plot on the same Fig.4.11 the real frequency response |G(jω)| in decibels,
obtained with the Program 4.4, to be compared with the manual approximation.
Program 4.4 Abstract design, desired ﬁlter
% Abstract design, desired filter
% transfer function numerator:
num1=conv([1 0.1],[1 25]); num=conv(num1,[1 40]);
%transfer function denominator:
den1=conv([1 1],[1 10]); den=conv(den1,[1 100]);
w=logspace(-2,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
AG=20*log10(abs(G)); %take decibels
semilogx(w,AG,'--b'); %plots decibels
axis([0.01 1000 -25 5]);
grid;
ylabel('dB'); xlabel('rad/s');
title('frequency response of desired filter');
Along this section only simple, real and negative poles and zeros have been spec-
iﬁed. Negative poles ensure stability. Transfer functions with zeros in the right hand
semiplane are called “non-minimum-phase” transfer functions. This type of transfer
functions are a source of difﬁculty in he context of control systems. We prefer to

194
4
Analog Filters
specify “minimum-phase transfer functions”, which have no zeros in the right hand
semiplane. Multiple poles (several poles having the same value) and complex poles
cause larger errors of the manual approximation with respect to the real frequency
response, so we do not use them.
Up to now this section was taking into consideration only |G(jω)| speciﬁcations,
without looking at phases. As a matter of fact, the complete manual approximation
procedure includes phases: when you add +20dB/decade to the gain slope you
also increase the phase by 90◦(along a sigmoid curve vs. frequency); and when
you add –20dB/decade to slope, you add –90◦to phase along a sigmoid. In other
words: the speciﬁcation of |G(jω)| includes implicitly the speciﬁcation of phases:
both speciﬁcations are connected.
The implementation of a transfer function using an active electronic circuit can
be done in several ways. For instance by partial fraction expansion, using as many
operational ampliﬁers as fractions, and then adding the outputs of the ampliﬁers.
Or by using Sallen-Key active circuit topologies, [5, 8]. Passive circuits may be
preferred, but they have limitations. Let us comment some details about passive
circuits.
Suppose you depart from a circuit like the one represented in Fig.4.12. The design
should focus on the impedance Z(s). The theory of network synthesis, [12], offer
several important results concerning impedances implemented with passive circuits.
Let us mention some of them:
• When s →∞, Z(s) tends to Ks, or K, or K/s. In other words, if
Z(s) = Ksm + b1 sm−1 + · · ·
sn + a1 sn−1 + · · · .
(4.10)
then m and n cannot differ in more than 1
• None of the Z(s) poles and zeros are in the right hand semiplane
• There are no multiple poles nor multiple zeros
• Z(s) is a positive real function, that is:
Fig. 4.12 A ﬁlter structure
R
Z(s)
Vi
Vo

4.3 A Basic Way for Filter Design
195
Re Z(jω) ≥0 , ∀ω
(4.11)
Since it is a positive real function, if we represent Z(jω) on the complex plane, the
corresponding curve will lay on the right hand semiplane.
The power dissipation of the impedance is given by:
Re Z(jω) · I2
(4.12)
If we use a passive circuit with only L and C components, the circuit is not
dissipative and Re Z(jω) = 0. In this case when s →∞, Z(s) tends to Ks, or K/s. The
poles and zeros of L-C circuits are interleaved: between every two poles there must
be one zero, and vice-versa. The synthesis of lossless impedances can systematically
be done according with Foster forms, Fig.4.13, or with Cauer forms, Fig.4.14.
L1
L2
Ln
C1
C2
Cn
C0
C1
L1
C0
C2
L2
Ln
Cn
(a)
(b)
Fig. 4.13 Foster forms

196
4
Analog Filters
C1
L1
Cn
Ln
L1
C1
Ln
Cn
(a)
(b)
Fig. 4.14 Cauer forms
Circuits with only R and C components are dissipative. In this case when s →∞,
Z(s) tends to K, or K/s. All poles are simple, negative and real. The root nearest to
the origin is a pole. The poles and zeros of R-C circuits are interleaved. The synthesis
of this kind of impedances can also be done according with Foster or Cauer forms
(substitute L by R).
Circuits with only R and L components are also dissipative. In this case when
s →∞, Z(s) tends to Ks, or K. All poles are simple, negative and real. The root
nearest to the origin is a zero. The poles and zeros of R-L circuits are interleaved.
Again, the synthesis of this kind of impedances can be done with Foster or Cauer
forms (substitute C by R).
A brief practical synthesis of network theory is given in [6]. The article [1] contains
a review of LC circuit design methods and a modern treatment of this topic. It would
be also recommended to read the interesting article of Darlington on the history of
passive circuit theory, [3].
In general this section contributes to highlight the importance of digital processors
in the ﬁeld of signal ﬁltering, since they give more design freedom. Anyway, it is
interesting to show some ﬁlter structures, because in part they introduce certain
ﬁltering algorithms.

4.4 Causality and the Ideal Band-Pass Filter
197
4.4
Causality and the Ideal Band-Pass Filter
Real, physical systems are causal. Effects follow causes: a ball moves after being
kicked. Circuits made with real components are causal. Filters made with computers
maybe not.
Linear causal systems have an impulse response g(t) such that:
g(t) = 0 , t < 0
(4.13)
It is important to consider that linear causal systems satisfy the Paley–Wiener con-
dition:
+∞

−∞
| ln |G(jω)||
1 + ω2
dω < ∞
(4.14)
This condition means that |G(jw)| can be zero at some frequencies but cannot be
zero over a ﬁnite band of frequencies. Other consequences are the following:
• |G(jw)| cannot have an inﬁnitely sharp cut-off from pass-band to stop-band
• ReG(jw) and ImagG(jw) are interdependent, so therefore |G(jw)| and φ(jω)
are also interdependent (in other words: |G(jw)| and φ(jω) cannot be arbitrarily
chosen)
Now let us consider again the example given in the chapter introduction: the radio
tuning. The problem will be illustrated with a ﬁgure. But before, it is pertinent to add
some considerations about signals.
The energy of a signal y(t) is given by:
E =
+∞

−∞
|f (t)|2 dt
(4.15)
The following concerns ﬁnite-energy signals. A signal y(t) is band-limited (BL) if
its Fourier transform is zero outside a ﬁnite frequency interval. That also means that
the PSD of the signal is zero outside a ﬁnite frequency band, which we call the
“bandwidth” of the signal. A signal y(t) is time-limited (TL) if y(t) = 0 for |t| > τ.
Paley and Wiener also showed that a time-limited signal cannot be band-limited,
and vice-versa. See [7] (Chap.5), for more details.
The particular case of the impulse δ(t), which is instantaneous, is illustrative. The
spectrum of this signal is ﬂat: it extends with amplitude 1 to inﬁnite frequency. In
practical terms, spikes are signals with a powerful interfering capability: you hear
in your car radio the spikes of neighbour cars (if they do not have anti-interference
devices), no matter the station frequency you are tuning.
To avoid mutual interference radio stations limit the bandwidth of the signals they
transmit. Figure4.15 shows on top a certain radio band, and several radio stations r1,

198
4
Analog Filters
Radio band
Δωr1
Δωr2
Δωr5
Δωr2
Filter
Signal
Fig. 4.15 Radio tuning with an ideal ﬁlter
r2, etc. are using parts of this band, with bandwidths Δωr1, Δωr2, etc. In the middle
of Fig.4.15 we represent an ideal ﬁlter which is used to extract from the radio band
the bandwidth of interest, for instance Δωr2, which belongs to r2. Below the ideal
ﬁlter, the Fig.4.15 shows the effect of the ﬁlter: to extract the signal y(t) transmitted
by r2.
As shown in Fig.4.15 the ideal ﬁlter should have a ﬂat amplitude response in a
certain bandwidth –the pass-band-, have zero response outside this bandwidth –the
stop-bands-, and have vertical transitions from the pass-band to the stop-bands. In
view of the Paley–Wiener condition this is impossible with a causal ﬁlter, but we can
approximate this target.
4.5
Three Approximations to the Ideal Low-Pass Filter
In this section three relevant approximations to the ideal low-pass ﬁlter are described.
It sufﬁces with the study of the low-pass case, since it can be easily translated to the
ideal band-pass or high-pass ﬁlters. Actually, there exist MATLAB Signal Processing
Toolbox routines for such purposes.
In order to use the functions of the MATLAB Signal Processing Toolbox related
with ideal ﬁlter approximations, it is important to know how these approximations
are speciﬁed. Figure4.16 shows the frequency response amplitude of a low-pass
ﬁlter. Some lines are added to indicate parameters of interest.
In general the frequency response amplitude of the ﬁlters to be studied in this
section has a pass-band part and a stop-band part. Each of the bands can be monotonic
(no oscillations) or have ripple (oscillations). These bands can be more or less narrow,
their wide being speciﬁed in decibels. The transition between bands should be sharp.
There is a trade-off between the ﬂatness of the bands and the sharpness of the
transition. The Butterworth ﬁlter is maximally ﬂat, with monotonic pass and stop
bands. The Chebyshev type 1 ﬁlter allows ripple only in the pass-band, getting a
faster roll-off. Alternatively, the Chebyshev type 2 ﬁlter allows ripple only in the

4.5 Three Approximations to the Ideal Low-Pass Filter
199
Fig. 4.16 Speciﬁcation of a
low-pass ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
pass-band 
stop-band 
stop-band, obtaining also a good roll-off. The elliptic ﬁlter has ripple in both the pass
and the stop bands, providing the fastest roll-off.
Let us describe in detail the ﬁlters just mentioned.
4.5.1
Butterworth Filter
The Butterworth ﬁlter provides the best Taylor series approximation to the ideal low-
pass ﬁlter. The Butterworth ﬁlter has n poles and no zeros. The order of the ﬁlter is
n. According with the ﬁltering needs, the designer decides a value for n. The slope
of the roll-off, in a logarithmic plane, is –20 ndB/decade: the larger n the steeper the
roll-off (however, large values of n can cause numerical difﬁculties). The transfer
function for order n is such that:
|G(jω)|2 =
1
1 + (ω/ωc)2n
(4.16)
The cut-off frequency ωc of the Butterworth ﬁlter is that frequency where the mag-
nitude response of the ﬁlter is √1/2.
The denominator of the transfer function is a Butterworth polynomial. Here is a
table of the ﬁrst polynomials:

200
4
Analog Filters
n
polynomial
1
(s + 1)
2
s2 + 1.4142 s + 1
3
(s + 1)(s2 + s + 1)
4
(s2 + 0.7654 s + 1) (s2 + 1.8478 s + 1)
5
(s + 1) (s2 + 0.6180 s + 1) (s2 + 1.9319s + 1)
Figure4.17 shows the frequency response of a Butterworth ﬁlter with n = 5. Note
that the magnitude has been represented in a linear scale.
The Fig.4.17 has been obtained with the Program 4.5, which uses the butter()
function. The cut-off frequency we have speciﬁed is 10rad/s.
Program 4.5 Frequency response of Butterworth ﬁlter
% Frequency response of Butterworth filter
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
w=logspace(0,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
axis([1 100 0 1.1]);
grid;
ylabel('Gain'); xlabel('rad/s');
title('frequency response of 5th
Butterworth filter');
Figure4.18 depicts the poles (plotted as diamonds) of the Butterworth ﬁlter on
the complex plane. The poles are placed on a semi-circumference of radius ωc at
equally spaced points.
Fig. 4.17 Frequency
response of 5th order
Butterworth ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s

4.5 Three Approximations to the Ideal Low-Pass Filter
201
Fig. 4.18 Poles of 5th order
Butterworth ﬁlter
-12
-10
-8
-6
-4
-2
0
2
-10
-8
-6
-4
-2
0
2
4
6
8
10
The Fig.4.18 has been generated by the Program 4.6. Instead of using the pzmap()
function, we opted for a more ﬂexible coding, using the pole() function, in order to
show a dashed circumference arc and the poles in the form of diamonds.
Program 4.6 Pole-zero map of Butterworth ﬁlter
% Pole-zero map of Butterworth filter
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
P=pole(G); %find the poles of G
alfa=-(pi/2):-0.1:-(3*pi/2); %set of angle values
%plots half a circumference:
plot(wc*cos(alfa),wc*sin(alfa),'--');
hold on;
plot(P,'dk'); %pole map
title('pole-zero map of 5th Butterworth filter');
Figure4.19, which has been obtained with the Program 4.7, shows the step
response of the 5th Butterworth ﬁlter. The step response has moderate overshoot
and ringing.
Program 4.7 Step response of Butterworth ﬁlter
% Step response of Butterworth filter
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
step(G,'k'); %step response of G
title('step response of 5th Butterworth filter');

202
4
Analog Filters
Fig. 4.19 Step response of
5th order Butterworth ﬁlter
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (sec)
Amplitude
Fig. 4.20 Effect of n on the
frequency response of the
Butterworth ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
2
4
6
8
Program 4.8 has been used to generate the Fig.4.20 in which we compare several
frequency responses of Butterworth ﬁlters, to visualize the effect of the order of the
ﬁlter. Using n = 4 instead of n = 2 has a dramatic effect; for larger values of n the
improvements are less and less visible.
Program 4.8 Comparison of frequency response of Butterworth ﬁlters
% Comparison of frequency response
% of Butterworth filters
wc=10; % desired cut-off frequency
N=2; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
w=logspace(0,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
hold on;
axis([1 100 0 1.1]);
grid;
ylabel('Gain'); xlabel('rad/s');
title('frequency response of Butterworth filter');

4.5 Three Approximations to the Ideal Low-Pass Filter
203
Fig. 4.21 Response of 5th
order Butterworth ﬁlter to a
square wave
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
for N=4:2:8, % more orders of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
end;
To complete our review of the Butterworth ﬁlter, Fig.4.21 shows the response of
the ﬁlter when it ﬁlters a square wave. The frequency of the square wave is 1Hz,
so the signal is inside the low-pass band. The effect of the ﬁlter is to extract the
fundamental sinusoidal harmonic of the signal, attenuating the rest of the harmonics.
Figure4.21 has been obtained with the Program 4.9.
Program 4.9 Response of Butterworth ﬁlter to square signal
% Response of Butterworth filter to square signal
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
% Input square signal
fu=1; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(6-tiv); %time intervals set (6 seconds)
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
plot(t,y,'k'); %plots output signal
axis([0 6 -1.5 1.5]);
xlabel('seconds');
title('response to square signal,
5th Butterworth filter');

204
4
Analog Filters
Fig. 4.22 Frequency
response of 5th order
Chebyshev 1 ﬁlter
100
101
102
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
4.5.2
Chebyshev Filter
There are two types of Chebyshev ﬁlters: type 1 has ripple in the pass-band and is
monotonic in the stop-band, type 2 has ripple in the stop-band and is monotonic in
the pass-band. In both type the order of the ﬁlter is n, with n being the number of
poles. Like in the Butterworth ﬁlter, the larger n the steeper the roll-off, but large
values of n can cause numerical difﬁculties.
From this point on, this section repeats the Programs 4.5, 4.6, 4.7, 4.8 and 4.9,
with only small changes in an initial fragment which is equal for all of them. Let us
denote this fragment as “FrB” for the case of Butterworth ﬁlter. The fragment FrB
is the following:
Fragment 4.10 FrB
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
To generate the next Figs.4.22, 4.23, 4.24, 4.25 and 4.26 we just substituted in the
corresponding programs the fragment FrB by another fragment, that we call “FrC1”,
devoted to the Chebyshev type 1 ﬁlter. The fragment FrC1 is the following:
Fragment 4.11 FrC1
wc=10; %desired cut-off frequency
N=5; %order of the filter
R=0.5; %decibels of ripple in the pass band
%analog Chebyshev 1 filter:
[num,den]=cheby1(N,R,wc,'s');

4.5 Three Approximations to the Ideal Low-Pass Filter
205
Fig. 4.23 Poles of 5th order
Chebyshev 1 ﬁlter
-10
-8
-6
-4
-2
0
2
-15
-10
-5
0
5
10
15
Fig. 4.24 Step response of
5th order Chebyshev 1 ﬁlter
0
0.5
1
1.5
2
2.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (sec)
Amplitude
Thus, it is not necessary to include the listings of the programs for the ﬁgures
concerning the Chebyshev type 1 ﬁlter.
The idea of the fragment substitution will be re-used for the rest of ﬁlters in this
section.
4.5.2.1
Type 1
The Chebyshev type 1 ﬁlter minimizes the absolute difference between the ideal
and actual magnitude of the frequency response over the complete pass-band, by
incorporating an equal ripple in this band. Its transfer function for order n is such
that:
|G(jω)|2 =
1
1 + ε2 T 2n (ω/ωp)
(4.17)

206
4
Analog Filters
Fig. 4.25 Effect of n on the
frequency response of the
Chebyshev 1 ﬁlter
100
101
102
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
2
4
6
8
Fig. 4.26 Response of 5th
order Chebyshev 1 ﬁlter to a
square wave
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
where Tn() is a Chebyshev polynomial of the nth order. These polynomials may be
generated recursively by using the relationship:
Tn+1(ω) = 2 ω Tn(ω) −Tn−1(ω)
(4.18)

4.5 Three Approximations to the Ideal Low-Pass Filter
207
Here is a table of the ﬁrst Chebyshev polynomials:
n
polynomial
1
ω
2
2 ω2−1
3
4ω3−3ω
4
8ω4−8ω2 +1
Along the pass-band T 2
n () oscillates between 0 and 1, and this causes |G(jω)| to
oscillate between 1 and 1 /
√
1 + ε2. Thus, the peak-to-peak value of pass-band
ripple in decibels, is:
RP = 20 log

1
1/
√
1 + ε2

= 10 log (1 + ε2)
(4.19)
The MATLAB function cheby1() recommends to specify a value of Rp of 0.5dB.
The smaller Rp, the wider is the transition from pass-band to stop-band.
The Chebyshev type 1 ﬁlter has n poles and no zeros. The cut-off frequency ωp
of the Chebyshev ﬁlter is the frequency at which the transition from pass-band to
stop-band has the value −−Rp dB.
The fragment FrC1 uses the cheby1() function. The cut-off frequency we have
speciﬁed is 10rad/s.
Figure4.22 shows the frequency response of a Chebyshev ﬁlter with n = 5. Note
that the magnitude has been represented in a linear scale.
Figure4.23 depicts the poles (plotted as diamonds) of the Chebyshev type 1 ﬁlter
on the complex plane. The ﬁgure keeps for reference a semi-circumference of radius
ωc . The poles are placed on a semi-ellipse. The eccentricity of the ellipse depends
on ε.
Figure4.24 shows the step response of the 5th Chebyshev type 1 ﬁlter. This step
response has more ringing than the Butterworth ﬁlter step response.
Several frequency responses of the Chebyshev type 1 ﬁlter, for different values
of n, are compared in Fig.4.25. Again, using n = 4 instead of n = 2 has a dramatic
effect; values of n > 8 do not get much improvements.
Figure4.26 shows the response of the Chebyshev type 1 ﬁlter when it ﬁlters a
square wave. The frequency of the square wave is 1Hz being inside the pass-band.
The effect of the ﬁlter is to extract the fundamental harmonic.
4.5.2.2
Type 2
The Chebyschev type 2 ﬁlter minimizes the absolute difference between the ideal
and actual magnitude of the frequency response in the stop-band, having equal ripple
in this band. The pass-band is monotonic. The transfer function of this ﬁlter for order
n is such that:

208
4
Analog Filters
|G(jω)|2 =
1
1 + (1/ε2 T 2n (ωs/ω))
(4.20)
Along the stop-band T 2
n () oscillates between 0 and 1, and this causes |G(jω)| to
oscillate between 0 and 1 /

1 + 1/ ε2. Thus, the peak-to-peak value of stop-band
ripple in decibels, is:
RP = 20 log

1
1/

1 + 1/ε2

= 10 log (1 + 1/ε2)
(4.21)
The MATLAB function cheby2() recommends to specify a value of Rs of 20dB. The
smaller Rs, the wider is the transition from pass-band to stop-band.
The Chebyshev type 2 ﬁlter has both poles and zeros. The cut-off frequency ωs of
the Chebyshev type 2 ﬁlter is the frequency at which the transition from pass-band
to stop-band crosses the value Rs dB over the bottom.
To generate the next Figs.4.27, 4.28, 4.29, 4.30 and 4.31 we substituted in the
corresponding programs (Butterworth ﬁlter) the fragment FrB by another fragment,
that we call “FrC2”, devoted to the Chebyshev type 2 ﬁlter. The fragment FrC2 is
the following:
Fig. 4.27 Frequency
response of 5th order
Chebyshev 2 ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s

4.5 Three Approximations to the Ideal Low-Pass Filter
209
Fig. 4.28 Poles and zeros of
5th order Chebyshev 2 ﬁlter
-18
-16
-14
-12
-10
-8
-6
-4
-2
0
2
-15
-10
-5
0
5
10
15
Fig. 4.29 Step response of
5th order Chebyshev 2 ﬁlter
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (sec)
Amplitude
Fragment 4.12 FrC2
wc=10; %desired cut-off frequency
N=5; %order of the filter
R=20; %decibels of ripple in the stop band
%analog Chebyshev 2 filter:
[num,den]=cheby2(N,R,wc,'s');
Notice that the fragment FrC2 uses the cheby2() function. Like in the previous
ﬁlters we have speciﬁed a cut-off frequency of 10rad/s.
Figure4.27 shows the frequency response of a Chebyshev type 2 ﬁlter with n = 5.
Note that the magnitude has been represented in a linear scale.
Figure4.28 depicts the poles (plotted as diamonds) and zeros (plotted as circles)
of the Chebyshev type 2 ﬁlter on the complex plane. The ﬁgure keeps a a semi-
circumference of radius ωc to serve as reference.
The Fig.4.28 has been generated by the Program 4.13, using the pole() function
and the zero() function.

210
4
Analog Filters
Fig. 4.30 Effect of n on the
frequency response of the
Chebyshev 2 ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
2
4
6
8
Fig. 4.31 Response of 5th
order Chebyshev 2 ﬁlter to a
square wave
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Program 4.13 Pole-zero map of Chebyshev 2 ﬁlter
% Pole-zero map of Chebyshev 2 filter
wc=10; % desired cut-off frequency
N=5; % order of the filter
R=20; %decibels of ripple in the stop band
%analog Chebyshev 2 filter :
[num,den]=cheby2(N,R,wc,'s');
G=tf(num,den); %transfer function
P=pole(G); %find the poles of G
Z=zero(G); %find the zeros of G
alfa=-(pi/2):-0.1:-(3*pi/2); %set of angle values
%plot half a circunference:
plot(wc*cos(alfa),wc*sin(alfa),'--');
hold on;

4.5 Three Approximations to the Ideal Low-Pass Filter
211
plot(P,'dk'); %pole map
axis([-18 2 -15 15]);
hold on;
plot(Z,'ok'); %zero map
title('pole-zero map of 5th Chebyshev 2 filter');
Figure4.29, shows the step response of the 5th Chebyshev type 2 ﬁlter.
The frequency responses of Chebyshev type 2 ﬁlters, with several values of n,
are compared in Fig.4.30. Similar comments as in previous responses comparisons
apply.
Figure4.31 shows the response of the ﬁlter when it ﬁlters a 1Hz square wave. The
distortion near the peaks is apparent. This is due to the stop-band ripples, which let
pass some high harmonics of the signal.
4.5.3
Elliptic Filter
Elliptic ﬁlters have equalized ripple in both the pass-band and the stop-band. The
amount of ripple in each band is independently adjustable. Elliptic ﬁlters minimize
transition width. They frequently are the best option, needing less ﬁlter order to
achieve most usual requirements. The transfer function of elliptic ﬁlters are such
that:
|G(jω)|2 =
1
1 + ε2 R2n(ζ, ω/ωc)
(4.22)
where Rn() is the nth order elliptic rational function, ε is the ripple factor and ζ is the
selectivity factor. The ripple factor speciﬁes the pass-band ripple, and a combination
of the ripple factor and the selectivity factor specify the stop-band ripple. As the
ripple in the stop-band approaches zero, the ﬁlter tends to become a Chebyshev type
1 ﬁlter; as the ripple in the pass-band approaches zero, the ﬁlter tends to become a
Chebyshev type 2 ﬁlter. If both pass-band and stop-band ripples approach zero, then
the ﬁlter tends to become a Butterworth ﬁlter.
The elliptic rational functions are as follows:
R1(ζ, x) = x
R2(ζ, x) =
(t + 1) x2 −1
(t −1) x2 + 1
R3(ζ, x) = x
(1 −x2
p) (x2 −x2
z )
(1 −x2z ) (x2 −x2p)
R4(ζ, x) = R2(R2(ζ, ζ), R2(ζ, x))
(4.23)

212
4
Analog Filters
Fig. 4.32 Frequency
response of 5th order elliptic
ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Fig. 4.33 Poles of 5th order
elliptic ﬁlter
-10
-8
-6
-4
-2
0
2
-15
-10
-5
0
5
10
15
where
t =

1 −1/ζ2
G =

4 ζ2 + (4ζ2(ζ2 −1))2/3
x2
p =
2 ζ2 √
G
√
8 ζ2(ζ2 + 1) + 12 Gζ2 −G3 −
√
G3
x2
z = ζ2/x2
p
(4.24)
The elliptic ﬁlter has both poles and zeros. The cut-off frequency ωc of this ﬁlter
is the frequency at which the transition from pass-band to stop-band has the value
−−Rp dB.
To generate the next Figs.4.32, 4.33, 4.34, 4.35 and 4.36 we substituted in the
corresponding programs (Butterworth ﬁlter) the fragment FrB by another fragment,
that we call “FrE”, devoted to the elliptic ﬁlter. The fragment FrE is the following:

4.5 Three Approximations to the Ideal Low-Pass Filter
213
Fig. 4.34 Step response of
5th order elliptiic ﬁlter
0
0.5
1
1.5
2
2.5
3
3.5
4
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (sec)
Amplitude
Fig. 4.35 Effect of n on the
frequency response of the
elliptic ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
2
4
6
8
Fig. 4.36 Response of 5th
order elliptic ﬁlter to a
square wave
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
seconds

214
4
Analog Filters
Fragment 4.14 FrE
wc=10; %desired cut-off frequency
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog elliptic filter:
[num,den]=ellip(N,Rp,Rs,wc,'s');
Notice that the fragment FrE uses the ellip() function. Like in the previous ﬁlters
we have speciﬁed a cut-off frequency of 10rad/s.
Figure4.32 shows the frequency response of a elliptic ﬁlter with n = 5. Note that
the magnitude has been represented in a linear scale.
Figure4.33 depicts the poles (plotted as diamonds) and the zeros (plotted as cir-
cles) of the elliptic ﬁlter on the complex plane. We keep the semi-circumference of
radius ωc for reference.
Program 4.15 Pole-zero map of elliptic ﬁlter
% Pole-zero map of elliptic filter
wc=10; % desired cut-off frequency
N=5; % order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog elliptic filter:
[num,den]=ellip(N,Rp,Rs,wc,'s');
G=tf(num,den); %transfer function
P=pole(G); %find the poles of G
Z=zero(G); %find the zeros of G
alfa=-(pi/2):-0.1:-(3*pi/2); %set of angle values
%plot half a circunference:
plot(wc*cos(alfa),wc*sin(alfa),'--');
hold on;
plot(P,'dk'); %pole map
hold on;
plot(Z,'ok'); %zero map
title('pole-zero map of 5th elliptic filter');
Figure4.34, shows the step response of the 5th elliptic ﬁlter. Notice the light
attenuation of the oscillations.
We compare in Fig.4.35 several frequency responses of elliptic ﬁlters, to visualize
the effect of the order of the ﬁlter. Now the effect of using n = 4 instead of n = 2 is
more impressive; while for n > 6 changes are difﬁcult to notice.
As shown by Fig.4.36 the response of the ﬁlter to a square wave exhibit some
distortion near the peaks. Like in the case of Chebyshev type 2 ﬁlter, this is due to
the stop-band ripples.

4.5 Three Approximations to the Ideal Low-Pass Filter
215
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Butterworth
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Chebyshev
 1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Chebyshev
 2
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Elliptic
Fig. 4.37 Comparison of the frequency response of the four ﬁlters
4.5.4
Comparison of Filters
Let us summarize the main characteristics of the four ﬁlters just studied. With respect
to the frequency domain, an important aspect is the sharpness of the transition from
the pass-band to the stop-band. Figure4.37 shows a view of the frequency responses
of the four ﬁlters in the same ﬁgure, to facilitate the visual comparison. The elliptic
ﬁlter has the fastest roll-off, but the price to pay is ripples in the pass-band and the
stop-band.
Program 4.16 Comparison of frequency response of the 4 ﬁlters
% Comparison of frequency response of the 4 filters
wc=10; %desired cut-off frequency
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
w=logspace(0,2); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response

216
4
Analog Filters
%plots linear amplitude:
subplot(2,2,1); semilogx(w,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('rad/s');
title('Butterworth');
%analog Chebyshev 1 filter:
[num,den]=cheby1(N,Rp,wc,'s');
G=freqs(num,den,w); %computes frequency response
%plots linear amplitude:
subplot(2,2,2); semilogx(w,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('rad/s');
title('Chebyshev 1');
%analog Chebyshev 2 filter:
[num,den]=cheby2(N,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response
%plots linear amplitude:
subplot(2,2,3); semilogx(w,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('rad/s');
title('Chebyshev 2');
%analog elliptic filter:
[num,den]=ellip(N,Rp,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response
%plots linear amplitude:
subplot(2,2,4); semilogx(w,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('rad/s');
title('Elliptic');
Figure4.38 plots in the same ﬁgure the step response of the four ﬁlters, in order
to compare the time domain behaviour of the ﬁlters.
Program 4.17 Comparison of step response of the 4 ﬁlters
% Comparison of step response of the 4 filters
wc=10; % desired cut-off frequency
N=5; % order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
subplot(2,2,1); step(G,'k'); %step response of G
title('Butterworth');
%analog Chebyshev 1 filter:
[num,den]=cheby1(N,Rp,wc,'s');
G=tf(num,den); %transfer function
subplot(2,2,2); step(G,'k'); %step response of G
title('Chebyshev 1');
%analog Chebyshev 2 filter:
[num,den]=cheby2(N,Rs,wc,'s');
G=tf(num,den); %transfer function
subplot(2,2,3); step(G,'k'); %step response of G
title('Chebyshev 2');
%analog elliptic filter:

4.5 Three Approximations to the Ideal Low-Pass Filter
217
Time (sec.)
Amplitude
Butterworth
0
0.2
0.4
0.6
0.8
1
1.2
0
0.5
1
1.5
From: U(1)
To: Y(1)
Time (sec.)
Amplitude
Chebyshev 1
0
0.5
1
1.5
2
2.5
0
0.5
1
1.5
From: U(1)
To: Y(1)
Time (sec.)
Amplitude
Chebyshev 2
0
0.5
1
1.5
2
0
0.5
1
1.5
From: U(1)
To: Y(1)
Time (sec.)
Amplitude
Elliptic
0
0.5 1
1.5
2
2.5
3
3.5
4
0
0.5
1
1.5
From: U(1)
To: Y(1)
Fig. 4.38 Comparison of the step response of the four ﬁlters
[num,den]=ellip(N,Rp,Rs,wc,'s');
G=tf(num,den); %transfer function
subplot(2,2,4); step(G,'k'); %step response of G
title('Elliptic');
4.5.5
Details of the MATLAB Signal Processing Toolbox
The MATLAB Signal Processing Toolbox offers an interesting and useful set of
functions related with the optimal ﬁlters. Let us include a concise view of these
functions.

218
4
Analog Filters
Fig. 4.39 Frequency
response of a band-pass 5th
order Butterworth ﬁlter
10
0
10
1
10
2
10
3
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
4.5.5.1
Band-Pass, High-Pass, Band-Stop
If we specify wb = [wlwh] and then use butter(N, wb, ‘s’), we obtain a Butterworth
band-pass ﬁlter. Program 4.18 provides an example, which generates Fig.4.39.
Program 4.18 Frequency response of band-pass Butterworth ﬁlter
% Frequency response of band-pass Butterworth filter
wl=10; % desired low cut-off frequency (rad/s)
wh=100; %desired high cut-off frequency (rad/s)
wb=[wl wh]; %the pass band
N=10; % order of the filter (5+5)
%analog band-pass Butterworth filter:
[num,den]=butter(N,wb,'s');
w=logspace(0,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
axis([1 1000 0 1.1]);
grid;
ylabel('Gain'); xlabel('rad/s');
title('frequency response of 5th band-pass
Butterworth filter');
If we use butter(N, wc, ‘high’, ‘s’), we get a Butterworth high-pass ﬁlter.
Figure4.40 shows an example, corresponding to the Program 4.19.
Program 4.19 Frequency response of high-pass Butterworth ﬁlter
% Frequency response of high-pass Butterworth filter
wh=100; %desired high cut-off frequency
N=5; % order of the filter
%analog high-pass Butterworth filter:
[num,den]=butter(N,wh,'high','s');
w=logspace(1,3); %logaritmic set of frequency values

4.5 Three Approximations to the Ideal Low-Pass Filter
219
Fig. 4.40 Frequency
response of a high-pass 5th
order Butterworth ﬁlter
10
1
10
2
10
3
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Fig. 4.41 Frequency
response of a band-stop 5th
order Butterworth ﬁlter
10
0
10
1
10
2
10
3
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
axis([10 1000 0 1.1]);
grid;
ylabel('Gain'); xlabel('rad/s');
title('frequency response of 5th high-pass
Butterworth filter');
Finally, if we use butter(N, wc, ‘stop’, ‘s’), we get a Butterworth band-stop ﬁlter.
Figure4.41 shows an example, corresponding to the Program 4.20.
Program 4.20 Frequency response of band-stop Butterworth ﬁlter
% Frequency response of band-stop Butterworth filter
wl=10; % desired low cut-off frequency

220
4
Analog Filters
wh=100; %desired high cut-off frequency
wb=[wl wh]; %the stop band
N=10; % order of the filter (5+5)
%analog band-stop Butterworth filter:
[num,den]=butter(N,wb,'stop','s');
w=logspace(0,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
semilogx(w,abs(G),'k'); %plots linear amplitude
axis([1 1000 0 1.1]);
grid;
ylabel('Gain'); xlabel('rad/s');
title('frequency response of 5th band-stop
Butterworth filter');
The same kind of speciﬁcations (using [wl wh], or ‘high’, or ‘stop’) apply for the
cheby1(), cheby2() and ellip() functions.
4.5.5.2
Filter Order
By means of the buttord(wp, ws, Rp, Rs, ‘s’) functions you can obtain the lowest order
of the analog Butterworth ﬁlter that has the pass-band and stop-band characteristics
you speciﬁed.
The same can be done with respect to the other ﬁlters, using the functions
cheb1ord(), cheb2ord(), and ellipord().
4.5.5.3
About Normalization
It is usual to consider normalized frequencies, so the cut-off frequency is 1. When
the cut-off frequency you have is not 1, the normalization consists in a change of
variable, from ω to ω’ = ω/ ωc.
The function buttap(n), returns the zeros, poles and gain of a normalized analog
Butterworth ﬁlter of order n.
The functions cheb1ap(), cheb2ap(), ellipap(), do the same concerning the other
ﬁlters.
Thefunctionlp2lp(num,den,wc)usesthenumeratoranddenominatorofanormal-
ized low-pass ﬁlter, and obtains the numerator and denominator of a “de-normalized”
low-pass ﬁlter (having wc cut-off frequency).
The function lp2bp() transforms the low-pass ﬁlter to a band-pass ﬁlter. The
function lp2hp() transforms the low-pass ﬁlter to a high-pass ﬁlter. And the function
lp2bs() transforms the low-pass ﬁlter to a band-stop ﬁlter.

4.6 Considering Phases and Delays
221
4.6
Considering Phases and Delays
In certain applications it is important to consider the “group delay”. If φ(jω) is the
phase of a ﬁlter G(jω), the group delay is:
τg(ω) = −d ϕ(jω)
d ω
(4.25)
In order to avoid distortion of signal shape, it is important to have the group delay
as constant as possible with respect to frequency.
In fact, a pure delay τ d is modelled as exp (−s τ d) in the Laplace domain. That
means φ(jω) is equal to −jω τ d. If we apply (4.25) to this case, we obtain a constant
group delay τ d. A pure delay does not distort signals.
4.6.1
Bessel Filter
The Bessel ﬁlter (or Thompson ﬁlter) has a maximally ﬂat group delay at zero
frequency and has almost constant group delay across the pass-band. The effect of
this is that the ﬁlter preserves well the wave shape of ﬁltered signals. The transfer
function of a nth order Bessel ﬁlter take the following form:
G(s) =
θn(0)
θn(s/ωc)
(4.26)
where θn(s) is a reverse Bessel polynomial. These polynomials can be deﬁned by a
recursion formula:
ϑ0(x) = 1
ϑ1(x) = x + 1
ϑn(x) = (2 n −1) ϑn−1(x) + x2 ϑn−2(x)
(4.27)
The cut-off frequency ωc of the Bessel ﬁlter is that frequency where the magnitude
response of the ﬁlter is √1/2.
To generate the next Figs.4.32, 4.33, 4.34, 4.35 and 4.36 we substituted in the
corresponding programs (Butterworth ﬁlter) the fragment FrB by another fragment,
that we call “FrT”, devoted to the Bessel ﬁlter. The fragment FrT is the following:
Fragment 4.21 FrT
wc=10; % desired cut-off frequency
N=5; % order of the filter
[num,den]=besself(N,wc); %analog Bessel filter

222
4
Analog Filters
Fig. 4.42 Frequency
response of 5th order Bessel
ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
Fig. 4.43 Poles of 5th order
Bessel ﬁlter
-10
-8
-6
-4
-2
0
2
-10
-8
-6
-4
-2
0
2
4
6
8
10
Notice that the fragment FrT uses the besself() function. Like in the previous
ﬁlters we have speciﬁed a cut-off frequency of 10rad/s.
Figure4.42 shows the frequency response of a Bessel ﬁlter with n = 5. Note that
the magnitude has been represented in a linear scale.
Figure4.43 depicts the poles (plotted as diamonds) of the Bessel ﬁlter on the
complex plane.
Figure4.44 shows the step response of the 5th Bessel ﬁlter.
Figure4.45 compares several frequency responses of Bessel ﬁlters, to visualize
the effect of the order of the ﬁlter.
Figure4.46 shows the response of the ﬁlter when it ﬁlters a square wave. The
frequency of the square wave is 10rad/s.

4.6 Considering Phases and Delays
223
Fig. 4.44 Step response of
5th order Bessel ﬁlter
0
0.5
1
1.5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Time (sec)
Amplitude
Fig. 4.45 Effect of n on the
frequency response of the
Bessel ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
rad/s
2
4
6
8
Finally, Fig.4.47 compares the phase of the frequency response of the Bessel
ﬁlter for various values of the ﬁlter order n. Figure4.47 has been obtained with the
Program 4.22.
Program 4.22 Comparison of frequency response phase of Bessel ﬁlters
% Comparison of frequency response phase
% of Bessel filters
wc=10; % desired cut-off frequency
N=2; % order of the filter
[num,den]=besself(N,wc); %analog Bessel filter
w=logspace(-1,3); %logaritmic set of frequency values
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'r'); %plots phase
hold on;
axis([0.1 1000 -800 90]);
grid;
ylabel('Phase'); xlabel('rad/s');
title('frequency response phase of Bessel filter');

224
4
Analog Filters
for N=4:2:8, % more orders of the filter
[num,den]=besself(N,wc); %analog Bessel filter
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'k'); %plots phase
end;
4.6.2
Comparison of Filter Phases and Group Velocities
Let us compare phase behaviours of the ﬁve ﬁlters. All ﬁlters are of 5th order.
Fig. 4.46 Response of 5th
order Bessel ﬁlter to a square
wave
0
1
2
3
4
5
6
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 4.47 Effect of n on the
frequency response phase of
the Bessel ﬁlter
10
-1
10
0
10
1
10
2
10
3
-800
-700
-600
-500
-400
-300
-200
-100
0
Phase
rad/s
2
4
6
8

4.6 Considering Phases and Delays
225
Fig. 4.48 Frequency
response phases of the ﬁve
5th ﬁlters
10
-1
10
0
10
1
10
2
10
3
-500
-400
-300
-200
-100
0
Phase
rad/s
B
C1 
T
C2 
E
Figure4.48 compares the phase of the ﬁve ﬁlters. The ﬁgure has been obtained
with the Program 4.23. Notice the use of the unwrap() function to avoid strange
jumps in the phase curves.
As shown in Fig.4.48, the phases of Butterworth (labelled as B) ﬁlter and the
Bessel (T) ﬁlter are similar, while the phase of Chebyshev type 1 ﬁlter (C1) is
different. The ﬁlters with zeros, the Chebyshev type 2 (C2) ﬁlter and the elliptic (E)
ﬁlter, have two jumps, which correspond to the zeros.
Program 4.23 Frequency response phases of the ﬁve ﬁlters
wc=10; %desired cut-off frequency
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
%logaritmic set of frequency values:
w=logspace(-1,3,500);
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'k'); %plots phase
hold on;
axis([0.1 1000 -500 90]);
grid;
ylabel('Phase'); xlabel('rad/s');
title('comparison of frequency response phase
of the filters');
%analog Chebyshev 1 filter:
[num,den]=cheby1(N,Rp,wc,'s');
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'r'); %plots phase
%analog Chebyshev 2 filter:
[num,den]=cheby2(N,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response

226
4
Analog Filters
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'g'); %plots phase
%analog elliptic filter:
[num,den]=ellip(N,Rp,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'b'); %plots phase
%analog Bessel filter:
[num,den]=besself(N,wc);
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
semilogx(w,180*unwrap(ph)/pi,'m'); %plots phase
Polar plots of G(jω) provide an interesting view of the combined behaviour of gain
and phase. Figure4.49 shows the polar plots corresponding to the frequency response
of the Butterworth ﬁlter (B), the Chebyshev type 1 ﬁlter (C1) and the Bessel ﬁlter
(T). This ﬁgure has been obtained with the Program 4.24.
Program 4.24 Comparison of frequency response phase of Butterworth, Chebyshev 1 and Bessel
ﬁlters in polar plane
% Comparison of frequency response phase of
% Butterworth, Chebyshev 1 and
% Bessel filters in polar plane
wc=10; %desired cut-off frequency
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
%logaritmic set of frequency values:
Fig. 4.49 Polar plot of
frequency response of 5th
Butterworth, Chebyshev 1
and Bessel ﬁlters
  0.5
  1
30
210
60
240
90
270
120
300
150
330
180
0
rad/s
Phase
C1
B
T

4.6 Considering Phases and Delays
227
w=logspace(-1,3,500);
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'k'); %polar plot
hold on;
ylabel('Phase'); xlabel('rad/s');
title('comparison of frequency response phase');
%analog Chebyshev 1 filter:
[num,den]=cheby1(N,Rp,wc,'s');
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'r'); %polar plot
%analog Bessel filter:
[num,den]=besself(N,wc);
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'m'); %polar plot
Let us study in more detail what happens with the ﬁlters having zeros. Figure4.50
shows the polar plots corresponding to the frequency response of the Chebyshev type
2 ﬁlter (C2) and the elliptic ﬁlter (E). The frequency response of the Butterworth ﬁlter
(B) has been added for reference. This ﬁgure has been obtained with the Program
4.25.
Program 4.25 Comparison of frequency response phase of Butterworth, Chebishev 2 and elliptic
ﬁlters in polar plane
% Comparison of frequency response phase of
% Butterworth, Chebishev 2 and
% elliptic filters in polar plane
Fig. 4.50 Polar plot of
frequency response of 5th
Chebyshev 2, elliptic and
Butterworth ﬁlters
  0.2
  0.4
  0.6
  0.8
  1
30
210
60
240
90
270
120
300
150
330
180
0
rad/s
Phase
E
B
C2 

228
4
Analog Filters
Fig. 4.51 Zoom in of the
polar plot of frequency
response of 5th Chebyshev 2,
elliptic and Butterworth
ﬁlters
  0.2
rad/s
Phase
E
B
C2 
wc=10; %desired cut-off frequency
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
%logaritmic set of frequency values:
w=logspace(-1,3,2000);
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'k'); %polar plot
hold on;
ylabel('Phase'); xlabel('rad/s');
title('comparison of frequency response phase');
%analog Chebyshev 2 filter:
[num,den]=cheby2(N,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'g'); %polar plot
%analog elliptic filter:
[num,den]=ellip(N,Rp,Rs,wc,'s');
G=freqs(num,den,w); %computes frequency response
ph=angle(G); %phase
polar(ph,abs(G),'b'); %polar plot
Perhaps, looking at Fig.4.50, the reader may wonder what happened with the
jumps that appear in Fig.4.48. That is why we zoomed on the central zone of Fig.4.50
to obtain Fig.4.51 and to see in detail around the origin. Both Chebyshev type 2 (C2)
and elliptic (E) ﬁlters have two loops before their ﬁnal trend to the origin. These two
loops cause the jumps in Fig.4.48.

4.6 Considering Phases and Delays
229
Fig. 4.52 Comparison of the
group delay of the ﬁve 5th
order ﬁlters
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
group delay
rad/s
E
T
C1
C2 
B
By means of an Euler approximation of the derivative, we can obtain a visualiza-
tion of the group delay of the ﬁve ﬁlters. An elimination sentence has been added
in the case of the Chebyshev type 2 ﬁlter and the elliptic ﬁlter, to avoid the effect
of their jump discontinuities. Figure 4.52 shows a comparison of the group delay of
the ﬁve ﬁlters. This ﬁgure has been obtained with the Program B.3, which has been
included in Appendix B.
As expected, Fig.4.52 shows an almost ﬂat proﬁle of the group delay curve of
the Bessel ﬁlter (T). Likewise, the group delay of the Butterworth ﬁlter (B) is fairly
smooth. The other three ﬁlters have “mountains”, being very signiﬁcant in the case
of the elliptic ﬁlter (in fact the ﬁgure crops its peak).
4.7
Some Experiments
Afterthedescriptionoftheanalogﬁlters,withspecialemphasisoftheapproximations
to the ideal ﬁlter, it is convenient to do some experiments, to gain still more insight.
4.7.1
Recovering a Signal Buried in Noise
The ﬁrst experiment is to mix noise and a sinusoidal signal, and then apply a ﬁlter
to recover the sinusoidal signal. This is done with the Program B.4, which uses a 5th
Butterworth band-pass ﬁlter. Notice the value N = 10 we speciﬁed for the butter()
function; this is due to N = 5 for the left cut-off transition and N = 5 for the right
cut-off transition of the ﬁlter pass-band. Figure4.53 shows the result: after a ﬁlter

230
4
Analog Filters
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
signal+noise
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
seconds
extracted signal
Fig. 4.53 Recovering a signal buried in noise
transient an almost sinusoidal signal is obtained, mixed with a little noise. Sound
has been included in the program, so you can hear ﬁrst the signal + noise, and after
some seconds the de-noised signal.
The Program B.4 has been included in Appendix B.
4.7.2
Adding and Extracting Signals
Let us take the following signal:
y(t) = sin(ωt) + 0.5 sin(3ωt) + 0.3 sin(5ωt)
(4.28)
We want to extract from y(t) the three sinusoidal components of the signal. This has
been done with the Program B.5, using three 5th Butterworth ﬁlters. One of the ﬁlters
is low-pass, the other is band-pass and the third is high-pass. The program demon-
strates the use of these three types of ﬁlters, and generates two ﬁgures: Figs.4.54 and
4.55. Transients are different for each type of ﬁlter.
After the extraction of the three components, we add them trying to get a recon-
struction of y(t). Figure4.55 shows the result: y(t) is on top and the reconstructed
signal is below. Clearly, the shape of the two signals is not the same due to differences
in gains and phases of the three ﬁlters.
The Program B.5 has been included in Appendix B.

4.7 Some Experiments
231
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
0
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y0
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-0.5
0
0.5
y5
seconds
Fig. 4.54 Extracting components from a compound signal
4.7.3
Near Cut-off Frequency
Now let us investigate what happens around the transition from pass-band to stop-
band in a low-pass ﬁlter. In this experiment three square signals are used. The ﬁlter
has a cut-off frequency of 10rad/s. One of the square signals has 5rad/s. frequency,
the other has 10rad/s. frequency and the third has 15rad/s. frequency. All ﬁve ﬁlters
are tested.
Let us begin with the 5th Butterworth ﬁlter. Figure4.56, which has been generated
with the Program 4.26, shows the output of the ﬁlter for the three square signals.
The shape of all signals look sinusoidal (the fundamental harmonic of the square
signals).
Program 4.26 Response of Butterworth ﬁlter to square signal near cut-off
% Response of Butterworth filter to square signal
% near cut-off
wc=10; % desired cut-off frequency
N=5; % order of the filter
%analog Butterworth filter:
[num,den]=butter(N,wc,'s');
G=tf(num,den); %transfer function
fs=100; %sampling frequency in Hz
tiv=1/fs; %time interval between samples;
t=0:tiv:(6-tiv); %time intervals set (6 seconds)

232
4
Analog Filters
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
ysum
seconds
Fig. 4.55 Original and reconstructed signals
0
1
2
3
4
5
6
-1
0
1
5 rad/s
0
1
2
3
4
5
6
-1
0
1
10 rad/s
0
1
2
3
4
5
6
-1
0
1
15 rad/s
seconds
Fig. 4.56 The three ﬁltered square signals when using 5th Butterworth ﬁlter

4.7 Some Experiments
233
0
1
2
3
4
5
6
-1
0
1
5 rad/s
0
1
2
3
4
5
6
-1
0
1
10 rad/s
0
1
2
3
4
5
6
-1
0
1
15 rad/s
seconds
Fig. 4.57 The three ﬁltered square signals when using 5th Chebyshev 1 ﬁlter
% Input square signal 1
wu=5; %signal frequency in rad/s
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
subplot(3,1,1); plot(t,y,'k'); %plots output signal
axis([0 6 -1.5 1.5]); ylabel('5 rad/s')
title('response to square signal, 5th
Butterworth filter');
% Input square signal 2
wu=10; %signal frequency in rad/s
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
subplot(3,1,2); plot(t,y,'k'); %plots output signal
axis([0 6 -1.5 1.5]); ylabel('10 rad/s')
% Input square signal 3
wu=15; %signal frequency in rad/s
u=square(wu*t); %input signal data set
[y,ty]=lsim(G,u,t); %computes the system output
subplot(3,1,3); plot(t,y,'k'); %plots output signal
axis([0 6 -1.5 1.5]); ylabel('15 rad/s')
xlabel('seconds');
Now let us apply 5th Chebyshev type 1 ﬁlter. Figure4.57 shows the output of the
ﬁlter for the three square signals. It is easy to create the program to generate this
ﬁgure, by simple substitutions of a few lines. The shape of all signals look sinusoidal,
and the attenuation in the stop band is signiﬁcant.
If we use the 5th Chebyshev type 2 ﬁlter, we obtain the results shown in Fig.4.58.
The signals show distortion and ripple.

234
4
Analog Filters
0
1
2
3
4
5
6
-1
0
1
5 rad/s
0
1
2
3
4
5
6
-1
0
1
10 rad/s
0
1
2
3
4
5
6
-1
0
1
15 rad/s
seconds
Fig. 4.58 The three ﬁltered square signals when using 5th Chebyshev 2 ﬁlter
Using the 5th elliptic ﬁlter the results, as shown in Fig.4.59, are somewhat dis-
torted.
Finally, using the 5th Bessel ﬁlter sinusoidal signals are obtained as shown in
Fig.4.60.
0
1
2
3
4
5
6
-1
0
1
5 rad/s
response to square signal, 5th elliptic filter
0
1
2
3
4
5
6
-1
0
1
10 rad/s
0
1
2
3
4
5
6
-1
0
1
15 rad/s
seconds
Fig. 4.59 The three ﬁltered square signals when using 5th elliptic ﬁlter

4.7 Some Experiments
235
0
1
2
3
4
5
6
-1
0
1
5 rad/s
0
1
2
3
4
5
6
-1
0
1
10 rad/s
0
1
2
3
4
5
6
-1
0
1
15 rad/s
seconds
Fig. 4.60 The three ﬁltered square signals when using 5th Bessel ﬁlter
4.8
Resources
4.8.1
MATLAB
4.8.1.1
Toolboxes
◦Filter Design Toolbox
http://antikafe-oblaka.ru/online-archive/download-mathworks-ﬁlter-design-tool
box-v42-for-matlab-v75-x64.html
◦Analog Filter Design Toolbox:
http://www.mathworks.com/matlabcentral/ﬁleexchange/9458-analog-ﬁlter-des
ign-toolbox/
◦AFD MATLAB Toolbox:
http://home.etf.rs/~tosic/afdmlall.htm

236
4
Analog Filters
4.8.1.2
Interactive Tools
◦FDATool (Filter Design and Analysis Tool):
www.mathworks.com/help/signal/ref/fdatool.html
4.8.1.3
MATLAB Code
◦Wilamowski, B. M., Gottiparthy, R. (2005). “Active and passive ﬁlter synthesis
using Matlab”.
http://www.InternationalJournalofEngineeringEducation, v.21, n.4, pp.561-571.
◦Audio Filter GUI Demo:
http://www.mathworks.com/matlabcentral/ﬁleexchange/19683-audio-ﬁlter-gui-
demo/
◦MATLAB Audio Processing Examples:
http://www.ee.columbia.edu/ln/rosa/matlab/
◦Digital Signal Processing ELEN E4810 (Columbia Univ.):
https://www.ee.columbia.edu/~dpwe/e4810/matscripts.html/
4.8.2
Web Sites
◦Deepa Kundur (Univ. Toronto):
http://www.comm.utoronto.ca/~dkundur/course/.
References
1. A.J. Casson, E. Rodriguez-Villegas, A review and modern approach to LC ladder synthesis. J.
Low Power Electr. Appl. 1(1), 20–44 (2011)
2. M.Z. Chen, M.C. Smith, Electrical and mechanical passive network synthesis, in Recent
Advances in Learning and Control (2008), pp. 30–50
3. S. Darlington, A history of network synthesis and ﬁlter theory for circuits composed of resistors,
inductors,andcapacitors.IEEETrans.CircuitsSyst.IFundam.TheoryAppl.46(1):4–13(1999)
4. H.G. Dimopoulos, Analog Electronic Filters (Springer, Heidelberg, 2012)
5. J. Karki, Analysis of the Sallen-Key architecture, Technical report, Texas Instruments (2002)
6. Z. Leonowicz. Selected Problems of Circuit Theory 1. Lecture Notes, Electrical Engineering of
Wroclaw University of Technology, Poland (2008). http://zet10.ipee.pwr.wroc.pl/record/274?
ln=en
7. B. Ninness, Fundamentals of Signals, Systems and Engineering. University of Newcastle,
Australia (2000). http://www.sigpromu.org/brett/elec2400/
8. O. Oz, J. Choma, Second Order Frequency Domain Effects of the Sallen–Key Filter. Technical
report, #07-1106 University of Southern California (2006)

References
237
9. L.D. Paarmann, Design and Analysis of Analog Filters: A Signal Processing Perspective
(Kluwer, 2003)
10. M.C.Smith,Synthesisofmechanicalnetworks:theinerter.IEEETrans.Autom.Control 47(10),
1648–1662 (2002)
11. M.C. Smith, T.H. Hughes, J. Z. Jiang, A Survey of Classical and Recent Results in RLC Circuit
Synthesis. Presentation at the Workshop on Dynamics and Control in Networks (2014). https://
www.lccc.lth.se/media/LCCC/WorkshopNetwork/mcs_lund_oct2014.pdf
12. C.L. Wadwha. Network Analysis and Synthesis (Anshan Publishers, 2008)
13. L. Wanhammar, Analog Filters Using MATLAB (Springer, Heidelberg, 2009)

Chapter 5
Digital Filters
5.1
Introduction
This chapter covers a central aspect of digital signal processing: digital ﬁlters. The
digital signal processing systems use samples of input signals, which constitute series
of numbers. The result may be also series of numbers, to be used as output signals.
The signal processing computations usually take into account a record of recent
values of the input and output samples. In the case of linear digital ﬁlters, the output
y(n) in the instant n, is computed as a linear combination of the input u(n) and
previous samples of input and output signals. For instance, the MATLAB ﬁlter(B,
A, U) function, uses the vectors of coefﬁcients A and B and the vector of signal
input samples U to implement a linear digital ﬁlter, which creates the ﬁltered output
according with the following equation:
a1 y(n) = b1 u(n) + b2u(n −1) + b3 u(n −2) + · · · + bnb+1 u(n −nb) −
−a2 y(n −1) −a3 y(n −2) −· · · −ana+1 y(n −na)
(5.1)
Taking the z transform of Eq.(5.1) and rearranging the expression, the following
discrete transfer function is obtained:
H(z) = Y(z)
U(z) = b1 + b2 z−1 + b3 z−2 + · · · + bnb+1 z−nb
a1 + a2 z−1 + a3 z−2 + · · · + ana+1 z−na
(5.2)
The transfer function can be also expressed in terms of positive z exponents (multiply
and divide H(z) by zm; if nb >≥na then m = nb, else m = na).
The frequency response of the digital ﬁlter is given by:
H(ω) = H(z)|z=e jω
(5.3)
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_5
239

240
5
Digital Filters
Given the transfer function of the digital ﬁlter, we can obtain by inverse z transform
the impulse response sequence h(n) of the ﬁlter. Now, in the time domain, the output
y(n) of digital ﬁlter can be computed by discrete convolution:
y(n) =
∞

m=−∞
h(m) u(n −m)
(5.4)
Notice that we introduced a slight change of nomenclature with respect to the chapter
on linear systems, denoting the ﬁlter transfer function as H(z) instead of G(z). It
happens that most literature on digital ﬁlters use H(z), while the literature on linear
systems (and control) commonly uses G(z). There is also another issue regarding
equations and indexes; recall that MATLAB arrays, like for instance x, starts with
x(1), but theoretical expressions usually start with x(0). As we try to make easy the
use of MATLAB, this has been taken into account where possible; for instance in
Eq. (5.1), the ﬁrst b coefﬁcient is b1, and the ﬁrst a coefﬁcient is a1.
Therearethreemainalternativesforthestructureofalineardigitalﬁlter,according
with the values of na and nb in Eq. (5.1):
• nb > 0, na = 0
• nb = 0, na > 0
• nb > 0, na > 0
These three alternatives give two types of digital ﬁlters: ﬁnite impulse response
ﬁlters (FIR ﬁlters), corresponding to na = 0, or inﬁnite impulse response ﬁlters (IIR
ﬁlters), corresponding to na > 0.
The ﬁlter designer may start with the design of an analog ﬁlter, translating it to
a digital ﬁlter in a second step, or may keep the design effort always in the digital
domain.
In general the digital domain offers a more generous space for design freedom and
ingenuity. For example, this chapter considers multi-band ﬁlters and also non-causal
ﬁlters. Coming to an even wider perspective, note that the digital signal processing
would be based on a computer program, with decisions, iterations, subroutines, etc.,
so a lot of possibilities can be opened. For instance, the digital signal processing used
in car mobile phones can attenuate trafﬁc and car noise, cancel internal voice echoes
in the car, and make more understandable the human voice.
Compared with analog ﬁlters made with electronic circuits, digital ﬁlters can be
better regarding ﬂatness, good roll-off and stop-band attenuation. However analog
electronic circuits have also advantages, such speed, more amplitude dynamic range,
and more frequency dynamic range.
In part this chapter can be seen as a simple continuation of the previous chapter.
In particular, the MATLAB functions butter(), cheby1(), cheby2(), ellip(), are also
used for IIR digital ﬁlters. This said, there are still several new aspects to be treated
in this chapter, speciﬁcally belonging to the digital domain such is the case of FIR
ﬁlters.

5.2 From Analog Filters to Digital Filters
241
5.2
From Analog Filters to Digital Filters
Suppose you have an analog ﬁlter that ﬁts well to your needs and you want a digital
version of the ﬁlter. There are several alternatives to do so. MATLAB offers two
functions for ﬁlter discretization: bilinear() for bilinear transformation, and impvar()
for the impulse invariance method [1]. A brief explanation follows, and then an
example is considered.
The bilinear transformation is a mapping from the s-plane to the z-plane. Given
the transfer function G(s) of an analog ﬁlter, the digital ﬁlter counterpart H(z) is
obtained by the following substitution:
s = 2
Ts
1 −z−1
1 + z−1
(5.5)
where Ts is the sampling period.
The idea of the impulse invariance method is to obtain the discrete impulse re-
sponse h(n) of the digital ﬁlter by sampling of the impulse response g(t) of the
analog ﬁlter:
h(n) = g(nTs)
(5.6)
Consider an example: we want a digital version of the following analog ﬁlter:
G(s) =
63
s + 63
(5.7)
Figure5.1 shows the frequency response of this low-pass analog ﬁlter; the ﬁgure has
been obtained with the Program 5.1. The corner frequency is 63rad/s (aprox. 10Hz).
Program 5.1 Frequency response of analog ﬁlter example
% Frequency response of analog filter example
%Analog filter (wc= 63rad/s = 10Hz.):
num=[63]; % transfer function numerator;
den=[1 63]; %transfer function denominator
%logaritmic set of frequency values in rad/s:
w=logspace(0,3);
G=freqs(num,den,w); %computes frequency response
AG=20*log10(abs(G)); %take decibels
FI=angle(G); %take phases (rad)
f=w/(2*pi); %frequencies in Hz.
subplot(2,1,1); semilogx(f,AG,'k'); %plots decibels
grid; axis([1 100 -25 5]);
ylabel('dB');
title('frequency response of analog filter example')
subplot(2,1,2); semilogx(f,FI,'k'); %plots phases
grid;axis([1 100 -1.5 0]);
ylabel('rad.'); xlabel('Hz.')

242
5
Digital Filters
10
0
10
1
10
2
-25
-20
-15
-10
-5
0
5
dB
10
0
10
1
10
2
-1.5
-1
-0.5
0
rad.
Hz.
Fig. 5.1
Frequency response of the analog ﬁlter example
Let us apply the bilinear transformation to obtain a digital ﬁlter counterpart. This is
done with the Program 5.2, which obtains the discrete transfer function and generates
the frequency response of the digital ﬁlter (Fig.5.2).
Program 5.2 uses the freqz() function to compute the frequency response of the
digital ﬁlter. There are several ways to specify the contents of the parenthesis in
this function. For instance the format H =freqz(numd, dend, w) uses normalized
frequency w. This frequency can take values between 0 and π radians/sample. The
value π corresponds to the Nyquist frequency (one half the sampling frequency). For
our example we prefer to use the signal frequency and the sampling frequency, both
in Hertz.
Program 5.2 Bilinear transformation from analog ﬁlter to digital ﬁlter
% Bilinear transformation
% from analog filter to digital filter
%Analog filter (wc= 63rad/s = 10Hz.):
num=[63]; % transfer function numerator;
den=[1 63]; %transfer function denominator
%Digital filter
fs=1200; %sampling frequency in Hz.
%bilinear transformation:
[numd,dend]= bilinear(num,den,fs);
%logaritmic set of frequency values in Hz:
f=logspace(-1,2);
G=freqz(numd,dend,f,fs); %computes frequency response
AG=20*log10(abs(G)); %take decibels
FI=angle(G); %take phases (rad)

5.2 From Analog Filters to Digital Filters
243
subplot(2,1,1); semilogx(f,AG,'k'); %plots decibels
grid;axis([1 100 -25 5]);
ylabel('dB');
title('frequency response for
the bilinear transformation')
subplot(2,1,2); semilogx(f,FI,'k'); %plots phases
grid;axis([1 100 -1.5 0]);
ylabel('rad.'); xlabel('Hz.')
Figure5.2 is very similar to Fig.5.1. But, let us see what happens if we decrease
the sampling frequency (a simple modiﬁcation of one line in Program 5.2). Figure5.3
shows the frequency response of the ﬁlter using a sampling frequency fs=100 (Hz);
things are clearly different.
Comparing Figs.5.2 and 5.3 it can be seen that increasing the sampling frequency
makes the frequency response of the discretized version of the analog ﬁlter be closer
to the frequency response of the original analog ﬁlter. It is also important to consider
that there will be aliasing if we try to ﬁlter signals with frequency above the Nyquist
frequency.
Now, let us apply the impulse invariance method to obtain a digital ﬁlter counter-
part for the same analog ﬁlter example. The Program 5.3 obtains the discrete transfer
function and represents in Fig.5.4 the frequency response of the digital ﬁlter.
10
0
10
1
10
2
-25
-20
-15
-10
-5
0
5
dB
10
0
10
1
10
2
-1.5
-1
-0.5
0
rad.
Hz.
Fig. 5.2
Frequency response of the digital ﬁlter obtained with bilinear transformation and
fs=1200Hz

244
5
Digital Filters
10
0
10
1
10
2
-50
-40
-30
-20
-10
0
dB
10
0
10
1
10
2
-2
-1
0
1
2
rad.
Hz.
Fig. 5.3
Frequency response of the digital ﬁlter obtained with bilinear transformation and
fs=100Hz
10
0
10
1
10
2
-25
-20
-15
-10
-5
0
5
dB
10
0
10
1
10
2
-1.5
-1
-0.5
0
rad.
Hz.
Fig. 5.4 Frequency response of the digital ﬁlter obtained with the impulse invariance method and
fs=1200Hz

5.2 From Analog Filters to Digital Filters
245
Program 5.3 Invariant impulse transformation from analog ﬁlter to digital ﬁlter
% Invariant impulse transformation
% from analog filter to digital filter
%Analog filter (wc= 63rad/s = 10Hz.):
num=[63]; % transfer function numerator;
den=[1 63]; %transfer function denominator
%Digital filter
fs=1200; %sampling frequency in Hz.
%invariant impulse transformation:
[numd,dend]= impinvar(num,den,fs);
%logaritmic set of frequency values in Hz:
f=logspace(-1,2);
G=freqz(numd,dend,f,fs); %computes frequency response
AG=20*log10(abs(G)); %take decibels
FI=angle(G); %take phases (rad)
subplot(2,1,1); semilogx(f,AG,'k'); %plots decibels
grid;axis([1 100 -25 5]);
ylabel('dB');
title('frequency response for impulse
invariance method')
subplot(2,1,2); semilogx(f,FI,'k'); %plots phases
grid; axis([1 100 -1.5 0]);
ylabel('rad.'); xlabel('Hz.')
10
0
10
1
10
2
-10
-5
0
5
dB
10
0
10
1
10
2
-1
-0.5
0
0.5
1
rad.
Hz.
Fig. 5.5 Frequency response of the digital ﬁlter obtained with the impulse invariance method and
fs=100Hz

246
5
Digital Filters
As before, let us see what happens if the sampling frequency decreases. With a
simple modiﬁcation of the Program 5.3 we obtain the Fig.5.5 that shows the result
for fs=100 (Hz).
Notice the marked differences compared with previous ﬁgures.
In general the advice for ﬁlter discretization is to check the results, with a visual-
ization of the frequency response of the digital ﬁlter that has been obtained.
5.3
FIR Digital Filters
The ﬁnite impulse response (FIR) digital ﬁlters compute the output y(n) in function
of the input u(n) according with the following equation:
y(n) = b1 u(n) + b2u(n −1) + b3 u(n −2) + · · · + bnb+1 u(n −nb)
(5.8)
The implementation of this ﬁlter requires no feedback, so FIR ﬁlters are always
stable.
There are several approaches to determine the ﬁlter coefﬁcients bi [23]. Indeed,
the reference is the ideal ﬁlter; in consequence, it is convenient to start this section
with some comments about duality and “brickwall” shapes in the frequency and the
discrete time domains.
5.3.1
Duality and Brickwall Shapes
With the help of the Fourier transform, it is easy to see an interesting duality between
two speciﬁc shapes. One of these shapes is a rectangle, a “brickwall”, such is the
case of the frequency response of an ideal ﬁlter. The other shape corresponds to the
sinc function:
sin c(x) = sin (x)
x
(5.9)
Figure5.6 shows a plot of this function, which has a well-known shape.
Program 5.4 The sinc function
% The sinc function
x=-10:0.01:10;
y=sinc(x); %the sinc function
plot(x,y,'k'); %plots the function
hold on;
plot([-10 10],[0 0],'--k'); %horizontal dotted line
xlabel('x'); title('sinc function')

5.3 FIR Digital Filters
247
Fig. 5.6 The sinc(x)
function
-10
-8
-6
-4
-2
0
2
4
6
8
10
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
x
The Fourier transform of a function y(t) is the following:
Y(ω) =
∞

−∞
y(t) e−jω t dt
(5.10)
The inverse Fourier transform is:
y(t) =
1
2 π
∞

−∞
Y(ω) e jω t dω
(5.11)
Suppose that Y(w) has a brickwall shape, so for instance:
Y(ω) =
 1, −ωc ≤ω ≤ωc
0,
ωc < |ω|
(5.12)
The corresponding to Y(ω) in the time domain is:
y(t) =
1
2 π
∞
−∞
Y(ω) e jω t dω =
1
2 π
ωc
−ωc
e jω t dω =
= 1
2 π
1
j t (e j ωc t −e−j ωc t) =
1
π t sin (ωct) =
= ωc
π
sin(ωct)
ωct
= ωc
π sinc(ωct)
(5.13)
Now suppose a symmetrical situation: y(t) has a brickwall shape, according to:
y(t) =
1, −tc ≤t ≤tc
0,
tc < |ω|
(5.14)

248
5
Digital Filters
The corresponding to y(t) in the frequency domain is:
Y(ω) =
∞
−∞
y(t) e−jω t dt =
tc
−tc
e−jω t dt =
−1
j ω (e−j ω tc −e j ω tc) =
=
2
ω sin (ω tc) = 2tc
sin(ω tc)
ω tc
= 2tc sinc(ω tc)
(5.15)
In consequence, when there is a brickwall shape in the time or frequency domain,
there is a sinc shape in the other domain.
Let us consider now the digital ﬁlters.
Based on the discrete inverse Fourier transform, the impulse response sequence
h(n) of a digital ﬁlter can be obtained with the following equation:
h(n) =
1
2 π
π

−π
H(ω) e jω n dω
(5.16)
The ideal low-pass ﬁlter has a “brickwall” shape in the frequency domain, using
(5.16) the corresponding h(n), with n from −∞to +∞, is found to be:
h(n) = sin(ωc n)
n π
= ωc
π sinc (ωc
π n)
(5.17)
Given the impulse response sequence h(n) of a digital ﬁlter, the frequency response
of the ﬁlter is:
H(ω) =
∞

n = −∞
h(n) e−jω n
(5.18)
The equivalent to h(t) having a brickwall shape is a sequence h(n) of ones (or any
other constant). Figure5.7, which has been generated with the Program 5.5, shows
the frequency response of a digital ﬁlter having as h(n) a sequence of 7 ones. The
sequence h(n) is shown on the left, and the frequency response H(w) on the right
(the normalized frequency runs from –π to π).
Program 5.5 h(n)=7 ones, H(w) of the digital ﬁlter
% h(n)= 7 ones, H(w) of the digital filter
n=-3:1:3; %sample times
h=ones(7,1); % vector of 7 ones
subplot(1,2,1); stem(n,h,'k'); %plots h(n)
axis([-4 4 0 1.2]); title('h(n)'); xlabel('n');
%discrete Fourier transform:
H1=real(fft(h,512)); Hf=H1/max(H1);
w=-pi:(2*pi/511):pi;
subplot(1,2,2); plot(w,fftshift(Hf),'k'); %plots H(w)
axis([-pi pi -0.3 1]); title('H(w)');
xlabel('normalized frequency');
hold on;
plot([-pi pi],[0 0],'--k'); %horizontal dotted line

5.3 FIR Digital Filters
249
-4
-2
0
2
4
0
0.2
0.4
0.6
0.8
1
1.2
h(n)
n
-2
0
2
-0.2
0
0.2
0.4
0.6
0.8
1
H(w)
normalized frequency
Fig. 5.7
Frequency response of ﬁlter with h(n)=ones(7,1)
If we add more ones to h(n) the main lobe of H(ω) becomes narrower, meaning
that the corner frequency of the digital ﬁlter decreases. Figure5.8, obtained with the
Program 5.6, shows the frequency response corresponding to h(n) being a series of
13 ones.
Program 5.6 h(n)=13 ones, H(w) of the digital ﬁlter
% h(n)= 13 ones, H(w) of the digital filter
n=-6:1:6; %sample times
h=ones(13,1); % vector of 13 ones
subplot(1,2,1); stem(n,h,'k'); %plots h(n)
axis([-7 7 0 1.2]); title('h(n)'); xlabel('n');
%discrete Fourier transform:
H1=real(fft(h,512)); Hf=H1/max(H1);
w=-pi:(2*pi/511):pi;
subplot(1,2,2); plot(w,fftshift(Hf),'k'); %plots H(w)
axis([-pi pi -0.3 1]); title('H(w)');
xlabel('normalized frequency');
hold on;
plot([-pi pi],[0 0],'--k'); %horizontal dotted line
The number of peaks in the frequency response is equal to the number of ones in
the corresponding h(n).

250
5
Digital Filters
-5
0
5
0
0.2
0.4
0.6
0.8
1
1.2
h(n)
n
-2
0
2
-0.2
0
0.2
0.4
0.6
0.8
1
H(w)
normalized frequency
Fig. 5.8
Frequency response of ﬁlter with h(n)=ones(33,1)
5.3.2
Truncation and Time-Shifting
Notice that the series of ones has been plotted in Figs.5.7 and 5.8 as symmetrical
about n = 0. This deserves an important comment about symmetry of the impulse
response sequence h(n).
The idea is to take advantage of:
e−jω n + e jω n = 2 cos (ω n)
(5.19)
In the case of h(n) being a series of 2N + 1 ones, with symmetry about n = 0, the
corresponding frequency response is:
H(ω) =
N

n = −N
e−jω n = 2 cos (ωN) + 2 cos (ω(N −1)) +
+ 2 cos (ω(N −2)) + · · · + 1
(5.20)
This frequency response is real. In consequence has zero phase.
In the case of h(n) being a series of 2N + 1 bk coefﬁcients, with symmetry about
n = 0, the corresponding frequency response is:

5.3 FIR Digital Filters
251
H(ω) =
N

n = −N
bn e−jω n = b0 + 2
N

k = 1
bk cos (ωN)
(5.21)
Again, this frequency response is real, with zero phase. This is the advantage of
symmetry in h(n).
Let us look closely to the ideal ﬁlter, with h(n), −∞≤n ≤∞, given by (5.17).
There are two main difﬁculties about this digital ﬁlter. One is that we cannot handle
inﬁnite length h(n) sequences. The other is that we do not know about the future
as would be required by (5.4). These difﬁculties derive from the ideal ﬁlter being
non-causal.
But we can approximate the ideal ﬁlter. First we can truncate h(n), and second we
can apply a time shift of N samples, so we know and use N samples of “the future
input”. This implies a pure delay of the ﬁlter: τ = N.Ts, which means a linear phase
in the frequency response. Figure5.9 illustrates the ideas.
Part of h(n) of the ideal ﬁlter is shown on top of Fig.5.9. The truncation of h(n)
is shown in the middle of Fig.5.9; we obtain a truncated impulse response sequence
ht(n) with 51 terms. Then we apply a time shift of 25 sampling periods, and we obtain
the hf(n) shown at the bottom of Fig.5.9; hf(n) is the impulse response sequence of
a causal digital ﬁlter.
It is important to note that the ﬁlter with hf(n) has the same frequency response
amplitude as the ﬁlter with ht(n).
Figure5.9 has been obtained with the Program 5.7. Notice the use of ifft() to
compute h(n) as the inverse Fourier transform of an ideal ﬁlter.
-60
-40
-20
0
20
40
60
0
0.02
0.04
0.06
non-causal h(n)
-60
-40
-20
0
20
40
60
0
0.02
0.04
0.06
h(n) truncation
-60
-40
-20
0
20
40
60
0
0.02
0.04
0.06
h(n) time-shift
n
Fig. 5.9
h(n) truncation and time-shift

252
5
Digital Filters
Program 5.7 Truncation and time shifting
% Truncation and time shifting
N=7;
%points of H(w):
H=[ones(N,1);zeros(128-N,1);
zeros(128-N,1);ones(N,1)]';
h1=ifft(H,128); %inverse Fourier transform
h=ifftshift(h1); %compose symmetrical plot
h=real(h);
subplot(3,1,1);
n=-64:1:63; %number of plotted h(n)terms
stem(n,h,'k'); %plots h(n)
axis([-65 65 -0.015 0.06]);
ylabel('non-causal h(n)');
title('h(n) truncation and time-shift');
subplot(3,1,2);
n=-25:1:25; %number of truncated ht(n)terms
ht=h((64-25):(64+25)); %truncation of h(n)
stem(n,real(ht),'k'); %plots ht(n)
axis([-65 65 -0.015 0.06]);
ylabel('h(n) truncation');
subplot(3,1,3);
n=0:1:50; %time-shift of 25 samples
hf=ht;
stem(n,real(hf),'k'); %plots hf(n)
axis([-65 65 -0.015 0.06]);
ylabel('h(n) time-shift'); xlabel('n');
The result of the truncation and time shifting is a causal digital ﬁlter that approx-
imates well the desired frequency response, but with signiﬁcant ripple. Figure5.10
shows an example of frequency response (we draw a symmetrical version, for −ω
and +∞). This ﬁgure has been generated by the Program 5.8.
Fig. 5.10 Frequency
response of truncated causal
ﬁlter
-3
-2
-1
0
1
2
3
0
0.2
0.4
0.6
0.8
1
H (w)
normalized frequency

5.3 FIR Digital Filters
253
Program 5.8 Frequency response of the truncated ﬁlter
% Frequency response of the truncated filter
N=100;
%points of H(w)
H=[ones(N,1);zeros(256-N,1);
zeros(256-N,1);ones(N,1)]';
h1=ifft(H,512); %inverse Fourier transform
h=ifftshift(h1); %compose symmetrical plot
h=real(h);
hf=h((256-15):(256+15)); %truncation of h(n)
%discrete Fourier transform:
H1=abs(fft(hf,512)); Hf=H1/max(H1);
w=-pi:(2*pi/511):pi;
plot(w,fftshift(Hf),'k'); %plots H(w)
axis([-pi pi -0.1 1.1]); title('H(w)');
xlabel('normalized frequency');
Thus far, the predominant context for the study has been provided by the Fourier
transform. But let us brieﬂy consider a complementary point of view: weighted
averaging.
Taking a causal FIR ﬁlter, its time domain response can be computed by convo-
lution between h(n) and u(n):
y(n) = h(1) u(n) + h(2) u(n −1) + h(3) u(n −2) + · · · + h(N + 1) u(n −N)
(5.22)
If h(1) = 1, h(2) = 1…h(N + 1) = 1, (h(n) has a brickwall shape), then:
y(n) = (N + 1) ¯u(n)
(5.23)
where ¯u(n) is the average of the last N + 1 values of u(n). We say the ﬁlter is a
moving-average ﬁlter.
Apart from the simple averaging, some weighting can be applied. For instance,
older inputs could be less considered as more recent inputs (some sort of forgetting
factors).
Note that since the delay of the ﬁlter is (N/2)Ts, the output of the ﬁlter corresponds
to what was the input N/2 sampling periods ago.
Looking again at the bottom of Fig.5.9, the peak of the truncated causal ﬁlter
gives the highest weight to u(n −N/2),and gives less weights to the past and the
future inputs u(n) with respect to u(n −N/2).
5.3.3
Windows
When you try to narrow the time limits of a signal, its bandwidth expands, and
vice-versa. The problem with a sharp truncation of the ideal h(n) is that it causes
signiﬁcant lobes both sides of the main lobe in the frequency response. A traditional
way to alleviate this problem is to multiply the ideal h(n) by another sequence hw(n),

254
5
Digital Filters
which has ﬁnite length and an special shape to obtain a better frequency response.
This multiplication tries to get a smoothed truncation of h(n). Denote as Hw(ω) the
frequency response corresponding to hw(n).The multiplication in the time domain
corresponds to a convolution in the frequency domain, so the windowed FIR ﬁlter
has the following frequency response:
HF(ω) = H(ω) ∗Hw(ω)
(5.24)
where the symbol * denotes convolution.
Indeed the truncation depicted in Fig.5.9 can be considered as a windowed ﬁlter,
using a brickwall window.
Filter designers have created many different windows, taking into account several
objectives and trade-offs [3, 6, 10, 12, 15, 21, 22]. We shall conﬁne our study to the
windows already implemented in the MATLAB Signal Processing Toolbox.
5.3.3.1
Triangular and Bartlett Windows
A simple design for a smoother truncation is a triangular window. The coefﬁcients
of the MATLAB triangular window are:
For N odd:
hw(n) =

2 n
N + 1,
1 ≤k ≤
N+1
2
2 (N−n +1)
N+1
,
N+1
2
≤k ≤N
(5.25)
For N even:
hw(n) =
 2 n−1
N ,
1 ≤k ≤
N
2
2 (N−n) +1
N
,
N
2 + 1 ≤k ≤N
(5.26)
The Bartlett window is similar to a triangular window, only that the Bartlett window
ends with zeros at n = 1 and n = N, while the triangular window is nonzero at those
points. The coefﬁcients of the MATLAB Bartlett window are:
For N odd:
hw(n) =

2 (n −1)
N −1 ,
1 ≤k ≤
N+1
2
2 −2 (n −1)
N−1 ,
N+1
2
≤k ≤N
(5.27)
For N even:
hw(n) =

2 (n −1)
N −1 ,
1 ≤k ≤
N
2
2 (N−n)
N−1 ,
N
2 + 1 ≤k ≤N
(5.28)
Figure5.11 shows the sequence hw(n) of a triangular window with 51 terms, and
the frequency response HF(ω) of the 50th corresponding windowed ﬁlter. The digital
FIR ﬁlter has been designed for a corner frequency of 10Hz. The ﬁgure has been
obtained with the Program 5.9. The sequence hw(n) is obtained with the triang()

5.3 FIR Digital Filters
255
0
0.2
0.4
0.6
0.8
1
1.2
hw(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz.
Hf(w) 
Fig. 5.11
Triangular window and frequency response of the windowed FIR ﬁlter
function, and the numerator of the transfer function of the FIR windowed ﬁlter is
obtained with the ﬁr1() function. The denominator is 1.
Program 5.9 hw(n) of triangular window, and frequency response Hf(w) of windowed ﬁlter
% hw(n) of triangular window, and
% frequency response Hf(w)of windowed filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
hw=triang(N+1);
numd=fir1(N,fc,hw); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)
stem(hw,'k'); %plots hw(n)
axis([1 51 0 1.2]);
title('triangle hw(n)'); xlabel('n');
subplot(1,2,2)
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
grid; axis([1 100 0 1.1]);
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) 50th windowed filter')

256
5
Digital Filters
5.3.3.2
The Hamming and Other Cosine-Based Windows
The window should keep the main lobe of h(ω) while trying to attenuate lateral lobes
in the ﬁlter frequency response. There are several windows that are based on a raised
cosine concept, according with the following general expression:
hw(n) = a −b cos

2π
n
N + 1

, n = 1, 2, . . . , N
(5.29)
The parameters a and b raise the sequence hw(n) over zero. For instance the window
of von Hann (the Hanning window) has the following expression:
hw(n) = 0.5 −0.5 cos

2π
n
N + 1

, n = 1, 2, . . . , N
(5.30)
The Hamming window is a further improvement, with the following parameters:
hw(n + 1) = 0.54 −0.46 cos

2π
n
N −1

, n = 1, 2, . . . , N −1
(5.31)
The Hamming window is most usual, being the default window when using the ﬁr1()
function.
The Blackman window improves the stop-band attenuations, at the expense of
widening the transition from pass-band to stop-band. This window has the following
expression:
hw(n) = 0.42 −0.5 cos

2π n −1
N −1

+ 0.08 cos

4π n −1
N −1

, n = 1, 2, . . . , N
(5.32)
Figure5.12 shows the sequence hw(n) of a Hamming window with 51 terms, and
the frequency response HF(ω) of the 50th corresponding windowed ﬁlter. The ﬁgure
has been obtained with the Program 5.10, which is very similar to the Program 5.9.
Program 5.10 hw(n) of Hamming window, and frequency response Hf(w) of windowed ﬁlter
% hw(n) of Hamming window, and
% frequency response Hf(w) of windowed filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
hw=hamming(N+1);
numd=fir1(N,fc,hw); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)
stem(hw,'k'); %plots hw(n)

5.3 FIR Digital Filters
257
axis([1 51 0 1.2]);
title('Hamming hw(n)'); xlabel('n');
subplot(1,2,2)
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
grid; axis([1 100 0 1.1]);
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) 50th windowed filter')
Figure5.13 compares the frequency response Hf(ω) of six Hamming FIR ﬁlters
with order N between 10 and 60.
Figure5.14, which has been obtained with the Program 5.11, compares the se-
quences hw(n) of the Hanning (Hn), Hamming (Hm) and Blackman (B) windows.
0
0.2
0.4
0.6
0.8
1
1.2
 hw(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz.
Hf(w) 
Fig. 5.12
Hamming window and frequency response of the windowed FIR ﬁlter

258
5
Digital Filters
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Hz.
Hf (w)
Fig. 5.13
Comparison of Hamming ﬁlter frequency response Hf(ω) for several orders N of the
ﬁlter
Fig. 5.14
Comparison of
hw(n) of Hanning, Hamming
and Blackman windows
5
10
15
20
25
30
35
40
45
50
0
0.2
0.4
0.6
0.8
1
hw(n) 
n
Blackman 
Hamming 
Hanning 

5.3 FIR Digital Filters
259
Program 5.11 Comparison of hw(n) of Hanning, Hamming and Blackman windows
% Comparison of hw(n) of Hanning, Hamming
% and Blackman windows
N=50; %even order
hw=hanning(N+1); %Hanning window
plot(hw,'k'); hold on;
hw=hamming(N+1); %Hamming window
plot(hw,'r');
hw=blackman(N+1); %Blackman window
plot(hw,'b'); hold on;
axis([1 51 0 1.1]);
title('hw(n) of 50th Hanning, Hamming and Blackman');
xlabel('n');
Figure5.15 compares the frequency response H f (ω) of the 50th FIR windowed
ﬁlter, using the Hanning (Hn), Hamming (Hm) and Blackman (B) windows. It is
hard to appreciate differences between the Hanning and Hamming windowed ﬁlters.
Program 5.12 Comparison of Hf(w) of Hanning, Hamming and Blackman windowed ﬁlters
% Comparison of Hf(w) of Hanning, Hamming
% and Blackman windowed filters
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
dend=[1]; %transfer function denominator
hw=hanning(N+1); %Hanning window
numd=fir1(N,fc,hw); %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
hold on;
hw=hamming(N+1); %Hamming window
numd=fir1(N,fc,hw); %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'r'); %plots gain
hw=blackman(N+1); %Blackman window
numd=fir1(N,fc,hw); %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'b'); %plots gain
axis([1 100 0 1.1]);
grid;
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) of 50 th Hanning, Hamming and
Blackman windowed filter')
Figure5.16 compares the frequency response Hw(ω) corresponding to the win-
dows themselves. Notice the ripples in the stop-band of the Hanning window (Hn).
Also note that the Blackman window (B) has a slower transition from pass-band to
stop-band.

260
5
Digital Filters
Fig. 5.15
Comparison of
Hf(ω) of Hanning, Hamming
and Blackman windowed
ﬁlters
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz.
Hf(w) 
B 
Hm 
Hn 
B 
Fig. 5.16
Comparison of
Hw(ω) of Hanning,
Hamming and Blackman
windows
10
0
10
1
10
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Hw(w) 
Hz.
B 
Hn 
Hm 
B 
Program 5.13 Comparison of Hw(w) of Hanning, Hamming and Blackman windows
% Comparison of Hw(w) of Hanning, Hamming
% and Blackman windows
fs=130; %sampling frequency in Hz.
N=50; %even order
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
dend=[1]; %transfer function denominator
hw=hanning(N+1); %Hanning window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
hold on;
hw=hamming(N+1); %Hamming window

5.3 FIR Digital Filters
261
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'r'); %plots gain
hw=blackman(N+1); %Blackman window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'b'); %plots gain
axis([1 100 0 1]);
title('Hw(w) of Hanning, Hamming and
Blackman windows');
grid; xlabel('Hz.');
The next table compares values of the side-lobe amplitude of Hw(ω), and the
transition width and stop-band attenuation of the corresponding ﬁlter response H f (ω)
for the three cases considered.
window
Side lobe (dB) Transition width Stop-band attenuation (dB)
Hanning
-31
3.1/N+1
-44
Hamming
-41
3.3/N+1
-53
Blackman
-57
5.5/N+1
-74
Figure5.17 shows in more detail what happens in the stop-band of Hw(ω) for the
three windows.
10
0
10
1
10
2
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
Hanning
10
0
10
1
10
2
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
Hamming
10
0
10
1
10
2
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
Blackman
Hz.
Fig. 5.17 Detail of the stop-band in the Hanning, Hamming and Blackman windows

262
5
Digital Filters
Program 5.14 Comparison of stop-band of Hw(w) of Hanning, Hamming and Blackman windows
% Comparison of stop-band of Hw(w) of Hanning,
% Hamming and Blackman windows
fs=130; %sampling frequency in Hz.
N=50; %even order
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
dend=[1]; %transfer function denominator
hw=hanning(N+1); %Hanning window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
subplot(1,3,1);semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 0.04]); title('Hanning');
grid;
hw=hamming(N+1); %Hamming window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
subplot(1,3,2); semilogx(f,abs(G),'r'); %plots gain
axis([1 100 0 0.04]); title('Hamming');
grid;
hw=blackman(N+1); %Blackman window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
subplot(1,3,3); semilogx(f,abs(G),'b'); %plots gain
axis([1 100 0 0.04]); title('Blackman');
grid; xlabel('Hz.');
5.3.3.3
The Kaiser Windows
Thereis atrade-off betweenmainlobewidthandside-lobeamplitudeinthefrequency
response of the windows themselves. Kaiser developed a family of windows with a
parameter β that controls this trade-off. The expression of a Kaiser window is the
following:
hw(n) = Io ( β
	
(1 −( (n −1 −α)/α)2)
Io(β)
, 1 ≤n ≤N + 1
(5.33)
where α = (N + 1)/2, and Io() is a zero-th order modiﬁed Bessel function:
Io(x) = 1 +
∞

k = 1
(x / 2)k
k!
2
(5.34)
As β increases, the stop-band attenuation of the Kaiser windowed ﬁlter improves.
The next table compares values of the side-lobe amplitude of Hw(ω), and the transi-
tion width and stop-band attenuation of the corresponding ﬁlter response H f (ω) for
several values of β.

5.3 FIR Digital Filters
263
Fig. 5.18 Comparison of
hw(n) of Kaiser window for
several values of β
0
0.2
0.4
0.6
0.8
1
1.2
hw(n)
n
β
Side lobe (dB) Transition width Stop-band attenuation (dB)
2
-19
1.5
-29
3
-24
2
-37
4
-30
2.6
-45
5
-37
3.2
-54
6
-44
3.8
-63
7
-51
4.5
-72
8
-59
5.1
-81
Figure5.18, which has been obtained with the Program 5.15, compares the se-
quences hw(n) of the Kaiser window for values of β from 1 to 8.
Program 5.15 Comparison of hw(n) of Kaiser window
% Comparison of hw(n) of Kaiser window
fs=130; %sampling frequency in Hz.
N=50; %even order
beta=1; %filter parameter
hw=kaiser(N+1,beta); %Kaiser window
plot(hw,'k'); %plots hw(n)
hold on;
for beta=2:8,
hw=kaiser(N+1,beta); %Kaiser window
plot(hw,'k'); %plots hw(n)
end
axis([1 51 0 1.2]);
title('50th Kaiser hw(n)'); xlabel('n');
Figure5.19 compares the frequency response H f (ω) of the 50th FIR windowed
ﬁlter, using the Kaiser window with values of β from1 to 6.

264
5
Digital Filters
Program 5.16 Comparison of Hf(w) of Kaiser window
% Comparison of Hf(w) of Kaiser window
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
dend=[1]; %transfer function denominator
for beta=1:6,
hw=kaiser(N+1,beta); %Kaiser window
numd=fir1(N,fc,hw); %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
subplot(2,3,beta);semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.2]);
grid;
end
title('Hf(w) of 50th Kaiser windowed filter');
xlabel('Hz.');
Figure5.20 shows in more detail the stop band of Hw(ω) of the Kaiser window
itself, for values of β from 1 to 6.
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Hz.
 Hf (w)
beta=1 
beta=2 
beta=3 
beta=4 
beta=5 
beta=6 
Fig. 5.19 Frequency response of the Kaiser windowed ﬁlter for several values of β

5.3 FIR Digital Filters
265
10
0
10
1
10
2
0
0.1
0.2
0.3
0.4
0.5
10
0
10
1
10
2
0
0.1
0.2
0.3
0.4
0.5
10
0
10
1
10
2
10
0
10
1
10
2
10
0
10
1
10
2
10
0
10
1
10
2
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
0
0.1
0.2
0.3
0.4
0.5
Hz.
Hw (w)
beta=1 
beta=2 
beta=3 
beta=4 
beta=5 
beta=6 
Fig. 5.20
Detail of the stop-and in the Kaiser window for several values of β
Program 5.17 Comparison of Hw(w) of Kaiser window
% Comparison of Hw(w) of Kaiser window
fs=130; %sampling frequency in Hz.
N=50; %even order
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
dend=[1]; %transfer function denominator
for beta=1:6,
hw=kaiser(N+1,beta); %Kaiser window
numd=2*hw/N; %transfer function numerator
G=freqz(numd,dend,f,fs); %computes frequency response
subplot(2,3,beta);semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 0.5]);
grid;
end
title('stop-band of Hw(w) of 50th Kaiser window');
xlabel('Hz.');

266
5
Digital Filters
Fig. 5.21 A typical
frequency response of a
digital ﬁlter
2
4
6
8
10
12
14
16
18
20
0
0.2
0.4
0.6
0.8
1
1.2
1.4
Gain
Hz.
Frequency response of a filter
passband ripple
stopband ripple
transition width 
wp 
ws 
5.3.4
Optimization
The design of the digital FIR could be guided by optimization criteria. Depending
on these criteria, windows can still be used or other type of solutions should be
adopted [15].
A typical digital ﬁlter frequency response is depicted in Fig.5.21. There are three
frequency ranges, Δp [0..ωp], Δt [ωp..ωs], Δs [ωs..π], corresponding to the pass-
band, the transition, and the stop-band respectively. Most concerns of ﬁlter designers
are related to ripple in the stop-band, looking for instance at the amplitude of the
side-lobe adjacent to the main lobe. Usually the ﬁlter design enters in a multiobjec-
tive scenario, trying to get good compromises of several interdependent targets. For
example, ripple and transition width are related: smaller ripple can be obtained at
the expense of a wider transition.
Measurable criteria to be optimized can be speciﬁed by considering the difference
(the error) between the ideal ﬁlter frequency response HD(ω) and the frequency
response of the actual ﬁlter H(ω):
E(ω) = H(ω) −HD(ω)
(5.35)
Notice that in Δp and Δs E(ω) is the ripple of the frequency response of the actual
ﬁlter. In Δt E(ω) has non-zero value except possibly for a few zeros.
In general the pass-band ripple and the stop-band ripple in windowed FIR ﬁlters
are mutually dependent. There are no-window FIR ﬁlters that allow for independent
speciﬁcation of both ripples. In this part of the chapter, optimal windowed FIR ﬁlters
are ﬁrst considered, and then optimal no-window FIR ﬁlters are studied.

5.3 FIR Digital Filters
267
5.3.4.1
Boxcar and Chebyshev Windows
The boxcar() window provided by the MATLAB Signal Processing Toolbox has a
brickwall shape: it consists of a series of ones. Using boxcar() for a windowed FIR
ﬁlter, despite the ripples it introduces, gives the least squares error ¯e for the entire
frequency range. In other words, this ﬁlter minimizes the L2-norm of the error:
¯e = ∥E(w)∥2 =
⎛
⎝1
2π
π

−π
|E(w)|2
⎞
⎠
1/2
(5.36)
According with the Parseval’s Theorem, Eq. (5.36) is equal to:
¯e =

∞

n=−∞
|h(n) −h D(n)|2
1/2
(5.37)
and:
¯e =

N

n=−N
|h(n) −h D(n)|2 +
−N−1

n=−∞
h2
D(n) +
∞

n=−N+1
h2
D(n)
1/2
(5.38)
Since using the boxcar window, h(n) = hD(n) for −m ≤n ≤m, then we get a
minimal value of ¯e:
¯e =
−N−1

n=−∞
h2
D(n) +
∞

n=N+1
h2
D(n)
1/2
(5.39)
Figure5.22 shows the sequence hw(n) of the boxcar window, a series of ones, and
the frequency response HF (ω) of the 50th corresponding windowed ﬁlter. The ﬁgure
has been obtained with the Program 5.10, similar to the Program 5.9.
Program 5.18 hw(n) of boxcar window, and frequency response Hf(w) of windowed ﬁlter
% hw(n) of boxcar window, and frequency
% response Hf(w)
% of windowed filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
hw=boxcar(N+1);
numd=fir1(N,fc,hw); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)
stem(hw,'k'); %plots hw(n)
axis([0 52 0 1.2]);
title('boxcar hw(n)'); xlabel('n');
subplot(1,2,2)

268
5
Digital Filters
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.2]); grid;
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) 50th windowed filter')
The integral inside parenthesis in Eq. (5.36) can be considered as the ‘energy’
of the error. This energy is distributed into pass-band and stop-band ripple, and the
transition. This is why smaller ripple can be obtained with a wider transition. In the
case of the boxcar ﬁlter, the transition is very rapid, as depicted in Fig.5.22, and so
ripple is signiﬁcant in Δp and Δs.
Another optimization criterion can be to focus on the ripple peaks, and try to have
all peaks under a minimal bound. Of course, it is enough to see what happens with
the maximum ripple peak. This is related to the L ∞-norm of the error:
∥E(ω)∥∞= maxω∈Δ |E(ω)|
(5.40)
Thus the optimization target is to minimize the L ∞-norm of the error in a certain
frequency range. The case belongs to the general topic of minimax optimization.
0
0.2
0.4
0.6
0.8
1
1.2
hw(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
1.2
Gain
Hz.
Hf(w) 
Fig. 5.22
Boxcar window and frequency response of the corresponding windowed FIR ﬁlter

5.3 FIR Digital Filters
269
0
0.2
0.4
0.6
0.8
1
1.2
hw(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz.
Hf(w) 
Fig. 5.23
Chebyshev window and frequency response of the corresponding windowed FIR ﬁlter
Using the chebwin() window (the name refers to Chebyshev), the side-lobe mag-
nitude can be speciﬁed to be R dB below the main lobe magnitude (this is a minimal
bound). Actually all side-lobes in this ﬁlter are of equal height. The ﬁlter has the
smallest transition width compared to other windowed ﬁlters [13, 16]. This is one of
the MATLAB windows for FIR ﬁlters that are adjustable, instead of being ﬁxed.
Figure5.23, generated with the Program 5.19, shows the sequence hw(n) of the
chebwin window for R = 20 dB, and the frequency response HF(ω) of the 50th
corresponding windowed ﬁlter.
Program 5.19 hw(n) of Chebyshev window, and frequency response Hf(w) of windowed ﬁlter
% hw(n) of Chebyshev window, and frequency
% response Hf(w)of windowed filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
R=20; %ripple (dB)
hw=chebwin(N+1,R); %Chebyshev window
numd=fir1(N,fc,hw); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)
stem(hw,'k'); %plots hw(n)
axis([1 51 0 1.2]);
title('Chebyshev hw(n)'); xlabel('n');
subplot(1,2,2)
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
G=freqz(numd,dend,f,fs); %computes frequency response

270
5
Digital Filters
semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) 50th windowed filter')
5.3.4.2
The Parks–McLellan Filter
A procedure for solving the minimax optimization of a digital ﬁlter weighted error
was proposed in 1972 by Parks and McClellan. It is based on the Chebyshev alterna-
tion theorem, and it uses the Remez exchange algorithm to get the optimal solution
[17, 25]. The FIR digital ﬁlter which is obtained is a no-window ﬁlter.
Recall Eq. (5.35). Now, the following weighted error is considered:
E(ω) = W(ω) [H(ω) −HD(ω)]
(5.41)
where W(ω) is a positive weighting function. It is used to specify a trade-off between
the ripple amplitudes in the pass-band Δp and the stop-band Δs (small ripple in the
stop-band is frequently desired). Let us take as ripple amplitudes ± δ p in Δp, and
δs in Δs. Then W(ω) can be chosen either as 1 in Δp and (δp/ δs) in Δs, or (δs/ δp)
in Δp and 1 in Δs.
The problem is to design a FIR digital ﬁlter that minimizes the weighted error.
Usually the transition band Δt is taken as a ‘don’t care region’, so the minimization
focus on the ripple in Δp and Δs. Denote F as the union of Δp and Δs, and L = N/2
(N is the order of the ﬁlter).
The alternation theorem states that a necessary and sufﬁcient condition for a ﬁlter
to be the optimal solution, is that there are at least L + 2 alternations of the weighted
error E(ω) in F.
That means there must be at least L + 2 extremal frequencies, ω0< ω1 < · · ·
< ωL+1 in F such that:
E(ω j) = −E(ω j+1) and
E(ω j)
 = maxω∈F |E(ω)|
(5.42)
Notice that the alternation theorem establishes that the optimum ﬁlter is equi-ripple.
From the alternation theorem and (5.21), equalling derivatives to zero, a set of
equations can be written, one equation for each extremal frequency.
The function remez() uses the Remez exchange algorithm to solve the set of
equations, obtaining the optimal FIR ﬁlter.
In practice, the ﬁlter design begins by using the function remezord() as follows:
[N, f o, mo, w] = remezord( f, m, dev, f s]
where f = [ωp ωp] speciﬁes the transition band (both frequencies in Hz); m contains
the desired magnitude response values at the pass-band and stop-band of the ﬁlter,

5.3 FIR Digital Filters
271
-0.05
0
0.05
0.1
0.15
0.2
h(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Hz.
 Hf(w)
Fig. 5.24
Impulse response h(n) and frequency response H(ω) of a 50th Parks–McClelland FIR
ﬁlter
so usually m = [10]. The vector dev has the same number of entries as m; it speciﬁes
the maximum ripple magnitude at the pass-band and the stop-band. The sampling
frequency fs is given in Hz. After running remezord() the ﬁlter is obtained with:
b = remez(N, f o, mo)
where N is the order of the ﬁlter, fo is a vector with the frequency band edges
(frequencies from 0 to 1; 1 corresponds to half fs), and mo is a vector with amplitudes
of the frequency response at the pass-band and the stop-band.
Figure5.24 shows the impulse response and the frequency response of a 50th
Parks–McClellan ﬁlter, obtained with the Program 5.20. Previously remezord() was
used to conﬁrm the values speciﬁed in remez().
Program 5.20 h(n) and Hf(w) of (Remez) Parks–McClellan ﬁlter
% h(n) and Hf(w) of (Remez) Parks-McClellan filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
%low-pass filter piecewise description:
F=[0 fc fc+0.05 1];
A=[1 1 0 0]; % " " "
numd=remez(N,F,A); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)

272
5
Digital Filters
stem(numd,'k'); %plots h(n)
axis([1 51 -0.05 0.2]);
title('h(n)'); xlabel('n');
subplot(1,2,2)
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.1]); grid;
xlabel('Hz.');
title('50th Parks-McClelland filter Hf(w)')
It is interesting to compare Figs.5.23 and 5.24. The Parks–McClelland ﬁlter is
equi-ripple and the Chebyshev windowed ﬁlter is almost equi-ripple. The reader is
invited to make graphical comparisons in more detail.
5.3.4.3
The Least-Squares Error Filter
The ﬁrls() function computes a FIR ﬁlter that minimizes the L2-norm of the weighted
error in F [2]. The ﬁlter obtained, which is a no-window ﬁlter, has less ripple ‘energy’
than any equi-ripple ﬁlter of the same order.
Figure5.25 shows the impulse response and the frequency response of a 50th least-
squares FIR ﬁlter. The ﬁgure has been obtained with the Program 5.21; notice in this
program that the ﬁlter has been speciﬁed like in the case of the Parks–McClelland
ﬁlter.
Program 5.21 h(n) and Hf(w) of least-squares error ﬁlter
% h(n) and Hf(w) of least-squares error filter
fs=130; %sampling frequency in Hz.
fc=10/(fs/2); %cut-off at 10 Hz.
N=50; %even order
%low-pass filter piecewise description:
F=[0 fc fc+0.05 1];
A=[1 1 0 0]; % " " "
numd=firls(N,F,A); %transfer function numerator
dend=[1]; %transfer function denominator
subplot(1,2,1)
stem(numd,'k'); %plots h(n)
axis([1 51 -0.05 0.2]);
title('h(n)'); xlabel('n');
subplot(1,2,2)
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.1]); grid;
xlabel('Hz.');
title('50th least-squares error filter Hf(w)')

5.3 FIR Digital Filters
273
5.3.5
Other FIR Filters
5.3.5.1
Raised Cosine Filters
A raised cosine ﬁlter is a low-pass ﬁlter which is typically used for pulse shaping in
digital data transmission systems, like in the case of modems [4, 11]. The frequency
response of this ﬁlter is ﬂat in the pass-band; it sinks in a cosine curve to zero in the
transition region; and it is zero outside the pass-band. The equations that deﬁne the
ﬁlter are the following:
H(ω) =
⎧
⎨
⎩
1,
f or ω < ωc(1 −β)
1
2 (1 + cos ( π (ω −ωc (1−β))
2 β ωc
)) , f or ωc(1 −β) < ω < ωc(1 + β)
0,
f or ωc(1 + β) < ω
(5.43)
where H(ω) is the frequency response of the ﬁlter, and β is a parameter denoted as
the ‘roll-off factor’. The roll-off factor can take values between 0 and 1. Figure5.26
shows several proﬁles of H(ω) for different values of the roll-off factor (0, 0.25,
0.5, 0.75, 1). The ﬁgure is obtained with the Program 5.22, which uses the ﬁrrcos()
function. Notice that for β = 1 the frequency response has a cosine proﬁle raised
over zero level; this is the origin of the ﬁlter name.
-0.05
0
0.05
0.1
0.15
0.2
h(n)
n
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Hz.
Hf(w)
Fig. 5.25
Impulse response h(n) and frequency response H(ω) of a 50th least-squares FIR ﬁlter

274
5
Digital Filters
Fig. 5.26
Frequency
responses H(ω) of a 50th
raised cosine FIR ﬁlter
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
1.2
Gain
Hz.
Hf(w)
beta=1 
beta=0 
Program 5.22 Frequency response of raised cosine ﬁlter
% frequency response of raised cosine filter
fs=130; %sampling frequency in Hz.
fc=10; %cut-off at 10 Hz.
N=50; %even order
beta=0; %roll-off factor
%transfer function numerator:
numd=firrcos(N,fc,beta,fs,'rolloff');
dend=[1]; %transfer function denominator
%logaritmic set of frequency values in Hz:
f=logspace(0,2,200);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); hold on; %plots gain
axis([1 100 0 1.2]); grid;
ylabel('Gain'); xlabel('Hz.');
title('Hf(w) 50th raised-cosine filter')
for beta=0.25:0.25:1,
%transfer function numerator:
numd=firrcos(N,fc,beta,fs,'rolloff');
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
end
Notice that the bandwidth of the ﬁlter is determined by ωc(1 + β), which marks
the beginning of the stop-band.
Some pages before (in Sect.5.3.3.2, about the Hamming and other cosine-based
windows) we mentioned a raised cosine concept. This was then applied to the win-
dows of FIR windowed ﬁlters. Now the concept is being applied to H(ω), the fre-
quency response of the cosine-raised ﬁlter, which is a non-window FIR ﬁlter.
Figure5.27, obtained with the Program 5.23, shows several proﬁles of the impulse
response of the cosine-raised ﬁlter for different values of β (0, 0.25, 0.5, 0.75, 1).
The important detail to observe is that all proﬁles have the same zero amplitude

5.3 FIR Digital Filters
275
Fig. 5.27 Impulse responses
h(n) of a 50th raised cosine
FIR ﬁlter
5
10
15
20
25
30
35
40
45
50
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
Impulse response 
beta=0 
beta=1 
crossings, at n = L, 2L, 3L, . . .; where L = fs/(2 fc), fs is the sampling frequency
and fc the cut-off frequency of the ﬁlter.
Program 5.23 Impulse response of raised cosine ﬁlter
% Impulse response of raised cosine filter
fs=130; %sampling frequency in Hz.
fc=10; %cut-off at 10 Hz.
beta=0; %roll-off factor
N=50; %even order
%transfer function numerator:
numd=firrcos(N,fc,beta,fs,'rolloff');
dend=[1]; %transfer function denominator
plot(numd,'-xk'); hold on; %plots impulse response
axis([1 51 -0.05 0.18]);
title('Impulse response of 50th raised
cosine filter');
for beta=0.25:0.25:1,
%transfer function numerator:
numd=firrcos(N,fc,beta,fs,'rolloff');
plot(numd,'-xk'); %plots impulse response
end
Consider a signal with frequency fu and period Tu. If the signal is sampled with
frequency fs, with fs > fu, we obtain Ns = fs/fu samples for each period Tu of the
signal. Figure5.28, obtained with the simple Program 5.24, shows a train of impulses
with a frequency fu. This series of impulses may correspond to digital data to be
transmitted. A sampler is synchronized with the data, so the impulses are translated
to Ns samples between impulses, containing the impulses themselves as shown in
Fig.5.28.

276
5
Digital Filters
Fig. 5.28
Train of impulses
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
0
0.2
0.4
0.6
0.8
1
1.2
seconds
Program 5.24 Train of impulses
% train of impulses
fs=130; %sampling frequency in Hz.
fu=10; %signal frequency in Hz.
Ns=fs/fu; %number of samples per signal period
tiv=1/fs; %time intervals between samples
nsm=ceil(Ns/2);
%impulse in the middle of zeros:
u1=zeros(1,Ns); u1(nsm)=1;
u=[u1,u1,u1,u1,u1]; %signal with 5 periods
t=0:tiv:((5/fu)-tiv); %time intervals set (5 periods)
stem(t,u,'kx'); %plot impulse train
title('train of impulses'); xlabel('seconds');
axis([0 0.5 0 1.2]);
The idea now is to make fc = fu. The ﬁlter will eliminate high frequency noise.
Since the input of the ﬁlter is a series of separated impulses, the output of the ﬁlter will
be a series of separated impulse responses. Due to the zero crossings of the impulse
responses at n = L, 2L, 3L, . . ., the successive impulse responses do not interfere
with each other. This makes easy to recover at the end of the transmission channel, the
original data. For this reason the raised cosine ﬁlter is used in telecommunications.
Figure5.29, obtained with the Program 5.25, shows the train of impulse responses
given by the ﬁlter output; notice the effect of the ﬁlter delay.
Program 5.25 Response of the raised cosine ﬁlter to the train of impulses
% response of the raised cosine filter
% to the train of impulses
fs=130; %sampling frequency in Hz.
fu=10; %signal frequency in Hz.
Ns=fs/fu; %number of samples per signal period
tiv=1/fs; %time intervals between samples

5.3 FIR Digital Filters
277
Fig. 5.29
Response of the
50th raised cosine FIR ﬁlter
to the train of impulses
0
0.05
0.1
0.15 0.2
0.25 0.3
0.35 0.4
0.45 0.5
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
seconds
% the train of impulses
nsm=ceil(Ns/2);
%impulse in the middle of zeros:
u1=zeros(1,Ns); u1(nsm)=1;
u=[u1,u1,u1,u1,u1]; %signal with 5 periods
t=0:tiv:((5/fu)-tiv); %time intervals set (5 periods)
% the filter
fc=fu; %cut-off frequency
beta=0.5; %roll-off factor
N=50; %even order
%transfer function numerator:
numd=firrcos(N,fc,beta,fs,'rolloff');
dend=[1]; %transfer function denominator
% the filter output
y=filter(numd,dend,u);
plot(t,y,'-kx')
title('raised cosine filter response');
xlabel('seconds');
Figure5.30 shows the output of the ﬁlter if we make fc = 2 fu, to separate more
the impulse responses. The reader may wish to explore what happens with 3 fu, 4 fu,
etc.
5.3.5.2
Savitzky–Golay Filter
In 1964 Savitzky and Golay proposed a ﬁlter that has found many applications, for
de-noising or smoothing cases where transients in signals or borders in images should
be highlighted [24]. For instance in chemical analysis using spectra, it is important
to make clear peak heights and line widths, while suppressing random noise. In fact,
the Savitzky–Golay ﬁlter can be used to estimate derivatives of the signal.

278
5
Digital Filters
Fig. 5.30 Response of the
50th raised cosine FIR ﬁlter
with 2 fu cut-off frequency to
the train of impulses
0
0.05
0.1
0.15 0.2
0.25 0.3
0.35 0.4
0.45 0.5
-0.05
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
seconds
The smoothing strategy of the ﬁlter is a least square ﬁtting of a lower order
polynomial to a number of consecutive points (let us denote this set of points as a
frame). Given a data point A, a polynomial is ﬁtted using A and surrounding points,
and then the value B of the polynomial is computed at A. Then the value B is taken
as the new smoothed data point. This is repeated for all data points. It was found
that a matrix of pre-computed coefﬁcients can be obtained for the polynomials.
Consequently, the ﬁlter just uses pre-computed tables or routines that generate these
tables. Therefore is more computationally efﬁcient than least square algorithms.
Let us take for example a frame of 7 data points and a polynomial order of 3.
Using the sgolay() function, MATLAB ﬁnds the 7 ﬁtting polynomials and computes
the coefﬁcients for a time-varying FIR ﬁlter. Figure5.31, obtained with the Program
5.26, shows these coefﬁcients in 7 plots. Each of the plots corresponds to a row of
the matrix obtained by the function sgolay(). The ﬁrst 3 rows, on top of the ﬁgure,
are to be applied to the signal during the terminal transient. The last 3 rows, at the
bottom of the ﬁgure, are to be applied to the signal during the startup transient. And
the center row is to be applied to the signal in the steady state.
Program 5.26 FIR ﬁlter coefﬁcients with Savitzky–Golay ﬁlter
% FIR filter coeeficients with Savitzky-Golay filter
K=3; %polynomial order
FR=7; %frame size
numd=sgolay(K,FR); %numerator rows
dend=[1]; %denominator
for rr=1:FR,
subplot(FR,1,rr);
plot(numd(rr,:),'-kx');
axis([0 FR+1 -0.3 1]);
end
The sgolayﬁlt() function automatically does all the ﬁltering work. Figure5.32
shows the output of the Savitzky–Golay ﬁlter for a noisy signal input. This input

5.3 FIR Digital Filters
279
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
0
1
2
3
4
5
6
7
8
0
0.5
1
Fig. 5.31
FIR ﬁlter coefﬁcients to be applied, according with the Savitzky–Golay strategy
signal has been obtained adding random noise to a sawtooth periodic signal. The
user is invited to experiment, in the Program 5.27, with the order of the polynomial,
K, and the size of the frame, FR (K must be less than FR). Notice that the ﬁltered
signal preserves an approximation of the sawtooth brisk changes.
Program 5.27 Frequency response of sgolay ﬁlter
% frequency response of sgolay filter
fs=300; %sampling frequency in Hz.
K=6; %polynomial order
FR=25; %frame size
%input signal
fu=3; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
tiv=1/fs; %time intervals between samples
t=0:tiv:(1-tiv); %time intervals set (1 seconds)
nn=length(t); %number of data points
us=sawtooth(wu*t); %sawtooth signal

280
5
Digital Filters
Fig. 5.32
On top, the pure
sawtooth signal. Below, the
sawtooth+noise input signal
and the response of the
Savitzky–Golay ﬁlter
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-2
-1
0
1
2
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-2
-1
0
1
2
seconds
ur=randn(nn,1); %random signal
u=us+0.16*ur'; %the signal+noise
%filter output
y=sgolayfilt(u,K,FR); %filter output signal
%figure
subplot(2,1,1)
plot(t,us,'k');
axis([0 1 -2 2]);
title('Savitzky-Golay filter')
subplot(2,1,2)
plot(t,u,'b'); hold on;
plot(t,y,'r')
axis([0 1 -2 2]);
xlabel('seconds');
5.3.5.3
Interpolated FIR Filters (IFIR)
There have been several design techniques created for reducing the computational
complexity of FIR ﬁlters. One of these techniques is the interpolated FIR ﬁlter. The
basic idea is to decompose the ﬁlter into two FIR ﬁlter sections, as represented in
Fig.5.33.
Suppose you choose an ‘stretching factor’ L = 4. What will happen is that in the
ﬁrst block the input signal is ﬁltered by a narrowband ﬁlter, and then zero padding is
Fig. 5.33
Block diagram of
an IFIR ﬁlter
F(zL)
I(z)

5.3 FIR Digital Filters
281
applied with three zeros between every signal sample. The output of the ﬁrst block
is four times the length of the input signal. Now, the second block performs an
interpolation between non-zero samples, obtaining the desired ﬁltered output.
MATLAB SPT provides the function intﬁlt() for a band-limited interpolator. The
example of use given by SPT, includes several steps:
1. Design of an interpolation ﬁlter, speciﬁed for a band limit factor alpha = 0.5,
meaning half the Nyquist frequency:
alpha = 0.5; h1=intﬁlt(4,2,alpha);
2. Given an input signal uof 200 samples, apply a narrowband FIR ﬁlter:
z = ﬁlter(ﬁr1(40,0.5),1,u);
3. Insert three zeros between every sample:
N = length(z);
r = reshape ([z zeros(N,3)]’, 4*N,1);
4. Interpolate using the interpolation ﬁlter designed in (1):
y = ﬁlter(h1,1,r);
The literature reports savings of around 72% of computational effort (much less
multipliers) [8].
The key references for this technique are [18, 19].
5.3.6
Details of FIR Filters in the MATLAB Signal
Processing Toolbox
Concerning FIR ﬁlters, three main types can be differentiated in the MATLAB Signal
Processing Toolbox. One is the window-based FIR ﬁlters, another is least square error
ﬁlters, and the other is equi-ripple ﬁlters. For each of the three types, the Toolbox
offers interesting options.
For instance, constraints can be speciﬁed for the least square error ﬁlter, using the
ﬁlter ﬁrcls1(), so the approximation to the ideal response cannot go out from these
constraints.
Recall from Programs 5.20 and 5.21 that a multi-band description of the desired
frequency response was used, in terms of a vector of frequencies and another vector
of response amplitudes. This is a feature that MATLAB extends to several FIR ﬁlters:
• For window-based FIR ﬁlters, there is the ﬁlter ﬁr2(), that includes a multi-band
description.
• For least square error ﬁlters, there is the constrained multi-band ﬁlter ﬁrcls().
• In the case of equi-ripple ﬁlters, remez() is already multi-band, and there is the
version cremez() that can handle non-linear phase ﬁlters with possibly complex
frequency responses.
The optimization procedure applied by remez() and cremez(), does consider tran-
sitions between bands as ‘don’t care’ regions, where no optimization is tried.

282
5
Digital Filters
Once the numerator coefﬁcients are obtained, with any of the FIR alternatives,
the output of the ﬁlter for a given input can be computed with ﬁlter(), which uses a
difference equation, or computed with fftﬁlt(), which uses an efﬁcient Fast Fourier
Transform (FFT) method of overlap-add. Actually it is faster to use this method
instead of the direct convolution provided by ﬁlter(), when the length nb of the ﬁlter
is greater than 60. The overlap and add method breaks the input sequence u(n) into
segments of length L. Then it implements the following procedure:
y = i f f t ( f f t(u(i : i + L −1), N). ∗f f t(b, N));
(5.44)
where N is the length of the FFT. The function fftﬁlt() chooses the values of N and
L for you. The procedure goes to the frequency domain via FFT, and then comes
back to the time domain with the inverse FFT. The complete output y(n) is built
by adding successive y(n) segments of length L + nb −1 making them overlap by
nb −1 points.
5.4
IIR Digital Filters
IIR ﬁlters offer more design freedom, usually with transfer functions having numer-
ator and denominator polynomials of moderate degree. However in general there is
no linear phase, and there is the risk of being unstable.
There are two main ways to obtain IIR digital ﬁlters. One is the classical approach,
with reference to analog ﬁlters. The other approach is the direct design.
5.4.1
Classical Approach
In the previous chapter, the Butterworth, Chebyshev, and elliptic ﬁlters were studied.
The MATLAB Signal Processing Toolbox offers digital versions of all these ﬁlters.
Recall that:
• The Butterworth ﬁlter, function butter(), has a maximally ﬂat magnitude response
in the pass-band.
• The Chebyshev type 1 ﬁlter, function cheby1(), is equi-ripple in the pass-band.
• The Chebyshev type 2 ﬁlter, function cheby2(), is equi-ripple in the stop-band.
• The elliptic ﬁlter, function ellip(), is equi-ripple in both the pass- and stop-bands.
For comparison purposes, the frequency responses of the four ﬁlters just cited are
presented in Fig.5.34. As can be seen in this ﬁgure, the frequency responses of the
digital ﬁlters are similar to their analog version. All the ﬁlters are of 5th order, much
less than the order of the FIR ﬁlters used in Sect.5.3. Notice that the horizontal axis
is in Hz. This ﬁgure has been generated with the Program 5.8.

5.4 IIR Digital Filters
283
Program 5.28 Comparison of frequency response of the 4 digital ﬁlters
% Comparison of frequency response
% of the 4 digital filters
fs=130; %sampling frequency in Hz
fc=10/(fs/2); %cut-off at 10 Hz
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%digital Butterworth filter:
[numd,dend]=butter(N,fc);
%logaritmic set of frequency values in Hz:
F=logspace(0,2);
G=freqz(numd,dend,F,fs); %computes frequency response
%plot linear amplitude:
subplot(2,2,1); semilogx(F,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('Hz'); title('Butterworth');
%digital Chebyshev 1 filter:
[numd,dend]=cheby1(N,Rp,fc);
G=freqz(numd,dend,F,fs); %computes frequency response
%plot linear amplitude:
subplot(2,2,2); semilogx(F,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('Hz'); title('Chebyshev 1');
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz
Butterw.
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz
Cheby1
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz
Cheby2
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hz
Elliptic
Fig. 5.34
Comparison of frequency response of the four digital ﬁlters

284
5
Digital Filters
0
10
20
30
40
-0.05
0
0.05
0.1
0.15
0.2
Butterw.
0
10
20
30
40
-0.1
0
0.1
0.2
0.3
Cheby1
0
10
20
30
40
-0.05
0
0.05
0.1
0.15
Cheby2
0
10
20
30
40
-0.1
-0.05
0
0.05
0.1
0.15
Elliptic
Fig. 5.35
Comparison of impulse response of the four digital ﬁlters
%digital Chebyshev 2 filter:
[numd,dend]=cheby2(N,Rs,fc);
G=freqz(numd,dend,F,fs); %computes frequency response
%plot linear amplitude:
subplot(2,2,3); semilogx(F,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('Hz'); title('Chebyshev 2');
%digital elliptic filter:
[numd,dend]=ellip(N,Rp,Rs,fc);
G=freqz(numd,dend,F,fs); %computes frequency response
%plots linear amplitude:
subplot(2,2,4); semilogx(F,abs(G),'k');
axis([1 100 0 1.1]); grid;
ylabel('Gain'); xlabel('Hz'); title('Elliptic');
Figure5.35 shows the impulse response of the previous 5th order digital ﬁlters.
The ﬁgure has been generated with the Program 5.29, which makes use of function
impz() to compute the impulse responses.
Program 5.29 Comparison of impulse response of the 4 digital ﬁlters
% Comparison of impulse response
% of the 4 digital filters
fs=130; %sampling frequency in Hz
fc=10/(fs/2); %cut-off at 10 Hz

5.4 IIR Digital Filters
285
nsa=50; %number of samples to visualize
N=5; %order of the filter
Rp=0.5; %decibels of ripple in the pass band
Rs=20; %decibels of ripple in the stop band
%digital Butterworth filter:
[numd,dend]=butter(N,fc);
%plot impulse response:
subplot(2,2,1); impz(numd,dend,nsa);
title('Butterworth');
%digital Chebyshev 1 filter:
[numd,dend]=cheby1(N,Rp,fc);
%plot impulse response:
subplot(2,2,2); impz(numd,dend,nsa);
title('Chebyshev 1');
%digital Chebyshev 2 filter:
[numd,dend]=cheby2(N,Rs,fc);
%plot impulse response:
subplot(2,2,3); impz(numd,dend,nsa);
title('Chebyshev 2');
%digital elliptic filter:
[numd,dend]=ellip(N,Rp,Rs,fc);
%plot impulse response:
subplot(2,2,4); impz(numd,dend,nsa);
title('Elliptic');
The digital ﬁlters are stable if all their poles are inside the unit circle in the z-
plane. Figure5.36, which has been obtained with the Program B.6, shows that all the
previous 5th digital ﬁlters are stable. The four ﬁlters have ﬁve zeros and ﬁve poles.
In the cases of Butterworth and Chebyshev1, the ﬁve zeros are almost coincident,
so they cannot be distinguished in the ﬁgure. There exists the function pzmap() to
draw pole-zero maps, but we preferred a personal way of doing the same, in order
to introduce larger marks for poles and zeros. The grid draw by zgrid() is helpful to
study the damping and natural frequencies corresponding to the poles.
The Program B.6 has been included in Appendix B.
Another classical way for the design of digital IIR ﬁlters is to use discretization
methods, based on bilinear transformation or based on impulse invariance. This was
already studied in Sect.5.2.
5.4.2
Direct Design
There is a number of direct methods to obtain digital IIR ﬁlters, departing from
a known desired frequency response, or from a known desired impulse response.
Notice that using the discrete inverse Fourier transform, ifft(), one can obtain the
impulse response from the frequency response.
The target of the IIR design is to obtain the discrete transfer function of the
IIR ﬁlter. The degree of the denominator is na, and the degree of the numerator is

286
5
Digital Filters
nb. There are two types of IIR ﬁlters: the all-pole IIR ﬁlters, with nb = 0, and the
recursive IIR ﬁlters, with nb > 0.
5.4.2.1
Frequency Domain Speciﬁcation
Suppose there is a desired frequency response described with a vector F of frequen-
cies, and a vector M of response amplitudes.
Departing from F and M, the function yulewalk() designs recursive IIR ﬁlters,
using modiﬁed Yule-Walker equations and a computation algorithm involving time
and frequency domains [9]. The algorithm performs a least square ﬁt to the speciﬁed
frequency response. The result is a discrete transfer function with na = nb.
Figure5.37 shows an example of IIR design via approximation with yulewalk()
to a desired frequency response proﬁle. We selected an order of 8 for the IIR ﬁlter.
The ﬁgure has been generated with the Program 5.30. Notice that frequencies are
speciﬁed in a normalized way from 0 to 1 (1 corresponds to half the sampling rate).
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Butterw.
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Cheby1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Cheby2
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Elliptic
Fig. 5.36
Comparison of pole-zero maps of the four digital ﬁlters

5.4 IIR Digital Filters
287
Fig. 5.37 Frequency
response amplitude of a
Yule–Walker ﬁlter
approximating desired
frequency response
amplitude (dotted line)
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
1.2
Gain
Hf(w) 
Hz.
Program 5.30 Frequency response of IIR yulewalk ﬁlter
% frequency response of IIR yulewalk filter
fs=130; %sampling frequency in Hz.
%frequency response specification (0 to 1):
F=[0 0.3 0.5 0.7 1];
M=[1 1 0 1 1]; %" "" ""
N=8; %order of the digital IIR filter
[numd,dend]=yulewalk(N,F,M); %filter computation
%linear set of frequency values in Hz:
f=linspace(0,65);
G=freqz(numd,dend,f,fs); %computes frequency response
plot(F*fs/2,M,'--k'); hold on;
plot(f,abs(G),'k'); %plots gain
axis([0 60 0 1.2]);
ylabel('Gain'); title('Hf(w) 5th yulewalk filter')
xlabel('Hz.');
It is interesting to examine the location of poles and zeros of the IIR ﬁlter just
being computed. Using the Program 5.31 we obtain the Fig.5.38, in which the pole-
zero map of the IIR ﬁlter is depicted in the z-plane with the unit circumference. It is
clear that all poles are inside the unit circle, so the ﬁlter is stable.
Program 5.31 Pole-zero map of IIR yulewalk ﬁlter
% pole-zero map of IIR yulewalk filter
fs=130; %sampling frequency in Hz.
%frequency response specification (0 to 1):
F=[0 0.3 0.5 0.7 1];
M=[1 1 0 1 1]; %" "" ""
N=8; %order of the digital IIR filter
[numd,dend]=yulewalk(N,F,M); %filter computation
theta=0:.1:2*pi; nn=length(theta); ro=ones(1,nn);
polar(theta,ro,'--k'); hold on; %draw a circumference
fdt=tf(numd,dend);
pzmap(fdt); %pole-zero map

288
5
Digital Filters
Fig. 5.38 Pole-zero map of
the obtained IIR ﬁlter
Real Axis
Imag Axis
-1
-0.8 -0.6 -0.4 -0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
It is also opportune to see how the phase of the IIR ﬁlter just obtained looks like.
It has been computed with the Program 5.32, and the result is shown in Fig.5.39.
Program 5.32 Phase of the frequency response of IIR yulewalk ﬁlter
% phase of the frequency response
% of IIR yulewalk filter
fs=130; %sampling frequency in Hz.
%frequency response specification (0 to 1):
F=[0 0.3 0.5 0.7 1];
M=[1 1 0 1 1]; %" "" ""
N=8; %order of the digital IIR filter
[numd,dend]=yulewalk(N,F,M); %filter computation
%linear set of frequency values in Hz:
f=linspace(0,65);
G=freqz(numd,dend,f,fs); %computes frequency response
plot(f,angle(G),'k'); %plot phases
ylabel('Phase (rad)');
title('Hf(w) 8th yulewalk filter')
xlabel('Hz.'); grid;
There are situations where a complex frequency response is desired. This is a
common case in automatic control applications, where amplitude and phase of the
system response are important. In this case, the function invfreqz() obtains IIR ﬁlters
that corresponds to a given complex frequency response. The result is a discrete
transfer function with possibly different values of na and nb.
The same desired frequency response amplitude in Fig.5.37 has been chosen for
the invfreqz() design. A desired frequency response phase speciﬁcation has been
added. After trying several values of na and nb, a reasonable approximation of the
speciﬁed frequency response was obtained, as shown in Fig.5.40. This ﬁgure has

5.4 IIR Digital Filters
289
Fig. 5.39 Phase of the
frequency response of the
obtained IIR ﬁlter
0
10
20
30
40
50
60
70
-1.5
-1
-0.5
0
0.5
1
1.5
Phase (rad)
Hf(w) 
Hz.
Fig. 5.40 Amplitude and
phase of the frequency
response of the obtained IIR
ﬁlter approximating desired
amplitude and phase
0
10
20
30
40
50
60
70
0
0.5
1
1.5
Gain
Hf(w) 
0
10
20
30
40
50
60
70
-2
-1
0
1
2
Phase
Hz.
been generated with the Program 5.33. Notice that invfreqz() needs the frequency
speciﬁcation in rad/s. Recall from Sect.4.4. that amplitude and phase are interdepen-
dent. A better speciﬁcation of the desired frequency response phase (following the
guide of Fig.5.39), would give the opportunity to more precise approximations.
Program 5.33 Frequency response of IIR invfreqz ﬁlter
% frequency response of IIR invfreqz filter
fs=130; %sampling frequency in Hz.
%frequency response specification (0 to 1):
F=[0 0.3 0.5 0.7 1];
A=[1 1 0 1 1]; %amplitude
PH=[0 -0.5 -1 0.5 0]; %phase in rad
W=F*pi; %frequencies in rad/s
%complex frequency response:
H=(A.*cos(PH))+(A.*sin(PH))*i;

290
5
Digital Filters
%degree of the digital IIR filter numerator
%and denominator
Nnum=2; Nden=4;
%filter computation:
[numd,dend]=invfreqz(H,W,Nnum,Nden);
%linear set of frequency values in Hz:
f=linspace(0,65);
%compute frequency response:
G=freqz(numd,dend,f,fs);
subplot(2,1,1)
plot(F*fs/2,A,'--k'); hold on;
plot(f,abs(G),'k'); %plots gain
ylabel('Gain'); title('Hf(w) invfreqz filter')
subplot(2,1,2)
plot(F*fs/2,PH,'--k'); hold on;
plot(f,angle(G),'k'); %plots gain
ylabel('Phase');
xlabel('Hz.');
Figure5.41, obtained with the Program 5.34, shows the pole-zero map of the IIR
ﬁlter.
Program 5.34 Pole-zero map of IIR invfreqz ﬁlter
% pole-zero map of IIR invfreqz filter
fs=130; %sampling frequency in Hz.
%frequency response specification (0 to 1):
F=[0 0.3 0.5 0.7 1];
A=[1 1 0 1 1]; %amplitude
PH=[0 -0.5 -1 0.5 0]; %phase in rad
W=F*pi; %frequencies in rad/s
%complex frequency response:
H=(A.*cos(PH))+(A.*sin(PH))*i;
%degree of the IIR filter numerator and denominator
Nnum=2; Nden=4;
%filter computation:
[numd,dend]=invfreqz(H,W,Nnum,Nden);
theta=0:.1:2*pi; nn=length(theta); ro=ones(1,nn);
polar(theta,ro,'--k'); hold on; %draw a circumference
fdt=tf(numd,dend);
pzmap(fdt); hold on; %pole-zero map
5.4.2.2
Time Domain Speciﬁcation
Now suppose there is a desired impulse response, described by a vector h of numbers.
The transfer function of a corresponding IIR ﬁlter must be determined. This problem
can be seen in the context of signal identiﬁcation, which tries to obtain a model of
a given signal (perhaps a noisy signal, or just noise). In our case, the signal is the
impulse response.
A main branch of signal identiﬁcation methods is centred in time-series models
(already introduced in the chapter on linear systems), such auto-regressive (AR)
models, moving-average (MA) models, and ARMA (combining AR and MA)

5.4 IIR Digital Filters
291
Fig. 5.41
Pole-zero map of
the obtained IIR ﬁlter
Real Axis
Imag Axis
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
models. Using the z transform, discrete transfer functions can be obtained, corre-
sponding to these models. Then, it can be seen that:
• FIR ﬁlters correspond to MA models
• IIR all-pole ﬁlters correspond to AR models
• IIR recursive ﬁlters correspond to ARMA models.
Given a signal, in our case the impulse response, MATLAB provides several
alternatives to obtain an AR model (an IIR all-pole ﬁlter). For instance, arcov() and
armcov(), using a covariance based approach, aryule(), with Yule-Walker equations,
and arburg(), which is the Burg’s method [5, 20, 29].
Figure5.42 shows the frequency response amplitude and the impulse response of
a simple all-pole IIR digital ﬁlter. It has been generated by the Program 5.35. The
impulse response will be used as the desired impulse response of the IIR ﬁlters to be
designed next.
Program 5.35 Reference IIR ﬁlter: frequency and impulse responses
%Reference IIR filter:frequency and impulse responses
fs=256; %sampling frequency in Hz
fmx=128; %input bandwidth in Hz
F=0:1:fmx-1; %response frequencies 0,1,2...Hz
%reference IIR filter
numd=1;
dend=[1 -0.5 0.1 0.5];
H=freqz(numd,dend,F,fs); %IIR frequency response
[h,th]=impz(numd,dend,64,fs); %impulse response
subplot(1,2,1)
plot(F,abs(H),'-rx'); hold on;
title('frequency response'); xlabel('Hz');
subplot(1,2,2)
plot(th,h,'-rx'); hold on;
axis([-0.02 0.25 -0.6 1.2]);
title('impulse response'); xlabel('seconds');

292
5
Digital Filters
0
50
100
150
0.5
1
1.5
2
2.5
3
3.5
4
4.5
frequency 
response
Hz
0
0.05
0.1
0.15 0.2
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
impulse
 response
seconds
Fig. 5.42 Frequency response amplitude and impulse response of a digital all-pole IIR ﬁlter
Let us check for instance the arburg() method. Figure5.43, generated with the
Program5.36,comparesthedesiredfrequencyandimpulseresponses,withtheresults
of arburg(). On the right hand side of Fig.5.43 slight divergences can be observed
between the reference impulse response (in x-marks) and the model obtained by
arburg() (in solid curve). Consequently, the frequency response of the IIR ﬁlter
given by arburg() (in solid curve), somewhat differs from the reference frequency
response (in x-marks), as can be seen on the left hand side.
Program 5.36 IIR from impulse response, using arburg
%IIR from impulse response, using arburg
fs=256; %sampling frequency in Hz
fmx=128; %input bandwidth in Hz
F=0:1:fmx-1; %response frequencies 0,1,2...Hz
numd=1;
dend=[1 -0.5 0.1 0.5];
H=freqz(numd,dend,F,fs); %IIR frequency response
[h,th]=impz(numd,dend,64,fs); %impulse response
subplot(1,2,1)
plot(F,abs(H),'rx'); hold on;
N=2; %IIR denominator degree
mdend=arburg(h, N); %IIR filter modelling
mnumd=1;
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('frequency response'); xlabel('Hz');
subplot(1,2,2)
plot(th,h,'rx'); hold on;

5.4 IIR Digital Filters
293
0
50
100
150
0
1
2
3
4
5
6
7
frequency
 response
Hz
0
0.05
0.1
0.15
0.2
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
impulse 
response
seconds
Fig. 5.43
Comparison of all-pole IIR desired and modelled response, for the arburg() case
[mh,mth]=impz(mnumd,mdend,64,fs);
plot(mth,mh,'k');
axis([-0.02 0.25 -0.8 1.2]);
title('impulse response'); xlabel('seconds');
Figure5.44, generated by Program 5.37, compares the results of the four men-
tioned methods, when modelling the impulse response of Fig.5.42. Only frequency
responses of the obtained IIR ﬁlters are shown. Both arcov() and aryule() are really
successful, giving the same denominator coefﬁcients as the IIR ﬁlter being used to
generate Fig.5.42.
Program 5.37 IIR from impulse response, the four methods
%IIR from impulse response, the four methods
fs=256; %sampling frequency in Hz
fmx=128; %input bandwidth in Hz
F=0:1:fmx-1; %response frequencies 0,1,2...Hz
numd=1;
dend=[1 -0.5 0.1 0.5];
figure(1)
H=freqz(numd,dend,F,fs); %IIR frequency response
[h,th]=impz(numd,dend,64,fs); %impulse response
subplot(2,2,1)
plot(F,abs(H),'rx'); hold on
N=3; %IIR denominator degree
mdend=arcov(h, N); %IIR filter modelling
mnumd=1;

294
5
Digital Filters
0
50
100
150
0
1
2
3
4
5
arcov() 
modelling
Hz
0
50
100
150
0
2
4
6
8
10
armcov() 
modelling
Hz
0
50
100
150
0
2
4
6
8
arburg()
 modelling
Hz
0
50
100
150
0
1
2
3
4
5
aryule()
 modelling
Hz
Fig. 5.44
Comparison of all-pole IIR desired and modelled responses, for the four methods
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('arcov() modelling'); xlabel('Hz');
subplot(2,2,2)
plot(F,abs(H),'rx'); hold on
N=6; %IIR denominator degree
mdend=armcov(h, N); %IIR filter modelling
mnumd=1;
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('armcov() modelling'); xlabel('Hz');
subplot(2,2,3)
plot(F,abs(H),'rx'); hold on
N=2; %IIR denominator degree
mdend=arburg(h, N); %IIR filter modelling
mnumd=1;
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);

5.4 IIR Digital Filters
295
plot(F,abs(HM),'k');
title('arburg() modelling'); xlabel('Hz');
subplot(2,2,4)
plot(F,abs(H),'rx'); hold on
N=3; %IIR denominator degree
mdend=aryule(h, N); %IIR filter modelling
mnumd=1;
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('aryule() modelling'); xlabel('Hz');
The function lpc() of the Signal Processing toolbox makes a linear prediction
that is coincident with the result of aryule(), since both use the same Yule-Walker
equations for the same kind of model.
There are also functions to perform ARMA modelling (IIR recursive ﬁlters). That
is the case of prony(), which uses the classical Prony’s method, [7, 26], and stmcb(),
which uses the Steiglitz–McBride iteration [27, 28].
Program 5.38 provides an example of using prony() to determine a model (the
transfer function of a recursive IIR ﬁlter) based on an impulse response. The pro-
gram begins by setting an example of impulse response, corresponding to a certain
IIR desired ﬁlter with numerator numd and denominator dend. Then the program
proceeds to use prony(). Notice that prony() requires a speciﬁcation of the numerator
and denominator degrees to be tried in the modelling effort. The real life case is
to have a certain impulse response with no idea of the numerator and denominator
degrees. The Program 5.38 generates the Fig.5.45, in which a good modelling result
is conﬁrmed.
Program 5.38 IIR from impulse response, using prony
%IIR from impulse response, using prony
fs=256; %sampling frequency in Hz
fmx=128; %input bandwidth in Hz
F=0:1:fmx-1; %response frequencies 0,1,2...Hz
%desired IIR response:
numd=[1 0.5 1];
dend=[1 -0.5 0.1 0.5];
H=freqz(numd,dend,F,fs); %IIR frequency response
[h,th]=impz(numd,dend,64,fs); %impulse response
subplot(1,2,1)
plot(F,abs(H),'rx'); hold on;
na=3; %IIR denominator degree
nb=2; %IIR numerator degree
[mnumd,mdend]=prony(h, nb,na); %IIR filter modelling
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('frequency response'); xlabel('Hz');
subplot(1,2,2)
plot(th,h,'rx'); hold on;
[mh,mth]=impz(mnumd,mdend,64,fs);
plot(mth,mh,'k');
axis([-0.02 0.25 -1.2 1.6]);
title('impulse response'); xlabel('seconds');

296
5
Digital Filters
0
50
100
150
0
1
2
3
4
5
6
7
8
frequency 
response
Hz
0
0.05
0.1
0.15
0.2
-1
-0.5
0
0.5
1
1.5
impulse 
response
seconds
Fig. 5.45
Comparison of recursive IIR desired and modelled response, for the prony() case
Program 5.39 offers an example of using stmcb(). It is similar to Program 5.38,
except for small changes in the IIR desired ﬁlter that were introduced only to avoid
repetition. The program generates the Fig.5.46, which also shows a good modelling
result.
Program 5.39 IIR from impulse response, using stmcb
%IIR from impulse response, using stmcb
fs=256; %sampling frequency in Hz
fmx=128; %input bandwidth in Hz
F=0:1:fmx-1; %response frequencies 0,1,2...Hz
%desired IIR response:
numd=[1 0.5 1];
dend=[1 -0.9 0.1 0.2];
H=freqz(numd,dend,F,fs); %IIR frequency response
[h,th]=impz(numd,dend,64,fs); %impulse response
subplot(1,2,1)
plot(F,abs(H),'rx'); hold on;
na=3; %IIR denominator degree
nb=2; %IIR numerator degree
[mnumd,mdend]=stmcb(h, nb,na); %IIR filter modelling
%IIR model frequency response:
HM=freqz(mnumd,mdend,F,fs);
plot(F,abs(HM),'k');
title('frequency response'); xlabel('Hz');
subplot(1,2,2)
plot(th,h,'rx'); hold on;
[mh,mth]=impz(mnumd,mdend,64,fs);

5.4 IIR Digital Filters
297
0
50
100
150
0
1
2
3
4
5
6
7
8
frequency
 response
Hz
0
0.05
0.1
0.15
0.2
-0.5
0
0.5
1
1.5
2
2.5
impulse
 response
seconds
Fig. 5.46
Comparison of recursive IIR desired and modelled response, for the stmcb() case
plot(mth,mh,'k');
axis([-0.02 0.25 -0.5 2.5]);
title('impulse response'); xlabel('seconds');
5.4.3
Details of IIR Filters in the MATLAB Signal
Processing Toolbox
5.4.3.1
Generalized Butterworth Digital Filter
The MATLAB Signal Processing Toolbox offers a generalized Butterworth ﬁlter
design, using the function maxﬂat(). The degree of the numerator and denominator
of the IIR ﬁlter can be speciﬁed, and maxﬂat() obtains the IIR maximally ﬂat ﬁlter.
Program 5.40 gives an example of using maxﬂat(). Figure5.47 has been obtained
with this program. On the left hand side of the ﬁgure the ﬂatness of the IIR ﬁlter can
be clearly observed. The pole-zero map of the IIR ﬁlter is shown on the right hand
side.

298
5
Digital Filters
10
0
10
1
10
2
0
0.2
0.4
0.6
0.8
1
Gain
Hf(w) 
Hz.
Real Axis
Imag Axis
-1
-0.5
0
0.5
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
Fig. 5.47 Frequency response and pole-zero map of maxﬂat() ﬁlter example
Program 5.40 Frequency response of IIR maxﬂat ﬁlter
% frequency response of IIR maxflat filter
fs=130; %sampling frequency in Hz.
fc=10; %cut-off at 10 Hz
wc=2*fc/fs; %normalized cut-off frequency (0 to 1)
%degree of the digital IIR filter numerator and
%denominator
Nnum=2; Nden=4;
%filter computation:
[numd,dend]=maxflat(Nnum,Nden,wc);
subplot(1,2,1)
%logaritmic set of frequency values in Hz:
f=logspace(0,2);
G=freqz(numd,dend,f,fs); %computes frequency response
semilogx(f,abs(G),'k'); %plots gain
axis([1 100 0 1.1]);
ylabel('Gain'); title('Hf(w) maxflat filter')
xlabel('Hz.'); grid;
subplot(1,2,2)
theta=0:.1:2*pi;
%draw a circunference:
plot(cos(theta),sin(theta),'--k'); hold on;
fdt=tf(numd,dend);
pzmap(fdt); hold on; %pole-zero map

5.4 IIR Digital Filters
299
Fig. 5.48
Comparison of
ﬁlter() and ﬁltﬁlt() effects
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-1
-0.5
0
0.5
1
1.5
cheby1() and filter() result
seconds
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-1
-0.5
0
0.5
1
1.5
cheby1() and filtfilt() result
seconds
5.4.3.2
A Digital Filter with no Delay
In ﬁeld applications, ﬁlters work usually in real time, for instance to improve the
sound of a telephone. However the study with MATLAB is frequently done off-line.
In this case the complete u(n) input sequence is at our disposal. Taking advantage of
this fact, the function ﬁltﬁlt() can compute the output of a ﬁlter using both past and
future input data, obtaining no delay. This is contrast with the delay inherent to the
use of ﬁlter().
Figure5.48 compares the effect of ﬁlter() and the effect of ﬁltﬁlt() using as input
example a sawtooth signal ﬁltered with a Chebyshev1 IIR ﬁlter. The ﬁgure has been
made with the Program 5.41. On top of the ﬁgure the delay of the ﬁlter() output with
respect to the input can be clearly observed. At the bottom of the ﬁgure, where the
effect of ﬁltﬁlt() is depicted, there is no delay.
Program 5.41 Comparing ﬁlter() with ﬁltﬁlt()
% comparing filter with filtfilt
fs=130; %sampling frequency in Hz.
fc=10; %cut-off at 10 Hz
wc=2*fc/fs; %normalized cut-off frequency (0 to 1)
% a chebyshev1 IIR filter
N=6; %order of the filter
R=0.5; %ripple in the passband
[numd,dend]=cheby1(N,R,wc); %filter computation
%sawtooth input signal
fu=8; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
tiv=1/fs; %time intervals between samples
t=0:tiv:(0.5-tiv); %time intervals set (0.5 seconds)
u=sawtooth(wu*t); %sawtooth signal
subplot(2,1,1)

300
5
Digital Filters
y=filter(numd,dend,u); %filter output
plot(t,u,'r'); hold on
plot(t,y,'k');
title('cheby1() and filter() result');
xlabel('seconds')
subplot(2,1,2)
z=filtfilt(numd,dend,u); %filtfilt output
plot(t,u,'r'); hold on
plot(t,z,'k');
title('cheby1() and filtfilt() result');
xlabel('seconds')
5.4.3.3
Special Filters
Both remez() and cremez() functions have options for the design of Hilbert ﬁlters
and differentiator ﬁlters. MATLAB recommends not to use ﬁltﬁlt() to compute the
effect of these ﬁlters.
The Hilbert ﬁlter is related with the Hilbert transform (there is a function, de-
noted hilbert(), which computes this transform). The Hilbert ﬁlter, also denoted as
quadrature ﬁlter, shifts 90◦the phase of the input signal, so if for example the in-
put is a cosine the output is a sine. Suppose that u(t) is the signal input, and ˆu(t)
is the ﬁlter output, so ˆu(t) is just u(t) shifted 90◦; the Hilbert transform of u(t) is
g+(t) = u(t) + j ˆu(t). It is clear how the ﬁlter can be used to obtain the Hilbert trans-
form. There are important applications of the Hilbert transform that will be studied
in the next chapter.
Figure5.49 shows the response of the Hilbert ﬁlter to a sinusoidal input. The ﬁgure
has been generated with the Program 5.43, which makes use of the function remez()
with the option ‘Hilbert’. The input curve has x-marks. Notice how, after the ﬁlter
transient, the output is shifted 90◦with respect to the input.
The ﬁlter has amplitude=1 along all its bandwidth. This has been taken into
account in the Program 5.42. Also, the ﬁlter has a sharp transition at 0Hz so the
bandwidth speciﬁed in the Program 5.42 avoids it.
Program 5.42 Effect of Hilbert ﬁlter
% effect of hilbert filter
fs=130; %sampling frequency in Hz.
N=50; %even order
F=[0.01 1]; %specification of frequency band
A=[1 1]; %specification of amplitudes
%transfer function numerator:
numd=remez(N,F,A,'hilbert');
dend=[1]; %transfer function denominator
fu=5; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
tiv=1/fs; %time intervals between samples
t=0:tiv:(1-tiv); %time intervals set (1 seconds)
u=sin(wu*t); %signal data set
y=filter(numd,dend,u); %response of the filter

5.4 IIR Digital Filters
301
plot(t,u,'-xr'); hold on;
plot(t,real(y),'k')
axis([0 1 -1.2 1.2]);
title('response of Hilbert filter');
xlabel('seconds')
Figure5.50 shows the response of a differentiator ﬁlter to a square wave input.
The ﬁgure has been generated with the Program 5.43, using remez() with the option
‘differentiator’. After the ﬁlter transient, the output of the ﬁlter shows the peaks
corresponding to the high values of signal derivative in the transitions of the square
wave; this is the expected effect of a differentiator.
Fig. 5.49
Input (with
x-marks) and output of
Hilbert ﬁlter
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.5
0
0.5
1
1.5
response of Hilbert filter
seconds
Fig. 5.50
Input (with
x-marks) and output of a
differentiator
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.5
0
0.5
1
1.5
response of a differentiator
seconds

302
5
Digital Filters
Program 5.43 Effect of a differentiator
% effect of a differentiator
fs=130; %sampling frequency in Hz.
N=50; %even order
F=[0.01 1]; %specification of frequency band
A=[0.01 1]; %specification of amplitudes
%transfer function numerator:
numd=remez(N,F,A,'differentiator');
dend=[1]; %transfer function denominator
fu=5; %signal frequency in Hz
wu=2*pi*fu; %signal frequency in rad/s
tiv=1/fs; %time intervals between samples
t=0:tiv:(1-tiv); %time intervals set (1 seconds)
u=square(wu*t); %signal data set
y=filter(numd,dend,u); %filter response
plot(t,u,'-xr'); hold on;
plot(t,real(y),'k')
axis([0 1 -1.2 1.2]);
title('response of a differentiator');
xlabel('seconds');
5.5
Experiments
In the ﬁrst experiment, a signal y(t) is made by adding three sinusoidal signals with
three different frequencies, then the three components are extracted from y(t) using
IIR digital ﬁlters, and ﬁnally y(t) is reconstructed adding the responses of the three
ﬁlters. The function ﬁltﬁlt() is used to get no delay.
In the second experiment two wav ﬁles with two piano notes are used as desired
IIR impulse responses. With these ‘responses’ and using stmcb() IIR models were
obtained. The experiment let us hear the original and the modelled sounds.
5.5.1
Adding and Extracting Signals
As in the previous chapter, in Sect.4.7.2., let us take the following signal:
y(t) = sin(ωt) + 0.5 sin(3ωt) + 0.3 sin(5ωt)
(5.45)
We want to extract from y(t) the three sinusoidal components of the signal. In
Sect.4.7.2. analog ﬁlters were used for this purpose, and signal reconstruction prob-
lems appeared because of ﬁltering delays. Now, digital ﬁlters are used, and the ﬁlters
response is computed using ﬁltﬁlt(), to get no delays. This has been done with the
Program B.7, using three 5th Butterworth ﬁlters. One of the ﬁlters is low-pass, the
other is band-pass and the third is high-pass. Slight changes in the corner frequencies,

5.5 Experiments
303
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
0
2
compound 
signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y0
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-0.5
0
0.5
y5
seconds
Fig. 5.51
Extracting components from a compound signal
with respect to the corner frequencies in the Program B.5, have been introduced for
better results. Figure5.51 shows the response of the three ﬁlters, in extracting the
three harmonics of the input signal (represented on top of the ﬁgure).
After the extraction of the three components, we add them trying to get a recon-
struction of y(t). Figure5.52 shows the result: y(t) is on top and the reconstructed
signal is below. Now the signals are similar since all three ﬁlters have no delay.
The Program B.7 has been included in Appendix B.
5.5.2
Modelling a Piano Note
From Internet two wav ﬁles were downloaded, with the sound of two piano notes.
Using wavread() the Program 5.44 obtains data ﬁles of these sounds. Now, let us
suppose these ﬁles represent the impulse response of two IIR ﬁlters. By using for
instance stmcb() we could try to model these sounds, that is: to obtain IIR transfer
functions with a similar impulse response.
The computation process takes time, patience. To alleviate this effort, the input
data set is reduced by 1/3, using the function decimate() that according with our
speciﬁcation(R = 3)takesoneofeverythreesamples.Figure5.53showstheimpulse
response of the IIR models we obtained.

304
5
Digital Filters
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
ysum
seconds
Fig. 5.52
Original and reconstructed signals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
-0.015
-0.01
-0.005
0
0.005
0.01
G6 model
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-0.1
-0.05
0
0.05
0.1
C6 model
Fig. 5.53
IIR models of G6 and C6 piano notes

5.5 Experiments
305
Program 5.44 Piano note modelling, using stmcb
%Piano note modelling, using stmcb
[y1,fs1]=wavread('piano-G6.wav'); %read wav file
R=3;
y1r=decimate(y1,R); %decimate the audio signal
soundsc(y1r);
[y2,fs2]=wavread('piano-C6.wav'); %read wav file
R=3;
y2r=decimate(y2,R); %decimate the audio signal
soundsc(y2r);
disp('computing G6 model');
% y1r is considered as the impulse response
% of a IIR filter
% let us get a model of this "filter"
na=40; %IIR denominator degree
nb=40; %IIR numerator degree
[mnumd,mdend]=stmcb(y1r,nb,na); %IIR filter modelling
%impulse response of the IIR model:
[h1,th1]=impz(mnumd,mdend,length(y1r),fs1/R);
subplot(2,1,1); plot(th1,h1); title('G6 model');
soundsc(h1); %hearing the impulse response
disp('computing C6 model');
% y2r is considered as the impulse
% response of a IIR filter
% let us get a model of this "filter"
na=40; %IIR denominator degree
nb=40; %IIR numerator degree
[mnumd,mdend]=stmcb(y2r,nb,na); %IIR filter modelling
%impulse response of the IIR model:
[h2,th2]=impz(mnumd,mdend,length(y2r),fs2/R);
subplot(2,1,2); plot(th2,h2); title('C6 model');
soundsc(h2); %hearing the impulse response
pause(1);
for n=1:5,
soundsc(h1); %hearing the impulse response
pause(0.4);
soundsc(h2); %hearing the impulse response
pause(0.4);
end
5.6
A Quick Introduction to the FDATool
The MATLAB SPT includes a very convenient interactive tool for the design and
analysis of ﬁlters. The target of this section is to introduce this tool.
Once MATLAB is working, you enter:
>>fdatool
An initial screen will appear, as shown in Fig.5.54.

306
5
Digital Filters
Fig. 5.54
FDATool initial screen
Notice that the default settings of the initial screen are:
• Response type: Lowpass
• Design Method: FIR Equiripple
• Filter Order: Minimum order
• Frequency Speciﬁcations: Hz, Fs=48000, Fpass=9600, Fstop=12000
• Magnitude Speciﬁcations: dB, Apass=1; Astop=80
The main panel, corresponding to Filter Speciﬁcations, indeed depicts these de-
fault speciﬁcations.
If you click on the magnitude response button (the 12th button from the left, near
the Help entry), a FIR ﬁlter is designed and the magnitude of its frequency response
is depicted, as shown in Fig.5.55.
By clicking on the 13th, 14th, etc., buttons you can see other aspects of the ﬁlter
design results, like phase response, or the ﬁlter coefﬁcients, etc.
The density factor refers to how accurately the equiripple ﬁlter’s coefﬁcients will
be designed.
Now, if you open the File sub-menu, and you click on Export to Simulink Model,
you will see the screen depicted in Fig.5.56.
Then, you click on Realize Model (bottom of the screen), wait, and a Simulink
panel will appear with a block called ‘Digital Filter’, that is the FIR ﬁlter you de-
signed. This block can be used as part of any Simulink system you wish to build and
test.

5.6 A Quick Introduction to the FDATool
307
Fig. 5.55
Result of the defaults FIR ﬁlter design
Fig. 5.56
Prepare for export to simulink model

308
5
Digital Filters
You can choose several types of FIR ﬁlters: equiripple, least-squares, maximally
ﬂat, etc. In case of windowed FIR ﬁlter, it can be triangular, Hamming, Hann, Bartlett,
Kaiser, etc.
In addition, you can choose several types of IIR ﬁlters: Butterworth, Chebyshev
I, Chebyshev II, elliptic, etc.
Other functions, like the Hilbert transform, the differentiator, the arbitrary group
delay, etc., are also available.
Graph windows can be open to send there the graphs generated by the tool. This
is useful for comparing several ﬁlter designs. Several graphs can be accumulated on
the same window.
The tool can be used in combination with the Filter Design Toolbox, being able
to control quantization aspects involved in the hardware realization of the ﬁlters.
There are many other features, which the reader is invited to explore. The tool is
supported by ample documentation (see also the Appendix A of [14]).
5.7
Resources
5.7.1
MATLAB
5.7.1.1
Toolboxes
• Filter Design Toolbox
http://antikafe-oblaka.ru/online-archive/download-mathworks-ﬁlter-design-tool
box-v42-for-matlab-v75-x64.html
• FIR Toolbox:
http://www.swmath.org/software/5132
• LTPDA-LISA Toolbox:
www.lisa.aei-hannover.de/ltpda/
• ERPLAB Toolbox (Electroencephalography):
http://erpinfo.org/erplab
5.7.1.2
Interactive Tools
• FDATool (Filter Design and Analysis Tool):
www.mathworks.com/help/signal/ref/fdatool.html
5.7.1.3
Matlab Code
• Matlab FIR Filter:
https://developer.mbed.org/handbook/Matlab-FIR-Filter

5.7 Resources
309
• Digital Signal Processing ELEN E4810 (Columbia Univ.):
https://www.ee.columbia.edu/~dpwe/e4810/matscripts.html/
5.7.2
Web Sites
• Introduction to Digital Filters (Stanford Univ.):
https://ccrma.stanford.edu/~jos/ﬁlters/
• FIR ﬁlter tools (miniDSP):
http://www.minidsp.com/applications/advanced-tools/ﬁr-ﬁlter-tools
• dspGuru:
http://www.dspguru.com/dsp/links/digital-ﬁlter-design-software
• 101science.com (links):
http://101science.com/dsp.htm
• The Lab Book Pages (FIR ﬁlters):
http://www.labbookpages.co.uk/audio/ﬁrWindowing.html
References
1. J. Bilmes, Filter Design: Impulse Invariance and Bilinear Transform. Lecture Notes, EE518,
UniversityofWashington(2001).http://ssli.ee.washington.edu/courses/ee518/notes/lec16.pdf
2. C.S. Burrus, A.W. Soewito, R.A. Gopinath, Least squared error FIR ﬁlter design with transition
bands. IEEE Trans. Signal Process. 40(6), 1327–1340 (1992)
3. S.Chakraborty,AdvantagesofBlackmanwindowoverHammingwindowmethodfordesigning
FIR ﬁlter. Int. J. Comput. Sci. Eng. Technol. (IJCSET) 4(8), 1181–1189 (2013)
4. E. Cubukcu, Root Raised Cosine (RRC) Filters and Pulse Shaping in Communication Systems
(2012)
5. M.J.L. De Hoon, T.H.J.J. Van der Hagen, H. Schoonewelle, H. Van Dam, Why Yule–Walker
should not be used for autoregressive modeling. Ann. Nuclear Energy 23(15), 1219–1228
(1996)
6. L. Deneire, FIR Filter Approximations. Lecture Presentation, Univ. Nice Sophia Antipols
(2010). http://www.i3s.unice.fr/~deneire/ﬁlt_cours_4.pdf
7. J.A. Dickerson, Signal Modeling. Lecture Notes, EE524, Iowa State Univ. (2006). http://home.
engineering.iastate.edu/~julied/classes/ee524/LectureNotes/l7b.pdf
8. F. Espic, A Survey about IFIR Filters and Their More Recent Improvements (2010). http://
felipeespic.com/depot/docs/DSP_CS1.pdf
9. B. Friedlander, B. Porat, The modiﬁed Yule–Walker method of ARMA spectral estimation.
IEEE Trans. Aerosp. Electr. Syst. 20(2), 158–173 (1984)
10. H.A. Gaberson, A comprehensive windows tutorial. Sound Vibration 40(3), 14–23 (2006)
11. K. Gentile, The care and feeding of digital pulse-shaping ﬁlters. RF Design 25(4), 50–58 (2002)
12. G. Heinzel, A. Rüdiger, R. Schilling, Spectrum and Spectral Density Estimation by the Discrete
Fourier Transform (DFT), Including a Comprehensive List of Window Functions and Some New
At-top Windows (2002). http://www.holometer.fnal.gov/GH_FFT.pdf
13. P.Kabal,Timewindowsforlinearpredictionofspeech.Technicalreport,Electrical&Computer
Engineering, McGill University (2003)
14. S.M. Kuo, W.-S.S. Gan, Digital Signal Processors: Architectures, Implementations, and Ap-
plications (Prentice Hall, Upper Saddle River, 2004)

310
5
Digital Filters
15. R.A. Losada, Practical FIR Filter Design in MATLAB (2003). http://in.mathworks.com/
matlabcentral/fx_ﬁles/3216/1/ﬁrdesign.pdf
16. P. Lynch, The Dolph–Chebyshev window: a simple optimal ﬁlter. Mon. Weather Rev. 125(4),
655–660 (1997)
17. J.H. McClellan, T.W. Parks, A personal history of the Parks–McClellan algorithm. IEEE Signal
Process. Mag. 22(2), 82–86 (2005)
18. A. Mehrnia, A.N. Willson Jr., On optimal iﬁr ﬁlter design, in Proceedings of IEEE International
Symposium Circuits and Systems, ISCAS’04, vol. 3, pp. 133–136 (2004)
19. Y. Neuvo, D. Cheng-Yu, S.K. Mitra, Interpolated ﬁnite impulse response ﬁlters. IEEE Trans.
Acoust. Speech Signal Process. 32(3), 563–570 (1984)
20. S.J. Orfanidis, Optimum Signal Processing: An Introduction (Collier Macmillan, London,
1988)
21. E. Punskaya, Design of FIR Filters. University of Columbia (2005). http://www.sigproc.eng.
cam.ac.uk/~op205
22. M.A. Samad, A novel window function yielding suppressed mainlobe width and minimum
sidelobe peak (2012). arXiv:1205.1618
23. T. Saramaki, Finite impulse response ﬁlter design, in Handbook for Digital Signal Processing,
ed. by S.K. Mitra, J.F. Kaiser (Wiley, New York, 1993), pp. 155–278
24. R.W. Schafer, What is a Savitzky–Golay ﬁlter? (Lecture Notes). IEEE Signal Process. Mag.
28(4), 111–117 (2011)
25. I. Selesnick, The Remez Algorithm. Lecture Notes, EL 713, NYU Polytechnic School of Engi-
neering (2011). http://eeweb.poly.edu/iselesni/EL713/index.html
26. S. Singh, Prony Analysis (2007). http://www.engr.uconn.edu/~sas03013/docs/PronyAnalysis.
pdf
27. K. Steiglitz, L.E. McBride, A technique for the identiﬁcation of linear systems. IEEE Trans.
Autom. Control 10(4), 461–464 (1965)
28. P. Stoica, T. Soderstrom, The Steiglitz–McBride identiﬁcation algorithm revisited-convergence
analysis and accuracy aspects. IEEE Trans. Autom. Control 26(3), 712–717 (1981)
29. K. Vos, A Fast Implementation of Burg’s Method (2013). https://opus-codec.org/docs/vos_
fastburg.pdf

Part III
Non-stationary Signals

Chapter 6
Signal Changes
6.1
Introduction
Signalchangesalongtimecanhaveameaning.Therearemanyelectronicinstruments
made for the monitoring of signal changes. For instance, the monitoring of heart
pace and other biomedical signals, the detection of intruders by means of infrared
sensors, etc. But there is not only interest from data acquisition applications; humans
do use signal changes along time for communication purposes, through modulation
techniques. Needless to say how important communications and data acquisition are
nowadays.
This chapter is devoted to signal changes along time, from the point of view
of data acquisition and processing. The intention of the chapter is to awake some
curiosity, in order to prepare for the next chapter, which introduces a series of analysis
methodologies for the study of non-stationary signals.
Along this chapter and the next one, the Hilbert transform will frequently appear.
Suppose we are measuring a signal y = Acos ωt, the Hilbert transform of y(t) is:
g(t) = A cos ωt + j A sin ωt = e jω t
(6.1)
Note that:
|g(t)| = A ,
d |g(t)|
dt
= A ω
(6.2)
In consequence, through g(t) we can determine the instantaneous amplitude and
frequency of a measured signal.
See [18, 21, 24, 52] for background information on the Hilbert transform. A basic
discrete version is introduced in [20]. For more insight it would be recommended to
read [31] and, for a generalization (monogenic signals), the article [14].
The MATLAB SPT hilbert() function provides an approximation to the Hilbert
transform. It would be useful in the frequency ranges where the approximation is
good enough.
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_6
313

314
6
Signal Changes
Indeed the Fourier transform will be extensively used in this chapter. Let us write
again (recall Eq.5.10) the expression of the Fourier transform of a signal y(t).
Y(ω) =
∞

−∞
y(t) e−jω t dt
(6.3)
Y(ω) is also known as the spectral density function of y(t). It should be represented
versus ω, with ω taking negative and positive values. In many cases there is a mirror
symmetry with respect to the vertical axis, so it is sufﬁcient to represent Y(ω) versus
ω, with ω taking only positive values. Also, for short, Y(ω) is denoted simply as the
spectrum of y(t).
Some books that would be recommended for this chapter are [4] on biomedical
signals, [48] on spectral analysis and [34] as general background using MATLAB.
6.2
Changes in Sinusoidal Signals
From the perspective of the Fourier decomposition of signals into sinusoids, the
sinusoidal signals are of fundamental interest.
Consider a generic sinusoidal signal:
y(t) = A sin(ω t + α)
(6.4)
The three parameters that can change along time are A, ω and α.
6.2.1
Changes in Amplitude
Changes in amplitude A results in certain shapes of the signal envelope.
For instance, a typical envelope shape is the exponential decay, where A =
exp(−Kt). Figure6.1 shows an example. The ﬁgure has been obtained with the
Program 6.1.
Program 6.1 Sine signal with decay
%
Sine
signal
with
decay
fy =40;
% signal
frequency
in
Hz
wy =2* pi*fy;
% signal
frequency
in
rad/s
fs =2000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(1 - tiv );
% time
intervals
set
(1
seconds )
KT =3;
% decay
constant
y=exp(-KT*t).* sin(wy*t);
% signal
data
set
plot (t,y,'k');
% plots
figure
axis ([0
1
-1.1
1.1 ]);
xlabel ('seconds');
title ('sine
signal
with
decay');

6.2 Changes in Sinusoidal Signals
315
Fig. 6.1 Sine signal with
decay
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds
Fig. 6.2 Audio sine signal
with decay
0
0.5
1
1.5
2
2.5
3
3.5
4
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
In this chapter, examples with sound will be frequently included, since it helps
for intuitive feeling. One of these examples is associated with Fig.6.2, which has the
same kind of pattern of the previous ﬁgure: an exponential decay. As it can be easily
noticed in the Program 6.2, the changes with respect to Program 6.1 are just about
sound (the MATLAB function sound()).
Program 6.2 Sound of a sine signal with decay
%
Sound
of
a
sine
signal
with
decay
fy =500;
% signal
frequency
in
Hz
wy =2* pi*fy;
% signal
frequency
in
rad/s
fs =5000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;

316
6
Signal Changes
t =0: tiv :(4 - tiv );
% time
intervals
set
(4
seconds )
KT =0 .7;
% decay
constant
y=exp(-KT*t).* sin(wy*t);
% signal
data
set
sound (y,fs );
% sound
plot (t,y,'g');
% plots
figure
axis ([0
4
-1.5
1.5 ]);
xlabel ('seconds');
title ('sound
of
sine
signal
with
decay');
When running the program and hearing the sound, it makes think about a sound
source that is moving away.
Often the analysis of measured signals pays special attention to the signal
envelopes. One of the uses of the Hilbert transform is for obtaining the envelope
of a signal. For example, let us apply the hilbert() function to the sinusoidal signal
with decay depicted in Fig.6.1. The result is shown in Fig.6.3, generated with the
Program 6.3. The function hilbert() makes an approximation to the Hilbert transform,
and the result, as shown in the ﬁgure, is a satisfactory approximation to the envelope
of the signal. This envelope can be easily analyzed to estimate the decay constant of
the signal.
Program 6.3 Hilbert and the envelope of sine signal with decay
% Hilbert
and
the
envelope
of
sine
signal
with
decay
fy =40;
% signal
frequency
in
Hz
wy =2* pi*fy;
% signal
frequency
in
rad/s
fs =2000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(1 - tiv );
% time
intervals
set
(1
seconds )
KT =3;
% decay
constant
y=exp(-KT*t).* sin(wy*t);
% signal
data
set
g= hilbert (y);
% Hilbert
transform
of
y
m=abs(g);
% complex
modulus
plot (t,y,'b');
hold
on;
% plots
figure
Fig. 6.3 Envelope of the
sine signal with decay
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds

6.2 Changes in Sinusoidal Signals
317
plot (t,m,'k',t,-m,'k');
% plots
envelope
axis ([0
1
-1.1
1.1 ]);
xlabel ('seconds');
title ('envelope
of
sine
signal
with
decay');
6.2.2
Changes in Frequency
Changes in signal frequency ω are also of interest. For instance it may be caused by
the Doppler effect. There are radars, ﬂow meters, speed meters, and other instruments
that use this effect. Likewise, the measurement of red shift - which is an example of
Doppler effect- of stars and galaxies is very important.
Figure6.4, generated with the Program 6.4, shows an example of sinusoidal signal
with frequency variation. The program uses vco(), which is a voltage-controlled
oscillator.
Program 6.4 Sine signal with frequency variation
%
Sine
signal
with
frequency
variation
fy =15;
% signal
central
frequency
in
Hz
fs =2000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(1 - tiv );
% time
intervals
set
(1
seconds )
x =(2* t) -1;
% frequency
control
ramp
(-1
to
1)
y=vco(x,fy ,fs );
% signal
data
set
plot (t,y,'k');
% plots
figure
axis ([0
1
-1.1
1.1 ]);
xlabel ('seconds');
title ('sine
signal
with
frequency
variation');
Fig. 6.4 Sine signal with
frequency variation
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds

318
6
Signal Changes
Fig. 6.5 Audio sine signal
with frequency variation
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds
Let us again have an audio version. A sound with increasing frequency is generated
by the Program 6.5. The Fig.6.5, also generated by the Program 6.5, shows the audio
signal.
Program 6.5 Sound of sine signal with frequency variation
%
Sound
of
sine
signal
with
frequency
variation
fy =600;
% signal
central
frequency
in
Hz
fs =6000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(4 - tiv );
% time
intervals
set
(4
seconds )
x=(t/2) -1;
% frequency
control
ramp
(-1
to
1)
y=vco(x,fy ,fs );
% signal
data
set
sound (y,fs );
% sound
plot (t,y,'k');
% plots
figure
axis ([0
0.5
-1.1
1.1 ]);
xlabel ('seconds');
title ('sound
of
sine
signal
with
frequency
variation');
The derivative of the Hilbert transform can also be used to estimate the instanta-
neous frequency of the signal.
The Program 6.6 uses hilbert( ) to compute the instantaneous frequency of the
signal in Fig.6.4 (it can be also applied, in the same way, for the audio signal in
Fig.6.5).
Figure6.6, made with the Program 6.6, shows on top the signal having frequency
variation, and below the instantaneous frequency. Although there are some diver-
gences in the corners of the frequency range, most of the plot offers good information
on the frequency variation of the signal along time.

6.2 Changes in Sinusoidal Signals
319
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.5
0
0.5
1
seconds
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
10
20
30
seconds
frequency of the sine signal in Hz
Fig. 6.6 Sine signal with frequency variation
Program 6.6 Hilbert and the frequency of sine signal with frequency variation
%
Hilbert
and
the
frequency
of
sine
signal
%
with
frequency
variation
fy =15;
% signal
central
frequency
in
Hz
fs =2000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(1 - tiv );
% time
intervals
set
(1
seconds )
x =(2* t) -1;
% frequency
control
ramp
y=vco(x,fy ,fs );
% signal
data
set
g= hilbert (y);
% Hilbert
transform
of
y
dg= diff (g)/ tiv;
% aprox.
derivative
w=abs(dg );
% frequency
in
rad/s
v=w /(2* pi );
%to
Hz
subplot (2 ,1 ,1)
plot (t,y,'k');
% plots
the
signal
axis ([0
1
-1.1
1.1 ]);
xlabel ('seconds')
title ('sine
with
frequency
variation');
subplot (2 ,1 ,2)
plot (t(2: fs),v,'k');
% plots
frequency
axis ([0
1
0
30]);
xlabel ('seconds');
title ('frequency
of
the
sine
signal
in
Hz');
In the next ﬁgure, the spectrogram will be used to estimate the evolution of signal
frequency along time. The spectrogram makes use of successive windowed Fourier
analysis. The window moves along several positions of the signal time.

320
6
Signal Changes
Fig. 6.7 Spectrogram of the
sine signal with frequency
variation
Time
Frequency
0
1
2
3
4
5
6
7
8
0
10
20
30
40
50
60
70
Program 6.7 Spectrogram of sine signal with frequency variation
%
Spectrogram
of
sine
signal
with
frequency
variation
fy =30;
% signal
central
frequency
in
Hz
fs =500;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(10 - tiv );
% time
intervals
set
(10
seconds )
x =((2* t )/10) -1;
% frequency
control
ramp
(-1
to
1)
y=vco(x,fy ,fs );
% signal
data
set
specgram (y ,256 , fs );
% plots
spectrogram
axis ([0
8
0
70]);
title ('spectrogram
of
the
sine
signal
with
frequency
variation');
Figure6.7, generated with the Program 6.7, shows an spectrogram of a sinusoidal
signal having frequency variation. The successive positions of the Fourier analysis
windows can be observed. The ﬁgure shows a ramp, corresponding to the linear
increase of frequency along time in this example.
In general the spectrogram is intended for complicated signals. It provides a tool
for time-frequency studies. Usually the spectrogram has some fuzzyness, which is
due to the uncertainty principle in signal processing: this will be a topic to be treated
in the next chapter.
6.3
Two Analytical Tools
The changes of signals along time call for analytic tools capable to move in a
joint time-frequency domain. Currently this is the objective of active and successful
research. As it has been just commented, the spectrogram is one of these tools. Let

6.3 Two Analytical Tools
321
us now introduce some other interesting techniques. Later on, in the next chapter,
this topic will be extended.
6.3.1
Cepstral Analysis
Seismic records may contain a mix of main waves and echoes. It is convenient to
detect this fact, and try to extract and separate all components. In the 1960s Bogert
et al. introduced the power cepstrum as a technique for ﬁnding echo arrival times in
a composite signal, [6].
Consider the simple case of a signal y(t) and one echo of this signal, the composite
signal z(t) is:
z(t) = y(t) + β y(t −τ)
(6.5)
The power spectrum of a signal is the square of the Fourier transform of the signal.
Let us denote as Z(ω) the Fourier transform of z(t) and Y(ω) the Fourier transform
of y(t). Then:
|Z(ω)|2 = |Y(ω)|2 
1 + β2 + 2β cos (ω τ)

(6.6)
The power spectrum of the composite signal has a sinusoidal envelope.
Taking logarithms, products are converted to sums. We obtain:
M(ω) = log |Z(ω)|2 = log |Y(ω)|2 + log

1 + β2 + 2β cos (ω τ)

(6.7)
Now M(ω) can be seen as a “signal” with some periodicity that can be analyzed with
its spectrum. In consequence we obtain the Fourier transform of M(ω). This is the
power cepstrum CP(q).
Bogert et al. [6], introduced several new words to describe the new technique.
For instance, cepstrum comes from spectrum by interchanging consonants. Also the
dominium q of CP(q) was called quefrency, and there are rahmonics, and liftering
(for ﬁltering).
Let us put an example. Trying to resemble a seismic record, a colored noise is
generated, and an echo is obtained with a simple delay of 0.6s. Figure6.8 obtained
with the Program 6.8, shows the two signals: the main signal and the echo.
Program 6.8 Coloured noise and echo
%
Coloured
noise
and
echo
Td =0 .6;
% time
delay
in
seconds
% input
signal
fs =30;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
tu =0: tiv :(39 - tiv );
% time
intervals
set
(39
seconds )
Nu= length (tu );
% number
of
data
points
u= randn (Nu ,1);
% random
input
signal
data
set
[fnum , fden ]= butter (2 ,0 .4 );
%low - pass
filter

322
6
Signal Changes
ur= filtfilt (fnum ,fden ,u);
% noise
filtering
% echo
signal
NTd=Td*fs;
% number
of
samples
along
Td
Ny=Nu+NTd;
yr= zeros (1, Ny );
yr (( NTd +1): Ny )= ur (1: Nu );
%y
is
u
delayed
Td
seconds
% time
intervals
set
(39+ Td
seconds ):
ty =0: tiv :(39+ Td - tiv );
% signal
adding
z=ur +(0 .7*yr (1: Nu))';
subplot (2 ,1 ,1)
plot (tu ,ur ,'k');
% plots
input
signal
title ('coloured
noise
signals');
ylabel ('input');
subplot (2 ,1 ,2)
plot (ty ,yr ,'k');
% plots
echo
signal
ylabel ('echo
signal');
xlabel ('seconds');
A composite signal is obtained by adding the two signals shown in Fig.6.8.
UsingtheProgram6.9,theFouriertransformsofthemainsignalandthecomposite
signal are obtained. Figure6.9 shows the results: on top the Fourier transform of the
main signal, and at the bottom the Fourier transform of the composite signal. Notice
the sinusoidal oscillation in this last transform.
0
5
10
15
20
25
30
35
40
-2
-1
0
1
2
input
0
5
10
15
20
25
30
35
40
-2
-1
0
1
2
echo signal
seconds
Fig. 6.8 Coloured noise signals; the signal below is a delayed version of the signal on top

6.3 Two Analytical Tools
323
0
5
10
15
0
20
40
60
80
100
input
0
5
10
15
0
50
100
150
200
composite signal
Hz
Fig. 6.9 Fourier transforms of main signal and composite signal
Program 6.9 Spectra of input and composite signals
%
Spectra
of
input
and
composite
signals
Td =0 .6;
% time
delay
in
seconds
% input
signal
fs =30;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
tu =0: tiv :(39 - tiv );
% time
intervals
set
(39
seconds )
Nu= length (tu );
% number
of
data
points
u= randn (Nu ,1);
% random
input
signal
data
set
[fnum , fden ]= butter (2 ,0 .4 );
%low - pass
filter
ur= filtfilt (fnum ,fden ,u);
% noise
filtering
% echo
signal
NTd=Td*fs;
% number
of
samples
along
Td
Ny=Nu+NTd;
yr= zeros (1, Ny );
yr (( NTd +1): Ny )= ur (1: Nu );
%y
is
u
delayed
Td
seconds
% signal
adding
z=ur +(0 .7*yr (1: Nu))';
ifr=fs/Nu;
% frequency
interval
fr =0: ifr :(( fs /2) - ifr );
% frequencies
data
set
subplot (2 ,1 ,1)
sur=fft(ur );
% plot
input
spectrum :
plot (fr , abs( sur (1:( Nu /2))) ,'k');
title ('spectra');
ylabel ('input');
subplot (2 ,1 ,2)
sz= fft(z);
% plot
composite
signal
spectrum :
plot (fr , abs(sz (1:( Nu /2))) ,'k');
ylabel ('composite
signal');
xlabel ('Hz');

324
6
Signal Changes
0
5
10
15
20
25
30
35
40
-4
-2
0
2
4
composite signal
0
2
4
6
8
10
12
14
16
18
20
-0.5
0
0.5
1
1.5
2
real cepstrum
seconds
Fig. 6.10 The composite signal and its real cepstrum
Now let us apply the real cepstrum to the composite signal z(t). This is done
with the Program 6.10, which uses the MATLAB SPT rceps() function. Figure6.10
presents the result. On top the ﬁgure shows the composite signal. At the bottom, the
ﬁgure shows the real cepstrum. There is a spike, that we indicated with an arrow,
telling that there is an echo, with 0.6s delay.
Program 6.10 Composite signal and cepstrum
%
Composite
signal
and
cepstrum
Td =0 .6;
% time
delay
in
seconds
% input
signal
fs =30;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
tu =0: tiv :(39 - tiv );
% time
intervals
set
(39
seconds )
Nu= length (tu );
% number
of
data
points
u= randn (Nu ,1);
% random
input
signal
data
set
[fnum , fden ]= butter (2 ,0 .4 );
%low - pass
filter
ur= filtfilt (fnum ,fden ,u);
% noise
filtering
% echo
signal
NTd=Td*fs;
% number
of
samples
along
Td
Ny=Nu+NTd;
yr= zeros (1, Ny );
yr (( NTd +1): Ny )= ur (1: Nu );
%y
is
u
delayed
Td
seconds
% signal
adding
z=ur +(0 .7*yr (1: Nu))';
ifr=fs/Nu;
% frequency
interval
fr =0: ifr :(( fs /2) - ifr );
% frequencies
data
set
subplot (2 ,1 ,1)
plot (tu ,z,'k');
% composite
signal

6.3 Two Analytical Tools
325
title ('composite
signal
and
its
cepstrum');
ylabel ('composite
signal');
subplot (2 ,1 ,2)
cz= rceps (z);
% real
cepstrum
plot (tu (1:( Nu /2)) , cz (1:( Nu /2)) ,'k');
% plots
cepstrum
ylabel ('cepstrum');
xlabel ('seconds');
The function rceps() computes the real cepstrum in the following way:
cz = real(ifft (log (abs (fft(z) )))))
Also in the 1960s, unrelated to the work of Bogert et al. Oppenheim was devel-
oping an homomorphic ﬁltering concept. The idea was to establish homomorphic
mappings between signal spaces where signals are nonadditively combined (convo-
lution, multiplication), and spaces where signals are added (and easily separated).
An example of homomorphic mapping for two convolved signals is the following
sequence: apply Fourier transform to get a product of transformed signals, apply a
complex logarithm to get a sum, apply inverse Fourier transform to go to quefrency
with the sum of two signals. All steps are with complex numbers, in order to pre-
serve the phase information. In this way, we obtain the complex cepstrum. After
the Oppenheim doctoral thesis on homomorphic systems, Schafer, then a student,
was introduced to Oppenheim and started to collaborate. The doctoral dissertation
of Schafer was on echo removal with the new technique. An account of this history
is given by Oppenheim and Schafer in the article [32]. Another, complementary his-
torical view, more related to cepstral analysis of mechanical vibrations, is described
in [39].
One of the main ways to deal with speech processing and recognition, [2], is to
represent the speech as the convolution of glottal pulses, an excitation signal, and
the vocal tract impulse responses, a ﬁlter. The cepstrum can be applied to separate
the glottal pulse shape from the tract impulse response.
When dealing with the human auditory system’s response, it is better to use the
so called ‘mel scale’ [47]. Based on this scale, a methodology called mel-frequency
cepstrum has been developed, see for instance [25, 29], with important practical
applications, including standards being used in mobile phones or MP3 encoded
music [45].
A classical guide for the use of cepstrum is [10]. Some recent contributions on
this methodology are [30, 40].
Several data bases of speech sounds are available from the web (see the Resources
section). For the next example an ‘I’ vowel, has been taken for illustrative purposes
from one of these data bases. Figure6.11 shows on top a view of the recorded signal,
and a zoomed view of this signal at the bottom. Two main harmonics are clearly
noticed. The ﬁgure can be used to estimate their periods.

326
6
Signal Changes
0
0.1
0.2
0.3
0.4
0.5
0.6
-0.5
0
0.5
signal
seconds
0.114
0.116
0.118
0.12
0.122
0.124
0.126
0.128
0.13
-0.5
0
0.5
signal
seconds
Fig. 6.11 The ‘I’ vowel
Program 6.11 Hear and plot vowel signal
% Hear
and
plot
vowel
signal
[y1 ,fs1 ]= wavread ('i.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
wav
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
figure (1)
subplot (2 ,1 ,1)
plot (t,y1 ,'k');
% plots
the
signal
axis ([0
(Ny* tiv)
-0.8
0.8 ]);
title ('vowel
sound');
ylabel ('signal');
xlabel ('seconds')
subplot (2 ,1 ,2)
ta =5000;
tb =5800;
% plot
a
zoom
on
the
signal :
plot (t(ta:tb),y1(ta:tb),'k');
axis ([( ta* tiv)
(tb*tiv)
-0.8
0.8 ]);
ylabel ('signal');
xlabel ('seconds')
The spectrum of the ‘I’ signal, as depicted in Fig.6.12, conﬁrms the presence of
two main harmonics.

6.3 Two Analytical Tools
327
Fig. 6.12 Spectrum of the
‘I’ signal
0
100
200
300
400
500
600
700
800
900
0
100
200
300
400
500
600
700
800
900
Hz
Program 6.12 Analyse vowel with spectrum
%
Analyse
vowel
with
spectrum
[y1 ,fs1 ]= wavread ('i.wav');
% read
wav
file
Ny= length (y1 );
sz= fft(y1 );
% spectrum
fiv=fs1/Ny;
fz =0: fiv :((0 .5*fs1)-fiv );
% frequency
interval
set
% plot
part
of
the
spectrum :
plot (fz (1:500) , abs(sz (1:500)) ,'k');
title ('vowel
sound
spectrum');
xlabel ('Hz')
Now let us apply the cepstrum. Figure6.13 shows the result. There is a little hill
around 0.0046s, which corresponds to the two harmonics (the glottal pulses), and a
decay proﬁle in the left corner (the impulse response of the vocal tract).
Program 6.13 Analyse vowel with cepstrum
%
Analyse
vowel
with
cepstrum
[y1 ,fs1 ]= wavread ('i.wav');
% read
wav
file
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
cz= rceps (y1 );
% real
cepstrum
% plot
the
signal :
plot (t (1:300) , abs(cz (1:300)) ,'k');
title ('vowel
sound
cepstrum');
xlabel ('seconds');

328
6
Signal Changes
Fig. 6.13 Cepstrum of the
‘I’ signal
0
1
2
3
4
5
6
7
x 10
-3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
seconds
Fig. 6.14 Spectrogram of a
quadratic chirp signal
Time
Frequency
0
2
4
6
8
10
12
14
16
18
0
50
100
150
200
250
300
6.3.2
Chirp Z-Transform
The research on non-stationary signals often take “chirp” signals as archetypical
cases. They are brief signals with a frequency that may increase or decrease along
time. This change of frequency could follow a linear, or quadratic, or any other law.
The signal shown in Fig.6.4 is an example of chirp signal. It has been created
using the MATLAB chirp() function, which generates a swept-frequency cosine
signal. This function has an option to select the way frequency sweeps in function
of time: linear, quadratic, or logarithmic.
Figure6.14 shows the spectrogram of a quadratic chirp signal, as generated by
the Program 6.14.

6.3 Two Analytical Tools
329
Program 6.14 Spectrogram of a chirp
%
Spectrogram
of
a
chirp
f0 =1;
% initial
frequency
in
Hz
f1 =200;
% final
frequency
in
Hz
fs =600;
% sampling
rate
in
Hz
t =0:(1/ fs ):20;
% time
intervals
set
(20
seconds )
t1 =20;
% final
time ;
y
=
chirp (t,f0 ,t1 ,f1 ,'quadratic');
% the
chirp
signal
specgram (y ,256 , fs );
% the
spectrogram
title ('quadratic
chirp
spectrogram');
The word “chirp” is also used to designate a modiﬁcation of the z-transform that
we are going to introduce now. It is called the “chirp z-transform”.
The z-transform of a discrete signal y(n) is given by:
Y(z) =
∞

n = −∞
y(n) z−n
(6.8)
If the signal y(n) is zero for all n < 0, we can write:
Y(z) =
∞

n = 0
y(n) z−n
(6.9)
The Discrete Fourier Transform (DFT) of a discrete signal y(n) with ﬁnite duration
of L samples, is given by:
Y(ω) =
L

n = 0
y(n) e−jω n
(6.10)
Actually the MATLAB fft() function is a DFT.
The DFT can be considered as the evaluation of the z-transform along the unit
circumference exp(jω). In fact, this may be the basis for DFT computation.
The chirp z-transform evaluates the z-transform along a general spiral contour in
the z-plane. The contour starting point can be speciﬁed, and also the length of the
arc. This contour is a “chirp” in the z-plane. The equation of the contour is:
zk = A W k ; k = 0, 1, . . . , M
(6.11)
where A is the starting point and the complex quantity W determines the spiralling
rate; if |W| >1, the contour spirals out, if |W| <1, it spirals in.
Notice that the DFT can be regarded as a particular case of the chirp z-transform;
when the chosen contour is the unit circumference.
The chirp z-transform was introduced in 1969 by Rabiner et al. [35, 36]. In a short
article, [37], Rabiner comments how this transform was originated, with occasion of

330
6
Signal Changes
Fig. 6.15 The result of
adding 10 and 11Hz sine
signals
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
seconds
meeting the right people at the right place. One of the algorithms that can be used
for the computation of the chirp z-transform is due to Bluestein [5].
An interesting aspect is that the chirp z-transform is evaluated in a number of
points along the contour. This number can be speciﬁed for a coarse or ﬁne grain
study. This is a main advantage when it is convenient to focus on certain frequency
regions, like in the case of speech studies. The next example focuses on this feature.
Consider the case of two added sine signals. The frequencies of the two signals
are similar: 10 and 11Hz. Figure6.15, obtained with the Program 6.15, depicts the
composite signal.
Program 6.15 Two added sines
%
Two
added
sines
fs =200;
% sampling
rate
in
Hz
t =0:(1/ fs ):1;
% time
intervals
set
(1
seconds )
f1 =10;
% sine1
frequency
in
Hz
f2 =11;
% sine2
frequency
in
Hz
% sum
of
two
sine
signals :
y=sin (2* pi*f1*t)+ sin (2* pi*f2*t);
Ny= length (y);
plot (t,y,'k')
title ('two
added
sine
signals');
xlabel ('seconds');
As shown on top of the Fig.6.16, the DFT speciﬁed in the program 6.16, cannot
discern the two sine signals. Instead, the chirp z-transform, as speciﬁed in the same
program, applies a zoom in the frequency zone of interest and make clear that there
are two different sine signals.
Notice that Program 6.16 uses the czt( ) function provided by the MATLAB SPT
for the chirp z-transform.

6.3 Two Analytical Tools
331
0
5
10
15
20
25
30
35
40
45
50
0
50
100
150
Fourier transform
Hz
5
10
15
20
25
0
50
100
150
chirp-z transform
Hz
Fig. 6.16 DFT and chirp-z transform of the composite signal
Program 6.16 Chirp-z transform of a signal
%
Chirp -z
transform
of
a
signal
fs =200;
% sampling
rate
in
Hz
t =0:(1/ fs ):1;
% time
intervals
set
(1
seconds )
f1 =10;
% sine1
frequency
in
Hz
f2 =11;
% sine2
frequency
in
Hz
% sum
of
two
sine
signals :
y=sin (2* pi*f1*t)+ sin (2* pi*f2*t);
Ny= length (y);
subplot (2 ,1 ,1)
fy =0:( fs/Ny ): fs;
sy= fft(y);
% the
Fourier
transform
plot (fy (1:50) , abs(sy (1:50)) ,'k');
title ('Fourier
transform');
xlabel ('Hz');
subplot (2 ,1 ,2)
cf1 =5;
cf2 =25;
%in
Hz
m =128;
% number
of
contour
points
% ratio
between
contour
points :
w=exp(-j *(2* pi *( cf2 - cf1 ))/( m*fs ));
a=exp(j *(2* pi* cf1 )/ fs );
% contour
starting
point
chy=czt(y,m,w,a);
% the
chirp -z
transform
fhiv =( cf2 - cf1 )/m;
% frequency
interval
fhy=cf1: fhiv :( cf2 - fhiv );
plot (fhy , abs(chy),'k');
title ('chirp -z
transform');
xlabel ('Hz');

332
6
Signal Changes
For an extended exposition of the chirp z-transform see [13]. A fast computation
method is proposed in [28]. A comparison with the Goertzel algorithm, in terms of
computational cost, is made by [38]. Medical diagnosis applications are described
in [19, 49].
6.4
Some Signal Phenomena
The ﬁeld of signal analysis is wide, with many speciﬁc interests. People involved
in Astronomy, Earthquakes, Medicine, Electronics, etc., look at the signals from
different perspectives. It is the purpose of this section to present some introductory
examples to start an exploration that the reader may continue, according with the
topics of his/her interest.
Since there are changes in the signals, most examples belong to a joint time-
frequency view, so the spectrogram is helpful. Anyway, to put some order the section
begins with spectrum changes, and continues with time domain changes.
6.4.1
Spectrum Shifts
The Doppler effect is a well-known example of spectrum shift. Take the case of a
car blowing his horn, its sound spectrum is mostly constant all the time, but from an
external ﬁxed ear this spectrum shifts as the car approaches and then passes by.
The Program 6.17 lets you hear the Doppler effect corresponding to the car exam-
ple just described. Figure6.17 shows the recorded signal.
Program 6.17 Hear and see car doppler WAV
%
Hear
&
see
car
doppler
WAV
[y1 ,fs1 ]= wavread ('doppler.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
wav
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
plot (t,y1 ,'g');
% plots
the
signal
axis ([0
(Ny* tiv)
-1.2
1.2 ]);
title ('car
doppler
sound');
ylabel ('signal');
xlabel ('seconds')
Let us determine the sound spectrum when the car approaches, and the sound
spectrum when the car goes away. Both spectra are heard from a ﬁxed position, not
in the car. Figure6.18, obtained with the Program 6.18, shows both spectra. It is easy
to compare them and to see that peaks go to the left, to lower frequencies.

6.4 Some Signal Phenomena
333
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
signal
seconds
Fig. 6.17 Car Doppler signal
Program 6.18 Spectral densities of Doppler signal begin and end
% Spectral
densities
of
Doppler
signal
begin
and
end
[y1 ,fs1 ]= wavread ('doppler.wav');
% read
wav
file
Ny= length (y1 );
tiv=R/fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
subplot (2 ,1 ,1)
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0
500
1000
1500
first 1/3 of car Doppler: spectral density
0
200
400
600
800
1000
1200
1400
1600
1800
2000
0
500
1000
1500
2000
2500
last 1/3 of car Doppler: spectral density
Hz
Fig. 6.18 Spectrum shift due to Doppler effect

334
6
Signal Changes
y1beg =y1 (1: Ny /3);
% first
1/3
of
signal
ff1=fft(y1beg , fs1 );
% Fourier
transform
plot ( abs( ff1 (1:2000)) ,'k');
title ('first
1/3
of
car
Doppler :
spectral
density');
subplot (2 ,1 ,2)
y1end =y1 (2* Ny /3: Ny );
% last
1/3
of
signal
ff3=fft(y1end , fs1 );
% Fourier
transform
plot ( abs( ff3 (1:2000)) ,'k');
title ('last
1/3
of
car
Doppler :
spectral
density');
xlabel ('Hz')
To complete the study, Fig.6.19 shows the spectrogram, as computed with the
Program 6.19. The signal has been decimated to get a short data set to be analyzed.
The decimated signal can be heard, to feel how the main information to be analysed
was kept.
Program 6.19 Spectrogram of car doppler signal
% Spectrogram
of
car
doppler
signal
[y1 ,fs1 ]= wavread ('doppler.wav');
% read
wav
file
R =12;
y1r= decimate (y1 ,R);
% decimate
the
audio
signal
soundsc (y1r , fs1/R);
% hear
the
decimated
signal
specgram (y1r ,256 , fs1/R);
% spectrogram
title ('spectrogram
of
1/12
decimated
car
Doppler
signal')
Christian Doppler presented what is now called the Doppler effect in 1842 at a
scientiﬁc meeting in Prague. In this presentation, he predicted that the color of a
star would shift to red if the star moves away from Earth, and would shift to blue if
approaching the Earth. He was presuming that stars only emit pure white light. More
historical details on the Doppler effect and its applications can be found in [27].
From some time ago, astronomers were able to observe the chemical spectrum of
stars, and redshift phenomena have been conﬁrmed, taking as reference the spectra
Fig. 6.19 Spectrogram of
the car Doppler signal
Time
Frequency
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
0
200
400
600
800
1000
1200
1400
1600
1800

6.4 Some Signal Phenomena
335
Fig. 6.20 Spectrogram of
siren signal
Time
Frequency
0
0.2
0.4
0.6
0.8
1
0
1000
2000
3000
4000
5000
6000
7000
8000
of elements commonly found in the Universe. It is interesting to note that redshift
may be due to different causes, not only Doppler [3, 23, 26].
Oneof themethods for measuringvelocityis theDoppler effect. ThereareDoppler
radars being used for trafﬁc, and for weather prediction. Also, there are ﬂowmeters
based on Doppler effect, that can be used in industrial or home applications, or for
monitoring of blood circulation through the heart, arteries, etc., [12, 22, 33, 51].
Apart from the Doppler effect, there are other situations where spectrum shifts
appear. This is the case, for instance, when one applies modulation to a signal. As
example, we selected a type of car alarm.
The Program 6.20 lets you hear the siren, and then it computes the spectrogram,
which is shown in Fig.6.20. The frequency modulating signal can be clearly recog-
nized. Thinking in terms of simple electronic circuits, the signal that modulates in
frequency the sound corresponds to an R-C charge-discharge oscillator.
Program 6.20 Spectrogram of siren signal
% Spectrogram
of
siren
signal
[y1 ,fs1 ]= wavread ('srn.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
the
signal
specgram (y1 ,256 , fs1 );
% spectrogram
title ('spectrogram
of
siren
signal')
6.4.2
Changes in Spectrum Shape
Indeed, the spectrum of a sinusoidal electrical signal is quite simple: only one peak.
However, if the signal goes through a low-quality ampliﬁer, saturation and nonlinear-
ities would change this scenario, and some harmonics would arise. Then, a change
of the spectrum shape occurs, and this reveals the presence of some problems.

336
6
Signal Changes
0
0.5
1
1.5
2
-0.4
-0.2
0
0.2
0.4
signal
seconds
0
0.01
0.02
0.03
0.04
0.05
0.06
-0.2
-0.1
0
0.1
0.2
signal
seconds
Fig. 6.21 Transformer sound
Consider the sound of a transformer. Figure6.21 shows a record of a transformer
sound, with a more detailed zoom view at the bottom. The ﬁgure has been obtained
with the Program 6.21. The program includes a sentence for hearing the sound.
Program 6.21 Hear and see transformer signal
% Hear
&
see
transformer
signal
[y1 ,fs1 ]= wavread ('transformer1.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
wav
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
subplot (2 ,1 ,1)
plot (t,y1 ,'k');
% plots
the
signal
axis ([0
(Ny* tiv)
-0.4
0.4 ]);
title ('transformer
sound');
ylabel ('signal');
xlabel ('seconds')
subplot (2 ,1 ,2)
nyz= ceil (Ny /40);
% zoom
% plot
a
zoom
on
the
signal :
plot (t(1: nyz),y1 (1: nyz),'k');
axis ([0
( nyz*tiv)
-0 .25
0 .25 ]);
ylabel ('signal');
xlabel ('seconds')
The next ﬁgure (Fig.6.22, obtained with the Program 6.22) shows the spectral
density of the transformer sound. A fundamental harmonic corresponding to the AC
frequency is clearly seen. There are other harmonics, which can mean nonlinearities
and energy losses.

6.4 Some Signal Phenomena
337
Fig. 6.22 Spectral density
of the transformer signal
0
50
100
150
200
250
300
350
400
0
500
1000
1500
2000
2500
Hz
Program 6.22 Spectral density of transformer signal
% Spectral
density
of
transformer
signal
[y1 ,fs1 ]= wavread ('transformer1.wav');
% read
wav
file
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
ff1=fft(y1 , fs1 );
% Fourier
transform
plot ( abs( ff1 (1:400)) ,'k');
title ('spectral
density');
xlabel ('Hz')
See [50] for a review of condition assessment of transformers in service. An
overview of transformer monitoring based on frequency response is given by [15].
The online diagnosis of transformer state, based on time-frequency analysis, is
described in [44].
The study of mechanical vibrations has great practical importance. A main aspect
is related with fatigue and the health of machinery and structures. As it can be
observed from specialized journals and periodic meetings, there is a lot of research
activity on vibrations, involving also sound. In this ﬁeld, spectral studies are quite
usual.
For example, there are machines that daily make many holes. This is the case
of printed circuit boards, which have hundreds of narrow holes. The drill degrades
along time and should be substituted after a while. A way to diagnose the drill state
is by analyzing the noise caused by the drill. Here it is important to detect signiﬁcant
changes in the noise spectrum shape. See the reviews of [17, 41] for more details.
In general the spectrum shape and its changes can be used for monitoring, diag-
nosis and recognition purposes.
In other order of things, the spectrum shape is closely connected with the per-
sonality of musical instruments. In particular, we refer here to timbre. The timbre

338
6
Signal Changes
Fig. 6.23 A quack signal
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
signal
seconds
is given by speciﬁc proportions of harmonics in the instrument sound. There are
many kinds of instruments, and in many cases one could speak of spectral evolutions
during each note, [8].
Speaking of personality, animals, like dogs, parrots, sheep, etc., emit peculiar
sounds. In most cases, the spectrum shape of these sounds does change from begin-
ning to end. For example, the familiar ‘quack’ sound of ducks. Program 6.23 let us
hear this sound, and also shows the signal in Fig.6.23.
Program 6.23 Hear and see quack WAV
% Hear
&
see
quack
WAV
[y1 ,fs1 ]= wavread ('duck_quack.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
wav
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
plot (t,y1 ,'k');
% plots
the
signal
axis ([0
(Ny* tiv)
-1.2
1.2 ]);
title ('duck
quack
sound');
ylabel ('signal');
xlabel ('seconds')
Let us see the signal spectrum at the beginning and at the end of the sound.
Figure6.24, obtained with the Program 6.24, shows the result. It is clear that the
shape of the spectrum changes.

6.4 Some Signal Phenomena
339
0
500
1000
1500
2000
2500
0
20
40
60
80
first 1/3 of quack: spectral density
Hz
0
500
1000
1500
2000
2500
0
5
10
15
20
last 1/3 of quack: spectral density
Hz
Fig. 6.24 Changes in the spectrum shape along the quack
Program 6.24 Spectral densities of quack signal begin and end
% Spectral
densities
of
quack
signal
begin
and
end
[y1 ,fs1 ]= wavread ('duck_quack.wav');
% read
wav
file
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
subplot (2 ,1 ,1)
y1beg =y1 (1: Ny /3);
% first
1/3
of
signal
ff1=fft(y1beg , fs1 );
% Fourier
transform
plot ( abs( ff1 (1:2500)) ,'k');
title ('first
1/3
of
quack :
spectral
density');
xlabel ('Hz')
subplot (2 ,1 ,2)
y1end =y1 (2* Ny /3: Ny );
% last
1/3
of
signal
ff3=fft(y1end , fs1 );
% Fourier
transform
plot ( abs( ff3 (1:2500)) ,'k');
title ('last
1/3
of
quack :
spectral
density');
xlabel ('Hz')
We will come back to animal sounds by the end of the chapter. By the way, it
happens that the antarctic minke whale also emits “quacks”, [43].

340
6
Signal Changes
6.4.3
Musical Instruments
Some words about musical instruments; since it is useful for the analysis of signals.
In particular let us focus now on the envelope.
6.4.3.1
Attenuation and Overlapping
When you hammer on a string or a bell, there is an initial louder sound and then an
attenuated prolongation of it. The Program 6.25 allows us to hear a triangle and to
see this signal in the Fig.6.25. There is a long sound attenuation time.
Program 6.25 Triangle signal
% Triangle
signal
[y1 ,fs1 ]= wavread ('triangle1.wav');
% read
wav
file
soundsc (y1 , fs1 );
Ny= length (y1 );
% number
of
signal
samples
tiv =1/ fs1;
% sampling
time
interval
t =0: tiv :((Ny -1)* tiv );
% time
data
set
plot (t,y1 ,'g');
% plots
the
signal
title ('triangle
sound');
xlabel ('seconds')
Now let us look at the Big Ben chime. Figure6.26, obtained with the Program
6.26, is the spectrogram of this chime.
Fig. 6.25 Triangle signal
0
1
2
3
4
5
6
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds

6.4 Some Signal Phenomena
341
Fig. 6.26 Spectrogram of
the Big Ben chime
Time
Frequency
0
2
4
6
8
10
12
14
16
0
200
400
600
800
1000
1200
1400
1600
1800
Program 6.26 Spectrogram of Big-Ben signal
% Spectrogram
of
Big - Ben
signal
% read
wav
file :
[y1 ,fs1 ]= wavread ('chime_big_ben1.wav');
R =3;
y1r= decimate (y1 ,R);
% decimate
the
audio
signal
soundsc (y1r , fs1/R);
% hear
the
decimated
signal
specgram (y1r ,512 , fs1/R);
% spectrogram
title ('spectrogram
of
1/3
decimated
Big - Ben
signal')
Notice in the Fig.6.26 that the distinct notes of the bells along time are clearly
noticed. It is also clear how the harmonics of the ﬁrst note are distributed (in a ﬁrst
vertical band in the ﬁgure). But observe that there is an attenuation of the ﬁrst note
that invades the vertical bands of the following notes, causing overlapping. This
phenomenon can be observed in all the subsequent notes.
Let us insist in the display of notes and attenuations. The Program 6.27 reproduces
the sound of an harp phrase. The phrase is like a musical siren. Figure6.27 shows the
spectrogram of this phrase. Notice that the way to plot the spectrogram is changed
with respect to the previous program; here we use the contour() function for a clearer
distinction of the notes.
Program 6.27 Spectrogram of harp signal
% Spectrogram
of
harp
signal
[y1 ,fs1 ]= wavread ('harp1.wav');
% read
wav
file
% soundsc (y1 , fs1 );
% hear
the
signal
R =2;
y1r= decimate (y1 ,R);
% decimate
the
audio
signal
soundsc (y1r , fs1/R);
% hear
the
decimated
signal
% spectrogram
computation :
[sgy ,fy ,ty ]= specgram (y1r ,512 , fs1/R);
contour (ty ,fy , abs( sgy ));
% plots
the
spectrogram
title ('spectrogram
of
1/2
decimated
harp
signal')
xlabel ('seconds');
ylabel ('Hz');
axis ([0
3
0
2000]);

342
6
Signal Changes
Fig. 6.27 Spectrogram of a
harp phrase
0
0.5
1
1.5
2
2.5
3
0
200
400
600
800
1000
1200
1400
1600
1800
2000
seconds
Hz
6.4.3.2
ADSR Envelope
In the early days of electronic music synthesizers, a way to imitate some character-
istics of conventional musical instruments was to use an ADSR envelope, [42]. The
idea is to generate a base sound, made with a mix of sinusoids to imitate a certain
instrument timbre, and then to impose a speciﬁed amplitude envelope to the sound.
The ADSR envelope has four connected segments: Attack-Decay-Sustain-
Release. The usual case is that the attack amplitude increases along time, while
the decay decreases to a certain level, then it comes a constant or quasi constant sus-
tain, and ﬁnally a release for the note attenuation. The Program 6.28 offers a simple
example. Figure6.28 shows the signal that is generated for a note; notice the ADSR
proﬁle of the envelope. The reader is invited to change this proﬁle. The program
plays three notes.
Program 6.28 ADSR synthesis of audio sine signal
%
ADSR
synthesis
of
audio
sine
signal
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(1 - tiv );
% time
intervals
set
(1
second )
fC =440;
%C
note
in
Hz
fE =659;
%E
note
in
Hz
fG =784;
%G
note
in
Hz
% setting
the
ADSR
envelope :
NA=fs /5;
evA= zeros (1, NA );
% evA
during
0.2
seconds
for
nn =1:NA ,
evA(nn )=2*( nn/NA );
% evA
linear
increase
end
ND=fs /5;
evD= zeros (1, ND );
% evD
during
0.2
seconds
for
nn =1:ND ,
evD(nn )=2 -( nn/ND );
% evD
linear
decrease
end
NS=fs /5;
evS= zeros (1, NS );
% evS
during
0.2
seconds

6.4 Some Signal Phenomena
343
Fig. 6.28 A synthesised
audio signal with ADSR
envelope
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
seconds
for
nn =1:NS ,
evS(nn )=1;
% evS
constant
end
NR =2* fs /5;
evR= zeros (1, NR );
% evR
during
0.4
seconds
for
nn =1:NR ,
evR(nn )=1 -( nn/NR );
% evR
linear
decrease
end
evT =[ evA ,evD ,evS ,evR ];
% the
total
envelope
%C
ADSR
note
yC= evT. * sin (2* pi*fC*t);
%E
ADSR
note
yE= evT. * sin (2* pi*fE*t);
%G
ADSR
note
yG= evT. * sin (2* pi*fG*t);
% playing
the
notes
soundsc (yC ,fs );
pause (0 .5 );
soundsc (yG ,fs );
pause (0 .5 );
soundsc (yE ,fs );
plot (t,yC ,'g');
% plots
the
C
ADSR
signal
axis ([0
1
-2.2
2.2 ]);
xlabel ('seconds');
title ('ADSR
sine
signal');
6.4.4
Changes in Signal Energy
In certain monitoring systems it is important to detect changes in signal ‘sizes’
(variance). For instance this is a ﬁrst aspect of interest while recording earthquakes.
Figure6.29 shows a real recording of an earthquake vertical acceleration signal
(in cm/sec2). The ﬁgure has been obtained with the Program 6.29, which reads an

344
6
Signal Changes
Fig. 6.29 Earthquake
vertical acceleration record
0
5
10
15
20
25
30
-500
-400
-300
-200
-100
0
100
200
300
400
500
seconds
earthquake record. When the earthquake takes place it would be pertinent to look at
the signal size. After, when opportune, scientists, and the reader, may process and
analyse this signal in search of certain hints.
Next chapter will devote more space to earthquakes and related databases.
Program 6.29 Read quake data ﬁle
%
Read
quake
data
file
fer =0;
while
fer ==0 ,
fid2 = fopen ('quake.txt','r');
if
fid2 == -1 ,
disp ('read
error')
else
quake1 = fscanf (fid2 ,'%f
\r\n');
fer =1;
end;
end;
fclose ('all');
Ns= length ( quake1 );
t =0:0 .01 :((Ns -1)*0 .01 );
% sampling
times
% plot
earthquake
vertical
acceleration :
plot (t, quake1 ,'k');
title ('earthquake
vertical
acceleration
(cm/ sec ^2)');
xlabel ('seconds');
As an example, Program 6.30 obtains the spectrum of the part of the quake signal
with more variance. Figure6.30 shows the result.

6.4 Some Signal Phenomena
345
Fig. 6.30 Spectrum of the
main part of quake signal
0
5
10
15
20
25
0
2000
4000
6000
8000
10000
12000
Hz
Program 6.30 Spectrum of central quake signal
% Spectrum
of
central
quake
signal
fer =0;
while
fer ==0 ,
fid2 = fopen ('quake.txt','r');
if
fid2 == -1 ,
disp ('read
error')
else
y1= fscanf (fid2 ,'%f
\r\n');
fer =1;
end;
end;
fclose ('all');
fs1 =100;
%in
Hz
y1central =y1 (300:1100);
% central
quake
signal
ff1=fft( y1central , fs1 );
% Fourier
transform
plot ( abs( ff1 (1:50)) ,'k');
title ('central
quake
signal :
spectral
density');
xlabel ('Hz')
axis ([0
25
0
13000]);
grid ;
6.4.5
Repetitions, Rhythm
Many natural phenomena are periodic or quasi periodic. In certain cases, repetition
with periods in a certain range could be critical. For instance respiration, heartbeat,
and other biorhythms. In the same vein, a typical characteristic of music is rhythm.
A way to detect periodicities in a signal is by means of the autocovariance. As
reference example, consider a 1Hz periodic impulse train, as it is shown on top of
Fig.6.31. The same ﬁgure shows at the bottom the autocovariance of this signal. In
this case the autocovariance says that the impulse repeats every 1s, every 2s, every
3s, etc.

346
6
Signal Changes
0
5
10
15
20
25
0
0.5
1
0
5
10
15
20
25
-10
0
10
20
30
seconds
autocovariance
Fig. 6.31 A periodic impulse train and its autocovariance
Program 6.31 Impulse train autocovariance
%
Impulse
train
autocovariance
fs =200;
% samplig
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples
t =0: tiv :(25 - tiv );
% time
intervals
set
(25
seconds )
Ns =25* fs;
% number
of
signal
samples
(25
seconds )
yp1 =[1 , zeros (1 ,199)];
yp= yp1;
for
nn =1:24 ,
yp= cat (2,yp , yp1 );
end
subplot (2 ,1 ,1)
% signal
plot
plot (t,yp ,'k')
axis ([0
25
-0.2
1.2 ]);
title ('Periodic
impulse
train');
subplot (2 ,1 ,2)
% autocovariance
plot
av= xcov (yp );
% signal
autocovariance
% plot
autocovariance :
plot (t(1: Ns),av(Ns :((2* Ns ) -1)) ,'k');
xlabel ('seconds');
title ('autocovariance');
Figure6.32 shows a real electrocardiogram (ECG) record. It is important to see
how periodic is the heartbeat. The autocovariance of this ECG record is shown at the
bottom of the ﬁgure. It looks similar to the case of the periodic pulse train, conﬁrming
the periodicity of the heartbeat. The ﬁgure has been generated with the Program 6.32
In addition, the program computes and displays in the MATLAB command window,
the heartbeat rate.

6.4 Some Signal Phenomena
347
0
5
10
15
20
25
6
7
8
9
10
normal ECG
0
5
10
15
20
25
-200
0
200
400
600
seconds
autocovariance
Fig. 6.32 An ECG record and its covariance
Program 6.32 Read ECG data ﬁle and compute autocovariance
%
Read
ECG
data
file
and
compute
autocovariance
fs =200;
% samplig
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples
fer =0;
while
fer ==0 ,
fid2 = fopen ('ECGnormal.txt','r');
if
fid2 == -1 ,
disp ('read
error')
else
Wdat = fscanf (fid2 ,'%f
\r\n');
fer =1;
end;
end;
fclose ('all');
Ns= length ( Wdat );
% number
of
signal
samples
t =0: tiv :((Ns -1)* tiv );
% time
intervals
set
subplot (2 ,1 ,1)
% signal
plot
plot (t,Wdat ,'k');
axis ([0
25
6
10]);
title ('normal
ECG');
subplot (2 ,1 ,2)
% autocovariance
plot
av= xcov ( Wdat );
% signal
autocovariance
fiv =1/ fs;
% frequency
interval
between
harmonics
hf =0: fiv :(( fs /2) - fiv );
% set
of
harmonic
frequencies
% plot
autocovariance :
plot (t(1: Ns),av(Ns :((2* Ns ) -1)) ,'k');
xlabel ('seconds');
title ('autocovariance');
axis ([0
25
-300
700]);
% find
heart
beat
frequency :--------------------
% maximum
( not
DC)
in
autocovariance :
[M
K]= max(av (( Ns +10):(2* Ns ) -1));

348
6
Signal Changes
0
5
10
15
20
25
4
6
8
10
12
ECG with problem
0
5
10
15
20
25
-1000
-500
0
500
1000
seconds
autocovariance
Fig. 6.33 ECG record showing problems
TW=hf(K);
% period
corresponding
to
maximum
FW =(1/ TW )*60;
% frequency
of
heart
beat
in
puls / min
nfw= num2str (FW );
% convert
to
string
format
msg =['heart
beat
per
min.
= ',nfw ];
disp ( msg );
% message
After some time, the ECG record from which the ﬁgure above has been depicted
showed clear changes. Figure6.33 depicts what happened. The autocovariance of
this part of the ECG, as shown at the bottom of the ﬁgure, also detects problems with
periodicity.
Program 6.33 Read ECG data ﬁle and compute autocovariance
%
Read
ECG
data
file
and
compute
autocovariance
fs =200;
% samplig
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples
fer =0;
while
fer ==0 ,
fid2 = fopen ('ECGproblem.txt','r');
if
fid2 == -1 ,
disp ('read
error')
else
Wdat = fscanf (fid2 ,'%f
\r\n');
fer =1;
end;
end;
fclose ('all');
Ns= length ( Wdat );
% number
of
signal
samples
t =0: tiv :((Ns -1)* tiv );
% time
intervals
set
subplot (2 ,1 ,1)
% signal
plot
plot (t,Wdat ,'k');

6.4 Some Signal Phenomena
349
axis ([0
25
3
12]);
title ('ECG
with
problem');
subplot (2 ,1 ,2)
% autocovariance
plot
av= xcov ( Wdat );
% signal
autocovariance
% plot
autocovariance :
plot (t(1: Ns),av(Ns :((2* Ns ) -1)) ,'k');
xlabel ('seconds');
title ('autocovariance');
axis ([0
25
-1000
1200]);
6.5
Some Complex Sounds
6.5.1
Animal Sounds
From distance, in a Zoo or perhaps in Africa, you can tell that this is a lion roaring,
or and elephant, etc. Animals utter speciﬁc sounds, like signatures. It is a matter of
current research what is the purpose of these sounds, [11]. Actually, one could speak
of certain sound structural designs, which might obey to speciﬁc targets. Therefore
it seems interesting to include in this subsection some examples of animal songs.
The Program 6.34 reproduces an elephant trumpeting, so it can be heard on your
computer. This program also generates the Fig.6.34 showing the spectrogram of the
elephant sound. Up to six harmonics can be observed all along the sound. Sweet
sounds, like the ﬂute notes, are near sine signals, with almost no harmonics. Strident
sounds come from introducing saturations and corners on a basic sine (this is the
case of square signals). The elephant sound exhibit at least six harmonics, which can
be consistently observed all along the spectrogram; it is clearly a strident trumpet.
Fig. 6.34 Spectrogram of
elephant trumpeting
seconds
Hz
0
0.5
1
1.5
2
2.5
0
500
1000
1500
2000
2500
3000
3500
4000

350
6
Signal Changes
Fig. 6.35 The mooing of a
cow signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
signal
seconds
Program 6.34 Spectrogram of Elephant signal
% Spectrogram
of
Elephant
signal
[y1 ,fs1 ]= wavread ('elephant1.wav');
% read
wav
file
% soundsc (y1 , fs1 );
% hear
the
signal
% spectrogram
computation :
[sgy ,fy ,ty ]= specgram (y1 ,512 , fs1 );
colmap1 ;
colormap ( mapg1 );
% user
colormap
% plot
the
spectrogram :
imagesc (ty ,fy , log10 (1+ abs( sgy )));
axis
xy;
title ('spectrogram
of
elephant
signal')
xlabel ('seconds');
ylabel ('Hz');
axis ([0
2.5
0
4000]);
Notice in the Program 6.34 that we deﬁned a special colormap in order to get
more clear graphics.
The next example is the mooing of a cow. Figure6.35 shows the sound signal,
which has a relatively soft attack. The ﬁgure has been generated with the Program
6.35.
Program 6.35 Hear and see cow WAV
% Hear
&
see
cow
WAV
[y1 ,fs1 ]= wavread ('cow1.wav');
% read
wav
file
soundsc (y1 , fs1 );
% hear
wav
Ny= length (y1 );
tiv =1/ fs1;
t =0: tiv :((Ny -1)* tiv );
% time
intervals
set
plot (t,y1 ,'k');
% plots
the
signal
axis ([0
(Ny* tiv)
-1.2
1.2 ]);
title ('cow
sound');
ylabel ('signal');
xlabel ('seconds')
The spectrogram of the mooing signal is shown in Fig.6.36, which has been
obtained with the Program 6.36. The beginning of the signal follows a pitch rising

6.5 Some Complex Sounds
351
Fig. 6.36 Spectrogram of
cow sound
seconds
Hz
0
0.5
1
1.5
0
500
1000
1500
2000
2500
3000
proﬁle. Then, in a second phase, a strident sustained sound is sent. It makes think
about certain ‘syntax’ of sounds, joining several basic pieces along time.
Program 6.36 Spectrogram of cow signal
% Spectrogram
of
cow
signal
[y1 ,fs1 ]= wavread ('cow1.wav');
% read
wav
file
% soundsc (y1 , fs1 );
% hear
the
signal
% spectrogram
computation :
[sgy ,fy ,ty ]= specgram (y1 ,512 , fs1 );
colmap1 ;
colormap ( mapg1 );
% user
colormap
% plot
the
spectrogram :
imagesc (ty ,fy , log10 (0 .3+ abs(sgy )));
axis
xy;
title ('spectrogram
of
cow
signal')
xlabel ('seconds');
ylabel ('Hz');
axis ([0
1 .57
0
3000]);
Background information on animal communication can be found in the books
[7, 16]. There is a web page on this topic (see the Resources section).
6.5.2
Music
Music has many aspects of interest from the time-frequency perspective. Let us just
write some remarks that could be relevant in certain signal analysis scenarios.
In a score there are bars indicating time intervals and rhythm. It is not necessary
repetitions of the signal to recognize rhythm.
Musical notes are events with duration. Time is divided into equal parts, and parts
are assigned to groups of notes.
In the time intervals peculiar grouping of notes may be noticed.

352
6
Signal Changes
There are complete sections of a melody that should be played aloud and others
lightly. This is part of the expression, changing the amplitude.
Scores only have notes, indication of rhythm, and expression. The timbre is not
speciﬁed, although there are pieces for piano, or violin, etc. Symphonies join the
sound of several instruments, according with some harmony purposes. It is a multi-
component sound.
There are several publications on time-frequency analysis of music signals, [Dor],
musical instruments, [1], and rhythm, [9].
With the advent of chaos theory and fractals, it was highlighted that in nature same
structures can appear at different scales. The proﬁle of a coast may be composed of
arcs, and zooming into one of these arcs a composition of similar, smaller arcs may
appear. Human music usually is structured, discretized, and in some way repeats
patterns.
In recent years, a multi-resolution view of signals has been promoted. A signal
will be decomposed into pieces, in time and frequency domains, and each piece will
be again decomposed (more or less like music into notes), [46].
6.6
Resources
6.6.1
MATLAB
6.6.1.1
Toolboxes
• DESAM Toolbox (spectral analysis of music):
http://www.tsi.telecom-paristech.fr/aao/en/2010/03/29/
desam-toolbox
• Speech and Audio Processing Toolbox:
http://mirlab.org/jang/matlab/toolbox/sap/
• VOICEBOX Speech Processing Toolbox:
http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html/
• XBAT: Bioacustics, animal sounds:
http://www.birds.cornell.edu/brp/software/xbat-introduction
• EEGLAB: Electrophysiological signal processing:
http://sccn.ucsd.edu/eeglab/
• The Open-Source Electrophysiological Toolbox:
http://spc.shirazu.ac.ir/products/Featured-Products/oset//
• Auditory Modeling Toolbox (AMT):
http://amtoolbox.sourceforge.net/

6.6 Resources
353
6.6.1.2
Matlab Code
• Music Signal Processing (Univ. Michigan):
http://web.eecs.umich.edu/~fessler/course/100/l/l09-synth.pdf
http://web.eecs.umich.edu/~fessler/course/100/l/l10-synth.pdf
• Musical Analysis and Synthesis in MATLAB (M.R. Petersen):
http://amath.colorado.edu/pub/matlab/music/
• MATLAB Audio Processing Examples:
http://www.ee.columbia.edu/ln/rosa/matlab/
• ECG simulation using MATLAB:
www.azadproject.ir/wp-content/uploads/2014/04/ECG.pdf
• ECGSYN: ECG waveform generator:
http://www.physionet.org/physiotools/ecgsyn/
• Cardiovascular signals:
http://www.micheleorini.com/matlab-code/
• The BioSig Project:
http://biosig.sourceforge.net/
6.6.2
Internet
6.6.2.1
Web Sites
• James M. Hillenbrand (sound ﬁles):
http://homepages.wmich.edu/~hillenbr/
• William F. Katz (vowel database):
http://wwwpub.utdallas.edu/~wkatz/
• Middle Welsh Vowels:
http://www.personal.psu.edu/staff/e/j/ejp10/cymcanol/
alphabyw/vowel-alone.html/
• Kedija Kedir Idris (speech analysis):
https://sites.google.com/site/ikedija/projects/
speech-signal-analysis-with-matlab-2012
• Digital Sound (Music Tutorials):
http://csweb.cs.wfu.edu/~burg/CCLI/Templates/home.php
• PhysioBank Archive Index (electrocardiograms, etc.):
http://www.physionet.org/physiobank/database/
• MIT-BIH Database and Software Catalog (computational physiology):
http://ecg.mit.edu/dbinfo.html
• eHeart- an introduction to electrocardiograms:
http://www.ndsu.edu/pubweb/~grier/eheart.html
• Principles of Animal Communication (animal sounds):
http://sites.sinauer.com/animalcommunication2e/

354
6
Signal Changes
• Vibrationdata:
http://www.vibrationdata.com
6.6.2.2
Link Lists
• Sam Kirkham:
http://samkirkham.com/scripts/index.html
• MATLAB Toolboxes:
http://stommel.tamu.edu/~baum/toolboxes.html
References
1. J.F. Alm, J.S. Walker, Time-frequency analysis of musical instruments. SIAM Rev. 44(3),
457–476 (2002)
2. M.A. Anusuya, S.K. Katti, Speech recognition by machine, a review (2010). arXiv:1001.2267
3. M.L. Bedran, A comparison between the doppler and cosmological redshifts. Am. J. Phys.
70(4), 406–408 (2002)
4. K.J. Blinowska, J. Zygierewicz, Practical Biomedical Signal Analysis Using MATLAB (CRC
Press, Boca Raton, 2012)
5. L.I. Bluestein, A linear ﬁlter approach to the computation of the discrete Fourier transform.
Northeast Electr. Res. Eng. Meet. Rec. 10, 218–219 (1968)
6. B.P. Bogert, M.J. Healy, J.W. Tukey, The quefrency alanysis of time series for echoes: cepstrum,
pseudo-autocovariance, cross-cepstrum and saphe cracking, in: Procedings of the Symposium
Time Series Analysis vol. 15 (1963), pp. 209–243
7. J. Bradbury, S. Vehrencamp, Principles of Animal Communication (Sinauer Press, Sunderland,
2011)
8. J.J. Burred, A. Robel, T. Sikora, Dynamic spectral envelope modeling for timbre analysis of
musical instrument sounds. IEEE Trans. Audio Speech Lang. Process. 18(3), 663–674 (2010)
9. X. Cheng, J.V. Hart, J.S. Walker, Time-frequency analysis of musical rhythm. Not. AMS 56(3),
356–372 (2009)
10. D.G. Childers, D.P. Skinner, R.C. Kemerait, The cepstrum: a guide to processing. Proc. IEEE
65(10), 1428–1443 (1977)
11. C. Clark Acoustic communication by animals, in Proceedings of the 3rd International Sympo-
sium on Acoustic Communication by Animals (2011)
12. K. Doi, Diagnostic imaging over the last 50 years: research and development in medical imaging
science and technology. Phys. Med. Biol. 51(13), 5–27 (2006)
13. C.E. Felder, A real-time variable resolution chirp z-transform. Master’s thesis, Rochester Insti-
tute of Technology (2007)
14. M. Felsberg, G. Sommer, The monogenic signal. IEEE Trans. Signal Process. 49(12), 3136–
3144 (2001)
15. C. Gonzalez, J. Pleite, V. Valdivia, J. Sanz, An overview of the on line application of frequency
response analysis (FRA), in Proceedings of the IEEE International Symposium Industrial Elec-
tronics, ISIE (2007), pp. 1294–1299
16. S.L. Hopp, M.J. Owren, C.S. Evans, Animal Acoustic Communication (Springer, Heidelberg,
1998)
17. E. Jantunen, A summary of methods applied to tool condition monitoring in drilling. Int. J.
Mach. Tools Manuf. 42(9), 997–1010 (2002)
18. M. Johansson, The Hilbert Transform. Master’s thesis, Växjö University, Sweden (1999)

References
355
19. J. Kaffanke, T. Dierkes, S. Romanzetti, M. Halse, J. Rioux, M.O. Leach, N.J. Shah, Application
of the chirp z-transform to MRI data. J. Magn. Reson. 178(1), 121–128 (2006)
20. R. Kandregula, The basic discrete Hilbert transform with an information hiding application
(2009). arXiv:0907.4176
21. F.W. King, Hilbert Transforms (Cambridge University Press, Cambridge, 2015)
22. J.A. Kisslo, D.B. Adams, Principles of Doppler Echocardiography and the Doppler Examina-
tion #1 (Ciba-Geigy, London, 1987)
23. E. Komatsu, Three Distinctive Redshifts (University of Texas, Austin, 2006). www.as.utexas.
edu/astronomy/education/spring06/komatsu/secure/lecture14.pdf
24. F.R. Kschischang, The Hilbert Transform (University of Toronto, 2006). http://web.eecs.utk.
edu/~roberts/ECE342/hilbert.pdf
25. B. Logan, Mel frequency cepstral coefﬁcients for music modeling, in Proceedings of the Inter-
national Symposium on Music Information Retrieval (ISMIR), pp. 1–11 (2000)
26. L. Marmet, On the Interpretation of Red-shifts: A Quantitative Comparison of Red-shift Mech-
anisms (2012). www.marmet.org/cosmology/redshift/mechanisms.pdf
27. D. Maulik, Doppler sonography: a brief history, in Doppler Ultrasound in Obstetrics and
Gynecology (Springer, Heidelberg, 2005), pp. 1–7
28. U. Meyer-Bäse, H. Natarajan, E. Castillo, A. García, Faster than the FFT: the chirp-z RAG-n
discrete fast fourier transform. Frequenz 60, 147–151 (2006)
29. S. Molau, M. Pitz, R. Schluter, H. Ney, Computing mel-frequency cepstral coefﬁcients on the
power spectrum, in Proceedings of the International Conference Acoustics, Speech, and Signal
Processing (ICASSP 2001), vol. 1 (2001), pp. 73–76
30. L. Muda, M. Begam, I. Elamvazuthi, Voice recognition algorithms using mel frequency cepstral
coefﬁcient (MFCC) and dynamic time warping (DTW) techniques (2010). arXiv:1003.4083
31. C.T. Nguyen, J.P. Havlicek, AM-FM models, partial Hilbert transform, and the monogenic
signal, in Proceedings of the IEEE International Conference Image Processing, (ICIP) (2012),
pp. 2337–2340
32. A.V. Oppenheim, R.W. Schafer, From frequency to quefrency: a history of the cepstrum. IEEE
Signal Process. Mag. 21(5), 95–106 (2004)
33. M.A. Quiñones, C.M. Otto, M. Stoddard, A. Waggoner, W.A. Zoghbi, Recommendations for
quantiﬁcation of doppler echocardiography: a report from the doppler quantiﬁcation task force
of the nomenclature and standards committee of the american society of echocardiography. J.
Am. Soc. Echocardiogr. 15(2), 167–184 (2002)
34. A. Quinquis, Digital Signal Processing Using Matlab (Wiley, Hoboken, 2008)
35. L. Rabiner, R.W. Schafer, C.M. Rader, The chirp z-transform algorithm. IEEE Trans. Audio
Electroacoust. 17(2), 86–92 (1969)
36. L. Rabiner, R.W. Schafer, C.M. Rader, The chirp z-transform algorithm and its application.
Bell Syst. Tech. J. 48(5), 1249–1292 (1969)
37. L. Rabiner, The chirp z-transform algorithm - a lesson in serendipity. IEEE Signal Process.
Mag. 21(2), 118–119 (2004)
38. P. Rajmic, Z. Prusa, C. Wiesmeyr, Computational cost of chirp z-transform and generalized
goertzel algorithm, in Proceedings of the IEEE 22 nd European Signal Processing Conference,
(EUSIPCO) (2014), pp. 1004–1008
39. R.B. Randall, A history of cepstrum analysis and its application to mechanical problems, in
International Conference on Surveillance 7, Chartres, France, October (2013), pp. 1–16
40. R.B. Randall, B. Peeters, J. Antoni, S. Manzato, New cepstral methods of signal pre-processing
for operational modal analysis, in Proceedings of ISMA (2012), pp. 755–764
41. A.G. Rehorn, J. Jiang, P.E. Orban, State-of-the-art methods and results in tool condition mon-
itoring: a review. Int. J. Adv. Manufact. Technol. 26(7–8), 693–710 (2005)
42. G. Reid, Synth Secrets (1999). http://www.soundonsound.com/sos/allsynthsecrets.htm
43. D. Risch, N.J. Gales, J. Gedamke, L. Kindermann, D.P. Nowacek, A.J. Read, A.S. Friedlaender,
Mysterious bio-duck sound attributed to the Antarctic minke whale (Balaenoptera bonaerensis).
Biol. Lett. 10(4), 1–5 (2015)

356
6
Signal Changes
44. Y. Shao, Z. Rao, Z. Jin, Online state diagnosis of transformer windings based on time-frequency
analysis. WSEAS Trans. Circ. Syst. 8(2), 227–236 (2009)
45. S. Sigurdsson, K.B. Petersen, T. Lehn-Schiøler, Mel frequency cepstral coefﬁcients: an evalu-
ation of robustness of MP3 encoded music, in Proceedings of the Seventh International Con-
ference on Music Information Retrieval, (ISMIR) (2006)
46. L.M. Smith, A Multiresolution Time-Frequency Analysis and Interpretation of Musical Rhythm.
Ph.D. thesis, University of Western Australia (2000)
47. S.S. Stevens, J. Volkmann, E.G. Newman, A scale for the measurement of the psychological
magnitude pitch. J. Acoust. Soc. Am. 8, 185–190 (1937)
48. P. Stoica, R. Moses, Spectral Analysis of Signals (Prentice Hall, Upper Saddle River, 2005)
49. R. Tong, R.W. Cox, Rotation of NMR images using the 2D chirp-z transform. Magn. Reson.
Med. 41(2), 253–256 (1999)
50. M. Wang, A.J. Vandermaar, K.D. Srivastava, Review of condition assessment of power trans-
formers in service. IEEE Electr. Insul. Mag. 18(6), 12–25 (2002)
51. M.M. Wood, L.E. Romine, Y.K. Lee, K.M. Richman, M.K. O’Boyle, D.A. Paz, D.H. Pretorius,
Spectral doppler signature waveforms in ultrasonography: a review of normal and abnormal
waveforms. Ultrasound Q. 26(2), 83–99 (2010)
52. B. Zhechev, Hilbert transform relations. Cybern. Inf. Technol. 5(2), 2–13 (2005)

Chapter 7
Time-Frequency Analysis
7.1
Introduction
This chapter is a logical continuation of the previous chapter on signal changes. The
consideration of non-stationary signals requires an assortment of analysis tools, to
highlight different aspects of importance. Many scientiﬁc and technical activities
are interested on such, for medical purposes, for earthquake study, for machine
maintenance, for astronomy, etc.
One of the analysis tools is the spectrogram. This tool has been already used in
the previous chapter, for several examples. It is really intuitive and useful. The spec-
trograms of the previous chapter speak clearly of the joint consideration of time and
frequency when signals are non-stationary. Now, in this chapter more analysis tools
will be introduced, covering their mathematical formulation and showing examples.
The spectrogram is included as a relevant tool, which also constitutes an archetype.
Since the joint time-frequency analysis is a vibrant research topic, there are many
initiatives and proposals of new methods, tools and applications. This chapter focuses
on roots, fundaments, letting for the ﬁnal sections some introduction to interesting
branches.
In particular, the chapter will deal with the short-term Fourier transform, the Gabor
transform, the continuous wavelet transform, the ambiguity function, the Wigner–
Ville transform, the chirplet transform, etc.
Some important aspects appear on the 2D time-frequency scene, like uncertainty
and localization. Perhaps this is surprising, but uncertainty, as in quantum mechanics,
has to be considered.
Before entering into details, it is convenient to bring up some deﬁnitions and
properties.
Most of the analysis tools use the Fourier transform. There are Fourier transform
pairs like for instance the signal and its spectrum (also called the spectral density
function):
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_7
357

358
7
Time-Frequency Analysis
y(t) =
1
2π
∞

−∞
Y(ω) ejω t dω
⇔Y(ω) =
∞

−∞
y(t) e−jω t dt
(7.1)
(a double arrow has been included to represent the mutual relationship)
Another Fourier transform pair is the signal autocorrelation and the power spectral
density (PSD):
Ry(τ) =
1
2π
∞

−∞
Sy(ω) ejωτ dω
⇔Sy(ω) =
∞

−∞
Ry(τ) e−jωτ dτ
(7.2)
Where the autocorrelation is deﬁned as:
Ry(τ) =
∞

−∞
y(t) y∗(t −τ) dτ
(7.3)
(the asterisk means complex conjugate)
The Parseval–Plancherel theorem states that:
∞

−∞
x(t) y∗(t) dt =
1
2π
∞

−∞
X(ω) Y ∗(ω) dω
(7.4)
In particular:
∞

−∞
|y(t)| 2dt =
1
2π
∞

−∞
|Y(ω)|2 dω
(7.5)
Both sides of Eq.(7.5) express the energy of the signal. The right-hand side can also
be written in function of the power spectral density, since:
∞

−∞
|Y(ω)|2 dω =
∞

−∞
Sy(ω) dω
(7.6)
Among the properties of the Fourier transform, let us highlight the following two,
which correspond to a frequency shift (ω0) or a time shift (t0):
y(t) ejω0t ←
→Y(ω −ω0)
(7.7)

7.1 Introduction
359
(the arrows denote Fourier transform with respect to t or ω)
y(t −t0) ←
→Y(ω ) e−jω t0
(7.8)
Property (7.7) is important for amplitude modulation study.
The cornerstone for the analysis of linear systems response is convolution. Recall
that the response of a system with impulse response g(t) is given by:
y(t) =
t

0
g(τ) u(t −τ) dτ
(7.9)
where u(t) is the input and y(t) the output.
The integral in (7.9) is the convolution of g(t) and u(t). Denote this convolution
as g(t)* u(t). There are two important properties of the Fourier transform concerning
convolution:
g(t) ∗u(t) ←
→G(ω) U(ω)
(7.10)
g(t)u(t) ←
→G(ω) ∗U(ω)
(7.11)
Let us now consider uncertainty and other aspects related to joint time-frequency
study.
7.2
Uncertainty
Variables that are Fourier transform duals of one-another, are denoted as ‘conjugate
variables’. The duality relation leads also to an uncertainty relation between them,
in the same vein as the Heisenberg uncertainty principle.
Examples of conjugate variables are time and frequency, time and energy, position
and momentum, angle and angular momentum, and Doppler and range in sonar or
radar applications.
Given two conjugate variables, say x and y, the uncertainty principle refers to a
product of errors in determining simultaneously x and y. If Δx is the error corre-
sponding to x, and Δy is the error corresponding to y, then ΔxΔy ≥q (q being a
certain constant).
For example, let y(t) be a certain pulse with energy E that can be computed with
Eq.(7.5). Deﬁne the temporal and spectral centres of the signal as:

360
7
Time-Frequency Analysis
tc = 1
E
∞

−∞
t y(t)2 dt ;
ωc =
1
2πE
∞

−∞
ω Y(ω)2 dω
(7.12)
The variances around the above deﬁned centres are:
σ2
y = 1
E
∞

−∞
(t −tc)2 y(t)2 dt
(7.13)
σ2
Y =
1
2πE
∞

−∞
(ω −ωc)2 Y(ω)2 dω
(7.14)
Then, using Fourier transform properties, it can be shown that:
σy σY ≥1
2
(7.15)
We can take variances as localization errors, so we could write:
Δt Δω ≥1
2
(7.16)
See [143] and references therein for mathematical details.
The constant at the right-hand side of Eq.(7.16) may change if other deﬁnitions
of errors are considered; see [121] for example, or the sampling limit of Gabor
(which will be treated later on, in Sect. 7.4.2). Anyway, the important point is that the
instantaneous time and frequency of a non-stationary signal cannot be simultaneously
exactly measured. This is one of the reasons to get somewhat blurry spectrograms.
Notice that if frequency is measured in Hz. and denoted as f , an expression
equivalent to (7.16) is ΔtΔf ≥1/4π.
An interesting signal is the ‘Gaussian pulse’:
y(t) =
1
√
2π
e
−

t2
2

(7.17)
The Fourier transform of this pulse is:
Y(ω) = e
−

ω2
2

(7.18)
The Gaussian pulse achieves ΔtΔω = 1/2. Thus, it is a good balance of time
concentrationandfrequencyconcentrationofthesignalarounditstimeandfrequency
centres. Note that this signal is neither time-limited nor band-limited.

7.2 Uncertainty
361
-0.2
-0.15
-0.1
-0.05
0
0.05
0.1
0.15
0.2
-1
-0.5
0
0.5
1
seconds
Gaussian-modulated 
sine signal
0
10
20
30
40
50
60
70
80
-0.05
0
0.05
Hz
signal spectrum
Fig. 7.1 GMP signal and spectrum
The MATLAB Signal Processing Toolbox provides the function gauspuls() to
generate a Gaussian-modulated sinusoidal pulse (GMP), which is a Gaussian pulse
multiplied by a cos(wct). Figure 7.1 (Program 7.1) depicts a GMP and its spectrum.
Program 7.1 GMP signal and spectrum
%
GMP
signal
and
spectrum
fy =100;
% signal
central
frequency
in
Hz
bw =0 .2;
% signal
relative
bandwidth
fs =1000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
t= -(0.2 - tiv ): tiv :(0.2 - tiv );
subplot (2 ,1 ,1)
y= gauspuls (t,fy ,bw );
% signal
data
set
plot (t,y,'k');
% plots
figure
axis ([ -0 .2
0.2
-1.2
1.2 ]);
xlabel ('seconds ');
title ('gauss
pulse
signal ');
subplot (2 ,1 ,2)
Y=fft(y )/( fs /2);
% Fourier
transform
of
the
signal
xf =0:1:( fs /2);
plot (xf (1:80) , real (Y (1:80)) , 'k');
% plots
spectrum
axis ([0
80
-0.05
0 .05 ]);
xlabel ('Hz ');
title ('signal
spectrum ');

362
7
Time-Frequency Analysis
7.3
Ambiguity
In simple words, the principle of sonar and radar is to send a pulse at time t0, wait
for the echo (that comes at time t1), and measure the time τ = t1 −t0 (τ is called
the time lag). From τ we can calculate distance to the target. It is always important
to check the coherency of pulses and echoes.
Suppose there are two targets at different distances. If the radar pulse was too
long, the echoes from the two targets would overlap making difﬁcult to determine
both distances. Thus, it is most convenient to use very short pulses. For example, a
1 µs pulse provides a radio burst about 300 m long. If better distance resolution is
required, shorter pulses are needed.
Another important factor to consider is signal to noise ratio (SNR), since echoes
should be recognized against a noise background. It can be shown that this SNR
is proportional to the transmitted pulse energy, irrespective of the pulse duration or
bandwidth.
The signal received by the radar antenna is ﬁltered to recover echoes. The linear
ﬁlter which maximizes the peak SNR is the so-called ‘matched-ﬁlter’. Denoting as
Y(ω) the radar signal spectrum, and Sn(ω) the PSD of the noise, the matched ﬁlter
for time instant tm is the following:
H(ω) = Y ∗(ω)
Sn(ω) e −j ω tm
(7.19)
There is a conﬂict. Short pulses are needed. High energy should be put in the pulses.
However the shorter the pulse, the more difﬁcult is to put enough energy in it. Chirp
signals provide a way of breaking this limitation. Note that bats do use voice chirps
as nature radar.
Given a chirp signal, an ‘antichirp’ matched ﬁlter can be devised that make the
signal into an impulse. This is also denoted as pulse compression. Thus, the complete
scheme is that the radar system sends a short pulse to the radio transmitter system
where the pulse is converted to a chirp signal with longer duration, so more energy
can be put in; the echo is received and processed by a matched ﬁlter, being converted
back to a short pulse with good resolution capabilities.
However if the target is moving, the Doppler effect induces degradation of the
matched ﬁlter output, and uncertainty increases. An important tool to determine
the position and velocity of a target from a narrow-band echo is the narrow-band
ambiguity function, deﬁned as follows:
Ξy(τ, θ) =
∞

−∞
y(t) y∗(t −τ) e−jθ t dt
(7.20)
where τ is the time lag and θ is the Doppler frequency shift.

7.3 Ambiguity
363
The wide-band ambiguity function is given by:
WΞy(τ, θ) = √α
∞

−∞
y(t) y∗(α(t −τ)) dt
(7.21)
with α = (c −v)/(c + v), where c is the radar wave celerity and v the radial velocity
of the target.
See [61] for a concise introduction of ambiguity functions. The book [112] pro-
vides a detailed treatment in the context of radar, with Matlab programs included.
The presentations [44, 94] offer opportune tutorials on radar. An extensive exposition
of radar is given in [52].
7.4
Transforms for Time-Frequency Studies
A typical way of attack in signal processing is the use of transforms, such for instance
the Fourier transform. The problems are translated to opportune domains for analysis,
processing and synthesis purposes. This methodology has been enriched with new
transforms that are useful for the time-frequency domain. The purpose of the section
is to introduce this area of signal processing, which is experimenting a great deal of
progress. The reader is invited to enlarge his view of this ﬁeld, with the hints given
in this section.
It is convenient ﬁrst to review some basic concepts and notation related to bases
and frames.
The inner product of two functions p(t) and q(t) is deﬁned as follows:
⟨p, q⟩=
∞

−∞
p(t) · q∗(t) dt
(7.22)
for continuous-time functions, or
⟨p, q⟩=
∞

k=−∞
p(k) q∗(k)
(7.23)
for discrete-time functions.
Two functions are orthogonal if their inner product is zero.
In the case of an orthonormal base {gk(t)} of functions, which spans a function
space, any element of this space can be written as:
y(t) =

k
ckgk(t)
(7.24)

364
7
Time-Frequency Analysis
With:
⟨y(t), gk(t)⟩= ck
(7.25)
In the case of a frame {gk(t)}, we can still write:
y(t) =

k
ckgk(t)
(7.26)
But now
⟨y(t), hk(t)⟩= ck
(7.27)
Where {hk(t)} is a dual basis such that
⟨gi(t), hk(t)⟩= δik
(7.28)
We say that the system {gk(t)} and {hk(t)} is bi-orthonormal.
Equation(7.26) is called the synthesis equation, and {gk(t)} the synthesis function.
Equation(7.27) is called the analysis equation, and {hk(t)} the analysis function.
More details on bases and frames are given in [99]. The short article of [79]
gives intuitive insights concerning frames. It would be also interesting to browse the
presentation of [167].
7.4.1
The Short-Time Fourier Transform
Suppose you wish to determine the spectral contents (the frequencies) of a certain
sound y(t) during the time interval 1 ≤t ≤2 s. A simple idea would be to compute
the Fourier transform of y(t) from t = 1 to t = 2, instead of t = −∞to t = ∞.
That means to compute:
STY(ω) =
tc+h

tc−h
y(t) e−jω t dt
(7.29)
with tc = 1.5 s and h = 0.5 s.
There is a problem, the Fourier transform would interpret the signal corners at
tc −h and tc + h as signal jumps, so large and undesired spectral high frequency
contents would appear. The problem can be mitigated introducing a smooth ‘window
function’ w(t −tc) covering the time interval of interest.
The short-time Fourier transform (STFT) is:
Fy(t, ω) =
∞

−∞
y(τ) w(τ −t) e−jω τ dτ
(7.30)

7.4 Transforms for Time-Frequency Studies
365
For every tc, the STFT describes the local spectral contents using the window w(t)
centered at tc.
The spectrogram is:
SFy(t, ω) =
Fy(t, ω)
2
(7.31)
The STFT is usually discretized, with t = ntl, ω = mωl. Thus:
Fy(ntl, mωl) =
∞

−∞
y(τ) w(τ −ntl) e−jmωl τ dτ
(7.32)
Accordingwith(7.32) theSTFTtakes theFourier transformonablockbyblockbasis.
The smaller tl the better the time resolution, but the poorer the frequency resolution.
The blocks could be overlapped or disjointed.
The Eq.(7.32) can be rewritten as inner product:
Fy(ntl, mωl) =

y(t), wn,m(τ)

=
∞

−∞
y(τ) w∗
n,m(τ) dτ
(7.33)
where:
wn,m(τ) = w(τ −ntl) ej m ωlτ
(7.34)
Several examples of spectrograms have been presented in previous sections in this
chapter.
Next two ﬁgures illustrate the uncertainty issue. A signal is built with 2 s of zeros,
6 s of pure 50 Hz.sinusoid, and ﬁnally 2 s of zeros. In total 10 s of a 3-component
signal. Figure7.2 shows the signal.
In a ﬁrst attempt, one wished to get good precision about time, with no care about
frequency precision. Therefore, only 16 levels of frequency have been speciﬁed in
0
5
10
15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10
15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
2 sec. 
6 sec. 
2 sec. 
Fig. 7.2 Sine signal for STFT testing

366
7
Time-Frequency Analysis
Fig. 7.3 STFT of 1-sine
signal and time ﬁne precision
seconds
Hz
0
1
2
3
4
5
6
7
8
9
0
20
40
60
80
100
120
specgram(), in the Program 7.2. The result is shown in Fig.7.3. The times of the
3 components of the signal have been well measured. However, there is noticeable
uncertainty about the signal frequencies.
Notice, in particular, the vertical lines on the band borders (approximately on time
= 2 s and time = 8 s). Since the sinusoidal signal is cut between 2 and 8 s, the cuts
mean a large continuum of frequencies well located in time.
Program 7.2 STFT of 1-sine signal
% STFT
of
1- sine
signal
fy =50;
% signal
frequency
in
Hz
fs =128;
% sampling
rate
in
Hz
tiv =1/ fs;
% time
between
samples
% time
of
first
signal
part
(2
seconds ):
t1 =0: tiv :(2 - tiv );
% time
for
yn
(6
seconds ):
tn =2: tiv :(8 - tiv );
% time
of
last
signal
part
(2
seconds ):
t2 =8: tiv :(10 - tiv );
y1 =0* exp(-j *2* pi*t1 );
yn= exp(-j *2* pi*fy*tn );
y2 =0* exp(-j *2* pi*t2 );
y=[ y1
yn
y2 ]';
% complete
signal
( column
vector )
t=[ t1
tn
t2 ];
% complete
signal
time
set
f =0:1:(( fs /2) -1);
% frequency
intervals
set
% spectrogram
computation :
[sgy ,fy ,ty ]= specgram (y ,16 , fs );
colmap1 ;
colormap ( mapg1 );
% user
colormap
% plot
the
spectrogram :
imagesc (ty ,fy , log10 (0 .1+ abs(sgy )));
axis
xy;
title (' Spectrogram
of
1- sine
packet ,
fine
time
precision ');
xlabel ('seconds ');
ylabel ('Hz ');

7.4 Transforms for Time-Frequency Studies
367
Fig. 7.4 STFT of 1-sine
signal and frequency ﬁne
precision
seconds
Hz
-1
0
1
2
3
4
5
6
7
0
20
40
60
80
100
120
Next, one tries better precision about frequency, and so 512 levels of frequencies
are speciﬁed in specgram() in the Program 7.3. The result is shown in Fig.7.4. The
frequency of the 50 Hz. sine, which is the second component of the signal, is well
determined at the price of error about the sine duration (according with the ﬁgure
from time = 1 s to time = 5 s).
Program 7.3 STFT of 1-sine signal
% STFT
of
1- sine
signal
fy =50;
% signal
frequency
in
Hz
fs =128;
% sampling
rate
in
Hz
tiv =1/ fs;
% time
between
samples
% time
of
first
signal
part
(2
seconds ):
t1 =0: tiv :(2 - tiv );
tn =2: tiv :(8 - tiv );
% time
for
yn
(6
seconds )
% time
of
last
signal
part
(2
seconds ):
t2 =8: tiv :(10 - tiv );
y1 =0* exp(-j *2* pi*t1 );
yn= exp(-j *2* pi*fy*tn );
y2 =0* exp(-j *2* pi*t2 );
y=[ y1
yn
y2 ]';
% complete
signal
( column
vector )
t=[ t1
tn
t2 ];
% complete
signal
time
set
f =0:1:(( fs /2) -1);
% frequency
intervals
set
% spectrogram
computation :
[sgy ,fy ,ty ]= specgram (y ,512 , fs );
colmap1 ;
colormap ( mapg1 );
% user
colormap
% plot
the
spectrogram :
imagesc (ty ,fy , log10 (0 .1+ abs(sgy )));
axis
xy;
title (` Spectrogram
of
1- sine
packet ,
fine
frequency
precision ');
xlabel ('seconds ');
ylabel ('Hz ');

368
7
Time-Frequency Analysis
A general comment about the examples just given is that in the case of signals,
uncertainty can be macroscopic. It is not the kind of very small sizes in quantum
mechanics or atomic physics.
There are many sources of information on the STFT available on Internet. Like-
wise, most books on time-frequency analysis have sections devoted on STFT, like
for instance [144].
7.4.2
The Gabor Expansion
In his ‘Theory of Communication’, year 1946, D. Gabor introduced a method to
expand a signal in a series of elementary functions which are constructed from a
single block by time and frequency (modulation) translations [66]. Figure7.5 shows
a photograph of Mr. Gabor, who received the Nobel prize for his invention of holog-
raphy.
The Gabor expansion is as follows:
y(t) =

n

m
cn,m gn,m(t)
(7.35)
where the elementary functions gn,m(t) are given by:
gn,m(t) = g(t −ntl) ej m ωl t
(7.36)
The elementary functions gn,m(t) are called ‘logons’ and also a Weyl–Heisenberg
system.
The function g(t) is called ‘atom’ or the ‘synthesis window’. Gabor proposed to
use the following g(t):
g(t) = C ej ωc t e−k(t −tc)2
(7.37)
which is a complex GMP.
Fig. 7.5 Dennis Gabor

7.4 Transforms for Time-Frequency Studies
369
Fig. 7.6 Idea of the Gabor
expansion
time
frequency
t l
wl
The idea of the Gabor expansion is illustrated in the Fig.7.6. The rectangles
correspond to the logons. The time-frequency plane is decomposed as a lattice,
spaced by tl and wl (the ‘lattice constants’). The origin is (tc, wc).
According with the Shannon sampling theorem two samples per signal period are
just enough for ulterior signal reconstruction (with an ideal ﬁlter). But in practical
applications, for good reasons, more signal samples are used. This is oversampling.
In a similar manner, looking at Fig.7.5, it may be beneﬁcial for signal processing to
use more density of rectangles, with more overlapping than in the ﬁgure.
Let us consider uncertainty again. Let be a basic kind of frequency measurement
in Hz., counting maxima of a signal along time, so frequency would be computed
as the number of maxima in an interval of time Δt. The shorter Δt that allows to
distinguish two frequencies f and f +Δf is such the maxima counts differ by at least
one:
(f + Δf ) Δt −f Δt ≥1
(7.38)
Then:
Δf Δt ≥1
(7.39)
This is the Gabor uncertainty principle, that can be derived in a rigorous way deﬁning
a nominal duration and a nominal bandwidth of a signal y(t). The nominal duration
of y(t) is the duration of a rectangle with the same area as y(t) and height equal to
y(0). The nominal bandwidth is the width of a rectangle with the same area as Y(f )
(Fourier transform of y(t)) and height equal to Y(0). Expression (7.39) is equivalent
to ΔtΔω ≥2π.
The Gabor original choice for tl and wl was such tl wl = 2π. Later on it has been
demonstrated that in this case the set {gn,m(t)} proposed by Gabor is not a frame. If
tl wl < 2π (oversampling) the set {gn,m(t)} is a frame and, recalling Eqs.(7.26) and

370
7
Time-Frequency Analysis
(7.27) we can obtain the cn,m coefﬁcients of Eq.(7.35) by choosing a bi-orthonormal
set of functions, like the following:
hn,m(t) = h(t −ntl) ej m ωl t
(7.40)
Thus:
cn,m =

y, hn,m(t)

=
∞

−∞
y(t) · h∗
n,m(t) dt
(7.41)
Expression (7.41) is called the Gabor transform. The h(t) function is called the
‘analysis window’.
When using the STFT function provided by MATLAB, it is possible to select
a Hamming window, or a Blackman window, or a Hanning window, etc. From
Eqs.(7.41) and (7.32) one can see that the Gabor transform is a particular case
of STFT, with a Gaussian window. In fact, the ﬁrst proposal of STFT was the Gabor
transform, as introduced in [66]. The book [56] is devoted to the Gabor transform
and its applications, and gives some historical details (see also [67]). Currently there
is a more general view of the Gabor transform, in a context of frames and using
other versions of the analysis and synthesis windows. Let us mention that some
authors recommend the use of the Zak transform to compute the Gabor expansions
[27, 73, 181].
In the case tl wl > 2π no frame is possible. However in the critical sampling
case tl wl = 2π frames and orthonormal bases are possible, but without good time-
frequency localization, as dictated by the Balian-Low theorem [20, 73].
Figure7.7 offers a 3D visualization of Gabor logons. Part of the code belonging
to programs of the next section, has been re-used here to obtain an energy density in
the time-frequency plane. The ﬁgure has been generated with the Program 7.4. This
program has three parts: the ﬁrst one generates a GMP, the second ﬁds the corre-
Fig. 7.7 Visualization of the
Gabor logons

7.4 Transforms for Time-Frequency Studies
371
sponding energy density in the time-frequency plane using the Wigner distribution
approach, the third part makes a simplistic reproduction of the 3D single GMP to
form the grid of mountains. Notice that the logons are larger than the minimum size
Gabor wished.
Program 7.4 Plot of 5x6 large logons
%
Plot
of
5x6
large
logons ;
1
basic
GMP
pulse
fy =100;
% central
frequency
in
Hz
fs =1000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% signal
time
intervals
set
(-0 .2
to
0.2
seconds ):
tp = -(0.2 - tiv ): tiv :(0 .2 );
%to
get
N
even
Np= length (tp );
bw =50/ fy;
% relative
bandwidth
( DeltaF =50
Hz)
yr= gauspuls (tp ,fy ,bw );
t =0: tiv :( Np* tiv)-tiv;
% time
intervals
f =0:1:(( fs /2) -1);
% frequency
intervals
y= hilbert (yr );
% analytical
signal
% Wigner :----------------------------------------
N= length (y);
% normalized
frequencies
set
-pi..pi :
theta =((0:N -1) -N /2) '*2* pi/N;
tau =(0:N -1) -N/2;
% normalized
time
att= zeros (N);
% intermediate
NxN
matrix
% matrix
computation
( theta (ii )* tau(jj )):
att= theta * tau /2;
ax1=exp(j* att );
%
NxN
matrix
ax2=exp(-j* att );
%
NxN
matrix
disp ('step
1');
my1= zeros (N);
% intermediate
NxN
matrix
my2= zeros (N);
% intermediate
NxN
matrix
for
ii =1:N,
% rows
ii
my1(ii ,:)= y(ii )* ax1(ii ,:);
% along
jj
my2(ii ,:)= y(ii )* ax2(ii ,:);
% along
jj
end;
disp ('step
2');
Y1= fft( my1 );
Y2= fft( my2 );
kappa =Y1.* conj (Y2 );
WD= fftshift ( fft(kappa ,[] ,2));
% ------------------------------------------------
% GMP= abs(WD (222:260 ,184:216));
GMP=abs(WD (216:266 ,180:218));
G1 =[ GMP
GMP
GMP
GMP
GMP
GMP ];
% adding
6
matrices
GG =[ G1;G1;G1;G1;G1 ];
% adding
5
rows
of
matrices
%3D
plot
with
perspective :
mesh (GG ); axis ([0
240
0
250]);
view (30 ,70);
title ('A
3D
view
of
Gabor
logons ');
xlabel ('x
0.1
sec ');
ylabel ('Hz ');

372
7
Time-Frequency Analysis
7.4.3
The Continuous Wavelet Transform
In simple words, for a rough determination of frequency it is enough to take one
cycle of a signal. Analysis windows could be as narrow as one signal cycle. Higher
signal frequencies would require shorter duration windows.
Audio equalizers are frequently based on the use of constant-Q band-pass ﬁlters,
with:
Q = Δω
ωc
(7.42)
where ωc is the band centre and Δω is the bandwidth.
Constant-Q ﬁlters can mirror the structure of musical scales, so for instance it is
possible to make a ﬁlter for each piano note. Recall that the frequency for each note
doubles from one scale to the next, and so the bandwidth occupied by scales doubles
from one scale to the next [25, 28].
The larger Δω of a ﬁlter the shorter its impulse response h(t).
Notice that the analysis window in Eq.(7.32) is the impulse response of a ﬁlter.
It is possible to devise a variation of the STFT which changes the analysis window
length so that a constant number of periods are within the window at each frequency.
For instance, this idea was applied to music analysis in the 80s. Actually this is a
type of wavelet transform.
The continuous Wavelet transform is:
Wy(τ, s) = ⟨y(t), ψ(t)⟩=
∞

−∞
y(τ)
1
√|s|ψ∗
	t −τ
s

dt
(7.43)
The original signal can be reconstructed with the inverse transform:
y(t) = 1
Cψ
∞

−∞
∞

−∞
Wy(τ, s)
1
√|s|ψ
	t −τ
s

dτ ds
s2
(7.44)
where:
Cψ =
1
2π
∞

−∞
|Ψ (ω)|2
|ω|
dω
(7.45)
and Ψ is the Fourier transform of ψ.
The constant Cψ is called the admissibility constant. The inverse transform is
possible if (admissibility condition):
0 < Cψ < + ∞
(7.46)

7.4 Transforms for Time-Frequency Studies
373
There is a ‘mother’ wavelet ψ. ‘Daughter’ wavelets are scaled and shifted copies of
the mother wavelet:
ψs,τ(t) =
1
√|s| ψ
	t −τ
s

(7.47)
The important variable is s, the scale variable, which corresponds to the inverse of
frequency; |s| >1 dilates the wavelet for low frequency analysis, |s| <1 compresses
the wavelet for high frequency analysis. When the scale is changed, the duration and
the bandwidth of the wavelet change but its shape remains the same (as in constant-Q
ﬁlters).
A typical wavelet combines the complex exponential and the Gaussian pulse, like
the GMP.
The admissibility condition implies that Ψ (0) = 0 (Ψ the Fourier transform of
ψ(t)), so ψ(t) has to oscillate.
One of the books that could be consulted for continuous wavelets is [bM]. There
is also a good tutorial on Internet [141]. See [3] for a more extended exposition.
The scalogram is:
SCy(τ, s) =
Wy(τ, s)
2
(7.48)
Extensive details on Wavelets will be considered in another book (a continuation of
this one).
Figure7.8 shows the scalogram corresponding to a square signal.
0
0.5
1
1.5
2
2.5
3
-1
-0.5
0
0.5
1
sec
signal
samples
scales
10
20
30
40
50
60
50
100
150
200
Fig. 7.8 Scalogram of square signal

374
7
Time-Frequency Analysis
The ﬁgure has been generated with the Program 7.5. Instead of using the cwt()
function of the MATLAB Wavelet Toolbox, which computes the continuous wavelet
transform of a signal, we preferred to use own code to give a ﬁrst idea of how to
compute and plot the scalogram. The cwt() function is, of course, much faster. We
also partially tried to accelerate our code, by doing some vectorization.
Program 7.5 Signal analysis by continuous wavelet transform
%
Signal
analysis
by
continuous
wavelet
transform
%
Morlet
Wavelet
%
Plot
of
signal
and
scalogram
%
Square
signal
fy =1;
% signal
frequency
in
Hz
wy =2* pi*fy;
% signal
frequency
in
rad/s
duy =3;
% signal
duration
in
seconds
fs =20;
% sampling
frequency
in
Hz
Ts =1/ fs;
% time
interval
between
samples ;
t =0: Ts :( duy -Ts );
% time
intervals
set
y= square (wy*t);
% signal
data
set
ND= length (y);
% number
of
data
CC= zeros (40 , ND );
%
CWT
nn =1: ND;
for
ee =1:200 ,
s=ee *0 .008 ;
% scales
for
rr =1:ND ,
% delays
a=Ts *(rr -1);
val =0;
% vectorized
part
t=Ts *(nn -1);
x=(t-a)/s;
% plug
coeffs.
% wavelet :
psi =(1/ sqrt (s ))*( exp (-( x. ^2)/2) .* cos (5*x ));
for
j =1:ND ,
val= val +(y(j).*psi(j ));
end;
CC(ee ,rr )= val;
end;
end;
figure
(1)
subplot (2 ,1 ,1)
plot (t,y,'k');
axis ([0
duy
min(y)-0 .1
max(y)+0 .1 ]);
xlabel ('sec ');
ylabel ('signal ');
title ('wavelet
analysis ');
subplot (2 ,1 ,2)
imagesc (CC );
colormap ('jet ');
xlabel ('samples ');
ylabel ('scales ');

7.5 Time-Frequency Distributions
375
7.5
Time-Frequency Distributions
In this section two important time-frequency distributions will be introduced: the
Wigner distribution and the Sussman ambiguity function (SAF). The section begins
with a subsection on basic concepts about distributions.
7.5.1
Densities
A two-dimensional density (or distribution) P(x, y) is a function that tells how a
quantity of interest distributes in the plane x −y. The total amount of the quantity
would be:
∞

−∞
∞

−∞
P(x, y) dx dy
(7.49)
One-dimensional densities, also called ‘marginal distributions’, or simply ‘mar-
ginals’, are obtained with the following integrals:
M(x) =
∞

−∞
P(x, y) dx ; M(y) =
∞

−∞
P(x, y) dy
(7.50)
In the case of signals, it is pertinent to study the energy density in the t −ω plane.
A good energetic distribution should satisfy several conditions. Here is a list of
some of the conditions:
• Positivity: the distribution should be positive everywhere
• Total energy should equal the total energy of the signal y(t):
∞

−∞
∞

−∞
P(t, ω) dt dω =
∞

−∞
| y(t)|2dt =
∞

−∞
| Y(ω)|2dω
(7.51)
• Marginal distributions should satisfy the following equations:
∞

−∞
P(t, ω) dt = |Y(ω)|2 ;
∞

−∞
P(t, ω) dω = |y(t)|2
(7.52)
• Time and frequency shift invariance: shifting the signal in time or frequency by
a certain amount, should shift the distribution in time or frequency by the same
amount:

376
7
Time-Frequency Analysis
y1(t) = y(t −τ) ⇒P1(t, ω) = P(t −τ, ω)
Y1(ω) = Y(ω −θ) ⇒P1(t, ω) = P(t, ω −θ)
(7.53)
Both the Wigner and the SAF distributions, to be introduced next, are quadratic
(energetic) distributions, based on the local (instantaneous) auto-correlation function
(LACF):
ky(t, τ) = y

t + τ
2

y∗
t −τ
2

(7.54)
By means of the Fourier transforms, the Wigner and SAF distributions can also be
expressed in function of the transformed LACF:
χy(ω, θ) = Y
	
ω + θ
2

Y ∗
	
ω −θ
2

(7.55)
The LACF provides a way to compute localized energy in the time-frequency plane.
7.5.2
The Wigner Distribution
Figure7.9 shows a photograph of Eugene Wigner, Nobel Laureate.
Fig. 7.9 Eugene Wigner

7.5 Time-Frequency Distributions
377
The Wigner (or Wigner–Ville) distribution is deﬁned as follows:
WDy(t, ω) =
∞
−∞
y

t + τ
2

y∗
t −τ
2

e−j ω τ dτ =
=
∞
−∞
ky(t, τ) e−j ω τ dτ
(7.56)
Using Fourier transform properties, the same Wigner distribution can be obtained
with:
WDy(t, ω) =
1
2π
∞
−∞
Y

ω + θ
2

Y ∗
ω −θ
2

ej t θ dθ =
=
1
2π
∞
−∞
χy(ω, θ) ej t θ dθ
(7.57)
7.5.2.1
Properties of the Wigner Distribution
Recall from the previous subsection the conditions for a good energetic distribution.
Due to its formulation, the Wigner distribution is real-valued. With the only excep-
tion of the following family of signals:
y(t) =
α
π
4
exp
	
−αt2
2 + jβ t2
2 + jω0t

(7.58)
the Wigner distribution does not satisﬁes the positivity condition. Actually it must
go negative somewhere in the time-frequency domain.
Marginals are satisﬁed, and so we have:
∞

−∞
WDy(t, ω) dt = |Y(ω)|2 ;
1
2π
∞

−∞
WDy(t, ω) dω = |y(t)|2
(7.59)
The Wigner distribution is time and frequency covariant, that is: it preserves time
and frequency shifts:
y1(t) = y(t −τ) ⇒WDy1(t, ω) = WDy(t −τ, ω)
Y1(ω) = Y(ω −θ) ⇒WDy1(t, ω) = WDy(t, ω −θ)
(7.60)
The above property can be also named as translation covariance.
In addition, the Wigner distribution is dilation covariant:
y1(t) =
√
ky(k t) , k > 0 ⇒WDy1(t, ω) = WDy

k t, ω
k

(7.61)
The instantaneous frequency of a signal can be obtained from the ﬁrst conditional
moment in frequency:

378
7
Time-Frequency Analysis
(ω)t =
1
2π|y(t)|2
∞

−∞
ω WDy(t, ω) dω
(7.62)
Likewise, the group delay can be obtained from the ﬁrst conditional moment in time:
(t)ω =
1
|Y(ω)|2
∞

−∞
t WDy(t, ω) dt
(7.63)
With respect to convolution, one has:
y(t) =
∞

−∞
h(t −τ) x(τ) dτ ⇒WDy(t, ω) =
=
∞

−∞
WDh(t −τ, ω) WDx(τ, ω) dτ
(7.64)
And with respect to modulation:
y(t) = p(t) x(t) ⇒WDy(t, ω) =
1
2π
∞

−∞
WDp(t, ω −θ) WDx(t, θ) dθ
(7.65)
The Moyal’s formula is:

∞

−∞
x(t) y∗(t) dt

2
=
1
2π
∞

−∞
∞

−∞
WDx(t, ω) WD∗
y(t, ω) dt dω
(7.66)
More mathematical details of the Wigner transform and its properties can be found
in the books [26, 41, 120]. A brief introduction is given by [155], and a concise
tutorial is given by [174]. See [55] and references therein for the Moyal’s formula
and its relationship with other identities.
7.5.2.2
Example of Wigner Distribution
Figure7.10 shows a signal composed of two GMPs with different frequencies and
duration. This signal will be used to show examples of the Wigner distribution and
the SAF. The program also computes the signal energy.

7.5 Time-Frequency Distributions
379
Fig. 7.10 A test signal
composed of two different
GMPs
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds
Program 7.6 2 GMPs signal
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set
t =0: tiv :1;
% complete
time
set
(1
second );
Ny= length (t);
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
y=[ y1
yn
y2 ];
plot (t,y,'k');
% plots
figure
% axis ([0
1.2
-1.2
1.2 ]);
xlabel ('seconds ');
title ('2
GMPs
signal ');
% print
signal
energy
( Parseval )
disp ('signal
energy :')
Pyt=tiv* sum (( abs(y)).^2)
% time
domain
computation
c=fft(y,fs )/ fs;
PYW=sum( abs(c)). ^2)
% frequency
domain
computation
Figure7.11 shows the Wigner distribution of the two GMPs signal. There are three
ellipses in the picture. The ellipse in the centre corresponds to interference. The other
two correspond to the GMP pulses.
The Fig.7.11 has been generated with the Program 7.7. The Fourier transform
property (7.7) has been used to compute the integrands according with Eq.(7.57). It
is always convenient to use the FFT for fast integrations. The signal to be analyzed

380
7
Time-Frequency Analysis
Fig. 7.11 Wigner
distribution of the previous
test signal
seconds
Hz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
120
140
should be an analytical signal, so hilbert() has been used. A special scaling has been
applied in imagesc() to highlight the main picture components.
Program 7.7 Wigner distribution of 2 GMPs signal
%
Wigner
distribution
of
2
GMPs
signal
clear
all
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set
t =0: tiv :1;
% complete
time
set
(1
second );
Ny= length (t);
% odd
number
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
yr =[ y1
yn
y2 ]';
%2
GMPs
signal
( column
vector )
y= hilbert (yr );
% analytical
signal
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times

7.5 Time-Frequency Distributions
381
fo= fft(aux ,Ny )/ Ny;
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .01+ abs(WD )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Wigner
distribution
of
the
2
GMPs
signal ');
% Marginals -----------------------------------------
margf = zeros (Ny ,1);
% frequency
marginal
for
nn =1:Ny ,
margf (nn )= tiv* sum(WD(nn ,:));
end;
margt = zeros (1, Ny );
% time
marginal
for
nn =1:Ny ,
margt (nn )= sum(WD (:, nn ));
end;
figure (2)
plot (f,margf ,'k');
% frequency
marginal
xlabel ('Hz ');
title ('frequency
marginal ');
figure (3)
plot (t,margt ,'k');
% time
marginal
xlabel ('seconds ');
title ('time
marginal ');
% print
y
signal
energy
disp ('signal
energy :')
e1= tiv* sum( abs( margt ))
e2= sum( abs( margf ))
The Program 7.7 also computes time and frequency marginals according with
Eq.(7.52). The results are shown in Figs.7.12 and 7.13. In this example, the marginals
clearly show the time and frequency localization of the signal components.
7.5.3
The SAF
The Sussman ambiguity function (SAF), is given by:
Ay(τ, θ) =
∞
−∞
y

t + τ
2

y∗
t −τ
2

e−j θ t dt =
=
∞
−∞
ky(t, τ) e−j θ t dt
(7.67)

382
7
Time-Frequency Analysis
Fig. 7.12 Frequency
marginal of the Wigner
distribution (2 GMP signal)
0
50
100
150
0
1
2
3
4
5
6
7 x 10-3
Hz
Fig. 7.13 Time marginal of
the Wigner distribution
(2 GMP signal)
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
seconds
Using Fourier transform properties, the same SAF distribution can be obtained with:
Ay(τ, θ) =
1
2π
∞
−∞
Y

ω + θ
2

Y ∗
ω −θ
2

ejτ ω dω =
=
1
2π
∞
−∞
χy(ω, θ) e−j τ ω dω
(7.68)

7.5 Time-Frequency Distributions
383
7.5.3.1
Properties of the SAF Distribution
The SAF is usually complex-valued. It has Hermitian even symmetry:
Ay(τ, θ) = A∗
y(−τ, −θ)
(7.69)
Recall from Eq.(7.3) the (time) autocorrelation Ry(τ) of the signal y(t). With the
SAF we have that:
Ry(τ) = Ay(τ, 0)
(7.70)
which is the horizontal axis for Ay(τ, θ). Likewise, the spectral autocorrelation of
the signal y(t) is given by the vertical axis for Ay(τ, θ).
The value of the SAF at the origin, Ay(0, 0), is equal to the energy of the signal
y(t). This value is also the maximum value of the SAF modulus:
Ay(τ, θ)
 ≤Ay(0, 0) , ∀τ, θ
(7.71)
For the cases of time and frequency shifting we have:
y1(t) = y(t −t0) ⇒Ay1(τ, θ) = Ay(τ, θ) exp(−j t0 θ)
Y1(ω) = Y(ω −ω0) ⇒Ay1(τ, θ) = Ay(τ, θ) exp (j ω0τ)
(7.72)
With respect to convolution, one has:
y(t) =
∞

−∞
h(t −τ) x(τ) dτ ⇒Ay(τ, θ) =
∞

−∞
Ah(τ −t, θ) Ax(t, θ) dt
(7.73)
And with respect to modulation:
y(t) = p(t) x(t) ⇒Ay(τ, θ) =
1
2π
∞

−∞
Ap(τ, θ −ω) Ax(τ, ω) dω
(7.74)
More information on the SAF can be found in the books [26, 41]. It is also part of
the contents of [55].
7.5.3.2
Example of SAF Distribution
Figure7.14 shows the SAF distribution of the 2 GMPs signal. The two pulses overlap
at the same place, centre of the ﬁgure. The two ellipses correspond to the interference:
it is placed according with 1/2 of the temporal distance between the centres of pulses,
and 1/2 of the frequency difference of the pulses.

384
7
Time-Frequency Analysis
Fig. 7.14 SAF of the
previous test signal
seconds
Hz
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-60
-40
-20
0
20
40
60
The Fig.7.14 has been generated with the Program 7.8, which is very similar to
the previous program, with some matrix elements reordering.
Since the pulses concentrate at the centre, and the interferences are clearly in
another place, a possible idea to eliminate the interferences is to ﬁlter out them over
the SAF, and then obtain the Wigner distribution from the ﬁltered SAF. This idea
will be explored in Sect.7.5.5.
Program 7.8 SAF of 2 GMPs signal
%
SAF
of
2
GMPs
signal
clear
all
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set
t =0: tiv :1;
% complete
time
set
(1
second );
Ny= length (t);
%
odd
number
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
yr =[ y1
yn
y2 ]';
%2
GMPs
signal
( column
vector )
y= hilbert (yr );
% analytical
signal
%SAF ------------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;

7.5 Time-Frequency Distributions
385
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
freq =-fN /2: fiv :( fN /2) - fiv;
te=t (end);
tim=-te /2: tiv:te /2;
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (tim ,freq , log10 (0 .005 + abs( SAF )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('SAF
of
the
2
GMPs
signal ');
% Energy
and
autocorrelations ----------------------
tcorr = zeros (1, Ny );
% temporal
autocorrelation
nt =1: Ny;
% vector
( for
indexes )
for
mtau =-md:md ,
aux=sum( zyz(Ny+nt).* conj ( zyz(Ny+nt -(2* mtau ))));
tcorr (md+ mtau +1)= tiv* aux;
end;
fcorr = zeros (Ny ,1);
% frequencial
autocorrelation
zerf = zeros (Ny ,1);
YW= fft(y,Ny )/ Ny;
ZYWZ =[ zerf ;YW; zerf ];
nf =1: fs;
% vector
( for
indexes )
mf=fs /2;
for
mtheta =-mf:mf ,
aux=sum( ZYWZ (fs+nf).* conj ( ZYWZ (fs+nf - mtheta )));
fcorr (mf+ mtheta +1)=( Ny/fs )* aux;
end;
of =( Ny +1)/2;
ot =( Ny +1)/2;
% SAF
origin
figure (2)
% frequencial
autocorrelation
plot (freq , abs( fcorr ),'rx ');
hold
on;
plot (freq , abs( SAF (:, ot )),'k');
xlabel ('Hz ');
title (' frequencial
autocorrelation ');
figure (3)
% temporal
autocorrelation
plot (tim , abs( tcorr ),'rx ');
hold
on;
plot (tim , abs(SAF(of ,:)) , 'k');
xlabel ('seconds ');
title ('temporal
autocorrelation ');
% print
y
signal
energy
disp ('signal
energy :')
o1= SAF(of ,ot)
The Program 7.8 also computes time and frequency autocorrelations, and com-
pares the obtained curves with the SAF values along its horizontal and vertical axes
(recall Eq.(7.70)). The results are shown in Figs.7.15 and 7.16. The comparison
shows very good agreement.

386
7
Time-Frequency Analysis
Fig. 7.15 Frequency
autocorrelation (2 GMP
signal)
-80
-60
-40
-20
0
20
40
60
80
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Hz
Fig. 7.16 Time
autocorrelation (2 GMP
signal)
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
seconds
7.5.4
From Wigner to SAF and Vice-Versa
The Wigner distribution and SAF are a Fourier transform pair, through a 2-
dimensional Fourier transform. For instance:
Ay(τ, θ) =
1
2π
∞

−∞
∞

−∞
WDy (t, ω) ej(ϖ τ −θ t) dω dt
(7.75)

7.5 Time-Frequency Distributions
387
Fig. 7.17 SAF obtained
from Wigner distribution
(2 GMP signal)
seconds
Hz
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-60
-40
-20
0
20
40
60
And,
WDy(t, ω) =
1
2π
∞

−∞
∞

−∞
Ay (τ, θ) ej(θ t −ω τ) dθ dτ
(7.76)
Notice that, by inverse Fourier transform:
ky(t, τ) =
1
2π
∞
−∞
WDy(t, ω) ejω τ dω =
=
1
2π
∞
−∞
Ay(τ, θ) ej θ t dθ
(7.77)
The Program 7.9 provides an example of how to obtain a Wigner distribution from
a SAF. The case considered is the two GMPs signal. The result of the program is
shown in Fig.7.17. Notice that we arrived to the same result shown in Fig.7.14.
Program 7.9 SAF from Wigner distribution of 2 GMPs signal
%
SAF
from
Wigner
distribution
of
2
GMPs
signal
clear
all
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set

388
7
Time-Frequency Analysis
t =0: tiv :1;
% complete
time
set
(1
second );
Ny= length (t);
% odd
number
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
yr =[ y1
yn
y2 ]';
%2
GMPs
signal
( column
vector )
y= hilbert (yr );
% analytical
signal
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/ Ny;
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
pks=WD;
% intermediate
variable
ax1= ifft (pks ,[] ,2);
% SAF
from
Wigner
distribution :
SAF= fftshift (fft(ax1 ,[] ,1) ');
% result
display
fiv=fN/Ny;
% frequency
interval
freq =-fN /2: fiv :( fN /2) - fiv;
te=t (end);
tim=-te /2: tiv:te /2;
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (tim ,freq , log10 (0 .005 + abs( SAF )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('SAF
of
the
2
GMPs
signal
( from
Wigner )');
To complete the example, now in the opposite sense the Program 7.10 provides
an example of how to obtain a SAF from a Wigner distribution. The case considered
is again the two GMPs signal. The result of the program is shown in Fig.7.18. Notice
that we obtained the same result shown in Fig.7.11.
Program 7.10 Wigner distribution from SAF, of 2 GMPs signal
%
Wigner
distribution
from
SAF ,
of
2
GMPs
signal
clear
all
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set
t =0: tiv :1;
% complete
time
set
(1
second );

7.5 Time-Frequency Distributions
389
Ny= length (t);
%
odd
number
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
yr =[ y1
yn
y2 ]';
%2
GMPs
signal
( column
vector )
y= hilbert (yr );
% analytical
signal
%SAF -------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
pks= ifftshift ( SAF );
% intermediate
variable
ax =(( ifft (pks ,[] ,1)));
% Wigner
from
SAF
distribution :
WD= real (( fft(ax ,[] ,2)) ');
% result
display
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .01+ abs(WD )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Wigner
dist.
of
2
GMPs
signal
( from
SAF)');
Fig. 7.18 Wigner
distribution obtained from
SAF, two GMPs signal
seconds
Hz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
50
100
150

390
7
Time-Frequency Analysis
7.5.5
About Interferences
The Wigner distribution has a problem: there are interferences. Some examples will
be shown below, and then a short mathematical discussion of interferences will be
done.
It is possible to attenuate or even eliminate the interferences: some basic examples
will be also introduced in a second part of this subsection.
7.5.5.1
Examples of Interferences
Based on a simple modiﬁcation of a part of the Program 7.2, a signal is built with
2 s of 10 Hz. sine, 6 s of zeros, and ﬁnally 2 s of 50 Hz. sine. In total 10 s of a
3-component signal. Figure7.19 shows the signal.
By means of the Program B.8, which has been listed in the Appendix B, we
obtain the Wigner distribution of the 2 sine signal. Figure7.20 shows the Wigner
distribution. There are 3 bands in the ﬁgure: on the left side a band corresponding to
10 Hz. sine, on the right a band corresponding to 50 Hz. Sine. The band in the centre
is interference.
The Program B.8 also generates the Figs.7.21 and 7.22, which show the time and
frequency marginals.
Consider now the quadratic chirp signal shown in Fig.7.23. The ﬁgure has been
generated with the Program 7.11.
0
5
10
15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10 15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
4 sec. 
1 sec. 
3 sec. 
0
5
10
15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
0
5
10 15
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
10 Hz
50 Hz
Fig. 7.19 Signal with 2 sine components

7.5 Time-Frequency Distributions
391
Fig. 7.20 Wigner
distribution of the signal
with 2 sine components
seconds
Hz
0
1
2
3
4
5
6
7
0
10
20
30
40
50
60
Fig. 7.21 Frequency
marginal of the Wigner
distribution (2 sine signal)
0
10
20
30
40
50
60
70
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Hz
Program 7.11 Quadratic chirp signal
% Quadratic
chirp
signal
f0 =5;
% initial
frequency
in
Hz
f1 =60;
% final
frequency
in
Hz
fs =5000;
% sampling
rate
in
Hz
tiv =1/ fs;
% time
between
samples
t1 =2;
% final
time
t =0: tiv :(t1 - tiv );
% time
intervals
set
(10
seconds )
f =0:1:(( fs /2) -1);
% frequency
intervals
set
yr= chirp (t,f0 ,t1 ,f1 ,'quadratic ')';
% the
chirp
signal
plot (t,yr ,'k');
title ('quadratic
chirp
signal ');
xlabel ('sec ');

392
7
Time-Frequency Analysis
Fig. 7.22 Time marginal of
the Wigner distribution
(2 sine signal)
0
1
2
3
4
5
6
7
8
-0.2
0
0.2
0.4
0.6
0.8
1
1.2
seconds
Fig. 7.23 Quadratic chirp
signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
sec
With little changes of the Program 7.7 we obtain the program B.9, also listed in
the Appendix B, to obtain the Wigner distribution of the quadratic chirp signal. The
result is shown in Fig.7.24. There is noticeable interference attached to the expected
quadratic curve, and two ‘shadows’ (two arcs) both sides of the main spot.
If we consider a two-component signal:
y(t) = a(t) + b(t)
(7.78)

7.5 Time-Frequency Distributions
393
Fig. 7.24 Wigner
distribution of the quadratic
chirp signal
seconds
Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
10
20
30
40
50
60
the Wigner distribution of this signal is given by:
WDy(t, ω) = WDa(t, ω) + WDb(t, ω) + WDab(t, ω) + WDba(t, ω) =
= WDa(t, ω) + WDb(t, ω) + 2 Re(WDab(t, ω))
(7.79)
where:
WDab(t, ω) =
∞

−∞
a

t + τ
2

b∗
t −τ
2

e−j ω τ dτ
(7.80)
is the cross-Wigner distribution.
The Eq.(7.79) contains an interference term, which is:
Iab(t, ω) = 2 Re(WDab(t, ω)
(7.81)
The main observation is that the quadratic nature of the Wigner distribution causes
interference terms. The interference has a geometry. Suppose two interfering points,
the interference is placed at the midpoint of a line joining the two points, and the
interference include oscillations perpendicular to this line and with a frequency pro-
portional to the distance between the two interfering points.
Recall that in the case of the SAF, the interferences appear out from the centre,
and the true information is shown in the center.
There are papers with detailed studies of interference geometry [63, 153], and
even with applications to music dissonance [48].

394
7
Time-Frequency Analysis
7.5.5.2
Basic Elimination of Interferences
There is a crude and simple way for interference elimination. The idea is to obtain
the SAF of the signal, then multiply the SAF by a mask of ones and zeros, to ﬁlter
out the interference terms and let pass only the information at the centre. After that,
the Wigner distribution can be obtained from the ﬁltered SAF, with no interferences.
Thisexperimenthasbeenmadeforthesignalwith2GMPs,Fig.7.10.TheProgram
7.12 applies the SAF mask ﬁltering and then obtains the Wigner distribution.
seconds
Hz
Filter window (kernel)
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-50
0
50
seconds
Hz
Filtered SAF
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
-50
0
50
Fig. 7.25 Kernel and ﬁltered SAF for 2 GMP signal
Fig. 7.26 Filtered Wigner
distribution for 2 GMP signal
seconds
Hz
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
0
20
40
60
80
100
120
140

7.5 Time-Frequency Distributions
395
Figure7.25 shows on top the mask, and the ﬁltered SAF at the bottom. Essentially,
we are ﬁltering Fig.7.14. We applied the term ‘Kernel’ to the mask.
Figure7.26 shows the Wigner distribution we obtain from the ﬁltered SAF. There
are no interferences.
Both Figs.7.25 and 7.26 have been generated with the Program 7.12.
Program 7.12 WD of 2 GMPs signal, with no interference
%
WD
of
2
GMPs
signal ,
with
no
interference
clear
all
%
2
GMPs
signal
fy1 =40;
% signal
1
central
frequency
in
Hz
fy2 =80;
% signal
2
central
frequency
bw =0 .2;
% signal
relative
bandwidth
fs =300;
% sampling
frequency
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .4
seconds ):
tp = -(0.2 - tiv ): tiv :(0.2 - tiv );
Np= length (tp );
y1= gauspuls (tp ,fy1 ,bw );
% signal
1
data
set
y2= gauspuls (tp ,fy2 ,bw );
% signal
2
data
set
t =0: tiv :1;
% complete
time
set
(1
second );
Ny= length (t);
%
odd
number
yn= zeros (1,Ny -(2* Np ));
% intermediate
signal
yr =[ y1
yn
y2 ]';
%2
GMPs
signal
( column
vector )
y= hilbert (yr );
% analytical
signal
%SAF ------------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
%A
simple
box
distribution
kernel
FI= zeros (Ny ,Ny );
% window
vertical
and
horizontal
1/2
width :
HV =50;
HH =70;
FI(md -HH:md+HH ,md -HV:md+HV )=1;
% box
kernel
% Product
of
kernel
and
SAF
fsaf = FI.* SAF;
pks= ifftshift ( fsaf );
% intermediate
variable
ax =(( ifft (pks ,[] ,1)));
% Wigner
from
SAF
distribution :
WD= real (( fft(ax ,[] ,2)) ');
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
freq =-fN /2: fiv :( fN /2) - fiv;

396
7
Time-Frequency Analysis
te=t (end);
tim=-te /2: tiv:te /2;
colmap1 ;
colormap ( mapg1 );
% user
colormap
subplot (2 ,1 ,1)
imagesc (tim ,freq , log10 (0 .005 + abs(FI )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Filter
window
( kernel )');
subplot (2 ,1 ,2)
imagesc (tim ,freq , log10 (0 .005 + abs( fsaf )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Filtered
SAF ');
% result
display
figure (2)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .01+ abs(WD )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Filtered
Wigner
distribution
of
the
2
GMPs
signal ');
The experiment has been repeated for the quadratic chirp shown in Fig.7.23.
Figure7.27 shows the mask to be applied and the ﬁltered SAF in this case.
Figure7.28 shows the Wigner distribution obtained from the ﬁltered SAF. The
most disturbing interferences have been eliminated.
The program for this experiment, Program B.10 (also in Appendix B), is very
similar to the Program 7.12.
seconds
Hz
Filter window (kernel)
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-20
0
20
seconds
Hz
Filtered SAF
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-20
0
20
Fig. 7.27 Kernel and ﬁltered SAF for chirp signal

7.5 Time-Frequency Distributions
397
Fig. 7.28 Filtered Wigner
distribution for chirp signal
seconds
Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
10
20
30
40
50
60
An important comment here is that the mask has been tailored in each particular
caseafterlookingattheSAF.Thatmeansasignal-dependentmethod.Otherexamples
of signals would require other masks.
7.5.6
Smoothing of the Wigner Distribution
In order to attenuate interferences of the Wigner distribution, a ﬁlter can be included
in the integrand. The ﬁlter can be unidimensional, or bidimensional.
7.5.6.1
1 D Filtering. Pseudo Wigner Distribution
A window h(τ), for instance a Hamming window, is simply put in the Wigner dis-
tribution integrand as follows:
PWDy(t, ω) =
∞

−∞
h(τ) y

t + τ
2

y∗
t −τ
2

e−j ω τ dτ
(7.82)
The result is called pseudo Wigner distribution. An alternative expression is the
following:
PWDy(t, ω) =
1
2π
∞

−∞
H(ω −θ) WDy(t, θ) dθ
(7.83)

398
7
Time-Frequency Analysis
The effect of the ﬁlter H(ω), which is the Fourier transform of h(τ), is to attenuate
oscillating interferences.
7.5.6.2
2 D Filtering. Smoothed Wigner Distribution
A bidimensional ﬁlter can be included in the Wigner distribution integrand as follows:
SWDy(t, ω) =
1
2π
∞

−∞
∞

−∞
(ψ(t′ −t, ω′ −ω) WDy (t′, ω′) dω′) dt′
(7.84)
where ψ(t, ω) is a low-pass ﬁlter in the time-frequency domain. The result is called
smoothed Wigner distribution.
In general low-pass 1 D or 2 D ﬁltering can attenuate and even suppress interfer-
ences. But this will reduce the resolution.
Better control of the smoothing can be gained using a separable smoothing, so
that it is easy to specify time and frequency characteristics:
ψ(t, ω) = g(t) H(ω)
(7.85)
And then:
SWDy(t, ω) =
∞

−∞
h(τ)
∞

−∞
g(t′ −t) ky (t′, τ) dt′ e−jωτdτ
(7.86)
Some authors call this expression the smoothed-pseudo Wigner distribution (SPWD).
See [83, 144] and references therein for the SPWD and other schemes for inter-
ference attenuation.
An interesting example of separable smoothing is given by:
ψ(t, ω) = exp(−α t2 −βω2)
(7.87)
It is shown that for αβ ≥1 the SPWD has non-negative values.
Recall from (7.30) and (7.31) that the spectrogram is the squared modulus of
the STFT, and that the STFT uses a window w(τ −t). The case just described, the
exponential with αβ = 1, is a possible spectrogram window. In general, it can be
shown using Moyal’s formula, that the spectrogram can be written as follows:
SFy(t, ω) =
1
2π
∞

−∞
∞

−∞
WDw(t′ −t, ω′ −ω) WDy (t′, ω′) dω′ dt′
(7.88)

7.5 Time-Frequency Distributions
399
where WDw is the Wigner distribution of the window. Therefore, the spectrogram is
a SWD.
Since the spectrogram is a smoothed Wigner distribution, it has less resolution
than the Wigner distribution.
It is also shown that the scalogram can be written in terms of the Wigner distrib-
ution, as follows:
Scy(τ, s) =
1
2π
∞

−∞
∞

−∞
WDψ
	t −τ
s
, sω

WDy (t, ω) dω dt
(7.89)
where ψ in WDψ is the wavelet.
7.6
Signal Representation
Depending on the purposes of the study of a signal, one or another feature should be
highlighted. Recall what has been done in Sect.2.6, where logarithmic axes where
used to decide whether a certain distribution was normal or Weibull. Likewise, we
could use change of variables or reference axes, perhaps in the time-frequency plane,
to have a clearer view of certain signal characteristics. In many cases, it is opportune
to use decomposition with a suitable basis of functions, in the vein of the Fourier
transform or the Gabor transform.
Therefore, one of the ﬁrst questions in the study of a particular type of signals is
to select a suitable signal representation, according with the actual targets.
It is time to recapitulate about the previous sections, which already introduced
several analysis tools in the time-frequency domain. More tools, with similar spirit,
will be derived in the next section. Each of these analysis tools provides a signal
representation. It happens that due to uncertainty there are many alternatives to map
a signal into a time-frequency plane, to show frequency components at a time point
and to see during what time a frequency component exists.
7.6.1
Types of Representations
There are linear and nonlinear time-frequency representations (TFR) [82]. A partic-
ular case of nonlinear TFRs is quadratic TFRs. An important example of quadratic
TFR is the Wigner distribution.

400
7
Time-Frequency Analysis
7.6.1.1
Linear TFRs
Given a linear combination of two signals:
y(t) = c1 y1(t) + c2 y2(t)
(7.90)
A linear TFR will possess the superposition property:
Ty(t, ω) = c1 Ty1(t, ω) + c2 Ty2(t, ω)
(7.91)
Examples of linear TFR are the STFT, the Gabor transform, and the wavelet trans-
form.
7.6.1.2
Quadratic TFRs
Consider again a linear combination of two signals, as given in Eq.(7.90). A quadratic
TFR corresponding to the combined signal would be:
Ty(t, ω) = c2
1 Ty1(t, ω) + c2
2 Ty2(t, ω) + c1 c∗
2Ty1y2(t, ω) + c2 c∗
1Ty2y1(t, ω)
(7.92)
Then, in addition to components corresponding to y1(t) and y2(t) there would be
cross-term interferences. These interferences cause difﬁculties for the interpretation
of graphical results. We already have seen interferences in recent ﬁgures.
7.6.2
Analysis Approaches
In general, in the time-frequency (TF) analysis one wants to see the contribution of
each TF point to the signal y(t). A common approach—as in the linear TFRs already
seen—is to use the inner product of an analysis function ξp(t) and the signal:
< y, ξp(t) > =
∞

−∞
y(t) ξp(t) dt
(7.93)
where p is a point of the TF plane.
Figure7.29 illustrates the approach. The analysis function ξp(t) has non-zero
values in a suitable (small) region.
Now, the idea is to use a family of analysis functions, corresponding to several
points in the TF plane. Two alternatives have been described so far:

7.6 Signal Representation
401
Fig. 7.29 Analysis in the TF
plane using inner product
time
frequency
ξp
y
7.6.2.1
Frequency Shift + Time Shift of ξp(t):
See Fig.7.30:
Examples of this approach are the STFT and the Gabor transform.
7.6.2.2
Scaling + Time Shift of ξp(t):
See Fig.7.31:
An example of this approach is the wavelet transform.
The ﬁrst alternative, frequency shift + time shift, corresponds to a tiling of the
TF plane with equal elements, as shown in Fig.7.32.
The second alternative, scaling + time shift, corresponds in the case of discrete
wavelets (to be studied in detail in another chapter) to a tiling with Q-constant, same
area elements. Figure7.33:
Fig. 7.30 Frequency shift +
time shift of the analysis
function
time
frequency
ξp0
ξp1
ξp2

402
7
Time-Frequency Analysis
Fig. 7.31 Scaling + time
shift of the analysis function
time
frequency
ξp
ξp1
ξp2
Fig. 7.32 Tiling of the TF
plane corresponding to the
ﬁrst alternative
time
frequency
From a more general perspective, the signal representation matters, and the associ-
ated techniques, has connections with modern physics (relativity, quantum mechan-
ics, orbits, etc.). Likewise it has an evident interest for mathematicians involved
in harmonic analysis and related topics. As we shall see in a moment, operators,
geometry, enter into scene.
7.6.3
Basic Time-Frequency Operators
Time shifting, frequency shifting, and scaling in the time-frequency domain can be
described with operators [73].

7.6 Signal Representation
403
Fig. 7.33 Tiling of the TF
plane corresponding to the
second alternative
time
frequency
Let us introduce operators and notation:
• Denote as Tλ the translation operator:
(Tλ f )(x) = f (x −λ)
(7.94)
• Denote as Mμ the modulation operator:
(Mμ f )(x) = ej μ xf (x )
(7.95)
• And denote as Ds the dilation (or scaling) operator:
(Ds f )(x) =
1
√|s|f
x
s

(7.96)
In general, given an operator Q and a function f , the notation (Qf )(x) means the
transformed f evaluated at x.
With this notation, important Fourier transform properties can be expressed in a
very compact format. The Fourier transform of f (x) would be denoted as follows:
(F f )(x) = ˆf (x)
(7.97)

404
7
Time-Frequency Analysis
Then (shortening the notation by assuming a certain x):
F (Tλf ) = Mλˆf ,
F (Mμf ) = Tμˆf
(7.98)
And
F (Dsf ) = D1/sˆf
(7.99)
In the equations above the Fourier transform is also an operator.
Notice in Eqs.(7.94)–(7.97) that x in f (x) could be as well time or frequency.
Shortening a bit more the notation, one can write, as equivalent to (7.98):
F Tλ = MλF ,
F Mμ = Tμ F
(7.100)
7.6.4
Geometric Transformations
Coming back to the use of an analysis function ξp(t), the TF analysis can be inter-
preted like using a lens, with a certain shape, to examine in detail parts of the TF plane.
In order to move the lens, or perhaps the signal, it is beneﬁcial to recall concepts
from geometric transformations in 2D.
7.6.4.1
Linear Transformations
Linear transformations of a vector x can be obtained with:
y = A x
where A is a 2x2 matrix.
Examples of linear transformations:
• Clockwise rotation:
	 y1
y2

=
	 cos ϕ sin ϕ
−sin ϕ cos ϕ

 	 x1
x2

(7.101)
• Scaling (enlarge or shrink):
	 y1
y2

=
	 s1 0
0 s2

 	 x1
x2

(7.102)

7.6 Signal Representation
405
• Shearing:
	 y1
y2

=
	 1 k
0 1

 	 x1
x2

parallel to the x axis
	 y1
y2

=
	 1 0
k 1

 	 x1
x2

(7.103)
parallel to the y axis
Linear transformations keep the origin ﬁxed.
7.6.4.2
Afﬁne Transformations
Afﬁne transformations are important in the time-frequency context. In general, an
afﬁne transformation is composed of some (or none) linear transformation, such as
rotation or scaling, and translation (shift). Therefore:
y = A x + b
(7.104)
Afﬁne transformations map straight lines to straight lines.
Iff det|A| = 1 the transformation preserves area.
A particular case is the afﬁne time transformation, which can be seen as a clock
change:
t →α t + β
(7.105)
In the STFT, the analysis function ξp(t) (the window wnm(t)) is translated from the
point (t0,ω0) to the point (t1, ω1) according with:
	t1
ω1

=
	 1 0
0 1

 	 t0
ω0

+
	Δt
Δω

(7.106)
In the wavelet, the analysis function ξp(t) is moved according with:
	 t1
ω1

=
	s 0
0 1/s

 	t0
ω0

+
	Δt
0

(7.107)
The afﬁne transformation can be expressed in ‘homogeneous coordinates’, as fol-
lows:
	 y
1

=
	A
b
0 0 1

 	 x
1

(7.108)
Equation(7.106) has the form of a linear transformation.

406
7
Time-Frequency Analysis
An interesting example of homogeneous coordinates is quaternions for 3D
applications.
7.6.5
Some Important Types of Matrices
There are two important types of matrices in the context of time-frequency studies:
unitary matrices and hermitian matrices.
A unitary matrix is a n × n complex matrix A such that:
A+ · A = A · A+ = I
(7.109)
Which is equivalent to:
A−1 = A+
(7.110)
where I is the identity matrix, and A+ is the conjugate transpose (the adjoint) of A.
Every unitary matrix A can be decomposed as:
A = V B V
(7.111)
With V unitary, and B diagonal and unitary.
Two important properties of unitary matrices are that:
| det A| = 1
(7.112)
∥A x∥2 = ∥x∥2
(7.113)
According with the second property, A preserves length (isometry).
A hermitian matrix is a n × n complex matrix H such that:
H = H+
(7.114)
Example of hermitian matrix:
	5
3 + 2i
3 −2i
7

(7.115)
Numbers in the main diagonal are real. All matrix eigenvalues are real.
A hermitian matrix can be diagonalized by a unitary matrix.
Matrix diagonalization is related with:
C vk = λk vk
(7.116)
where vk are eigenvectors and λk are eigenvalues.

7.6 Signal Representation
407
7.6.6
Linear Operators
The transformations made with matrices constitute an intuitive introduction to
operators. Nevertheless, operators are more general: they can be associated to matri-
ces, but also to integrals, derivatives, etc.
Many time-frequency studies work on a Hilbert space: a complete linear space
with an inner product. In many cases the action of a linear operator H on x(t) is an
integral like the following:
H x(t) =

h(t, τ) x(τ) dτ
(7.117)
with h(t, τ) the kernel function (impulse response).
In some cases it is possible to establish an eigenequation with the form of (7.116),
and to obtain eigenvalues and eigenfunctions.
Given an operator H, the adjoint operator H+ is deﬁned by:
< H x, y > = < x, H+y >
(7.118)
Operators such that:
H = H+
(7.119)
are called self-adjoint.
Any self-adjoint operator has an orthonormal basis, formed by eigenfunctions, in
which the operator can be represented as a diagonal matrix. The diagonal is formed
with the eigenvalues, which are real numbers.
Self-adjoint operators are used in quantum physics, and denoted as hermitian
operators, for observables like position or momentum. Frequency is another observ-
able.
An inner product induces a norm:
∥x∥= √< x, x >
(7.120)
Bounded linear operators satisfy:
∥H x∥≤M ∥x∥
(7.121)
with ﬁnite M.
The bilinear form of an operator is given by:
QH(x, y) = < H x, y >
(7.122)

408
7
Time-Frequency Analysis
Based on (7.117), the bilinear form can be written as follows:
QH(x, y) =
 
h(t, τ) x(τ)y∗(τ) dτ
(7.123)
In the case of y = x, the bilinear form is called quadratic form.
The quadratic form of a hermitian operator is always real-valued.
Unitary operator U is a bounded linear operator on a Hilbert space such that:
U+ · U = U · U+ = I
(7.124)
Unitary operators preserve the inner product:
< Ux, Uy > = < x, y >
(7.125)
Therefore:
∥U y∥2 = ∥y∥2
(7.126)
That is, unitary operators preserve energy.
It is important to stress here that the basic time-frequency operators mentioned in
Sect.7.6.1. are unitary operators.
Any unitary operator can be written as:
U = exp(j H)
(7.127)
where H is a hermitian operator.
The eigenvalues of unitary operators are complex numbers on the unit circle.
Projection operators are idempotent:
P2 = P
(7.128)
If P is hermitian the projection is orthogonal, otherwise the projection is oblique.
Linear time-invariant systems are systems that commute with Tτ (time shift):
G Tτ = Tτ G
(7.129)
The symplectic group Sp in a two-dimensional space consists of all matrices that
correspond to area-preserving linear geometric transformations, so detA = 1. The
corresponding group operation is matrix multiplication. Some examples of matrices
belonging to Sp are the following:
B =
	1 b
0 1

, C =
	1 0
c 1

Dd =
	 1/d 0
0
d

, R =
	 cos θ
−sin θ
sin θ
cos θ

(7.130)

7.6 Signal Representation
409
Any matrix A belonging to Sp can be written in at least one of the following products:
let A =
	p r
s q

A = C D1/d B, with c = s
p, b = r
p
(7.131)
A = C1 B C2, with c1 = q −1
r
, c2 = p −1
r
(7.132)
A = B1 C B2, with b1 = p −1
s
, b2 = q −1
s
(7.133)
A = B Dd C, with b = r
q, c = s
q
(7.134)
To each symplectic matrix A, one can associate a unitary operator:
U = μ(A)
(7.135)
The mapping:
A →μ(A)
(7.136)
is called a metaplectic representation of Sp. The operator U is called a metaplectic
operator.
Composition property:
μ(A1 A2) = μ(A1) μ(A2)
(7.137)
Examples of metaplectic operators:
• The dilation operator, which corresponds to A = Dd.
• The Fourier transform
Recall the ‘clock change’ (7.105) (time afﬁne transformation). This clock change
can be represented with an operator Uαβ. Notice that the composition of two clock
changes is another clock change: the set (α , β) is a group (an afﬁne group) with
a group operation that corresponds to the composition of two clock changes. The
mapping to Uαβ is a unitary representation of this group.
The term ‘symplectic’ is associated with planetary mechanics, many-body prob-
lems, billiards in planetary or galactic systems, etc. The origins of symplectic maps
can be traced back to H. Poincaré, who proposed (with an incomplete proof) a the-
orem related to area preserving maps, one year before his death in 1912. The book
[70] on symplectic twist maps (existence of periodic orbits) includes more historical
detail. Besides, the symplectic group appears in Quantum mechanics, and comes by
the hands of Heisenberg and Gabor to our time-frequency ﬁeld [73].

410
7
Time-Frequency Analysis
A brief academic text on linear symplectic transformations is offered by [85]. The
decomposition into products of matrices is treated in [21].
7.6.7
Covariance
Among the properties of the Wigner distribution, there was a mention to translation
covariance and dilation covariance (vid. Sect.7.5.2). In preparation of next sections it
is convenient to consider covariance in more detail. If the reader looks to the scientiﬁc
literature on time-frequency analysis, he might ﬁnd some confusion about terms like
invariance or covariance, so our wish here is to clarify concepts.
It is interesting to note here connections with relativity theory, where covariant
and contravariant variables are considered.
Suppose there is a signal y(x), a transform S, and a parameterized operator Qα
acting on the signal.
The transform S is invariant to the operator Qα if:
(S Qα y)(x) = (S y)(x)
(7.138)
(for instance: translation or dilation invariance).
Take a translation operator Tλ . The transform S is covariant to the operator Tλ if:
(S Tλ y)(x) = (S y) (x −λ)
(7.139)
This translational covariance could refer as well to time-shift and to frequency-shift
covariance. In the case of translational covariance, a change of η in the variable
corresponds to a translation in the signal representation by η.
Let Pθτ be a translation operator causing a time-frequency shift. The transform S
is covariant to the operator Pθτ if:
(S Pθτ y)(t, ω) = (S y)(t −τ, ω −θ)
(7.140)
Take a dilation operator Ds. The transform S is covariant to the operator Ds if:
(S Ds y)(x) =
1
√|s|(S y)
x
s

(7.141)
Covariance is important since signal changes, like for instance frequency shifts, are
pictured as noticeable position changes in the transform plot.
Other examples of invariance and covariance exist, like for instance rotation
covariance or translation invariance.

7.6 Signal Representation
411
There is an important observation: translation and modulation operators do not
commute:
(Tλ Mμ y)(x) = Tλ ejμ x y(x) = ejμ (x−λ) y(x −λ) =
= ejμ x e−jμ λ y(x −λ) = e−jμ λ ejμ x Tλ y(x) = e−jμ λ Mμ Tλ y(x)
(7.142)
This fact is signiﬁcant or not, depending on the application. The magnitude of
exp(−jμλ) is 1, so it is not observed in many time-frequency plots (some authors
consider it as invariance to phase).
The short time Fourier transform STFT can be written as follows:
< y, m > = < y, Mθ Tτ w > =
∞

−∞
y(t) w∗(t −τ) e−jθ t dt
(7.143)
(this expression has minor changes with respect to the expression in Sect.7.4.1),
Denote as STF the STFT as operator. Then:
(STF Tτ y)(t, ω) = (STF y)(t −τ, ω) e−jθ τ
(STF Mθ y)(t, ω) = (STF y)(t, ω −θ)
(7.144)
The STFT is covariant to the modulation operator, and to time-shift if phase is not
taken into account.
The continuous wavelet transform can be written as:
< y, l > = < y, Tτ Ds g > =
1
√|s|
∞

−∞
y(t) g∗
	t −τ
s

dt
(7.145)
Denote as WLT the wavelet transform as operator. Then:
(WLT Tτ y)(t, s) = (WLT y)(t −τ, s)
(WLT Da y)(t, s) = (WLT y)
 t
a, as

(7.146)
The continuous wavelet transform is covariant to the dilation operator, and to time-
shift.
There is a number of interesting papers, published in the middle of the 90s, which
introduce the operator point of view together with covariance considerations, for
instance [81, 154]. Later on, [145] provides a settled view of this initiative.
Imposing covariance conditions to the quadratic time-frequency distributions, it
has been found that all quadratic distributions covariant to time-shift and frequency
shift belong to the Cohen’s class; and all quadratic distributions covariant to time-
shift and to dilation (scaling) belong to the afﬁne class. The next section is devoted
to these two classes.

412
7
Time-Frequency Analysis
7.7
The Cohen’s Class and the Afﬁne Class
In this section, two main types of quadratic time-frequency distributions will be
introduced, namely the Cohen’s class and the afﬁne class. These distributions include
many particular cases, some of them already described in this chapter.
A suitable book for this section is precisely due to Cohen [bC].
7.7.1
The Cohen’s Class
Many types of distributions can be considered as particular cases of the Cohen’s class
of distributions.
Here are four equivalent expressions of the Cohen’s class
CDy(t, ω) =
∞
−∞
∞
−∞
(ϕ(t −t′, τ) ky (t′, τ) dt′) e−j ω τ dτ =
=
1
4π2
∞
−∞
∞
−∞
(Φ(ω −ω′, θ) χy (ω′, θ) dω′) ej t θ dθ =
=
1
2π
∞
−∞
∞
−∞
(ψ(t′ −t, ω′ −ω) WDy (t′, ω′) dω′) dt′ =
=
1
2π
∞
−∞
∞
−∞
(Ψ (τ, θ) Ay (τ, θ) ej(θ t −ϖ τ) dθ) dτ
(7.147)
Take for instance one of these expressions:
CDy(t, ω) =
1
2π
∞

−∞
∞

−∞
(Ψ (τ, θ) Ay (τ, θ) ej(θ t −ϖ τ) dθ) dτ
(7.148)
This expression corresponds to the basic way of interference elimination described
in Sect. 7.5.5.2. Inside the integral there is the multiplication of the kernel Ψ (τ, θ)
and the SAF.
If we choose the equivalent expression:
CDy(t, ω) =
1
2π
∞

−∞
∞

−∞
(ψ(t′ −t, ω′ −ω) WDy (t′, ω′) dω′) dt′
(7.149)
This can be seen as a smoothed Wigner distribution, using as low-pass ﬁlter the
kernel ψ(t, ω).
The kernels of the four Cohen’s class expressions are Fourier transform pairs:
ϕ(t, τ) ⇔Φ (ω, θ) ;
ψ(t, ω) ⇔Ψ (τ, θ)
(7.150)

7.7 The Cohen’s Class and the Afﬁne Class
413
φ(t,τ)
Φ(ω,θ)
Ay
Ψ(τ,θ)
ψ(t,ω)
WDy
Ky
χy
t
t
t
τ
τ
τ
ω
ω
ω
ω
t
τ
θ
θ
θ
θ
Fig. 7.34 Fourier relationships between kernels (Cohen’s class)
Figure7.34 presents a diagram showing how the four kernels are connected by
Fourier transforms.
7.7.1.1
Properties of the Cohen’s Class
The actual properties of a member of the Cohen’s class depend on the kernel. Taking
for instance the kernel Ψ (τ, θ):
• Time covariance: Ψ (τ, θ) independent of t
• Frequency covariance: Ψ (τ, θ) independent of ω
• Real-valued: Ψ (τ, θ) = Ψ (−τ, −θ)
• Time marginal: Ψ (0, θ) = 1
• Frequency marginal: Ψ (τ, 0) = 1
• Instantaneous frequency: Ψ (0, θ) = 1 and
∂
∂τ Ψ (τ, θ)

τ=0 = 0
• Group delay: Ψ (τ, 0) = 1 and
∂
∂θΨ (τ, θ)

θ=0 = 0
In many cases, some of the properties are sacriﬁced in favour of more attenuation
of interferences.
7.7.1.2
Members of the Cohen’s Class
All quadratic distributions covariant to time shift and frequency shift belong to the
Cohen’s class.
The Wigner distribution is a distinguished member. It corresponds to the kernel
Ψ (τ, θ) = 1.

414
7
Time-Frequency Analysis
The smoothed Wigner distribution, the smoothed-pseudo Wigner distribution, and
the spectrogram also belong to the Cohen’s class.
Although the ﬁrst to derive its expression was Cohen, the distribution with the
following kernel is usually named as the Born–Jordan distribution:
Ψ (τ, θ) = sin(τθ/2)
(τθ/2)
(7.151)
The Choi–Williams distribution is obtained with the following kernel:
Ψ (τ, θ) = exp(−α (τθ)2)
(7.152)
The distribution of Choi–Williams satisﬁes all the properties listed in Sect.7.7.1.1.
The value of the kernel at the origin is 1. The parameter α controls the extension
of the kernel: for large values the extension is small, and the risk of loosing true
information increases. When α goes to zero, the distribution converges to Wigner.
The cone-shape distribution corresponds to the following kernel:
Ψ (τ, θ) = sin(τθ/2)
(τθ/2)
exp(−α τ 2)
(7.153)
Again, in the cone-shape distribution the parameter α controls the extension of
the kernel. Unlike the Choi–Williams distribution, the cone-shape distribution sup-
presses values of the SAF on the horizontal axis. Signal components having their
centres on the same frequency, cause interferences that will appear on the SAF hor-
izontal axis: these interferences will be eliminated.
The Rihaczek distribution, also denoted as Kirwood distribution, corresponds to
the following complex kernel:
Ψ (τ, θ) = exp(j τθ/2)
(7.154)
The Margenau–Hill distribution corresponds to the following kernel, which is the
real part of the Rihaczek distribution kernel:
Ψ (τ, θ) = cos(τθ/2)
(7.155)
The Page distribution corresponds to the following kernel:
Ψ (τ, θ) = exp(j |τ| θ)
(7.156)

7.7 The Cohen’s Class and the Afﬁne Class
415
Fig. 7.35 The
Choi–Williams kernel
Figure7.35 shows a 3D view of the Choi–Williams kernel. The ﬁgure has been
generated with the Program 7.13: it is easy to play with changes in the parameter α
and the time and frequency variables to see how the kernel shape changes.
Program 7.13 The Choi–Williams kernel
%
The
Choi - Williams
kernel
x= linspace (0 ,1) -0 .5;
% 100
values
between
-0.5
and
0.5
theta =x *100*2* pi;
% frequencies
tau=x *1;
% delays
alpha =0 .001 ;
% parameter
Psi= zeros (100 ,100);
for
n =1:100 ,
for
m =1:100 ,
p=- alpha *( tau(n )^2)*( theta (m )^2);
Psi(n,m)= exp(p);
end;
end;
mesh ( Psi );
view (30 ,60);
%3D
plot
set(gca ,' XTickLabel ',
{' -0.5 ';' -0.3 ';' -0.1 ';'0.1 ';
'0.3 ';
'0.5 '});
set(gca ,' YTickLabel ',
{' -50';' -30 ';' -10 ';'10 ';
'30 ';
'50 '});
title ('Choi - Williams
kernel ');
xlabel ('sec ');
ylabel ('Hz ');

416
7
Time-Frequency Analysis
7.7.2
The Afﬁne Class
There are four equivalent expressions of the afﬁne class
ADy(t, ω) =
|ω|
2π
∞
−∞
∞
−∞
(ϕA(ω(t −t′), ωτ) ky (t′, τ) dt′) dτ =
=
1
2π|ω|
∞
−∞
∞
−∞

ΦA

ω′
ω , θ
ω

χy (ω′, θ) dω′
ej t θ dθ =
=
1
2π
∞
−∞
∞
−∞

ψA

ω(t −t′), ω′
ω

WDy (t′, ω′) dω′
dt′ =
=
1
2π
∞
−∞
∞
−∞

ΨA

ωτ, θ
ω

Ay (τ, θ) ejθ t dθ

dτ
(7.157)
The four kernels are connected by Fourier transforms in the same manner as in
Fig.7.34.
7.7.2.1
Members of the Afﬁne Class
All quadratic distributions covariant to scaling and to time shift belong to the afﬁne
class.
A representative member of the afﬁne class is the scalogram.
Notice that the scalogram can be seen as an afﬁne smoothing of the Wigner
distribution.
Actually, a member of the afﬁne class is the afﬁne-smoothed pseudo Wigner
distribution (ASPWD):
ASPWDy(t, ω) = 1
s
∞

−∞
∞

−∞
h
τ
s

g
	t′ −t
s

ky (t′, τ) dt′ dτ
(7.158)
This distribution includes a separable kernel, as in the case of the SPWD (vid. 7.86).
The Wigner distribution itself is also a member of the afﬁne class. That means
that there is an intersection set of distributions, belonging to the Cohen’s class and
to the afﬁne class.
7.7.2.2
Localized Bi-frequency Kernel Distributions
There is a particular formulation of the afﬁne class for the case in which it is opportune
to use a curve H(υ) in the local auto-correlation (we employ here the frequency υ in
Hz):

7.7 The Cohen’s Class and the Afﬁne Class
417
ADy(t, υ) =
1
|s|
∞

−∞
G(υ) Y
	H(υ) −(υ/2)
s

Y∗
	H(υ) + (υ/2)
s

e −jυ t/sdυ
(7.159)
where G(υ) is any function.
The Bertrand distribution corresponds to:
G(υ) =
υ/2
sinh(υ/2) ; H(υ) = υ
2 coth
υ
2

(7.160)
An interesting ﬁeld of application for this distribution is for signals with hyperbolic
group delay (the group delay is a hyperbola in the time-frequency plane). Nature is
wise: bats use this kind of signals, which are very appropriate for target localization.
The D-Flandrin distribution corresponds to:
G(ω) = 1 −(υ/4)2 ; H(υ) = 1 + (υ/4)2
(7.161)
This distribution is appropriate for signal with group delay proportional to 1/√υ.
The Time-Frequency Toolbox, and its companion tutorial [13] give more details
and pertinent references concerning the Bertrand and the D-Flandrin distributions.
7.7.3
Classiﬁcation of TFRs
It seems opportune to summarize the last sections with a brief classiﬁcation of the
TFRs we have seen.
There are two main groups: linear and non-linear.
7.7.3.1
Linear TFRs
• STFT
• Gabor
• Wavelet
7.7.3.2
Non-linear TFRs
• Cohen’s class:
– Wigner distribution
– SWD
– SPWD
– Spectrogram
– Born–Jordan distribution
– Choi–Williams distribution

418
7
Time-Frequency Analysis
– Cone-shape
– Rihaczec
– Margenau–Hill
– Page
• Afﬁne class:
– Wigner distribution
– ASPWD
– Scalogram
– Bertrand distribution
– D-Flandrin
All these TFRs are signal-independent. There exist other analytic tools that adapt
to the signal.
Two important and extensive reviews of TFRs are [39, 82]. It is also worthwhile
to consult the tutorial [13] and play with the demos of the Time-Frequency Toolbox
(see the Resources section for the web site address).
7.8
Linear Canonical Transformation
The linear canonical transformation (LCT) is a general transform that includes as
particular cases many important operators and transforms. The particular cases are
speciﬁed by giving values to a set of parameters.
Pertinent references about the LCT are [35, 78, 80, 158]. A fast algorithm is
proposed in [36].
Let us write a 2x2 matrix:
M =
	a b
c d

(7.162)
Then, if b is nonzero, the LCT is deﬁned as follows:
(LM y)(u) = < y, C∗
M (u, x) > =
∞

−∞
y(x) CM(u, x) dx
(7.163)
where the kernel is:
CM(u, x) =

1
j b exp
 j
2 b (a x2 −2x u + d · u2)

(7.164)
For b = 0, the LCT is deﬁned as the limit for |b| →0, and therefore:
(LMy) (u) =
√
d exp
 j
2 (c d · u2)

y(d · u)
(7.165)

7.8 Linear Canonical Transformation
419
7.8.1
Particular Cases
Let us give certain speciﬁc values to a, b, c, d. The results we are going to obtain
have evident connections with the geometric transformations in Sect.7.9.1.
7.8.1.1
Fourier Transform
Take:
M =
	 0
1
−1
0

(7.166)
With these parameters, the LCT kernel is:
CM(u, x) =

1
j exp
 j
2 (−2x u )

=

1
j e−j x u
(7.167)
Substitution in (7.163) gives, with a factor, the Fourier transform.
The Fourier transform causes a rotation by 90◦in the time-frequency plane. This
is coherent with the fact that the matrix M corresponds to a 90◦rotation
7.8.1.2
Fractional Fourier Transform
Now choose:
M =
	 cos ϕ
sin ϕ
−sin ϕ
cos ϕ

(7.168)
With these parameters, the LCT kernel is:
CM(u, x) =

1
j sin ϕ exp

j
2 sin ϕ (cos ϕ · x2 −2x u + cos ϕ · u2)

(7.169)
Hence:
(LM y)(u) =

1
j sin ϕ exp
	j
2 cot ϕ · u2

·
·
∞

−∞
exp

j
2 sin ϕ (cos ϕ · x2 −2x u )

· y(x) dx
(7.170)
This last expression is the fractional Fourier transform. This transform causes a
rotation by an arbitrary angle in the time-frequency plane.

420
7
Time-Frequency Analysis
The fractional Fourier transform is being successfully applied to optics and other
ﬁelds dealing with waves [4, 119, 130].
7.8.1.3
Fresnel Transform
Take:
M =
	
1
λ z
2π
0
1

(7.171)
where z is distance and λ is wavelength. The matrix corresponds to shearing.
With these parameters, the LCT kernel is:
CM(u, x) =

2π
j λ z exp
 j π
λ z (x2 −2x u + u2)

(7.172)
And then:
(LM y)(u) =

2π
j λ z
∞

−∞
exp
 jπ
λ z (u −x )2

· y(x) dx
(7.173)
This last expression is, with a factor, the Fresnel transform. Actually, the Fresnel
transform is equal to:
exp(jπ z/λ)
√
2π
(LM y)(u)
(7.174)
Again, the connection with optics and waves is clear.
7.8.1.4
Scaling Operator
Take:
M =
	 s
0
0
1/s

(7.175)
Since b = 0,
(LMy) (u) =
1
√s y
u
s

(7.176)
Therefore, one obtains the dilation (scaling) operator.

7.8 Linear Canonical Transformation
421
7.8.1.5
Chirp Multiplication
Let us insist on b = 0:
M =
	 1
0
c
1

(7.177)
In this case:
(LMy) (u) = exp
	 j
2 c u2

· y(u)
(7.178)
This is called chirp multiplication. Let us denote as CM the corresponding operator.
7.8.1.6
Chirp Convolution
This is similar to the Fresnel case:
M =
	 1
b
0
1

(7.179)
Then:
(LM y)(u) =

1
j b
∞

−∞
exp
 j
2 b (x −u )2

· y(x) dx
(7.180)
This last expression is called chirp convolution, and it is also the Gauss–Weierstrass
transform. Let us denote as CC the corresponding operator.
7.8.2
Decomposition of the LCT
The matrix M, and thus the LCT, can be decomposed into:
M =
	 a
b
c
d

=
	 1
0
d−1
b
1

 	 1
b
0
1

 	 1
0
a−1
b
1

(7.181)
The central factor is a chirp convolution. The other two factors are chirp multiplica-
tions.
Therefore the LCT can be obtained with three steps, using a composition of
operators: CMk CC CMl (with k = (d −1)/b, and l = (a −1)/b).

422
7
Time-Frequency Analysis
There is an interesting alternative, as follows:
M =
	 a
b
c
d

=
	 1
0
d
b
1

 	 b
0
0
1
b

 	 0
1
−1 0

 	 1
0
a
b
1

(7.182)
In this factorization, there is a scaling matrix and a Fourier transform surrounded
by chirp multiplications. This is an operator composition: CMmDb F CMn (with
m = d/b, and n = a/b).
7.8.3
Effect on the Wigner Distribution
Suppose that the signal yL is obtained by transforming with the LCT the signal y(t).
Let us compare the Wigner distribution of y(t) and the Wigner distribution of yL.
It is found that:
(WD yL) (t′, ω′) = (WD y)(t, ω)
(7.183)
where:
	 t′
ω′

= M
	 t
ω

(7.184)
That means that the Wigner distribution is transformed by an afﬁne transform. For
instance, a particular case is rotation (when the LCT is a fractional Fourier transform,
see [108]).
7.8.4
Comments
One of the LCT cases, the case d, is the scaling operator. It is interesting to note that
there are functions satisfying the following eigenequation:
y
	 t
s

= λ y(t)
(7.185)
These functions are scaling-invariant, also denoted as self-similar or fractals. They
are eigenfunctions of the scaling operator.
Another LCT case is the fractional Fourier transform. Nowadays many other frac-
tional versions of traditional transforms—like Laplace transform, Hilbert transform,
etc.—have been formulated.
It is possible to apply a quadratic transform on the result of another quadratic
transform. For example Wigner on Wigner. The name of these compositions is quartic
transforms [128].

7.8 Linear Canonical Transformation
423
0
50
100
150
200
250
300
350
400
450
-1
-0.5
0
0.5
1
a cosine signal
0
50
100
150
200
250
300
350
400
450
-2
-1
0
1
2
Fractional Fourier transform (a=0.55)
Fig. 7.36 Fractional Fourier transform of one cosine cycle
7.8.5
Example of Fractional Fourier Transform
There are a few fractional Fourier transform (FrFT) implementations available on
Internet. The article [34] compares in detail two of these implementations, and the
web page of NALAG (K.U. Leuven) bring access to the corresponding MATLAB
codes. We followed in the next example, with some modiﬁcations, the alternative
proposed by [131]. The idea of the implementation is to apply a LCT decomposition
into chirp multiplication – chirp convolution – chirp multiplication. Due to bandwidth
needs, the implementation uses a sinc interpolation to increase the number of data
samples [34].
The example considered below is the FrFT of a cosine cycle. Figure7.36 presents
on top the signal, and below its fractional Fourier transform. The transform shows
two symmetrical modulated chirps, with zero amplitude where the cosine is zero.
The ﬁgure has been generated with the Program 7.14. Notice in the program how the
range of the exponent covers from 0 to 1.5.
Program 7.14 Fractional Fourier transform
% Fractional
Fourier
transform
% using
decomposition
% choose
parameter
a
( fractional
power )
0<a <1 .5
a=0 .55;
% for
instance
%
the
signal
to
be
transformed ----------------------
% cosine
signal
t =0:0 .015 :2* pi;

424
7
Time-Frequency Analysis
y=cos(t);
Ny= length (y);
% odd
length
yin=y;
% changes
for
a <0 .5
if
(a <0 .5),
shft
=
rem ((0: Ny -1)+ fix(Ny /2) , Ny )+1;
sqN
=
sqrt (Ny );
a=a +1;
y( shft )= ifft (y( shft ))* sqN;
end;
alpha =a*pi /2;
% sinc
interpolation
for
doubling
signal
data
zy= zeros (2* Ny -1 ,1);
zy (1:2:2* Ny -1)= y;
aux1 =zy (1:2* Ny -1);
aux2 = sinc ([ -(2*Ny -3):(2* Ny -3)] '/2);
m= length ([ aux1 (:); aux2 (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
yitp = ifft ( fft(aux1 ,P).* fft(aux2 ,P ));
yitp = yitp (1:m);
yitp = yitp (2*Ny -2 :end -2* Ny +3);
% interpolated
signal
% sandwich
zz= zeros (Ny -1 ,1);
ys =[ zz;
yitp ;
zz ];
%
the
fractional
transform --------------------------
% chirp
premultiplication
htan = tan( alpha /2);
aex =( pi/Ny )*( htan /4)*(( -2* Ny +2:2* Ny -2) '. ^2);
chr=exp(-j* aex );
yc= chr. *ys;
% premultiplied
signal
% chirp
convolution
sa= sin( alpha );
cc=pi/Ny/sa /4;
aux1 = exp(j*cc *( -(4*Ny -4):4* Ny -4) '. ^2);
m= length ([ aux1 (:); yc (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
ym= ifft ( fft(aux1 ,P).*fft(yc ,P ));
ym=ym (1:m);
ym=ym (4*Ny -3:8* Ny -7)* sqrt (cc/pi );
% convolved
signal
% chirp
post
multiplication
yq= chr. *ym;
% normalization
yp= exp(-j*(1 -a)* pi /4)* yq(Ny :2 :end -Ny +1);
%
display -------------------------------------
figure (1)
subplot (2 ,1 ,1)
plot (t,yin ,'k');
axis ([0
2* pi
-1.1
1.1 ]);
title ('a
cosine
signal ');
subplot (2 ,1 ,2)
plot (t, real (yp),'k');
axis ([0
2* pi
-2
2]);
title ('Fractional
Fourier
transform
(a=0 .55)');

7.8 Linear Canonical Transformation
425
A recent, new implementation of the FrFT is [32].
The FrFT of a chirp with rate cot(α), where α = a·π/2, is a δ. This is an interesting
feature that can be used to detect chirps in multi-component signals. Actually, there
are FrFT applications that scan many different values of the exponent in order to
notice any FrFT peak.
Figure7.37 shows an example of FrFT response to the chirp with rate cot(α).
The Fig.7.37 has been obtained with the Program B.11, that has been included in
Appendix B. The bottom plot corresponds to the absolute value of the FrFT.
The roots of the FrFT can be traced back to 1937. See [103] for an interesting and
concise history of FrFT, in which it is recognized that the articles [119, 130] marked
the origin of an expanding interest on this method. A convenient survey of FrFT,
including applications, is [33] (see also [158]). The reference book for this topic is
[132]. A most cited article is [7].
Some illustrative applications are optimal ﬁltering [101], ﬁngerprint veriﬁcation
[93], separation of chirplets [38, 42, 110], medical ultrasound imaging [76], study
of marine mammals communication [107], etc.
More information on the FrFT can be obtained from the Fractional Fourier Trans-
form web page (related to [132]) (see the Resources section).
0
50
100
150
200
250
300
350
400
450
500
-1
-0.5
0
0.5
1
0
50
100
150
200
250
300
350
400
450
500
0
5
10
15
20
25
Fractional Fourier 
transform (a=0.8)
Fig. 7.37 Fractional Fourier transform of a chirp which rate corresponds to the transform exponent

426
7
Time-Frequency Analysis
7.9
Adaptation and Decomposition for Better Signal
Representation
A main problem of quadratic TFRs is interference. Given two points of a signal,
interferences appear in the line joining these points. Therefore, signal arcs are accom-
panied by interferences external to the arcs, except for the case that the signal points
draw a straight line: mutual interferences lay on this same line. Therefore, sev-
eral signal representation approaches try to introduce changes to convert signal arcs
into straight lines. This could be done by axis changes, by signal manipulation—for
instance, modulation—, or by using different transforms. The Fourier transform uses
a basis of exp(jwt) functions, other transforms can be obtained by basis change.
Another point of view is related with compression. Suppose the case of a multi-
component signal with 3 s of 15 Hz. sinusoid, then 12 s of 22 Hz triangular, and then
8 s of 7 Hz square. Of course the signal could be sampled, and be saved as a sample
data ﬁle. But you can also describe it as SN(3, 15), TR(12, 22), SQ(8, 7). It would
be really compact, and accurate; and you can easily draw from it a TF plane with no
interferences. This is a decomposition using a dictionary of basis functions.
Consider a single sea wave. It may be considered as the superposition of sev-
eral sinusoidal harmonics. It happens that low frequency harmonics travel with high
speed, while high frequency harmonics travel with low speed. These speed differ-
ences make the wave proﬁle to change as the wave propagates. This is a general
phenomenon in dispersive media. If you send an underwater chirped sound, starting
withlowfrequencyandendinginhighfrequency,itispossiblethatharmonicsbecome
coincident at a given distance, with large energy concentration. For the reverse case,
sending from high frequency to low frequency, propagation could get a linear fre-
quency evolution from an initial curved frequency evolution. Whales send sound
chirps. Bats do the same. There is a great deal of scientiﬁc interest on chirps, to the
point that they are candidates to be basis functions or to be included in a dictionary
of basis functions.
It has been found that phase information is useful for great improvements of linear
and quadratic TFRs. For instance, this information can be used for reassignment,
recognizing local ‘centres of gravity’ of signal energy in the TF plane.
In this section several proposals will be introduced for better signal representation.
7.9.1
The Chirplet Transform
Tilings with rectangular pieces have been used so far, like in the Gabor transform or
in the wavelet transform.
One of the ﬁrst papers on the chirplet transform [116], uses the following metaphor
to introduce this new transform: suppose one grabs the four corners of the rectangular

7.9 Adaptation and Decomposition for Better Signal Representation
427
Fig. 7.38 An oblique
perspective
tile and moves wherever each corner, as one wishes. Chirplets are based on such tiles,
which may be rectangular or not. There is a mother chirplet, and babies [114].
Clearly, the STFT, the wavelet transform, etc., are particular cases of chirplet
transform.
Next photograph (Fig.7.38), which is a view of the building where the author
of this book works, suggests that trapezoidal tiles would be a suitable approach for
block-by-block image processing taking into account the perspective. This is roughly
the idea of using chirplets in the time-frequency plane.
7.9.1.1
Types of Chirplets
There are several types of chirplets:
• Co-linear: 8 parameters, which are the time-frequency coordinates of each corner.
• Perspective: 7 parameters. For example the oblique photography of a brick-wall
(bricks are taken in perspective).
• Afﬁne: 6 parameters, for shifting and shearing of the basic rectangular tile.
Figures7.38 and 7.39 show afﬁne transformations of the rectangular tile.
• Symplectic: 5 parameters. Constant time-bandwidth product tiles.
• Time and frequency shear invariant: 4 parameters.
Figure7.39 shows in the time-frequency plane four afﬁne transformations of the
basic rectangular tile.
Figure7.40 shows in the time-frequency plane, two more afﬁne transformations
and two perspective projections of the basic rectangular tile.
Some authors have proposed to include also rotations for tiling changes [31].
According with [60], in general a chirp has the following form:
h(t) = a(t) · exp(j ϕ(t))
(7.186)

428
7
Time-Frequency Analysis
time
frequency
time
frequency
time
frequency
time
frequency
time-shift
freq.-shift
dilation in time
dilation in freq
Fig. 7.39 Four afﬁne transformations of the rectangular tiling
time
frequency
time
frequency
time
frequency
time
frequency
perspective in time
shear in time
shear in freq
perspective in freq
Fig. 7.40 Two more afﬁne transformations, and two perspective projections

7.9 Adaptation and Decomposition for Better Signal Representation
429
Fig. 7.41 Wigner
distribution of prolate signal
0
20
40
60
80
100
0
20
40
60
80
100
where a(t) is a positive, low-pass and smooth amplitude function whose evolution is
slowcomparedtotheoscillationsofφ(t).Expression(7.186)canbeseenaswindowed
oscillating signal. The form of the window a(t) may rectangular, or Gaussian, or any
other alternative. The choice of window, and the choice of φ(t) originate several
categories of chirplets.
7.9.1.2
Illustration Using Prolate Function
An interesting way to illustrate the tiling transformations is by means of prolate func-
tions. This was made, for instance, in [116]. With the term ‘prolate’ we schematically
refer to the Discrete Prolate Spheroidal Sequences, also denoted as Slepians. This
type of signals y(t) have most of the energy of y(t) and Y(ω) concentrated in a
time-frequency rectangle.
Figure7.41 shows the Wigner distribution of an example. The ﬁgure has been
generated with the Program 7.15 which uses the dpss() MATLAB SPT function to
generate the prolate signal. The program uses modulation to place the signal near
the centre of the ﬁgure (frequency-shift).
Program 7.15 Wigner distribution of prolate signal
% Wigner
distribution
of
prolate
signal
% The
prolate
signal
for
our
example
[yy ,c]= dpss (1001 ,110);
ys= sum(yy ');
% frequency
shift
to
center :
pp =(1:1001)/1001;
ys=ys.* cos (300*2* pi*pp );
ym=ys (300:700);
ym=ym - mean (ym );
zey= zeros (1 ,200);
yr =[ zey ,ym , zey ];
y= hilbert (yr )';
Ny= length (y);

430
7
Time-Frequency Analysis
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
t =0:(110/800):110;
f =0:(110/800):110;
% result
display
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .0005 + abs(WD )));
axis
xy;
title ('Wigner
distribution
of
prolate
signal ');
Figure7.42 shows the effect of frequency shear. The ﬁgure has been generated
with the Program 7.16, which is similar to Program 7.15 with the only difference of
some new lines devoted to frequency shearing.
Fig. 7.42 Wigner
distribution of prolate
function with frequency
shear
0
20
40
60
80
100
0
20
40
60
80
100

7.9 Adaptation and Decomposition for Better Signal Representation
431
Program 7.16 Wigner distribution of prolate signal with frequency shear
% Wigner
distribution
of
prolate
signal
%
with
frequency
shear
% The
prolate
signal
for
our
example
[yy ,c]= dpss (1001 ,110);
ys= sum(yy ');
% frequency
shift
to
center :
pp =(1:1001)/1001;
ys=ys.* cos (300*2* pi*pp );
ym=ys (300:700);
ym=ym - mean (ym );
yc= hilbert (ym );
% central
signal
% frequency
shear
fb =0;
fe =40;
tt =(1:401)/401;
M=fb +((fe -fb )* tt );
% set
of
modulation
frequencies
yx= yc.* exp (-(i *2* pi*M).*tt );
% chirp
modulation
zey= zeros (1 ,200);
y=[ zey ,yx , zey ]';
Ny= length (y);
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
t =0:(110/800):110;
f =0:(110/800):110;
% result
display
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .0005 + abs(WD )));
axis
xy;
title ('Wigner
distribution
of
prolate
signal
with
frequency
shear ');
7.9.1.3
Decomposition of the Chirplet Transform
Recall the effect of the LCT on Wigner (Sect.7.8.3.). In a similar way, the chirplet
transform can be seen as a change of coordinates in the time-frequency plane. This
change can be decomposed into several steps. For example:
	t′
ω′

=
	 1
0
−q
1

 	 1
−p
0
1

 	 e−a
0
0
ea

×
		 t
ω

−
	τ
0

−
	0
θ


(7.187)

432
7
Time-Frequency Analysis
Fromlefttoright,thedecompositionin(7.187)reads:shearinginfrequencydirection,
shearing in time direction, scaling, time-shift, frequency shift.
7.9.1.4
Gaussian Chirplets
Many research papers on signals observed in nature, like biomedical signals or
earthquake records, use chirplet transform on a basis of Gaussian chirplet atoms
[18, 124, 159].
A typical formulation of a Gaussian chirplet atom is the following:
hθ(t) =
1
(π d)1/4 · exp
	
−(t −t0)2
2 d

· exp

−j ·

ω0 + c
2(t −t0)

(t −t0)

(7.188)
where t0 is location in time, ω0 is location in frequency, c is chirp rate, and d is
duration. The set of four parameters is denoted as θ= [t0,ω0, c, d].
The atoms satisfy that:
∥h∥2 = < h , h > = 1
(7.189)
The chirplet transform of a signal y(t) is given by:
aθ = < y , hθ > =
∞

−∞
y(t) h∗
θ(t) dt
(7.190)
Now, let us study in more detail the Gaussian chirplet atoms. Figure7.43 shows
an example corresponding to a particular selection of parameter values (see Program
7.17).
Program 7.17 Gaussian chirplet
%
Gaussian
chirplet
t0 =5;
w0 =10;
d =6;
c=2;
% chirplet
parameters
t =0:0 .05 :12;
% times
vector
g=exp ( -(0 .5/d )*((t-t0). ^2));
v=exp(-j*( w0 +((0 .5*c )*(t-t0 ))).*(t-t0 ));
h =(1/(( pi*d )^0 .25 ))* g.*v;
plot (t, real (h),'k');
title ('Gaussian
chirplet
signal ');
xlabel ('sec ');
Notice in Fig.7.43 that the signal frequency changes linearly along time. For
this reason, sometimes the chirplet is denoted as linear FM chirplet. Note that when
chirp rate c > 0, the signal frequency increases, when is c < 0 the signal frequency
decreases.
Consider now the Wigner distribution of the chirplet atom example. It is shown
in Fig.7.44 (the corresponding program is listed in Appendix B).
The Wigner representation of Gaussian chirplet atoms are straight lines. Using
these lines piecewise approximations of any signal can be done. This approximation

7.9 Adaptation and Decomposition for Better Signal Representation
433
Fig. 7.43 Example of
Gaussian chirplet atom
0
2
4
6
8
10
12
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
sec
Fig. 7.44 Wigner
distribution of a Gaussian
chirplet atom
rad/s
seconds
0
2
4
6
8
10
12
0
10
20
30
40
50
60
can be expressed as a weighted sum of N chirplets:
˜y(t) =
N

i=1
aθihθi(t)
(7.191)
where θi are sets of parameters:
θ1 = [t01, ω01, c1, d1], θ2 = [t02, ω02, c2, d2],…, θN = [t0N, ω0N, cN, dN].

434
7
Time-Frequency Analysis
7.9.1.5
Other Chirplets
Coming back to Eq.(7.186) there are power-law chirps, with a(t) proportional to
|t| −p and with φ(t) = d|t|β.
As an extension of power-law chirps, there are hyperbolic chirps, with φ(t) =
dlog|t|. The hyperbolic chirplet transform is particularly useful for the analysis of
Doppler tolerant signals, like the chirps used by bats for echolocation.
In connection with radar and sonar, a version of chirplets that include the Doppler
effect equation has been proposed, with the name ‘dopplerlets’. Here is one of the
proposed equations for the dopplerlet:
hθ(t) =
1
(π d)1/4 · exp
	
−(t −t0)2
2 d

·
· exp
⎛
⎝−j · 2
λ

r2
0 +

L0 −
	
v0t + 1
2 a0t2

2
⎞
⎠
(7.192)
where λ is the wavelength of the transmitted signal, r0 is the miss distance, L0 is the
relative distance from transmitter to target, v0 and a0 are the velocity and acceleration
of the moving target.
The dopplerlets are used to analyse the radar echoes in order to estimate the target
motion parameters (see [183, 184] and references therein). Figure7.45, which has
been generated with the Program 7.18, shows an example of dopplerlet.
Fig. 7.45 Example of
dopplerlet
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
-2.5
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
sec

7.9 Adaptation and Decomposition for Better Signal Representation
435
Program 7.18 Dopplerlet
%
Dopplerlet
% chirplet
parameters :
t0 =0 .3;
d=0 .01;
landa =0 .1;
r0 =10;
L0 =1000;
v0 =500;
ac = -20;
t =0:0 .005 :0 .8;
% times
vector
g=exp ( -(0 .5/d )*((t-t0). ^2));
mx=L0 -(( v0*t )+(0 .5*ac *( t. ^2)));
me= sqrt (r0 ^2+( mx. ^2));
v=exp(-j *(2/ landa )* me );
h =(1/(( pi*d )^0 .25 ))* g.*v;
figure (1)
plot (t, real (h),'k');
title ('dopplerlet
signal ');
xlabel ('sec ');
There are signals with periodic, or quasi-periodic, frequency ﬂuctuations, like
for instance music tones with vibrato effect, radar echoes from sea waves or
mechanical vibrations. For these cases the so-called ‘warblets’ have been proposed
[9, 115, 177]. Here is an example of warblet expression,
hθ(t) =
1
(π d)1/4 · exp
	
−(t −t0)2
2 d

·
· exp (−j · [ω0 + α · sin(ω · (t −t0) + ϕ)] (t −t0))
(7.193)
Figure7.46 shows an example of warblet, and Fig.7.47 shows the evolution of the
warblet instantaneous frequency. Clearly it is a case of sinusoidal FM. Both ﬁgures
have been generated with the Program 7.19.
Fig. 7.46 Example of
warblet
0
2
4
6
8
10
12
-0.5
-0.4
-0.3
-0.2
-0.1
0
0.1
0.2
0.3
0.4
0.5
sec

436
7
Time-Frequency Analysis
Fig. 7.47 Instantaneous
frequency of the warblet
example
0
2
4
6
8
10
12
8
8.5
9
9.5
10
10.5
11
11.5
12
sec
rad/s
Program 7.19 Warblet
%
Warblet
% chirplet
parameters
t0 =5;
w0 =10;
d =6;
ma =2;
mw =1;
mf =0;
t =0:0 .05 :12;
% times
vector
g=exp ( -(0 .5/d )*((t-t0). ^2));
insf =w0 +( ma* sin (( mw *(t-t0 ))+ mf ));
v=exp(-j*( insf. *(t-t0 )));
h =(1/(( pi*d )^0 .25 ))* g.*v;
figure (1)
plot (t, real (h),'k');
title ('warblet
signal ');
xlabel ('sec ');
figure (2)
plot (t,insf ,'k')
title (' instantaneous
frequency
of
warblet
signal ');
xlabel ('sec ');
ylabel ('rad/ sec ');
Many other types of chirplets have been proposed. For instance, harmonic versions
for audio signals, or versions based on mechanical wave physics, or based on damped
sinusoids, etc. A lot of efforts has been devoted to biomedical signals, and earthquake
signals, trying to match speciﬁc chirplet designs to the kind of expected signals in
each context; an interesting example is provided by evoked potentials and their use
for brain-machine interface.
7.9.1.6
Matching Pursuit, and Other Approximation Methods
In 1993 Mallat and Zhang proposed [113], an iterative method that they denoted as
matching pursuit, to decompose nonstationary signals into elementary components
(atoms). The algorithm selects atoms from a set of atoms denoted as Dictionary.

7.9 Adaptation and Decomposition for Better Signal Representation
437
Consider a signal y(t) to be decomposed into atoms. After the ﬁrst M iterations
of the matching pursuit, one obtains:
y(t) =
M

i=1
Cihθi(t) + RMy
(7.194)
where RMy is the residual of the approximation.
The algorithm starts with R0y = y(t), and it selects the best atom of the dictionary:
the atom which gives the largest inner product with the signal (7.190). Next, the
residual R1y is obtained (removing the identiﬁed component from the signal), and
a second best atom is selected, the one which gives the largest inner product <
R1y, hθ >. This second component is removed; a second residual is determined, and
so on.
The matching pursuit algorithm was introduced considering the use of Gabor
atoms. After some years it was proposed to use the algorithm for chirp atoms. The use
of an orthonormal basis of chirp atoms could be too inﬂexible for certain applications,
and redundant dictionaries could be a better alternative. Several improvements of the
algorithm have been proposed, like the ridge pursuit, for fast signal matching [72].
In general, there is a matter of obtaining hints from the signal, for better adaptation of
the dictionary. The research continues with several approaches: best basis selection,
heuristic optimization, analytic determination of best atoms, etc.
Ingeneralthedictionaryshouldcontaingoodmatchingcandidates.Inotherwords:
the dictionary should be built using a priori knowledge, guessing underlying models
of the signal components.
If there are good pieces for matching in the dictionary, good representations of
signals could be obtained with few parameters: this is denoted as sparse representa-
tions.
7.9.2
Unitary Equivalence Principle
Using unitary operators U, it is possible to obtain unitary equivalents of the opera-
tor A:
˜A = U−1 A U
(7.195)
This expression can be used to build convenient operators, by composition of a
central operator and two unitary operators: one for pre-processing, and the other for
post-processing.
The unitary equivalence principle was introduced in [16, 17] for the signal analysis
context. An illustrative example they proposed was the case of a triangular wave
subject to a FM modulation. This signal suffers noise contamination. It is problematic
to try a ﬁlter for signal cleaning, since the FM modulation causes a large bandwidth
to let pass. A solution is to demodulate the signal, apply a conventional ﬁlter, and
then re-modulate the signal.

438
7
Time-Frequency Analysis
7.9.2.1
Example: Time-Warping
Another example is provided by scientists dealing with marine mammal signals [92].
The Wigner representation of these signals suffers from interferences. The proposal
for representation improvement is to use a time-warping technique, by means of a
time-warping operator:
(Ww y)(t) =

| ˙w(t)| y(w(t))
(7.196)
where w(t) is a time-warping function.
In general w(t) is chosen to linearize the behaviour of a signal, or to transform a
non-stationary characteristic of the signal into a stationary one.
To unwarp the signal, one uses:
C
	
w(t),
ω′
˙w (w−1(t))

(7.197)
For instance, consider the signal:
y(t) = exp (j · θ · tK)
(7.198)
Let us build a unitary time-warping operator. Select the following time-warping
function:
w(t) = t1/K
(7.199)
Therefore:
˙w(t) = 1
K (t1−K)1/K
(7.200)
Then, one applies the change of variables:
t′ = w−1(t) = tK
ω′ = ˙w(w−1(t)) · ω = ˙w(tK) · ω =
=
1
K

(tK)1−K)
1/K · ω =
1
K (t1−K) · ω
(7.201)
The warped signal is:
(Ww y)(t) = (t1−K)1/(2K)
√
K
ejθ t
(7.202)
The following ﬁgures have been obtained for the case θ = 5, K = 1.4.

7.9 Adaptation and Decomposition for Better Signal Representation
439
Fig. 7.48 The modulated
signal
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
seconds
Figure7.48 shows the original modulated signal (a chirp). The ﬁgure has been
generated with the Program 7.20.
Program 7.20 Modulated signal
%
Modulated
signal
fs =100;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
between
samples
% time
intervals
set
(10
seconds )(t >0):
t=tiv:tiv :(10+ tiv );
fsig =5;
% signal
base
frequency
in
Hz
wsig = fsig *2* pi;
% signal
base
frequency
in
rad/s
K=1 .4;
% modulation
exponent
y=exp(-i* wsig *( t.^K)) ';
% the
modulated
signal
yr= real (y);
plot (t,yr ,'k');
axis ([0
2
-1
1]);
title ('Modulated
signal
( first
2
seconds )');
xlabel ('seconds ');
Figure7.49 shows the Wigner distribution of the original signal. There are inter-
ferences.
The ﬁgure has been obtained with the Program B.13, which is similar to other
programs already listed. Hence, Program B.13 has been included in the Appendix B.
Now, let us apply the change of variables to obtain the warped signal.
Next three ﬁgures, which have been obtained with the Program 7.21, show impor-
tant aspects of the process.
Figure7.50 shows the Wigner distribution of the warped signal. It is a straight line
corresponding to a constant frequency θ/K. No interferences.

440
7
Time-Frequency Analysis
Fig. 7.49 Wigner
distribution of the original
signal
seconds
Hz
1
2
3
4
5
6
7
8
9
10
5
10
15
20
25
Fig. 7.50 Wigner
distribution of the warped
signal
base frequency
warped time
0
5
10
15
20
25
5
10
15
20
25
Program 7.21 Wigner distribution of a warped modulated signal
% Wigner
distribution
of
a
warped
modulated
signal
fs =50;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
between
samples
% time
intervals
set
(10
seconds )(t >0):
t=tiv:tiv :(10+ tiv );
fsig =5;
% signal
base
frequency
in
Hz
wsig = fsig *2* pi;
% signal
base
frequency
in
rad/s
K=1 .4;
% modulation
exponent
% the
original
modulated
signal :
oy= exp(-i* wsig *( t.^K)) ';
Ny= length (oy );
% odd
number
fiv=fs /(2* Ny );
f=fiv:fiv :( fs /2);
% frequencies
set
Cex =(1 -K )/(2* K);

7.9 Adaptation and Decomposition for Better Signal Representation
441
Cwp =( t.^ Cex )/ sqrt (K);
% factor
y=( Cwp. * exp(-i* wsig *t)) ';
% warped
signal
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
wrt= zeros (Ny ,1);
fcc= zeros (Ny ,1);
wrt=t.^K;
% warped
time
% frequency
conversion
coefficient
:
fcc =( t. ^(1 -K ))/K;
% result
display
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (wrt ,f, log10 (0 .1+ abs(WD )));
axis
xy;
title ('Wigner
distribution
of
the
warped
modulated
signal ');
ylabel ('base
frequency ');
xlabel ('warped
time ');
figure (2)
plot (t,wrt ,'k');
title ('time
warping ');
grid ;
xlabel ('t');
ylabel ('warped
time ');
figure (3)
plot (t,fcc ,'k');
title ('frequency
conversion
along
time ');
grid ;
xlabel ('t'),
ylabel ('fcc ');
Figure7.51 shows the time-warping.
The conversion of frequencies can be written as follows:
ω′ = 1
K (t1−K) · ω = fcc · ω
(7.203)
where we introduced the factor fcc. Figure7.52 shows the value of this factor along
time.
The next step is to invert the frequency conversion, directly on the Wigner distribu-
tion.ThisisdoneintheProgram7.22,bychangingimagematrixindexes.Figure7.53.
shows the result: a well localized Wigner distribution, with some dispersion due to
the matrix discretization.

442
7
Time-Frequency Analysis
Fig. 7.51 The time-warping
relationship
0
2
4
6
8
10
12
0
5
10
15
20
25
30
t
warped time
Fig. 7.52 The frequency
conversion factor
0
2
4
6
8
10
12
0
0.5
1
1.5
2
2.5
3
3.5
t
fcc
Program 7.22 Unwarping the Wigner distribution of the warped signal
% Unwarping
the
Wigner
distribution
of
warped
signal
clear
all
fs =50;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
between
samples
% time
intervals
set
(10
seconds )(t >0):
t=tiv:tiv :(10+ tiv );
fsig =5;
% signal
base
frequency
in
Hz
wsig = fsig *2* pi;
% signal
base
frequency
in
rad/s
K=1 .4;
% modulation
exponent
% the
original
modulated
signal :
oy= exp(-i* wsig *( t.^K)) ';
Ny= length (oy );
% odd
number
fiv=fs /(2* Ny );
f=fiv:fiv :( fs /2);
% frequencies
set

7.9 Adaptation and Decomposition for Better Signal Representation
443
Cex =(1 -K )/(2* K);
Cwp =( t.^ Cex )/ sqrt (K);
% factor
y=( Cwp. * exp(-i* wsig *t)) ';
% warped
signal
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5* aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
WD (:, nt )=2* real (fo );
%a
column
( harmonics
at
time
nt)
end
wrt= zeros (Ny ,1);
fcc= zeros (Ny ,1);
wrt=t.^K;
% warped
time
% frequency
conversion
coefficient :
fcc =( t. ^(1 -K ))/K;
% Unwarping ----------------
UWD= zeros (Ny ,Ny );
for
j =1:Ny ,
% times
kk =1;
k =1;
while
k <=Ny ,
% frequencies
kk =1+ round (k/fcc(j ));
if
kk <=Ny ,
UWD(kk ,j)= WD(k,j);
% expansion
else
k=Ny;
end;
k=k +1;
end
end
% result
display
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .1+ abs( UWD )));
axis
xy;
xlabel ('seconds ');
ylabel ('Hz ');
title ('Unwarped
Wigner
distribution ');
7.9.2.2
Some Application Alternatives
Notice that the Fourier transform is a unitary operator. Also, recall from Sect.7.6.1.
that frequency-shift (modulation) and time-shift operators are unitarily equivalent:
Mμ = F−1Tμ F
(7.204)

444
7
Time-Frequency Analysis
Fig. 7.53 Unwarped Wigner
distribution
seconds
Hz
1
2
3
4
5
6
7
8
9
10
5
10
15
20
25
In general, the warping operator can be written as:
(Ww y)(x) =

| ˙w(x)| y(w(x))
(7.205)
For the case of x being frequency, the operator is applied in the frequency domain
using Fourier transform: F−1 Ww F.
Examples of useful warpings are:
w(x) = |x|k · sgn(x)
(7.206)
And,
w(x) = ex
(7.207)
This last warping, in the frequency domain, is useful for hyperbolic chirps and similar
signals.
The previous detailed example of time-warping was centred on the Wigner distri-
bution. A similar process, for warping in general, could be applied for Cohen’s class
distributions, or afﬁne class distributions (see [17]).
Frequency warping is a common technique in the context of speech processing,
[104, 134, 165].
7.9.3
The Reassignment Method
When one sees the spectrogram of a chirp, one could easily guess from the blurred
image how the curve goes. One could imagine that energy becomes dispersed around
a main narrow path, which corresponds to maximum energy. It would be beneﬁcial,

7.9 Adaptation and Decomposition for Better Signal Representation
445
Fig. 7.54 Re-assigning idea
for the sake of recovering a neat curve, to counteract the energy dispersion by re-
assigning lower energy points to maximum energy points. Figure7.54, tries to illus-
trate the idea considering a zone of maximum energy surrounded by lower energy
zones; by re-assigning, all energy comes to the concentrated area.
Notice that the concentrated area is not at the centre of the region considered.
Then, it is not a matter of geometrical centre. Instead, it can be considered as a centre
of gravity (or energy).
Insisting a little bit more on the concept, Fig.7.55 shows part of a spectrogram,
where a centre of maximum energy is surrounded by other lower energy rectangles.
After re-assignment, this part of the spectrogram reduces to the maximum energy
rectangle.
Recall that in Sect.7.5.6, in Eq.(7.88) the spectrogram was expressed as follows:
SFy(t, ω) =
1
2π
∞
−∞
∞
−∞
WDw(t′ −t, ω′ −ω) WDy (t′, ω′) dω′ dt′
where WDw is the Wigner distribution of the window.
Notice that this expression means that the value of the spectrogram at any given
point (t, ω) is the sum of a whole energy distribution around its geometrical centre.
Fig. 7.55 A re-assigned spectrogram example

446
7
Time-Frequency Analysis
However, it would be better to compute the coordinates of the centre of gravity,
denoted as (ˆt, ˆω). This can be done as follows [11]:
ˆt(t, ω) =
1
2π SFy(t, ω)
∞

−∞
∞

−∞
t′ WDw(t′ −t, ω′ −ω) WDy (t′, ω′) dω′ dt′
ˆω(t, ω) =
1
2π SFy(t, ω)
∞

−∞
∞

−∞
ω′ WDw(t′ −t, ω′ −ω) WDy (t′, ω′) dω′ dt′
And then, the reassigned spectrogram would be:
rSFy(t, ω) =
1
2π
∞

−∞
∞

−∞
SFy(t′, ω′) δ(ˆt(t′, ω′) −t) δ( ˆω(t′, ω′) −ω)dω′ dt′
The ﬁrst contributions on this approach related ˆt and ˆω to the phase of the STFT.
After some years, it was shown [12] (see also [62]), that it is possible to obtain (ˆt, ˆω)
using the normal STFT—that will be denoted as Fw
y to indicate that the window w
is employed—and two additional STFTs, as follows:
ˆt(t, ω) = t −Re

Ft w
y
Fw
y

ˆω(t, ω) = t + Im

Fdw/dt
y
2π Fw
y

There is a number of MATLAB implementations available from Internet, like the one
provided by Auger, or the code linked to the Thesis [127], or some of the functions
included in the LTFAT toolbox.
One of the applications of reassignment is in speech processing [64, 65]. The
article [64] includes a brief history of reassignment, with a mention to the important
contribution from Kodera et al. in the late 70s.
In order to give an example one of the signals included in the previous chapter, the
siren signal, has been chosen. Figure7.56 shows again the conventional spectrogram
of this signal.
The reassignment method has been applied, and the result is shown in Fig.7.57.
A gray scale has been selected for a better view. Notice that the ﬁne details of the
signal are now clearly visible.

7.9 Adaptation and Decomposition for Better Signal Representation
447
Fig. 7.56 Original
spectrogram of the siren
signal
Time
Frequency
0
0.2
0.4
0.6
0.8
1
0
1000
2000
3000
4000
5000
6000
7000
8000
Fig. 7.57 Reassigned
spectrogram of the siren
signal
Time
Frequency
0.2
0.4
0.6
0.8
1
1.2
1000
2000
3000
4000
5000
6000
7000
8000
The Fig.7.57 has been generated with the Program B.14, which has been included
in Appendix B. The implementation is based on the code available from the web page
of Kelly Fitz (see the Resources section).
7.10
Other Methods
Although our wish is not to embark on an exhaustive treatise of time-frequency
analysis methods, there are still some transforms that should be succinctly mentioned.

448
7
Time-Frequency Analysis
7.10.1
The Modiﬁed S-Transform
According with [169], the S-transform can be viewed as an intermediate step between
the STFT and the continuous wavelet. This transform was introduced by Stockwell
et al. in 1996 [162]. The literature refers to it as S-transform or Stockwell-transform,
and it is deﬁned as follows:
S(τ, f ) =
∞

−∞
y(t) w(τ −t, f ) e−j 2π f tdt
(7.208)
The window function w() is generally chosen to be positive and Gaussian:
w(τ −t, f ) =
|f |
√
2π
exp
	f 2(τ −t)2
2

(7.209)
This window is narrower at higher frequencies and wider at low frequencies. Com-
paredtotheSTFT,theS-transformprovidesbettertimeresolutionathighfrequencies,
and better frequency resolution at low frequencies. It is invertible, and it does not
have cross-term interferences.
Several generalizations and extensions of the window have been proposed
[117, 118, 138, 139]. In particular [117] suggests using:
w(τ −t, f ) =
|f |
k
√
2π
exp

−1
2
	f (τ −t)
k

2
, k > 0
(7.210)
When k increases, the frequency resolution increases, while the time resolution
decreases.
In [10] a modiﬁcation of the window is introduced. Instead of using a constant
value of k, it is made dependent on the frequency as: k = m f + q. In this way, an
improved progressive resolution is achieved.
In order to compare the S-transform with the STFT, a linear chirp has been chosen.
Figure7.58 shows its spectrogram.
The modiﬁed S-transform of [10] has been applied to that linear chirp, obtaining
the result shown in Fig.7.59. Obviously, it is less blurry than the spectrogram.
Both Figs.7.58 and 7.59 have been generated with the Program 7.23, which is
based on the implementation due to K.S. Dash available from the Mathworks ﬁle
exchange site.

7.10 Other Methods
449
Fig. 7.58 Spectrogram of a
linear chirp
Time
Frequency
0
0.5
1
1.5
2
2.5
0
10
20
30
40
50
60
70
80
90
100
Fig. 7.59 Modiﬁed
S-transform of the linear
chirp
Time
Frequency
0
0.5
1
1.5
2
2.5
0
10
20
30
40
50
60
70
80
90
100
Program 7.23 Modiﬁed S-transform
% Modified
S- transform , example
with
linear
chirp
%
the
signal
tiv =0 .005 ;
t =0: tiv :(3 - tiv );
fs =1/ tiv;
yc= exp(-j *70*( t. ^2));
y= real (yc );
Ny= length (y);
% even
length
m=Ny /2;
%
The
transform -------------------------------------
%
preparation :
f =[0: m
-m +1: -1]/ Ny;
% frequencies
vector
S=fft(y);
% signal
spectrum
%
Form
a
matrix
of
Gaussians
( freq.
domain )

450
7
Time-Frequency Analysis
q =[1./f (2:m +1)] ';
k =1+(5* abs(f ));
W =2* pi* repmat (f,m ,1).* repmat (q ,1, Ny );
for
nn =1:m,
W(nn ,:)= k(nn )*W(nn ,:);
% modified
S- transform
end
MG= exp ((- W. ^2)/2);
%
the
matrix
of
Gaussians
%
Form
a
matrix
with
shifted
FFTs
Ss= toeplitz (S(1:m+1) ' ,S);
Ss =[ Ss (2:m +1 ,:)];
% remove
first
row
( freq.
zero )
%
S- transform
ST= ifft ( Ss.*MG ,[] ,2);
st0= mean (y)* ones (1, Ny );
% zero
freq.
row
ST =[ st0;ST ];
% add
zero
freq.
row
%
display
---------------------------------------
figure (1)
specgram (y ,64 , fs );
title (' spectrogram
of
the
linear
chirp ');
figure (2)
Sf =0:(2* fs/Ny ):( fs /2);
imagesc (t,Sf ,abs(ST ));
axis
xy;
% set(gca ,'Ydir ',' Normal ');
title ('S- transform
of
the
linear
chirp ');
xlabel ('Time ');
ylabel ('Frequency ');
Suppose you have a sine wave contaminated with some spikes (for instance, due to
false contacts). It was shown in [162] that the STFT would have difﬁculties to capture
these spikes (islands on the time-frequency plane), while the S-transform would
clearly detect them. This suggests a method for power quality analysis [43]. Another
interesting scenario is the mix of several sine waves with different frequencies; the
S-transform is well suited for discerning these components; for example, in [118]
the S-transform is used for gear vibration decomposition.
Other applications of the S-transform are the classiﬁcation of multichannel
electrocorticogram for brain-computer interfacing [176], the analysis of newborn
electroencephalographic(EEG) data[10], removingpowerlineinterferencefrombio-
medical signals [87], denoising of seismograms [136], heart sound analysis [106],
study of soil and building oscillations [49], alcoholism-related analysis of EEG [95],
image compression [170], etc.
S-transform windows must satisfy the condition:
∞

−∞
w(τ −t, f ) dτ = 1
(7.211)
This condition assures,[139], that averaging of S(τ, f ) over all values of τ yields
Y(f ), the Fourier transform of y(t):

7.10 Other Methods
451
∞
−∞
S(τ, f )dτ =
∞
−∞
y(t)e−j 2π f t ×
∞
−∞
w(τ −t, f ) dτ dt =
=
∞
−∞
y(t)e−j 2π f t dt = Y(f )
(7.212)
SincefromY(f )theoriginalsignaly(t)canberecovered,theS-transformisinvertible.
7.10.2
The Fan-Chirp-Transform
The Fan-Chirp (FC) transform was introduced in 2006 [96]. See also [172] for more
details and a good discussion. This transform can be deﬁned as:
X(f , β) =
∞

−∞
y(t) ξ∗(t, f , α) dt
(7.213)
where ξ ( ) is the basis of the transform, having the following expression:
ξ(t, f , α) =

|φ′α(t)| exp(−j 2π f φα(t))
(7.214)
and:
φα(t) =
	
1 + 1
2α t

· t
(7.215)
where α is the chirp rate, and φ′
α(t) is the time derivative of φα(t).
The term “Fan” comes from the geometry on the T-F plane corresponding to
the transform. This geometry is depicted in Fig.7.60, where a tilted chirp has been
sketched.
It happens that the Fan geometry is suitable for the analysis of speech and animal
song segments, and for music. The FC transform can be regarded as the Fourier
transform of a warped-time version of the signal [172].
Like before, a linear chirp has been chosen in order to compare with the STFT.
Figure7.61 shows the spectrogram of the linear chirp.
And Fig.7.62 shows the result obtained with the Fan-Chirp transform, which is
clearly better.
Figures7.61 and 7.62 have been generated with a program that has been included
in Appendix B. The program is a simpliﬁed version of the code available from the
COVAREP speech processing project (web site address in the Resources section).
In addition to the examples of speech analysis included in [51, 96, 172], there
are other reported applications related to music, like [19, 37, 160]. Detailed acad-
emic treatment can be found in [166, 173]. The article [171] discusses several fast
implementations of the transform.

452
7
Time-Frequency Analysis
Fig. 7.60 Geometry
corresponding to the
Fan-Chirp transform
T
F
Fig. 7.61 Spectrogram of a
linear chirp
Time
Frequency
2
4
6
8
10
12
14
16
0
50
100
150
200
250
7.10.3
The Mellin Transform
It is easy to imagine that the wovel ‘a’ pronounced by a whoman would be shorter
than the ‘a’ pronounced by a man. Hence, it is natural for the speech processing
community to deal with time-warping, in order to recognize the same ‘a’, or any
other speech component, from different people. Although the main interest of the
Mellin transform belongs to a theoretical and mathematical context, it is also suitable
for time-warping (also called: scaling). An illustrative example of application is [46],
for vowel recognition.
ThehistoryoftheMellintransformtakesustoaclassicworld.Aﬁrstconsideration
was due to Riemann (1876). Explicit formulations were given by Cahen (1894) and

7.10 Other Methods
453
Fig. 7.62 Fan-Chirp
transform of the linear chirp
Time
Frequency
2
4
6
8
10
12
14
16
0
50
100
150
200
250
Mellin (1896). The Mellin transform of a complex-valued continuous function f ()
is deﬁned as:
M(f , a) =
∞

0
f (t) tp−1dt
(7.216)
where p = a + jb. In general, the integral does exist only for a1 < a < a2; the
values of a1 and a2 depending on f ().These two constants form the strip of deﬁnition
S(a1, a2), which could extend to a half-plane or even the whole complex plane.
With the change of variable t = exp(−x), the Mellin transform can also be
expressed as:
M(f , a) =
∞

−∞
f (e−x) e−pxdx
(7.217)
Denote as L() the two-sided Laplace transform. It can be shown that:
M(f (t), s) = L(f (e−x))
(7.218)
Also, denoting as F() the Fourier transform, then:
M(f (t), p = a + j 2πω) = F(f (e−x) e−ax)
(7.219)
Notice that the Mellin transform can be obtained via Fourier transform. Actually,
[45, 47] proposes a fast Mellin transform by computing an exponential time warp-
ing, followed by multiplication of f () and exp(−ax), ﬁnally followed by Fourier
transform.

454
7
Time-Frequency Analysis
An important property is the following; given a function h(t) = f (β t), then:
M(h(t), a) = β−p M(f (t), a)
(7.220)
It can be said that the Fourier transform is a restriction of the two-sided Laplace
transform, by taking s = jω. Similarly, the ‘scale transform’ is a restriction of the
Mellin transform on the vertical line p = −j c + (1/2).
Therefore, the scale transform is:
D(f (t)) =
∞

0
f (t) e(−jc−1/2) ln tdt
(7.221)
(t is expressed as exp( ln t))
The most relevant property of the scale transform is its scale invariance. If a
function h(t) is a scaled version of f (t), it happens that the transform magnitude of
both functions is the same. In mathematical terms, given that h(t) = f (β t), then:
D(h(t)) = βjc D(f (t))
(7.222)
Hence:
|D(h(t))| = |D(f (t))|
(7.223)
Recall that a scale modiﬁcation is a compression or expansion of the time axis of the
original function [47]. In the 2D context—for instance, images—this invariance is
useful for keeping recognizable shapes at different sizes. This is a main reason for
the scale transform (sometimes confused with the Mellin transform) to be popular.
A formal mathematical exposition of the Mellin transform can be found in
[23, 40]. Two illustrative applications are [75] for monitoring of structure health,
usingultrasonicguidedwaves,and[161]forestimationofdirectionalbrainanisotropy
from EEG signals. A fast Mellin transform implementation can be found in the Time-
Frequency Toolbox, in which this transform has been used for the coding of wide-and
and narrow-band ambiguity functions, and for the coding of the Bertrand distribu-
tion [13, 129]. Other implementations of the transform have been proposed, like [47,
186].
Fractional versions of the Mellin transform can be found in [24, 71, 126]. They
are well suited to analyze signals subject to hyperbolic frequency modulation, like
for instance chirps that you see on the T-F plane as hyperbolic arcs.

7.10 Other Methods
455
7.10.4
The Empirical Mode Decomposition
and Hilbert–Huang Transform
The Fourier decomposition of signals uses exponentials as a ﬁxed type of basis
functions.Waveletsdosomethingsimilar.Instead,theempiricalmodedecomposition
(EMD) obtains with an algorithm oscillating components of a signal, the typology of
these components not being ﬁxed a priori. The EMD components are called ‘intrinsic
mode functions’ (IMF).
7.10.4.1
The Empirical Mode Decomposition
Let us introduce the EMD algorithm in words, as in [111]. After that, a graphical
illustration is added for rapid understanding.
IMF functions are denoted as imf(), and residuals as r(). The signal to be decom-
posed is y(t). Here is the algorithm:
1. Initialize: r0(t) = y(t) , i = 1
2. Extract the i-thIMF:
(a) Initialize: h0(t) = ri(t) , j = 1
(b) Extract the local minima and maxima of hj−1(t)
(c) Interpolate the local maxima and the local minima by a cubic spline to obtain
upper and lower envelopes of hj−1(t)
(d) Compute the mean mj−1(t) of the two envelopes
(e) hj(t) = hj−1(t) −mj−1(t)
(f) go to (b) with j = j + 1, unless stopping criterion is met
3. imfi(t) = hj(t) ; ri(t) = ri−1(t) −imfi(t)
4. If ri(t) has at least 2 extrema then go to 2 with i = i + 1
Else, end of the algorithm with the residue ri(t)
Next example is taken from [97]. Suppose we sampled the following signal:
y(t) = 0.5 t + sin(π t) + sin(2π t) + sin(6π t) + η
(7.224)
where η is noise.
Figure7.63 shows the signal, the ﬁrst IMF and the ﬁrst residue.
From the 1st residue one obtains the second IMF and the second residue, as shown
in Fig.7.64.
LetusapplytheEMDalgorithm.Thelocalmaximaandminimamustbeidentiﬁed,
and cubic splines are used to create an upper and a lower envelope. Figure7.65 shows
the two envelopes.
Now, the mean of the two envelopes is obtained. The result is shown in Fig.7.66.
It is a curve in the middle, between upper and lower envelopes.

456
7
Time-Frequency Analysis
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-5
0
5
the signal
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-5
0
5
1st IMF
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-2
0
2
4
1st residue
Fig. 7.63 A signal, its 1st IMF, and its 1st residue
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-2
0
2
4
1st residue
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-5
0
5
2nd IMF
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-2
0
2
4
2nd residue
Fig. 7.64 The 1st residue, the 2nd IMF, and the 2nd residue

7.10 Other Methods
457
Fig. 7.65 The upper and
lower envelopes
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-3
-2
-1
0
1
2
3
4
5
6
the two envelopes
Fig. 7.66 The mean of the
two envelopes
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-3
-2
-1
0
1
2
3
4
5
6
Next step is to subtract the mean from the signal. Both the mean and the signal
are shown in Fig.7.67.
The result of the subtraction is the ﬁrst IMF, Fig.7.68, which is the component
with highest frequency.
The remaining signal r = y −imf1 is the ﬁrst residue, and is less oscillated
than the original signal. The procedure for obtaining the second IMF starts from this
residue and uses the same steps just described.
Perhaps the example just considered might give the false impression that IMFs
are sinusoidal functions. Far from that, in general IMFs are not sinusoidal; instead,
they usually are non-stationary signals: next examples will give you an idea of how
they may look.

458
7
Time-Frequency Analysis
Fig. 7.67 The signal and the
mean of envelopes
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-2
-1
0
1
2
3
4
5
Fig. 7.68 The ﬁrst IMF
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
-5
-4
-3
-2
-1
0
1
2
3
4
5
Note also that, in general, the extraction of one IMF takes some iterations before
the stopping criterion cited in the 2(f) step of the algorithm is met. In the simple
example just described we found the ﬁrst IMF in just one iteration, but this is not the
normal case.
The iterative procedure for extraction of a set of IMFs is called ‘sifting’. The
signal is sequentially decomposed into the highest frequency component imf1 to the
lowest frequency component imfn. The ﬁnal result is that:
y(t) =
n

i=1
imfi(t) + rn(t)
(7.225)

7.10 Other Methods
459
IMF functions must satisfy two conditions:
• The number of extrema and the number of zero crossings must be equal or differ
at most by one
• The mean value of the upper and the lower envelope of the IMF is zero everywhere
7.10.4.2
The Hilbert–Huang Transform
The Hilbert transform has already been introduced in the previous chapter, but it is
opportune to add some more details [111]. Given a real signal y(t), it is possible to
build the corresponding analytic signal (also called ‘complex trace’):
Y(t) = y(t) + j H(y(t))
(7.226)
where H(y(t)) is the quadrature signal obtained with the Hilbert transform:
H(y(t) ) = 1
π PV
	 ∞
−∞
y(t)
t −τ dτ

(7.227)
where PV(
 ∞
−∞) is the Cauchy principal value of the integral.
The analytic signal can also be obtained by:
1. Taking the Fourier transform of y(t)
2. Zeroing he amplitude for negative frequencies and doubling the amplitude for
positive frequencies
3. Taking the inverse transform.
The analytic signal can be expressed as: X(t) = A(t) ejθ(t). Its instantaneous
frequency is deﬁned as:
ω(t) = dθ(t)
dt
(7.228)
Suppose that a set of IMFs has been obtained using EMD. If one computes the
analytic signal corresponding to each IMF, one gets:
z1(t) = imf1(t) + j H(imf1(t)) = A1(t) exp(j θ1(t))
(7.229)
z2(t) = imf2(t) + j H(imf2(t)) = A2(t) exp(j θ2(t))
(7.230)
zn(t) = imfn(t) + j H(imfn(t)) = An(t) exp(j θn(t))
(7.231)
The expansion of the signal in terms of these functions is:
y(t) =
n

i=1
Ai(t) exp(j θi(t))
(7.232)

460
7
Time-Frequency Analysis
Based on this expansion, it is possible to build a 3D plot of the amplitude at a given
time and frequency. This can be also represented with pseudo-colors on the time-
frequency plane. The resulting distribution is called ‘the Hilbert spectrum, H(t, ω)’.
For computation purposes, it is interesting to consider that, given the analytic
signal Y(t) = y(t) + j H(y(t)), then:
• Instantaneous amplitude: A(t) =

y(t)2 + H(y(t))2
• Instantaneous phase: θ(t) = arctan H(y(t))
y(t)
• Instantaneous frequency: f (t) =
ω(t)
2π =
1
2π
d
dt θ(t)
The methodology just described was introduced in [90], year 1998. In 2003,
Dr. Huang received the NASA Government Invention of the Year award. The name
“Hilbert–Huang Transform (HHT)” was also coined by NASA.
The EMD and the HHT have attracted a lot of attention, the main articles on this
methodology being cited thousands of times. The number of reported applications is
quite large and varied. For instance, in the review of HHT applications to geophysical
studies [89], more than 120 contributions are cited. In the review of biomedical appli-
cations [105], 32 contributions are cited. There is a complete book on engineering
applications [88]. Another book [146], is devoted to the HHT analysis of hydrolog-
ical and environmental time series. The applications extend also to economic data
analysis
There is a number of published improvements and variants. Of special relevance
is the ‘Ensemble Empirical Mode Decomposition’ [175], which adds white noise to
the data, builds and ensemble, applies sifting, and treats the mean as the ﬁnal true
result.
See the Resources section for interesting web sites related to EMD. It is also most
convenient to read the algorithmic details discussed in [148].
7.10.4.3
Examples
In a ﬁrst example we will apply empirical mode decomposition to a normal electro-
cardiogram, which is shown in Fig.7.69.
The ﬁrst ﬁve IMFs found with EMD are shown in Fig.7.70. Notice how the
frequencies of the IMFs decrease from one IMF to the next IMF. Pay attention to the
scales of each plot.
In order to present more details, concerning the lower frequency components, the
6th to 10th IMFs are also shown in Fig.7.71.
The three ﬁgures of this example have been obtained with the Program 7.24, which
is a slightly modiﬁed version of a program written by Ivan Magrin–Chagolleau,
available at the MIT web address cited in the Resources section.

7.10 Other Methods
461
200
400
600
800
1000
1200
1400
6
6.5
7
7.5
8
8.5
9
9.5
Fig. 7.69 The electrocardiogram
0
200
400
600
800
1000
1200
1400
-0.1
0
0.1
0
200
400
600
800
1000
1200
1400
-1
0
1
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
Fig. 7.70 The ﬁrst ﬁve IMFs

462
7
Time-Frequency Analysis
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.5
0
0.5
0
200
400
600
800
1000
1200
1400
-0.2
0
0.2
Fig. 7.71 The 6th to 10th IMFs
Program 7.24 EMD example
%
EMD
example
%
ECG
signal
% read
data
file
fs =200;
fer =0;
while
fer ==0 ,
fid2 = fopen ('ECGa.txt ','r');
if
fid2 == -1 ,
disp ('read
error ')
else
ecgdat = fscanf (fid2 ,'%f
\r\n');
fer =1;
end;
end;
fclose ('all ');
y= ecgdat (1:1400) ';
% select
a
signal
segment
Ny= length (y);
%
EMD
decomposition
--------------------------------
nim =10;
% number
of
imfs
to
be
found
Mimf = zeros (nim ,Ny );
for
nn =1: nim ,
h=y;
% initial
signal
StD =1;
% standard
deviation
( used
for
stop
criterion )
while
StD >0.3 ,
%
find
max/ min
points
D= diff (h);
% derivative
popt =[];
%to
store
max
or
min
points
for
i =1:Ny -2,
if
D(i)==0 ,

7.10 Other Methods
463
popt =[ popt ,i];
elseif
sign (D(i ))~= sign (D(i +1));
popt =[ popt ,i +1];
% the
zero
was
between
i
and
i+1
end;
end;
if
size (popt ,2) <2
% got
a
final
residue
break
end;
% distinguish
maxima
and
minima
No= length ( popt );
%
if
first
one
is
a
maximum
if
popt (1) > popt (2) ,
pmax = popt (1:2: No );
pmin = popt (2:2: No );
else
pmax = popt (2:2: No );
pmin = popt (1:2: No );
end;
% force
endpoints
pmax =[1
pmax
Ny ];
pmin =[1
pmin
Ny ];
% create
envelopes
using
spline
interpolation
maxenvp = spline (pmax ,h( pmax ) ,1: Ny );
minenvp = spline (pmin ,h( pmin ) ,1: Ny );
% mean
of
envelopes
m
=
( maxenvp + minenvp )/2;
oldh =h;
h=h-m;
% subtract
mean
to
h
% compute
StD
ipsi =0 .0000001 ;
StD=sum ((( oldh -h). ^2)./( oldh. ^2+ ipsi ));
end
Mimf (nn ,:)= h;
% store
IMF(nn)
y=y-h;
% subtract
the
IMF
from
the
signal
end
%
display -------------------------------------
figure (1)
for
jj =1:5 ,
subplot (5 ,1 , jj)
plot ( Mimf (jj ,:) , 'k');
end
figure (2)
for
jj =6:10 ,
subplot (5 ,1 ,jj -5)
plot ( Mimf (jj ,:) , 'k');
end
figure (3)
plot ( ecgdat (1:1400) , 'k');
title ('heartbeat
signal ')
axis ([1
1400
6
9.5 ]);
The second example takes a synthetic signal you can hear, which sounds as a
“boink” with a brisk strident beginning. Figure7.72 shows the ﬁrst ﬁve IMFs obtained
by EMD.

464
7
Time-Frequency Analysis
0
500
1000
1500
2000
2500
3000
-1
0
1
0
500
1000
1500
2000
2500
3000
-1
0
1
0
500
1000
1500
2000
2500
3000
-0.5
0
0.5
0
500
1000
1500
2000
2500
3000
-0.2
0
0.2
0
500
1000
1500
2000
2500
3000
-0.1
0
0.1
Fig. 7.72 The ﬁrst ﬁve IMFs
Fig. 7.73 The Hilbert
spectrum
As shown in Fig.7.73, the Hilbert spectrum is composed of many points, which
tend to be dispersed; although one can recognize the frequency modulation of the
“boink” (it is like a damped oscillation). The spectrum has been generated with a
program that has been included in Appendix B. The ﬁrst part of this program is

7.10 Other Methods
465
similar to Program 7.24. Probably, the dispersion of points is due to several factors:
numerical errors, too simplistic calculation of phase derivatives, etc. Some patience
is required for running this program, about one minute, while MATLAB generates
the image for the Hilbert spectrum.
7.10.5
More Transforms
A few more transforms are now brieﬂy introduced. Most of them are intended for
speciﬁc areas of application.
7.10.5.1
The Constant-Q Transform
Consider a band-pass ﬁlter with a center frequency f and a bandwidth Δ, the Q factor
corresponding to this ﬁlter has the value:
Q = Δ
f
(7.233)
The discrete Fourier transform can be written as follows:
N−1

n=0
y(n) e−j 2π n k/N ; k = 0, 1, . . . , N −1
(7.234)
One would say that this transform uses a set of N band-pass ﬁlters, with all ﬁlters
having the same bandwidth ΔP = ΔT
N , where ΔT = fs
2 is the total bandwidth covered
by the transform. Each of the ﬁlters has a center frequency fi , i = 0, 1, . . . , N −1.
If for example N = 5, and (fs/2 ) = 100, then the bandwidth of each ﬁlter would
be 20, and the center frequencies would be:
f1 = 10 , f2 = 30, f3 = 50, f4 = 70, f5 = 90.
Notice that each of the ﬁlters has a different value of Q.
The idea of the constant-Q transform is to use N ﬁlters having the same Q. In
music applications, the center frequencies are chosen as:
fi = f1 2(k−1)/B
(7.235)
where f1 is the lowest center frequency, and B is the number of ﬁlters per octave (the
piano has 12 notes per octave). The value of Q would be:
Q = 1/( 21/B −1)
(7.236)

466
7
Time-Frequency Analysis
and the constant-Q transform is:
1
Nk
Nk−1

n=0
y(n) ω(n) e−j 2π n Q/Nk ; k = 0, 1, . . . , N −1
(7.237)
where w(n) is some window function (for instance a Hamming window), and:
Nk = ceil
	
Q fk
ΔT

(7.238)
Although the idea of constant Q transform was around from the 70s, the practical mise
en scène of this transform was due to [28] in 1991, followed by an efﬁcient algorithm
proposed in [30]. Some implementations in MATLAB are succinctly presented in
[25]. There is a toolbox for constant-Q transform for music [156]. A framework for
invertible constant-Q transform is introduced in [84].
7.10.5.2
The Harmonic Transform
Sinusoidal speech modelling uses a sum of sinusoids with time-varying amplitudes
and frequencies, being the frequencies harmonically related as multiples of a funda-
mental frequency.
The harmonic transform of a signal is deﬁned as follows:
HTy(ω) =
∞

−∞
y(t) φ′
u(t) exp(−jω φu(t)) dt
(7.239)
whereφu(t)istheunitphasefunction,whichisthephaseofthefundamentalharmonic
divided by its instantaneous frequency. The derivative of this function is φ′
u(t).
The harmonic transform was introduced by [179] in 2004. A discrete version
is applied to speech decomposition in [185]. Background information on speech
analysis with an harmonics approach is given by [14].
7.10.5.3
The Hilbert Vibration Decomposition (HVD)
While the EMD extracts components from highest to lowest frequencies, the Hilbert
vibration decomposition uses the analytic form of a signal to extract its compo-
nents from highest to lowest instantaneous amplitude. The component with highest
instantaneous amplitude is called the dominant mode.
The signal is decomposed as follows:
y(t) = a (t) ejφ(t) =

k
ak(t) exp(j φk(t))
(7.240)

7.10 Other Methods
467
The way used by HVD to extract the dominant mode is to low-pass the phase of the
signal, in order to remove the oscillations due to secondary components, so it only
remains the phase of the dominant mode. Using this phase, the dominant mode is
extracted by synchronous demodulation.
Once the dominant mode is extracted, the process is repeated with the residue.
The HVD method was proposed by Feldman in 2006 [57]. More recently, this
author has published a book on this topic [58], which includes pertinent algorithmic
and implementation details. An illustrative application of HVD is offered by [22].
In [137], the HVD method is compared with ensemble EMD and wavelets in a real
application.
We take this opportunity to recommend the article [59], which is a tutorial review
on the Hilbert transform in vibration analysis.
It is also quite interesting the proposed generalization of [50], called variational
mode decomposition, which, in contrast with EMD, is able to precisely separate any
pair of harmonics no matter how close their frequencies are.
7.10.5.4
Some Other Approaches
It seems appropriate for certain types of signals to use nonequispaced sampling.
This is an idea that has inspired some developments, like the nonequispaced DFT
proposed in [91]; see references therein for related work.
Following the main characteristic of EMD, which is to adapt the representation
basis to the signal itself, an empirical wavelet was proposed by [69]. Likewise, but
more in the context of dictionaries, [86] suggests a matching pursuit using IMFs of
the form a(t) cos(θ(t)).
In [68], a time-varying ﬁlter interpretation of the Fourier transform is introduced.
This perspective extends to warped variants of the transform.
7.10.5.5
Overviews
Useful overviews are [157] on T-F representations using energy concentrations, and
[150] on analysis techniques for non-stationary waveforms in power systems.
7.11
Experiments
Obviously, many experiments and exercises could be proposed on the basis of signal
phenomena described in the previous chapter, and the analysis methods just intro-
duced. The purpose of this section is to present some motivating examples. All the
programs developed for this section have been included in Appendix B.

468
7
Time-Frequency Analysis
7.11.1
Fractional Fourier Transform of a Rectangular Signal
The target of this experiment is to explore what happens when changing the exponent
of the FrFT. A rectangular signal has been chosen to this effect.
Figure7.74 shows the FrFT of the rectangular signal, using as exponents of the
transform 0.55, 0.7, 0.8, and 0.9. It is clear that the result is a symmetrical chirp that
narrows as the exponent tends to 1.
Figure7.75 shows the FrFT of the rectangular signal, when the FrFT exponent
is 0.99. It can be observed that the transform tends to the sinc signal (which is the
Fourier transform of the rectangle).
The Wigner analysis of the FrFT results for exponents 0.55, 0.7, 0.8, and 0.9
provides a visual explanation of the chirp narrowing that has been noticed in Fig.7.74.
It is a matter of rotation on the time-frequency plane. As the exponent tends to 1, the
rotation tends to 90◦.
Figure7.76 shows the results of the Wigner analysis. A basic technique, thresh-
olding, has been applied to partially remove interferences.
0
50
100
150
200
250
300
350
400
0
0.5
1
0
50
100
150
200
250
300
350
400
-2
0
2
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.7
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.8
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.9
a=0.55
Fig. 7.74 Fractional Fourier transforms of a rectangular signal, using different values of the
exponent

7.11 Experiments
469
180
190
200
210
220
230
240
250
260
-4
-2
0
2
4
6
8
Fig. 7.75 The fractional Fourier transform of the rectangle becomes close to the sinc signal for
a = 0.99
a=0.55
100
200
300
400
50
100
150
200
250
a=0.7
100
200
300
400
50
100
150
200
250
a=0.8
100
200
300
400
50
100
150
200
250
a=0.9
100
200
300
400
50
100
150
200
250
Fig. 7.76 Wigner analysis
7.11.2
Filtered Wigner Analysis of Nature Chirps
Since the term “chirp” was taken from a typical bird song, it seems opportune to do
some justice to birds by considering a brief T-F analysis of two examples.

470
7
Time-Frequency Analysis
7.11.2.1
Bat Chirps (Biosonar)
Radar and sonar are important pieces in our normal life, being used in airports and
aircrafts, ships, trafﬁc, etc. The fact that some animals were also using echolocation
from long time before our inventions, awakes a lot of curiosity and, for good reasons,
an interest on learning more from Nature. It would be recommended to read [8] for a
detailed treatment of biosonars from the signal processing point of view. In addition,
some speciﬁc problems of the T-F analysis of such signals are studied in [74].
Indeed, bats are one of the archetypes when one thinks on animal echolocation.
The bat signal chosen for our example is commonly found in T-F toolboxes. It is also
found in a number of T-F related papers, like for instance [98, 149]. It seems that
bats are able to detect ﬂying insects, taking also into account their wing oscillating
motion.
Figure7.77 shows the result of T-F analysis using ﬁltered Wigner; where the
ﬁltering was made with a rectangular mask in the SAF domain (like in Sect.7.5.5).
Some interference was allowed in order to make visible a light fourth component
of the signal, on top of the other three components. The components are hyperbolic
arcs, so they are “Doppler tolerant” [135].
AninterestingexercisewouldbetotryotherT-Fanalysismethodsforinterference-
free signal representation, like for instance [168]. Also, it would be challenging to
focus on echolocation in the water: dolphins, etc.
Fig. 7.77 Bat chirp
seconds
Hz
0
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09
0.1
0
0.5
1
1.5
2
x 104

7.11 Experiments
471
7.11.2.2
Bird Tweet
Ornithology is a wide scientiﬁc ﬁeld, which also attracts the attention of many
enthusiasts. There are important web sites and research centers for this ﬁeld. The
particular case of bird songs presents some interesting research questions, like for
instance acoustic communication in birds [100], vocalization [125], etc.
A humble, simple bird tweet has been chosen for our example. Figure7.78 shows
the result of using ﬁltered Wigner, with a program very similar to the program used
for the bat chirp. It looks like a linear chirp, so it would be suitable for a fractional
Fourier transform [5, 6].
As a matter of curiosity, let us mention the article [15] on cat auditory cortex
neurons and bird chirps.
7.11.3
Wavelet Analysis of Lung and Heart Sounds
The typical mental image of a pediatric doctor includes a phonendoscope. Surely,
auscultatory sounds are important for diagnosis of certain diseases, concerning res-
piration, heart, etc. Computers could help for the analysis of these sounds (see [53]
and references therein). Next examples focus on lung sounds and heart sounds.
In the three examples considered below a continuous wavelet (CWT) analysis was
applied, using Morlet wavelet. The scalograms visualize information on frequencies,
normal behaviour, and peculiar events.
The ﬁles with lung sounds have been obtained from the R.A.L.E. repository
(web address in the Resources section). Next two examples consider normal, healthy
respiration, and another case with crackles.
Fig. 7.78 Bird tweet
seconds
Hz
0
0.05
0.1
0.15
0
500
1000
1500
2000
2500
3000
3500

472
7
Time-Frequency Analysis
7.11.3.1
Normal Respiration
Background information on the analysis of respiratory sounds can be obtained from
[147]. Known trackers are crackles, cough sound, rhonchs, snoring, squawk, stridor,
and wheeze. The use of computers for respiratory sound analysis is reviewed in [133].
The ﬁle with the bronchial sound of a normal respiration corresponds to a 26 year
old man. Figure7.79 shows the sound signal along 10 s.
Using the computer for zooming on different segments of this signal reveals details
of the diaphragm and lung combined work. There is a periodic triggering of air push-
pull, with different pressures and corresponding different sounds. The response of
the system to the triggering ressembles the step response of a second order system,
with attenuated oscillations at the beginning of the push or pull periods. It seems that
there are elastic phenomena.
Figure7.80 shows a segment of the signal corresponding to one air push and
pull cycle. The wavelets highlight the main oscillations. The second part of the
signal, inspiration, takes longer time. The ﬁrst part, expiration, tends to lower sound
frequency.
The program used in this example includes a last sentence for you to hear the
signal segment.
7.11.3.2
Respiration with Crackles
This example uses a sound ﬁle corresponding to bronchial breathing of left lower
lung of a 16 year old boy with tuberculosis. Figure7.81 shows the sound signal along
10 s. When you hear this signal you immediately notice there are crackles.
Fig. 7.79 Sound of normal
respiration (10 s)
0
1
2
3
4
5
6
7
8
9
10
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
time

7.11 Experiments
473
0
0.5
1
1.5
2
-0.5
0
0.5
sec
signal
samples
scales
500
1000
1500
2000
2500
3000
3500
4000
4500
5
10
15
20
25
30
Fig. 7.80 Scalogram of a signal segment
Fig. 7.81 Sound of
respiration with crackles
(10 s)
0
1
2
3
4
5
6
7
8
9
10
-0.5
0
0.5
1
time
Figure7.82 shows a signal segment and its corresponding scalogram, which is
clearly different to a normal respiration scalogram. The wavelets indicate now the
presence of crackles.
Like before, the program used in this example includes sound you can hear.

474
7
Time-Frequency Analysis
7.8
8
8.2
8.4
8.6
8.8
9
9.2
9.4
-0.5
0
0.5
1
sec
signal
samples
scales
500
1000
1500
2000
2500
3000
3500
10
20
30
Fig. 7.82 Scalogram of a signal segment
7.11.3.3
Heart Sound
This example focuses on heart sound, not to be confused with the electrocardiogram
(ECG). Background information on phonocardiogram signal processing can be found
in [1, 163].
Figure7.83 shows a signal segment and its scalogram. The segment captures
the sound of two consecutive beats. Notice how similar are the two beats on the
scalogram.
A typical problem in auscultation is that respiratory sounds are usually contam-
inated by heart sound. There is a number of research papers proposing methods to
overcome this difﬁculty, like for example [142]. In other order of things, it is interest-
ing to cooment that there are initiatives for using mobile phones as phonendoscopes,
for self-monitoring.
7.11.4
Fan-Chirp Transform of Some Animal Songs
The purpose of the next experiments is to take advantage of the Fan-Chirp transform
features, which are suitable for certain animal songs that match a fan geometry on
the T-F plane.

7.11 Experiments
475
0
0.5
1
1.5
-1
-0.5
0
0.5
1
sec
signal
samples
scales
1000
2000
3000
4000
5000
6000
5
10
15
20
25
Fig. 7.83 Scalogram of heart sound (2 beats)
7.11.4.1
Duck Quack
As a ﬁrst case, the duck-quack has been selected. It is a short signal, only 0.16 s.
Figure7.84 shows the spectrogram, which is not very clear.
If you play a little with the Fan-Chirp transform, you will notice that it admits the
manual adjusting of some parameters, and the images obtained could include more
or less details as you consider them opportune for study.
Fig. 7.84 Spectrogram of
the quack
Time
Frequency
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
1000
2000
3000
4000
5000

476
7
Time-Frequency Analysis
Fig. 7.85 Fan-Chirp
transform of the quack
Time
Frequency
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
1000
2000
3000
4000
5000
Figure7.85 shows a result obtained with the Fan-Chirp transform of the quack,
using a version of the transform that uses a series of time-windows (like the spec-
trogram). The presence of a series of tilted chirps is clear. There are three evident
harmonics; the one on top having a curved shape. Curved shapes might correspond
to messages targeted to a certain distance.
The program for this example is quite similar to the program used before
(Sect.7.10.2) for the Fan-Chirp transform of the linear chirp.
Fig. 7.86 Spectrogram of
the dog bark
Time
Frequency
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0
2000
4000
6000
8000
10000

7.11 Experiments
477
7.11.4.2
Dog Bark
This example is an usual dog bark. Its spectrogram, as depicted in Fig.7.86 shows
a kind of wou-aah–hou main structure inside a strident energetic cloud. Actually,
that structure of 3 segments has been described in the literature as having a “chevron
shape” [109, 123]. According with [109], the noisy cloud is part of the ‘canonical
bark’.
This sound seems to be not particularly well suited for a fan structure on the T-F
plane. Anyway, the Fan-Chirp transform has been applied, with the result shown in
Fig.7.87.
There is controversy on why dogs bark, as can be noticed from the [178] introduc-
tory discussion. It seems that barking characteristics change in function of contexts,
like experiencing a disturbance or just playing. The contribution of [109] puts the
accent on mobbing, while extending the perspective to other animals that bark. An
interesting observation, being investigated in [140], is that repetitive vocalizations
can have internal variations that encodes some information content (for instance,
alarm calls telling about the species of a predator).
Besides barking, there are other types of canid songs. For instance, [122] studies
the differences on information content of coyote barks and howls. Howls, with his
tonal, frequency modulated, relatively long vocalizations, seem to be optimal for
long distance information transmission. Barks seem appropriate for alarm calls, for
acoustic ranging, and for orientation towards the sound source. Barks and howls
probably have complementary purposes.
Many of the recent papers on Nature signals do use time-frequency analysis tools,
and other contributions from signal processing and transmission theory and practice.
Fig. 7.87 Fan-Chirp
transform of the dog bark
Time
Frequency
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0
2000
4000
6000
8000
10000

478
7
Time-Frequency Analysis
7.11.5
Modiﬁed S-Transform Analysis of Some Cases
As it was remarked in Sect.7.10.1, one of the S-transform virtues is that it is adequate
for cases with sustained sinusoidal oscillations. The next two examples were selected
to exploit this feature.
7.11.5.1
Respiration with Wheezing
In this example, we come back to lung sounds, using the R.A.L.E. repository. Now,
the focus is put on wheezing.
Figure7.88 shows the respiration sound recorded over the right anterior upper
chest of an 8 year old boy with asthma. Wheezing occurs at several segments of the
signal. The middle plot in the ﬁgure zooms on a part of the signal where wheezing
appears as an increase of line width. Further zooming reveals, as depicted in the
bottom plot, the higher frequency oscillation corresponding to the wheeze.
Figure7.89 shows the spectrogram of the selected signal segment. It is a relatively
clear visualization that shows 2–3 harmonics that correspond to the wheeze.
After some easy adjusting, the modiﬁed S-transform obtains the result presented
in Fig.7.90, which emphasizes, with better contrast and detail, relevant aspects.
0
1
2
3
4
5
6
7
8
9
10
-1
0
1
Respiration signal, complete
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
-0.5
0
0.5
selected signal segment
1.56
1.58
1.6
1.62
1.64
1.66
1.68
1.7
1.72
1.74
1.76
-0.1
0
0.1
zoom on wheezing part
Fig. 7.88 Respiration with wheezing, 3 levels of detail

7.11 Experiments
479
Fig. 7.89 Spectrogram of
the signal segment with
wheezing
Time
Frequency
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
200
400
600
800
1000
1200
Fig. 7.90 Modiﬁed
S-transform of the signal
segment with wheezing
Time
Frequency
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0
200
400
600
800
1000
1200
7.11.5.2
Whale Song
Some whale songs have a kind of mysterious, novelistic character. Perhaps this is
one of the reasons for the growing interest on thse songs; or perhaps another reason
could be better ways for acoustic underwater communication. There is a number of
web sites with whale and dolphin songs (web address in the Resources section).
A relatively long sound ﬁle has been selected for this experiment. It is a whale
moan call. Since the computer work takes a long time, the ﬁle has been divided into
two parts. Figure7.91 shows the spectrograms of this sound.
The modiﬁed S-transform has been applied to the sound ﬁle in the same manner,
dividing it into two parts. The result is presented in Fig.7.92.
The program used for this experiment has a last sentence for you to hear the whale
call.

480
7
Time-Frequency Analysis
Time
Frequency
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Time
Frequency
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Fig. 7.91 Spectrogram of whale song (divided into 2 parts)
Time
Frequency
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Time
Frequency
2.2
2.4
2.6
2.8
3
3.2
3.4
3.6
3.8
4
0
200
400
600
800
1000
1200
1400
1600
1800
Fig. 7.92 Modiﬁed S-transform of whale song (divided into 2 parts)
Anautomaticclassiﬁcationofkillerwhalevocalizationsusingdynamictimewarp-
ing has been presented in [29]. Since whale sounds seems to be composed of several
components along time, a segmentation of the killer whale vocalization has been
proposed by [2]. The broadband social acoustic signalling of delphinids is studied
in [102]. Some aspects of dolphin’s signals are discussed by [152].
7.11.5.3
An Earthquake in the Gulf of California
A lot of scientiﬁc, engineering, technical, etc. efforts are devoted to earthquakes.
There is a vast literature on this ﬁeld. The time-frequency methodology is a logical
choice for the study of some ground motion aspects.
There are important web sites with lots of information and data ﬁles (see the
Resources section). Background information on processing techniques in earthquake
seismology is available from an open access book [77]. In addition, [164] presents a
practical study of a real earthquake.

7.11 Experiments
481
There are two main classes of seismic waves: body and surface waves. Body waves
travel through the interior of the earth, and are of higher frequencies than surface
waves. A further decomposition into wave types is:
• Body waves:
– P-waves (primary waves), which are the fastest waves
– S-waves (secondary wave), which are the second arriving waves
P-waves are longitudinal waves, and S-waves are lateral and more destructive.
• Surface waves:
– Love waves, which move the ground from side to side, producing horizontal
motion
– Rayleigh waves are like ocean waves, producing vertical and horizontal motion
(in the same direction of the wave motion)
ThedelaybetweenarrivalsofPwavesandSwavesprovidesimportantinformation
for geophysical analysis. For this reason the example we selected is one in which
both arrivals are clearly observed on the seismogram. It is the case of a 6.7 earthquake
in the Gulf of California, on 2010-10-21.
Images with the records obtained from 3-channel professional seismographs are
available from the IRIS Wilber web site. In addition, data ﬁles with seismograms
from educational entities can be obtained from the IRIS Seismographs in Schools
web site. In our case, we selected the record provided by the Oregon Shakes station
at Depoe Bay.
The standard format for earthquake data ﬁles is called “Seismic Analysis Code”.
Files are named with the extension “.sac”. Each ﬁle has a header with 3 matrices
of ﬂoating numbers, integer numbers, and characters. The series of data comes after
the header. The program made for this example reads a .sac ﬁle with the seismogram
data. More information on the SAC data ﬁle format is available from the web site
cited in the Resources section.
Figure7.93 shows the time history signal to be analyzed. Indications of the arrivals
of P waves, S waves, and surface waves have been added to the plot on top. For our
study we selected the main initial part of the signal (bottom plot).
The spectrogram of the selected signal segment, Fig.7.94, is not very clear,
although it gives information on the frequencies.
The application of the modiﬁed S-transform reveals more details of the seismic
signal. Figure7.95. In particular, the three concentrations of energy corresponding
to the P waves, the S waves and the surface waves can be clearly discerned. It is also
interesting to observe the behaviour of the frequency ranges and boundaries.
As it was shown in [139], it is possible to extract a region of interest in the
S-transform result, using a mask (like the basic elimination of interference,
Sect.7.5.5.). Then, by application of inverse S-transform one could recover the P-
wave or any other detectable component. With a similar technique, it is also possible
to denoise the earthquake signal.

482
7
Time-Frequency Analysis
0
2
4
6
8
10
12
14
16
18
-4
-2
0
2
4
sec
0
2
4
6
8
10
12
-5
0
5
sec
Fig. 7.93 The Earthquake signal at two detail levels
Fig. 7.94 Spectrogram of
the signal segment
Time
Frequency
1
2
3
4
5
6
7
8
9
10
11
0
10
20
30
40
50
60
70
80
90
100
Taking the opportunity offered by the earthquake example, here is a demonstration
of how a T-F region can be extracted in order to obtain a signal of interest. By using
a rectangular mask, the region corresponding to the S wave has been selected, as
shown in Fig.7.96.

7.11 Experiments
483
Fig. 7.95 Modiﬁed
S-transform of the signal
segment
Time
Frequency
0
2
4
6
8
10
12
0
10
20
30
40
50
60
70
80
90
100
Fig. 7.96 Extraction of a
T-F region of interest
500
1000
1500
2000
200
400
600
800
1000
1200
Now, if one applies the inverse S-transform to this region, one obtains the result
shown in Fig.7.97 which would be an approximation to the S wave.
The program made for this example contains a part with the S-transform inversion,
which is based on the istran() function available from MathWorks ﬁle exchange.
In relation with the analysis of earthquake data, it is worthwhile to refer to
[18, 54, 138, 151, 180, 182].

484
7
Time-Frequency Analysis
0
500
1000
1500
2000
2500
-1
-0.5
0
0.5
1
800
900
1000
1100
1200
1300
1400
-1
-0.5
0
0.5
1
Fig. 7.97 The extracted signal segment at two levels of detail
7.12
Resources
7.12.1
MATLAB
7.12.1.1
Toolboxes
• LTFAT: The Large Time-Frequency Analysis Toolbox:
http://ltfat.sourceforge.net/
• The Time-Frequency Toolbox:
http://tftb.nongnu.org/
• TFSAP: Time-Frequency Signal Analysis & Processing Toolbox:
http://time-frequency.net/tf/
• XBAT: Bioacoustics, animal sounds:
http://www.birds.cornell.edu/brp/software/xbat-introduction
• EEGLAB: Electrophysiological signal processing:
http://sccn.ucsd.edu/eeglab/
• Auditory Modeling Toolbox (AMT):
http://amtoolbox.sourceforge.net/
• WarpTB: Matlab Toolbox for Warped DSP:
http://legacy.spa.aalto.ﬁ/software/warp/

7.12 Resources
485
7.12.1.2
Matlab Code
• Discrete TFDs:
http://tfd.sourceforge.net/
• Fractional Fourier Transform (NALAG):
http://nalag.cs.kuleuven.be/research/software/FRFT/
• Reassignment (K. Fitz):
http://www.cerlsoundgroup.org/Kelly/timefrequency.html
• Reassignment (Boston Lab):
http://people.bu.edu/timothyg/styled-7/index.html
• Recursive Reassignment (G. K. Nilsen):
http://www.ii.uib.no/~geirkn/rrspec/
• COVAREP project (speech technologies):
https://github.com/covarep/covarep
• Cardiovascular signals:
http://www.micheleorini.com/matlab-code/
• Empirical Mode Decomposition (MIT):
http://www.mit.edu/~gari/CODE/HRV/emd.m
• Empirical Mode Decomposition (P. Flandrin):
http://perso.ens-lyon.fr/patrick.ﬂandrin/emd.html
• Project SEIZMO (Earthquakes):
http://epsc.wustl.edu/~ggeuler/codes/m/seizmo/
• Geophysical, earthquakes, etc.:
http://geoweb.princeton.edu/people/simons/software.html
• Geophysical Wavelet Library:
http://users.math.uni-potsdam.de/~gwl/
7.12.2
Internet
An important source of knowledge and MATLAB code is the Time-Frequency Tool-
box of the gdr-isis.org.
7.12.2.1
Web Sites
• Steve Mann:
http://www.eecg.toronto.edu/~mann/
• The Fractional Fourier Transform (book):
http://kilyos.ee.billent.edu.tr/~haldun/wileybook.html
• The Chirplet Transform:
http://wearcam.org/chirplet.htm
• Empirical Mode Decomposition:
https://www.clear.rice.edu/elec301/Projects02/empiricalMode/

486
7
Time-Frequency Analysis
• PhysioBank Archive Index (Electrocardiograms, etc.):
http://www.physionet.org/physiobank/database/
• R.A.L.E. repository (Lung sounds):
http://www.rale.ca/Default.htm
• Ocean Mammal Institute (Whale songs):
http://www.oceanmammalinst.com/songs.html
• CornellLab (Whale sounds):
http://www.listenforwhales.org/page.aspx?pid=442
• IRIS Seismographs in Schools (Earthquake data ﬁles):
http://www.iris.edu/hq/ssn/events
• IRIS Wilber (Earthquake technical information):
http://ds.iris.edu/wilber3/ﬁnd_event
• SAC Data File Format (format of Earthquake data ﬁles):
http://ds.iris.edu/ﬁles/sac-manual/manual/ﬁle_format.html
• Vibrationdata:
http://www.vibrationdata.com
• BIRDNET (birds):
http://www.nmnh.si.edu/BIRDNET/
• Song Bird Science:
http://songbirdscience.com/
• Tyson Hilmer’s website (sonar, spectrograms):
http://www.tysonhilmer.com/
7.12.2.2
Link Lists
• MATLAB Audio Processing:
http://www.ee.columbia.edu/ln/rosa/matlab/
• MATLAB Toolboxes
http://stommel.tamu.edu/~baum/toolboxes.html
References
1. A.K. Abbas, R. Bassam, Phonocardiography signal processing. Synth. Lect. Biomed. Eng.
4(1), 1–194 (2009)
2. O. Adam, Segmentation of killer whale vocalizations using the Hilbert-Huang transform.
EURASIP J. Adv. Signal Process. ID 245936, 1–10 (2008)
3. L. Aguiar-Conraria, M.J. Soares, The continuous wavelet transform. Technical report, NIPE
WP 16/2011 Universidade do Minho, Portugal (2011)
4. T. Alieva, V. Lopez, F. Agullo-Lopez, L.B. Almeida, The fractional Fourier transform in
optical propagation problems. J. Modern Opt. 41(5), 1037–1044 (1994)
5. O.A. Alkishriwo, The discrete linear chirp transform and its applications. Ph.D. thesis, Uni-
versity of Pittsburg (2006)

References
487
6. O.A. Alkishriwo, L.F. Chaparro, A. Akan, Signal separation in the Wigner distribution domain
using fractional Fourier transform, in Proceedings of the 19th European Signal Processing
Conference (2011), pp. 1879–1883
7. L.B. Almeida, The fractional Fourier transform and time-frequency representations. IEEE
Trans. Signal Process. 42(11), 3084–3091 (1994)
8. R.A. Altes, Signal processing for target recognition in biosonar. Neural Netw. 8(7), 1275–1295
(1995)
9. R. Ashino, M. Nagase, R. Vaillancourt, Gabor, wavelet and chirplet transforms in the study
of pseudodifferential operators (1998). http://shigi.cc.osaka-kyoiku.ac.jp/~ashino/pdf/2528.
pdf
10. S. Assous, B. Boashash, Evaluation of the modiﬁed S-transform for time-frequency synchrony
analysis and source localization. EURASIP J. Adv. Signal Process. 2012(49), 1–18 (2012)
11. F. Auger, Time-frequency reassignment (2001). http://perso.ens-lyon.fr/patrick.ﬂandrin/
fapfecm.pdf
12. F. Auger, P. Flandrin, Improving the readability of time-frequency and time-scale represen-
tations by the reassignment method. IEEE Trans. Signal Process. 43(5), 1068–1089 (1995)
13. F. Auger, P. Flandrin, P. Goncalves, O. Lemoine, Time-frequency toolbox tutorial (1995).
http://tftb.nongnu.org/
14. E. Azarov, A. Petrovsky, M. Parﬁeniuk, High-quality time stretch and pitch shift effects for
speechandaudiousingtheinstantaneousharmonicanalysis.EURASIPJ.Adv.SignalProcess.
ID 712749, 1–10 (2010)
15. O. Bar-Yosef, Y. Rotman, I. Nelken, Responses of neurons in cat primary auditory cortex to
bird chirps: effects of temporal and spectral context. J. Neurosci. 22(19), 8619–8632 (2002)
16. R.G. Baraniuk, D.L. Jones, Warped wavelet bases: unitary equivalence and signal processing.
Proc. IEEE Int. Conf. Acoust. Speech Signal Process. 3, 320–323 (1993)
17. R.G. Baraniuk, D.L. Jones, Unitary equivalence: a new twist on signal processing. IEEE
Trans. Signal Process. 43(10), 2269–2282 (1995)
18. T. Bardainne, P. Gaillot, N. Dubos-Sallée, J. Blanco, G. Sénéchal, Characterization of seismic
waveforms and classiﬁcation of seismic events using chirplet atomic decomposition. Example
from the Lacq gas ﬁeld (Western Pyrenees, France). Geophys. J. Int. 166(2), 699–718 (2006)
19. M. Bartkowiak, Application of the fan-chirp transform to hybrid sinusoidal+noise modeling of
polyphonic audio, in Proceedings of the European Signal Processing Conference (EUSIPCO)
(2008), pp. 1–10
20. J.J. Benedetto, C. Heil, D.F. Walnut, Gabor systems and the Balian-Low theorem, in Gabor
Analysis and Algorithms (Birkhäuser, Boston, 1998), pp. 85–122
21. M. Benzi, N. Razouk, On the Iwasawa decomposition of a symplectic matrix. Appl. Math.
Lett. 20, 260–265 (2007)
22. M. Bertha, J.C. Golinval, Experimental modal analysis of a beam travelled by a moving mass
using Hilbert vibration decomposition, in Proceedings of the 9th International Conference on
Structural Dynamics, EURODYN (2014), pp. 2789–2795
23. J. Bertrand, P. Bertrand, J. Ovarlez, The Mellin transform, in The Transforms and Applications
Handbook, ed. by A.D. Poularikas (CRC Press, Boca Raton, 2000)
24. E. Biner, O. Akay, Digital computation of the fractional Mellin transform, in Proceedings of
the 13th European Signal Processing Conference (EUSIPCO’05) (2005), pp. 1–4
25. B. Blankertz, The constant Q transform (2005). http://doc.ml.tu-berlin.de/bbci/material/
publications/Bla_constQ.pdf
26. B. Boashash, Time Frequency Analysis (Elsevier, Amsterdam, 2003)
27. H. Bolcskei, F. Hlawatsch, Discrete Zak transforms, polyphase transforms, and applications.
IEEE Trans. Signal Process. 45(4), 851–866 (1997)
28. J.C. Brown, Calculation of a constant Q spectral transform. J. Acoust. Soc. Am. 89(1), 425–
434 (1991)
29. J.C. Brown, P.J. Miller, Automatic classiﬁcation of killer whale vocalizations using dynamic
time warping. J. Acoust. Soc. Am. 122(2), 1201–1207 (2007)

488
7
Time-Frequency Analysis
30. J.C.Brown,M.S.Puckette,AnefﬁcientalgorithmforthecalculationofaconstantQtransform.
J. Acoust. Soc. Am. 92(5), 2698–2701 (1992)
31. A. Bultan, A four-parameter atomic decomposition of chirplets. IEEE Trans. Signal Process.
47(3), 731–745 (1999)
32. A.Bultheel,Atwo-phase implementationofthe fractional Fouriertransform.Technical report,
TW 588, Department of Computer Science, K.U. Leuven (2011)
33. A. Bultheel, H. Martínez-Sulbaran, A shattered survey of the fractional Fourier transform.
Technical report, TW 337, Department of Computer Science, K.U. Leuven (2002)
34. A. Bultheel, H. Martínez-Sulbaran, Computation of the fractional Fourier transform. Appl.
Comput. Harmon. Anal. 16(3), 182–202 (2004)
35. A. Bultheel, H. Martínez-Sulbaran, Recent developments in the theory of the fractional Fourier
and linear canonical transforms. Bull. Belg. Math. Soc.-Simon Stevin 13(5), 971–1005 (2007)
36. R.G. Campos, J. Figueroa, A fast algorithm for the linear canonical transform. Signal Process.
91(6), 1444–1447 (2011)
37. P. Cancela, E. López, M. Rocamora, Fan chirp transform for music representation, in Proceed-
ings of the 13th International Conference on Digital Audio Effects DAFx10, Graz, Austria
(2010), pp. 1–8
38. C. Capus, Y. Rzhanov, L. Linnett, The analysis of multiple linear chirp signals, in Proceedings
of the IEE Seminar on Time-Scale and Time-Frequency Analysis and Applications (2000), pp.
4/1–4/7
39. L. Cohen, Time-frequency distributions-a review. Proc. IEEE 77(7), 941–981 (1989)
40. L. Cohen, The scale representation. IEEE Trans. Signal Process. 41(12), 3275–3292 (1993)
41. L. Cohen, Time-Frequency Analysis (Prentice Hall, Englewood Cliffs, 1995)
42. D.M. Cowell, S. Freear, Separation of overlapping linear frequency modulated (LFM) sig-
nals using the fractional Fourier transform. IEEE Trans. Ultrason. Ferroelectr. Freq. Control
57(10), 2324–2333 (2010)
43. P.K. Dash, K.B. Panigrahi, G. Panda, Power quality analysis using S-transform. IEEE Trans.
Power Deliv. 18(2), 406–411 (2003)
44. M. Davis, Radar frequencies and waveforms. Conference presentation, Georgia Technology
(2003). http://www.its.bldrdoc.gov/media/31078/DavisRadar_waveforms.pdf
45. A. De Sena, D. Rocchesso, A fast Mellin transform with applications in DAFX, in Proceedings
of the 7th International Conference on Digital Audio Effects (DAFx’04) (2004), pp. 65–69
46. A. De Sena, D. Rocchesso, A study on using the Mellin transform for vowel recognition, in
Proceedings of the 7th International Conference on Digital Audio Effects (DAFx’04) (2004),
pp. 5–8
47. A. De Sena, D. Rocchesso, A fast Mellin and scale transform. EURASIP J. Adv. Signal
Process. ID 89170, 1–9 (2007)
48. W.J. DeMeo, Characterizing musical signals with Wigner-Ville interferences. Proc. ICMC 2,
1–8 (2002)
49. R. Ditommaso, M. Mucciarelli, F.C. Ponzo, S-transform based ﬁlter applied to the analysis
of non-linear dynamic behaviour of soil and buildings, in Proceedings of the 14th European
Conference on Earthquake Engineering, vol. 30 (2010), pp. 1–8
50. K. Dragomiretskiy, D. Zosso, Variational mode decomposition. IEEE Trans. Signal Process.
62(3), 531–544 (2014)
51. R. Dunn, T.F. Quatieri, Sinewave analysis/synthesis based on the fan-chirp transform, in
Proceedings of the IEEE Workshop. Applications of Signal Processing to Audio and Acoustics
(2009), pp. 247–250
52. I.J.H. Ender, Introduction to Radar Part I. Ruhr-Universität Bochum (2011). Available on
Internet
53. T.H. Falk, E. Sejdic, T. Chau, W.Y. Chan, Spectro-temporal analysis of auscultatory sounds,
in New Developments in Biomedical Engineering, ed. by D. Campolo (INTECH, 2010)
54. J. Fan, P. Dong, Time-frequency analysis of earthquake record based on S-transform and its
effect on structural seismic response, in Proceedings of the IEEE International Conference
on Engineering Computation, ICEC’09 (2009), pp. 107–109

References
489
55. D.C. Farden, L.L. Scharf, A uniﬁed framework for the Sussman, Moyal, and Janssen formulas.
IEEE Signal Process. Mag. 23(3), 124–125 (2006)
56. H.G. Feichtinger, T. Strohmer, Gabor Analysis and Algorithms: Theory and Applications
(Birkhäuser, Boston, 1998)
57. M. Feldman, Time-varying vibration decomposition and analysis based on the Hilbert trans-
form. J. Sound Vib. 295(3–5), 518–530 (2006)
58. M. Feldman, Hilbert Transform Applications in Mechanical Vibration (Wiley, New York,
2011)
59. M. Feldman, Hilbert transform in vibration analysis. Mech. Syst. Signal Process. 25, 735–802
(2011)
60. P. Flandrin, Time-frequency and chirps, in Proceedings of the SPIE-AeroSense’01 (2001).
http://perso.ens-lyon.fr/patrick.ﬂandrin/publis.html
61. P. Flandrin, Ambiguity functions, in Time-Frequency Signal Analysis and Processing, ed. by
B. Boashash (Elsevier, Amsterdam, 2003), pp. 160–167
62. P. Flandrin, F. Auger, E. Chassande-Mottin, Time-frequency reassignment: from principles
to algorithms. Appl. Time-Freq. Signal Process. 5, 179–203 (2003)
63. P. Flandrin, P. Gonçalves, Geometry of afﬁne time-frequency distributions. Appl. Comput.
Harmon. Anal. 3(1), 10–39 (1996)
64. S.A. Fulop, K. Fitz, Algorithms for computing the time-corrected instantaneous frequency
(reassigned) spectrogram, with applications. J. Acoust. Soc. Am. 119(1), 360–371 (2006)
65. S.A. Fulop, K. Fitz, Separation of components from impulses in reassigned spectrograms. J.
Acoust. Soc. Am. 121(3), 1510–1518 (2007)
66. D. Gabor, Theory of communication. Part 1: the analysis of information. J. Inst. Electr. Eng.-
Part III: Radio Commun. Eng. 93(26), 429–441 (1946)
67. R.X. Gao, R. Yan, From Fourier transform to wavelet transform: a historical perspective, in
Wavelets: Theory and Applications (Springer, New York, 2011), pp. 17–32
68. P.K. Ghosh, T.V. Sreenivas, Time-varying ﬁlter interpretation of Fourier transform and its
variants. Signal Process. 86(11), 3258–3263 (2006)
69. J. Gilles, Empirical wavelet transform. IEEE Trans. Signal Process. 61(16), 3999–4010 (2013)
70. C. Golé, Symplectic Twist Maps: Global Variational Techniques, vol. 18 (World Scientiﬁc,
Singapore, 2001)
71. O. González-Gaxiola, J.A. Santiago, An α-Mellin transform and some of its applications. Int.
J. Contemp. Math. Sci. 7(45–48), 2353–2361 (2012)
72. R. Gribonval, Fast matching pursuit with a multiscale dictionary of Gaussian chirps. IEEE
Trans. Signal Process. 49(5), 994–1001 (2001)
73. K. Gröchenig, Foundations of Time-Frequency Analysis (Birkhäuser, Boston, 2001)
74. T. Gudra, K. Herman, Some problems of analyzing bio-sonar echolocation signals generated
by echolocating animals living in the water and in the air. J. Acoust. Soc. Am. 123(5), 3778–
3778 (2008)
75. J.B. Harley, Y. Ying, J.M. Moura, I.J. Oppenheim, L. Sobelman, J.H. Garrett, D.E. Chimenti,
Application of Mellin transform features for robust ultrasonic guided wave structural health
monitoring. Proc. AIP Conf.-Am. Inst. Phys. 1, 1551–1559 (2012)
76. S. Harput, Use of chirps in medical ultrasound images. Ph.D. thesis, University of Leeds
(2012)
77. J. Havskov, L. Ottemöller, Processing earthquake data (2009). ftp://ftp.geo.uib.no/pub/
seismo/SOFTWARE/DOCUMENTATION/processing_earthquake_data.pdf
78. J.J. Healy, J.T. Sheridan, Analytical and numerical analysis of ABCD systems. Proc. SPIE
6994, 402–1 (2008) (pp. 402 1–8)
79. C. Heil, A frame? Not. AMS 60(6), 748–750 (2013)
80. B.M. Hennelly, J.T. Sheridan, Generalizing, optimizing, and inventing numerical algorithms
for the fractional Fourier, Fresnel, and linear canonical transforms. J. Opt. Soc. Am. A 22(5),
917–927 (2005)
81. F. Hlawatsch, H. Bölcskei, Uniﬁed theory of displacement-covariant time-frequency analy-
sis, in Proceedings of the IEEE-SP International Symposium on Time-Frequency Time-Scale
Analysis (TFTS-94), Philadelphia (PA) (1994), pp. 524–527

490
7
Time-Frequency Analysis
82. F. Hlawatsch, G.F. Boudreaux-Bartels, Linear and quadratic time-frequency signal represen-
tation. IEEE Signal Process. Mag. 9(2), 21–67 (1992)
83. F. Hlawatsch, T.G. Manickam, R.L. Urbanke, W. Jones, Smoothed pseudo-Wigner distribu-
tion, Choi-Williams distribution, and cone-kernel representation: ambiguity-domain analysis
and experimental comparison. Signal Process. 43(2), 149–168 (1995)
84. N. Holighaus, M. Dorﬂer, G.A. Velasco, T. Grill, A framework for invertible, real-time
constant-Q transforms. IEEE Trans. Audio Speech Lang. Process. 21(4), 775–785 (2013)
85. D.D. Holm, Notes on Linear Symplectic Transformations. Handout, Imperial College London
(2012) (Available on Internet)
86. T.Y. Hou, Z. Shi, Data-driven time-frequency analysis. Appl. Comput. Harmon. Anal. 35(2),
284–308 (2013)
87. C.C. Huang, S.F. Liang, M.S. Young, F.Z. Shaw, A novel application of the S-transform in
removing powerline interference from biomedical signals. Physiol. Meas. 30(1), 13–27 (2009)
88. N.E. Huang, N.O. Attoh-Okine, The Hilbert-Huang Transform in Engineering (CRC Press,
Boca Raton, 2005)
89. N.E. Huang, Z. Wu, A review on Hilbert-Huang transform: method and its applications to
geophysical studies. Rev. Geophys. 46(2), 1–23 (2008)
90. N.E. Huang, Z. Shen, S.R. Long, M.C. Wu, H.H. Shih, Q. Zheng, H.H. Liu, The empirical
mode decomposition and the Hilbert spectrum for nonlinear and non-stationary time series
analysis. Proc. R. Soc. Lond. A: Math. Phys. Eng. Sci. 454, 903–995 (1998)
91. J.J. Hwang, S.G. Cho, J. Moon, J.W. Lee, Nonuniform DFT based on nonequispaced sampling.
WSEAS Trans. Inf. Sci. Appl. 2(9), 1403–1408 (2005)
92. C. Ioana, A. Quinquis, Y. Stephan, Feature extraction from underwater signals using time-
frequency warping operators. IEEE J. Ocean. Eng. 31(3), 628–645 (2006)
93. R. Iwai, H. Yoshimura, High-accuracy and high-security individual authentication by the
ﬁngerprint template generated using the fractional Fourier transform, in Fourier Transforms
– Approach to Scientiﬁc Principles, ed. by G. Nikolic (InTech Open, 2011)
94. D. Jenn, Radar fundamentals. Seminar presentation, Naval Postgraduate School, Monterey
(2011). http://faculty.nps.edu/jenn/Seminars/RadarFundamentals.pdf
95. K.A. Jones, B. Porjesz, D. Chorlian, M. Rangaswamy, C. Kamarajan, A. Padmanabhapil-
lai, H. Begleiter, S-transform time-frequency analysis of p300 reveals deﬁcits in individuals
diagnosed with alcoholism. Clin. Neurophysiol. 117(10), 2128–2143 (2006)
96. M. Képesi, L. Weruaga, Adaptive chirp-based time-frequency analysis of speech signals.
Speech Commun. 48, 474–492 (2006)
97. D. Kim, Introduction to EMD (empirical mode decomposition) with application to a sci-
entiﬁc data. Seminar presentation (2006). http://dasan.sejong.ac.kr/~dhkim/main/research/
talks/EMDintroSeminar.pdf
98. Y. Kopsinis, E. Aboutanios, D.A. Waters, S. McLaughlin, Time-frequency and advanced
frequency estimation techniques for the investigation of bat echolocation calls. J. Acoust.
Soc. Am. 127(2), 1124–1134 (2010)
99. J. Kovacevic, A. Chebira, Life beyond bases: the advent of frames. IEEE Signal Process. Mag.
24, 86–104 (2007)
100. A. Kumar, Acoustic communication in birds. Resonance 8(6), 44–55 (2003)
101. M.A. Kutay, H.M. Ozaktas, O. Ankan, L. Onural, Optimal ﬁltering in fractional Fourier
domains. IEEE Trans. Signal Process. 45(5), 1129–1143 (1997)
102. M.O. Lammers, W.W. Au, D.L. Herzing, The broadband social acoustic signaling behavior
of spinner and spotted dolphins. J. Acoust. Soc. Am. 114(3), 1629–1639 (2003)
103. K.G. Larkin, A beginner’s guide to the fractional Fourier transform, part 1. Aust. Opt. Soc.
News 9(2), 18–21 (1995)
104. L. Lee, R. Rose, A frequency warping approach to speaker normalization. IEEE Trans. Speech
Audio Process. 6(1), 49–60 (1998)
105. C.F. Lin, J.D. Zhu, Hilbert-Huang transformation-based time-frequency analysis meth-
ods in biomedical signal applications. Proc. Inst. Mech. Eng. Part H: J. Eng. Med.
0954411911434246 (2012)

References
491
106. G. Livanos, N. Ranganathan, J. Jiang, Heart sound analysis using the S-transform. Proceedings
IEEE Comput. Cardiol. 27, 587–590 (2000)
107. J. Locke, P.R. White, The performance of methods based on the fractional Fourier transform
for detecting marine mammal vocalizations. J. Acoust. Soc. Am. 130(4), 1974–1984 (2011)
108. A.W. Lohmann, Image rotation, Wigner rotation, and the fractional Fourier transform. J. Opt.
Soc. Am. A 10, 2181–2186 (1993)
109. K. Lord, M. Feinstein, R. Coppinger, Barking and mobbing. Behav. Process. 81(3), 358–368
(2009)
110. Y. Lu, A. Kasaeifard, E. Oruklu, J. Saniie, Fractional Fourier transform for ultrasonic chirplet
signal decomposition. Adv. Acoust. Vib. 2012, 1–13 (2012)
111. I. Magrin-Chagnolleau, R.G. Baraniuk, Empirical mode decomposition based frequency
attributes, in Proceedings of the 69th SEG Meeting (1999), pp. 1949–1952
112. B.R. Mahafza, Radar System Analysis and Design Using MATLAB (Chapman & Hall/CRC,
Boca Raton, 2005)
113. S.G. Mallat, Z. Zhang, Matching pursuit with time-frequency dictionaries. IEEE Trans. Signal
Process. 41(12), 3397–3415 (1993)
114. S. Mann, S. Haykin, The chirplet transform: a generalization of Gabor’s logon transform. Vis.
Interface 91, 205–212 (1991)
115. S. Mann, S. Haykin, Adaptive chirplet transform: an adaptive generalization of the wavelet
transform. Opt. Eng. 31(6), 1243–1256 (1992)
116. S. Mann, S. Haykin, Time-frequency perspectives: the “chirplet” transform. Proc. IEEE Int.
Conf. Acoust. Speech Signal Process. 3, 417–420 (1992)
117. L. Masinha, R.G. Stockwell, R.P. Lowe, Pattern analysis with two-dimensional spectral local-
ization: applications of two-dimensional S-transforms. Phys. A 239, 286–295 (1997)
118. P.D. McFadden, J.G. Cook, L.M. Forster, Decomposition of gear vibration signals by the
generalised S-transform. Mech. Syst. Signal Process. 13(5), 691–707 (1999)
119. D. Mendlovic, H.M. Ozaktas, Fractional Fourier transforms and their optical implementation:
I. J. Opt. Soc. Am. A 10, 1875–1881 (1993)
120. A. Mertins, Signal Analysis: Wavelets, Filter Banks, Time-Frequency Transforms and Appli-
cations (Wiley, New York, 1999)
121. P.A. Millette, The Heisenberg uncertainty principle and the Nyquist-Shannon sampling the-
orem (2011). arXiv:1108.3135
122. B.R. Mitchell, M.M. Makagon, M.M. Jaeger, R.H. Barrett, Information content of coyote
barks and howls. Bioacoustics 15(3), 289–314 (2006)
123. E.S. Morton, Animal communication: What do animals say? Am. Biol. Teach. 45(6), 343–348
(1983)
124. A. Naït-Ali (ed.), Advanced Biosignal Processing (Springer, New York, 2009)
125. L. Neal, F. Briggs, R. Raich, X.Z. Fern, Time-frequency segmentation of bird song in noisy
acoustic environments, in Proceedings of the IEEE International Conference on Acoustic
Speech and Signal Processing (ICASSP) (2011), pp. 2012–2015
126. Y. Nikolova, α-Mellin transform and one of its applications. Math. Balk. 26(1–2), 185–190
(2012)
127. G.K. Nilsen, Recursive time-frequency reassignment. Master’s thesis, University of Bergen
(2007)
128. J.C. O’Neill, P. Flandrin, Virtues and vices of quartic time-frequency distributions. IEEE
Trans. Signal Process. 48(9), 2641–2650 (2000)
129. J.P. Ovarlez, J. Bertrand, P. Bertrand, Computation of afﬁne time-frequency distributions using
the fast Mellin transform. Proc. IEEE Int. Conf. Acoust. Speech Signal Process. 5, 117–120
(1992)
130. H.M. Ozaktas, D. Mendlovic, Fourier transforms of fractional order and their optical inter-
pretation. Opt. Commun. 101, 163–169 (1993)
131. H.M. Ozaktas, M.A. Kutay, G. Bozdag, Digital computation of the fractional Fourier trans-
form. IEEE Trans. Signal Process. 44, 2141–2150 (1996)

492
7
Time-Frequency Analysis
132. H.M. Ozaktas, Z. Zalevsky, M.A. Kutay, The Fractional Fourier Transform (Wiley, New York,
2001)
133. R. Palaniappan, K. Sundaraj, N.U. Ahamed, A. Arjunan, S. Sundaraj, Computer-based respi-
ratory sound analysis: a systematic review. IETE Tech. Rev. 30(3), 248–256 (2013)
134. K. Paliwal, B. Shannon, J. Lyons, K. Wójcicki, Speech-signal-based frequency warping. IEEE
Signal Process. Lett. 16(4), 319–322 (2009)
135. A. Papandreou, F. Hlawatsch, G.F. Boudreaux-Bartels, The hyperbolic class of quadratic
time-frequency representations. I. Constant-Q warping, the hyperbolic paradigm, properties,
and members. IEEE Trans. Signal Process. 41(12), 3425–3444 (1993)
136. S. Parolai, Denoising of seismograms using the S-transform. Bull. Seismol. Soc. Am. 99(1),
226–234 (2009)
137. L.I. Peng, Z. Yong, L. Hongtao, Z. Yong, D. Zhaobin, Analysis of non-stationary and nonlinear
low-frequency oscillation of a realistic bulk power system in a time-frequency perspective
(2010). http://geogin.narod.ru/hht/link01/readpdf
138. C.R. Pinnegar, Polarization analysis and polarization ﬁltering of three-component signals with
the time-frequency S-transform. Geophys. J. Int. 165(2), 596–606 (2006)
139. C.R. Pinnegar, L. Mansinha, The S-transform with windows of arbitrary and varying shape.
Geophysics 68(1), 381–385 (2003)
140. J. Placer, C.N. Slobodchikoff, J. Burns, J. Placer, R. Middleton, Using self-organizing maps
to recognize acoustic units associated with information content in animal vocalizations. J.
Acoust. Soc. Am. 119(5), 3140–3146 (2006)
141. R.Polikar,Thewavelettutorial.PartIII(2006).http://users.rowan.edu/~polikar/WAVELETS/
WTpart3.html
142. M.T.Pourazad,Z.Moussavi,G.Thomas,Heartsoundcancellationfromlungsoundrecordings
using time-frequency ﬁltering. Med. Biol. Eng. Comput. 44(3), 216–225 (2006)
143. J. Prestin, E. Quak, H. Rauhut, K. Selig, On the connection of uncertainty principles for
functions on the circle and on the real line. J. Fourier Anal. Appl. 9(4), 387–409 (2003)
144. S.Qian, IntroductiontoTime-Frequency andWavelet Transforms (Prentice Hall,UpperSaddle
River, 2002)
145. S. Qian, D. Chen, Joint time-frequency analysis. IEEE Signal Process. Mag. 16(2), 52–67
(1999)
146. A.R. Rao, E. Hsu, Hilbert-Huang Transform Analysis of Hydrological and Environmental
Time Series (Springer, New York, 2008)
147. S. Reichert, R. Gass, C. Brandt, E. Andrès, Analysis of respiratory sounds: state of the art.
Clin. Med. Circ. Respir. Pulm. Med. 2, 45–58 (2008)
148. G. Rilling, P. Flandrin, P. Goncalves, On empirical mode decomposition and its algorithms,
in Proceedings of the IEEE-EURASIP Workshop on Nonlinear Signal and Image Processing,
vol. 3 (2003), pp. 8–11
149. B. Ristic, B. Boashash, Scale domain analysis of a bat sonar signal, in Proceedings of the
IEEE International Symposium on Time-Frequency and Time-Scale (1994), pp. 373–376
150. R.P. Rodrigues, P.M. Silveira, P.F. Ribeiro, A survey of techniques applied to non-stationary
waveforms in electrical power systems, in Proceedings of the IEEE 14th International Con-
ference on Harmonics and Quality of Power (2010), pp. 1–8
151. Z.E. Ross, Y. Ben-Zion, Automatic picking of direct P, S seismic phases and fault zone head
waves. Geophys. J. Int. 199(1), 368–381 (2014)
152. V. Ryabov, Some aspects of analysis of dolphins’ acoustical signals. Open J. Acoust. 1(2),
41–54 (2011)
153. N. Saulig, V. Sucic, B. Boashash, An automatic time-frequency procedure for interference
suppression by exploiting their geometrical features, in Proceedings of the 7th International
Workshop on Systems, Signal Processing and their Applications (WOSSPA) (2011), pp. 311–
314
154. A.M. Sayeed, D.L. Jones, On the equivalence of generalized joint signal representations, in
Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Process-
ing, ICASSP-95, vol. 3 (1995), pp. 1533–1536

References
493
155. D.P. Scarpazza, A brief introduction to the Wigner distribution. Report. Dipartimento di Elet-
tronica e Informazione, Politecnico di Milano (2003). www.scarpaz.com/attic/Documents/
TheWignerDistribution.pdf
156. C. Schörkhuber, A. Klapuri, Constant-Q transform toolbox for music processing, in Pro-
ceedings of the 7th Sound and Music Computing Conference, Barcelona, Spain (2010), pp.
3–6
157. E. Sejdiæ, I. Djuroviæ, J. Jiang, Time-frequency feature representation using energy concen-
tration: an overview of recent advances. Digit. Signal Process. 19(1), 153–183 (2009)
158. E. Sejdiæ, I. Djuroviæ, L. Stankovi, Fractional Fourier transform as a signal processing tool:
an overview of recent developments. Signal Process. 91(6), 1351–1369 (2011)
159. P.D. Spanos, A. Giaralis, N.P. Politis, Time-frequency representation of earthquake accelero-
grams and inelastic structural response records using the adaptive chirplet decomposition and
empirical mode decomposition. Soil Dyn. Earthq. Eng. 27(7), 675–689 (2007)
160. H. Spontón, Pitch content visualization for musical analysis using fan chirp transform (2013).
http://dx.doi.org/10.5201/ipol
161. C. Stamoulis, B.S. Chang, Estimation of directional brain anisotropy from EEG signals using
the Mellin transform and implications for source localization, in Proceedings of the IEEE
International Conference on Digital Signal Processing (DSP) (2011), pp. 1–6
162. R.G. Stockwell, L. Mansinha, R.P. Lowe, Localization of the complex spectrum: the S-
transform. IEEE Trans. Signal Process. 44(4), 998–1001 (1996)
163. Z. Syed, D. Leeds, D. Curtis, F. Nesta, R.A. Levine, J. Guttag, A framework for the analysis
of acoustical cardiac signals. IEEE Trans. Biomed. Eng. 54(4), 651–662 (2007)
164. B. Teymur, S.P.G. Madabhushi, D.E. Newland, Analysis of earthquake motions recorded
during the Kokaeli earthquake. Report CUED/D-Soils/TR312 (2000). www-civ.eng.cam.ac.
uk/geotech_new/publications/TR/TR312.pdf
165. S. Umesh, L. Cohen, N. Marinovic, D.J. Nelson, Scale transform in speech analysis. IEEE
Trans. Speech Audio Process. 7(1), 40–45 (1999)
166. M. Van der Seijs, Improvements on time-frequency analysis using time-warping and timbre
techniques. Master’s thesis, TU Delft (2011)
167. J. Van Verth, M. Ko, Intro to frames, dictionaries and K-SVD. Conference presentation (1999).
www.essentialmath.com/GDC2014/GDC14_frames.pdf
168. J.G. Vargas-Rubio, B. Santhanam, An improved spectrogram using the multiangle centered
discrete fractional Fourier transform, in Proceedings of the IEEE International Conference
on Acoustics, Speech, and Signal Processing (ICASSP’05), vol. 4 (2005), pp. 505–508
169. S. Ventosa, C. Simon, M. Schimmel, J.J. Dañobeitia, A. Manuel, The S-transform from a
wavelet point of view. IEEE Trans. Signal Process. 56(7), 2771–2780 (2008)
170. Y. Wang, J. Orchard, On the use of the Stockwell transform for image compression Proc.
IS&T/SPIE Electron. Imaging, 724504 (2009)
171. L. Weruaga, M. Képesi, Speech analysis with the fast chirp transform, in Proceedings of the
EUSIPCO (2004), pp. 1011–1014
172. L. Weruaga, M. Képesi, The fan-chirp transform for non-stationary harmonic sounds. Signal
Process. 87, 1504–1522 (2007)
173. S.T. Wisdom, Improved statistical signal processing of nonstationary random processes using
time-warping. Master’s thesis, University of Washington (2014)
174. P. Wolfe, Quadratic Time-Frequency Representations. Lecture Presentation, Harvard Univer-
sity (2009). http://isites.harvard.edu/fs/docs/icb.topic541812.ﬁles/lec19_spr09.pdf
175. Z. Wu, N.E. Huang, Ensemble empirical mode decomposition: a noise-assisted data analysis
method. Adv. Adapt. Data Anal. 1(1), 1–41 (2009)
176. F. Xu, W. Zhou, Y. Zhen, Q. Yuan, Classiﬁcation of ECoG with modiﬁed S-transform for
brain-computer interface. J. Comput. Inf. Syst. 10(18), 8029–8041 (2014)
177. Y. Yang, Z.K. Peng, G. Meng, W.M. Zhang, Characterize highly oscillating frequency modu-
lation using generalized warblet transform. Mech. Syst. Signal Process. 26, 128–140 (2012)
178. S. Yin, B. McCowan, Barking in domestic dogs: context speciﬁcity and individual identiﬁca-
tion. Anim. Behav. 68(2), 343–355 (2004)

494
7
Time-Frequency Analysis
179. F. Zhang, G. Bi, Y. Chen, Harmonic transform. IEE Proc. Vis. Image Signal Process. 151,
257–263 (2004)
180. H. Zhang, C. Thurber, C. Rowe, Automatic P-wave arrival detection and picking with mul-
tiscale wavelet analysis for single-component recordings. Bull. Seismol. Soc. Am. 93(5),
1904–1912 (2003)
181. M. Zibulski, Y.Y. Zeevi, Frame analysis of the discrete Gabor-scheme. IEEE Trans. Signal
Process. 42(4), 942–945 (1994)
182. D. Zigone, D. Rivet, M. Radiguet, M. Campillo, C. Voisin, N. Cotte, J.S. Payero, Triggering
of tremors and slow slip event in Guerrero, Mexico, by the 2010 mw 8.8 Maule, Chile,
earthquake. J. Geophys. Res.: Solid Earth 117(B09), 1–17 (2012)
183. H. Zou, Y. Chen, L. Qiao, S. Song, X. Lu, Y. Li, Acceleration-based dopplerlet transform-part
ii: Implementations and applications to passive motion parameter estimation of moving sound
source. Signal Process. 88(4), 952–971 (2008)
184. H. Zou, S. Song, Z. Liu, Y. Chen, Y. Li, Acceleration-based dopplerlet transform-part i: theory.
Signal Process. 88(4), 934–951 (2008)
185. P. Zubrycki, A. Petrovsky, Accurate speech decomposition into periodic and aperiodic compo-
nents based on discrete harmonic transform, in Proceedings of the European Signal Processing
Conference EUSIPCO (2007), pp. 2336–2340
186. P.E. Zwicke, I. Kiss, A new implementation of the Mellin transform and its application to
radar classiﬁcation of ships. IEEE Trans. Pattern Anal. Mach. Intell. 2, 191–199 (1983)

Chapter 8
Modulation
8.1
Introduction
Signal changes are introduced by man for communication purposes. Of course there
are many ways to communicate information at distance. For instance by using
coloured ﬂags, or mirrors, etc. In this chapter we will refer to electric and to electro-
magnetic signals.
In the case of radio waves, through the space, sinusoidal signals are used, and
the information is translated to changes in these signals. It is said that a signal is
modulated, via speciﬁc intentional signal changes. In a previous chapter the generic
sinusoidal signal was considered:
y(t) = A sin(ω t + α)
(8.1)
and it was highlighted that the three parameters that one can change along time are
A, ω and α. There are three main ways of modulation: amplitude, frequency or phase
modulation.
There will be a sender station where the signal is modulated with information,
and another distant receiver station where the signal is demodulated to recover the
information.
When the communication is established with cable or ﬁbre optics, pulses can be
employed instead of sinusoidal signals. Pulses have some advantages, in terms of
easy recognition in the presence of noise and distortions. In this case, changes in the
pulses can be used for transmission of information. This is modulation of pulses.
This chapter has two main sections, devoted to modulation of sinusoidal signals
and to modulation of pulses. There are another ﬁnal sections with complementary
aspects: multiplexing, experiments, etc.
There are many books that can be used as basic bibliography, like for instance
[10, 11, 23].
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1_8
495

496
8
Modulation
8.2
Modulation and Demodulation of Sinusoidal Signals
This section studies amplitude and frequency modulation of sinusoidal signals. Radio
and TV stations use these alternatives. Also, modulation phenomena can be observed
in nature and human artefacts, like musical instruments. The main idea is to transmit
informationusingvariationsofamplitudeorfrequencyofahigh-frequencysinusoidal
signal.
The ﬁrst radio transmission of audio signals took place on December 1904 from a
radio tower at Brant Rock, Massachusetts. It was a ﬁrst example of using amplitude
modulation. Figure8.1 shows a photograph of the tower and the man who created
the transmission system, Reginald Fessenden.
See on Internet (web page cited in the Resources section at the end of this chapter)
more historical details of this pivotal event.
8.2.1
Amplitude Modulation and Demodulation
The topic of amplitude modulation will be introduced with two examples. Then some
formal aspects will be treated. Demodulation is next. Finally some extensions of the
topic will be covered.
Figure8.2 depicts a block diagram that conceptually represents the radio trans-
mission scheme. By means of modulation and demodulation the information can
be transmitted on the air using electromagnetic waves. Of course, it can be also
transmitted through ﬁbre optics, cable, ultrasound, etc.
Since products of sinusoids appear frequently in modulation, let us recall two
useful relationships:
cos ω1t cos ω2t = 1
2 cos(ω1 + ω2) t + 1
2 cos(ω1 −ω2) t
(8.2)
Fig. 8.1 The Brant Rock radio tower, and Mr. R. Fessenden

8.2 Modulation and Demodulation of Sinusoidal Signals
497
Fig. 8.2 Radio transmission can be done using modulation and demodulation
cos2 ω t = 1 + cos 2ω t
2
(8.3)
8.2.1.1
Double Sideband Modulation
Consider a signal −1 < a(t) < 1 to be transmitted. There is also another signal
c(t) = cos(ωct) that will be used as carrier. For instance, c(t) may be translated
to a radio-frequency electromagnetic signal, able to travel long distances. Let us
modulate the amplitude of c(t), obtaining another signal y(t) that inherits the long
distance capabilities of c(t):
y(t) = (1 + μ a(t)) c(t) = (1 + μ a(t)) cos(ωct)
(8.4)
where μ can take values from 0 to 1; this parameter sets the modulation depth.
Figure8.3 shows the signals involved in the AM modulation.
Suppose that a(t) = cos(wat), with wa noticeably less than wc. Figure8.4 shows
an example of y(t). What can be observed is a sinusoidal signal with frequency wc
having amplitude variations. These amplitude variations, the envelope, correspond
to a(t). In this way we printed the information we want to transmit, a(t), on the
carrier c(t). Now the signal y(t) is ready for transmission.
Fig. 8.3 AM modulation
diagram

498
8
Modulation
Fig. 8.4 Amplitude
modulation of sine signal
0
0.005
0.01
0.015
0.02
0.025
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
For the case that a(t) = cos(wat), the mathematical expression of y(t) is:
y(t) = cos(ωct) + 1
2 μ cos(ωa + ωc) t + 1
2 μ cos(ωa −ωc) t
(8.5)
The Fig.8.4 has been obtained with the Program 8.1. There is a parameter MD in
the program that can be modiﬁed to produce more or less modulation depth on the
signal y(t). The reader is invited to change this parameter and see the results. MD
can have values between 0 and 1.
Notice in the Fig.8.4 that the same information appears duplicated on the mod-
ulated signal. In fact it is easy to observe a(t) on top, and −a(t) at the bottom, as
envelopes.
Program 8.1 Amplitude modulation of sine signal
%
Amplitude
modulation
of
sine
signal
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
MD =0 .4;
% modulation
depth
A =1+( MD*sin (wa*t));
% amplitude
y=A.* sin(wc*t);
% modulated
signal
data
set
plot (t,y,'k');
% plots
modulated
signal
axis ([0
0.03
-1.5
1.5 ]);
xlabel ('seconds ');
title ('amplitude
modulation
of
sine
signal ');

8.2 Modulation and Demodulation of Sinusoidal Signals
499
Fig. 8.5 Amplitude
modulation of audio sine
signal
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
By means of the Program 8.2 an audio version of the amplitude modulation is
provided. Perhaps the pitch is too strident, take care. The reader is invited to change
the signal frequency fa in the program, and hear the results. Figure8.5 shows a
view (quite dense) of the modulated signal In musical terms, the effect we hear is
reverberation.
Program 8.2 Amplitude modulation of audio sine signal
%
Amplitude
modulation
of
audio
sine
signal
fa =5;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t=0: tiv :(0.5 -tiv );
% time
intervals
set
(0 .5
seconds )
MD =0 .5;
% modulation
depth
A =1+( MD*sin (wa*t));
y=A.* sin(wc*t);
% modulated
signal
data
set
plot (t,y,'g');
% plots
modulated
signal
axis ([0
0.5
-1.5
1.5 ]);
xlabel ('seconds ');
title ('amplitude
modulation
of
sine
signal ');
t=0: tiv :(3 - tiv );
% time
intervals
set
(3
seconds )
A =1+( MD*sin (wa*t));
y=A.* sin(wc*t);
% modulated
signal
data
set
sound (y,fs)

500
8
Modulation
8.2.1.2
Modulation and Signal Spectrum
Let Y(ω) be the spectral density function of y(t). One of the properties of the Fourier
transform is the following:
F(y(t) e jωct) = Y (ω −ωc)
(8.6)
where F denotes Fourier transform.
Therefore multiplying y(t) by exp( jωct) causes the spectral density of y(t) to be
translated in frequency by wc rad/s.
In particular, in the case of:
y(t) = b(t) cos (ωc t)
(8.7)
The spectral density of y(t) is:
Y(ω) = 1
2 B(ω + ωc) + 1
2 B(ω −ωc)
(8.8)
where B(ω) is the spectral density of b(t). The curve B(ω) versus ω has mirror
symmetry about the vertical axis.
Consequently, in this case, if we depict Y(ω) versus ω, we obtain two symmetrical
curves, one is B(ω + ωc) and the other B(ω −ωc).
Notice that b(t) could be (1+μa(t)). This is interesting because of the amplitude
modulation Eq.(8.4).
Figure8.6 shows on top the spectral density function of a square wave (this is
A(ω)). In the middle this square signal is used to modulate in amplitude a sine signal
(the modulated signal is y(t)). At the bottom the spectral density function of the
modulated signal (Y(ω)) is shown. This spectral density function consists in two
symmetrical frequency shifted versions of A(ω). For this reason, we speak of double
sideband (DSB) modulation. The ﬁgure has been generated with the Program 8.3.
The reader is invited to modify MD and see the results.
Program 8.3 Amplitude modulation of sine signal and spectra
%
Amplitude
modulation
of
sine
signal
and
spectra
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =16*1024;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
a= square (wa*t);
% modulating
signal
a(t)
( square
wave )
MD =0 .4;
% modulation
depth
A =1+( MD*a);
% amplitude
y=A.* sin(wc*t);
% modulated
signal
data
set
subplot (3 ,1 ,1)

8.2 Modulation and Demodulation of Sinusoidal Signals
501
ffa=fft(a,fs );
% Fourier
transform
of
a(t)
sa= fftshift ( real (ffa )); sa=sa/max (sa );
w1=-fs /2: -1;
w2 =1: fs /2;
w=[ w1
w2 ];
%w =( -63*80):80:(64*80);
plot (w,sa );
% plots
spectral
density
of
a(t)
axis ([ -1500
1500
-1
1]);
xlabel ('Hz ');
title
('A(w)');
subplot (3 ,1 ,2);
plot (t,y,'k');
% plots
modulated
signal
axis ([0
0.03
-1.5
1.5 ]);
xlabel ('seconds ');
title ('y(t)');
subplot (3 ,1 ,3)
ffy=fft(y,fs );
% Fourier
transform
of
y(t)
sy= fftshift ( real (ffy )); sy=sy/max (sy );
plot (w,sy );
% plots
spectral
density
of
y(t)
axis ([ -3000
3000
-1
1]);
xlabel ('Hz ');
title ('Y(w)');
It is clear that having a double sideband modulation is a waste of energy for
communication. As it shall be treated later on, it is possible to generate a single
sideband modulation (SSB).
Fig. 8.6 Spectral density
and amplitude modulation
-1500
-1000
-500
0
500
1000
1500
-1
-0.5
0
0.5
1
Hz
A(w)
0
0.005
0.01
0.015
0.02
0.025
0.03
-1
0
1
seconds
y(t)
-3000
-2000
-1000
0
1000
2000
3000
-1
-0.5
0
0.5
1
Hz
Y(w)

502
8
Modulation
8.2.1.3
Demodulation of Double Sideband Modulated Signals
Let us take the case of a radio signal y(t) captured with an antenna and a
pass-band ﬁlter. Each radio station uses a carrier with a speciﬁed frequency. Two
radio stations in the same territory use different carrier frequencies. The pass-band
ﬁlter is used in the receiver to isolate the carrier of interest, to hear one preferred
radio station. Suppose that y(t) is a DSB amplitude modulated signal. We want to
recover the modulating signal a(t) that comes as envelopes of y(t). This process is
called demodulation.
There are several alternative methods for demodulation. For instance, the Hilbert
transform can be used to get the envelope a(t). This can be done with digital process-
ing. Another alternative is to multiply the incoming signal by
cosωct, to obtain the following:
d(t) = (1 + μ a(t)) cos(ωct) cos(ωct) = (1 + μ a(t)) cos2(ωct) =
= (1 + μ a(t)) ∗1 + cos 2ωc t
2
= 0.5 + 0.5 μ a(t) + 1
2 (1 + μ a(t)) cos 2ωc t
(8.9)
Now, a(t) can be extracted from d(t) ﬁltering out the term with cos2ωct; this can be
done with a low-pass ﬁlter (the DC term, 0.5, can also be easily eliminated).
In electronics practice, it is usual to demodulate the signal by using a diode
followed by a simple low-pass ﬁlter with one resistance and one capacitor in parallel.
Program 8.4 tries a simulation of the diode + RC circuit, to obtain how the output
looks like. Figure8.7 shows the result. The diode rectiﬁes the modulated signal
y(t) obtaining the top positive half of the signal d1(t). The rectiﬁed signal d1(t) is
depicted on top of the ﬁgure. The effect of the RC ﬁlter is depicted at the bottom of
the ﬁgure. This is the demodulated signal. It is an approximation of a(t) having some
ripple. In reality the ripple is quite small, because the carrier frequency is usually
much higher than in our simulation.
Program 8.4 Demodulation of a DSB signal
%
Demodulation
of
a
DSB
signal
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =2000;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
%the
DSB
signal
MD =0 .4;
% modulation
depth
A =1+( MD*sin (wa*t));
% amplitude
y=A.* sin(wc*t);
% modulated
signal
data
set
% demodulation
N= length (t);
d1= zeros (1,N);
% diode
simulation

8.2 Modulation and Demodulation of Sinusoidal Signals
503
for
tt =1:N,
if
y(tt )<0
d1(tt )=0;
else
d1(tt )=y(tt );
end;
end;
%the
R-C
filter
R =1000;
C=0 .000003 ;
fil=tf ([R],[R*C
1]);
% transfer
function
d2= lsim (fil ,d1 ,t);
% response
of
the
filter
subplot (2 ,1 ,1)
plot (t,d1 ,'k');
% plots
diode
output
xlabel ('seconds ');
title ('rectified
modulated
signal ');
subplot (2 ,1 ,2)
plot (t,d2 ,'k');
% plots
filter
output
xlabel ('seconds ');
title ('demodulated
signal ')
The action of the diode is clearly nonlinear. The same can be said about
multiplication. In linear systems, output frequencies are the same as input frequen-
cies. In nonlinear system this is different; there are differences between input and
output frequency contents. Modulation and demodulation are nonlinear processes.
The method of multiplication (Eq.(8.9)) requires synchronization of frequencies:
it is needed to determine the frequency of y(t) to generate cosωct. In practice this
is not trivial. One of the electronics methods in radio receivers is based on the use
of phase-locked loops (PLL), which include a voltage controlled oscillator (VCO)
that tries to generate a sine signal in synchrony with the input signal; differences in
phase are compensated in real time with a feedback loop acting on the VCO.
Fig. 8.7 Diode
demodulation of DSB
amplitude modulated signal
0
0.005
0.01
0.015
0.02
0.025
0.03
0
0.5
1
1.5
seconds
rectified modulated
 signal
0
0.005
0.01
0.015
0.02
0.025
0.03
0
100
200
300
400
500
seconds
demodulated signal

504
8
Modulation
Fig. 8.8 Suppressed carrier
amplitude modulation of sine
signal
0
0.005
0.01
0.015
0.02
0.025
0.03
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
8.2.1.4
Variants of Amplitude Modulation
Consider a simple amplitude modulation scheme, by just multiplication:
y(t) = a(t) cos (ωc t)
(8.10)
where a(t), with values between −1 and 1, is the modulating signal and coswc t
is the carrier. Figure8.8, made with Program 8.5, shows an example of this kind of
modulation. Both a(t) and y(t) were superposed. It can be seen that the modulating
signal comes from one to the other signal side again and again. If the diode based
demodulation was applied, the results obtained would be wrong.
Program 8.5 Suppressed carrier amplitude modulation of sine signal
%
Suppressed
carrier
amplitude
modulation
%
of
sine
signal
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
KD =50;
% decay
constant
fc =1500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% modulating
signal
( damped
oscillation ):
A=exp(-KD*t).*sin(wa*t);
y=A.* sin(wc*t);
% modulated
signal
data
set
plot (t,y,'b');
hold
on;
% plots
modulated
signal
plot (t,A,'k');
% plots
modulating
signal
axis ([0
0.03
-1.5
1.5 ]);

8.2 Modulation and Demodulation of Sinusoidal Signals
505
The modulation considered in Eq.(8.10) is called suppressed carrier DSB
amplitude modulation. To demodulate a signal with such modulation, a multipli-
cation scheme can be applied, like in Eq.(8.9). In this case, we obtain:
d(t) = a(t) cos(ωct) cos(ωct) = a(t) cos2(ωct) =
= a(t) 1 + cos 2ωc t
2
= 0.5 a(t) + 1
2 a(t) cos 2ωc t
(8.11)
and a(t) can be recovered by ﬁltering out the term with cos 2ωct.
Using the orthogonality of sines and cosines makes it possible to transmit two
different signals a1(t) and a2(t) on the same carrier. This is called quadrature mul-
tiplexing. The modulated signal can be:
y(t) = a1(t) cos ωct + a2(t) sin ωct
(8.12)
The two signals can be recovered by multiplication based demodulation:
y(t) cos ωct = 0.5 a1(t) + 1
2 a1(t) cos 2ωct + 1
2 a2(t) sin 2ωct
(8.13)
y(t) sin ωct = 0.5 a2(t) −1
2 a2(t) cos 2ωct + 1
2 a1(t) sin 2ωct
(8.14)
Let us focus on single sideband (SSB) amplitude modulation. Consider again the
expression (Eq.(8.4)), of amplitude modulation with a(t) = cos(ωat):
y(t) = cos(ωct) + 1
2 μ cos(ωa + ωc) t + 1
2 μ cos(ωa −ωc) t
(8.15)
We want to transmit just the term with cos(ωa + ωc)t, which is one of the bands.
To obtain this term we can use the Hilbert transform, to get two complex signals:
exp( jωat) and exp( jωct). If we multiply both signals:
e jωat e jωct = e j(ωa+ωc)t
(8.16)
Now, if we take the real part, we obtain the desired term:
Re (e j(ωa+ωc)t) = cos (ωa + ωc) t
(8.17)
The modulation procedure can be generalized to any a(t): multiply the Hilbert trans-
form of a(t) by the carrier, and take the real part.
The SSB amplitude modulation signal can be demodulated multiplying it by
cosωct.
Figure8.9, generated by the Program 8.6, shows on top a sawtooth modulating
signal, and below the corresponding SSB amplitude modulated signal. Since the

506
8
Modulation
Fig. 8.9 SSB amplitude
modulation with a sawtooth
signal
0.01
0.015
0.02
0.025
0.03
0.035
0.04
-1
-0.5
0
0.5
1
a(t)
0.01
0.015
0.02
0.025
0.03
0.035
0.04
-3
-2
-1
0
1
2
3
seconds
y(t)
Hilbert transform take inﬁnite values in the discontinuities of the signal, there are
large values of the modulated signals near the corners of the sawtooth. In general
it is convenient to smooth the modulating signal in case of using SSB amplitude
modulation [2].
Program 8.6 SSB amplitude modulation of sine signal
%
SSB
amplitude
modulation
of
sine
signal
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .05
seconds ):
t=0: tiv :(0 .05 -tiv );
A= sawtooth (wa*t);
% modulating
signal
% Hilbert
transform
of
modulating
signal :
gA= hilbert (A);
C=cos(wc*t);
% carrier
signal
gC= hilbert (C);
% Hilbert
transform
of
carrier
gSSB = gA.*gC;
% multiplication
y= real ( gSSB );
% real
part
subplot (2 ,1 ,1)
plot (t,A,'k');
% plots
modulating
signal
axis ([0 .01
0.04
-1.2
1.2 ]);
ylabel ('a(t)');
title ('SSB
amplitude
modulation
with
sawtooth
signal ')
subplot (2 ,1 ,2)
plot (t,y,'k');
% plots
modulated
signal
axis ([0 .01
0.04
-3
3]);
xlabel ('seconds ');
ylabel ('y(t)');

8.2 Modulation and Demodulation of Sinusoidal Signals
507
8.2.2
Frequency Modulation and Demodulation
Following the same structure of the previous part, frequency modulation will be
introduced with two examples. Then some formal aspects, demodulation, and some
extensions of the topic will be treated.
Frequency modulation is a particular case of angle modulation:
y(t) = A cos(ωc t + ϕ(t))
(8.18)
The angle modulation is introduced through variations of ϕ(t).The particular relation
of ϕ(t) with the modulating signal a(t) can take several forms. For instance:
ϕ(t) = K a(t), phase modulation
ϕ(t) = K
t
−∞
a(τ) dτ, frequency modulation
The second expression takes into account that the instantaneous frequency devi-
ation is given by dϕ/dt. Frequency modulation has better properties than phase
modulation. Therefore let us focus on frequency modulation.
8.2.2.1
Frequency Modulation
Like in amplitude modulation, consider a signal −1 < a(t) < 1 to be transmitted.
There is also another signal c(t) = cos(ωct) that will be used as carrier. Let us
modulate the frequency of c(t), obtaining another, modulated, signal y(t).
y(t) = A cos
⎛
⎝ωc t + K
t
−∞
a(τ) dτ
⎞
⎠
(8.19)
In the particular case that a(t) = cos(ωat), the modulated signal can be expressed
as follows:
y(t) = A cos (ωct + β sin ωat)
(8.20)
The FM radio transmission was invented by Mr. E.H. Armstrong. In 1933 he had a
working prototype. Before that, he also contributed with other inventions, like the
application of positive feedback for regeneration, and the superheterodyne receiver.
Figure8.10 shows a photograph of Armstrong, and another photograph with his wife
(the “portable” superheterodyne radio was built by Armstrong as a present for her).
Like before, we cite in the Resources section a web page with more historical
details.
Figure8.11 shows an example of frequency modulated signal, with a(t) =
cos(ωat). The ﬁgure has been generated by the Program 8.7.

508
8
Modulation
Fig. 8.10 Mr. Armstrong, his wife and a superheterodyne radio
0.05
0.055
0.06
0.065
0.07
0.075
0.08
0.085
0.09
0.095
0.1
-1.5
-1
-0.5
0
0.5
1
1.5
seconds
Fig. 8.11 Frequency modulation of sine signal

8.2 Modulation and Demodulation of Sinusoidal Signals
509
Fig. 8.12 Frequency of FM
modulated signal in audio
example
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
400
450
500
550
seconds
Program 8.7 Frequency modulation of sine signal
%
Frequency
modulation
of
sine
signal
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1000;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t=0: tiv :(0.1 -tiv );
% time
intervals
set
(0 .1
seconds )
beta =20;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));
plot (t,y,'k');
% plots
modulated
signal
axis ([0 .05
0.1
-1.5
1.5 ]);
xlabel ('seconds ');
title ('frequency
modulation
of
sine
signal ');
Another frequency modulation example is offered by the Program 8.8. Frequen-
cies have been speciﬁed in order to make audible results. The effect is like a traditional
siren alarm sound. Figure8.12 has been also generated with the Program 8.8. Using
the derivative of the Hilbert transform, the program obtains the instantaneous fre-
quency of the modulated signal, and depicts it in the ﬁgure. The reader is invited to
use also the spectrogram.

510
8
Modulation
Program 8.8 Frequency modulation of audio sine signal
%
Frequency
modulation
of
audio
sine
signal
fa =2;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =500;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =3000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t=0: tiv :(5 - tiv );
% time
intervals
set
(5
seconds )
beta =20;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));
% instantaneous
frequency
estimation
gy= hilbert (y);
dg= diff (gy )/ tiv;
w=abs(dg );
v=w /(2* pi );
Nv= length (v);
% plot
frequency
of
modulated
signal :
plot (t(2: Nv +1) ,v,'k');
axis ([0 .05
5
400
550]);
xlabel ('seconds ');
title ('frequency
of
modulated
signal ');
%5
seconds
of
sound
sound (y,fs );
8.2.2.2
Frequency Modulation and Signal Spectrum
The spectral density of frequency modulated signals is usually complicated. To give
an idea it is better to consider the simplest case: the modulating signal is a sinusoidal
signal, so we can use Eq.(8.22). Let us expand this equation:
y(t) = A cos (ωct + β sin ωat) =
= A [cos ωct cos (β sin ωat) −sin ωct sin (β sin ωat)]
(8.21)
It is well known that
cos (β sin ωat) = Jo(β) +
∞

ne even
2 Jn(β) cos n ωa t
(8.22)
sin (β sin ωat) =
∞

ne odd
2 Jn(β) sin n ωa t
(8.23)
where Jn (β) are Bessel functions. Using the bessel() MATLAB function one can
see the expression of any of these Jn(β).
If we substitute in Eq.(8.21), according with Eqs.(8.22) and (8.23), and with some
development:

8.2 Modulation and Demodulation of Sinusoidal Signals
511
Fig. 8.13 Spectral density of
frequency modulated signal
-3000
-2000
-1000
0
1000
2000
3000
-1
0
1
beta=1
-3000
-2000
-1000
0
1000
2000
3000
-1
0
1
beta=5
-3000
-2000
-1000
0
1000
2000
3000
-1
0
1
Hz
beta=10
y(t) = A Jo(β) cos ωct+
+
∞
	
ne even
AJn(β) [ cos (ωc + n ωa) t + cos (ωc −n ωa) t ] +
+
∞
	
ne odd
AJn(β) [ cos (ωc + n ωa) t −cos (ωc −n ωa) t ]
(8.24)
Notice that y(t) contains an inﬁnite number of harmonics. Figure8.13, generated
with the Program 8.9, shows three examples of spectral densities of the frequency
modulated signal, corresponding to three example values of β. It is clear that the
bandwidth occupied by the frequency modulated signal increases as β increases.
The following equation gives an approximation of the modulated signal band-
width:
Δωy = 2 (β + 2) Δωa
(8.25)
where Δωa is the modulating signal bandwidth.
Program 8.9 Spectra of frequency modulated signals
%
Spectra
of
frequency
modulated
signals
%
frequency
modulation
of
sine
signal
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1000;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t=0: tiv :(0.1 -tiv );
% time
intervals
set
(0 .1
seconds )
% vector
of
frequencies
for
spectra :
w1=-fs /2: -1;
w2 =1: fs /2;
w=[ w1
w2 ];
beta =20;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));

512
8
Modulation
subplot (3 ,1 ,1)
beta =1;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));
ffy=fft(y,fs );
% Fourier
transform
of
y(t)
sy= fftshift ( real (ffy )); sy=sy/max (sy );
plot (w,sy );
% plots
spectral
density
of
y(t)
axis ([ -3000
3000
-1
1]);
ylabel ('beta =1 ');
title ('Y(w)');
subplot (3 ,1 ,2)
beta =5;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));
ffy=fft(y,fs );
% Fourier
transform
of
y(t)
sy= fftshift ( real (ffy )); sy=sy/max (sy );
plot (w,sy );
% plots
spectral
density
of
y(t)
axis ([ -3000
3000
-1
1]);
ylabel ('beta =5 ');
subplot (3 ,1 ,3)
beta =10;
% modulation
depth
% modulated
signal
data
set:
y=cos (( wc*t)+( beta *sin(wa*t )));
ffy=fft(y,fs );
% Fourier
transform
of
y(t)
sy= fftshift ( real (ffy )); sy=sy/max (sy );
plot (w,sy );
% plots
spectral
density
of
y(t)
axis ([ -3000
3000
-1
1]);
xlabel ('Hz ');
ylabel ('beta =10 ');
8.2.2.3
Demodulation of Frequency Modulated Signal
Frequency modulated signals can be demodulated via the Hilbert transform, as it has
been done by the Program 8.8 with the audio example. An alternative in terms of
electronic circuits is given by phase-locked-loop schemes.
8.2.3
Digital Modulation of Sine Signals
The same modulation methods already described can be applied for the transmission
of digital information. We want to transmit a series a(n) of bits.
Figure8.14 depicts the approach. The carrier is a sine signal with suitable fre-
quency.
Figure8.15 shows examples of the three basic modulation methods being used
for the transmission of series of bits. There is a speciﬁc terminology for these mod-
ulations, Amplitude Shift Keying (ASK), Frequency Shift Keying (FSK) and Phase
Shift Keying (PSK).
The Fig.8.15 has been generated with the Program B.30, which has been included
in Appendix B. Other ways of generating this ﬁgure are possible, but it was preferred

8.2 Modulation and Demodulation of Sinusoidal Signals
513
Fig. 8.14 Pulse modulation
of sine signal
Fig. 8.15 ASK, FSK and
PSK modulation of sine
signal
0
0.05
0.1
0.15
0.2
0.25
0
0.5
1
bits message
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
ASK
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
FSK
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
PSK
seconds
for tutorial purposes to build the signals by pieces. The cat() function was used for
concatenation of pieces. Notice the following details:
• The amplitude modulation ASK was done by switching between the full carrier
(bit = 1) or just zero signal (bit = 0).
• The frequency modulation FSK was done by switching between two carriers with
different frequencies.
• The phase modulation PSK was done by switching between 0◦carrier phase or
180◦carrier phase.

514
8
Modulation
0
0.5
1
x 10-3
-1
-0.5
0
0.5
1
0 phase
seconds
0
0.5
1
x 10-3
-1
-0.5
0
0.5
1
1 phase
seconds
0
0.5
1
x 10-3
-1
-0.5
0
0.5
1
2 phase
seconds
0
0.5
1
x 10-3
-1
-0.5
0
0.5
1
3 phase
seconds
Fig. 8.16 The four basic pieces of 4_PSK modulation
Comparing the three modulation methods, it is PSK the method that is more
energy-constant along time (supposing the more frequency the more energy in the
signal).
In the 1970s, an audio FSK standard, called the Kansas City standard, was used to
store data on audio cassettes. The Bell 202 modem uses a 1200 Hz. tone for mark (1)
and 2200 Hz. for space (0). The Bell 202 standard is used for caller ID in a number
of systems.
The phase modulation, PSK, admits several versions as the 360◦of phase can be
divided into N equal parts. For instance, let us take N = 4 and suppose that we divide
bit messages into groups of two bits. Also suppose that we employ just one sine cycle
to transmit one pair of bits. Then we could transmit 00 with one 0◦phase cycle, 01
with one 90◦phase cycle, 10 with one 180◦phase cycle, and 11 with 270◦phase
cycle. Figure8.16, generated with the Program B.20, shows the four sine cycles that
we can use.
Program 8.10 Pieces of 4-PSK
%
Pieces
of
4- PSK
fc =1000;
% carrier
frequency
in
Hz
ns =20;
% number
of
samples
per
carrier
cycle
fs=ns*fc;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set ,
1
sine
period :
t=0: tiv :((1/ fc)- tiv );
% carrier
signals
for
1
bit
time
c1=sin (2* pi*fc*t);

8.2 Modulation and Demodulation of Sinusoidal Signals
515
c2=sin ((2* pi*fc*t)+( pi /2));
c3=sin ((2* pi*fc*t)+( pi ));
c4=sin ((2* pi*fc*t )+(3* pi /2));
% plots
of
the
four
modulated
PSK
pieces
subplot (2 ,2 ,1)
plot (t,c1 ,'k');
% plots
0
phase
modulated
signal
ylabel ('0
phase ');
xlabel ('seconds ');
title ('pieces
of
4- PSK
modulation
of
sine
signal ');
axis ([0
1/ fc
-1.2
1.2 ]);
subplot (2 ,2 ,2)
plot (t,c2 ,'k');
% plots
1
phase
modulated
signal
ylabel ('1
phase ');
xlabel ('seconds ');
axis ([0
1/ fc
-1.2
1.2 ]);
subplot (2 ,2 ,3)
plot (t,c3 ,'k');
% plots
2
phase
modulated
signal
ylabel ('2
phase ');
xlabel ('seconds ');
axis ([0
1/ fc
-1.2
1.2 ]);
subplot (2 ,2 ,4)
plot (t,c4 ,'k');
% plots
3
phase
modulated
signal
ylabel ('3
phase '); xlabel ('seconds ');
axis ([0
1/ fc
-1.2
1.2 ]);
Using the four modulated cycles, messages can be decomposed into bit pairs and
then be transmitted. Figure8.17 shows an example of message and the corresponding
4-PSK modulated signal. This ﬁgure has been obtained with the Program 8.11.
Fig. 8.17 Example of
4-PSK modulation
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
x 10-3
0
0.5
1
message
seconds
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
x 10-3
-1
-0.5
0
0.5
1
4-PSK signal
seconds

516
8
Modulation
Program 8.11 A message via 4-PSK
%
A
message
via
4- PSK
%
the
pulses
( bits ):
%the
modulating
signal
( bits ):
a=[0
1
0
0
1
0
1
1
0
1];
Nb= length (a);
% number
of
message
bits
fc =1000;
% carrier
frequency
in
Hz
% number
of
samples
per
carrier
cycle
%
( also
per
bit - pair ):
ns =20;
fs=ns*fc;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set ,
1
sine
period
(1
bit - pair ):
t=0: tiv :((1/ fc)- tiv );
% time
intervals
set
for
the
complete
message :
tmsg =0: tiv :(( Nb /(2* fc ))- tiv );
% carrier
signals
for
bit - pair
time
c1=sin (2* pi*fc*t);
c2=sin ((2* pi*fc*t)+( pi /2));
c3=sin ((2* pi*fc*t)+( pi ));
c4=sin ((2* pi*fc*t )+(3* pi /2));
subplot (2 ,1 ,1)
xx=Nb /(10* fc );
% for
vertical
lines
us1= ones (1, ns /2);
% vector
of
ns /2
ones
as=a (1)* us1 ;
for
nn =2:Nb ,
as= cat (2,as ,a(nn )* us1 );
end
% plot
modulating
signal :
plot (tmsg ,as ,'k');
hold
on;
for
nn =1:4 ,
plot ([ nn*xx
nn*xx ],[-0 .2
1.2],':b');
end
axis ([0
Nb /(2* fc)
-0.2
1.2 ]);
ylabel ('message ');
xlabel ('seconds ');
title ('4- PSK
modulation ');
subplot (2 ,1 ,2)
cs=c2;
% first
two
bits
of
a:
(0
1)
cs=cat (2,cs ,c1 );
% append
2nd
two
bits
of
a:
(0
0)
cs=cat (2,cs ,c3 );
% append
3rd
two
bits
of
a:
(1
0)
cs=cat (2,cs ,c4 );
% append
4th
two
bits
of
a:
(1
1)
cs=cat (2,cs ,c2 );
% append
5th
two
bits
of
a:
(0
1)
plot (tmsg ,cs ,'k');
hold
on;
% plots
modulated
signal
for
nn =1:4 ,
plot ([ nn*xx
nn*xx ],[-1 .2
1.2],':b');
end
axis ([0
Nb /(2* fc)
-1.2
1.2 ]);
ylabel ('4- PSK
signal ');
xlabel ('seconds ');
Indeed N can take any desired value, giving rise to N-PSK modulation schemes.
Care is needed for large values of N, since the recognition of different phases can
become difﬁcult due to noise and distortion.
There are many digital communication schemes using the basic modulation meth-
ods ASK, FSK or PSK, as the basis for multiplexing techniques. The topic of mul-
tiplexing will be treated in a later section of this chapter.

8.2 Modulation and Demodulation of Sinusoidal Signals
517
There are many books and other publications on digital communication systems,
like for instance [16]. The web page of Mathworks on digital communication books
contains a list of those that include MATLAB programs; in particular, let us mention
[15, 20, 21]. Other papers of interest are [3, 8].
8.2.4
Details of the MATLAB Signal Processing Toolbox
The MATLAB Signal Processing Toolbox provides two main functions to deal with
modulation issues: modulate() and demod(). These functions offer options for several
modulation methods:
• Modulation of sinusoid signal: amplitude (SSB, DSB, and quadrature), frequency,
phase
• Modulation of pulses: width, time.
Until now, the functions modulate() and demod() have not been used in the exam-
ples. The reader is invited to use them to check the results already presented. For
example, the Program 8.12 uses modulate() to generate the same results of Fig.8.4,
as it can be seen in Fig.8.18.
Fig. 8.18 Amplitude
modulation of sine signal
0
0.005
0.01
0.015
0.02
0.025
0.03
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
seconds

518
8
Modulation
Program 8.12 Amplitude modulation of sine signal, using modulate()
%
Amplitude
modulation
of
sine
signal ,
%
using
modulate ()
fa =80;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
a=sin(wa*t);
y= modulate (a,fc ,fs ,'amdsb -tc ');
plot (t,y,'k');
% plots
modulated
signal
axis ([0
0.03
-2.2
2.2 ]);
xlabel ('seconds ');
title ('amplitude
modulation
of
sine
signal ');
8.3
Modulation and Demodulation of Pulses
Pulses have the advantage of ﬁtting well with digital electronics. So they offer a
convenient way for cable or ﬁbre optic transmission.
Pulse trains can be used as carriers for analog signals, via modulation of the pulses.
Likewise digital information can be translated to pulse codiﬁcation for serial
transmission.
8.3.1
Sampling. Demodulation of modulated pulses
Analogsignals canbeproperlysampledtoobtainaseries of numbers—thesamples—
from which the original analog signals can be recovered. These numbers can be used
to modulate pulses, one pulse per sample.
The demodulation process is a reconstruction process, obtaining the series of
numbers from the modulated pulses, which is a matter of measuring, and then ﬁltering
to get the recovered continuous time analog signal.
When speaking of sampling, the evident reference is the sampling theorem
enounced by Mr. C.E. Shannon in 1949 [13], which is a milestone of historical rele-
vance. It was preceded by a ﬁrst statement from H. Nyquist in 1928. A photograph
of Shannon is reproduced in Fig.8.19.
For the interested reader, a short history of the origins of the sampling theorem
is offered by [9], and a good introductory book on Information Theory is [5]. It is
also quite interesting the article [18] titled “Sampling—50 years after Shannon”. It
would also be recommended to read new views of sampling in [1, 19].
With respect to Information Theory, another very important reference is again due
to Shannon, in a very readable article published in 1948 [12] and which has been

8.3 Modulation and Demodulation of Pulses
519
Fig. 8.19 C.E. Shannon
reprinted with corrections in 2001 [14]. This article has been cited more than 72,000
times.
8.3.2
Modulation of Pulses
Pulses can be modulated in amplitude (PAM), in width (PWM), and in phase (PPM)
(this is also called pulse time modulation, PTM).
Figure8.20 depicts a diagram of the modulation of pulses for transmission pur-
poses.
Figure8.21, shows from top to bottom, a sinusoid modulating signal, a PWM
modulated signal, and a PTM modulated signal. The ﬁgure has been generated with
the Program 8.13, which uses modulate(). The plot of PTM modulated signal includes
a reference non-modulated signal to highlight the PTM phase shifts.
Fig. 8.20 Modulation of
pulses

520
8
Modulation
Fig. 8.21 Analog
modulation of pulses
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
modulating
 sine
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
PWM
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
PTM
seconds
Program 8.13 Modulation of pulses
%
Modulation
of
pulses
%
the
modulating
signal
is
a
sine
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
nsc =8;
% number
of
samples
per
signal
period
fs=nsc *fa;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% sampling
the
modulating
signal
a=0 .5 +0 .3* sin(wa*t);
subplot (3 ,1 ,1);
plot (t,a,'k');
ylabel ('modulating
sine ');
title ('analog
modulation
of
pulses ');
axis ([0
0 .025
-0.2
1.2 ]);
%PWM
modulation
[PWMy ,ty ]= modulate (a,fa ,fs ,'pwm ','centered ');
subplot (3 ,1 ,2)
plot (ty/nsc ,PWMy ,'k');
ylabel ('PWM ');
axis ([0
0 .025
-0.2
1.2 ]);
%PTM
modulation
[PTMy ,ty ]= modulate (a,fa ,fs ,'ptm ' ,0.3 );
subplot (3 ,1 ,3)
b=0 .5* ones (1, length (a));
[PTMb ,tb ]= modulate (b,fa ,fs ,'ptm ' ,0.3 );
plot (tb/nsc ,PTMb ,'-.r ');
hold
on;
% reference
pulses
plot (ty/nsc ,PTMy ,'k');
% modulated
pulses
ylabel ('PTM ');
axis ([0
0 .025
-0.2
1.2 ]);
xlabel ('seconds ');

8.3 Modulation and Demodulation of Pulses
521
Fig. 8.22 Demodulation of
pulses
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
modulating
 sine
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
PWM
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
PTM
seconds
In order to check the function demod(), Program 8.14 repeats the modulation alter-
natives previously seen, and then demodulates the results of each modulation method
(PWM and PTM). Figure8.22 shows the demodulation results, to be compared with
the original modulating signal on top of the ﬁgure. Notice that the sampling rate
(nsc in the program) has been increased, and that the value of the default PTM pulse
width has been changed from 0.3 to 0.1.
See [17] for an extensive treatment of PWM techniques.
The Program 8.14 is useful for testing the effects of sampling rate on the signal
recovery quality.
Program 8.14 Demodulation of pulses
%
Demodulation
of
pulses
%
the
modulating
signal
is
a
sine
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
nsc =20;
% number
of
samples
per
signal
period
fs=nsc *fa;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% sampling
the
modulating
signal
a=0 .5 +0 .3* sin(wa*t);
subplot (3 ,1 ,1);
plot (t,a,'k');
ylabel ('modulating
sine ');
title (' demodulation
of
pulses ');
axis ([0
0 .025
-0.2
1.2 ]);
%PWM
modulation
[PWMy ,ty ]= modulate (a,fa ,fs ,'pwm ','centered ');
%PWM
demodulation
PWMa = demod (PWMy ,fa ,fs ,'pwm ','centered ');
subplot (3 ,1 ,2)
plot (t,PWMa ,'k');
ylabel ('PWM ');

522
8
Modulation
axis ([0
0 .025
-0.2
1.2 ]);
%PTM
modulation
[PTMy ,ty ]= modulate (a,fa ,fs ,'ptm ' ,0.1 );
%PTM
demodulation
PTMa = demod (PTMy ,fa ,fs ,'ptm ');
subplot (3 ,1 ,3)
plot (t,PTMa ,'k');
ylabel ('PTM ');
axis ([0
0 .025
-0.2
1.2 ]);
xlabel ('seconds ');
8.3.3
Coding
Practical sampling of analog signals is done with analog to digital converters. That
means that samples are discretizations, or quantiﬁcations, of analog values. For
instance, if a 8 bit analog to digital converter is used, continuous analog values
are discretized into 255 possible values, represented by 8 bit words. Thus samples
are converted into bit sets. The bit sets can be transmitted in serial format, using a
carrier or directly pulses. The generic term for this kind of process is Pulse-Code
Modulation (PCM).
A simple way to compress the information coding is to take relative values instead
of absolute values in each signal sample. That is, only the difference between the
present sample and the previous sample is quantized and transmitted. This is called
Differential Pulse-code Modulation (DPCM). When smooth changes in signals are
guaranteed, the differences can be coded with just two bits (0, <, or >, with respect
to previous sample), this is Delta Modulation (DM).
8.3.4
Inter-symbol Interference
Pulses occupy a large bandwidth. Pulse shaping is commonly applied for band-
limited communication channels. At the same time, the pulse shaping focuses on
avoiding inter-symbol interference (ISI). This interference takes place when pulses
deformations along the communication channel end-up with overlapping and mixing.
Raised-cosine digital ﬁlters have been considered in the previous chapter. This
kind of ﬁlter is appropriate to avoid ISI problems. In practice, two square-root raised-
cosine ﬁlters are used, one in the sender side and the other in the receiver side. In
this case, the receiver ﬁlter has an impulse response that matches the received signals
(they are also impulse responses of the same kind of ﬁlter). Consequently, it is said
that matched ﬁlters are used.
The ﬁrrcos() function has an option (‘sqrt’) to work as a square root raised-cosine
ﬁlter.

8.4 Transmission Media. Multiplexing
523
8.4
Transmission Media. Multiplexing
Every radio station is assigned a carrier frequency. Radio bands, like for instance
AM, offer a determined bandwidth that has to be divided into sub-bands, one per
radio station.
If pulses were used as carriers, the same radio station would appear in several
different sub-bands, because pulses include several harmonics (Fourier decomposi-
tion). This is highly undesirable. Only sinusoid signals appear in just one sub-band.
Consequently, carriers for radio waves are sinusoids.
The situation is different when cables or ﬁbre optic are used. In this case, pulses
(conveniently shaped) could be used as the transmission signals.
Onceacommunicationchannelhasbeenestablished,everyeffortisdonetoexploit
the channel for more and more independent message signals being transmitted at the
same time. This is multiplexing.
In part, the multiplexing topic was already introduced in Eq.(8.12), using two
orthogonalsignals,sineandcosine,totransmittwoanalogsignalsinthesamechannel
(quadrature modulation).
There are two main alternatives for multiplexing: frequency domain or time
domain.
8.4.1
Frequency Domain Multiplexing
With frequency division multiplexing (FDM) several carriers with different frequen-
cies are used; it is like having several senders and several receivers, each pair being
tuned to one of the available frequencies.
A sketch of the concept is presented in Fig.8.23.
Bluetooth, for example, operates in the 2.4 GHz band and uses FDM with 79
channels from 2.402 GHz to 2.480 GHz.
The Global System for Mobile Communications protocol (GSM) uses FDM, with
124 uplink and 124 downlink channels, see [7] for a MATLAB implementation.
The Orthogonal FDM (OFDM) uses several channels, and each channel utilizes
multiple sub-carriers. Each sub-carrier is orthogonal to one another. Some of the
ﬂavours of WiFi are OFDM.
Two interesting papers on wireless communications are [6, 22]; a Thesis on ultra-
wideband systems is [4].
8.4.2
Time Domain Multiplexing
Imagine a motorized rotary switch with N positions. There are N message signals
connected to the N inputs of the switch. The output of the switch would be a series of

524
8
Modulation
Fig. 8.23 Frequency division multiplexing
interleaved samples of the N signals: mxa = a1(t1), a2(t2) . . . aN(t N), . . .. This
function can be done with solid state electronics. The signal mxa can be transmitted
by cable, ﬁber optics, etc. Time domain multiplexing is called TDM.
Figure8.24 shows a sketch of TDM.
A main TDM trend is characterized by the use of digital modulation of sine signals.
For example, quite a lot of practical research has been directed towards multiplexing
with PSK versions, like quadrature PSK (QPSK) and others that may use a set of
distinct phase shifts.
Fig. 8.24 Time division multiplexing

8.5 Experiments
525
8.5
Experiments
Let us do some experiments dealing with modulation and communications. The ﬁrst
one will become relatively long, since it considers three alternatives. The second is
much shorter and responds to a curiosity.
8.5.1
Communication and Noise
A main concern of communication systems is the quality degrading due to the inﬂu-
ence of noise in the communication channel. In the case of data communication this
may represent a serious problem of data integrity.
The situation being considered is depicted in Fig.8.25.
Let us simulate a complete communication system, with a sender, a communi-
cation channel, and a receiver. In this scenario, three different modulation methods
will be studied for comparison. One period of sine signal is being transmitted. The
reader is invited to modify aspects of the programs, such for instance the level of
noise added in the channel.
8.5.1.1
AM Communication
Figure8.26, obtained with the Program 8.15 shows the results with AM modulation.
The signal sent is represented on top. The plot in the middle is the modulated signal.
The plot at the bottom is the demodulated signal.
Notice the ripple in the demodulated sine signal.
Fig. 8.25 Noisy communication

526
8
Modulation
Fig. 8.26 AM
communication in the
presence of noise
0
0.005
0.01
0.015
0.02
0.025
-1
0
1
modulating
 sine
0
0.005
0.01
0.015
0.02
0.025
-2
0
2
comm. signal
0
0.005
0.01
0.015
0.02
0.025
-1
0
1
demodulated
 sine
seconds
Program 8.15 AM and noise in the communication channel
%
AM
and
noise
in
the
communication
channel
%
the
modulating
signal
is
a
sine
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1500;
% carrier
frequency
in
Hz
fs =30000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% modulating
signal
a=sin(wa*t);
subplot (3 ,1 ,1);
plot (t,a,'k');
title ('noise
in
the
AM
communication ');
ylabel ('modulating
sine ');
axis ([0
0 .025
-1.2
1.2 ]);
%AM
modulation
y= modulate (a,fc ,fs ,'amdsb -tc ');
Nnu= length (y);
% number
of
data
points
nu= randn (Nnu ,1);
% random
noise
signal
data
set
% adding
noise
to
communication
channel :
yn=y+(0 .2*nu )';
subplot (3 ,1 ,2)
plot (t,yn ,'k');
ylabel ('comm.
signal ');
axis ([0
0 .025
-2.4
2.4 ]);
%PTM
demodulation
da= demod (yn ,fc ,fs ,'amdsb -tc ' ,0.5 );
subplot (3 ,1 ,3)
plot (t,da ,'k');
ylabel ('demodulated
sine ');
axis ([0
0 .025
-1.2
1.2 ]);
xlabel ('seconds ');

8.5 Experiments
527
8.5.1.2
PWM Communication
Figure8.27, obtained with the Program 8.16, shows the results with PWM
modulation. The demodulated signal is smoother than the result of the AM system.
Program 8.16 PWM and noise in the communication channel
%
PWM
and
noise
in
the
communication
channel
%
the
modulating
signal
is
a
sine
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
nsc =60;
% number
of
samples
per
signal
period
fs=nsc *fa;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% sampling
the
modulating
signal
a=0 .5 +0 .3* sin(wa*t);
subplot (3 ,1 ,1);
plot (t,a,'k');
ylabel ('modulating
sine ');
title ('noise
in
the
PWM
communication ');
axis ([0
0 .025
-0.2
1.2 ]);
%PWM
modulation
[PWMy ,ty ]= modulate (a,fa ,fs ,'pwm ','centered ');
Nnu= length (ty );
% number
of
data
points
nu= randn (Nnu ,1);
% random
noise
signal
data
set
% adding
noise
to
communication
channel :
PWMyn = PWMy +(0 .2*nu )';
subplot (3 ,1 ,2)
plot (ty/nsc ,PWMyn ,'k');
ylabel ('comm.
signal ');
axis ([0
0 .025
-0.8
1.8 ]);
%PWM
demodulation
PWMa = demod (PWMyn ,fa ,fs ,'pwm ','centered ');
subplot (3 ,1 ,3)
plot (t,PWMa ,'k');
ylabel ('demodulated
sine ');
axis ([0
0 .025
-0.2
1.2 ]);
xlabel ('seconds ');
8.5.1.3
PTM Communication
Now, Fig.8.28, obtained with the Program 8.17, shows the results with PTM
modulation. The demodulated signal is also generally smooth, but it shows some
errors due to the noise.

528
8
Modulation
Fig. 8.27 PWM
communication in the
presence of noise
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
modulating
 sine
0
0.005
0.01
0.015
0.02
0.025
-0.5
0
0.5
1
1.5
comm. signal
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
demodulated
 sine
seconds
Fig. 8.28 PTM
communication in the
presence of noise
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
modulating
 sine
0
0.005
0.01
0.015
0.02
0.025
-0.5
0
0.5
1
1.5
comm. signal
0
0.005
0.01
0.015
0.02
0.025
0
0.5
1
demodulated
 sine
seconds
Program 8.17 PTM and noise in the communication channel
%
PTM
and
noise
in
the
communication
channel
%
the
modulating
signal
is
a
sine
fa =40;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
nsc =60;
% number
of
samples
per
signal
period
fs=nsc *fa;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .03
seconds ):
t=0: tiv :(0 .03 -tiv );
% sampling
the
modulating
signal
a=0 .5 +0 .3* sin(wa*t);
subplot (3 ,1 ,1);
plot (t,a,'k');
ylabel ('modulating
sine ');
title ('noise
in
the
PTM
communication ');

8.5 Experiments
529
axis ([0
0 .025
-0.2
1.2 ]);
%PTM
modulation
[PTMy ,ty ]= modulate (a,fa ,fs ,'ptm ' ,0.1 );
Nnu= length (ty );
% number
of
data
points
nu= randn (Nnu ,1);
% random
noise
signal
data
set
% adding
noise
to
communication
channel :
PTMyn = PTMy +(0 .2*nu )';
subplot (3 ,1 ,2)
plot (ty/nsc ,PTMyn ,'k');
ylabel ('comm.
signal ');
axis ([0
0 .025
-0.8
1.8 ]);
%PTM
demodulation
PTMa = demod (PTMyn ,fa ,fs ,'ptm ');
subplot (3 ,1 ,3)
plot (t,PTMa ,'k');
ylabel ('demodulated
sine ');
axis ([0
0 .025
-0.2
1.2 ]);
xlabel ('seconds ');
8.5.2
Cepstrum of Analog AM Modulation
Let us see the cepstrum of an AM modulated signal. Figure8.29, obtained with
the Program 8.18, shows the result. On top a weighted sum of the message signal
and the carrier has been plotted for comparison purposes. The cepstrum of the AM
modulated signal is depicted at the bottom of the ﬁgure. It shows a composition of
signals corresponding to the message signal and to the carrier.
Fig. 8.29 Cepstrum of AM
modulated signal, compared
with added signals on top
0
0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05
0
2
4
6
added signals
0
0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05
0
0.1
0.2
0.3
cepstrum of AM signal
seconds

530
8
Modulation
Program 8.18 Analyze AM modulation with cepstrum
% analyze
AM
modulation
with
cepstrum
%
AM
sine
signal
fa =100;
% signal
frequency
in
Hz
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =1000;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =10000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% time
intervals
set
(0 .5
seconds ):
t=0: tiv :(0.1 -tiv );
Nt= length (t);
MD =0 .5;
% modulation
depth
A =1+( MD*sin (wa*t));
% amplitude
y=A.* sin(wc*t);
% modulated
signal
data
set
subplot (2 ,1 ,1)
ysum =1 .5*A+ real (y);
% signals
adding
% plot
added
signals :
plot (t(2: Nt /2) , ysum (2: Nt /2) , 'k');
title ('added
signals ');
axis ([0
0.05
0
6]);
subplot (2 ,1 ,2)
cz= rceps (y);
% real
cepstrum
% plot
the
cepstrum :
plot (t(2: Nt /2) , abs(cz (2: Nt /2)) , 'k');
title ('cepstrum
of
AM
signal ');
xlabel ('seconds ');
axis ([0
0.05
0
0.3 ]);
8.6
Resources
8.6.1
MATLAB
8.6.1.1
Toolboxes
• Communications System Toolbox:
http://www.mathworks.com/products/communications
• Modulation Toolbox for MATLAB:
http://isdl.ee.washington.edu/projects/modulationtoolbox/
8.6.1.2
Matlab Code
Almost all books listed in the Mathworks web page on digital communication books,
contain Matlab codes.
• Overview of Communication Topics:
http://web.cecs.pdx.edu/~ece2xx/ECE223/Slides/Communications.pdf

8.6 Resources
531
• fsk example:
http://www.mathworks.com/matlabcentral/ﬁleexchange/33688-fsk/content/
fsk.m
• Matlab simulated signals (University of Oulu):
https://www.ee.oulu.ﬁ/research/ouspg/MATLAB/simulated/signals
• ask, psk, fsk, etc. :
www.srmuniv.ac.in/sites/default/ﬁles/ﬁles/comm_lab_manual_ﬁnal.pdf
• Matlab intro/refresher, ﬂat-top PAM, PCM (Colorado):
http://ecee.colorado.edu/~mathys/ecen4652/pdf/lab01.pdf
8.6.2
Internet
8.6.2.1
Web Sites
• Digital Communication Systems using Matlab and Simulink (D. Silage):
http://astro.temple.edu/~silage/digitalcommMS.htm
• Early Radio & Vintage Crystal Sets:
http://debyclark.blogspot.com.es/2013/02/early-radio-vintage-crystal-sets.html
• Invention History: The Father of FM:
http://inventorspot.com/father_of_fm
References
1. A. Aldroubi, K. Gröchenig, Nonuniform sampling and reconstruction in shift-invariant spaces.
SIAM Rev. 43(4), 585–620 (2001)
2. A. Basit, W. Aziz, F. Zafar, Implementation of ssb modulation/demodulation using Hilbert
transform in matlab. J. Expert Syst. 1(13), 79–83 (2012)
3. M. Boulmalf, Y. Semmar, A. Lakas, K. Shuaib, Teaching digital and analog modulation to
undergradute information technology students using matlab and simulink, in Proceedings of
the IEEE Education Engineering (EDUCON) (2010), pp. 685–691
4. L.P. Christensen, Signal processing for ultra-wideband systems. Ph.D. thesis, Technical Uni-
versity of Denmark (2003)
5. T.M. Cover, J.A. Thomas, Elements of Information Theory (Wiley, New York, 2006)
6. R. Cristi, Wireless Communications with Matlab and Simulink. Lecture Presentation. Monterey
Naval Postgraduate School (2009)
7. N. Deshpande, Matlab implementation of GSM trafﬁc channel. Ph.D. thesis, University of
South Florida (2003)
8. J. Feng, Digital Communications and Signal Processing - with Matlab Examples. Lecture
Notes. University of Warwick (2007)
9. H.D. Lüke, The origins of the sampling theorem. IEEE Commun. Mag. 37(4), 106–108 (1999)
10. U.Madhow,IntroductiontoCommunicationSystems (CambridgeUniversityPress,Cambridge,
2014)
11. J.G. Proakis, M. Salehi, G. Bauch, Contemporary Communication Systems Using Matlab (Cen-
gage Learning, Boston, 2013)

532
8
Modulation
12. C.E. Shannon, A mathematical theory of communication. Bell Syst. Tech. J. 27, 379–423
(1948)
13. C.E. Shannon, Communication in the presence of noise. Proc. IRE 37(1), 10–21 (1949)
14. C.E. Shannon, A mathematical theory of communication. ACM SIGMOBILE Mob. Comput.
Commun. Rev. 5(1), 3–55 (2001)
15. D. Silage, Digital Communication Systems Using MATLAB and Simulink (Bookstand Publish-
ing, Gilroy, 2009)
16. B. Sklar, Digital Communications: Fundamentals and Applications (Prentice Hall, Englewood
Cliffs, 2001)
17. P.H. Tran, Matlab/simulink implementation and analysis of three pulse-width-modulation
(PWM) techniques. Ph.D. thesis, Boise State University (2012)
18. M. Unser, Sampling-50 years after Shannon. Proc. IEEE 88(4), 569–587 (2000)
19. M. Vetterli, P. Marziliano, T. Blu, Sampling signals with ﬁnite rate of innovation. IEEE Trans.
Signal Process. 50(6), 1417–1428 (2002)
20. M. Viswanathan, V. Mathuranathan, Simulation of Digital Communication Systems Using Mat-
lab. eBook (2013)
21. Y.S. Yang, W.Y. Cho, W.G. Jeon, J.W. Lee, J.H. Paik, J.K. Kim, M.H. Lee, K.S. Woo, Mat-
lab/Simulink for Digital Communication (A-Jin Publishing, 2009)
22. Y. Zeng, Adaptive modulation schemes for optical wireless communication systems. Ph.D.
thesis, University of Warwick (2010)
23. R.E. Ziemer, W.H. Tranter, Principles of Communication Systems (Wiley, New York, 2015)

Appendix A
Transforms and Sampling
A.1
Introduction
Along the book, the pertinent theory elements have been invoked when directly
linked with the topic being studied. It seems also convenient to have an appendix
with a summary of the essential theory, which could be consulted as reference at any
moment.
It is clear that the main focus should be put on the Fourier transform, and on
sampling. Other transforms are also of interest.
Indeed, this appendix could become quite long if all formal mathematics was
considered, together with extensions, applications, etc. There are books, which will
cited in this appendix, that treat in detail these aspects. Our aim here is more to
summarize main points, and to provide reference materials.
Tables of common transform pairs are provided by [29].
A.2
The Fourier Transform
The origins of the Fourier transform can be dated at 1807. It was included in the
Fourier’s book on heat propagation. We would recommend to see [28] in order to
grasp the history of harmonic analysis, up to recent times.
A recent book on the Fourier transform principles and applications is [11]. Besides
it, there are many other books that include detailed expositions of the Fourier trans-
form.
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1
533

534
Appendix A: Transforms and Sampling
A.2.1
Deﬁnitions
A.2.1.1
Fourier Series
Fourier series have the form:
y(t) = a0 +
∞

n=1
an cos (n · w0 t) +
∞

n=1
bn sin(n · w0 t)
(A.1)
This series may not converge. It is possible to be sure of convergence by com-
plying with the Dirichlet conditions. These conditions are sufﬁcient conditions for
a periodic, real-valued function f (t) to be equal to its Fourier series at each point
where f () is continuous. The conditions are:
• f (x) be absolutely integrable (L1) over a period
• f (x) must have a ﬁnite number of extrema in any given bounded interval
• f (x) must have a ﬁnite number of ﬁnite discontinuities in any given bounded
interval
• f (x) must be bounded
A brief lecture note from [35] or [18] would give you more details of the Fourier
series convergence issues.
Supposing that f (t) can be represented with a Fourier series, and that f (t) =
f (t + 2π), the coefﬁcients of the corresponding expression can be computed as
follows:
a0 = 1
π
 π
−π
f (t) dt
(A.2)
an = 1
π
 π
−π
f (t) cos(nt) dt
(A.3)
bn = 1
π
 π
−π
f (t) sin(nt) dt
(A.4)
It is convenient to remark that there are square integrable functions (L2) that are
not integrable (L1), [16].
Given a periodic signal f (t), if:
• f (−t) = f (t), then it is an even signal
• f (−t) = −f (t), then it is an odd signal
The product of two signals z(t) = x(t) · y(t) has the following result:

Appendix A: Transforms and Sampling
535
x(t) y(t) z(t)
even even even
even odd odd
odd even odd
odd odd even
In the case of an even signal f (t):
 π
−π
f (t) dt = 2 ·
 π
0
f (t) dt
(A.5)
and in the case of an odd signal f (t):
 π
−π
f (t) dt = 0
(A.6)
Suppose f (t) is even, then:
an = 1
π
 π
−π
f (t) cos(nt) dt = 2
π
 π
0
f (t) cos(nt) dt
(A.7)
since both f (t) and cos( ) are even functions.
On the other hand:
bn = 1
π
 π
−π
f (t) sin(nt) dt = 0
(A.8)
since sin( ) is an odd function and the product with f (t) will render also an odd
function.
In the case of an odd signal f (t), there is a reverse situation: the an coefﬁcients
are zero, while bn coefﬁcients can be non-zero.
There are other equivalent expressions for the Fourier series, using only sines or
only cosines (in both cases the harmonic terms have amplitude and phase), or using
complex exponentials.
A.2.1.2
Fourier Integral
Given an aperiodic function f (t), it can be supposed to be periodic with period =
inﬁnity. As the period tends to inﬁnity, the summations of the Fourier series tend to
integrals.
The Fourier transform of a function f (t) is the following:
F(ω) =
∞

−∞
f (t) e−jω t dt
(A.9)

536
Appendix A: Transforms and Sampling
The inverse Fourier transform is:
f (t) =
1
2 π
∞

−∞
F(ω) e jω t dω
(A.10)
Dirichlet conditions also apply in this context.
Part of the literature prefers to use Hz instead of radians/s for the frequency. In
this case, the transforms are:
F(ν) =
∞

−∞
f (t) e−j2π ν t dt
(A.11)
and,
f (t) =
∞

−∞
F(ν) e j2π ν t dω
(A.12)
where we used ν for the frequency in Hz.
There are the following relationships:
f (t)
F(ν)
real
F(−ν) = |F(ν)|∗
imaginary F(−ν) = −|F(ν)|∗
even
F(−ν) = F(ν)
odd
F(−ν) = −F(ν)
In addition:
f (t)
F(ν)
real and even
F(ν) real and even
real and odd
F(ν) imaginary and odd
imaginary and even F(ν) imaginary and even
imaginary and odd F(ν) real and odd
TwofunctionsareorthogonalifandonlyiftheirFouriertransformsareorthogonal.
A.2.1.3
Discrete Fourier Transform (DFT)
The Discrete Fourier Transform (DFT) of a discrete signal f (n) with ﬁnite duration
of N samples, is given by:

Appendix A: Transforms and Sampling
537
F(ω) =
N−1

n = 0
f (n) e−jω n
(A.13)
Usually, one considers a discretized frequency, so the DFT becomes:
F(ωk) = Fk =
N−1

n = 0
f (n) e−jωk n =
N−1

n = 0
f (n) e−j2π n k/N ; k = 0, 1, . . . , N −1
(A.14)
The DFT can be expressed in matrix form:
⎛
⎜⎜⎜⎜⎜⎜⎝
F0
F1
F2
.
.
FN−1
⎞
⎟⎟⎟⎟⎟⎟⎠
=
⎛
⎜⎜⎜⎜⎜⎜⎝
W 0
N W 0
N W 0
N . . .
W 0
N
W 0
N W 1
N W 2
N . . .
W N−1
N
W 0
N W 2
N W 4
N . . .
W 2 (N−1)
N
. . . . . .
. . . .
W 0
N W N−1
N
W (N−1) 2
N
. . .
W (N−1)(N−1)
N
⎞
⎟⎟⎟⎟⎟⎟⎠
⎛
⎜⎜⎜⎜⎜⎜⎝
f (0)
f (1)
f (2)
.
.
f (N −1)
⎞
⎟⎟⎟⎟⎟⎟⎠
(A.15)
where:
WN = e
−j 2π
N
(A.16)
Then, in compact form, the DFT is:
¯Y = W ¯y
(A.17)
The matrix W is symmetric, that is, WT = W. Also, the matrix W has the ﬁrst row
and the ﬁrst column all ones. The matrix W can be pre-computed and stored or put
into ﬁrmware.
The inverse DFT can be written as:
¯y = 1
N W∗¯Y
(A.18)
where W* is the complex conjugate of W, obtained by conjugating every element
of W. No matrix inversion is required.
A.2.2
Examples
Let us obtain some interesting results concerning a selection of signals frequently
found in practical applications.

538
Appendix A: Transforms and Sampling
A.2.2.1
Square Wave
Consider the square wave depicted in Fig. A.1.
It is an odd signal with period 2π. The sine coefﬁcients of its Fourier series can
be computed as follows:
bn = 2
π
 π
0
f (t) sin(nt) dt = 2
π

−cos nt
n
π
0
= 2
π
2
1, 0
2, 2
3 , 0
4, 2
5, . . .

(A.19)
A.2.2.2
Single Pulse
Let us study the single pulse depicted in Fig. A.2.
Suppose that T = 1. The Fourier transform of this signal is:
 1/2
−1/2
e−jω t dt =

e−j ω t
−j ω
1/2
−1/2
= e jω/2 −e−jω/2
jω
= sin(ω/2)
ω/2
= sinc(ω)
(A.20)
A.2.2.3
Single Triangle
Consider now the single triangle depicted in Fig. A.3.
The signal is:
Fig. A.1 Square wave
Fig. A.2 Single pulse

Appendix A: Transforms and Sampling
539
Fig. A.3 Single triangle
f (t) =
 1 −|t|, |t| < 1
0 , |t| ≥1
(A.21)
The Fourier transform of this signal is:
 ∞
−∞f (t) e−jω tdt = 2 ·
 1
0 (1 −t) cos(ω t) dt =
2
ω sin(ω) −2
ω
 1
0 t d(sin(ω t))
=
2
ω

sin(ω) −(t sin(ω))|1
0 −
 1
0 sin(ω t) dt

=
2
ω2 cos(ω t)|1
0 =
=
2
ω2 (1 −cos(ω )) =
 2
ω
2 sin2(ω/2) = sinc2(ω)
(A.22)
Since the triangle can be built as the convolution of two rectangular signals, the
result above can be also obtained as follows:
F{triangle(t)} = F{rect(t) ∗rect(t)} = sinc(ω) · sin c(ω) = sinc2(ω)
(A.23)
This approach is convenient for the treatment of splines, which can be obtained
by convolutions of rectangles.
A.2.3
Properties
Both the direct and the inverse Fourier transforms are linear operators, so the trans-
form of a sum of functions is equal to the sum of transforms of these functions.
Next table shows a ﬁrst set of properties, playing with time and frequency.
A second table show results related to differentiation, convolution, and
conjugation.
Property
Signal
Fourier transform
time shifting
f (t −to)
e−jω to F(ω)
frequency shifting
e jωo t f (t)
F(ω −ωo)
scaling
f (t/α)
|α| · F(α ω)
time shift and scaling
f ((t −to)/α)
|α| · e−jω to F(α ω)
frequency shift and scaling |α| e jωo t f (α t) F((ω −ωo)/α)
time reversal
f (−t)
F(−ω)

540
Appendix A: Transforms and Sampling
Operation
Signal
Fourier transform
t-Differentiation
d
dt f (t)
j ω · F(ω)
ω-Differentiation t · f (t)
d
dω F(ω)
t-Convolution
x(t) ∗y(t) X(ω) Y(ω)
ω-Convolution
x(t) y(t)
X(ω) ∗Y(ω)
Conjugation
f (t)
F(−ω)
A.2.4
Theorems
The following theorem was proposed, in a primitive version, by Parseval in 1799 for
periodic functions. In 1910 it was extended by Plancherel for the real line. It states
that:
∞

−∞
x(t) y∗(t) dt =
1
2π
∞

−∞
X(ω) Y ∗(ω) dω
(A.24)
In particular, for x(t) = y(t), the theorem establishes that:
∞

−∞
|y(t)| 2dt =
1
2π
∞

−∞
|Y(ω)|2 dω
(A.25)
Both sides of Eq. (A.11) express the energy of the signal. The right-hand side can
also be written in function of the power spectral density, since:
∞

−∞
|Y(ω)|2 dω =
∞

−∞
Sy(ω) dω
(A.26)
In cases where the power spectral density Sy(ω) is deﬁned, the Wiener-Khinchin
theorem says it can be obtained from the autocorrelation Ry(τ) as follows:
Sy(ω) =
∞

−∞
Ry(τ) e−jωτ dτ
(A.27)
If one assumes that Sy(ω) and Ry(τ) satisfy the conditions for valid Fourier inversion,
then both functions form a Fourier transform pair:
Ry(τ) =
1
2π
∞

−∞
Sy(ω) e jωτ dω
⇔Sy(ω) =
∞

−∞
Ry(τ) e−jωτ dτ
(A.28)

Appendix A: Transforms and Sampling
541
Where the autocorrelation is deﬁned as:
Ry(τ) =
∞

−∞
y(t) y∗(t −τ) dτ
(A.29)
(the asterisk means complex conjugate)
Norbert Wiener enounced this theorem for deterministic functions in 1930. A.
Khinchin obtained a similar result for wide-sense stationary random processes in
1934. In [17] this result is extended for non-wide sense stationary random processes.
A.2.5
Tables of Fourier Transforms
Here is a table with the Fourier transforms of some functions of interest.
Function
Fourier transform
δ(t) (Dirac)
1
e j ω0t
2πδ(ω −ω0)
sgn(t)
2/jω
j/πt
sgn(ω)
cos(ω0t)
π[δ(ω −ω0) + δ(ω + ω0)]
sin(ω0t)
π
j [δ(ω −ω0) −δ(ω + ω0)]
e−α |t|
2 α
α2+ω2
u(t) (unit step) π δ(ω) +
1
jω
u(t) e−α t
1
α+ jω
A.2.6
The Fast Fourier Transform (FFT)
The FFT is not a new transform; it is just a fast algorithm to compute the DFT. It
obeys to a divide and conquer strategy. The basic idea is to compute the DFT of
length N using the DFT of two sub-series of length N/2. In turn, each sub-series
can be sub-divided into two, and so on.
Then, while the brute-force computation of the DFT would be an O(N 2) process,
the FFT would be an O(N log2 N) process. For example, a DFT taking 2 weeks of
computation, would take 30 s if using the FFT.
The main observation was made by Danielson and Lanczos in 1942, and was
expressed as the following lemma:

542
Appendix A: Transforms and Sampling
Fk =
N−1

n = 0
f (n) e−j2π n k/N =
=
N/2−1

n = 0
f (2n) e−j2π (2n )k/N +
N/2−1

n = 0
f (2n + 1) e−j2π (2n+1 )k/N =
N/2−1

n = 0
f (2n) e−j2π n k/(N/2) + Wk
N/2−1

n = 0
f (2n + 1) e−j2π nk/(N/2) =
= Fe
k + Wk Fo
k
(A.30)
where Fkis the k-th term of the DFT. As you can see, the idea is to consider separately
the even and the odd terms, so the complete DFT can be obtained by computing N/2
length DFTs. Most conveniently, the idea can be recursively applied for each DFT.
Suppose you have 106 samples. The direct computation of the DFT would need
(N)2 = 1012 multiplications. If, however, you choose the decomposition into two N/2
length DFTs, the computation of the complete DFT would take N + 2 · (N/2)2 ≈
N 2/2 = 5 · 1011 multiplications. Nevertheless, the memory required for the algo-
rithm recursions would be quite large, in the order of n . 102 Mb. Fortunately, there
are means to reduce it to about 10Mb.
A main path for work saving is to exploit symmetries and to avoid redundancies.
For example, the WN factors, which are also called “twiddle factors” or “phase
factors”, being roots of unity. These factors have the following properties:
• Symmetry: W k+N/2
N
= −W k
N
• Periodicity: W k+N
N
= W k
N
• Recursion: W 2
N = WN/2
Now, we are going to propose some examples. A simpliﬁed expression of the
DFT would be used, in the following terms:
Fk =
N−1

n=0
W kn
N fn
(A.31)
Suppose that N = 4. Then, WN = e−jπ/2 = −j. The result of the DFT would be:
Fk = f0 + (−j)k f1 + (−j)2k f2 + (−j)3k f3 =
=
f0 + (−j)k f1 + (−1)k f2 + jk f3
(A.32)
To save computations, we could re-arrange terms as follows:
F0 = ( f0 + f2) + ( f1 + f3)
(A.33)
F1 = ( f0 −f2) −j ( f1 −f3)
(A.34)
F2 = ( f0 + f2) −( f1 + f3)
(A.35)

Appendix A: Transforms and Sampling
543
Fig. A.4 Basic diagram
Fig. A.5 The N = 4 DFT computation procedure
F3 = ( f0 −f2) + j ( f1 −f3)
(A.36)
Then, the computation can proceed in two steps: ﬁrst the terms in parenthesis, and
second adding the results. This scheme implies a drastic reduction of memory usage.
It is convenient to describe the procedure using Butterﬂy diagrams. The basic one
is shown in Fig. A.4.
Figure A.5 shows a complete Butterﬂy diagram for the N = 4 DFT computation:
In the case of N = 8, the DFT would be obtained by a decomposition into three
stages, as depicted in Fig. A.6.
Again, a Butterﬂy diagram helps to describe the computational procedure for
N = 8. This is shown in Fig. A.7.
Notice the order of the inputs to the Butterﬂy diagram. This corresponds to bit-
reversal, as described in the following table:
0
1
2
3
4
5
6
7
000 001 010 011 100 101 110 111
000 100 010 110 001 101 011 111
0
4
2
6
1
5
3
7

544
Appendix A: Transforms and Sampling
Fig. A.6 The N=8 DFT decomposition into 3 stages
Fig. A.7 The N = 8 DFT computation procedure
The FFT algorithm just described was introduced by Cooley and Tukey in 1965,
in a heavily cited article [5]. Later on, it was recognized that C.F. Gauss, the famous
mathematician, already introduced the algorithm in 1805.
More examples could be added for N = 16, N = 32, and in general for N = 2n,
which will be solved by a similar decomposition strategy, with 4, 5, …, n stages. This
type of algorithm belongs to radix-2 algorithms. See [4] for an extensive treatment
of theory and implementation of FFT. In addition, the books [37, 38] provide an
intuitive and practical view.

Appendix A: Transforms and Sampling
545
An interesting academic contribution is [9], in which, besides a good
description of the FFT accompanied with a MATLAB implementation, one ﬁnds
a set of applications for image processing.
There are other FFT approaches, like those brieﬂy described in [12]. One of the
books that deals in detail with the FFT is [23].
A.2.7
Fourier in Two Dimensions
A.2.7.1
2D DFT
The 2D DFT is deﬁned as follows:
Fk,l =
M−1

m=0
N−1

n = 0
f (m, n) e−j 2πk m/M · e−j 2π ln /N
(A.37)
The inverse 2D DFT is:
f (k,l) = 1
M
1
N
M−1

m=0
N−1

n = 0
Fm,n · e j 2πk m/Me j 2π ln /N
(A.38)
A.2.7.2
2D Fourier Integral
In 2D the Fourier transform of a function f (x1, x2) is the following:
F(ω1, ω2) =
∞

−∞
∞

−∞
f (x1, x2) e−j(ω1 x1+ω2 x2) dx1dx2
(A.39)
The inverse Fourier transform is:
f (x1, x2) =
∞

−∞
∞

−∞
F(ω1, ω2) e j(ω1 x1+ω2 x2) dω1dω2
(A.40)
A.2.8
Other Aspects
There are other aspects related to the Fourier transform that deserve at least a brief
consideration.

546
Appendix A: Transforms and Sampling
A.2.8.1
Uncertainty Principle
In the ﬁeld of harmonic analysis, there is an uncertainty principle saying that it is not
possible to localize at the same time the value of a function and its Fourier transform,
since we have that:
 ∞
−∞
x2| f (x)|2dx

·
 ∞
−∞
ω2|F(ω)|2dw

≥∥f (x)∥4
2
16π2
(A.41)
Thus, at least one of f () or F() must not be too concentrated near the origin. Other
formulations exist that imply that for any pair of points a and b, then F() cannot be
concentrated around b if f () is concentrated around a.
This formulation can be related to the Heisenberg’s principle, and also with the
sampling theorem of Shannon [19].
The theorem of Benedicks [3], establishes that the set of points where f () is
non-zero and the set of points where F() is non-zero cannot both be ﬁnite. This is a
generalization of the fact that a signal cannot be both time-limited and band-limited.
Another uncertainty principle, elicited by Hardy in 1933, can be intuitively
expressed as follows: it is not possible for f () and F() to both be rapidly decreasing.
Speciﬁcally, given to positive constants p and q and assuming that for some positive
constant C:
| f (x)| ≤C e−px2 , |F(ω)| ≤C e−qx2
(A.42)
Then:
• f ( ) = 0, i f pq > 1/4
• f ( ) = A e−p x2, i f pq = 1/4, where A is some constant
• there are inﬁnitely many f ( ) , i f pq < 1/4
Intuitively, one would say that if one of f () or F() has brisk decays, the other
must be smooth.
See [25] and references therein for more details. The article [13] presents a survey
of the various forms of the uncertainty principle an some new interpretations.
A.2.8.2
Scientiﬁc Activity. Applications
Although this is an appendix, it is still a suitable occasion for mentioning important
scientiﬁc activities and applications that live because of the Fourier transform. Most
of the items to be introduced next, have web pages with much more information.
The addresses of these pages are included in the Resources section at the end of this
appendix.
Harmonic analysis is a very alive scientiﬁc ﬁeld, involving many publications,
conferences, and other research activities. One representative node is the Norbert
Wiener Center for Harmonic Analysis. In Europe, there is a vigorous initiative of the

Appendix A: Transforms and Sampling
547
European Science Foundation, establishing a Research Network for Harmonic and
Complex Analysis.
Another center of activity is the Numerical Harmonic Analysis Group, NuHAG,
at the University of Wien, Austria.
As it can be easily guessed, vibrations are a clear target for the application of
Fourier. A review of this type of applications is made in [34]. Since this is a subject
of industrial interest, like for example machine condition monitoring, there is a
signiﬁcant number of books combining theory and technical aspects, like [10, 26,
31]. There are also academic approaches for dynamic structures, like the Thesis [36].
Seismic waves are also a suitable target for Fourier analysis, including waves
induced by Jupiter [20], or more usual cases in the form of earthquakes [27, 32]. See
[15] for an interesting proposal of using local time-frequency decompositions.
In other order of things, a short tutorial on Fourier and Laplace is given by one of
the Berkely Science Books on Internet, vol. 4 on Good Vibrations.
A.3
Sampling
The sampling theorem was published in 1949 in a scientiﬁc article of C.E. Shannon,
[25].
The sampling theorem can be expressed in several ways. One of these formula-
tions, [8], is given next.
If f(t) is L1 and band-limited to a frequency W in Hz, then it can be expanded as:
f (t) =
∞

n=−∞
f (nT ) sinc
t −nT
T

(A.43)
with T= 1/(2W).
The expression above implies that the signal f (t) can be recovered from its
samples.
It happens that the Fourier transform of sinc(t/T) is equal to T for |ν| < W and 0
for |ν| > W. Then, one has the following Fourier pair:
sinc
t −nT
T

↔
 T · e−j 2π ν nT , f or |ν| < W
0,
f or |ν| > W
(A.44)
A direct way to prove the theorem is included in [8], as follows:
• By time-frequency symmetry, we have also a Fourier series expansion in the fre-
quency domain:
F(ν) =
∞

n=−∞
Fn e−j 2π ν T , −W ≤ν ≤W
(A.45)

548
Appendix A: Transforms and Sampling
• Taking now the inverse Fourier transform:
f (t) =
∞

n=−∞
Fn
T sinc
t −nT
T

(A.46)
it can be seen, by evaluating both sides at t=nT, that Fn/T = u(nT ).
It is now convenient to simplify the notation, in order to consider an interesting
aspect. Denote:
ψn(t) = sinc
t −nT
T

(A.47)
Then, one has the following result:
∞

−∞
ψn(t) ψm(t) dt =
0,
f or n ̸= m
T, f or n = m
(A.48)
Therefore,thefunctionsinsidetheintegralareorthogonal.Inconsequence,theexpan-
sion considered in the sampling theorem is an orthogonal expansion.
Other aspects, like aliasing, oversampling, and an interpretation of the FFT in
view of the sampling theorem, are considered in [14].
For those interested on Information Theory, it would be recommended to read the
classic book of Pierce [22]. Another book on this topic, in a tutorial style is [30].
A.4
Other Transforms
Besides the Fourier transform, other transforms have been employed in this book. In
particular, the Laplace transform and the z-transform. This section is mainly devoted
to these two transforms. Both are related with the Fourier transform.
In the third subsection, a more general view of transforms will be considered.
A.4.1
The Laplace Transform
The history of the Laplace transform involves contributions from Euler, Lagrange,
Laplace and others. It could be situated around 1785.
The book [6] provides an introduction to the Laplace transform and the Fourier
series. A classic book on the Laplace transform is [24].
An important use of the Laplace transform is directly related with the represen-
tation via transfer functions of continuous time linear systems.

Appendix A: Transforms and Sampling
549
The Laplace transform of a real-valued function f (t) is a unilateral transform
deﬁned by:
F(s) =
∞

0
f (t) e−s t dt
(A.49)
where s is a complex variable:
s = σ + j ω
(A.50)
A brief notation to represent the transform is: L{ f }.
There is also a bilateral Laplace transform:
F(s) =
∞

−∞
f (t) e−s t dt
(A.51)
Notice that if you only take the imaginary part of s, this bilateral transform is the
Fourier transform. Then, the Fourier transform can be considered as a particular case
of the bilateral Laplace transform.
The Laplace transform is a linear operator, that is:
L{ f (t) + g(t)} = L{ f (t)} + L{g(t)} = F(s) + G(s)
(A.52)
L{α f (t)} = α L{ f (t)} = α F(s)
(A.53)
The properties of the Laplace transform are similar to those of the Fourier transform.
With respect to time and frequency, a succinct account is given in the following
table:
Property
Signal
Laplace transform
time shifting
f (t −to)h(t −to) e−s to F(s)
frequency shifting eα t f (t)
F(s −α)
scaling
f (t/α), α > 0
α · F(α s)
where h(t −to) is the Heaviside function.
Next table shows results related to differentiation, convolution, and conjugation
Operation
Signal
Laplace transform
t-Differentiation
d
dt f (t)
s · F(s) −f (0)
ω-Differentiation t · f (t)
−d
ds F(s)
t-Convolution
x(t) ∗y(t) X(s) Y(s)
Conjugation
f (t)
F(s∗)

550
Appendix A: Transforms and Sampling
The initial value theorem says:
f (0+) = lim
s→∞s F(s)
(A.54)
And the ﬁnal value theorem is:
f (∞) = lim
s→0 s F(s)
(A.55)
Next table includes the Laplace transforms of some functions of interest.
A.4.2
The z Transform
The name “z-transform” was coined by Ragazzini and Zadeh in 1952. An advanced
z-transform was developed, later on, by E.I. Jury.
Like the Laplace transform in continuous time systems, an important use of the
z-transform is related to the representation of discrete time systems via discrete
transfer functions. The book [7] gives extensive information on the z-transform.
Function
Laplace transform
1
1/s
δ
1
t
1/s2
eα t
1
s−α
cos(ωt)
s
s2+ω2
sin(ωt)
ω
s2+ω2
e−α t cos(ω t)
s+α
(s+α)2+ω2
e−α t sin(ω t)
ω
(s+α)2+ω2
The bilateral z-transform of a discrete signal f (n) is given by:
F(z) =
∞

n = −∞
f (n) z−n
(A.56)
where z is a complex number:
z = A e jΦ = A (cos Φ + j sin Φ)
(A.57)
If the signal f (n) is zero for all n < 0, we can write:
F(z) =
∞

n = 0
f (n) z−n
(A.58)

Appendix A: Transforms and Sampling
551
which is the unilateral z-transform.
Notice that the DFT can be obtained from the unilateral z-transform by the
following change of variable:
z = e jω
(A.59)
The z-transform is a linear operator. Its properties are summarized in the next two
tables:
Property
Signal
z-transform
time shifting f (n −k) z−k F(z)
z-scaling
αn f (n)
F(z/α)
time reversal f (−n)
F(z−1)
Operation
Signal
z-transform
Accumulation
∞

k=−∞
f (n)
1
1−z−1 F(z)
z-Differentiation n · f (n)
−z d
dz F(z)
t-Convolution
x(n) ∗y(n) X(z) Y(z)
Conjugation
f (n)
F(z∗)
The initial value theorem:
f (0) = lim
z→∞F(z)
(A.60)
The ﬁnal value theorem:
f (∞) = lim
z→1 (z −1) F(z)
(A.61)
Here is a table with the z-transforms of some functions of interest.
Function
z-transform
δ
1
u(t)
z
z−1
t
T z
(z−1)2
e−α t
z
z−e−αT
cos(ωt)
z(z−cos ωT )
z2−2 z cos ωT +1
sin(ωt)
z sin ωT
z2−2 z cos ωT +1
e−α t cos(ω t)
z2−z e−αT cos ωT
z2−2 z e−αT cos ωT +e−2 αT
e−α t sin(ω t)
z e−αT sin ωT
z2−2 z e−αT cos ωT +e−2 αT

552
Appendix A: Transforms and Sampling
A.4.3
Transforms in General
Any vector in 3D can be represented on an orthonormal basis of vectors. Roughly
speaking, it is possible to do something similar in function spaces, using bases of
orthonormal functions. The Fourier transform provides one of these bases, made with
functions of the form e jω t.
A suitable mathematical framework for this perspective—function spaces- is
Hilbert space [21]. There is abundant scientiﬁc literature on this ﬁeld. One of the
aspects covered refers to different orthonormal bases. For example, there are Legen-
dre polynomials, Laguerre functions, Hermite functions, orthonormal polynomials,
wavelets, etc. Corresponding to these bases, there are many transforms that one could
use.
There is a voluminous text on Internet, [1] more than 100 Mb, with tables of
integral transforms. Another set of books, [2], complete the scene with a compendium
of higher transcendental functions.
An important observation is that linear dynamical systems can be described with
derivatives and integrals. The derivative of an exponential function is again an expo-
nential function. The same happens with integrals. More formally, it happens that
exponential functions are eigenfunctions of the linear dynamical systems. This is an
important reason for adopting the Laplace transform or the Fourier transform.
Of course, there are other scientiﬁc areas where other transforms are more appro-
priate; like in the case of dealing with spherical geometries (for instance, the Earth
as a geode).
Another aspect of interest in the Hilbert space framework is operator theory. The
Fourier transform is a unitary operator, which represents an important archetype. It
seems opportune to highlight some properties of the Fourier transform as operator.
If one takes the Fourier transform of the Fourier transform, one obtains:
2π f (−t) =
∞

−∞
F(ω) e−jω t dω
(A.62)
(time-reversed, multiplied by a constant).
The result above is still simpler if one uses frequency in Hz and so there is no 2π
constant. It also happens that:
F F F = F−1
(A.63)
Three times the Fourier transform is just the inverse transform. And,
F F F F = I
(A.64)
Four times the Fourier transform is the identity operator.
An interesting Fourier transform pair is the following:

Appendix A: Transforms and Sampling
553
f (t) = e−t2/2
⇔F(ω) =
√
2π e−ω2/2
(A.65)
Notice that f (t) and F(ω) have the same shape, the shape of a Gaussian. Many other
examples of functions with the same shape in time and frequency domains can be
built, as shown in [33] (see also [27] for some motivating connections).
A.5
Resources
A.5.1
MATLAB
A.5.1.1
Toolboxes
• Signal Processing Toolbox:
http://www.mathworks.com/products/signal/
• Control System Toolbox:
http://www.mathworks.com/products/control/
A.5.1.2
Matlab Code
• SFTPACK:
http://people.sc.fsu.edu/~jburkardt/m_src/sftpack/sftpack.html
• Digital Signal Processing Demos (Purdue University):
https://engineering.purdue.edu/VISE/ee438/demos/Demos.html
• MATLAB Demos:
http://fourier.eng.hmc.edu/e59/matlabdemos/
• Digital Signal Processing (Spectrograms):
http://cnx.org/contents/a806bd3a-194f-4ed2-9609-9436b4ced26e@2.44:31/
Digital_Signal_Processing:_A_U
• Audio Signal Processing Basics:
http://www.cs.tut.ﬁ/sgn/arg/intro/basics.html
• Sound Processing:
http://www.numerical-tours.com/matlab/audio_1_processing/
• Speech Processing:
http://cvsp.cs.ntua.gr/~nassos/resources/speech_course_2004/Online
SpeechDemos/speechDemo_2004_Part1.html

554
Appendix A: Transforms and Sampling
A.5.2
Internet
A.5.2.1
Web Sites
• The Fourier Transform:
http://www.thefouriertransform.com/
• Mathematics of the DFT (Stanford University):
https://ccrma.stanford.edu/~jos/st/
• FFTW:
http://www.fftw.org/
Sparse Fast Fourier Transform:
http://groups.csail.mit.edu/netmit/sFFT/index.html
• Educational MATLAB GUIs:
http://users.ece.gatech.edu/mcclella/matlabGUIs/
• The Norbert Wiener Center for Harmonic Analysis and Applications:
http://www.norbertwiener.umd.edu/About/index.html
• Harmonic and Complex Analysis and its Applications (European Science Foun-
dation):
http://org.uib.no/hcaa/
• Numerical Harmonic Analysis Group:
http://www.univie.ac.at/nuhag-php/home/
• Speech Spectrogram:
https://www.projectrhea.org/rhea/index.php/Speech_Spectrogram
• A Wavelet Tour of Signal Processing (includes Fourier):
http://cas.ensmp.fr/~chaplais/Wavetour_presentation/Wavetour_presentation_
US.html\#dyadique
• Good Vibrations, Fourier Analysis and the Laplace Transform (Berkeley):
http://berkeleyscience.com/synopsis4.htm
• Earthquakes:
https://quakewatch.wordpress.com/2012/11/15/magnitude-6-0-guerrero-
mexico-15-nov-12/
A.5.2.2
Link Lists
• D.W. Simpson:
https://www.dwsimpson.com/fourieranalysis.html
• Math Archives
http://archives.math.utk.edu/topics/fourierAnalysis.html

Appendix A: Transforms and Sampling
555
References
1. H. Bateman, in Tables of Integral Transforms (1954), http://authors.library.
caltech.edu/43489/
2. H. Bateman, W. Magnus, F. Oberhettinger, F.G. Tricomi, Higher Transcendental
Functions (McGraw-Hill, New York, 1955)
3. M. Benedicks, On Fourier transforms of functions supported on sets of ﬁnite
Lebesgue measure. J. Math. Anal. Appl. 106(1), 180–183 (1985)
4. C.S.S. Burrus, T.W. Parks, DFT/FFT and Convolution Algorithms: Theory and
Implementation (Wiley, New York, 1991)
5. J.W. Cooley, J.W. Tukey, An algorithm for the machine calculation of complex
fourier series. Math. Comput. 19(90), 297–301 (1965)
6. P. Dyke, An Introduction to Laplace Transforms and Fourier Series (Springer,
Berlin, 2014)
7. T.S. ElAli. Discrete Systems and Digital Signal Processing with Matlab (CRC
Press, Boca Raton, 2011)
8. R. Gallager, The Sampling Theorem (Handout, ETH Zurich, 1991), www.nari.
ee.ethz.ch/teaching/wirelessIT/handouts/sampling.pdf.
9. R. Gil, Effective algorithms in harmonic analysis and applications to signal
processing. Master’s thesis, Universitat de Barcelona (2011)
10. S. Goldman, Vibration Spectrum Analysis (Industrial Press, Inc., New York,
1999)
11. E.W. Hansen, Fourier Transforms: Principles and Applications (Wiley, New
York, 2014)
12. G. Hiary, FFT and It’s Applications. Lecture Notes in Mathematics, vol. 5603,
The Ohio State University (2014), https://people.math.osu.edu/hiary.1/5603_
F14_notes/Math_Project.pdf.
13. P.Jaming,Uncertaintyprinciplesfororthonormalbases(2006),arXiv:math/0606396
14. G. Lerman, The Shannon sampling theorem and its implications. Lecture Notes
in Mathematics, vol. 467, University of Minnesota (2006), www.math.umn.edu/
~lerman/math5467/shannon_aliasing.pdf
15. Y. Liu, S. Fomel, Seismic data analysis using local time-frequency decomposi-
tion. Geophys. Prospect. 61(3), 516–525 (2013)
16. K. Long. Math 5311 – A Short Introduction to Function Spaces. Lecture
Notes, Texas Tech University (2009), www.math.ttu.edu/~klong/5311-Spr09/
funcSpace.pdf.
17. W. Lu, N. Vaswani, The Wiener-Khinchin theorem for non-wide sense stationary
random processes, (2009), arXiv:0904.0602
18. R.J. McEliece, On the Convergence of Fourier Series and Transforms. Lecture
Notes, EE32a, Caltech (2001), www.systems.caltech.edu/EE/Courses/EE32a/
handout/FS_Convergence.pdf.
19. P.A. Millette, The Heisenberg uncertainty principle and the Nyquist-Shannon
sampling theorem, (2011), arXiv:1108.3135

556
Appendix A: Transforms and Sampling
20. B. Mosser, J.P. Maillard, D. Mékarnia, New attempt at detecting the Jovian
oscillations. Icarus 144(1), 104–113 (2000)
21. J. Muscat, Functional Analysis (Springer, Berlin, 2014)
22. J.R. Pierce, An Introduction to Information Theory: Symbols, Signals and Noise
(Dover, New York, 1980)
23. K.R. Rao, D.N. Kim, J.J. Hwang, Fast Fourier Transform – Algorithms and
Applications (Springer, Berlin, 2010)
24. J.L. Schiff, The Laplace Transform (Springer, Berlin, 1991)
25. C.E. Shannon, Communication in the presence of noise. Proc. IRE 37(1), 10–21
(1949)
26. J.K. Sinha, Vibration Analysis, Instruments, and Signal Processing (CRC Press,
New York, 2014)
27. L.R. Soares, H.M. de Oliveira, R.J.S. Cintra, R.C. de Souza, Fourier eigenfunc-
tions, uncertainty Gabor principle and isoresolution wavelets, in Anais do XX
Simpósio Bras. de Telecomunicações, Rio de Janeiro (2003)
28. R.S. Stankovic, J.T. Astola, M.G. Karpovsky, Remarks on history of abstract
harmonic analysis. Presentation (2002), www.cs.tut.ﬁ/~jta/computing-history-
material/fourierhistory.pdf
29. M.Ph. Stoecklin, Tables of Common Transform Pairs (2012), www.mechmat.
ethz.ch/Lectures/tables.pdf
30. J.V. Stone, Information Theory: A Tutorial Introduction. (Sebtel Press, Shefﬁeld,
2015)
31. J.I. Taylor, The Vibration Analysis Handbook (Vibration Consultants, Florida,
2003)
32. H. Thráinsson, A.S. Kiremidjian, S.R. Winterstein, Modeling of earthquake
ground motion in the frequency domain. Technical report, John A. Blume Earth-
quake Engineering Center (2000)
33. P.P. Vaidyanathan, Eigenfunctions of the Fourier transform. IETE J. Educ. 49(2),
51–58 (2008)
34. R. Wald, T. Khoshgoftaar, J.C. Sloan, Fourier transforms for vibration analy-
sis: a review and case study, in Proceedings IEEE International Conference on
Information Reuse and Integration (IRI), pp. 366–371 (2011)
35. W. Xu. Pointwise Convergence of Fourier Series: The Theorems of Fejér and
Dirichlet. Lecture Notes, MA, vol. 433, University of Warwick (2014), www2.
warwick.ac.uk/fac/sci/maths/people/staff/weijun_xu/ma433_14/pointwise.pdf
36. X. Zhang, The Fourier Spectral Element Method for Vibration Analysis of Gen-
eral Dynamic Structures. PhD thesis, Wayne State University, 2011.
37. A.E. Zonst, Understanding FFT Applications (Citrus Press, Titusville, 2003)
38. A.E. Zonst, Understanding the FFT (Citrus Press, Titusville, 2003)

Appendix B
Long Programs
B.1
Introduction
Some of the programs developed for the chapters of this book are long. In order to
simplify the use of the book, it has been preferred to assemble these programs in the
present appendix.
Small version of the ﬁgures generated by the programs have been added, to help
identifying each program.
B.2
Chapter 2: Statistical Aspects
B.2.1
Markov Chain (2.10.1.)
Weather prediction model with three states (Fig.B.1).
Program B.1 Example of Markov Chain (weather prediction)
%
Example
of
Markov
Chain
%
with
3
states
% transition
matrix
T =[0 .65
0 .20
0 .15;
0 .30
0.24
0 .46;
0 .52
0.12
0 .36 ];
% initial
probabilities
pC =0 .5;
pS =0 .4;
pR =0 .1;
% initial
state
rand ('state',sum (100* clock ));
u= rand (1);
if
u<pC ,
X =1;
elseif
u<pC+pS ,
X=2;
else
X =3;
end;
% initialize
result
R
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1
557

558
Appendix B: Long Programs
Fig. B.1 Weather prediction
model with three states
(Fig. 2.55)
0
10
20
30
40
50
60
1
1.5
2
2.5
3
n
if
X==1 ,
R='C';
end;
% clouds
if
X==2 ,
R='S';
end;
% Sun
if
X==3 ,
R='R';
end;
% rain
rX= zeros (1 ,60);
% for
state
historic
rX (1)= X;
% run
the
process ---------------
for
nn =2:60 ,
u= rand (1);
% state
transitions
if
X==1 ,
if
u<T(1 ,1) ,
X =1;
elseif
u <(T(1 ,1)+T(1 ,2)) ,
X =2;
else
X =3;
end;
end;
if
X==2 ,
if
u<T(2 ,1) ,
X =1;
elseif
u <(T(2 ,1)+T(2 ,2)) ,
X =2;
else
X =3;
end;
end;
if
X==3 ,
if
u<T(3 ,1) ,
X =1;
elseif
u <(T(3 ,1)+T(3 ,2)) ,
X =2;
else
X =3;
end;
end;
rX(nn )=X;
% store
result
% concatenation
if
X==1 ,
R=[R,'C'];
end;
% clouds
if
X==2 ,
R=[R,'S'];
end;
% sunny
if
X==3 ,
R=[R,'R'];
end;
% rain
end;
disp (R);
plot (rX ,'k');
axis ([0
60
0.8
3.2 ]);

Appendix B: Long Programs
559
0
10
20
30
40
50
60
1
1.5
2
2.5
HMM (synthetic speech): the hidden states
n
0
10
20
30
40
50
60
1
2
3
4
5
the emissions
n
Fig. B.2 Simple HMM model of speech (Fig. 2.59)
title ('3
states
Markov
Chain :
transitions');
xlabel ('n');
B.2.2
Hidden Markov Chain (HMM) (2.10.2.)
A simplistic model of speech (Fig.B.2).
Program B.2 Example of HMM (synthetic speech)
%
Example
of
HMM
( synthetic
speech )
%
with
2
states
%
each
state
has
4
emission
alternatives
% transition
matrix
( probabilities )
T =[0 .3
0.7;
0.5
0.5 ];
% emissions
from
state
1
( probabilities )
E1 =[0 .2
0.3
0.2
0.3 ];
% emissions
from
state
2
( probabilities )
E2 =[0 .3
0.3
0.2
0.2 ];
% initial
probabilities
pC =0 .6;
% consonant
(X =1)
pW =0 .4;
% wovel
(X =2)
% initial
state
rand ('state',sum (100* clock ));
u= rand (1);

560
Appendix B: Long Programs
X =2;
if
u<pC ,
X =1;
end;
% initial
emission
u= rand (1);
if
X==1 ,
if
u<E1 (1) ,
EM =1;
R='W';
elseif
u <( E1 (1)+ E1 (2)) ,
EM =2;
R='H';
elseif
u <( E1 (1)+ E1 (2)+ E1 (3)) ,
EM =3;
R='T';
else
EM =4;
R='_';
end;
end;
if
X==2 ,
if
u<E2 (1) ,
EM =1;
R='A';
elseif
u <( E2 (1)+ E2 (2)) ,
EM =2;
R='E';
elseif
u <( E2 (1)+ E2 (2)+ E2 (3)) ,
EM =3;
R='O';
else
EM =4;
R='U';
end;
end;
rX= zeros (1 ,60);
% for
state
record
rE= zeros (1 ,60);
% for
emission
record
rX (1)= X;
rE (1)= EM;
% run
the
process -------------------
for
nn =2:60 ,
u= rand (1);
% state
transitions
if
X==1 ,
X =2;
if
u<T(1 ,1) ,
X =1;
end;
end;
if
X==2 ,
X =1;
if
u<T(2 ,2) ,
X =2;
end;
end;
% emission
u= rand (1);
if
X==1 ,
if
u<E1 (1) ,
EM =1;
R=[R,'W'];
elseif
u <( E1 (1)+ E1 (2)) ,
EM =2;
R=[R,'H'];
elseif
u <( E1 (1)+ E1 (2)+ E1 (3)) ,
EM =3;
R=[R,'T'];
else
EM =4;
R=[R,'_'];
end;
end;
if
X==2 ,
if
u<E2 (1) ,
EM =1;
R=[R,'A'];
elseif
u <( E2 (1)+ E2 (2)) ,
EM =2;
R=[R,'E'];
elseif
u <( E2 (1)+ E2 (2)+ E2 (3)) ,
EM =3;
R=[R,'O'];
else
EM =4;
R=[R,'U'];
end;
end;
rX(nn )=X;
% store
result
rE(nn )= EM;
end;
disp (R);
% print
result
% display
subplot (2 ,1 ,1)
plot (rX ,'k');
title ('HMM
( synthetic
speech ):
the
hidden
states');

Appendix B: Long Programs
561
xlabel ('n');
axis ([0
60
0.8
2.2 ]);
subplot (2 ,1 ,2)
plot (rE ,'k');
axis ([0
60
0.5
4.5 ]);
title ('the
emissions');
xlabel ('n');
B.3
Chapter 4: Analog Filters
B.3.1
Comparison of Filter Phases and Group Velocities
(4.6.2.)
A comparison of the group delay of the ﬁve analog 5th order ﬁlters (Fig.B.3).
Program B.3 Comparison of group delay of 5 ﬁlters
%
Comparison
of
group
delay
of
5
filters
wc =10;
% desired
cut - off
frequency
N =5;
% order
of
the
filter
Rp =0 .5;
% decibels
of
ripple
in
the
pass
band
Rs =20;
% decibels
of
ripple
in
the
stop
band
% analog
Butterworth
filter :
[num , den ]= butter (N,wc ,'s');
% logaritmic
set
of
frequency
values :
w= logspace (0 ,2 ,500);
% computes
frequency
response :
G= freqs (num ,den ,w);
ph= angle (G);
ph= unwrap (ph );
% phase
npp= length (w);
gd= zeros (npp ,1);
Fig. B.3 Comparison of the
group delay of the ﬁve 5 th
order ﬁlters (Fig. 4.52)
100
101
102
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
group delay
rad/s
E
T
C1
C2 
B

562
Appendix B: Long Programs
for
np =2: npp ,
gd(np )=( ph(np)-ph(np -1))/( w(np)-w(np -1));
end;
semilogx (w (2: npp),-gd (2: npp),'k');
% plots
group
delay
hold
on;
axis ([1
100
0
2]);
grid ;
ylabel ('group
delay');
xlabel ('rad/s');
title ('comparison
of
group
delay
of
the
filters');
% analog
Chebyshev
1
filter :
[num , den ]= cheby1 (N,Rp ,wc ,'s');
G= freqs (num ,den ,w);
% computes
frequency
response
ph= angle (G);
ph= unwrap (ph );% phase
for
np =2: npp ,
gd(np )=( ph(np)-ph(np -1))/( w(np)-w(np -1));
end;
semilogx (w (2: npp),-gd (2: npp),'r');
% plots
group
delay
% analog
Chebyshev
2
filter :
[num , den ]= cheby2 (N,Rs ,wc ,'s');
G= freqs (num ,den ,w);
% computes
frequency
response
ph= angle (G);
ph= unwrap (ph );% phase
for
np =2: npp ,
gd(np )=( ph(np)-ph(np -1))/( w(np)-w(np -1));
% elimination
of
discontinuity :
if
gd(np)>1,
gd(np )= gd(np -1);
end;
end;
semilogx (w (2: npp),-gd (2: npp),'g');
% plots
group
delay
% analog
elliptic
filter :
[num , den ]= ellip (N,Rp ,Rs ,wc ,'s');
G= freqs (num ,den ,w);
% computes
frequency
response
ph= angle (G);
ph= unwrap (ph );% phase
for
np =2: npp ,
gd(np )=( ph(np)-ph(np -1))/( w(np)-w(np -1));
% elimination
of
discontinuity :
if
gd(np)>1,
gd(np )= gd(np -1);
end;
end;
semilogx (w (2: npp),-gd (2: npp),'b');
% plots
group
delay
[num , den ]= besself (N,wc );
% analog
Bessel
filter
G= freqs (num ,den ,w);
% computes
frequency
response
ph= angle (G); ph= unwrap (ph );
% phase
for
np =2: npp ,
gd(np )=( ph(np)-ph(np -1))/( w(np)-w(np -1));
end;
semilogx (w (2: npp),-gd (2: npp),'m');
% plots
group
delay
B.3.2
Recovering a Signal Buried in Noise (4.7.1.)
Recovering a sinusoidal signal buried in noise (Fig.B.4).
Program B.4 Recovering a sinusoid buried in noise
%
Filtering
the
sine + noise
signal
fs =4000;
% sampling
frequency
in
Hz

Appendix B: Long Programs
563
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(4 - tiv );
% time
intervals
set
(4
seconds )
N= length (t);
% number
of
data
points
yr =0 .5* randn (N ,1);
% random
signal
data
set
fy =400;
% sinusoidal
signal
frequency
(400
Hz)
ys= sin(fy *2* pi*t);
% sinusoidal
signal
y=ys+yr';
% the
signal + noise
% plot
sine + noise
( first
0.1
sec ):
subplot (2 ,1 ,1);
plot (t (1:400) , y (1:400) ,'k');
axis ([0
0.1
-2
2]);
ylabel ('signal + noise');
title ('filtering
the
sine + noise
signal');
fl =370;
%
desired
low
cut - off
frequency
in
Hz
fh =430;
%
desired
high
cut - off
frequency
in
Hz
wl=fl *2* pi;
wh=fh *2* pi;
%
to
rad/s
wb =[ wl
wh ];
% the
pass
band
of
the
filter
N =10;
%
order
of
the
filter
(5+5)
% analog
Butterworth
filter :
[num , den ]= butter (N,wb ,'s');
G=tf(num , den );
% transfer
function
of
the
filter
yout = lsim (G,y,t);
% filter
output
% plot
extracted
signal
( first
0.1
sec. ):
subplot (2 ,1 ,2);
plot (t (1:400) , yout (1:400) ,'k');
axis ([0
0.1
-2
2]);
xlabel ('seconds');
ylabel ('extracted
signal');
sound (y,fs );
% sound
of
sine + noise
pause (5);
sound (yout ,fs );
% sound
of
extracted
signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
signal+noise
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
seconds
extracted signal
Fig. B.4 Recovering a signal buried in noise (Fig. 4.53)

564
Appendix B: Long Programs
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
0
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y0
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-0.5
0
0.5
y5
seconds
Fig. B.5 Extracting components from a compound signal (Fig. 4.54)
B.3.3
Adding and Extracting Signals (4.7.2.)
Extract the 3 sinusoidal signals, which previously have been added into one signal
(Figs.B.5 and B.6).
Program B.5 Adding and recovering experiment
%
Adding
and
recovering
experiment
fs =4000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(0.1 -tiv );
% time
intervals
set
(0 .1
seconds )
fu0 =100;
% base
sinusoidal
signal
frequency
(100
Hz)
u0= sin( fu0 *2* pi*t);
% fundamental
harmonic
u3= sin (3* fu0 *2* pi*t);
%3rd
harmonic
u5= sin (5* fu0 *2* pi*t);
%5th
harmonic
u=
u0
+
(0 .5*u3)
+
(0 .3*u5 );
% input
signal
%
extracting
the
fundamental
harmonic
fh =120;
% desired
cut - off
of
a
low - pass
filter
wh=fh *2* pi;
%
to
rad/s
N =5;
%
order
of
the
filter
% analog
low - pass
Butterworth
filter :
[num , den ]= butter (N,wh ,'s');
G=tf(num , den );
% transfer
function
of
the
filter
y0= lsim (G,u,t);
% response
of
the
low - pass
filter

Appendix B: Long Programs
565
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
ysum
seconds
Fig. B.6 Original and reconstructed signals (Fig. 4.55)
%
extracting
the
3rd
harmonic
fl =280;
%
desired
low
cut - off
frequency
in
Hz
fh =320;
%
desired
high
cut - off
frequency
in
Hz
wl=fl *2* pi;
wh=fh *2* pi;
%
to
rad/s
wb =[ wl
wh ];
% the
pass
band
of
the
filter
N =10;
%
order
of
the
filter
(5+5)
% analog
band - pass
Butterworth
filter :
[num , den ]= butter (N,wb ,'s');
G=tf(num , den );
% transfer
function
of
the
filter
y3= lsim (G,u,t);
% response
of
the
band - pass
filter
%
extracting
the
5th
harmonic
fh =480;
%
desired
high
cut - off
frequency
in
Hz
wh=fh *2* pi;
%
to
rad/s
N =5;
%
order
of
the
filter
% analog
high - pass
Butterworth
filter :
[num , den ]= butter (N,wh ,'high','s');
G=tf(num , den );
% transfer
function
of
the
filter
y5= lsim (G,u,t);
% response
of
the
high - pass
filter
figure (1)
subplot (4 ,1 ,1);
plot (t,u,'k');
% the
complete
signal
ylabel ('compound
signal');
title ('adding
and
recovering
experiment');
% the
recovered
fundamental
harmonic :
subplot (4 ,1 ,2);
plot (t,y0 ,'k');
ylabel ('y0');
% the
recovered
3rd
harmonic
subplot (4 ,1 ,3);
plot (t,y3 ,'k');
ylabel ('y3');
% the
recovered
5th
harmonic
subplot (4 ,1 ,4);
plot (t,y5 ,'k');
ylabel ('y5');

566
Appendix B: Long Programs
xlabel ('seconds');
% ------------------------
ysum =y0+y3+y5;
% adding
recovered
harmonics
figure (2)
% the
complete
input
signal :
subplot (2 ,1 ,1);
plot (t,u,'k');
ylabel ('compound
signal');
title ('adding
and
recovering
experiment');
% the
added
harmonics :
subplot (2 ,1 ,2);
plot (t,ysum ,'k');
ylabel ('ysum');
xlabel ('seconds');
B.4
Chapter 5: Digital Filters
B.4.1
Classical Approach (IIR Filters) (5.4.1.)
Pole-zero maps of the four IIR ﬁlters (Fig.B.7).
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Butterw.
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Cheby1
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Cheby2
-1
-0.5
0
0.5
1
-1
-0.5
0
0.5
1
Elliptic
Fig. B.7 Comparison of pole-zero maps of the four digital ﬁlters (Fig. 5.36)

Appendix B: Long Programs
567
Program B.6 Comparison of pole-zero maps of the 4 digital ﬁlters
%
Comparison
of
pole - zero
maps
of
%
the
4
digital
filters
fs =130;
% sampling
frequency
in
Hz
fc =10/( fs /2);
%cut - off
at
10
Hz
N =5;
% order
of
the
filter
Rp =0 .5;
% decibels
of
ripple
in
the
pass
band
Rs =20;
% decibels
of
ripple
in
the
stop
band
[numd , dend ]= butter (N,fc );
% digital
Butterworth
filter
tfd=tf(numd , dend );
[P,Z]= pzmap ( tfd );
% poles
and
zeros
of
the
filter
subplot (2 ,2 ,1);
% plots
pole - zero
map
plot (P,'kx','Markersize' ,10);
hold
on;
plot (Z,'ko','Markersize' ,8);
zgrid ;
axis ([ -1 .1
1
-1
1]);
title ('Butterworth');
% digital
Chebyshev
1
filter :
[numd , dend ]= cheby1 (N,Rp ,fc );
tfd=tf(numd , dend );
[P,Z]= pzmap ( tfd );
% poles
and
zeros
of
the
filter
subplot (2 ,2 ,2);
% plots
pole - zero
map
plot (P,'kx','Markersize' ,10);
hold
on;
plot (Z,'ko','Markersize' ,8);
zgrid ;
axis ([ -1 .1
1
-1
1]);
title ('Chebyshev
1');
% digital
Chebyshev
2
filter :
[numd , dend ]= cheby2 (N,Rs ,fc );
tfd=tf(numd , dend );
[P,Z]= pzmap ( tfd );
% poles
and
zeros
of
the
filter
subplot (2 ,2 ,3);
% plots
pole - zero
map
plot (P,'kx','Markersize' ,10);
hold
on;
plot (Z,'ko','Markersize' ,8);
zgrid ;
axis ([ -1 .1
1
-1
1]);
title ('Chebyshev
2');
% digital
elliptic
filter :
[numd , dend ]= ellip (N,Rp ,Rs ,fc );
tfd=tf(numd , dend );
[P,Z]= pzmap ( tfd );
% poles
and
zeros
of
the
filter
subplot (2 ,2 ,4);
% plots
pole - zero
map
plot (P,'kx','Markersize' ,10);
hold
on;
plot (Z,'ko','Markersize' ,8);
zgrid ;
axis ([ -1 .1
1
-1
1]);
title ('Elliptic');
B.4.2
Adding and Extracting Signals (5.5.1.)
Extract the 3 sinusoidal signals, which previously have been added into one signal
(Figs.B.8 and B.9).

568
Appendix B: Long Programs
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
0
2
compound
signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y0
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-1
0
1
y3
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-0.5
0
0.5
y5
seconds
Fig. B.8 Extracting components from a compound signal (Fig. 5.51)
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
compound signal
0
0.01
0.02
0.03
0.04
0.05
0.06
0.07
0.08
0.09
0.1
-2
-1
0
1
2
ysum
seconds
Fig. B.9 Original and reconstructed signals (Fig. 5.52)

Appendix B: Long Programs
569
Program B.7 Adding and recovering with ﬁltﬁlt experiment
%
Adding
and
recovering
experiment
fs =4000;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
t =0: tiv :(0.1 -tiv );
% time
intervals
set
(0 .1
seconds )
fu0 =100;
% base
sinusoidal
signal
frequency
(100
Hz)
u0= sin( fu0 *2* pi*t);
% fundamental
harmonic
u3= sin (3* fu0 *2* pi*t);
%3rd
harmonic
u5= sin (5* fu0 *2* pi*t);
%5th
harmonic
u=
u0
+
(0 .5*u3)
+
(0 .3*u5 );
% input
signal
%
extracting
the
fundamental
harmonic
fh =120/( fs /2);
% desired
cut - off
of
a
low - pass
filter
N =5;
%
order
of
the
filter
% digital
low - pass
Butterworth
filter :
[numd , dend ]= butter (N,fh );
% response
of
the
low - pass
filter :
y0= filtfilt (numd ,dend ,u);
%
extracting
the
3rd
harmonic
fl =220/( fs /2);
%
desired
low
cut - off
frequency
fh =380/( fs /2);
%
desired
high
cut - off
frequency
fb =[ fl
fh ];
% the
pass
band
of
the
filter
N =10;
%
order
of
the
filter
(5+5)
% digital
band - pass
Butterworth
filter :
[numd , dend ]= butter (N,fb );
% response
of
the
band - pass
filter :
y3= filtfilt (numd ,dend ,u);
%
extracting
the
5th
harmonic
fh =420/( fs /2);
%
desired
high
cut - off
frequency
in
Hz
N =5;
%
order
of
the
filter
% digital
high - pass
Butterworth
filter :
[numd , dend ]= butter (N,fh ,'high');
% response
of
the
high - pass
filter :
y5= filtfilt (numd ,dend ,u);
figure (1)
subplot (4 ,1 ,1);
plot (t,u,'k');
% the
complete
signal
ylabel ('compound
signal');
title ('adding
and
recovering
experiment');
% the
recovered
fundamental
harmonic :
subplot (4 ,1 ,2);
plot (t,y0 ,'k');
ylabel ('y0');
% the
recovered
3rd
harmonic :
subplot (4 ,1 ,3);
plot (t,y3 ,'k');
ylabel ('y3');
% the
recovered
5th
harmonic :
subplot (4 ,1 ,4);
plot (t,y5 ,'k');
ylabel ('y5');
xlabel ('seconds');
% ------------------------
ysum =y0+y3+y5;
% adding
recovered
harmonics
figure (2)
% the
complete
input
signal :
subplot (2 ,1 ,1);
plot (t,u,'k');
ylabel ('compound
signal');
title ('adding
and
recovering
experiment');

570
Appendix B: Long Programs
% the
added
harmonics :
subplot (2 ,1 ,2);
plot (t,ysum ,'k');
ylabel ('ysum');
xlabel ('seconds');
B.5
Chapter 7: Time-Frequency Analysis
B.5.1
Interferences in the Wigner Distribution (7.5.5.)
Wigner distribution of a signal with 2 sine components (Fig.B.10).
Program B.8 Wigner distribution of a 2-sine signal
% Wigner
distribution
of
a
2- sine
signal
clear
all
%
2- sine
signal
f1 =10;
% initial
frequency
in
Hz
f2 =50;
% final
frequency
in
Hz
fs =128;
% sampling
rate
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
between
samples
% time
of
first
signal
part
(4
seconds ):
t1 =0: tiv :(4 - tiv );
tn =4: tiv :5;
% time
inter - signal
parts
(1
seconds )
% time
of
last
signal
part
(3
seconds ):
t2 =5: tiv :(8 - tiv );
y1= exp(-j *2* pi*f1*t1 );
y2= exp(-j*2* pi*f2*t2 );
yn =0* exp(-j *2* pi*tn );
Fig. B.10 Wigner
distribution of a 2 sine signal
(Fig. 7.20)
seconds
Hz
0
1
2
3
4
5
6
7
0
10
20
30
40
50
60

Appendix B: Long Programs
571
y=[ y1
yn
y2]';
% complete
signal
( column
vector )
t=[ t1
tn
t2 ];
% complete
signal
time
set
Ny= length (y);
% odd
number
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .1+ abs(WD )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Wigner
distribution
of
a
two - sine
signal');
% Marginals -----------------------------------------
margf = zeros (Ny ,1);
% frequency
marginal
for
nn =1:Ny ,
margf (nn )= tiv* sum(WD(nn ,:));
end;
margt = zeros (1, Ny );
% time
marginal
for
nn =1:Ny ,
margt (nn )= sum(WD (:, nn ));
end;
figure (2)
plot (f,margf ,'k');
% frequency
marginal
xlabel ('Hz');
title ('frequency
marginal');
figure (3)
plot (t,margt ,'k');
% time
marginal
xlabel ('seconds');
title ('time
marginal');
% print
y
signal
energy
disp ('signal
energy :')
e1= tiv* sum( abs( margt ))
e2= sum( abs( margf ))
Wigner distribution of a chirp signal (Fig.B.11).
Program B.9 Wigner distribution of a chirp signal
% Wigner
distribution
of
a
chirp
signal
clear
all;
%
chirp
signal
f0 =5;
% initial
frequency
in
Hz

572
Appendix B: Long Programs
Fig. B.11 Wigner
distribution of a chirp signal
(Fig. 7.24)
seconds
Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
10
20
30
40
50
60
f1 =60;
% final
frequency
in
Hz
fs =128;
% sampling
rate
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
between
samples
t1 =2;
% final
time
t =0: tiv:t1;
% time
intervals
set
(10
seconds )
yr= chirp (t,f0 ,t1 ,f1 ,'quadratic')';
% the
chirp
signal
y= hilbert (yr );
% analitical
signal
Ny= length (y);
% odd
number
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .5+ abs(WD )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Wigner
distribution
of
a
chirp
signal');

Appendix B: Long Programs
573
B.5.2
Filtering the SAF to Eliminate Interferences (7.5.5.)
Example of chirp signal (Figs.B.12 and B.13).
seconds
Hz
Filter window (kernel)
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-20
0
20
seconds
Hz
Filtered SAF
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
-20
0
20
Fig. B.12 Kernel and ﬁltered SAF for chirp signal (Fig. 7.27)
Fig. B.13 Filtered Wigner
distribution for chirp signal
(Fig. 7.28)
seconds
Hz
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
10
20
30
40
50
60

574
Appendix B: Long Programs
Program B.10 WD of chirp signal, with no interference
%
WD
of
chirp
signal ,
with
no
interference
clear
all
%
chirp
signal
f0 =5;
% initial
frequency
in
Hz
f1 =60;
% final
frequency
in
Hz
fs =128;
% sampling
rate
in
Hz
fN=fs /2;
% Nyquist
frequency
tiv =1/ fs;
% time
between
samples
t1 =2;
% final
time
t =0: tiv:t1;
% time
intervals
set
(10
seconds )
yr= chirp (t,f0 ,t1 ,f1 ,'quadratic')';
% the
chirp
signal
y= hilbert (yr );
% analitical
signal
Ny= length (y);
% odd
number
%SAF -------------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
%A
simple
box
distribution
kernel
FI= zeros (Ny ,Ny );
% window
vertical
and
horizontal
1/2
width :
HV =30;
HH =40;
FI(md -HH:md+HH ,md -HV:md+HV )=1;
% box
kernel
% Product
of
kernel
and
SAF
fsaf = FI.* SAF;
pks= ifftshift ( fsaf );
% intermediate
variable
ax =(( ifft (pks ,[] ,1)));
% Wigner
from
SAF
distribution :
WD= real (( fft(ax ,[] ,2))');
% result
display
figure (1)
fiv=fN/Ny;
% frequency
interval
freq =-fN /2: fiv :( fN /2) - fiv;
te=t (end);
tim=-te /2: tiv:te /2;
colmap1 ;
colormap ( mapg1 );
% user
colormap
subplot (2 ,1 ,1)
imagesc (tim ,freq , log10 (0 .05+abs(FI )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Filter
window
( kernel )');
subplot (2 ,1 ,2)
imagesc (tim ,freq , log10 (0 .1+ abs( fsaf )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Filtered
SAF');
% result
display
figure (2)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap

Appendix B: Long Programs
575
0
50
100
150
200
250
300
350
400
450
500
-1
-0.5
0
0.5
1
0
50
100
150
200
250
300
350
400
450
500
0
5
10
15
20
25
Fractional Fourier 
transform (a=0.8)
Fig. B.14 Fractional Fourier transform of a chirp which rate corresponds to the transform exponent
imagesc (t,f, log10 (1+ abs(WD )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Filtered
Wigner
distribution
of
the
chirp
signal');
B.5.3
Example of Fractional Fourier Transform (7.8.5.)
Example of FrFT response to the chirp with rate cot(α) (Fig.B.14).
Program B.11 Fractional Fourier transform (chirp signal)
% Fractional
Fourier
transform
% using
decomposition
% Example
of
chirp
signal
% choose
parameter
a
( fractional
power )
0<a <1 .5
a=0 .9;
% for
instance
alpha =a*pi /2;
%
the
signal
to
be
transformed -----------------------
% chirp
signal
Ny =501;
% odd
length
aex =( pi/Ny )* cot( alpha )*((0: Ny -1)'. ^2);
ch1=exp(-j* aex );
y=ch1;
yin=y;
% changes
for
a <0 .5
if
(a <0 .5),
shft
=
rem ((0: Ny -1)+ fix(Ny /2) , Ny )+1;
sqN
=
sqrt (Ny );

576
Appendix B: Long Programs
a=a +1;
y( shft )= ifft (y( shft ))* sqN;
end;
% sinc
interpolation
for
doubling
signal
data
zy= zeros (2* Ny -1 ,1);
zy (1:2:2* Ny -1)= y;
aux1 =zy (1:2* Ny -1);
aux2 = sinc ([ -(2*Ny -3):(2* Ny -3)] '/2);
m= length ([ aux1 (:); aux2 (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
yitp = ifft ( fft(aux1 ,P).* fft(aux2 ,P ));
yitp = yitp (1:m);
yitp = yitp (2*Ny -2 :end -2* Ny +3);
% interpolated
signal
% sandwich
zz= zeros (Ny -1 ,1);
ys =[ zz;
yitp ;
zz ];
%
the
fractional
transform ---------------------------
% chirp
premultiplication
htan = tan( alpha /2);
aex =( pi/Ny )*( htan /4)*(( -2* Ny +2:2* Ny -2)'. ^2);
chr=exp(-j* aex );
yc= chr. *ys;
% premultiplied
signal
% chirp
convolution
sa= sin( alpha );
cc=pi/Ny/sa /4;
aux1 = exp(j*cc *( -(4*Ny -4):4* Ny -4)'. ^2);
m= length ([ aux1 (:); yc (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
ym= ifft ( fft(aux1 ,P).*fft(yc ,P ));
ym=ym (1:m);
ym=ym (4*Ny -3:8* Ny -7)* sqrt (cc/pi );
% convolved
signal
% chirp
post
multiplication
yq= chr. *ym;
% normalization
yp= exp(-j*(1 -a)* pi /4)* yq(Ny :2 :end -Ny +1);
%
display -------------------------------------
figure (1)
subplot (2 ,1 ,1)
plot ( real ( yin),'k');
axis ([0
Ny
-1.1
1.1 ]);
title ('a
chirp
signal');
subplot (2 ,1 ,2)
plot ( abs(yp),'k');
axis ([0
Ny
-1
25]);
title ('Fractional
Fourier
transform
(a=0 .8)');
B.5.4
The Chirplet Transform (7.9.1.)
Example of Wigner distribution of chirplet atom (Fig.B.15).

Appendix B: Long Programs
577
Fig. B.15 Wigner
distribution of a Gaussian
chirplet atom (Fig. 7.44)
rad/s
seconds
0
2
4
6
8
10
12
0
10
20
30
40
50
60
Program B.12 Wigner distribution of Gaussian chirplet
%
Wigner
distribution
of
Gaussian
chirplet
t0 =5;
w0 =10;
d =6;
c=2;
% chirplet
parameters
t =0:0 .05 :12;
% times
vector
g=exp ( -(0 .5/d )*((t-t0). ^2));
v=exp(-j*( w0 +((0 .5*c )*(t-t0 ))).*(t-t0 ));
h =(1/(( pi*d )^0 .25 ))* g.*v;
y=h';
Ny= length (y);
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
% result
display
Ts =0 .05;
% sampling
period
ws =(2* pi )/ Ts;
wiv=ws /(2* Ny );
w =0: wiv :(( ws /2) - wiv );
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,w, log10 (0 .01+ abs(WD )));
axis
xy;
title ('Wigner
distribution
of
Gaussian
chirplet');
ylabel ('rad/s');
xlabel ('seconds');

578
Appendix B: Long Programs
Fig. B.16 Wigner
distribution of the original
signal (Fig. 7.49)
seconds
Hz
1
2
3
4
5
6
7
8
9
10
5
10
15
20
25
B.5.5
Unitary Equivalence Principle (7.9.2.)
Example of time-warping (Fig.B.16).
Program B.13 Wigner distribution of a modulated signal
% Wigner
distribution
of
a
modulated
signal
fs =50;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
between
samples
% time
intervals
set
(10
seconds )(t >0):
t=tiv:tiv :(10+ tiv );
fsig =5;
% signal
base
frequency
in
Hz
wsig = fsig *2* pi;
% signal
base
frequency
in
rad/s
K=1 .4;
% modulation
exponent
y=exp(-i* wsig *( t.^K))';
% the
modulated
signal
Ny= length (y);
% odd
number
% WIGNER -------------------------------------------
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/( Ny );
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
% result
display
fiv=fs /(2* Ny );
f=fiv:fiv :( fs /2);
% frequencies
set

Appendix B: Long Programs
579
figure (1)
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 (0 .1+ abs(WD )));
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Wigner
distribution
of
a
modulated
signal');
B.5.6
The Reassignment Method (7.9.3.)
Example of siren signal (Figs.B.17 and B.18).
Fig. B.17 Original
spectrogram of the siren
signal (Fig. 7.56)
Time
Frequency
0
0.2
0.4
0.6
0.8
1
0
1000
2000
3000
4000
5000
6000
7000
8000
Fig. B.18 Reassigned
spectrogram of the siren
signal (Fig. 7.57)
Time
Frequency
0.2
0.4
0.6
0.8
1
1.2
1000
2000
3000
4000
5000
6000
7000
8000

580
Appendix B: Long Programs
Program B.14 Reassigned STFT
%
Reassigned
STFT
%
Example
of
siren
clear
all;
[y,fs ]= wavread ('srn.wav');
% read
wav
file
Ny= length (y);
%
Reassignment --------------------------------------
% original
spectrogram
nft =256;
% FFT
length
Nw =256;
% window
length
m=Nw /2;
W= hamming (Nw );
SY= specgram (y,nft ,fs ,W,m);
sqm=abs(SY). ^2;
nzix = find (sqm >0);
%non - zero
elements
% build
reassignment
windows
m=Nw /2;
% frequency
ramp
framp =[(0:m -1) ,( -m: -1)]'+0 .5;
framp = framp /Nw;
Wx=- imag ( ifft ( framp. *fft(W )));
Wdt=Wx*fs;
% time
ramp
tramp =(-m:m -1)'+0 .5;
Wx= tramp. *W;
Wt=Wx/fs;
% compute
auxiliary
spectrograms
SYdt = specgram (y,nft ,fs ,Wdt ,m);
SYt= specgram (y,nft ,fs ,Wt ,m);
% compute
freq.
corrections
[nr ,nc ]= size (SY );
fcorrect = zeros (nr ,nc );
fcorrect ( nzix )=- imag ( SYdt ( nzix ).* conj (SY( nzix ))) ...
./sqm( nzix );
% analysis
bin
freqs
(Hz)
Fb =((0: nr -1)'* fs/nft )* ones (1, nc );
rF=Fb+ fcorrect ;
% reassigned
freqs
% compute
time
corrections
tcorrect = zeros (nr ,nc );
tcorrect ( nzix )= real ( SYt( nzix ).* conj (SY( nzix ))) ...
./sqm( nzix );
% analysis
frame
times
( sec)
framets =((( Nw -1)/2)+( ones (nr ,1)*(0: nc -1))*( Nw -m))/ fs;
rT= framets + tcorrect ;
% reassigned
times
( sec)
% image
plot
preparation
----------------------------
% crop
&
threshold
fmax =0 .5*fs;
fmin =0;
tmax =Ny/fs;
tmin =0;
thr = -50;
% threshold
in
dB
( edit !)--------
Smax = max( abs(SY (:)));
Mx =20* log10 ( abs(SY )/ Smax );
inzone = find (rF < fmax
&
rF > fmin
&
rT < tmax
&
rT > tmin );
ax= find (Mx > thr );
vdx= intersect ( inzone ,ax );
cSY=SY( vdx );
%it
is
a
vector
cF=rF(vdx );
cT=rT(vdx );

Appendix B: Long Programs
581
% create
image
nh= max (500 , size (SY ,2)*2);
nv= max (400 , size (SY ,1)*2);
Tmax = max(cT );
Tmin =min(cT );
dt =( Tmax - Tmin )/(nh -2);
nmax = ceil ( Tmax /dt );
nmin = floor ( Tmin /dt );
Tn= Tmin +( dt *(0: nmax - nmin ));
Fmax = max(cF );
Fmin =min(cF );
df =( Fmax - Fmin )/(nv -2);
kmax = ceil ( Fmax /df );
kmin = floor ( Fmin /df );
Fk= Fmin +( df *(0: kmax - kmin ));
%
Z
Z= zeros (nv ,nh );
for
nn =1: length ( cSY),
n=1- nmin +( cT(nn )/ dt );
k=1- kmin +( cF(nn )/ df );
alpha =n- floor (n);
beta =
k- floor (k);
kf= floor (k);
kc= ceil (k);
nf= floor (n);
nc= ceil (n);
Z(kf ,nf )=Z(kf ,nf )+((1 - alpha )*(1 - beta )* cSY(nn ));
Z(kc ,nf )=Z(kc ,nf )+((1 - alpha )*( beta )* cSY(nn ));
Z(kf ,nc )=Z(kf ,nc )+(( alpha )*(1 - beta )* cSY(nn ));
Z(kc ,nc )=Z(kc ,nc )+(( alpha )*( beta )* cSY(nn ));
end;
%
applying
the
threshold
Zmin =10^(0 .05* thr );
Zbak =10^(0 .05 *( thr -10));
% background
aux= find ( abs(Z)<= Zmin );
% background
includes
values
below
threshold :
Z(aux )= Zbak ;
%
display
----------------------------------------
figure (1)
specgram (y,nft ,fs );
title ('spectrogram
of
siren ,
before
reassignment');
figure (2)
imagesc (Tn ,Fk ,20* log10 ( abs(Z )));
axis ([ min(Tn),max(Tn),min(Fk),max(Fk )]);
axis
xy;
colormap (1- gray );
% gray
scale
title ('Reassigned
spectrogram
of
the
siren
signal');
xlabel ('Time');
ylabel ('Frequency');
B.5.7
The Fan-Chirp Transform (7.10.2.)
Example of linear chirp (Figs.B.19 and B.20).
Program B.15 Short Time Fan-Chirp transform
%
Short
Time
Fan - Chirp
transform
%
(as
a
spectrogram )
% example
with
linear
chirp
%
transform
parameters

582
Appendix B: Long Programs
Fig. B.19 Spectrogram of a
linear chirp (Fig. 7.61)
Time
Frequency
2
4
6
8
10
12
14
16
0
50
100
150
200
250
Fig. B.20 Fan-Chirp
transform of the linear chirp
(Fig. 7.62)
Time
Frequency
2
4
6
8
10
12
14
16
0
50
100
150
200
250
nft =256;
% FFT
length
nsf =32;
% number
of
signal
segments
Ny= nft* nsf;
% signal
length ,
even
nw =65;
% window
size ,
odd
nov =60;
% overlapping
mo=nw -nov;
% must
be
positive
win= hamming (nw );
% window
win= win. / sum(win );
%
the
signal
tiv =0 .002 ;
t =0: tiv :(Ny -1)* tiv;
fs =1/ tiv;
yc= exp(-j *30*( t. ^2));
y= real (yc)';
%
reference
estimated
frequencies
tr =0:0 .050 :21;

Appendix B: Long Programs
583
fr =0:0 .50 :210;
Fc=fs *(0: nft /2)/ nft;
% frequency
centers
%
time
centers :
Tc =[];
idx =1;
kk =1;
while
idx (end) <=Ny ,
idx =((kk -1)* mo )+(1: nw );
if
idx (end) <=Ny ,
Tc =[ Tc
( idx ((nw -1)/2+1) -1)/ fs ];
end
kk=kk +1;
end
%
interpolated
estimated
frequencies
fe= interp1 (tr ,fr ,Tc ,'linear','extrap');
nTc= length (Tc );
aux= ones (nTc ,1);
FT= zeros ( nft /2+1 , nTc );
for
kk =1: nTc ,
idx =((kk -1)*( mo ))+(1: nw );
%
derivative
approximation :
if
kk ==1
||
kk == nTc ,
A =0;
else
A =(1/ fe(kk ))+( fe(kk +1) - fe(kk -1)) ...
/( Tc(kk +1) - Tc(kk -1));
end
A=A/fs;
aslp (kk )=A;
%
the
FCh
transform
for
the
signal
frame :
z=y(idx).* win;
% signal
windowed
segment
M= length (z);
N=nft;
ks =(-N /2+1: N/2)';
ks =[ ks(N/2 :end );
ks (1:N /2 -1)];
nn =-(M -1)/2:(M -1)/2;
aux =( -2* pi/N).*ks *((1+0 .5*A*nn).*nn );
E=exp(j* aux );
q= ones (N ,1);
FTx=sum(q*(z'.* sqrt ( abs ((1+ A*nn )))) .*E ,2);
FT (:, kk )= FTx (1 :end /2+1);
end
%
display -------------------------------------------
figure (1)
specgram (y,nft ,fs ,win , nov );
figure (2)
imagesc (Tc ,Fc , abs(FT ));
axis
xy;
% imagesc (Tc ,Fc ,20* log10 ( abs(FT )) ,[ -100
-20]);
%
axis
xy;
xlabel ('Time');
ylabel ('Frequency');
B.5.8
The Empirical Mode Decomposition and
Hilbert-Huang Transform (7.10.4.)
Example of boink signal (Figs.B.21 and B.22)

584
Appendix B: Long Programs
0
500
1000
1500
2000
2500
3000
-1
0
1
0
500
1000
1500
2000
2500
3000
-1
0
1
0
500
1000
1500
2000
2500
3000
-0.5
0
0.5
0
500
1000
1500
2000
2500
3000
-0.2
0
0.2
0
500
1000
1500
2000
2500
3000
-0.1
0
0.1
Fig. B.21 The ﬁrst ﬁve IMFs (Fig. 7.72)
Fig. B.22 The Hilbert
spectrum (Fig. 7.63)

Appendix B: Long Programs
585
Program B.16 EMD and Hilbert Spectrum example
%
EMD
and
Hilbert
Spectrum
example
" boink "
signal
% read
data
file
[yin , fin ]= wavread ('boink.wav');
% read
wav
file
% select
a
signal
segment
and
decimate
by
1/2:
y=yin (1:2:6000) ';
fs= fin /2;
sy=y;
% for
sound
Ny= length (y);
%
EMD
decomposition
--------------------------------
nim =5;
% number
of
imfs
to
be
found
Mimf = zeros (nim ,Ny );
for
nn =1: nim ,
h=y;
% initial
signal
% standard
deviation
( used
for
stop
criterion ):
StD =1;
while
StD >0.3 ,
%
find
max/ min
points
D= diff (h);
% derivative
popt =[];
%to
store
max
or
min
points
for
i =1:Ny -2,
if
D(i)==0 ,
popt =[ popt ,i];
elseif
sign (D(i ))~= sign (D(i +1));
% the
zero
was
between
i
and
i +1:
popt =[ popt ,i +1];
end;
end;
if
size (popt ,2)
<2
% got
a
final
residue
break
end;
% distinguish
maxima
and
minima
No= length ( popt );
%
if
first
one
is
a
maximum
if
popt (1) > popt (2) ,
pmax = popt (1:2: No );
pmin = popt (2:2: No );
else
pmax = popt (2:2: No );
pmin = popt (1:2: No );
end;
% force
endpoints
pmax =[1
pmax
Ny ];
pmin =[1
pmin
Ny ];
% create
envelopes
using
spline
interpolation
maxenvp = spline (pmax ,h( pmax ) ,1: Ny );
minenvp = spline (pmin ,h( pmin ) ,1: Ny );
% mean
of
envelopes
m
=
( maxenvp + minenvp )/2;
oldh =h;
h=h-m;
% subtract
mean
to
h
% compute
StD
ipsi =0 .0000001 ;
StD=sum ((( oldh -h). ^2)./( oldh. ^2+ ipsi ));
end
Mimf (nn ,:)= h;
% store
IMF(nn)
y=y-h;
% subtract
the
IMF
from
the
signal

586
Appendix B: Long Programs
end
%
Prepare
the
Hilbert
spectrum
image
Fq= zeros (nim ,Ny );
% frequencies
Am= zeros (nim ,Ny );
% amplitudes
kk =1/(2* pi );
for
ni =1: nim ,
X= hilbert ( Mimf (ni ,:));
Am(ni ,:)= abs(X);
Ph= atan2 ( imag (X), real (X ));
Fq(ni ,2 :end )= kk* diff (Ph );
% frequencies
end
% build
a
picture ,
selecting
some
imfs
kk= floor (Ny /2);
Phht = zeros (kk ,Ny );
%
find
TF
points
and
associate
Amplitude
for
nn =1:Ny ,
for
ni =1:3 ,
% choose
imf (1) ,(2)
and
(3)
% find
a
freq.
point :
aux =1+ floor (Ny*abs(Fq(ni ,nn )));
Phht (aux ,nn )= Am(ni ,nn );
end;
end;
%
display -------------------------------------
figure (1)
for
jj =1: nim ,
subplot (nim ,1,jj)
plot ( Mimf (jj ,:) ,'k');
end
disp ('please
wait
for
second
figure')
figure (2)
L =1400;
% number
of
selected
image
lines
q =2*L/Ny;
% corresponding
max
freq.
tiv =1/ fs;
fiv=fs/Ny;
t =0: tiv :(Ny -1)* tiv;
f =0: fiv :(q*fs /2) - fiv;
colormap ('gray');
AA= Phht (1:L ,:) >0 .08;
% visualization
threshold
contourf (t,f ,1- AA );
title ('Hilbert
Spectrum');
xlabel ('Time');
ylabel ('Hz');
soundsc (sy ,fs)
B.5.9
Fractional Fourier Transform of a Rectangular Signal
(7.11.1.)
Examples for several values of a (Figs.B.23 and B.24).

Appendix B: Long Programs
587
0
50
100
150
200
250
300
350
400
0
0.5
1
0
50
100
150
200
250
300
350
400
-2
0
2
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.7
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.8
0
50
100
150
200
250
300
350
400
-2
0
2
a=0.9
a=0.55
Fig. B.23 Fractional Fourier transforms of a rectangular signal, using different values of the expo-
nent (Fig. 7.74)
Fig. B.24 The fractional
Fourier transform of the
rectangle becomes close to
the sinc signal for a = 0.99
(Fig. 7.75)
180
190
200
210
220
230
240
250
260
-4
-2
0
2
4
6
8

588
Appendix B: Long Programs
Program B.17 Fractional Fourier transform of a rectangle signal
% Fractional
Fourier
transform
using
decomposition
% Study
for
a
set
of
exponents
%
the
signal
to
be
transformed -----------------------
% rectangular
signal
y=[ zeros (70 ,1); ones (301 ,1); zeros (70 ,1)];
Ny= length (y);
% odd
length
ry= zeros (5, Ny );
% room
for
outputs
for
nn =1:5 ,
% choose
parameter
a
( fractional
power )
0.5 <a <1 .5
if
nn ==1 ,
a=0 .55;
end;
if
nn ==2 ,
a=0 .7;
end;
if
nn ==3 ,
a=0 .8;
end;
if
nn ==4 ,
a=0 .9;
end;
if
nn ==5 ,
a=0 .99;
end;
alpha =a*pi /2;
% sinc
interpolation
for
doubling
signal
data
zy= zeros (2* Ny -1 ,1);
zy (1:2:2* Ny -1)= y;
aux1 =zy (1:2* Ny -1);
aux2 = sinc ([ -(2*Ny -3):(2* Ny -3)] '/2);
m= length ([ aux1 (:); aux2 (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
yitp = ifft ( fft(aux1 ,P).* fft(aux2 ,P ));
yitp = yitp (1:m);
yitp = yitp (2*Ny -2 :end -2* Ny +3);
% interpolated
signal
% sandwich
zz= zeros (Ny -1 ,1);
ys =[ zz;
yitp ;
zz ];
%
the
fractional
transform ----------------------
% chirp
premultiplication
htan = tan( alpha /2);
aex =( pi/Ny )*( htan /4)*(( -2* Ny +2:2* Ny -2)'. ^2);
chr=exp(-j*aex );
yc= chr. *ys;
% premultiplied
signal
% chirp
convolution
sa= sin( alpha );
cc=pi/Ny/sa /4;
aux1 = exp(j*cc *( -(4*Ny -4):4* Ny -4)'. ^2);
m= length ([ aux1 (:); yc (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
ym= ifft ( fft(aux1 ,P).* fft(yc ,P ));
ym=ym (1:m);
ym=ym (4*Ny -3:8* Ny -7)* sqrt (cc/pi );
% convolved
signal
% chirp
post
multiplication
yq= chr. *ym;
% normalization
yp= exp(-j*(1 -a)* pi /4)* yq(Ny :2 :end -Ny +1);
% result
recording
aux= real (yp );
ry(nn ,:)= aux (:)';
end;
%
display -------------------------------------
figure (1)

Appendix B: Long Programs
589
subplot (5 ,1 ,1)
plot (y,'k');
axis ([0
Ny
-0.1
1.1 ]);
title ('a
rectangular
signal');
subplot (5 ,1 ,2)
plot (ry (1 ,:) ,'k');
axis ([0
Ny
-3
3]);
title ('Fractional
Fourier
transform
(a=0 .55)');
subplot (5 ,1 ,3)
plot (ry (2 ,:) ,'k');
axis ([0
Ny
-3
3]);
title ('a=0 .7');
subplot (5 ,1 ,4)
plot (ry (3 ,:) ,'k'); axis ([0
Ny
-3
3]);
title ('a=0 .8');
subplot (5 ,1 ,5)
plot (ry (4 ,:) ,'k'); axis ([0
Ny
-3
3]);
title ('a=0 .9');
figure (2)
plot (ry (5 ,:) ,'k');
axis ([180
Ny -180
-4
9]);
title ('Fractional
Fourier
transform
(a=0 .99)');
Wigner analysis of the fractional Fourier transform results (for the rectangular
signal) (Fig.B.25).
a=0.55
100
200
300
400
50
100
150
200
250
a=0.7
100
200
300
400
50
100
150
200
250
a=0.8
100
200
300
400
50
100
150
200
250
a=0.9
100
200
300
400
50
100
150
200
250
Fig. B.25 Wigner analysis (Fig. 7.6)

590
Appendix B: Long Programs
Program B.18 Wigner analysis of FFR of rectangle signal
% Fractional
Fourier
transform
% using
decomposition
%
the
signal
to
be
transformed -----------------------
% rectangular
signal
y=[ zeros (70 ,1); ones (301 ,1); zeros (70 ,1)];
Ny= length (y);
% odd
length
ry= zeros (4,Ny ,Ny );
% room
for
outputs
for
nn =1:4 ,
% choose
parameter
a
( fractional
power )
0.5 <a <1 .5
if
nn ==1 ,
a=0 .55;
end;
if
nn ==2 ,
a=0 .7;
end;
if
nn ==3 ,
a=0 .8;
end;
if
nn ==4 ,
a=0 .9;
end;
alpha =a*pi /2;
% sinc
interpolation
for
doubling
signal
data
zy= zeros (2* Ny -1 ,1);
zy (1:2:2* Ny -1)= y;
aux1 =zy (1:2* Ny -1);
aux2 = sinc ([ -(2*Ny -3):(2* Ny -3)] '/2);
m= length ([ aux1 (:); aux2 (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
yitp = ifft ( fft(aux1 ,P).* fft(aux2 ,P ));
yitp = yitp (1:m);
yitp = yitp (2*Ny -2 :end -2* Ny +3);
% interpolated
signal
% sandwich
zz= zeros (Ny -1 ,1);
ys =[ zz;
yitp ;
zz ];
%
the
fractional
transform ------------------------
% chirp
premultiplication
htan = tan( alpha /2);
aex =( pi/Ny )*( htan /4)*(( -2* Ny +2:2* Ny -2)'. ^2);
chr=exp(-j*aex );
yc= chr. *ys;
% premultiplied
signal
% chirp
convolution
sa= sin( alpha );
cc=pi/Ny/sa /4;
aux1 = exp(j*cc *( -(4*Ny -4):4* Ny -4)'. ^2);
m= length ([ aux1 (:); yc (:)]) -1;
P =2^ nextpow2 (m);
% convolution
using
fft:
ym= ifft ( fft(aux1 ,P).* fft(yc ,P ));
ym=ym (1:m);
ym=ym (4*Ny -3:8* Ny -7)* sqrt (cc/pi );
% convolved
signal
% chirp
post
multiplication
yq= chr. *ym;
% normalization
yp= exp(-j*(1 -a)* pi /4)* yq(Ny :2 :end -Ny +1);
% Wigner
analysis
yh= hilbert (yp );
zerx = zeros (Ny ,1);
aux= zerx ;
lm =(Ny -1)/2;
zyz =[ zerx ;
yh;
zerx ];
% sandwich
zeros - signal - zeros
% space
for
the
Wigner
distribution ,
a
matrix :
WD= zeros (Ny ,Ny );

Appendix B: Long Programs
591
mtau =0: lm;
% vector ( used
for
indexes )
for
nt =1:Ny ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux (1: lm +1)=( zyz( tpos ).* conj ( zyz( tneg )));
aux (1)=0 .5*aux (1);
% will
be
added
2
times
fo= fft(aux ,Ny )/ Ny;
%a
column
( harmonics
at
time
nt ):
WD (:, nt )=2* real (fo );
end
%
using
a
threshold
for
interference
attenuation
msx =( abs(WD )>0 .14 );
% matrix
of
0
or
1
entries
fWD=WD.* msx;
% select
SAF
entries
over
threshold
ry(nn ,: ,:)= fWD;
% for
figure
1
end;
%
display -------------------------------------
aux= zeros (Ny ,Ny );
k =10;
hh =1: Ny;
vv =1:Ny -150;
figure (1)
subplot (2 ,2 ,1)
aux (: ,:)= ry (1 ,: ,:);
imagesc ( log10 (k+ abs( aux(vv ,hh ))));
axis
xy;
title ('a=0 .55');
subplot (2 ,2 ,2)
aux (: ,:)= ry (2 ,: ,:);
imagesc ( log10 (k+ abs( aux(vv ,hh ))));
axis
xy;
title ('a=0 .7');
subplot (2 ,2 ,3)
aux (: ,:)= ry (3 ,: ,:);
imagesc ( log10 (k+ abs( aux(vv ,hh ))));
axis
xy;
title ('a=0 .8');
subplot (2 ,2 ,4)
aux (: ,:)= ry (4 ,: ,:);
imagesc ( log10 (k+ abs( aux(vv ,hh ))));
axis
xy;
title ('a=0 .9');
B.5.10
Filtered Wigner Analysis of Nature Chirps (7.11.2.)
Bat chirps (biosonar) (Fig.B.26).
Program B.19 Filtered (mask) WD of Bat signal
%
Filtered
( mask )
WD
of
Bat
signal
%
the
signal
[yin ,fs ]= wavread ('bat1.wav');
% read
wav
file
yo= yin (3900:8500);
% select
the
part
with
sound
tiv =1/ fs;
fN=fs /2;
% Nyquist
freq.
%
force
odd
length
aux=mod( length (yo ) ,2);
if
aux ==0 ,
yo=yo (1:( end -1));
end;
y= hilbert (yo );
Ny= length (y);
t =0: tiv :(Ny -1)* tiv;

592
Appendix B: Long Programs
Fig. B.26 Bat chirp
(Fig. 7.7)
seconds
Hz
0
0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1
0
0.5
1
1.5
2
x 104
%SAF -----------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
%A
simple
box
distribution
kernel
FI= zeros (Ny ,Ny );
% window
vertical
and
horizontal
1/2
width :
HV =100;
HH =200;
FI(md -HH:md+HH ,md -HV:md+HV )=1;
% box
kernel
% Product
of
kernel
and
SAF
fsaf = FI.* SAF;
pks= ifftshift ( fsaf );
% intermediate
variable
ax =(( ifft (pks ,[] ,1)));
% Wigner
from
SAF
distribution :
WD= real (( fft(ax ,[] ,2))');
% display ----------------------------------------
figure (1)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 ( abs(WD )) ,[ -6
0]);
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Filtered
Wigner
distrib.
of
the
Bat
signal');

Appendix B: Long Programs
593
Fig. B.27 Bird tweet
(Fig. 7.78)
seconds
Hz
0
0.05
0.1
0.15
0
500
1000
1500
2000
2500
3000
3500
Bird tweet (Fig.B.27).
Program B.20 Filtered (mask) WD of Bird signal
%
Filtered
( mask )
WD
of
Bird
signal
%
the
signal
[yo ,fs ]= wavread ('bird.wav');
% read
wav
file
tiv =1/ fs;
fN=fs /2;
% Nyquist
freq.
%
force
odd
length
aux=mod( length (yo ) ,2);
if
aux ==0 ,
yo=yo (1:( end -1));
end;
y= hilbert (yo );
Ny= length (y);
t =0: tiv :(Ny -1)* tiv;
%SAF ----------------------------------------------
zerx = zeros (Ny ,1);
%a
vector
zyz =[ zerx ;
y;
zerx ];
% sandwich
zeros - signal - zeros
aux= zerx ;
SAF= zeros (Ny ,
Ny );
% space
for
the
SAF ,
a
matrix
nt =1: Ny;
% vector
( used
for
indexes )
md =(Ny -1)/2;
for
mtau =-md:md ,
tpos =Ny+nt+ mtau ;
%a
vector
tneg =Ny+nt - mtau ;
%a
vector
aux=zyz( tpos ).* conj ( zyz( tneg ));
%a
column
( frequencies ):
SAF (:, md+ mtau +1)= fftshift (fft(aux ,Ny )/ Ny );
end
%A
simple
box
distribution
kernel
FI= zeros (Ny ,Ny );
% window
vertical
and
horizontal
1/2
width :
HV =100;
HH =200;
FI(md -HH:md+HH ,md -HV:md+HV )=1;
% box
kernel
% Product
of
kernel
and
SAF
fsaf = FI.* SAF;
pks= ifftshift ( fsaf );
% intermediate
variable

594
Appendix B: Long Programs
ax =(( ifft (pks ,[] ,1)));
% Wigner
from
SAF
distribution :
WD= real (( fft(ax ,[] ,2))');
% display ------------------------------------------
figure (1)
fiv=fN/Ny;
% frequency
interval
f =0: fiv :(fN - fiv );
% frequency
intervals
set
colmap1 ;
colormap ( mapg1 );
% user
colormap
imagesc (t,f, log10 ( abs(WD )) ,[ -3
0]);
axis
xy;
xlabel ('seconds');
ylabel ('Hz');
title ('Filtered
Wigner
distrib.
of
the
Bird
signal');
B.5.11
Wavelet Analysis of Lung and Heart Sounds (7.11.3.)
Normal respiration (Figs.B.28 and B.29).
Program B.21 Signal analysis by Morlet continuous wavelet transform
%
Signal
analysis
by
continuous
wavelet
transform
%
Morlet
Wavelet
%
Lung
study :
normal
%
the
signal
[yin , fin ]= wavread ('bronchial.wav');
% read
wav
file
yo= yin (: ,1);
% one
of
the
2
stereo
channels
ndc =5;
% decimation
value
yo=yo (1: ndc :end );
% signal
decimation
fs= fin/ ndc;
wy =2* pi*fs;
% signal
frequency
in
rad/s
Ts =1/ fs;
% time
interval
between
samples ;
%
plot
preparation
L= length (yo );
Fig. B.28 Sound of normal
respiration (10s) (Fig. 7.79)
0
1
2
3
4
5
6
7
8
9
10
-1
-0.8
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
1
time

Appendix B: Long Programs
595
Fig. B.29 Scalogram of a
signal segment (Fig. 7.80)
0
0.5
1
1.5
2
-0.5
0
0.5
sec
signal
samples
scales
500 1000 1500 2000 2500 3000 3500 4000 4500
5
10
15
20
25
30
to =0: Ts :((L -1)* Ts );
% extract
signal
segment -----------------------------
ti =0;
% initial
time
of
signal
segment
( sec)
duy =2 .2;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
ND= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+ND -1));
% the
signal
segment
% CWT
algorithm --------------------------------------
CC= zeros (30 , ND );
%
CWT
nn =1: ND;
for
ee =1:30 ,
s=ee *0 .004 ;
% scales
for
rr =1:ND ,
% delays
a=Ts *(rr -1);
val =0;
% vectorized
part
t=Ts *(nn -1);
x=(t-a)/s;
% plug
coeffs.
% wavelet :
psi =(1/ sqrt (s ))*( exp (-( x. ^2)/2) .* cos (5*x ));
for
j =1:ND ,
val= val +(y(j).*psi(j ));
end;
CC(ee ,rr )= val;
end;
end;
% display -----------------------------------------
figure (1)
plot (to ,yo ,'k')
axis ([0
10
-1
1]);
title ('Complete
respiration
signal')
xlabel ('time');
figure
(2)

596
Appendix B: Long Programs
0
1
2
3
4
5
6
7
8
9
10
-0.5
0
0.5
1
time
Fig. B.30 Sound of respiration with crackles (10 s) (Fig. 7.81)
subplot (2 ,1 ,1)
plot (tsg ,y,'k');
axis ([ ti
ti+ duy
min(y)-0 .1
max(y)+0 .1 ]);
xlabel ('sec');
ylabel ('signal');
title ('Respiration
signal ,
a
segment');
subplot (2 ,1 ,2)
imagesc (CC );
colormap ('jet');
title ('wavelet
analysis')
xlabel ('samples');
ylabel ('scales');
% sound
soundsc (y,fs );
Respiration with crackles (Figs.B.30 and B.31).
Program B.22 Signal analysis by Morlet continuous wavelet transform
%
Signal
analysis
by
continuous
wavelet
transform
%
Morlet
Wavelet
%
Lung
study :
crackles
%
the
signal
[yin , fin ]= wavread ('crackles.wav');
% read
wav
file
yo= yin (: ,1);
% one
of
the
2
stereo
channels
ndc =5;
% decimation
value
yo=yo (1: ndc :end );
% signal
decimation
fs= fin/ ndc;
wy =2* pi*fs;
% signal
frequency
in
rad/s
Ts =1/ fs;
% time
interval
between
samples ;
%
plot
preparation
L= length (yo );
to =0: Ts :((L -1)* Ts );
% extract
signal
segment -----------------------------
ti =7 .8;
% initial
time
of
signal
segment
( sec)

Appendix B: Long Programs
597
7.8
8
8.2
8.4
8.6
8.8
9
9.2
9.4
-0.5
0
0.5
1
sec
signal
samples
scales
500
1000
1500
2000
2500
3000
3500
10
20
30
Fig. B.31 Scalogram of a signal segment (Fig. 7.82)
duy =1 .6;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
ND= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+ND -1));
% the
signal
segment
% CWT
algorithm --------------------------------------
CC= zeros (30 , ND );
%
CWT
nn =1: ND;
for
ee =1:30 ,
s=ee *0 .004 ;
% scales
for
rr =1:ND ,
% delays
a=Ts *(rr -1);
val =0;
% vectorized
part
t=Ts *(nn -1);
x=(t-a)/s;
% plug
coeffs.
% wavelet :
psi =(1/ sqrt (s ))*( exp (-( x. ^2)/2) .* cos (5*x ));
for
j =1:ND ,
val= val +(y(j).* psi(j ));
end;
CC(ee ,rr )= val;
end;
end;
% display -----------------------------------------
figure (1)
m=L /2;
% only
the
first
half
of
the
signal
(10
sec. )
plot (to (1:m),yo (1:m),'k')
title ('Complete
respiration
signal')
xlabel ('time');

598
Appendix B: Long Programs
0
0.5
1
1.5
-1
-0.5
0
0.5
1
sec
signal
samples
scales
1000
2000
3000
4000
5000
6000
5
10
15
20
25
Fig. B.32 Scalogram of heart sound (2 beats) (Fig. 7.83)
figure
(2)
subplot (2 ,1 ,1)
plot (tsg ,y,'k');
axis ([ ti
ti+ duy
min(y)-0 .1
max(y)+0 .1 ]);
xlabel ('sec');
ylabel ('signal');
title ('Respiration
signal ,
a
segment');
subplot (2 ,1 ,2)
imagesc (CC );
colormap ('jet');
title ('wavelet
analysis')
xlabel ('samples');
ylabel ('scales');
% sound
soundsc (y,fs );
Heart sound (Fig.B.32).
Program B.23 Signal analysis by Morlet continuous wavelet transform
%
Signal
analysis
by
continuous
wavelet
transform
%
Morlet
Wavelet
%
Heart
sound :
normal
%
the
signal
[yin , fin ]= wavread ('heart1.wav');
% read
wav
file
ndc =5;
% decimation
value
yo= yin (1: ndc :end );
% signal
decimation
fs= fin/ ndc;
wy =2* pi*fs;
% signal
frequency
in
rad/s
Ts =1/ fs;
% time
interval
between
samples ;
%
plot
preparation
L= length (yo );
to =0: Ts :((L -1)* Ts );

Appendix B: Long Programs
599
% extract
signal
segment -----------------------------
ti =0;
% initial
time
of
signal
segment
( sec)
duy =1 .5;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
ND= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+ND -1));
% the
signal
segment
% CWT
algorithm --------------------------------------
CC= zeros (25 , ND );
%
CWT
nn =1: ND;
for
ee =1:25 ,
s=ee *0 .0005 ;
% scales
for
rr =1:ND ,
% delays
a=Ts *(rr -1);
val =0;
% vectorized
part
t=Ts *(nn -1);
x=(t-a)/s;
% plug
coeffs.
% wavelet :
psi =(1/ sqrt (s ))*( exp (-( x. ^2)/2) .* cos (5*x ));
for
j =1:ND ,
val= val +(y(j).*psi(j ));
end;
CC(ee ,rr )= val;
end;
end;
% display -----------------------------------------
figure
(1)
subplot (2 ,1 ,1)
plot (tsg ,y,'k');
axis ([ ti
ti+ duy
min(y)-0 .1
max(y)+0 .1 ]);
xlabel ('sec');
ylabel ('signal');
title ('Respiration
signal ,
a
segment');
subplot (2 ,1 ,2)
imagesc (CC );
colormap ('jet');
title ('wavelet
analysis')
xlabel ('samples');
ylabel ('scales');
% sound
soundsc (y,fs );
B.5.12
Fan-Chirp Transform of Animal Songs (7.11.4.)
Duck quack (Figs.B.33 and B.34).

600
Appendix B: Long Programs
Fig. B.33 Spectrogram of
the quack (Fig. 7.84)
Time
Frequency
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
1000
2000
3000
4000
5000
Fig. B.34 Fan-Chirp
transform of the quack
(Fig. 7.85)
Time
Frequency
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0
1000
2000
3000
4000
5000
Program B.24 Short Time Fan-Chirp transform
%
Short
Time
Fan - Chirp
transform
(as
a
spectrogram )
% example
with
quack
%
the
signal
[y,fs ]= wavread ('duck_quack.wav');
% read
wav
file
%
transform
parameters
nft =256;
% FFT
length
Ny= length (y);
% signal
length ,
even
nw =97;
% window
size ,
odd
nov =90;
% overlapping
mo=nw -nov;
% must
be
positive
win= hamming (nw );
% window
win= win. / sum(win );
%
reference
estimated
frequencies
tr =0:0 .0002 :0 .2;
fr =40* tr;

Appendix B: Long Programs
601
Fc=fs *(0: nft /2)/ nft;
% frequency
centers
%
time
centers :
Tc =[];
idx =1;
kk =1;
while
idx (end) <=Ny ,
idx =((kk -1)* mo )+(1: nw );
if
idx (end) <=Ny ,
Tc =[ Tc
( idx ((nw -1)/2+1) -1)/ fs ];
end
kk=kk +1;
end
%
interpolated
estimated
frequencies
fe= interp1 (tr ,fr ,Tc ,'linear','extrap');
nTc= length (Tc );
aux= ones (nTc ,1);
FT= zeros ( nft /2+1 , nTc );
for
kk =1: nTc ,
idx =((kk -1)*( mo ))+(1: nw );
%
derivative
approximation :
if
kk ==1
||
kk == nTc ,
A =0;
else
A =(1/ fe(kk ))+( fe(kk +1) - fe(kk -1))/ ...
(Tc(kk +1) - Tc(kk -1));
end
A=A/fs;
aslp (kk )=A;
%
the
FCh
transform
for
the
signal
frame :
z=y(idx).* win;
% signal
windowed
segment
M= length (z);
N=nft;
ks =(-N /2+1: N/2)';
ks =[ ks(N/2 :end );
ks (1:N /2 -1)];
nn =-(M -1)/2:(M -1)/2;
aux =( -2* pi/N).*ks *((1+0 .5*A*nn).*nn );
E=exp(j* aux );
q= ones (N ,1);
FTx=sum(q*(z'.* sqrt ( abs ((1+ A*nn )))) .*E ,2);
FT (:, kk )= FTx (1 :end /2+1);
end
%
display --------------------------------------------
figure (1)
specgram (y,nft ,fs );
title ('spectrogram
of
duck
quack')
figure (2)
imagesc (Tc ,Fc ,20* log10 ( abs(FT )) ,[ -40
-1]);
axis
xy;
title ('Fan - Chirp
transform
of
duck
quack')
xlabel ('Time');
ylabel ('Frequency');
Dog bark (Figs.B.35 and B.36).

602
Appendix B: Long Programs
Fig. B.35 Spectrogram of
the dog bark (Fig. 7.86)
Time
Frequency
0.05 0.1
0.15 0.2
0.25 0.3
0.35 0.4
0.45
0
2000
4000
6000
8000
10000
Fig. B.36 Fan-Chirp
transform of the dog bark
(Fig. 7.87)
Time
Frequency
0.05
0.1
0.15
0.2
0.25 0.3
0.35 0.4
0.45
0
2000
4000
6000
8000
10000
Program B.25 Short Time Fan-Chirp transform
%
Short
Time
Fan - Chirp
transform
(as
a
spectrogram )
% example
with
dog
bark
%
the
signal
[y,fs ]= wavread ('dog1.wav');
% read
wav
file
%
transform
parameters
nft =256;
% FFT
length
Ny= length (y);
% signal
length ,
even
nw =137;
% window
size ,
odd
nov =110;
% overlapping
mo=nw -nov;
% must
be
positive
win= hamming (nw );
% window
win= win. / sum(win );
%
reference
estimated
frequencies
tr =0:0 .0004 :0 .44;
fr =20* tr;

Appendix B: Long Programs
603
Fc=fs *(0: nft /2)/ nft;
% frequency
centers
%
time
centers :
Tc =[];
idx =1;
kk =1;
while
idx (end) <=Ny ,
idx =((kk -1)* mo )+(1: nw );
if
idx (end) <=Ny ,
Tc =[ Tc
( idx ((nw -1)/2+1) -1)/ fs ];
end
kk=kk +1;
end
%
interpolated
estimated
frequencies
fe= interp1 (tr ,fr ,Tc ,'linear','extrap');
nTc= length (Tc );
aux= ones (nTc ,1);
FT= zeros ( nft /2+1 , nTc );
for
kk =1: nTc ,
idx =((kk -1)*( mo ))+(1: nw );
%
derivative
approximation :
if
kk ==1
||
kk == nTc ,
A =0;
else
A =(1/ fe(kk ))+( fe(kk +1) - fe(kk -1))/ ...
(Tc(kk +1) - Tc(kk -1));
end
A=A/fs;
aslp (kk )=A;
%
the
FCh
transform
for
the
signal
frame :
z=y(idx).* win;
% signal
windowed
segment
M= length (z);
N=nft;
ks =(-N /2+1: N/2)';
ks =[ ks(N/2 :end );
ks (1:N /2 -1)];
nn =-(M -1)/2:(M -1)/2;
aux =( -2* pi/N).*ks *((1+0 .5*A*nn).*nn );
E=exp(j* aux );
q= ones (N ,1);
FTx=sum(q*(z'.* sqrt ( abs ((1+ A*nn )))) .*E ,2);
FT (:, kk )= FTx (1 :end /2+1);
end
%
display --------------------------------------------
figure (1)
specgram (y,nft ,fs );
title ('spectrogram
of
dog
bark')
figure (2)
imagesc (Tc ,Fc ,20* log10 ( abs(FT )) ,[ -50
0]);
axis
xy;
title ('Fan - Chirp
transform
of
dog
bark')
xlabel ('Time');
ylabel ('Frequency');
soundsc (y,fs );
B.5.13
Modiﬁed S-Transform Analysis of Some Cases
(7.11.5.)
Respiration with wheezing (Figs.B.37, B.38 and B.39).

604
Appendix B: Long Programs
0
1
2
3
4
5
6
7
8
9
10
-1
0
1
Respiration signal, complete
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
-0.5
0
0.5
selected signal segment
1.56
1.58
1.6
1.62
1.64
1.66
1.68
1.7
1.72
1.74
1.76
-0.1
0
0.1
zoom on wheezing part
Fig. B.37 Respiration with wheezing, 3 levels of detail (Fig. 7.88)
Fig. B.38 Spectrogram of
the signal segment with
wheezing (Fig. 7.89)
Time
Frequency
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0
200
400
600
800
1000
1200
Program B.26 Modiﬁed S-transform
%
Modified
S- transform.
Lung
sound :
wheezing
%
the
signal
[yin , fin ]= wavread ('wheezing.wav');
% read
wav
file
ndc =4;
% decimation
value
yo= yin (1: ndc :end );
% signal
decimation
No= length (yo );
fs= fin/ ndc;
wy =2* pi*fs;
% signal
frequency
in
rad/s
Ts =1/ fs;
% time
interval
between
samples ;

Appendix B: Long Programs
605
Fig. B.39 Modiﬁed
S-transform of the signal
segment with wheezing
(Fig. 7.90)
Time
Frequency
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
1.9
0
200
400
600
800
1000
1200
to =0: Ts :(No -1)* Ts;
% extract
signal
segment -----------------------------
ti =1 .1;
% initial
time
of
signal
segment
( sec)
duy =0 .8;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
aux= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+aux -1))';
% the
signal
segment
( transpose )
% force
even
length
if
mod(aux ,2) >0 ,
y=y(1 :end -1);
tsg=tsg (1 :end -1);
end;
Ny= length (y);
% length
of
signal
segment
m=Ny /2;
%
The
transform -------------------------------------
%
preparation :
f =[0: m
-m +1: -1]/ Ny;
% frequencies
vector
S=fft(y);
% signal
spectrum
%
Form
a
matrix
of
Gaussians
( freq.
domain )
q =[1./f (2:m+1)]';
k =1+(5* abs(f ));
W =2* pi* repmat (f,m ,1).* repmat (q ,1, Ny );
for
nn =1:m,
W(nn ,:)= k(nn )*W(nn ,:);
% modified
S- transform
end
MG= exp ((- W. ^2)/2);
%
the
matrix
of
Gaussians
%
Form
a
matrix
with
shifted
FFTs
Ss= toeplitz (S(1:m+1)',S);
Ss =[ Ss (2:m +1 ,:)];
% remove
first
row
( freq.
zero )
%
S- transform
ST= ifft ( Ss.*MG ,[] ,2);
st0= mean (y)* ones (1, Ny );
% zero
freq.
row
ST =[ st0;ST ];
% add
zero
freq.
row
%
display
-------------------------------------------
figure (1)
subplot (3 ,1 ,1)

606
Appendix B: Long Programs
plot (to ,yo ,'k')
axis ([0
10
-1.1
1.1 ]);
title ('Respiration
signal ,
complete')
subplot (3 ,1 ,2)
plot (tsg ,y,'k');
title ('selected
signal
segment')
subplot (3 ,1 ,3)
% signal
specific
( edit ):
plot ( tsg (1300:1800) , y (1300:1800) ,'k');
axis ([1 .56
1 .76
-0 .12
0 .12 ]);
title ('zoom
on
wheezing
part')
figure (2)
specgram (y ,256 , fs );
title ('Respiration
signal ,a
segment');
figure (3)
Sf =0:(2* fs/Ny ):( fs /2);
imagesc (tsg ,Sf ,20* log10 ( abs(ST )) ,[ -70
0]);
axis
xy;
title ('S- transform
of
the
signal
segment');
xlabel ('Time');
ylabel ('Frequency');
Whale song (Figs.B.40 and B.41).
Time
Frequency
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Time
Frequency
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Fig. B.40 Spectrogram of whale song (divided into 2 parts) (Fig. 7.91)
Time
Frequency
0
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
0
200
400
600
800
1000
1200
1400
1600
1800
Time
Frequency
2.2 2.4 2.6 2.8
3
3.2 3.4 3.6 3.8
4
0
200
400
600
800
1000
1200
1400
1600
1800
Fig. B.41 Modiﬁed S-transform of whale song (divided into 2 parts) (Fig. 7.92)

Appendix B: Long Programs
607
Program B.27 Modiﬁed S-transform of Whale signal
%
Modified
S- transform
of
Whale
signal
%
the
signal
[yin , fin ]= wavread ('whale1.wav');
% read
wav
file
ndc =6;
% decimation
value
yo= yin (1: ndc :end );
% signal
decimation
fs= fin/ ndc;
Ts =1/ fs;
% time
interval
between
samples ;
% extract
signal
segment -----------------------------
ti =2 .1;
% initial
time
of
signal
segment
( sec)
duy =2;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
aux= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+aux -1))';
% the
signal
segment
( transpose )
% force
even
length
if
mod(aux ,2) >0 ,
y=y(1 :end -1);
tsg=tsg (1 :end -1);
end;
Ny= length (y);
% length
of
signal
segment
m=Ny /2;
%
The
transform -------------------------------------
%
preparation :
f =[0: m
-m +1: -1]/ Ny;
% frequencies
vector
S=fft(y);
% signal
spectrum
%
Form
a
matrix
of
Gaussians
( freq.
domain )
q =[1./f (2:m+1)]';
k =1+(20* abs(f));
W =2* pi* repmat (f,m ,1).* repmat (q ,1, Ny );
for
nn =1:m,
W(nn ,:)= k(nn )*W(nn ,:);
% modified
S- transform
end
MG= exp ((- W. ^2)/2);
%
the
matrix
of
Gaussians
%
Form
a
matrix
with
shifted
FFTs
Ss= toeplitz (S(1:m+1)',S);
Ss =[ Ss (2:m +1 ,:)];
% remove
first
row
( freq.
zero )
%
S- transform
ST= ifft ( Ss.*MG ,[] ,2);
st0= mean (y)* ones (1, Ny );
% zero
freq.
row
ST =[ st0;ST ];
% add
zero
freq.
row
%
display
-------------------------------------------
figure (1)
specgram (y ,256 , fs );
title ('Whale
signal ,a
segment');
figure (2)
Sf =0:(2* fs/Ny ):( fs /2);
imagesc (tsg ,Sf ,20* log10 ( abs(ST )) ,[ -60
0]);
axis
xy;
% set(gca ,'Ydir ','Normal ');
title ('S- transform
of
the
signal
segment');
xlabel ('Time');
ylabel ('Frequency');
soundsc (yo ,fs );
El Centro earthquake (Figs.B.42, B.43 and B.44).

608
Appendix B: Long Programs
0
2
4
6
8
10
12
14
16
18
-4
-2
0
2
4
sec
0
2
4
6
8
10
12
-5
0
5
sec
Fig. B.42 The Earthquake signal at two detail levels (Fig. 7.93)
Fig. B.43 Spectrogram of
the signal segment
(Fig. 7.94)
Time
Frequency
1
2
3
4
5
6
7
8
9
10
11
0
10
20
30
40
50
60
70
80
90
100
Program B.28 Modiﬁed S-transform (Earthquake)
%
Modified
S- transform
%
taking
an
Earthquake
SAC
file
% read
SAC
file -----------------------------------
F= fopen ('1010211753 osor.sac', 'r','ieee -be');
if
(F== -1)
disp ('file
access
error');
pause

Appendix B: Long Programs
609
Fig. B.44 Modiﬁed
S-transform of the signal
segment (Fig. 7.95)
Time
Frequency
0
2
4
6
8
10
12
0
10
20
30
40
50
60
70
80
90
100
end
%
read
header :
head1 = fread (F,
[5,
14] , 'float32');
head2 = fread (F,
[5,
8], 'int32');
head3 = fread (F,
[24 ,
8], 'char');
%
read
data :
yin= fread (F,'single');
% read
signal
data
fclose (F);
Tin =0 .01* head1 (1);
% sampling
period
( sec)
fin =1/ Tin;
% preparation
for
analysis -------------------------
ndc =3;
yd= yin (1: ndc :end );
%
decimate
by
ndc
fs= fin/ ndc;
Ts=Tin*ndc;
yo =( yd /100)';
% the
signal
No= length (yo );
wy =2* pi*fs;
% signal
frequency
in
rad/s
to =0: Ts :(No -1)* Ts;
% extract
signal
segment -----------------------------
ti =0;
% initial
time
of
signal
segment
( sec)
duy =12;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
aux= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+aux -1))';
% the
signal
segment
( transpose )
% force
even
length
if
mod(aux ,2) >0 ,
y=y(1 :end -1);
tsg=tsg (1 :end -1);
end;
Ny= length (y);
% length
of
signal
segment
m=Ny /2;
%
The
transform -------------------------------------
%
preparation :
f =[0: m
-m +1: -1]/ Ny;
% frequencies
vector
S=fft(y);
% signal
spectrum

610
Appendix B: Long Programs
%
Form
a
matrix
of
Gaussians
( freq.
domain )
q =[1./f (2:m+1)]';
k =1+(5* abs(f ));
W =2* pi* repmat (f,m ,1).* repmat (q ,1, Ny );
for
nn =1:m,
W(nn ,:)= k(nn )*W(nn ,:);
% modified
S- transform
end
MG= exp ((- W. ^2)/2);
%
the
matrix
of
Gaussians
%
Form
a
matrix
with
shifted
FFTs
Ss= toeplitz (S(1:m+1)',S);
Ss =[ Ss (2:m +1 ,:)];
% remove
first
row
( freq.
zero )
%
S- transform
ST= ifft ( Ss.*MG ,[] ,2);
st0= mean (y)* ones (1, Ny );
% zero
freq.
row
ST =[ st0;ST ];
% add
zero
freq.
row
%
display
-------------------------------------------
figure (1)
subplot (2 ,1 ,1)
plot (to ,yo ,'k')
axis ([0
to (end)
-4
4]);
title ('Earthquake
signal ,
complete')
xlabel ('sec')
subplot (2 ,1 ,2)
plot (tsg ,y,'k');
xlabel ('sec')
title ('selected
signal
segment')
figure (2)
specgram (y ,64 , fs );
title ('Earthquake
signal ,a
segment');
figure (3)
Sf =0:(2* fs/Ny ):( fs /2);
imagesc (tsg ,Sf ,20* log10 ( abs(ST )) ,[ -40
-10]);
axis
xy;
% set(gca ,'Ydir ','Normal ');
title ('S- transform
of
the
signal
segment');
xlabel ('Time');
ylabel ('Frequency');
Extraction of a zone of interest (Figs.B.45 and B.46).
Program B.29 Inversion of Modiﬁed S-transform (Earthquake)
%
Inversion
of
Modified
S- transform
%
taking
an
Earthquake
SAC
file
%
and
using
a
mask
to
extract
a
seismic
wave
% read
SAC
file -----------------------------------
F= fopen ('1010211753 osor.sac', 'r','ieee -be');
if
(F== -1)
disp ('file
access
error');
pause
end
%
read
header :
head1 = fread (F,
[5,
14] , 'float32');
head2 = fread (F,
[5,
8], 'int32');
head3 = fread (F,
[24 ,
8], 'char');
%
read
data :
yin= fread (F,'single');
% read
signal
data
fclose (F);
Tin =0 .01* head1 (1);
% sampling
period
( sec)

Appendix B: Long Programs
611
500
1000
1500
2000
200
400
600
800
1000
1200
Fig. B.45 Extraction of a TF region of interest (Fig. 7.96)
0
500
1000
1500
2000
2500
-1
-0.5
0
0.5
1
800
900
1000
1100
1200
1300
1400
-1
-0.5
0
0.5
1
Fig. B.46 The extracted signal segment at two levels of detail (Fig. 7.97)
fin =1/ Tin;
% preparation
for
analysis -------------------------
ndc =3;
yd= yin (1: ndc :end );
%
decimate
by
ndc
fs= fin/ ndc;
Ts=Tin*ndc;
yo =( yd /100)';
% the
signal
No= length (yo );
wy =2* pi*fs;
% signal
frequency
in
rad/s
to =0: Ts :(No -1)* Ts;
% extract
signal
segment -----------------------------
ti =0;
% initial
time
of
signal
segment
( sec)

612
Appendix B: Long Programs
duy =12;
% signal
segment
duration
( sec)
tsg=ti:Ts :( duy+ti );
% time
intervals
set
Ni =1+( ti*fs );
% number
of
the
initial
sample
aux= length ( tsg );
% how
many
samples
in
signal
segment
y=yo(Ni :( Ni+aux -1))';
% the
signal
segment
( transpose )
% force
even
length
if
mod(aux ,2) >0 ,
y=y(1 :end -1);
tsg=tsg (1 :end -1);
end;
Ny= length (y);
% length
of
signal
segment
m=Ny /2;
%
The
transform -------------------------------------
%
preparation :
f =[0: m
-m +1: -1]/ Ny;
% frequencies
vector
S=fft(y);
% signal
spectrum
%
Form
a
matrix
of
Gaussians
( freq.
domain )
q =[1./f (2:m+1)]';
k =1+(5* abs(f ));
W =2* pi* repmat (f,m ,1).* repmat (q ,1, Ny );
for
nn =1:m,
W(nn ,:)= k(nn )*W(nn ,:);
% modified
S- transform
end
MG= exp ((- W. ^2)/2);
%
the
matrix
of
Gaussians
%
Form
a
matrix
with
shifted
FFTs
Ss= toeplitz (S(1:m+1)',S);
Ss =[ Ss (2:m +1 ,:)];
% remove
first
row
( freq.
zero )
%
S- transform
ST= ifft ( Ss.*MG ,[] ,2);
st0= mean (y)* ones (1, Ny );
% zero
freq.
row
ST =[ st0;ST ];
% add
zero
freq.
row
%
creating
a
mask
MK= zeros (1+m,Ny );
MK (40:260 ,900:1300)=1;
%
extract
TF
region
with
the
mask
XR= ST.*MK;
%
inverse
S- transform
IS= zeros (1,m);
%
averaging
along
time
for
each
freq
for
nn =1:m,
IS(nn )= sum(XR(nn ,:));
end;
%
change
the
sign
of
imaginary
part
ISr= real (IS );
ISi= imag (IS );
ISi = -1* ISi;
IS= ISr +(i* ISi );
%
obtain
selected
signal
sy= real ( ifft (IS ));
%
resample
for
original
length
ry= resample (sy ,2 ,1);
%
display
-------------------------------------------
figure (1)
imagesc (20* log10 (abs(XR )) ,[ -40
-10]);
axis
xy;
title ('extracted
TF
region')
figure (2)
subplot (2 ,1 ,1)
plot (ry ,'k');

Appendix B: Long Programs
613
title ('the
selected
signal
segment')
subplot (2 ,1 ,2)
plot (ry ,'k');
axis ([800
1400
-1
1]);
title ('zoom
on
signal
segment');
B.6
Chapter 8: Modulation
B.6.1
Digital Modulation of Sine Signals (8.2.3.)
Example of digital modulation methods applied to a sine signal (Fig.B.47).
Program B.30 Pulse modulations of sine signal
%
Pulse
modulations
of
sine
signal
%
the
pulses
( bits ):
% the
modulating
signal
( bits ):
a =[0
1
0
1
1
0
1
0
0
1];
fa =40;
% signal
frequency
in
Hz
0
0.05
0.1
0.15
0.2
0.25
0
0.5
1
bits message
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
ASK
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
FSK
0
0.05
0.1
0.15
0.2
0.25
-1
0
1
PSK
seconds
Fig. B.47 ASK, FSK and PSK modulation of sine signal (Fig. 8.15)

614
Appendix B: Long Programs
wa =2* pi*fa;
% signal
frequency
in
rad/s
fc =6* fa;
% carrier
frequency
in
Hz
wc =2* pi*fc;
% carrier
frequency
in
rad/s
fs =10* fc;
% sampling
frequency
in
Hz
tiv =1/ fs;
% time
interval
between
samples ;
% sampling
the
modulating
signal
nsa=fs/fa;
% number
of
samples
per
bit
Nb= length (a);
% number
of
bits
in
a
% number
of
samples
in
the
complete
message :
nsmsg =Nb* nsa;
tmsg = nsmsg * tiv;
% time
for
the
message
t =0: tiv :( tmsg - tiv );
% time
intervals
set
% time
for
a
bit
tb =1/ fa;
t1 =0: tiv :(tb -tiv );
% carrier
signal
for
1
bit
time
c1= sin (2* pi*fc*t1 );
% modulating
signal
samples
us1= ones (1, nsa );
%a
vector
of
nsa
ones
as=a (1)* us1;
for
nn =2:Nb ,
as= cat (2,as ,a(nn )* us1 );
end
subplot (4 ,1 ,1)
plot (t,as ,'k');
% plots
the
modulating
signal
ylabel ('bits
message');
title ('digital
modulation
of
sine
signal');
axis ([0
0 .25
-0.2
1.2 ]);
% ASK
modulation
% modulated
signal
samples
ASKy =a (1)* c1;
% carrier
signal
for
first
bit
for
nn =2:Nb ,
ASKy = cat (2, ASKy ,a(nn )* c1 );
end
subplot (4 ,1 ,2)
plot (t,ASKy ,'k');
axis ([0
0 .25
-1.2
1.2 ]);
ylabel ('ASK');
% FSK
modulation
fc1 =4* fa;
fc2 =8* fa;
% two
fcarrier
frequencies
c1= sin (2* pi* fc1*t1 );
% carrier
for
bit =0
c2= sin (2* pi* fc2*t1 );
% carrier
for
bit =1
% modulated
signal
samples
% carrier
signal
for
the
first
bit:
if
a (1)==0 ,
FSKy =c1;
else
FSKy =c2;
end;
for
nn =2:Nb ,
if
a(nn )==0 ,
FSKy = cat (2, FSKy ,c1 );
else
FSKy = cat (2, FSKy ,c2 );
end;
end
subplot (4 ,1 ,3)
plot (t,FSKy ,'k');
axis ([0
0 .25
-1.2
1.2 ]);
ylabel ('FSK');
% PSK
modulation

Appendix B: Long Programs
615
pc1 =0;
pc2=pi;
% two
fcarrier
phases
c1= sin ((2* pi*fc*t1 )+ pc1 );
% carrier
for
bit =0
c2= sin ((2* pi*fc*t1 )+ pc2 );
% carrier
for
bit =1
% modulated
signal
samples
% carrier
signal
for
the
first
bit:
if
a (1)==0 ,
PSKy =c1;
else
PSKy =c2;
end;
for
nn =2:Nb ,
if
a(nn )==0 ,
PSKy = cat (2, PSKy ,c1 );
else
PSKy = cat (2, PSKy ,c2 );
end;
end
subplot (4 ,1 ,4)
plot (t,PSKy ,'k');
axis ([0
0 .25
-1.2
1.2 ]);
ylabel ('PSK');
xlabel ('seconds');

Index
A
ADSR envelope, 342
Afﬁne transformations, 405
Afﬁne-smoothed pseudo Wigner distribu-
tion (ASPWD), 416
Alias, 23, 25
AM modulation, 497
Ambiguity, 362
Angle modulation, 507
arburg(), 291, 292
arcov(), 291
ARIMA model, 172
ARMA model, 155, 158
ARMAX model, 158
armax( ), 175
armcov(), 291
aryule( ), 174
aryule(), 291
Autocorrelation, 40, 541
Autocovariance, 41, 345
Auto-regressive, AR, 155
Average power, 41
B
Backshift operator, 155
Balian-Low theorem, 370
Band-limited signal, 197
Band-pass ﬁlter, 191
Band-stop ﬁlter, 192
Bartlett window, 254
Basic time-frquency operators, 402
Bat chirps (biosonar, 470
Bayes’rule, 89
Bayesian networks, 94
Belief networks, 96
Bell 202 standard, 514
Bertrand distribution, 417
Bessel analog ﬁlter, 221
bessel(), 510
besself(), 222
Beta PDF, 50
betapdf(), 51
Bilinear form (operators), 407
Bilinear transformation, 241
bilinear(), 241
Binomial PDF, 57
binomial(), 57
Bi-orthonormal system, 364
Bird tweet, 471
Bit reversal, 543
Bivariate Gaussian PDF, 55
Biweight kernel, 70
Blackman window, 256
Bluetooth, 523
Bode plot, 122
Born-Jordan distribution, 414
Box and Müller method, 87
Boxcar windowed FIR ﬁlter, 267
boxcar(), 267
boxplot( ), 106
buttap(n), 220
butter(), 200, 282
Butterﬂy diagrams, 543
Butterworth analog ﬁlter, 199
buttord(), 220
C
Cauer forms, 195
Central limit theorem, 34, 87
Central moments, 39
Cepstral analysis, 321
Cepstrum of an AM modulated signal, 529
© Springer Science+Business Media Singapore 2017
J.M. Giron-Sierra, Digital Signal Processing with Matlab Examples, Volume 1,
Signals and Communication Technology, DOI 10.1007/978-981-10-2534-1
617

618
Index
Chabyshev analog ﬁlters, 204
Chapman-Kolmogorov equation, 92
Characteristic function, 42
cheb1ap(), 220
cheb1ord(), 220
cheb2ap(), 220
cheb2ord(), 220
chebwin(), 269
cheby1(), 207, 282
cheby2(), 208, 282
Chebyshev alternation theorem, 270
Chebyshev windowed FIR ﬁlter, 268
Chi-square PDF, 50
chi2pdf(), 50
Chirp convolution, 421
Chirp multiplication, 421
chirp(), 328
Chirp-z transform, 329
Choi-Williams distribution, 414
Classical decomposition, 178
Communication system simulation, 525
Conditional probability, 30, 90
Conjugate variables, 359
Constant-Q band-pass ﬁlters, 372
Continuous Wavelet transform, 372
Convolution, 126, 359
Cooley and Tukey, 544
Corner frequency, 186
Covariance-stationary, 158
Covariant transform, 410
cremez(), 281
Cross-correlation, 135
Cross power spectrum, 136
cwt(), 374
czt(), 330
D
2D DFT, 545
D-Flandrin distribution, 417
Danielson and Lanczos, 541
DARMA model, 156
Decomposition of the chirplet transform,
431
Delta modulation (DM), 522
demod(), 517
Demodulation of DSB modulated signals,
502
Demodulation of frequency modulated sig-
nal, 512
Density histogram, 62
Dictionary of atoms, 436
Differential
pulse-code
modulation
(DPCM), 522
Differentiator ﬁlter, 301
Digital amplitude modulation ASK, 513
Digital frequency modulation FSK, 513
Digital phase modulation PSK, 513
Dilation covariance, 377
Dilation operator, 409
Discrete convolution, 134, 240
Discrete Fourier transform (DFT), 329, 536
Discrete transfer function, 133, 239
Dissipative circuits, 196
disttool, 106
Dog bark, 477
Doppler effect, 332
Dopplerlets, 434
dpss(), 429
Drift, 167
Dual basis, 364
Duck quack, 475
E
Earthquake, 343, 480
eig(), 140
Electrocardiogram (ECG), 346
ellip(), 214, 282
ellipap(), 220
ellipord(), 220
Empirical mode decomposition, 455
Epanechnikov kernel, 70
Even signal, 16, 534
Expected value, 39
Exponential PDF, 49
exppdf(), 75
F
Fan-Chirp transform of some animal songs,
474
Fast Fourier transform, 15
FDATool, 305
fft(), 15, 329
fftﬁlt(), 282
Filter optimization criteria, 266
ﬁlter(), 157, 239, 282
ﬁltﬁlt(), 299
Final value theorem (Laplace trf.), 550
Final value theorem (z-transf), 551
FIR ﬁlter, 240, 246
ﬁr1(), 255
ﬁr2(), 281
ﬁrcls(), 281
ﬁrcls1(), 281
ﬁrls(), 272
ﬁrrcos(), 273, 522

Index
619
First order ﬁlter, 185
fminbnd(), 178
Foster forms, 195
Fourier series, 13
Fourier transform, 41, 247, 314, 419
Fourier transform pair, 357
Fractional Fourier transform, 419
Frame, 364
Frequency division multiplexing (FDM),
523
Frequency domain multiplexing, 523
Frequency modulation, 507
Frequency response, 121
Frequency response of a digital ﬁlter, 239
Frequency warping, 444
freqz(), 242
Fresnel transform, 420
G
Gabor expansion, 368
Gabor transform, 370
Gabor uncertainty principle, 369
Gamma function, 47
Gamma PDF, 48
gampdf(), 48
gauspuls(), 361
Gaussian chirplets, 432
Gaussian pulse, 360
Generalized Butterworth digital ﬁlter, 297
Generating function, 41
Geometric PDF, 59
geopdf(), 59
Global System for Mobile Communications
protocol (GSM), 523
Group delay, 221
H
Half-wave symmetry, 18
Hamming window, 256
Hanning window, 256
Heart sound, 474
Hermitian matrix, 406
Hermitian operator, 407
Hidden markov chain (HMM), 103
High-pass ﬁlter, 187
Hilbert-Huang transform, 459
Hilbert space, 407
Hilbert transform, 313
hilbert(), 300, 313
hist(), 33
Histogram, 33
Hyperbolic chirps, 434
I
Ideal ﬁlter, 198
Ideal ﬁlter approximations, 199
ifft(), 251, 285
IIR design based on ARMA modelling, 295
IIR ﬁlter, 240, 282
IIR ﬁlter design based on time domain specs,
290
IIR ﬁlter design using invfreqz(), 288
IIR ﬁlter design with Yule-Walker equations,
286
Importance sampling, 77
Impulse invariance method, 241
Impulse response, 126
Impulse response of a digital ﬁlter, 240
Impulse response sequence (digital ﬁlter),
248
impvar(), 241
impz(), 284
Independent random events, 29, 91
Initial value theorem (Laplace trf.), 550
Initial value theorem (z-transf), 551
Inner product, 363
Instantaneous frequency of the modulated
signal, 509
Inter-symbol interference (ISI), 522
Interpolated FIR ﬁlters, 280
intﬁlt(), 281
Intrinsic mode functions (IMF), 455
Invariant transforms, 410
Inverse 2D DFT, 545
Inverse DFT, 537
Inverse Fourier transform, 247
Inverses of distribution functions, 82
Inversion sampling, 80
invfreqz(), 288
istran(), 483
J
Joint probability, 91
K
Kaiser window, 262
Kansas City standard, 514
Kirwood distribution, 414
L
Lag operator, 157
Lag polynomial, 157
Laplace transform, 41, 118

620
Index
LCT decomposition, 421
Least-Squares error ﬁlter, 272
Likelihood function, 63
Lindenberg CLT, 88
Linear canonical transformation (LCT), 418
Linear geometric transformations, 404
Linear phase ﬁlter, 251
L2-norm of the error, 267
L ∞-norm of the error, 268
Localauto-correlationfunction(LACF),376
Localized bi-frequency kernel distributions,
416
Log-likelihood function, 63
Log-normal PDF, 36
lognpdf(), 37
lognrnd(), 36
Logons, 368
Low-pass ﬁlter, 186
lp2bp(), 220
lp2bs(), 220
lp2hp(), 220
lp2lp(), 220
lpc(), 295
lsim(), 130
Lung sounds, 478
Lyapunov CLT, 88
M
Margenau-Hill distribution, 414
Marginal distributions, 375
Marginally stable, 128
Markov chain, 96, 97
Markov chain Monte Carlo (MCMC), 99
Markov process, 96
Markov property, 97
Matched ﬁlter (radar), 362
Matching pursuit, 436
maxﬂat(), 297
Mean, 39
mean(), 41
Measurement noise, 150
Median, 39
median(), 41
Mel scale, 325
Metaplectic representation, 409
Method of moments, 65
Metropolis algorithm, 100
Metropolis–Hastings algorithm, 101
mhsample(), 103
MIMO systems, 138
Minimum-phase, 194
Mixture of Gaussians, 67
Mode, 39
Modiﬁed S-transform analysis of some
cases, 478
modulate(), 517
Modulation and demodulation of pulses, 518
Moment generating function, 41
Moments, 39
Monte Carlo, 72
Monte Carlo integration, 72
Moving-average ﬁlter, 253
Moving-average, MA, 155
Moyal’s formula, 378
Multivariate Gaussian PDF, 55
Music synthesizers, 342
N
Narrow-band ambiguity function, 362
Network synthesis theory, 194
Noncausal digital ﬁlter, 299
Nonequispaced DFT, 467
Non-minimum phase, 193
Normal (Gaussian) PDF, 33
Normal Reference Rule (for histograms), 62
Normalized frequency, 242
normpdf(), 34
normplot(), 60
Notch ﬁlter, 192
N-PSK modulation schemes, 516
Nyquist frequency, 242
O
Odd signal, 14, 534
Operator theory, 552
Orthogonal FDM (OFDM), 523
Orthogonal functions, 363, 536
Orthonormal base of functions, 363
P
Page distribution, 414
Paley–Wiener condition, 197
Parks-McLellan ﬁlter, 270
Parseval, 540
Parseval-Plancherel theorem, 358
Parzen estimation, 70
Parzen window, 69
Passive circuits, 190
Periodogram, 159
periodogram(), 159
Phase factors, 542
Phase modulation, 507
Plancherel, 540

Index
621
Poisson PDF, 58
poisspdf(), 58
Polar methods, 87
pole(), 201
polyﬁt(), 178
Power cepstrum, 321
Power spectral density (PSD), 43
Power spectrum, 43
Power spectrum of a signal, 321
Probability density, 30
Probability distribution, 30
Process noise, 145
Projection operator, 408
Prolate functions, 429
prony(), 295
Prony’s method, 295
Pseudo Wigner distribution, 397
4-PSK modulated signal, 515
Pulse amplitude modulation (PAM), 519
Pulse compression (radar), 362
Pulse phase modulation (PPM), 519
Pulse time modulation,(PTM), 519
Pulse train signal, 19
Pulse width modulation (PWM), 519
Pulse-code modulation (PCM), 522
Pure delay (of ﬁlter), 251
pwelch(), 43
pzmap(), 120, 285
Q
Quadratic form (operators), 408
Quadrature ﬁlter, 300
Quadrature multiplexing, 505
Quadrature PSK (QPSK), 524
R
Radio transmission scheme, 496
Radix-2 algorithms, 544
Raised cosine ﬁlter, 273
rand(), 31
randn(), 34
Random walk, 171
random(), 80
randtool, 106
Rayleigh PDF, 54
raylpdf(), 54
rceps(), 324
Rejection sampling, 85
Remez exchange algorithm, 270
remez(), 270
remezord(), 270
Respiratory sounds, 472
Response to any input signal, 130
Reverberation, 499
Ridge pursuit, 437
Rihaczek distribution, 414
S
Sallen-Key, 194
Sampling frequency, 22
Sampling rate and signal recovery quality,
521
Savitzky-Golay ﬁlter, 277
sawtooth(), 8
Scale variable, 373
Scaling operator, 420
Scalogram, 373
Seismic Analysis Code, 481
Seismic waves, 481
Self-adjoint operator, 407
sgolay(), 278
sgolayﬁlt(), 278
Shannon’s sampling theorem, 518
Shannon’s theorem, 25
Short-time Fourier transform (STFT), 364
Sifting, 458
sin(), 5
Sinc function, 246
sinc(), 21
Single
sideband
amplitude
modulation
(SSB), 505
Slepians, 429
slicesample(), 103
Smoothed Wigner distribution, 398
Smoothed-pseudo
Wigner
distribution
(SPWD), 398
Sound attenuation, 340
sound(), 10, 315
Sparse representation, 437
specgram(), 366
Spectral density, 159
Spectral density function, 314
Spectrogram, 320, 365
Spectrum of the AM modulated signal, 500
Spectrum of the FM modulated signal, 510
Spectrum shape, 335
Splines, 539
square(, 7
ss2tf(), 141
Stability, 128
Standard deviation, 33, 41
State space Gauss–Markov model, 141
State variables, 138

622
Index
std(), 41
Steiglitz-McBride iteration, IIR ﬁlters, 295
Step response, 126
step(), 126
stmcb(), 295
Stochastic ﬁnite state machine (FSM), 97
Stochastic process, 96
Strict-sense stationary, 41
Student’s t PDF, 52
Sunspot activity, 159
Suppressed carrier DSB amplitude modula-
tion, 505
Sussman ambiguity function (SAF), 381
Symplectic group, 408
T
tf(), 120
tf2ss(), 141
tfe(), 137
The afﬁne class, 416
The chirplet transform, 426
The Cohen’s class, 412
The constant-Q transform, 465
The Fan-Chirp-transform (FC), 451
The harmonic transform, 466
The Hilbert spectrum, 460
The
Hilbert
vibration
decomposition
(HVD), 466
The Mellin transform, 452
The modiﬁed S-transform, 448
The reassignment method, 444
The scale transform, 454
Thompson analog ﬁlter, 221
Tiling of the TF plane, 401
Timbre (sound), 337
Time-domain multiplexing (TDM), 523
Time-frequency representations, 399
Time-limited signal, 197
Time-warping, 438
tpdf(), 52
Transfer function, 118
Translation covariance, 377, 410
Tremolo, 12
Trend, 167
triang(), 254
Triangular kernel, 70
Triangular signal, 18
Triangular window, 254
Truncated impulse response, 251
Twiddle factors, 542
Two-dimensional density, 375
Types of chirplets, 427
U
Uncertainty principle, 359, 546
Unconditional probability, 90
Uniform PDF, 31
unifpdf(), 32
Unitary equivalence principle, 437
Unitary matrix, 406
Unitary operator, 408
unwrap(), 225
V
var(), 41
Variance, 41
Variational mode decomposition, 467
vco(), 317
W
Warblets, 435
Wavelet analysis of lung and heart sounds,
471
Weakly-stationary, 158
weibplot(), 61
Weibull PDF, 53
Weighted averaging, 253
weibpdf(), 53
Weyl-Heisenberg system, 368
Whale song, 479
White noise, 42
Wide-band ambiguity function, 363
Wiener-Khinchin theorem, 540
WiFi, 523
Wigner-Ville distribution, 377
Windowed FIR ﬁlters, 253
Wold’s decomposition, 158
Y
Yule–Walker equations, 168
yulewalk(), 286
Z
Z transform, 133, 329
Zak transform, 370
zero(), 209
zgrid(), 134, 285


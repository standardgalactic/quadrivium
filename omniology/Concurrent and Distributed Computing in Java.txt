
Concurrent and Distributed 
Computing in Java 

This Page Intentionally Left Blank

Concurrent and Distributed 
Computing in Java 
Vijay K. Garg 
University of Texas at Austin 
IEEE PRESS 
A JOHN WILEY & SONS, INC., PUBLICATION 

Copyright 
2004 by John Wiley & Sons, lnc. All rights reserved. 
Published by John Wiley & Sons, inc., Hoboken, New Jersey. 
Published simultaneously in Canada. 
No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or 
by any means, electronic, mechanical, photocopying, recording, scanning or otherwise, except as 
permitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior 
written permission of the Publisher, or authorization through payment of the appropriate per-copy fee to 
the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923, (978) 750-8400, fax 
(978) 646-8600, or on the web at www.copyright.com. Requests to the Publisher for permission should 
be addressed to the Permissions Department, John Wiley & Sons, Inc., 11 1 River Street, Hoboken, NJ 
07030, (201) 748-601 I, fax (201) 748-6008. 
Limit of LiabilityiDisclaimer of Warranty: While the publisher and author have used their best efforts in 
preparing this book, they make no representation or warranties with respect to the accuracy or 
completeness of the contents of this book and specifically disclaim any implied warranties of 
merchantability or fitness for a particular purpose. No warranty may be created or extended by sales 
representatives or written sales materials. The advice and strategies contained herein may not be 
suitable for your situation. You should consult with a professional where appropriate. Neither the 
publisher nor author shall be liable for any loss of profit or any other commercial damages, including 
but not limited to special, incidental, consequential, or other damages. 
For general information on our other products and services please contact our Customer Care 
Department within the U S .  at 877-762-2974, outside the U.S. at 317-572-3993 or fax 317-572-4002. 
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print, 
however, may not be available in electronic format. 
Library 
Congress Cataloging-in-Publication Data: 
Garg, Vijay Kumar, 1938- 
Concurrent and distributed computing in Java / Vijay K. Garg. 
p. cm. 
Includes bibliographical references and index. 
ISBN 0-471 -43230-X (cloth) 
I. Parallel processing (Electronic computers). 
processing. 3. Java (Computer program language). 
1. Title. 
2. Electronic data processing-Distributed 
QA76.58G35 2004 
0 0 5 . 2 ' 7 5 6 ~ 2 2  
2003065883 
Printed in the United States 
America. 
1 0 9 8 7 6 5 4 3 2 1  

To 
my teachers and 
my students 

This Page Intentionally Left Blank

Contents 
List of Figures 
xiii 
Preface 
xix 
1 Introduction 
1 
1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1 
1.2 Distributed Systems versus Parallel Systems . . . . . . . . . . . . . .  
3 
1.3 Overview of the Book . . . . . . . . . . . . . . . . . . . . . . . . . .  
4 
1.4 Characteristics of Parallel and Distributed Systems . . . . . . . . . .  
6 
1.5 Design Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
7 
1.6 Specification of Processes and Tasks . . . . . . . . . . . . . . . . . .  8 
1.6.1 
Runnable Interface . . . . . . . . . . . . . . . . . . . . . . . .  
11 
1.6.2 
Join Construct in Java . . . . . . . . . . . . . . . . . . . . . .  
11 
1.6.3 Thread Scheduling . . . . . . . . . . . . . . . . . . . . . . . .  
13 
1.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
13 
1.8 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
15 
2 Mutual Exclusion Problem 
17 
2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
17 
2.2 Peterson’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .  
20 
2.3 Lamport’s Bakery Algorithm . . . . . . . . . . . . . . . . . . . . . .  
24 
2.4 
Hardware Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
27 
2.4.1 
Disabling Interrupts . . . . . . . . . . . . . . . . . . . . . . .  
27 
2.4.2 
Instructions with Higher Atomicity . . . . . . . . . . . . . . .  27 
2.5 
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
28 
2.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
30 
3 Synchronization Primitives 
31 
3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
31 
3.2 
Semaphores . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
31 
vii 

... 
Vlll 
CONTENTS 
3.2.1 
The Producer-Consumer Problem . . . . . . . . . . . . . . .  
3.2.2 
The Reader-Writer Problem . . . . . . . . . . . . . . . . . . .  
3.2.3 
The Dining Philosopher Problem . . . . . . . . . . . . . . . .  
3.3 Monitors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.4 Other Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.5 Dangers of Dea. dlocks . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
4 Consistency Conditions 
4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.2 System Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.3 Sequential Consistency . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.4 Linearizabilky . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.5 Other Consistency Conditions . . . . . . . . . . . . . . . . . . . . . .  
4.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
4.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
5 Wait-Free Synchronization 
5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.2 Safe, Regular, and Atomic Registers . . . . . . . . . . . . . . . . . .  
5.3 Regular SRSW R.egist,er . . . . . . . . . . . . . . . . . . . . . . . . .  
5.4 SRSW Multivalued R.egister . . . . . . . . . . . . . . . . . . . . . . .  
5.5 MRSW Register 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.6 MRMW Register . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.7 Atomic Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.8 Consensus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.9 Universal Constructions . . . . . . . . . . . . . . . . . . . . . . . . .  
5.10 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.1 1 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
6 Distributed Programming 
6.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.2 InetAddress Class . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.3 Sockets based on UDP . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.3.1 
Datagram Sockets . . . . . . . . . . . . . . . . . . . . . . . .  
6.3.2 
Datagrampacket Class . . . . . . . . . . . . . . . . . . . . . .  
6.3.3 
Example Using Dat#agraIns . . . . . . . . . . . . . . . . . . .  
Sockets Rased on TCP . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.4.1 
Server Sockets . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.4 
33 
36 
36 
42 
46 
49 
50 
51 
53 
53 
54 
55 
57 
60 
62 
63 
65 
65 
66 
70 
71 
73 
74 
76 
78 
84 
87 
87 
89 
89 
89 
90 
90 
91 
92 
94 
96 

CONTENTS 
ix 
6.4.2 
Example 1: A Name Server . . . . . . . . . . . . . . . . . . .  96 
6.4.3 
Example 2: A Linker . . . . . . . . . . . . . . . . . . . . . . .  
100 
6.5 Remote Method Invocations . . . . . . . . . . . . . . . . . . . . . . .  
101 
6.5.1 
Remote Objects . . . . . . . . . . . . . . . . . . . . . . . . .  
105 
6.5.2 
Parameter Passing . . . . . . . . . . . . . . . . . . . . . . . .  
107 
6.5.3 
Dealing with Failures . . . . . . . . . . . . . . . . . . . . . .  
108 
6.5.4 
Client Program . . . . . . . . . . . . . . . . . . . . . . . . . .  
108 
6.6 Other Useful Classes . . . . . . . . . . . . . . . . . . . . . . . . . . .  
109 
6.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
109 
6.8 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
110 
7 Models and Clocks 
111 
7.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
111 
7.2 Model of a Distributed System . . . . . . . . . . . . . . . . . . . . .  
112 
7.3 Model of a Distributed Computation . . . . . . . . . . . . . . . . . .  114 
7.3.1 
Interleaving Model . . . . . . . . . . . . . . . . . . . . . . . .  
114 
7.3.2 
Happened-Before Model . . . . . . . . . . . . . . . . . . . . .  
114 
7.4 Logical Clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
115 
7.5 Vector Clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
117 
7.6 Direct-Dependency Clocks . . . . . . . . . . . . . . . . . . . . . . . .  
122 
7.7 Matrix Clocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
125 
7.8 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
126 
7.9 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
127 
8 Resource Allocation 
129 
8.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
129 
8.2 Specification of the Mutual Exclusion Problem . . . . . . . . . . . .  130 
8.3 Centralized Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . .  
132 
8.4 Lamport’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . .  
135 
8.5 Ricart and Agrawala’s Algorithm . . . . . . . . . . . . . . . . . . . .  
136 
8.6 Dining Philosopher Algorithm . . . . . . . . . . . . . . . . . . . . . .  
138 
8.7 Token-Based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .  
142 
8.8 Quorum-Based Algorithms . . . . . . . . . . . . . . . . . . . . . . . .  
144 
8.9 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
146 
8.10 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
147 
9 Global Snapshot 
149 
9.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
149 
9.2 Chandy and Lamport’s Global Snapshot Algorithm . . . . . . . . . .  151 
9.3 Global Snapshots for non-FIFO Channels . . . . . . . . . . . . . . .  154 

CONTENTS 
9.4 Channel Recording by the Sender . . . . . . . . . . . . . . . . . . . .  
154 
9.6 Problenis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
161 
9.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
162 
10.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
163 
10.2 Unstable Predicate Detection . . . . . . . . . . . . . . . . . . . . . .  
164 
10.3 Application: Distributed Debugging . . . . . . . . . . . . . . . . . .  169 
10.4 A Token-Based Algorithm for Detecting Predicates . . . . . . . . . .  169 
10.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
173 
10.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
176 
11.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
177 
11.2 Diffusing Computation . . . . . . . . . . . . . . . . . . . . . . . . . .  
177 
11.3 Dijkstra and Scholten’s Algorithm . . . . . . . . . . . . . . . . . . .  180 
11.3.1 An Optimization . . . . . . . . . . . . . . . . . . . . . . . . .  
181 
11.5 Locally Stable Predicates . . . . . . . . . . . . . . . . . . . . . . . .  
185 
11.6 Application: Deadlock Detection . . . . . . . . . . . . . . . . . . . .  
188 
11.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
189 
11.8 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
189 
12 Message Ordering 
191 
12.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
191 
12.2 Causal Ordering . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
193 
12.2.1 Application: Causal Chat . . . . . . . . . . . . . . . . . . . .  
196 
12.3 Synchronous Ordering . . . . . . . . . . . . . . . . . . . . . . . . . .  
196 
12.4 Total Order for Multicast Messages . . . . . . . . . . . . . . . . . . .  203 
12.4.1 Centralized Algorithm . . . . . . . . . . . . . . . . . . . . . .  
203 
12.4.2 Lamport’s Algorithm for Total Order . . . . . . . . . . . . .  204 
12.4.3 Skeen’s Algorithm . . . . . . . . . . . . . . . . . . . . . . . .  
204 
12.4.4 Application: Replicated State Machines . . . . . . . . . . . .  205 
12.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
205 
12.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
207 
9.5 Application: Checkpointing a Distributed Application . . . . . . . .  157 
10 Global Properties 
163 
11 Detecting Termination and Deadlocks 
11.4 Termination Detection without Acknowledgment Messages . . . . . .  182 
13 Leader Election 
209 
13.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
209 
13.2 Ring-Based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . .  
210 

CONTENTS 
13.2.1 Chang-Roberts Algorithm . . . . . . . . . . . . . . . . . . . .  
210 
13.2.2 Hirschberg-Sinclair Algorithm . . . . . . . . . . . . . . . . .  212 
13.3 Election on General Graphs . . . . . . . . . . . . . . . . . . . . . . .  
213 
13.3.1 Spanning Tree Construction . . . . . . . . . . . . . . . . . . .  213 
13.4 Application: Computing Global Functions . . . . . . . . . . . . . . .  215 
13.5 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
217 
13.6 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
219 
14 Synchronizers 
221 
14.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
221 
14.2 A Simple synchronizer . . . . . . . . . . . . . . . . . . . . . . . . . .  
223 
14.2.1 Application: BFS Tree Construction . . . . . . . . . . . . . .  225 
14.3 Synchronizer 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
226 
14.4 Synchronizerp . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
228 
14.5 Synchronizer 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
230 
14.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
232 
14.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
232 
15 Agreement 
233 
15.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
233 
15.2 Consensus in Asynchronous Systems (Impossibility) . . . . . . . . .  234 
15.3 Application: Terminating Reliable Broadcast . . . . . . . . . . . . .  238 
15.4 Consensus in Synchronous Systems . . . . . . . . . . . . . . . . . . .  239 
15.4.1 Consensus under Crash Failures . . . . . . . . . . . . . . . . .  240 
15.4.2 Consensus under Byzantine Faults . . . . . . . . . . . . . . .  243 
15.5 Knowledge and Common Knowledge . . . . . . . . . . . . . . . . . .  244 
15.6 Application: Two-General Problem . . . . . . . . . . . . . . . . . . .  248 
15.7 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
249 
15.8 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
250 
16 Transactions 
253 
16.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
253 
16.2 ACID Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  254 
16.3 Concurrency Control . . . . . . . . . . . . . . . . . . . . . . . . . . .  
255 
16.4 Dealing with Failures . . . . . . . . . . . . . . . . . . . . . . . . . . .  
256 
16.5 Distributed Commit . . . . . . . . . . . . . . . . . . . . . . . . . . .  
257 
16.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
261 
16.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
262 

xii 
CONTENTS 
17 Recovery 
263 
17.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
263 
17.2 Zigzag Relation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
265 
17.3 Communication-Induced Checkpointing . . . . . . . . . . . . . . . .  267 
17.4 Optimistic Message Logging: Main Ideas . . . . . . . . . . . . . . . .  268 
17.4.1 Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
269 
17.4.2 Fault-Tolerant Vector Clock . . . . . . . . . . . . . . . . . . .  270 
17.4.3 Version End Table . . . . . . . . . . . . . . . . . . . . . . . .  
272 
17.5 An Asynchronous Recovery Protocol . . . . . . . . . . . . . . . . . .  272 
17.5.1 Message Receive . . . . . . . . . . . . . . . . . . . . . . . . .  
274 
17.5.2 On R.estart after a Failixe . . . . . . . . . . . . . . . . . . . .  
274 
17.5.3 On Receiving a Token . . . . . . . . . . . . . . . . . . . . . .  
274 
17.5.4 On Rollback 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
276 
17.6 Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
277 
17.7 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
278 
18 Self-stabilization 
279 
18.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
279 
18.2 Mutual Exclusion with K-State Machines . . . . . . . . . . . . . . .  280 
18.3 Self-St abilizing Spanning Trce Construction . . . . . . . . . . . . . .  285 
18.4 Problenw . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
286 
18.5 Bibliographic Remarks . . . . . . . . . . . . . . . . . . . . . . . . . .  
289 
A . Various Utility Classes 
291 
Bibliography 
297 
Index 
305 

List of Figures 
1.1 A parallel system 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1.2 A distributed system 
. . . . . . . . . . . . . . . . . . . . . . . . . .  
1.3 A process with four threads . . . . . . . . . . . . . . . . . . . . . . .  
1.4 HelloWorldThread.java 
. . . . . . . . . . . . . . . . . . . . . . . . .  
1.5 FooBar.java 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
1.6 Fibonacci.java 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
2.1 
Interface for acccssing the critical section . . . . . . .  
2.2 
A program to test mutual exclusion . . . . . . . . . . . . . . . . . .  
2.3 An attempt that violates mutual exclusion . . . . . . . . . . . . . .  
2.4 An attempt that can deadlock . . . . . . . . . . . . . . . . . . . . . .  
2.5 An attempt with strict alternation . . . . . . . . . . . . . . . . . . .  
2.6 Peterson’s algorithm for mutual exclusion . . . . . . . . . . . . . . .  
2.7 Lamport’s bakery algorithm . . . . . . . . . . . . . . . . . . . . . . .  
2.8 TestAndSet hardware instruction . . . . . . . . . . . . . . . . . . . .  
2.9 Mutual exclusion using TestAndSet 
. . . . . . . . . . . . . . . . . .  
2.10 Semantics of 
operation 
. . . . . . . . . . . . . . . . . . . . . .  
2.11 Dekker.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.1 Binary semaphore . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.2 Counting semaphore . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.3 A shared buffer implemented with a circular array . . . . . . . . . .  
3.4 Bounded buffer using semaphores 
. . . . . . . . . . . . . . . . . . .  
3.5 Producer-consumer algorithm using semaphores . . . . . . . . . . .  
3.6 Reader-writer algorithm using semaphores . . . . . . . . . . . . . . .  
3.7 The dining philosopher problem 
. . . . . . . . . . . . . . . . . . . .  
3.8 Dining Philosopher 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.9 
Inkrface . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
3.10 Dining philosopher using semaphores . . . . . . . . . . . . . . . . . .  
3.11 A pictorial view of a Java monitor . . . . . . . . . . . . . . . . . . .  
2 
2 
9 
11 
12 
14 
18 
19 
20 
21 
21 
22 
25 
27 
28 
28 
29 
32 
33 
34 
35 
37 
38 
39 
40 
41 
41 
44 
xiii 

LIST OF FIGURES 
3.12 Bounded buffer monitor . . . . . . . . . . . . . . . . . . . . . . . . .  
45 
3.13 Dining philosopher using monitors . . . . . . . . . . . . . . . . . . .  47 
3.14 Linked list . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
48 
4.1 Concurrent. histories illustrating sequential consistency . . . . . . . .  56 
4.2 Sequential consistency does not satisfy locality . . . . . . . . . . . .  58 
4.3 Summary of consistency conditions . . . . . . . . . . . . . . . . . . .  62 
5.1 Safe and unsafe read-write registers . . . . . . . . . . . . . . . . . .  
5.2 Concurrent histories illustrating regularity . . . . . . . . . . . . . .  
5.3 Atomic and nonatomic registers 
. . . . . . . . . . . . . . . . . . . .  
5.4 Construction of a regular boolean regist. er . . . . . . . . . . . . . . .  
5.5 Const. ruction of a multivalued register . . . . . . . . . . . . . . . . .  
5.6 Construction of a niultireader register . . . . . . . . . . . . . . . . .  
5.7 Construction of a mukiwriter register . . . . . . . . . . . . . . . . .  
5.8 Lock-free atomic snapshot algorithm . . . . . . . . . . . . . . . . . .  
5.9 Consensus Interface 
. . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.10 Impossibility of wait-free consensus with atomic read-write registers 
5.11 TestAndSet class . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.12 Consensus using TestAndSet object . . . . . . . . . . . . . . . . . .  
5.13 CompSwap object . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.14 Consensus using CompSwap object 
. . . . . . . . . . . . . . . . . .  
5.15 Load-Linked and Store-Conditional object . . . . . . . . . . . . . . .  
5.16 Sequential queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
5.17 Concurrent queue . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.1 A datagram server . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.2 A datagram client . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.3 Simple name table . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.4 Name server . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.5 A client for name server . . . . . . . . . . . . . . . . . . . . . . . . .  
6.6 Topology class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.7 Connector class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.8 Message class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.9 Linker class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.10 R.emote interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
6.11 A name service implementation 
. . . . . . . . . . . . . . . . . . . .  
6.12 A RMI client program . . . . . . . . . . . . . . . . . . . . . . . . . .  
67 
68 
69 
71 
72 
75 
76 
77 
78 
80 
81 
82 
82 
83 
84 
85 
86 
93 
95 
97 
98 
99 
100 
102 
103 
104 
105 
106 
109 
7.1 An example of topology of a distributed system . . . . . . . . . . . .  113 
7.2 
A simple distributed program with two processes . . . . . . . . . . .  113 

LIST OF FIGURES 
7.3 A run in the happened-before model . . . . . . . . . . . . . . . . . .  115 
7.4 A logical clock algorithm . . . . . . . . . . . . . . . . . . . . . . . . .  
117 
7.5 A vector clock algorithm . . . . . . . . . . . . . . . . . . . . . . . .  
119 
7.6 The 
class that extends the 
class . . . . . . . . . .  120 
7.7 A sample execution of the vector clock algorithm . . . . . . . . . . .  121 
7.9 A sample execution of the direct-dependency clock algorithm . . . . .  123 
7.10 The matrix clock algorithm . . . . . . . . . . . . . . . . . . . . . . .  
124 
7.8 A direct-dependency clock algorithm . . . . . . . . . . . . . . . . . .  122 
8.1 
8.2 
8.3 
8.4 
8.5 
8.6 
8.7 
8.8 
8.9 
Testing a lock implementation . . . . . . . . . . . . . . . . . . . . .  
131 
ListenerThread . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
132 
Process.java 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
133 
A centralized mutual exclusion algorithm . . . . . . . . . . . . . . .  134 
Lamport’s mutual exclusion algorithm . . . . . . . . . . . . . . . . .  137 
Ricart and Agrawala’s algorithm . . . . . . . . . . . . . . . . . . . .  
139 
(a) Conflict graph; (b) an acyclic orientation with P2 and P4 as 
sources; (c) orientation after 
and P4 finish eating . . . . . . . . .  141 
An algorithm for dining philosopher problem . . . . . . . . . . . . .  143 
. . . . . .  145 
A token ring algorithm for the mutual exclusion problem 
9.1 Consistent and inconsistent cuts . . . . . . . . . . . . . . . . . . . .  
151 
9.2 Classification of messages . . . . . . . . . . . . . . . . . . . . . . . .  
153 
9.3 Chandy and Lamport’s snapshot algorithm . . . . . . . . . . . . . .  155 
9.4 Linker extended for use with Sendercamera . . . . . . . . . . . . . .  158 
9.5 A global snapshot algorithm based on sender recording . . . . . . . .  159 
9.6 Invocation of the global snapshot algorithm . . . . . . . . . . . . . .  160 
10.1 WCP (weak conjunctive predicate) detection algorithm-checker 
pro- 
cess . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
167 
10.2 Circulating token with vector clock 
. . . . . . . . . . . . . . . . . .  170 
. . . . . .  171 
10.4 Monitor process algorithm at Pi . . . . . . . . . . . . . . . . . . . . .  
172 
10.5 Token-based WCP detection algorithm . . . . . . . . . . . . . . . . .  174 
11.1 A diffusing computation for the shortest path 
. . . . . . . . . . . .  179 
11.2 Interface for a termination detection algorithm . . . . . . . . . . . .  179 
11.3 Termination detection algorithm . . . . . . . . . . . . . . . . . . . .  
183 
11.4 A diffusing computation for the shortest path with termination . . .  184 
11.5 Termination detection by token traversal . . . . . . . . . . . . . . . .  186 
12.1 A FIFO computation that is not causally ordered . . . . . . . . . . .  191 
10.3 An application that runs circulating token with a sensor 

xvi 
LIST OF FIGURES 
12.2 An algorithm for causal ordering of messages at Pi . . . . . . . . . .  
12.3 Structure of a causal message . . . . . . . . . . . . . . . . . . . . . .  
12.4 CausalLinker for causal ordering of messages . . . . . . . . . . . . .  
12.5 A chat program . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
12.6 A computation t. hat is synchronously ordered . . . . . . . . . . . . .  
12.7 A computation that is not synchronously ordered . . . . . . . . . . .  
12.8 The algorithm at. Pi for synchronous ordering of messages 
12.9 The algorithm for synchronous ordering of messages 
13.2 Configurat. ions for the worst case (a) and the best case (b) . . . . . .  
13.3 A spanning tree construction algorithm 
13.5 A broadcast algorithm 
. . . . . . . . . . . . . . . . . . . . . . . . .  
. . . . . .  
. . . . . . . . .  
13.1 The leader election algorithm . . . . . . . . . . . . . . . . . . . . . .  
. . . . . . . . . . . . . . . .  
13.4 A convergecast algorithm . . . . . . . . . . . . . . . . . . . . . . . .  
13.6 Algorithm for computing a global function . . . . . . . . . . . . . . .  
13.7 Compiit. ing t. he global sum . . . . . . . . . . . . . . . . . . . . . . . .  
14.1 Algorithm for the simple synchronizer at Pj . . . . . . . . . . . . . .  
14.2 Irnplementatiori of the simple synchronizer . . . . . . . . . . . . . . .  
14.3 An algorithm that generates a tree on an asynchronous network 
. . 
14.4 BFS tree algorithm using a synchronizer . . . . . . . . . . . . . . . .  
14.5 Alpha synchronizer . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
15.1 (a) Commutativity of disjoint events; (b) asynchrony of messages . . 
15.2 (a) Case 1: proc(e) # p r o c ( f ) ;  (b) case 2: proc(e) =proc(f) . . . .  
15.3 Algorithm at Pi for consensus under crash failures . . . . . . . . . .  
15.4 Consensus in a synchronous environment . . . . . . . . . . . . . . . .  
15.5 Consensus tester . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
15.6 An algorithm for Byzantine General Agreement 
. . . . . . . . . . .  
16.1 Algorithm for the coordinator of the two-phase commit protocol . . 
16.2 Algorithm for the participants in the two-phase commit protocol . . 
17.1 An example of the domino effect . . . . . . . . . . . . . . . . . . . .  
17.2 Examples of zigzag paths . . . . . . . . . . . . . . . . . . . . . . . .  
17.3 A distributed computation . . . . . . . . . . . . . . . . . . . . . . . .  
17.4 Formal description of the fault-tolerant vector clock . . . . . . . . .  
17.5 Formal description of the version end-table mechanism . . . . . . . .  
17.6 An optimistic protocol for asynchronous recovery . . . . . . . . . . .  
193 
194 
195 
197 
198 
198 
201 
202 
211 
212 
214 
216 
216 
218 
219 
223 
224 
226 
227 
229 
234 
237 
241 
242 
243 
245 
259 
260 
264 
266 
271 
273 
273 
275 
18.1 K-state self-stabilizing algorithm . . . . . . . . . . . . . . . . . . . .  
280 

LIST OF FIGURES 
xvii 
18.2 A move by the bottom machine in the K-state algorithm . . . . . .  
18.3 A move by a normal machine in the K-state algorithm 
. . . . . . .  
18.4 Self-stabilizing algorithm for mutual exclusion in a ring for the bottom 
machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
18.5 Self-stabilizing algorithm for mutual exclusion in a ring for a normal 
machine . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
18.6 Self-stabilizing algorithm for (BFS) spanning tree . . . . . . . . . . .  
18.7 Self-stabilizing spanning tree algorithm for the root . . . . . . . . . .  
18.8 Self-stabilizing spanning tree algorithm for nonroot nodes . . . . . .  
18.9 A Java program for spanning tree . . . . . . . . . . . . . . . . . . . .  
280 
281 
283 
284 
285 
286 
287 
288 
A.l Util.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
292 
A.2 Symbols.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
293 
A.3 Matrix.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
293 
A.4 MsgList.java. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
294 
A.5 1ntLinkedList.java . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
294 
A.6 PortAddr.java . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  
295 

This Page Intentionally Left Blank

Preface 
This book is designed for a senior undergraduate-level course or an introductory 
graduate-level course on concurrent and distributed computing. This book grew 
out of my dissatisfaction with books on distributed systems (including books au- 
thored by me) that included pseudocode for distributed algorithms. There were two 
problems with pseudocode. First, pseudocode had many assumptions hidden in it 
making it more succinct but only at the expense of precision. Second, translating 
pseudocode into actual code requires effort and time, resulting in students never ac- 
tually running the algorithm. Seeing the code run lends an extra level of confidence 
in one’s understanding of the algorithms. 
It must be emphasized that all of the Java code provided in this book is for 
educational purposes only. I have deliberately avoided error checking and other 
software engineering principles to keep the size of the code small. In the majority 
of cases, this led to Java code, that kept the concepts of the algorithm transparent. 
Several examples and exercise problems are included in each chapter to facilitate 
classroom teaching. I have made an effort to include some programming exercises 
with each chapter. 
I would like to thank the following people for working with me on various projects 
discussed in this book: Craig Chase (weak predicates), Om Damani (message log- 
ging), Eddy Fromentin (predicate detection), Joydeep Ghosh (global computation), 
Richard Kilgore (channel predicates), Roger Mitchell (channel predicates), Neeraj 
Mittal (predicate detection and control, slicing, self-stabilization, distributed shared 
memory), Venkat Murty (synchronous ordering), Michel Raynal (control flow prop- 
erties, distributed shared memory), Alper Sen (slicing), Chakarat Skawratonand 
(vector clocks), Ashis Tarafdar (message logging, predicate control), Alexander Tom- 
linson (global time, mutual exclusion, relational predicates, control flow properties) 
and Brian Waldecker (weak and strong predicates). Anurag Agarwal, Arindam 
Chakraborty, Selma Ikiz, Neeraj Mittal, Sujatha Kashyap, Vinit Ogale, and Alper 
Sen reviewed parts of the book. I owe special thanks to Vinit Ogale for also helping 
me with figures. 
I thank the Department of Electrical and Computer Engineering at The Uni- 
xix 

v<:rsit,y of Texas at Austin, where I was given the opportunity to develop and teach 
courses on concurrent and distributed systems. Students in these courses gave me 
very useful fedback. 
I was support,ed in part by many grants from the National Science Foundat.ion 
over the last, 14 years. Many of t,he results reported in this book would not have 
been discovered by me and my research group without that support. I also thank 
John Wiley & Sons, Inc. for supporting the project. 
Finally, I thank my parents, wife and children. Without their love and support, 
this book would not have been even conceived. 
There are many concurrent and distributed programs in this book. Although I 
have tried t,o ensure t,hat t’liere are no “bugs” in these programs, some are, no doubt,, 
st,ill lurking in the code. I would be grat,eful if any bug that is discovered is reported 
t,o me. The list of known errors and the supplerrientary material for the book will 
be rriaint,ained on my homepage: 
Included in tjhe Website is a program that allows animation of most of the algorithms 
in the book. It also includes all the source code given in the book. The reader can 
access t,he source code with the user name as 
and the password as 
Vijay K. Garg 
Austin. Texas 

Chapter 1 
Introduction 
1.1 Introduction 
Parallel and distributed computing systems are now widely availabli . A parullel sys- 
tem consists of multiple processors that communicate with each otl er using shared 
memory. As the number of transistors on a chip increases, miiltipro essor chips will 
become fairly common. With enough parallelism available in applicE :ions, such sys- 
terns will easily beat sequential systems in performance. Figure 1.1 lows a parallel 
system with multiple processors. These processors communicate 
ith each other 
using the shared memory. Each processor may also have local mem Iry that is not 
shared with other processors. 
We define distributed systems as those computer syst.ems that co kain mult.iple 
processors connected by a communication network. In these syste 11s processors 
communicate with each other using messages that are sent over the I 2twork. Such 
systems are increasingly available because of decrease in prices of coa iuter proces- 
sors and the high-bandwidth links to connect them. Figure 1.2 shows 
distributed 
system. The communication network in the figure could be a local ,rea network 
such as an Ethernet, or a wide area network such 
the Internet. 
Programming parallel and distributed systems requires a different set of tools 
and techniques than that required by the traditional sequential software. The focus 
of this book is on these techniques,. 
1 

2 
CPU 
Local 
memory 
CHAPTER 1. INTRODUCTION 
CPU 
CPU 
CPU 
CPU 
Local 
Local 
Local 
Local 
memory 
memory 
memory 
memory 
1 
Shared memory 
I 
Figure 1.1: A parallel system 
Figure 1.2: A distributed system 

1.2. DISTRIBUTED SYSTEMS VEItSUS PARALLEL SYSTEMS 
1.2 Distributed Systems versus Parallel Systems 
In this book, we make a distinction between distributed systems and parallel sys- 
tems. This distinction is only at a logical level. Given a physical system in which 
processors have shared memory, it is easy to simulate messages. Conversely, given 
a physical system in which processors are connected by a network, it is possible 
to simulate shared memory. Thus a parallel hardware system may run distributed 
software and vice versa. 
This distinction raises two important questions. Should we build parallel hard- 
ware or distributed hardware? Should we write applications assuming shared mem- 
ory or message passing? At the hardware level, we would expect the prevalent model 
to be multiprocessor workstations connected by a network. Thus the system is both 
parallel and distributed. Why would the system not be completely parallel? There 
are many reasons. 
0 Scalability: Distributed systems are inherently more scalable than parallel 
systems. In parallel systems shared memory becomes a bottleneck when the 
number of processors is increased. 
0 Modularity and heterogeneity: A distributed system is more flexible because a 
single processor can be added or deleted easily. Furthermore, this processor 
can be of a type completely different from that of the existing processors. 
0 Data sharing: Distributed systems provide data sharing as in distributed 
databases. Thus multiple organizations can share their data with each other. 
0 Resource sharing: Distributed systems provide resource sharing. For example, 
an expensive special-purpose processor can be shared by multiple organiza- 
tions. 
0 Geographic structure: The geographic structure of an application may be in- 
herently distributed. The low communication bandwidth may force local pro- 
cessing. This is especially true for wireless networks. 
0 Reliability: Distributed systems are more reliable than parallel systems be- 
cause the failure of a single computer does not affect the availability of others. 
0 
cost Availability of high-bandwidth networks and inexpensive worksta- 
tions also favors distributed computing for economic reasons. 
Why would the system not be a purely distributed one? The reasons for keeping 
a parallel system at each node of a network are mainly technological in nature. With 
the current technology it is generally faster to update a shared memory location than 

4 
CHAPTER 1. INTRODUCITON 
to send a message to another processor. This is especially true when the new value of 
the variable must be communicated to multiple processors. Consequently, it is more 
efficient to get fine-grain parallelism from a parallel system than from a distributed 
system. 
So far our discussion has been at the hardware level. As mentioned earlier, the 
interface provided to the programmer can actually be independent of the underlying 
hardware. So which model would then be used by the programmer? At the program- 
ming level, we expect that programs will be written using multithreaded distributed 
objects. In this model, an application consists of multiple heavyweight processes 
that communicate using messages (or remote method invocations). Each heavy- 
weight process consists of multiple lightweight processes called threads. Threads 
communicate through the shared memory. This software model mirrors the hard- 
ware that is (expected to be) widely available. By assuming that there is at most one 
thread per process (or by ignoring the parallelism wit,hin one process), we get the 
usual model of a distributed system. By restricting our attention to a single heavy- 
weight, process, we get, the usual model of a parallel system. We expect the system to 
have aspects of distributed objects. The main reason is the logical simplicity of the 
distributed object model. A distributed program is more object,-oriented because 
data in a remote object can be accessed only through an explicit message (or a re- 
mote procedure call). The object orientation promotes reusability as well as design 
simplicity. Furthermore, these object would be multithreaded because threads are 
useful for implement.ing efficient objects. For many applications such as servers, it. 
is useful to have a large shared data structure. It is a programming burden and 
inefficient. to split the data structure across multiple heavyweight processes. 
1.3 Overview of the Book 
This book is intended for a one-semester advanced undergraduate or introductory 
graduate course on concurrent and distributed systems. It can also be used as 
a supplementary book in a course on operating systems or distributed operating 
systems. For an undergraduate course, the instructor may skip the chapters on 
consistency conditions, wait-free synchronization, synchronizers, recovery, and self- 
stabilization without, any loss of continuity. 
Chapter 1 provides the motivation for parallel and distributed systems. It com- 
pares advantages of distributed systems with those of parallel systems. It gives the 
defining characteristics of parallel and distributed systems and the fundamental dif- 
ficulties in designing algorit,hms for such systems. It also introduces basic constructs 
of starting threads in Java. 
Chapters 2-5 deal with multithreaded programming. Chapter 2 discusses the 

1.3. OVERVIEW OF THE BOOK 
5 
mutual exclusion problem in shared memory systems. This provides motivation to 
students for various synchronization primitives discussed in Chapter 3. Chapter 3 
exposes students to multithreaded programming. For a graduate course, Chapters 
2 and 3 can be assigned for self-study. Chapter 4 describes various consistency 
conditions on concurrent executions that a system can provide to the programmers. 
Chapter 5 discusses a method of synchronization which does not use locks. Chapters 
4 and 5 may be skipped in an undergraduate course. 
Chapter 6 discusses distributed programming based on sockets 
well as remote 
method invocations. It also provides a layer for distributed programming used by 
the programs in later chapters. This chapter is a prerequisite to understanding 
programs described in later chapters. 
Chapter 7 provides the fundamental issues in distributed programming. It dis- 
cusses models of a distributed system and a distributed computation. It describes 
the interleaving model that totally orders all the events in the system, and the hap- 
pened before model that totally orders all the events on a single process. It also 
discusses mechanisms called clocks used to timestamp events in a distributed com- 
putation such that order information between events can be determined with these 
clocks. This chapter is fundamental to distributed systems and should be read before 
all later chapters. 
Chapter 8 discusses one of the most studied problems in distributed systems-- 
mutual exclusion. This chapter provides the interface 
and discusses various 
algorithms to implement this interface. 
is used for coordinating resources in 
distributed systems. 
Chapter 9 discusses the abstraction called 
that can be used to compute 
a consistent snapshot of a distributed system. We describe Chandy and Lamport’s 
algorithm in which the receiver is responsible for recording the state of a channel 
well as a variant of that algorithm in which the sender records the state of the 
channel. These algorithms can also be used for detecting stable global properties- 
properties that remain true once they become true. 
Chapters 10 and 11 discuss the abstraction called Sensor that can be used to 
evaluate global properties in a distributed system. Chapter 10 describes algorithms 
for detecting conjunctive predicates in which the global predicate is simply a con- 
junction of local predicates. Chapter 11 describe algorithms for termination and 
deadlock detection. Although termination and deadlock can be detected using tech- 
niques described in Chapters 9 and 10, we devote a separate chapter for termination 
and deadlock detection because these algorithms are more efficient than those used 
to detect general global properties. They also illustrate techniques in designing 
distributed algorithms. 
Chapter 12 describe methods to provide messaging layer with stronger properties 
than provided by the Transmission Control Protocol (TCP). We discuss the causal 

6 
CHAPTER 1. INTRODUCTION 
ordering of messages, the synchronous and the total ordering of messages. 
Chapter 13 discusses two abstractions in a distributed system-Elect ion and 
GlobalFunction. We discuss election in ring-based systems 
well as in general 
graphs. Once a leader is elected, we show that a global function can be computed 
easily via a convergecast and a broadcast. 
Chapter 14 discusses synchronizers, a method to abstract out asynchrony in the 
system. A synchronizer allows a synchronous algorithm to be simulated on top of an 
asynchronous system. We apply synchronizers to compute the breadth-first search 
(BFS) tree in an asynchronous network. 
Chapters 1-14 assume that there are no faults in the system. The rest of the 
book deals with techniques for handling various kinds of faults. 
Chapter 15 analyze the possibi1it)y (or impossibility) of solving problems in the 
presence of various types of faults. It includes the fundamental impossibility result of 
Fischer, Lynch, and Paterson that shows that consensus is impossible to solve in the 
presence of even one unannounced failure in an asynchronous system. It also shows 
that the consensus problem can be solved in a synchronous environment under crash 
and Byzant,ine faults. It also discusses the ability to solve problems in the absence 
of reliable communication. The two-generals problem shows that agreement on a 
bit (gaining common knowledge) is impossible in a distributed system. 
Chapter 16 describes t,he notion of a transaction and various algorithms used in 
implementing transactions. 
Chapter 17 discusses methods of recovering from failures. It includes both check- 
pointing and message-logging techniques. 
Finally, Chapter 18 discusses self-stabilizing systems. We discuss solutions of 
the mutual exclusion problem when the state of any of the processors may change 
arbitrarily because of a fault. We show that it is possible to design algorithms that 
guarantee that the system converges to a legal state in a finite number of moves 
irrespective of the system execution. We also discuss self-stabilizing algorithms for 
maintaining a spanning tree in a network. 
There are numerous starred and unstarred problems at the end of each chapter. 
A student is expected to solve unstarred problems with little effort. The starred 
problems may require the student to spend more effort and are appropriate only for 
graduate courses. 
1.4 Characteristics of Parallel 
Distributed Systems 
Recall t,hat we distinguish between parallel and distributed systems on the basis of 
shared memory. A distributed system is characterized by absence of shared memory. 
Therefore, in a distributed system it is impossible for any one processor to know 

DESIGN GOALS 
the global state of the system. As a result, it is difficult to observe any global 
property of the system. We will later see how efficient algorithms can be developed 
for evaluating a suitably restricted set of global properties. 
A parallel or a distributed system may be tightly coupled or loosely coupled de- 
pending on whether multiple processors work in a lock step manner. The absence of 
a shared clock results in a loosely coupled system. In a geographically distributed 
system, it is impossible to synchronize the clocks of different processors precisely 
because of uncertainty in communication delays between them. As a result, it is 
rare to use physical clocks for synchronization in distributed systems. In this book 
we will see how the concept of causality is used instead of time to tackle this prob- 
lem. In a parallel system, although a shared clock can be simulated, designing a 
system based on a tightly coupled architecture is rarely a good idea, due t.0 loss of 
performance because of synchronization. In this book, we will assume that systems 
are loosely coupled. 
Distributed systems can further be classified into synchronous and asynchronous 
systems. A distributed system is asynchronous if there is no upper bound on the 
message communication time. Assuming asynchrony leads to most general solu- 
tions to various problems. 
However, 
things get difficult in asynchronous systems when processors or links can fail. In 
an asynchronous distributed system it is impossible to distinguish between a slow 
processor and a failed processor. This leads to difficulties in developing algorithms 
for consensus, election, and other important problems in distributed computing. We 
will describe these difficulties and also show algorithms that work under faults in 
synchronous systems. 
We will see many examples in this book. 
Design Goals 
The experience in large parallel and distributed software systems has shown that 
their design should take the following concepts into consideration [TvSO~]: 
0 Fault tolerance: The software system should mask the failure of one or more 
components in the system, including processors, memory, and network links. 
This generally requires redundancy, which may be expensive depending on 
the degree of fault tolerance. Therefore, cost-benefit analysis is required to 
determine an appropriate level of fault tolerance. 
0 Transparency: The system should be 
user-friendly 
possible. This requires 
that the user not have to deal with unnecessary details. For example, in a 
heterogeneous distributed system the differences in the internal representation 
of data (such as the little endian format versus the big endian format for 

8 
CHAPTER 1. 1NTR.ODUCTION 
integers) should be hidden from the user, a concept called access transparency. 
Similarly, the use of a resource by a user should not require the user to know 
where it is locat,ed (location transparency), whether it is replicat,ed (replication 
transparency) , whether it is shared (concurrency transparency), or whether it 
is in volatile meniory or hard disk (persistence transparemy). 
0 Fleribility: The system should be able to interact, with a large number of other 
systems arid services. This requires that the system adhere tjo a fixed set of 
rules for syntax and semantics, preferably a standard, for interaction. This is 
oft,en facilitated by specification of services provided by the system through 
an interface definition lungvnge. Another form of flexibility can be given to 
t,he user by a separation bet,weeri policy and mechanism. For example, in 
the context of Web caching, the mechanism refers to the implementation for 
stsoring hhe Web pages locally. The policy refers to the high-level decisions 
such 
size of the cache, which pages are to be cached, and how lorig t>hose 
pages should remain in the cache. Such quest,ions may be answered better by 
t,he user and therefore it is better for users t,o build their own caching policy 
on t,op of the caching mechariisrri provided. By designing the system 
one 
monolithic component, we lose the flcxibilit,y of using different policies with 
different users. 
Scalability: If the system is not. designed to be scalable, then it, may have iin- 
satisfactory performance when the number of users or the resources increase. 
For example, a distributed system with a single server may become overloaded 
when the number of clients request,ing the service from the server increases. 
Generally, the system is either cornplet(e1y decentralized using distribnted al- 
gorit,lims or partially decentralized using a hierarchy of servers. 
1.6 Specification of Processes and Tasks 
In this book we cover the programming concepts for shared memory-based languages 
arid tfist,ribiited languages. It should be noted that the issues of concurrency arise 
even on a single CPU computer where a system may be organized as a collection 
of cooperating processes. In fact, t,he issues of synchronization and deadlock have 
roots in t,he development, of early operating systems. For this reason, we will refer 
to const,ructs described in this section 
concurrent programming. 
Before we embark on concurrent programming constructs, it is necessary to 
understand the distinction between a program and a process. A computer program 
is simply a set of instructions in a high-level or a machine-level language. It, is only 
when we execute a program that we get one or more processes. When the program is 

1.6. SPECIFICATION OF PROCESSES AND TASKS 
9 
sequential, it results in a single process, and when concurrent-multiple 
processes. 
A process can be viewed as consisting of three segments in the memory: code, data 
and execution stack. The code is the machine instructions in the memory which the 
process executes. The data consists of memory used by static global variables and 
runtime allocated memory (heap) used by the program. The stack consists of local 
variables and the activation records of function calls. Every process has its own 
stack. When processes share the addrcss space, namely, code and data, then they 
are called lzghtwezght processes or threads. Figure 1.3 shows four threads. All threads 
share the address space but have their own local stack. When process has its own 
code and data, it is called a heavyviezght process, or simply a process. Heavyweight 
processes may share data through files or by sending explicit messages to each other. 
___ 
Stack ! 
Thread 
Stack ! 
Thread 
Stack 
1 
Thread 
Stack 
1 
Thread 
Memory 
1 
I 
J 
Figure 1.3: A process with four threads 
Any programming language that supports concurrent programming must have 
a way to specify the process structure, and how various processes communicate 
and synchronize with each other. There are many ways a program may specify 
the process structure or creation of new processes. We look at the most popular 
ones. In UNIX, processes are organized as a tree of processes with each process 
identified using a unique process id (pid). UNIX provides system calls fork and wazt 
for creation and synchronization of processes. When a process executes a fork call, 

10 
CHAPTER 1. INTRODUCTION 
a child process is creatcd with a copy of the address space of the parent process. 
The only difference between the parent process and the child process is the value of 
the return code for the fork. The parent process gets the pid of the child process 
as the return code, and the child process gets the value 0 as shown in the following 
example. 
= 
== 0 )  { 
// 
<< 
// 
<< 
3 
The wait call is used for the parent process to wait for termination of the child 
process. A process terminates when it executes the last instruction in the code or 
makes an explicit call to the system call exit. When a child process terminates, the 
parent process, if waiting, is awakened and the pid of the child process is ret,urned 
for t,lie wait, call. In this way, the parent process can determine which of its child 
processes terminated. 
Frequcntly, t,he child process makes a call to the execwe system call, which loads 
a binary file into memory and starts execution of that file. 
Another programming construct for launching parallel hsks is cobegin-coend 
(also called parbegin-parend). Its syntax is given below: 
cobegin 
I/ 
coend 
This construct' says that 
and 
must be executed in parallel. Further, if one 
of them finishes earlier than the other, it should wait for the other one to finish. 
Combining the cobegin-coend wit>h the sequencing, or the series operator, semicolon 
(;), we can create any series-parallel task structure. For example, 
starts off with one process that executes 
When 
is finished, we have two 
processes (or threads) t,hat execute 
and 
in parallel. When both the statements 
are done, only then 
is started. 
Yet another method for specification of concurrency is to explicitly create thread 
objects. For example, in Java there is a predefined class called 
One can 

1.6. SPECIFICATION OF PROCESSES AND TASKS 
11 
extend the class Thread, override the method 
and then call start 0 to launch 
the thread. For example, a thread for printing “Hello World” can be launched as 
shown in Figure 1.4. 
public class HelloWorldThread extends Thread { 
public void r u n ( )  { 
System.out. p r i n t l n  ( ” H e l l o  World”); 
1 
public s t a t i c  void main( String [I 
a r g s )  { 
HeIloWorldThread t = new HelloWorldThread ( )  ; 
t .  start ( ) ;  
1 
1 
I 
Figure 1.4: HelloWor1dThread.java 
1.6.1 Runnable Interface 
In the HelloWorld example, the class HelloWorldThread needed to inherit methods 
only from the class Thread. What if we wanted to extend a class, say, Foo, but also 
make the objects of the new class run as separate thread? Since Java does not have 
multiple inheritance, we could not simply extend both Foo and the Thread class. 
To solve this problem, Java provides an interface called Runnable with the following 
single method: 
public void r u n 0  
To design a runnable class FooBar that extends Foo, we proceed as shown in 
Figure 1.5. The class FooBar implements the Runnable interface. The main function 
creates a runnable object f l  of type FooBar. Now we can create a thread ti by 
passing the runnable object f 1 
an argument to the constructor for Thread. This 
thread can then be started by invoking the start method. The program creates 
two threads in this manner. Each of the threads prints out the string getName0 
inherited from the class Foo. 
1.6.2 Join Construct in Java 
We have seen that we can use s t a r t ( )  to start a thread. ‘lhe folIowing example 
shows how a thread can wait for other thread to finish execution via the j o i n  
mechanism. We write a program in Java to compute the nth Fibonacci number F, 

12 
CHAPTER 1. INTILODUCTION 
:lass Foo { 
String name; 
public Foo(String s )  { 
name = s ;  
I 
1 
1 
public void setName(String s )  { 
name = s ;  
public String getName() { 
return name; 
:lass FooBar extends Foo implements Runnable { 
public FooBar(String s )  { 
super ( s ) ; 
} public void run ( )  { 
for ( i n t  i = 0; i < 10; i++) 
Systern.oiit. p r i n t l n  (getName() + ” :  Hello World”); 
1 
public static void main( String [ I  a r g s )  { 
FooBar f l  = new FooBar ( ”Romeo” ) ; 
Thread t l  = new Thread(f1 ) ;  
t l .  s t a r t  ( ) ;  
FooBar f2 = new FooBar(” J u l i e t  ” ) ;  
Thread t2 = new ’Thread ( f 2  ); 
t 2 .  start, ( ) ;  
1 
Figure 1.5: FooBar.java 

1.7. PROBLEMS 
using the recurrence relation 
for n 
2. The base cases are 
and 
13 
To compute 
the run method forks two threads that compute 
and Fn-2 
recursively. The main thread waits for t.hese two threads to finish their computation 
using join. The complete program is shown in Figure 1.6. 
1.6.3 Thread Scheduling 
In the FooBar example, we had two threads. The same Java program will work 
for a single-CPU machine 
well as for a multiprocessor machine. In a single-CPU 
machine, if both threads are runnable, which one would be picked by the system to 
run? The answer to this question depends on the priority and the scheduling policy 
of the system. 
The programmer may change the priority of threads using s e t p r i o r i t y  and 
determine the current priority by g e t p r i o r i t y .  M I N I R I O R I T Y  and MAXPRIORITY 
are integer constants defined in the Thread class. The method s e t p r i o r i t y  can 
use a value only between these two constants. By default, a thread has the priority 
NORMPRIORITY. 
A Java thread that is running may block by calling sleep, 
or any system 
function that is blocking (these calls will be described later). When this happens, a 
highest-priority runnable thread is picked for execution. When the highest-priority 
thread is running, it may still be suspended when its time slice is over. Another 
thread at the same priority level may then be allowed to run. 
1.7 Problems 
1.1. Give advantages and disadvantages of a parallel programming model over a 
distributed system (message-based) model. 
1.2. Write a Java class that allows parallel search in an array of integer. It provides 
the following s t a t i c  method: 
public s t a t i c  i n t  parallelSearch(int 
int[] A ,  i n t  numThreads) 

14 
CHAPTER 
INTRODUCTION 
public class Fibonacci extends Thread { 
int n ;  
int r e s u l t ;  
public Fibonacci ( i n t  ii) { 
this . n  = n ;  
public void run ( )  { 
1 
if ( ( n  == O ) ( l ( n  == 1 ) )  result = 1; 
else { 
Fibonacci f l  = new Fibonacci (11-1); 
Fibonacci f 2  = new Fibonacci ( n - 2 ) ;  
f l .  s t a r t  ( ) ;  
f 2 .  s t a r t  ( ) ;  
try { 
f l .  j o i n  ( ) ;  
f 2 .  j o i n  ( ) ;  
} catch ( InterruptrdException e ) { } ;  
r e s u l t  = f l  . getResult ( )  + f 2 .  getResult ( ) ;  
I 
1 
1 
public int getResult ( ) {  
return r e s u l t  ; 
public static void main(String 
a r g s )  { 
Fibonacci f l  = new Fibonacci ( I n t e g e r .  p a r s r I n t  ( a r g s  [ O ] ) ) ;  
f l .  s t a r t  ( ) ;  
try { 
f l .  j o i n  ( ) ;  
} catch ( IiiterruptedException e ) { } ;  
System. o u t .  p r i n t l n  ("Answer is " + f l  . grtResult 
1 
1 
Figure 1.6: Fibonacci.java 

1.8. BIBLIOGRAPHIC REhlARKS 
This method creates as many threads as specified by numThreads, divides the 
array A into that many parts, and gives each thread a part of the array to 
search for 
sequentially. If any thread finds x, then it returns an index i such 
that A [ i ]  
= x. Otherwise, the method returns -1. 
1.3. Consider tJhe class shown below. 
class Schedule { 
s t a t i c  i n t  x = 0; 
s t a t i c  i n t  y = 0; 
public s t a t i c  i n t  opl(){x = 
return y;) 
public s t a t i c  i n t  op2(){y = 2; return 
1 
If one thread calls opl and the other thread calls op2, then what values may 
be returned by opl and op2? 
1.4. Write a multithreaded program in Java that sorts an array using recursive 
The main thread forks two threads to sort the two halves of 
merge sort. 
arrays, which are then merged. 
1.5. Write a program in Java that uses two threads ho search for a given element 
in a doubly linked list. One thread traverses the list in the forward direction 
and the other. in the backward direction. 
1.8 Bibliographic Remarks 
There are many books available on distributed systenis. The reader is referred t,o 
books by Attiya and Welch [AWM], Barbosa [BarSB], Chandy and Misra [CM89], 
Garg [GarSG, Gar021, Lynch [LynSG], Raynal [R.ay88], and Tel [TelS4] for the range 
of topics in distributed algorithms. Couloris, Dollimore and Kindberg [CDK94], and 
Chow and Johnson [CJ97] cover some other practical aspects of distributed systems 
such as distributed file systems, which are not covered in this book. Goscinski 
[GosSl] and Singhal and Shivaratri [SS94] cover concepts in distributed operating 
systems. The book edited by Yang and Marsland [YM94] includes many papers 
that, deal with global time and state in distributed systems. The book edited by 
Mullender [SM94] covers many other topics such as protection, fault tolerance, and 
real-time communications. 
There are many books available for concurrent coniputing in Java 
well. The 
reader is referred to the books by Farley [Far98], Hartley [HarSS] and Lea [Lea%] 
as examples. These books do not discuss distributed algorithms. 

This Page Intentionally Left Blank

Chapter 2 
Mutual Exclusion Problem 
2.1 Introduction 
When processes share dat,a, it is important to synchronize their access to the data 
so that updates are not lost as a result of concurrent accesses and the data are not 
corrupted. This can be seen from the following example. Assume that the initial 
value of a shared variable z is 0 and that there are two processes, Po and PI such 
that each one of them increments 
by the following statement in some high-level 
programming language: 
z = z + l  
It is natural for the programmer to assume that the final value of 
is 2 after both 
the processes have executed. However, this may not happen if the programmer does 
not ensure that z = + 1 is executed atomically. The statement 
= z + 1 may 
compile into the machine-level code of the form 
LD R, z 
INC R 
ST R, 
; load register R from z 
; increment register R 
; store register R to 
Now the execution of Po and PI may get interleaved 
follows: 
17 

18 
CHAPTER 2. MUTUAL EXCLUSION PROBLEM 
Po: LD R, x 
Po: INC R 
; load register R from z 
; increment register R 
; load register R from 
; increment register R 
; store register R to x 
; store register R to 
PI: LD R, 5 
PI: INC R 
PI: ST R,x 
Po: ST R,x 
Thus both processes load the value 0 into their registers and finally store 1 into z 
resulting in the “lost update” problem. 
To avoid this problem, the stat,ement = + 1 should be executed 
section of the code that needs to be executed atomically is also called a critical 
region or a critical section. The problem of ensuring that a critical section is ex- 
ecuted atomically is called the mutual exclusion problem. This is one of the most 
fundamental problems in concurrent computing and we will study it in detail. 
The mutual exclusion problem can be abst,ract,ed as follows. We are required 
to implement t,he interface shown in Figure 2.1. A process that wants to enter 
t.he critical section (CS) makes a call to 
with its own identifier 
the 
argument. The process or t,he t,hread t,hat makes this call rctnrns from this niet.hod 
only when it, has t,he exclusive a( 
t,o the crit,ical section. When the process has 
finished accessing t.lie critical section, it makes a call to the method 
public interface Lock { 
public void requestCS (int pid ) ;  / / m a y  b l o c k  
public void releaseCS (int pid ); 
1 
Figure 2.1: Interface for accessing the critical section 
The entry prot,ocol given by the niet.hod 
arid the exit, protocol given 
by the method 
should be such t,hat the mutual exclusion is not violated. 
‘To t.est the Lock, we use the program shown in Figure 2.2. This program t,ests 
thc Bakery algorithm that will bc presented lat,er. The user of the program may test 
a different algorithm for a lock implementation by invoking the constructor of that, 
lock implement,at,ion. The program launches N threads as specified by 
LO]. Each 
thread is an object of the class 
Let us now look at the class 
This class has two methods, 
arid 
and it 
ovcrrides the 
method of t,he 
class as follows. Each thread repeatedly 
ent.cw the crit,ical section. After exit,ing from the critical section it, spends an imde- 
t,ernriinetl annolint, of time in the noncritical section of the code. In our example, we 
simply usc a random number t,o sleep in the crit,ical and the noncritical sections. 

2.1. INTRODUCTION 
19 
mport j a v a .  u t i l  .Random; 
Bublic c l a s s  MyThread extends Thread { 
i n t  myId; 
Lock lock; 
Random r = new Random(); 
public MyThread( i n t  i d ,  Lock l o c k )  { 
myId = id ; 
t h i s . l o c k  = lock; 
1 
void IionCriticalSection ( )  { 
System.out. p r i n t l n  (myId + ” is not in C S ) ;  
Util . m y S l e e p ( r . n e x t I n t  (1000)); 
} 
void C r i t i c a l s e c t i o n  ( )  { 
System. o u t .  p r i n t l n  (myId + ” is in CS *****” ) ;  
// critical section code 
Util . mysleep( r .  nextInt ( 1 0 0 0 ) ) ;  
1 
public void run ( )  { 
while ( t r u e )  { 
lock. requestCS (myId); 
C r i t i c a l s e c t i o n  ( ) ;  
lock. releaseCS (myId); 
nonCriticalSection ( ) ;  
1 
1 
public s t a t i c  void main( String [ I  args ) throws Exception { 
MyThread t [ I  ; 
i n t  N = I n t e g e r .  parseInt (args [ O ] ) ;  
t = new MyThread [ N] ; 
Lock lock = new Bakery(N);//or 
any other mutez algorithm 
for ( i n t  i = 0 ;  i < N; i + + )  { 
t [ i ]  := new MyThread(i, l o c k ) ;  
t [ i ] .  s t a r t  (); 
1 
1 
Figure 2.2: A program to test mutual exclusion 

20 
CHAPTER 2. MUTUAL EXCLUSION PROBLEM 
Let us now look at some possible protocols, one may attempt, to solve the mut,ual 
exclusion problem. For simplicity we first assume that there are only two processes, 
PO and PI. 
2.2 Peterson’s Algorithm 
Our first atkenipt would be t,o use a shared boolean variable openDoor initialized to 
true. The entry protocol would be to wait for openDoor to be true. If it is true, 
then a. process can enter the critical section after setting it to f a l s e .  On exit, the 
process resets it to true. This algorithm is shown in Figure 2.3. 
class Attempt1 implements Lock { 
boolean openDoor = t r u e ;  
public void requestCS(int i )  { 
while (! openDoor ) ; // busy w a i t  
operiDoor = f a l s e ;  
} 
} 
public void releaseCS ( i n t  i ) { 
openDoor = true ; 
1 
Figure 2.3: An attempt that violates mutual exclusion 
This attempt does not work because the testing of openDoor and setking it, to 
f a l s e  is not done atomically. Conceivably, one process might check for the openDoor 
and go past the while statement in Figure 2.3. However, before that process could 
set openDoor to f a l s e ,  the other process starts executing. The other process now 
checks for the value of openDoor and also gets out of busy wait. Both the processes 
now can set openDoor t,o false and enter the critical section. Thus, mutual exclusion 
is violated. 
In the attempt described above, the shared variable did not record who set the 
openDoor to false. One may t,ry to fix t,his problem by keeping two shared variables, 
wantCS 
and wantCS 
as shown in Figure 2.4. Every process Pi first, sets its 
own wantCS bit, to true at line 4 and t,hen waits until the wantCS for the other 
process is false at line 5. We have used 1 - to get the process identifier of t,he 
other process when there arc only two processes - PO and PI. To release the critical 
sect.ion, P, simply resets its wantCS bit to false. Unfortunately, this attempt also 
does not work. Both processes could set their wantCS to true and then indefinitely 
loop, wait.irig for t,he other process to set its wantCS false. 

2.2. PETERSON’S ALGORITHM 
21 
1 class Attempt2 implements Lock { 
2 
boolean wantCS[] = { false , false }; 
3 
public void requestCS(int i )  { // entry protocol 
4 
wantCS[i] = true; 
//declare 
intent 
5 
while (wantCS [l - i ] ) ; // busy wait 
7 
public void releaseCS (int i )  { 
8 
wantCS[ i ]  = false ; 
6
1
 
9
)
 
10 1 
Figure 2.4: An attempt that can deadlock 
Yet another attempt to fix the problem is shown in Figure 2.5. This attempt 
is based on evaluating the value of a variable turn. A process waits for its turn to 
enter the critical section. On exiting the critical section, it sets turn to 
class Attempt3 implements Lock { 
int t u r n  = 0; 
public void requestCS (int i )  { 
while ( t u r n  == 1 - i )  ; 
public void releaseCS (int i )  { 
} 
t u r n  = 1 - i ;  
Figure 2.5: An at,tempt with strict alternation 
This protocol does guarantee mutual excluszon. It also guarantees that if both 
processes are trying to enter the critical section, then one of them will succeed. 
However, it suffers from another problem. In this protocol, both processes have to 
alternate with each other for getting the critical section. Thus, after process Po exits 
from the critical scction it cannot enter the critical section again until process PI 
has entered the critical section. If process PI is not interested in the critical section, 
then process Po is simply stuck waiting for process PI. This is not desirablc. 
By combining the previous two approaches, however, we get Peterson’s algorithm 
for the mutual exclusion problem in a two-process system. In this protocol, shown 
in Figure 2.6, we maintain two flags, wantCS 
and wantCS 
as in Attempt2, 
and the turn variable 
in Attempt3. To request the critical section, process P, 
sets its wantCS flag to true at line 6 and then sets the turn to the other process P3 

22 
CHAPTER 2. MUTUAL EXCLIJSION PROBLEM 
1 class PetersonAlgorithrn implements Lock { 
2 
boolean wantCS [I = { false , false }; 
3 
int t u r n  = 1; 
4 
public void requestCS(int i )  { 
5 
int j = 
- i ;  
6 
wantCS [ i ] = true ; 
8 
while (wantCS[ j ]  && (turn == j ) )  ; 
1
7
 
turn = j ;  
9
1
 
12 
} 
} 
, 10 
public void releaseCS (int i )  { 
11 
wantCS[ i ]  = false ; 
at line 7 .  After that, it waits at line 8 so long 
the following condition is true: 
&& 
== 
Thus a process enters the critical sectmion only if either it is its turn to do so or if 
the other process is not interested in the critical section. 
To release the critical section, Pi simply resets the flag 
at line 11. 
This allows Pj to enter the crit,ical section by making the condition for it,s while 
loop false. 
Figure 2.6: Peterson’s algorithm for mutual exclusion 
We show that Peterson’s algorithm satisfies the following desirable properties: 
1. Mutual exclusion: Two processes cannot be in the critical section at the same 
time. 
2. Progress: If one or more processes are trying to enter the critical section and 
there is no process inside the critical section, then at least one of the processes 
succeeds in entering the critical section. 
3 .  Starmtion-freedom: If a process is trying to enter the critical section, then it 
eventually succeeds in doing so. 
We first show that mutual exclusion is satisfied by Peterson’s algorithm. Assume 
without loss of generality that PO was the first one to enter the critical section. To 
enter the critical section, Po must have either read wantCS[l] as false, or turn as 
0. We now perform a case analysis: 

2.2. PETERSON’S ALGORITHM 
23 
Case 1: PO read wantCS[l] false. If wantCS[l] is false, then for 
to enter 
the critical section, it would have to set wantCS[l] to true. From this case, we get 
the following order of events: PO reads wantCS[I] as false before 
sets the value 
of wuntCS[l] as true. This order of events implies that PI would set turn = 0 
before checking the entry condition and after the event of PO reading wantCS[l]. 
On the other hand, .Pi, set turn = 1 before reading uiantCS[l]. Therefore, we have 
the following order of events in time: 
0 PO sets turn to 1. 
0 Po reads wantCS[1] as false. 
0 Pi sets wantCS[1] as true. 
0 Pi sets turn to 0. 
0 PI reads turn 
Clearly, turn can be only 0 when PI reads it. Now let us look at the set of values 
of wantCS[O] that PI can possibly read. From the program, we know that PO sets 
wantCS[O] true before reading wantCS[l]. Similarly, 
sets wantCS[l] before 
reading wantCS[O]. We know that PO read wantCS[l] false. Therefore, 
sets 
wantCS[l] true after PO reads wantCS[l]. This implies that we have the following 
order of events: 
0 Po sets wantCS[O] 
true. 
0 PO reads wantCS[l] as false. 
0 PI sets wantCS[1] true. 
0 PI reads wantCS[O]. 
Therefore, PI can only read wantCS[O] 
true. Because PI reads turn 
0 and 
wantCS[O] true, it cannot enter the critical section. 
Case 
PO read turn 
0. This implies the following order of events: PI sets 
turn = 0 between Po setting turn = 1 and Po reading the value of turn. Since 
PI rcads the value of turn only after setting turn = 0, we know that it can read 
turn only 
0. Also, wantCS[O] is set before PO sets turn = 1. Therefore, PO 
sets wantCS[O] before PI sets turn = 0. This implies that PI reads the value of 
wantCS[OJ 
true. Thus, even in this case, Pi reads turn 
0 and wantCS(O] 
true. It follows that 1’1 cannot enter the critical section. 

24 
CHAPTER 2. MIJTUAL EXCLUSION PROBLEM 
It is easy to see that the algorithm satisfies the progress property. If both the 
processes are forever checking the entry protocol in the while loop, then we get 
wantCS[2] A (turn = 2) A iuantsCS[I] A (turn = I) 
which is clearly false because (turn = 2) A (turn = 1) is false. 
The proof of freedom from starvation is left 
an exercise. The reader call also 
verify that Peterson’s algorithm does riot require strict alternation of the critical 
sections-a 
process can repeatedly use the critical section if the other process is riot 
interested in it. 
2.3 Lamport’s Bakery Algorithm 
Alt,hough Peterson’s algorithm satisfies all the properties that we initially required 
from the prot,ocol, it works only for two processes. Alt,hough the algorithm can be 
exknded t,o N processes by repeated invocation of the entry protocol, the result’irig 
algorithm is Inore complex. 
We now describe Lamport’s bakery algorithm, which overcomes this disadvan- 
tage. The algorithm is similar to that used by bakeries in serving customers. Each 
cnstorner who arrives at the bakery r 
ives a number. The server serves the cus- 
tomer wit.h the srnallcst numbcr. In a concurrent system, it is difficult to ensure 
t,hat every process gets a unique number. So in case of a tie, we use process ids t.o 
choose the smaller process. 
The algorithm shown in Figure 2.7 requires a process Pi t,o go through two main 
steps before it, can enter the critical section. In the first step (lines 15-21), it is 
required to r:hoose a number. To do t,hat, it, reads the numbers of all other processes 
and chooses its number as one bigger than the maximimi number it read. We will 
call t,his st,ep the doorway. In the second st,ep the process Pi checks if it, can enter 
the critical scct,ion as follows. For every otlier process Pj, process 
first checks 
whether Pj is currently in the doorway at, line 25. If Pj is in the doorway, then Pi 
waits for Pj to get out, of the doorway. At lines 26-29, Pi waits for t,he nu.mher[j] 
to be 0 or (wunzber[i],i) < (nvrnber[,j],j). When Pi is successful in verifying this 
condition for all other processes, it can enter the critical section. 
We first prove the assertion: 
(Al) If a process P, is in critical section and some other process P k  has already 
chosen it.s number, then (number[i], < (nr~mber[k], 
k ) .  
If the process P, is in critical section, then it managed to get out of the kth iter- 
ation of the for loop in the second step. This implies that either (number[k] = 0) 

2.3. LAMPORT'S BAKERY ALGORITHM 
25 
1 class Bakery implements Lock { 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 1 
int N; 
boolean [ I  choosing ; // i n s i d e  doorway 
int [ I  number; 
public Bakery (int numProc) { 
N = numProc; 
choosing = new boolean [ N] ; 
number = new int 
for ( i n t  j = 0 ;  j < 
j + + )  { 
choosing [ j ] = false ; 
number [ j ] = 0 ;  
I 
} public void requestCS(int i )  { 
// step 1 :  d o o r w a y :  choose a numher 
choosing [ i ] = true ; 
for (int j = 0; j < N; j++) 
if ( number [ j ] > number [ i ] ) 
number [ i ] = number [ j ] ; 
number [ i I++; 
choosing [ i ] = false ; 
// step 2 :  check if 
n.umber i s  the smallest 
for ( i n t  j = 0; j < N; j + + )  { 
while (choosing [ j  1 )  ; // process j in doorzuuy 
while ( (  number [ j ] != 0) && 
((number(j1 < n u m b e r [ i ] )  1 1  
((number[ j ]  == number [ i ] )  && j < i ) ) )  
; // hu,sy wait 
1 
1 
I 
public void releaseCS (int i ) { // exit protocol 
number [ i ] = 0 ;  
Figure 2.7: Lamport's bakery algorithm 

26 
CIIAPlER 2. MUTUAL EXCLUSION PROBLEM 
or ((n,umher[i], 
i )  < (n.uniber[k], 
k ) )  at t,liat iteration. First assume that process 
read nurn.ber[k] 
0. 
This means that process P k  must not have finished 
choosing the numbrr yet. There are t,wo cases. Either Pk has not entercd the 
doorway or it has entered t,he doorway but not, exited yet. If 
has not entered 
t,he doorway, it will read the latest, value of number[i] and is guaranteed to have 
number[k] > number[i]. If it had entered the doorway, then this entry must be 
after Pi had checked choosing[k] because Pi waits for p k  to finish choosing before 
checking t,he condition (r?mm.ber[k] = 0) V ((number[i], 
i )  < (number[k], 
k ) ) .  This 
again means that that 
will read the latest value of nurnber[z] and t h e f o r e  
(number[i] < n,umber[k]). If ((nurn,ber[i], 
i )  < (number[k], 
k ) )  at, the kth iteration, 
this will corit,inue t,o hold because nu.rnber[i] does not change and number[k] can 
only increase. 
We now claim the assertion: 
(A2) If a process Pi is in critical section, then (numher[i] > 0). 
(A2) is true because it is clear from the program text that the value of any number 
is at least 0 and a process executes increment operation on its number at line 20 
before entering the critical section. 
Showing that the bakery algorithm sat,isfies mutual exclusion is now trivial. If 
two processes Pi and Pk are in critical section, then from (A2) we know that both of 
their numbers are nonzero. From (Al) it, follows t,hat (number[i], 
i )  < (nunzberlk], k )  
and vice versa. which 
a contradiction. 
The bakery algorithm also satisfies starvation freedom because any process that 
is waiting to enter the critical section will eventually have the smallest nonzero 
number. This process will then succeed in entering the critical section. 
It can be shown that the bakery algorithm does not make any assumptions on 
atomicity of any read or write operation. Note that the bakery a1gorit)hm does not 
use any variable t,hat can be written by more than one process. Process Pi writes 
only on variables nurnber[i] and choose[i]. 
There are two main disadvantages of the bakery algorithm: (1) it requires O ( N )  
work by each process to obtain the lock even if there is no contention, and (2) it 
requires each process to use timestamps that are unbounded in size. 

2.4. HARDWARE SOLUTIONS 
27 
2.4 Hardware Solutions 
As we have seen, pure software solutions to mutual exclusion can be quite complex 
and expensive. However, mutual exclusion can be provided quite easily with the 
help of hardware. We discuss some techniques below. 
2.4.1 Disabling Interrupts 
In a single-CPU system, a process may disable all the interrupts before entering the 
critical section. This means that the process cannot be context-switched (because 
context switching occurs when the currently running thread receives a clock inter- 
rupt when its current timeslice is over). On exiting the critical section, the process 
enables interrupts. Although this method can work for a single-CPU machine, it has 
many undesirable features. First, it is infeasible for a multiple-CPU system in which 
even if interrupts are disabled in one CPU, another CPU may execute. Disabling 
interrupts of all CPUs is very expensive. Also, many system facilities such 
clock 
registers are maintained using hardware interrupts. If interrupts are disabled, then 
these registers may not show correct values. Disabling interrupts can also lead to 
problems if the user process has 
bug such 
an infinite loop inside the critical 
section. 
2.4.2 Instructions with Higher Atomicity 
Most machines provide instructions with a higher level of atomicity than read or 
write. The testAndSet instruction provided by some machines does both read and 
write in one atomic instruction. This instruction reads and returns the old value 
of a memory location while replacing it with a new value. We can abstract the 
instruction as a testAndSet method on an object of the class TestAndSet 
shown 
in Figure 2.8. 
int myvalue = -1; 
public synchronized int testAndSet ( int newvalue) { 
int oldvalue = myvalue; 
myvalue = newvalue ; 
return oldvalue ; 
Figure 2.8: TestAndSet hardware instruction 

28 
CHAPTER 2. MlJ‘rUAL EXCLUSION PROBLEM 
If the 
instruction is available, then one can develop a very simple 
protocol for mutual exclusion as shown in Figure 2.9. 
class HWMutex implements Lock { 
TestAndSet lockFlag ; 
public void reyuestCS(int i )  { // entrg p r o t o c o l  
while ( lockFlag . t,estAndSet (1) == 1) ; 
} public void releasrCS(int i )  { // e z i t  protocol 
lockFlag . testAndSet ( 0 ) ;  
Figure 2.9: Mutual exclusion using TestAndSet 
This algorithm satisfies the niiit,ual exclusion and progress property. However, it 
does not satisfy starvat,ion freedom. Developing such a protocol is left 
an exercise. 
Sonietiincs machines provide the instruction swap, which can swap two memory 
locations in one atomic step. Its semantics is shown in Figure 2.10. The reader is 
invit,ed to design a mutual exclusion protocol using swap. 
public class Sgncli{ 
public static synchronized void swap( boolean ml, boolean m2) { 
boolean temp = ml; 
ml = 1112; 
1112 = temp; 
1 
1 
Figure 2.10: Seinantics of swap operation 
2.5 
Problems 
2.1. Show- that any of the following modifications to Peterson’s algorithm makes it 
incorrpct: 
(a) A process in Pet,erson’s algorithm sets the turn variable t,o it(se1f instead 
(b) A process sets the turn variable before setting the wantCS variable. 
of sctting it to the other process. 

2.5. PROBLEMS 
29 
:lass Dekker implements Lock { 
boolean wantCS 
= { f a l s e  , false ); 
i n t  turn = 1; 
public void requestCS(int i )  { // e n t r y  p r o t o c o l  
i n t  j = 1 - i ;  
wantCS [ i ] = true ; 
while (wantCS[ j 
{ 
i f  ( t u r n  == j ) { 
wantCS[ i ]  = f a l s e  ; 
while (turn == j ) ; // b u s y  w a i t  
wantCS [ i ] = true ; 
) 
1 
1 
public void releaseCS ( i n t  i ) { // exzt p r o t o c o l  
t u r n  = 1 - i ;  
want,CS[ i ]  = f a l s e  ; 
1 
1 
Figure 2.11: Dekker.java 
2.2. Show that Peterson’s algorithm also guarantees freedom from starvation. 
2.3. Show that the bakery algorithm does not work in absence of 
variables. 
2.4. Consider the software protocol shown in Figure 2.11 for mutual exclusion 
between two processes. Does this protocol satisfy (a) mutual exclusion, and 
(b) livelock freedom (both processes trying to enter the critical section and 
none of them succeeding)? Does it satisfy starvation freedom? 
2.5. Modify the bakery algorithm to solve k-mutual exclusion problem, in which 
at most k processes can be in the critical section concurrently. 
2.6. Give a mutual exclusion algorithm that uses atomic swap instruction. 
2.7. Give a mutual exclusion algorithm that uses TestAndSet instruction arid is 
free from starvation. 
*2.8. Give a mutual exclusion algorithm on N processes that requires O(1) time in 
absence of cont,t:ntion. 

30 
CHAPTER 2. MUTUAL EXCLUSION PROBLEM 
2.6 Bibliographic Remarks 
The mutual exclusion problem was first introduced by Dijkstra [Dij65a]. Dekker 
developed the algorithm for mutual exclusion for two processes. Dijkstra [DijGb] 
gave the first solution to the problem for N processes. The bakery algorithm is due 
to Lamport [Lam74], and Peterson’s algorithm is taken from a paper by Peterson 
[Pet81]. 

Chapter 3 
Synchronization Primitives 
3.1 Introduction 
All of our previous solutions to the mutual exclusion problem were wasteful in one 
regard. If a process is unable to enter the critical section, it repeatedly checks for 
the entry condition to be true. While a process is doing this, no useful work is 
accomplished. This way of waiting is called busy wait. Instead of checking the entry 
condition repeatedly, if the process checked the condition only when it could have 
become true, it would not waste CPU cycles. Accomplishing this requires support 
from the operating system. 
In t,his chapter we introduce synchronization primitives that avoid busy wait. 
Synchronization primitives are used for mutual exclusion 
well 
to provide order 
between various operations by different threads. Although there are many types of 
synchronization const tucts in various programming languages, two of them are most 
prevalent: semaphores and monitors. We discuss these constructs in this chapter. 
3.2 Semaphores 
Dijkstra proposed the concept of semaphore that solves the problem of busy wait. 
A semaphore has two fields, its value and a queue of blocked processes, and two 
operations associated with it - 
P() and V(). The semantics of a binary semaphore 
is shown in Figure 3.1. The value of a semaphore (or a binary semaphore) can be 
only false or true. The queue of blocked processes is initially empty and a process 
may add itself to the queue when it makes a call to Po. When a process calls P() 
and value is true, then the value of the semaphore becomes false. However, if 

32 
CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
the value of the semaphore is false, then the process gets blocked at line 7 until it 
becomes true. The invocation of U t i l  .mywait () at line 8 achieves this. The class 
U t i l  is shown in the appendix, but for now simply assume that this call inserts the 
caller process into the queue of blocked processes. 
When the value becomes true, thc process can make it false at, line 9 and return 
from PO. The call to V ( )  makes the value true and also notifies a process if the 
queue of processes sleeping on that semaphore is nonempty. 
1 public class BinarySernaphore { 
2 
boolean value ; 
3 
BinaryScniaphore (boolean initvalue ) { 
4 
value = initValue ; 
f i  
public synchronized void P ( )  { 
7 
while ( v a l u e  == f a l s e )  
8 
U t i l  . mywait( t h i s  ) ;  // ~n queue 
b l o c k e d  p r o c e s s e s  
9 
value = f a l s e ;  
5
}
 
10 
11 
public synchronized void V() { 
12 
value = t r u e ;  
13 
notify ( ) ;  
15 
Figure 3.1: Binary semaphore 
Now, mutual exclusion is almost trivial t,o implement: 
BinarySernaphore mutex = new BinarySemaphore(true1; 
mutex.P(); 
c r i t i c a l S e c t i o n 0 ;  
mutex.V(); 
Another variant of semaphore allows it to take arbitrary integer 
its value. 
1 hese semaphores are called countin,g semupphores. Their semantics is shown in 
Figure 3.2. 
Semaphores can be used t,o solve a wide variety of synchronization problems. 
Note that Java does not provide semaphores as basic language constnict, but they 
can easily be iniplemented in Java using the idea of monitors, which we will cover 
later. For now we simply assume that semaphores are available t,o us and solve 
synchronizat,ion problems using them. 
r
1
 

3.2. SEMAPHORES 
public class CountingSeniaphore { 
int value; 
public CountingSeniaphore (int initvalue ) { 
value = initvalue; 
1 
public synchronized void P ( )  { 
value --; 
if (v.alue < 0 )  Util .mywait( t h i s ) ;  
1 
public synchronized void V() { 
value ~t+; 
if (v.slue <= 0) notify ( ) ;  
1 
1 
Figure 3.2: Counting semaphore 
3.2.1 The Producer-Consumer Problem 
We first consider the producer-consumer problem. In this problem, there is a shared 
buffer between two processes called the producer and the consumer. The producer 
produces items that .are deposited in the buffer and the consumer fetches items from 
the buffer and consuimes them. For simplicity, we assume that our items are of type 
double. Since the buffer is shared, each process must access the buffer in a mutually 
exclusive fashion. We use an array of double of size size as our buffer. The buffer 
has two pointers, iriBuf and outBuf , which point to the indices in the array for 
depositing an it.em and fetching an item, respectively. The variable count keeps 
track of the number of items current.ly in the buffer. Figure 3.3 shows the buffer as 
a circular array in which inBuf and outBuf are incremented modulo s i z e  to keep 
track of the slots for depositing and fetching items. 
In this problem, we see that besides mutual exclusion, there are two additional 
synchronization constraints that need to be satisfied: 
1. The consumer should not fetch any item from an empty buffer. 
2. The producer should not deposit any item in the buffer if it is full. The buffer 
can become full if the producer is producing items at a greater rate than the 
rate at which the items are consumed by the consumer. 
Such form of synchronization is called conditional synchronization. It requires a 
process to wait for some condition to become true (such as the buffer to become 
nonempty) before continuing its operations. The class BoundedBuf f e r  is shown in 
Figure 3.4. It uses mutex semaphore to ensure that all shared variables are accessed 

itiBuf 
CHAPTER 3. SYNCHRONIZATION PRIhlITlVES 
Figure 3.3: A shared buffer implemented with a circular array 

3.2. SEMAPHORES 
in mutually exclusive fashion. The counting semaphore 
is used for making a 
producer wait in case the buffer is full, and the semaphore 
is used to make 
a consumer wait when the buffer is empty. 
In the method 
line 10 checks whether the buffer is full. If it is, the 
process making call waits using the semaphore 
Note that this semaphore 
has been init,ialized to the value 
and therefore in absence of a consumer, first 
calls to 
do not block. At this point, the buffer would be full and 
any call to 
will block. If the call to 
does not block, then 
we enter the critical section to access the shared buffer. The call 
at line 
serves as entry to the critical section, and 
serves as the exit from the 
critical section. Once inside the critical section, we deposit the value in 
using 
the pointer 
at line 
(see Figure 3.4). Line 15 makes a call to 
to wake up any consumer that may be waiting because the buffer was empty. The 
method 
is dual 
the method 
1 class BoundedBuffer { 
2 
final int size = 10; 
3 
double[] buffer = new double[ size 1 ;  
4 
i n t  inBuf = 0, outBuf = 0; 
5 
BinarySeniaphore mutex = new BinarySemaphore ( true ) ; 
6 
Countingsemaphore isEmpty = new CountingSemaphore (0) ; 
7 
Countingsemaphore isFull = new Countingsemaphore ( size ) ;  
8 
9 
public void deposit (double value) { 
10 
isFull . P O ;  // wait if buffer is f u l l  
11 
mutex.P(); // ensures mutual ezclusion 
12 
buffer [ inBuf] = value ; // update the buffer 
13 
inBuf = (inBuf + 1) % s i z e  ; 
14 
mutex.. V() ; 
15 
isEmpty . V( ) ; // n o t i f y  any waiting consumer 
16 
} 
17 
public double fetch () { 
18 
double value ; 
19 
isEmpty.P(); // wait if buffer is empty 
20 
mutex. P ( )  ; // ensures mutual ezclusion 
21 
value = buffer [ outBuf] ; //read from buffer 
22 
outBuf = (outBuf + 1) % size; 
23 
mutex:. V( ) ; 
24 
isFull . V ( ) ;  // n o t i f y  any waiting producer 
25 
return value ; 
26 
1 
27 1 
Figure 3.4: Bounded buffer using semaphores 

36 
CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
The class 
f 
can be exercised through the producer-consumer pro- 
gram shown in Figure 3.5. This program starts a 
thread and a 
t,hread, repeatedly making calls to 
and 
respectively. 
3.2.2 The Reader-Writer Problem 
Next we show the solution to the reader-writer problem. This problem requires us 
to design a protocol to coordinate a 
ss to a shared database. The requirements 
are 
follows: 
1. No reud-write confiict: The protocol should ensure that a reader and a writer 
do not access the database concurrently. 
2. No write-write conflict: The protocol should ensure that two writers do not 
access t,he database concurrently. 
Furt,her, we would like multiple readers to be able to access the database con- 
curredy. A solut,ion using semaphores is shown in Figure 3.6. We assume that the 
readers follow the protocol that they call 
before reading the database 
and call 
after finishing the read. Writers follow a similar protocol. We 
use the 
semaphore to ensure that either there is a single writ,er accessing the 
database or only readers are accessing it. To count tlie number of readers accessing 
the database, we use the variable 
The methods 
and 
are quite simple. Any writer that wants 
to use the database locks it. using 
If the database is not locked, this 
writer gets the access. Now no other reader or writer can access the database until 
this writ,er releases the lock using 
Now let us look at the 
and the 
methods. In 
a reader first increments 
If it is the first reader 
equals 
l), then it needs to lock the database; otherwise, there are already other readers 
accessing the database and this reader can also start using it. In 
the 
variable 
is decremented and the last reader to leave the database unlocks 
it, using the call 
V () . 
This protocol has tlie disadvantage that a writer may starve in the presence 
of continuously arriving readers. A st,arvation-free solution to the reader-writer 
problem is left 
an exercise. 
3.2.3 The Dining Philosopher Problem 
This problem, first posed and solved by Dijkstra, is useful in bringing out issues 
associated with concurrent programming and symmetry. The dining problem con- 
sists of multiple philosophers who spend their time thinking and eating spaghetti. 

3.2. SEMAPHORES 
I 
import j a v a .  u t i l  .Random; 
c l a s s  Producer implements Runnable { 
BoundedBuffer b = n u l l  ; 
public Producer ( BoundedBuffer i n i t b  ) { 
b = i n i t b ;  
new T h r e a d ( t h i s ) .  s t a r t  ( ) ;  
I 
public void run ( )  { 
double item; 
Random r = new Random(); 
while ( t r u e )  { 
item = r . nextDouble ( )  ; 
System. o u t .  p r i n t l n  ("produced item " + item); 
b. deposit ( i t e m ) ;  
Uti.1. mysleep (200); 
I 
} 
I 
c l a s s  Consumer implements Runnable { 
BoundedBuffer b = n u l l  ; 
public Conmmer( BoundedBuffer i n i t b  ) { 
b = i n i t b ;  
new T h r e a d ( t h i s ) .  s t a r t  ( ) ;  
1 
public void run ( )  { 
double item; 
while ( t r u e )  { 
item = 1). fetch ( ) ;  
System. o u t .  p r i n t l n  ("fetched item " + i t e m ) ;  
Util . mysleep (50); 
1 
I 
1 
c l a s s  ProducerConsunier { 
public s t a t i c  void main(String [ I  a r g s )  { 
BoundeiSBuffer buffer = new BoundedBuffer ( )  ; 
Producer produccr = new Producer ( b u f f e r  ); 
Consumer consnmer = new Consumer( buffer ); 
1 
1 
Figure 3.5: Producer-consumer algorithm using semaphores 

I 
bublic void endwrite 
f 
CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
class Readerwriter { 
int niimReaders = 0: 
BinarySeniaphore mutex = new BiiiarySemaphore (true); 
BinarySemaphore wlock = new BinarySemaphore (true ) ; 
public void startRead ( )  { 
mutrx. P ( )  ; 
numReaders++; 
if (numReaders == 1) w l o r k . P ( ) ;  
miitex. 
) ; 
1 
public void endRead() { 
mutex. P ( )  ; 
nuniReaders --; 
if ( numReaders == 0) wlock . V ( )  ; 
rnutex . V() ; 
} public void s t a r t w r i t e  () { 
wlock . P O ;  
public void endwrite ( )  { 
wlock . V ( )  ; 
1 
1 
Figure 3.6: Reader-writer algorithm using seniaphores 

3.2. SEMAPHOltES 
39 
Figure 3.7: The dining philosopher problem 

CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
However, a philosopher requires shared resources, such as forks, to eat spaghetti 
(see Figure 3.7). We are required to devise a protocol to coordinate access to the 
shared resources. A computer-minded reader may substitute processes for philoso- 
phers and files for forks. The task of eating would then corrcspond to an operation 
tihat requires access to shared files. 
:lass Philosopher implements Runliable { 
int id = 0 ;  
Resource r = null; 
public Philosopher ( i n t  initld , Resource initr ) { 
id = i n i t I d ;  
r = initr ; 
new Thread(this). sttart ( ) ;  
} public void run ( )  { 
while ( t r u e )  { 
try { 
System. out. println (”Phil ” + id + ” thinking”); 
Tliread . sleep (30) ; 
System. o u t .  print,ln (”Phil ” + id + ” hungry”); 
r .  acquire (id ) ;  
System. out. println (”Phil ” + id + ” eat,ing”); 
Thread. sleep ( 4 0 )  ; 
r .  release ( i d  ) ;  
return; 
} catch ( InterruptedException 
e )  { 
t 
Figure 3.8: Dining Philosopher 
Let, 11s first model the process of a philosopher. The class 
is shown 
in Figure 3.8. Each philosopher Pi repeatedly cycles through the following states 
~ 
thinking, hmgry, arid eating. To eat, a philosopher requires resoiirces (forks) 
for which it makes call to 
Thus, t,he protocol to acquire resources is 
abstracted as an interface 
shown in Figure 3.9. 
The first attzerript t,o solve this problem is shown in Figure 3.10. It uses a binary 
semaphore for each of the forks. To acquire the resources for eating, a philosopher i 
grabs the fork on its left by using 
at line 12, and the fork on the right 
by iising 
% nl 
at line 13. To release the resources, the philosopher 
invokes 
on both the forks at lines 16 arid 17. 
This att,empt, illustrat,es the dangers of symmetry in a distributed system. This 

3.2. SEMAPHORES 
41 
public void acquire ( i n t  i ); 
public void release ( i n t  i ); 
Figure 3.9: 
Interface 
class DiningPhilosopher implements Resource { 
2 
int n = 0; 
3 
BinarySemaphore [I fork = null; 
4 
public DiningPhilosopher ( i n t  initN ) { 
5 
n = i n i t N ;  
6 
fork = new BinarySemaphore [ n ]  ; 
7 
for ( i n t  i = 0 ;  i < n ;  i++) { 
8 
9 
t 
fork [ i ] = new BinarySemaphore ( t r u e ) ;  
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
1 
public void acquire ( i n t  i )  { 
fork [ i 1 .  PO; 
fork [ (  i + 1) % .].PO; 
1 
public void 
fork [ i 1 .  
fork [ (  i 
release ( i n t  i )  { 
vo; 
+ 1) % n ] . V ( ) ;  
bublic s t a t i c  void main( String [ I  
args) { 
DiningPhilosopher dp = new DiningPhilosopher ( 5 ) ;  
for ( i n t  i = 0 ;  i < 5 ;  i++) 
new Philosopher ( i  , d p ) ;  
t 
Figure 3.10: Dining philosopher using semaphores 

42 
CHAPlER 3. SYNCHRONIZATION PRIMITIVES 
protocol can result in deadlock when each philosopher is able to grab its left fork 
and then waits for its right neighbor to release its fork. 
There are many ways t,hat one can extend t,he solution to ensure frecdorn from 
deadlock. For example: 
1. We can introduce asymmeth-y by requiring one of the philosophers to grab 
forks in a different. order (i.e., tlie right fork followed by the left. fork instead 
of vice versa). 
2. We can require philosophers to grab both the forks at the same time. 
3. Assume that a philosopher has to stand before grabbing any fork. ,4110~ at 
most four philosophers to be st’anding at any given time. 
It is left as an exercise for t,he reader to design a protocol t,hat, is free from deadlocks. 
The dining philosoplier problem also illustrates the distinction between deadlock 
freedom arid starvation frcedorn. Assume that we require a philosopher to grab both 
t,lie forks at, the sanie time. Although this eliminates deadlock, we st.ill have t’he 
problem of a philosopher being starved because its neighbors continuously alternate 
in eat,ing. The reader is invited to come up with a solution that is free from deadlock 
as wcll as starvat,ion. 
3.3 Monitors 
The Monitor is a high-level object-oriented construct for synchronization in con- 
current. programming. A monitor can be viewed as a class that can be used in 
coric:urrent programs. As any class, a monitor has data variables and methods 
t,o manipulate that data. Becaiise multiple threads can access tlie shared data at 
t,lie sanie time, monitors support. the not,ion of entry methods to guarantee mutual 
cxclusion. It is guaranteed that, at most. one thread can be executing in any entry 
niethotl at. any tsinie. Sornctimes t>lie phrase “t,hread t is inside the monitor” is used 
t.o denote that, thread t is executing an entry method. It is clear that at most one 
thread can be in the monitor at any time. Thus associated wit’h every nionit,or 
object is a queue of threads that are wait’ing to enter the monitor. 
As we have seen before, concurrent programs also require conditional synchro- 
nizatiorr when a thread must wait. for a certain condition to become true. To address 
condit,ional synchronization, t,he rnonitor construct, supports the notion of condition 
variubles. A condition variable has t,wo operat,ioris defined on it: wait and notify 
(also called a siynal). For any condition variable 5 ,  any thread, say, t l ,  t,hat makes 
ii call to rc.urait() is blocked and put int,o a queue associated with 2. When an- 
other t,hread, say, t 2 ,  makes a call t.o z.nolify(), if the queue associated with .7: 

3.3. MONITORS 
is nonempty, a thread is removed from the queue and inserted into the queue of 
threads that are eligible to run. Since at most one thread can be in the monitor, 
this immediately poses a problem: which thread should continue after the notify 
operation-the 
one that called the notify method or the thread that was waiting. 
There are two possible answers: 
1. One of the threads that was waiting on the condition variable continues exe- 
cution. Monitors that follow this rule are called Hoare monitors. 
2. The thread that made the notify call continues its execution. When this thread 
goes out of the monitor, then other threads can enter the monitor. This is the 
semantics followed in Java. 
One advantage of Hoare’s monitor is that the thread that was notified on the 
condition starts its execution without intervention of any other thread. Therefore, 
the state in which this thread starts executing is the same as when the notify was 
issued. On waking up, it can assume that the condition is true. Therefore, using 
Hoare’s mointor, a thread’s code may be 
Assuming that t 2  notifies only when B is true, we know that tl can assume B on 
waking up. In Java-style monitor, even though t 2  issues the notify, it continues its 
execution. Therefore, when tl gets its turn to execute, the condition B may not, be 
true any more. Hence, when using Java, the threads usually wait for the condition 
The thread tl can take a 
only 
a hint that B may be true. Therefore, 
it explicitly needs to check for truthness of B when it wakes up. If B is actually 
false, it issues the 
(1 call again. 
In Java, we specify an object to be a monitor by using the keyword 
with its methods. To get conditional synchronization, Java provides 
which inserts the thread in the wait queue. For simplicity, we use 
instead of 
in Java. The only difference is that 
catches the 
which wakes up a thread in the wait queue. 
which wakes up all the threads in the wait queue. 

CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
Java does not have condition variables. Thus, associated with each object there 
is a single 
queue for conditions. This is sufficient for most programming needs. 
If one needs, it is also easy to simulate condition variables in Java. A pictorial 
representation of a Java monitor is shown in Figure 3.11. There are two queues 
associated with an object- a queue of threads waiting for the lock associated with 
the monitor and another queue of threads waiting for some condition to become 
true. 
DATA 
Synchronized method 
Synchronized method 
Nonsynchronized method 
Nonsynchronized method 
fi 
Queue of processes 
waiting for monitor lock 
Figure 3.11: A pictorial view of a Java nionit,or 
Let us solve some synchronization problems with Java monitors. We first look 
at the producer-consumer problem. The 
f 
shown in Figure 
3.12 has two entry methods: 
and 
This means that if a thread is 
executing the method 
or 
then no other thread can execut,e 
or 
The 
keyword at lines 5 and 14 allows niutual exclusion 
in access of shared variables and corresponds to acquiring the monitor lock. Let us 
now look at the method 
At line 
if the buffer is full, (i.e., 
is equal 
t.o 
then the thread that called 
must wait for a slot in the 
to tie consunled. Therefore, it invokes the method 
When a thread waits 
for the condition, it, goes in a queue waiting to be notified by some other thread. 
It, also has to release the monitor lock so that other threads can enter the monitor 
and make the condition on which this thread is waiting true. When bhis thread is 

3.3. MONITORS 
45 
notified, it has to acquire the monitor lock again before continuing its execution. 
Now assume that the condition in the while statement at line 6 is false. Then 
the value can be deposited in the buffer. The variable inBuf points to the tail 
of the circular buffer. It is advanced after the insertion at line 9 and the count 
of the number of items in the buffer is incremented at line 10. We are not really 
done yet. While designing a monitor, one also needs to ensure that if some thread 
may be waiting for a condition that may have become true, then that thread must 
be notified. In this case, a consumer thread may be waiting in the method fetch 
for some item to become available. Therefore, if count is 1, we notify any waiting 
thread at line 12. 
The method fetch is very similar to deposit. 
1 class BoundedBufferMonitor { 
2 
3 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
final int sizeBuf = 10; 
double [ I  buffer = new double [ sizeBuf ] ; 
int inBuf = 0 ,  outBuf = 0 ,  count = 0; 
public synchronized void deposit (double value ) { 
while (count == sizeBuf) // b u f f e r  f u l l  
Util . mywait( this ) ;  
buffer [ inBuf] = value ; 
inBuf = (inBuf + 1 )  % sizeBuf; 
count ++; 
if (count == 1 )  // i t e m s  a v a i l a b l e  f o r  f e t c h  
notify ( ) ;  
} public synchronized double fetch ( )  { 
double value ; 
while (count == 0) // b u f f e r  e m p t y  
value = buffer [ outBuf ] ; 
outBuf = (outBuf + 1 )  % sizeBuf; 
count --; 
if (count == sizeBuf - 1) // e m p t y  s l o t s  a w a z l a b l e  
notify (); 
return value : 
Util . mywait( this ) ;  
Figure 3.12: Bounded buffer monitor 
Now let us revisit the dining philosophers problem. In the solution shown in 
Figure 0.13, a philosopher 
uses the method t e s t  at line 18 to determine if any of 
neighboring philosophers is eating. If not, then this philosopher can start eating. 
Otherwise the philosopher must wait for the condition (stateCi1 == eating) at 

46 
CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
line 19 to begin eating. This condition can become true when one of the neighbor- 
ing philosophers finishes eating. After eating, the philosopher invokes the method 
t,o check at. lines 24 and 25, whether the left neighbor or the right neighbor 
can now eat. If any of them can eat, this philosopher wakes up all the waiting 
philosophers by invoking 
at line 32. This solution guarantees mutual 
exclusion of neighboring philosophers and is also free from deadlock. However, it, 
does not. guarantee freedom from starvation. The reader should devise a protocol 
for starvation freedom. 
Other Examples 
In this section we give another example of concurrent programming in Java. Figure 
3.14 shows a thread-safe implementation of a queue that is based on a linked list. 
The class 
declared at line 2 contains a String 
data and the reference to the 
next, node in the linked list. To 
data, we first create a 
node at line 8. 
This node is inserted at the 
If the linked list is empty, this is the only node 
in the linked list and both 
and 
are made to point to this node at lines 
To 
a node, a t,hread must wait at line 22 if 
is null (the linked 
list is empty). Otherwise, the data in t,he 
node is returned and 
is moved 
t.o t,he 
node. 
As ment,ioned earlier, whenever a t,hread needs to execute a synchronized method, 
it, needs to get the monitor lock. The keyword 
can also be used with 
any statenlent as 
The expression 
must re- 
sult in a reference to an object on evaluation. The semantics of the above construct 
is that the 
can be executed only when the thread has t8he lock for t,he 
object given by the 
Thus a synchronized method 
{ 
( ) ; 
> 
can simply be viewed as a short form for 
{ 
{ 
1 
; 
3 
Just as nonstatic methods can be 
so can the static methods. A 
static met,hod results in a classwide lock. 

3.4. OTHER EXAMPLES 
1 class DiningMonitor implements Resource { 
2 
int n = 0; 
3 
int s t a t e  
= null; 
4 
static final int thinking = 0 ,  hungry = 1, eating = 2; 
5 
public DiningMonitor (int initN ) { 
6 
n = i n i t N ;  
7 
s t a t e  = new int [ n ] ;  
8 
for ( i n t  i = 0 ;  i < n ;  i + + )  s t a t e  [ i ]  = t h i n k i n g ;  
9
)
 
12 
1 
15 
1 
10 
int l e f t  ( i n t  i )  { 
11 
return (11 + i - I )  % 11; 
13 
int r i g h t  ( i n t  i )  { 
14 
return ( i  + 1) % n; 
16 
public synchronized void acquire (int i ) { 
17 
s t a t e  [ i ]  = hungry; 
18 
t e s t  ( i  ); 
19 
while ( s t a t e  [ i ]  != r a t i n g )  
20 
Util . mywait( this ); 
22 
public synchronized void release (int i )  { 
23 
s t a t e  [ i ]  = thinking ; 
24 
25 
t e s t  ( r i g h t  ( i  ) ) ;  
27 
void t e s t  (int i )  { 
28 
if ( (  stat.e [ l e f t  ( i  ) ]  != e a t i n g )  && 
29 
( s t a t e  [ i ]  == hungry) && 
30 
( s t a t e  [ r i g h t  ( i  ) ]  != eating ) )  { 
31 
s t a t e  [ i ]  = eating ; 
32 
notifyAl1 ( ) ;  
21 
j 
t e s t  ( l e f t  ( i  ) ) ;  
26 
1 
33 
1 
34 
} 
35 
public static void main( String 
a r g s )  { 
36 
DiningMonitor 
= new DiningMonitor ( 5 )  ; 
37 
for ( int i = 0; i < 5 ;  i++) 
38 
new Philosopher ( i  , 
39 
} 
40 1 
Figure 3.13: Dining philosopher using monitors 

CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
1 public class List,Queue { 
2 
class Node { 
3 
public String data ; 
4 
public Node next.; 
6 
Node 
= null, t a i l  = null; 
7 
public synchronized void enqueue ( S t r i n g  d a t a )  { 
8 
Node temp = new Node ( ) ;  
9 
t,eiiip. data = data ; 
10 
temp. next = null; 
11 
if ( t a i l  == null) { 
12 
t,ail = t,emp; 
13 
head = t a i l  ; 
14 
} else { 
15 
t a i l  . next = temp; 
16 
t,ail = temp; 
18 
notify ( ) ;  
20 
public synchronized String dequeue ( )  { 
21 
while ( head == null) 
22 
Util .niyWait(this); 
2 3 
String returnval = head. d a t a ;  
24 
head = head. npxt ; 
25 
return rcturrival ; 
5
1
 
17 
1 
19 
1 
26 
1 
27 1 
Figure 3.14: Linked list 

3.5. DANGERS OF DEADLOCKS 
~~~~~~ 
~ 
~ 
~ 
class BCell { // can r e s u l t  
d e a d l o c k s  
int value; 
public synchronized int getvalue ( )  { 
return value ; 
} 
} 
public synchronized void setvalue ( int i ) { 
value = i ;  
public synchronized void swap(BCel1 x )  { 
int temp = getvalue (); 
setvalue ( x .  getvalue 
x. setvalue (temp); 
} 
1 
One also needs to be careful with inheritance. When an extended class over- 
rides a synchronized method with an unsynchronized method, the method of the 
original class stjays synchronized. Thus, any call to super.method0 will result in 
synchronization. 
3.5 Dangers of Deadlocks 
Since every synchronized call requires a lock, a programmer who is not careful can 
introduce deadlocks. For example, consider the following class that allows a cell t.0 
be swapped with the other cell. An object of class BCell provides three methods: 
getvalue, setvalue and swap. Although the implementation appears correct at 
first glance, it suffers from deadlock. Assume that we have two objects, p and q, 
as instances of class BCell. What happens if a thread tl invokes p.swap(q) and 
another thread, say, t 2 ,  invokes q. swap(p) concurrently? Thread tl acquires the 
lock for the monitor object p and tz acquires the lock for the monitor object q. Now, 
t,hread tl invokes q.getValue0 
part of the swap method. This invocation has 
to wait because object q is locked by tz. Similarly, t 2  has to wait for the lock for p ,  
and we have a deadlock! 
The program that avoids the deadlock is given below. It employs a frequently 
used strategy of totally ordering all the objects in a system and then acquiring locks 
only in increasing order. In this program, both p. swap(q) and q. swap(p) result in 
either p.doSwap(q) or q.doSwap(p), depending on the identityHashCode value of 
thc objects p and 

CHAPTER 3. SYNCHRONIZATION PRIhlITIVES 
class Cell { 
int valce; 
public synchronized int getvalue ( 
return value : 
1 
1 
public synchronized void s e t v a l u e  ( i n t  i ) { 
value = i ;  
protected synchronized void doSwap( Cell x )  { 
int temp = getvalue ( ) ;  
srtValue ( x .  getValtie 
x .  setvalue (temp); 
1 
public void swap( Cell 
{ 
if ( t h i s  == x )  
return ; 
else if ( System. ideiitityHashCode ( t h i s )  
< System ideiitityHashCode ( x ) )  
doSwap(x ) : 
x .  doSwap( t h i s ) ;  
else 
1 
Soinr other useful methods in Java Thread class are as follows: 
1. Thc interrupt 
method allows a thread to be interrupted. If thread tl calls 
f2.infer~rupt(), 
then tz gets an InterruptedException. 
2. The y i e l d 0  method allows a t,hread to yield the CPU to other threads t,em- 
porarily. It. does not require any int,eraction with ot,her threads, and a program 
wit,hout y i e l d 0  would be funct,ionally equivalent to yield() call. A thread 
may choose to yield() if it is waiting for some data to become available from 
say Inputstream. 
3 .  The method holdsLock(x) returns true if the current thread holds the monitor 
lock of the object 2 .  
3.6 Problems 
3.1. Show that if the P O  and V O  operations of a binary semaphore are not exe- 
cutrd atomically, then mutual cxclusion may be violated. 
3.2. Show that a counting semaphore can be implemented using binary semaphorrs. 
(Hznt: Use a sharrd variable of type integer and two binary semaphorrs) 

3.7. BIBLIOGRAPHIC REMARKS 
3.3. Give a starvation-free solution to the reader-writer problem using semaphores. 
3.4. The following problem is known as the sleeping barber problem. There is one 
thread called barber. The barber cuts the hair of any waiting customer. If there 
is no customer, the barber goes to sleep. There are multiple customer threads. 
A customer waits for the barber if there is any chair left in the barber room. 
Otherwise, the cust.omer leaves immediately. If there is a chair available, t<hen 
the customer occupies it. If the barber is sleeping, then the customer wakes 
the barber. Assume that there are n chairs in the barber shop. Write a Java 
class for SleepingBarber using semaphores that allows the following methods: 
// 
// 
How will you extend your algorit,hm to work for the barber shop with multiple 
barbers. 
3.5. Give a deadlock-free solution to the dining philosophers problem using semaphores. 
Assume that one of the philosophers picks forks in a different order. 
3.6. Assume that there are three threads-P, 
Q, and R-that 
repeatedly print 
“P” , “Q”, and “R’ respectively. Use seniaphores to coordinate the printing 
such t,hat the number of “ R  printed is always less than or equal to the sum 
of “P” and “Q” printed. 
3.7. Write a monitor for the sleeping barber problem. 
3.8. Show how condition variables of a monitor can be implemented in Java. 
3.9. Write a monitor class 
that allows a process to sleep until the counter 
reaches a certain value. The 
class allows two operations: 
() 
and 
3.10. Write a Java class for 
with a minimum and a maximum value. 
This class provides two methods: 
and 
Decrement 
at the minimum value and increment at the maximum value result in the 
calling thread waiting until the operation can be performed without violating 
the bounds on the counter. 
3.7 Bibliographic Remarks 
The semaphores were introduced by Dijkstra [Dij65a]. The monitor concept, was in- 
troduced by Brinch Hansen [Ha11721 and the Hoare-style monit.or, by Hoare [Hoa74]. 

52 
CHAPTER 3. SYNCHRONIZATION PRIMITIVES 
Solutions to classical synchronization problems in Java are also discussed in the book 
by Hartley [Har98]. The example of deadlock and its resolution based on resource 
ordering is discussed in the book by Lea [Lea99]. 

Chapter 4 
Consistency Conditions 
Introduction 
In the presence of conmrrency, one needs to revisit the correctness conditions of 
executions, specifically, which behaviors are correct when multiple processes invoke 
methods concurrently on a shared object. Let us define 
concurrent object as one 
that allows multiple processes to execute its operations concurrently. For example, 
a concurrent queue in 
shared memory system may allow multiple processes to 
invoke enqueue and dequeue operations. The natural question, then, is to define 
which behavior of the object under concurrent operations is consistent (or correct). 
Consider the case when a process P enqueues z in an empty queue. Then it calls the 
method dequeue, while process Q concurrently enqueues y. Is the queue's behavior 
acceptable if process P gets y as the result of dequeue? The objective of this chapter 
is to clarify such questions. 
The notion of consistency is also required when objects are replicated in parallel 
or distribut'ed system. There are two reasons for replicating objects: fault t,olerance 
and efficiency. If an object has multiple copies and 
processor that contains one of 
the copies of the object goes down, the system may still be able to function correctly 
by using other copies. Further, accessing remote object may incur large overhead 
because of communication delays. Suppose that we knew that most accesses of the 
object are for read only. In this case, it may be better to replicate that object. A 
process can read the value from the replica that is closest to it in the system. Of 
course, when we perform 
write on this object, we have to worry about consistency 
of data. This again requires us to define data consistency. Observe that any system 
that uses caches, such as 
multiprocessor system, also has to grapple with similar 

54 
CHAPTER 4. CONSISTENCY CONDITIONS 
issues. 
4.2 System Model 
A concurrent system consists of a set of sequential processes that communicate 
through concurrent objects. Each object has a name and a type. ‘The type dcfines 
the set of possible values for objects of this type and the set of primitive operations 
that provide the only means bo manipulate objects of t,his type. Execution of an op- 
eration t,akes some time; this is modeled by t.wo events, namely, an invocation event 
and a response event. Let 
be an operation on object z issued at P; arg and 
res denote op’s input and output parameters, respectively. Invocation and response 
events iiz71(op(arg)) on z at P and 
from 5 at P will be abbreviated 
and resp(op) when parameters, object name, and process identity are not 
necessary. For any operation e, we use proc(e) t,o denote the process and object(e) to 
denote the set of objects associated with the operation. In this chapter, we assume 
t,hat all operat,ions are applied by a single process on a single object. In the problem 
set, we explore generalizations to operations that span multiple objects. 
A history is an execution of a concurrent system modeled by a directed acyclic 
graph ( H ,  < H ) ,  where H is the set, of operations and <H is an irreflexive transitive 
relation t,hat, captures the occurred before relation between operat,ions. Sometimes 
we simply use H to denote the history when <H is clear from the context. Formally, 
for any t>wo operations e and f: 
e <H 
if 
resp(e) occurred before 
in real time. 
Observe that. t,his relation includes t,he following relations: 
Process order: (proc(e) = proc(f)) A (resp(e) occurred before in,u(f)). 
Object order: (ohject(e) n object(f) # 0) A (resp(e) occurred before 
A process subhistory HIP ( H  at P )  of a history H is a sequence of all t,hose 
events e in H such that proc(e) = P. An object subhistory is defined in a similar 
way for an object z, denoted by Hlz ( H  at, z). Two histories are equivalent if they 
are composed of exactly t,hr sarri 
A hist.ory ( H ,  < H )  is a sequential history if <H is a total order. Such a history 
would happen if there was only one sequential process in the system. A sequential 
history is legal if it meets t,he sequential specification of all the objects. For example, 
if we are considering a read-write register 2 
a shared object, then a sequential 
history is legal if for every read operation that, retjurns its value 
TI, there exists a 
write on that object with value u, and there does not exist another write operation 
t of invocation and response events. 

4.3. SEQUENTIAL CONSISTENCY 
55 
on that object with a different value betwecn the write arid the read operations. 
For a sequential queue, if the queue is nonempty then a 
operation should 
return the item that was enqueued earliest and has not been already dequeued. If 
the queue is empty, then the dequeue operation should return null. 
Our goal is to determine whether a given concurrent history is correct. 
Sequential Consistency 
Definition 4.1 (Sequentially Consistent) A history ( H ,  <ff) is sequentially con- 
sistent if there exists a sequenbial history S equivalent to H such that S is legal and 
it satisfies process order. 
Thus a. history is sequentially consistent if its execution is equivalent to a legal 
sequential execution and each process behavior is identical in the concurrent and 
sequential execution. In the following histories, P, Q, and R are processes operating 
on shared registers 5 ,  
and z. We assume that all registers have 0 initially. The 
response of a read operation is denoted by ok(u), where 
is the value returned, 
and the response of a write operation is denoted by ok(). The histories are shown 
graphically in Figure 4.1. 
1. HI = P write(z, l), Q read(z), Q ok(O), P ok(). 
Note that H1 is a concurrent history. Q invokes the read(z) operation be- 
fore the write(%, 1) operation is finished. Thus write(s, 1) and read(z) are 
concurrent operations in H I .  HI is sequentially consistent because it is equiv- 
alent to the following legal sequential history. 
S = Q read(z), Q ok(O), P write(z, l), P ok(). 
To see the equivalence, note that 
HllP = SIP = P write(x, l), P ok(). 
HllQ = SlQ = Q read(z), Q ok(0). 
2. H2 = P write(z, l), P ok(), Q read(%), Q ok(0). 
Somewhat surprisingly, H2 is also sequentially consistent. Even though P 
got the response of its write before Q, it is okay for Q to have read an old 
value. Note that H2 is a sequential history but not legal. However, it is equiv- 
alent to the following legal sequential history: 
Q read(%), Q ok(O), P write(z, l), P ok(). 
3. H3 = P write(z, l), Q read(z), P ok(), Q ok(O), P read(z), P ok(0). 

56 
CIlAPTER 4. CONSISTENCY CONDITIONS 
read(x) 
ok (0) 
Q
-
 
read(x) 
ok (2) 
Q
-
 
Figure 4. I: Conciirrent histories illustrating sequential consistency 

4.4. LINEARIZABILITY 
H3 is not sequentially consistent. Any sequential history equivalent to H3 
must, preserve process order. Thus the read operation by P must come after 
the write operation. This implies that the read cannot return 0. 
4. H4 = P write(x, l), Q read(x), P ok(), Q 4 2 ) .  
H4 is also not sequentially consisbent,. There is no legal sequential hist.ory 
equivalent to H4 because the read by Q returns 2, which was never written on 
the register (and was not the initial value). 
Linearizability 
Linearizability is a stronger consist,ency condition than sequential consistency. In- 
tuitively, an execution of a concurrent system is linearizable if it could appear to 
an external observer as a sequence composed of the operations invoked by processes 
that respect object specifications and real-time precedence ordering on operations. 
So, linearizabilit,y provides the illusion that each operation on shared objects is- 
sued by concurrent processes takes effect inst,antaneoiisly at some point between the 
beginning and the end of its execution. Formally, this is stated 
follows. 
Definition 4.2 (Linearizable) A history ( H ,  < H )  is linearizable 
there exists a 
sequential history (S, <) equivalent to H such that S is legal and it preserves < H .  
Since <H includes process order, it follows that a linearizable history is always 
sequentially consistent. Let us reexamine some histories t,hat we saw earlier. 
1. H I  = P write(x, l), Q read(x), Q ok(O), P &(). 
HI is linearizable because the following legal sequential history, 
preserves < H .  
Q read(x), Q ok(O), P write(x, l), P ok() 
2. 
= P write(x, l), P ok(), Q read(s), Q ok(0). 
H2 is sequentially consistent but not linearizable. The legal sequential his- 
tory used for showing sequential consistency does not preserve < H .  
A key advantage of linearizability is that it is a local property, that is, if for all 
objects x ,  Hlz is linearizable, then H is linearizable. Sequential consistency does 
not have this property. For example, consider two concurrent queues, s and t. 
Process P enqueues x in s and t. Process Q enqueues y in t and then in s. Now P 
gets y from deq on s and Q get 
when it does deq on t. 

58 
CHAPTER 4. CONSISTENCY CONDITIONS 
s.enq(x) 
s.ok() 
t.enq(x) 
r.ok() 
s.deq() 
s.ok(y) 
(4 H 
P F 7 - - - - 3  
H 
H 
- 
s.enq(y) 
s.ok() 
t.deq() 
t.ok(x 
w 
H 
(b) HIS 
s.enq(x) 
s.ok() 
H 
(c) Hlt 
t.enq(x) 
t.ok() 
P
H
 
t.enq(y) 
t.ok() 
Q H  
s.deq() 
s.ok(y) 
H 
s.enq(y) 
s.ok() 
Q H  
t.deq() 
t.ok(x) 
H 
Figure 4.2: Sequential consistency does not satisfy localit,y 

4.4. LINEARIZABILITY 
59 
Consider the following histories shown in Figure 4.2: 
Both Hls and Hit are sequentially consistent but H is not. 
To see that the linearizability is a local property, assume that (S,,<,) is a 
linearization of Hlz, that is, (Sz, <,) 
is a sequential history that is equivalent to 
Hlz. We construct an acyclic graph that orders all operations on any object and 
also preserves occurred before order < H .  Any sort of this graph will then serve 
a linearization of H .  The graph is constructed 
follows. The vertices are all the 
operations. The edges are all the edges given by union of all 
and < € I .  This 
graph totally orders all operations on any object. Moreover, it preserves < H .  The 
only thing that remains to be shown is that it is acyclic. Since <, are acyclic, it 
follows that any cycle, if it exists, must involve at least two objects. 
We will show that cycle in this graph implies a cycle in < H .  If any two consecu- 
tive edges in the cycle are due to just 
or just < H ,  then they can be combined due 
to transitivity. Note that e <, 
<y g for distinct objects z and y is not possible 
because all operations are unary (e 
implies that 
operates on both 
and y). Now consider any sequence of edges such that e <H 
g <H h. 
e <H 
implies res(e) precedes i n u ( f )  { definition of <H } 
<, g implies 
precedes res(g) { <, is a total order } 
g <H h implies res(g) precedes inw(h) { definition of <H }. 

60 
CHAPTER 4. CONSISTENCY CONDITIONS 
These relations can be combined to give that res(e) precedes 
Therefore, 
e <H h. Thus any cycle in the graph can be reduced to a cycle in < H ,  a contradiction 
because <H is irreflexive. 
So far we have only looked at consistency conditions for complete histories, 
that is, histories in which every invocation operation has a corresponding response 
operation. We can generalize the consistency conditions for partial histories as 
follows. A part,ial histtory H is linearizable if there exists a way of completing the 
history by appending response events such that, t,he complete history is linearizable. 
For example) consider the following history: 
HS = P write(z, I), Q read(z), 
ok(0) 
is linearizable because 
P v1rite(z, l), Q reud(z), 
ok(O), P ole(;) 
is linearizable. This generalization allows us to deal with systems in which some 
processes may fail arid consequentsly some response operations may be missing. 
4.5 
Other Consistency Conditions 
Alt,hough we have focused on sequential consistency and linearizability, there are 
many consistericy conditions t.hat are weaker than sequential consistency. A weakcr 
consistency condition allows more efficient implementation at the expense of in- 
crrased work by the programmer, who has to ensure that the application works 
correct,ly despite weaker consistency c:onditioris. 
Consider a program consisting of two processes, P arid 
with t,wo shared 
variables z and y. Assume that the initial values of z arid y are both 0. P writes 
1 in z and then reads t,he value of y; Q writes 1 in y and then reads the value of 
2. Strong consistency conditions such as sequential consistency or linearizability 
prohibit, the results of both reads from being 0. However, if we assume that t.he 
minimum possible time to read plus t.he minimum possible time t,o write is less 
than the communication latency, tjhen bot,h reads must return 0. The latency is the 
information delivery tsime, and each processor cannot possibly know of the events 
that have tmnspired at the other processor. So, no matter what the protocol is, if 
it irnplemcnts sequential consistency, it, must, be slow. 
Causal consistency is weaker than sequential consistency. Causal consistency 
allows for implementation of read and write operations in a distributed environment 
that, do riot, always incur conimunication delay; that is, causal consistency allows for 
cheap read and write operations. 
Wit,h sequcnt>ial consistency, all processes agree on the same legal seqiient,ial 
history S. The agreement defined by causal consistency is weaker. Given a history 
H ,  it is not required that two processes P and Q agree on t,he same ordering for 

4.5. OTHER CONSISTENCY CONDITIONS 
61 
the write operations, which are not ordered in H .  The reads are, however, required 
to be legal. Each process considers only those operations that can affect it, that is, 
its own operations and only write operations from other processes. Formally, for 
read-write objects causal consistency can be defined 
follows. 
Definition 4.3 (Causally Consistent) A history ( H ,  < H )  is causally consistent 
if for each process Pi, there is a legal sequential history 
<si) where 
is the set 
of all operations of 
and all write operations an H ,  and 
respects the following 
order: 
order: If Pi performs operation e before 
then e is ordered before f 
in 
Object order: If any process 
performs a write on an object x with value 
and another process Q reads that value 
then the write by P is ordered before 
read by Q in Si. 
Intuitively, causal consistency requires that causally related writes be seen by all 
processes in the same order. The concurrent writes may be seen in different order 
by different processes. 
It can be proved that sequential consistency implies causal consistency but the 
converse does not hold. As an example, consider history H I  in which PI does 
wl(z,l), rl(z,2) and 
does w2(x,2), ~2(z,1). 
The history is causally consistent because the following serializations exist: 
s1 = 
sz = wz(z,2),wl(x,l),rz(z,l) 
Thus we require only that there is a legal sequential history for every process and 
not one for the entire system. PI orders w1 before w2 in 
and P 2  orders 2 ~ 2  
before 
wl but that is considered causally consistent because w1 and w2 are concurrent 
writes. It can be easily proved that history H1 is not sequentially consistent. 
The following history is not even causally consistent. Assume that the initial 
value of 
is 0. The history at process P is 
The history at process Q is 
HIP = P r ( z , 4 ) ,  w(x, 3). 
HIQ = Q r ( x ,  31, Q 
4). 
Since Q reads the value 3 and then writes the value of 
the write by Q should 
be ordered after the write by P. P’s read is ordered before its write; therefore, it 
cannot return 4 in a causally consistent history. 
The table in Figure 4.3 summarizes the requirements of all consist.ency conditions 
considered in this chapter. The second column tells us whether the equivalent legal 

62 
CHAPTER 4. CONSISTENCY CONDITIONS 
Sequential 
Global 
Process order 
Causal 
Per process 
Process, object order 
Process order 
Figure 4.3: Summary of consistency conditions 
history required for the corisistency condition is global. The third column tells us 
the requirement on the legal history in ternis of the order preserved. For example, 
linearizability requires that there be a single equivalent legal history that preserves 
the occurred before order. 
4.6 
Problems 
4.1. Corisider a concurrent stack. Which of the following histories are linearizable? 
Which of the them are sequentially consistent? Justify your answer. 
P wsh(z), P OW, Q PMY), 
Q OW, P 
P 4 5 )  
P p 7 N z ) , Q  PUNY), 
P OW, Q OW, Q 
Q ok(z) 
4.2. Assume that all processors in the system maintain a cache of a subset of objects 
Give an algorithm that guarantees sequential 
accessed by that processor. 
consistency of reads and writes of the objects. 
4.3. Assume that you have an implementation of a concurrent system that guaran- 
tees causal consistency. Show that if you ensure that the system does not have 
any concurrent writes, then the system also ensures sequential consistency. 
4.4. FIFO consistency requires that the writes done by the same process be seen 
in t,he same order. Writes done by different processes may be seen in different 
order. Show a history that is FIFO-consistent but not causally consistent. 
4.5. Givcri a posct ( H ,  < H )  denoting a system execution, we define a relation 
+H 
thc transitive closure of union of process and object order. We call 
( H ,  < H )  normal if there exists an equivalent sequential history that preserves 
+ H .  
Show that when all operations are unary, a history is linearizable iff it 
is normal. 
4.6. Consider the following history of six events in which operations span multiple 
objects, assuming that A and B are initialized to 0: 

4.7. BIBLIOGRAPHIC REMARKS 
63 
e q  = iwu(lurite(1)) on 
A 
at 
PI 
ev2 = inv(sw.m()) 
on 
A,B at 
P2 
evg = resp(write()) from A 
at 
PI 
ev4 = inv(write(2)) on 
B 
at 
P3 
ev5 = resp(write()) from B 
at 
P3 
ev6 = resp(sum(2)) fr o m A,B at 
P2 
Show that this history is not linearizable but normal. 
*4.7. Assume that every message delay is in the range [d - d] for 0 < u < d. Show 
that in any system that ensures sequential consistency of read-write objects, 
the sum of delays for a read operation and a write operation is at least d. 
*4.8. (due to Taylor [Tay83]) Show that the problem of determining whether ( H ,  <H 
) is sequentially consistent for read-write registers is NP-complete. 
*4.9. (due to Mittal and Garg [MG98]) Generalize the definition of sequential con- 
sistency and linearizability for the model in which operations span multiple 
objects. Give dist,ributed algorithms to ensure sequential consistency and lin- 
earizability in this model. 
4.7 Bibliographic Remarks 
Sequential consistency was first proposed by Lamport [Lam79]. The notion of lin- 
earizability for read/write registers was also introduced by Lamport [Lam861 under 
the name of atomicity. The concept was generalized to arbitrary data types and 
termed 
linearizability by Herlihy and Wing [HW90]. Causal consistency wa5 
introduced by Hutto and Ahamad [HASO]. 

This Page Intentionally Left Blank

Chapter 5 
Wait-Free Synchronization 
Introduction 
The synchronization mechanisms that we have discussed so far are based on locking 
data structures during concurrent accesses. The lock-based synchronization mech- 
anisms are inappropriate in fault tolerance and real-time applications. When we 
use lock-based synchronization, if a process fails inside the critical section, then all 
other processes cannot perform their own operations. Even if no process ever fails, 
lock-based synchronization is bad for real-time systems. Consider a thread serving 
a request with a short deadline. If another thread is inside the critical section and 
is slow, then this thread may have to wait and therefore miss its deadline. Using 
locks also require the programmer to worry about deadlocks. In this chapter, we 
introduce synchronization mechanisms that do not use locks and are therefore called 
Zock- free. If lock-free synchronization also guarantees that each operation finishes in 
a bounded number of steps, then it is called wait-free. 
To illustrate lock-free synchronization, we will implement various concurrent ob- 
jects. The implementation of a concurrent object may use other simpler concurrent 
objects. One dimension of simplicity of an object is based on whether it allows 
multiple readers or writers. We use SR, MR, SW, and MW to denote single reader, 
multiple reader, single writer, and multiple writer, respectively. The other dimen- 
sion is the consistency condition satisfied by the register. For a single-writer register, 
Lamport. has defined the notions of safe, regular, and atomic registers. 
In this chapter, we discuss these notions and show various lock-free and wait-free 
constructions of concurrent objects. 
65 

66 
CHAP'rER 5. WAIT-FREE SYNCHRONIZATION 
5.2 
Safe, Regular, and Atomic Registers 
A regiskr is safe if a read that does not overlap with the write returns the most 
recent value. If the read overlaps with the writ,e, then it can return any value. 
For an example of safe and unsafe register histories, consider the histories shown 
in Figure 5.1. History (a) is unsafe because the read returns the value 4 but the 
most recent value written t,o the register is 3, which had completed before the read 
started. History (b) is also unsafe. Even though read returns a value that had been 
written l)efore, it is not the most recent value. History (c) is safe because W(x, 3 )  
and W(x,4) are concurrent and therefore W(x,4) could have taken effect before 
W ( x ,  3). Histories (d) and (e) are safe because the read operation overlaps a write 
operation and therefore can return any value. 
A register is regular if it is safe and when the read overlaps with one or more 
writes, it returns either the value of the most recent write that preceded the read 
or the value of one of the overlapping writes. Consider histories shown in Figure 
5.2. Hishory (a) is regular because the read opcration returns the value 3, which is 
the most' recent writme t>hat precedcd the read operation. History (b) is also regular 
because the read operation returns the value 4, which is the value written by a 
concurrent write operation. History (c) is not regular (although it is safe) because 
t.he value it returns does not match either the most recent coniplcted write or a 
concurrent. write. History (d) is also regular. It illustrates that there may be more 
than one concurrent writ,es with the read operation. 
A register is atomic if its histories are linearizable. Clearly, atomicity implies 
regularity, which in turn implies safety. Consider the histories shown in Figure 5.3. 
History (a) is regular but not atomic. It is regular because both the reads arc 
valid. It is not, atoniic because the second read returns a value older than the one 
returned by the first read. History (b) is atornic. It corresponds to the linearization 
in which the W ( x , 3 )  operation took effect after the first R(z) operat,ion. History 
(c) is atomic. It corresponds to the linearization in which the W ( x , 3 )  operation 
t,ook effect after tjhe W(x, 4) operation, and the read operation by the third process 
occiirred aftjer W(z,4) and before W ( x , 3 ) .  History (d) is not atomic. Since the 
first read ret,urned 3, W(x, 3) happens before W(z,4) in any linearization. Since 
the second read is after both of the writes haxe finished, it can return only 4. 
Surprisingly, it is possible to build a multiple-reader multiple-writer (MRMW) 
atomic niultivalued register from single-reader single-writer (SRSW) safe boolean 
registers. This can be achieved by the following chain of constructions: 
1. SRSW safe boolean register to SRSW regular boolean register 
2. SRSW regular boolean register t,o SRSW regular multivalued register 

5.2. SAFE, REGULAR, AND ATOMIC REGISTERS 
Figure 5.1: Safe and unsafe read--write registers 

68 
CHAPTER 5. WAIT-FREE SYNCHRONIZATION 
Figure 5.2: Coricurrcmt histories illustrating regularity 

5.2. SAFE, REGULAR, AND ATOMIC REGISTERS 
69 
Figure 5.3: Atomic and rionatomic registers 

CHAPTER 5. WAIT-FREE SYNCHRONIZATION 
3. SRSW regular register to SRSW atomic register 
4. SRSW atomic register to MRSW atomic register 
5. MRSW at,omic register to MRMW atomic register 
We show some of these constructions next 
Regular SRSW Register 
We abstract a register as an object of a class with two methods, getvalue and 
setvalue, used for reading and writing, respectively. Assume that we have a safe 
single-reader single-writer boolean register. Since we have a single reader and a 
single writer, we need to worry about, the semantics of getvalue and setvalue 
only when they overlap. In the presence of concurrency, we require that the value 
that is written not become corruptfed. The value that is read can be arbitrary. If 
another getvalue is performed after the write has finished, and there is no other 
setvalue in progress, then the value returned should be the value written by the 
last setvalue. We will use the following Java code as an abstraction for a SRSW 
safe boolean register. 
class SafeBoolean { 
boolean value ; 
public boolean getVialue ( )  { 
return value ; 
1 
public void setvalue (boolean b )  { 
value = b ;  
e that this registm is riot regular exactly for one scenari-if 
setvalue ant 
getvalue are invoked concurrently, the value being written is the same as the pre- 
vious value and getvalue returns a different value. This scenario is shown in Figure 
5.1 (e) . 
To make our register regular, we avoid accessing the shared register when the 
previous value of the register is the same 
the new value that is being written. 
The construction of a regular SRSW register from a safe SRSW register in given in 
Figure 5.4. 
Line 8 ensures that the writer does not access value if the previous value prev 
is the same as the value being written, b. Thus an overlapping read will return 
the correct value. If the new value is different, then the read can return arbitrary 

5.4. SRSW MULTIVALUED REGISTER 
71 
1 class RegularBoolean { 
2 
boolean prev; // not shared 
3 
SafeBoolean value ; 
public boolean getvalue ( )  { 
5 
return value. getvalue ( )  ; 
7 
public void setvalue (boolean b) { 
8 
if (prev != b) { 
9 
value. setvalue ( b ) ;  
6
1
 
10 
prev = b ;  
11 
1 
12 
1 
13 1 
Figure 5.4: Construction of a regular boolean register 
value from {true, false}, but that is still acceptable because one of t,hem is the 
previous value and the other is the new value. This construction exploits the fact 
that the value is binary and will not, work for multivalued registers. 
5.4 SRSW Multivalued Register 
We skip the construction from a SRSW regular boolean register to a SRSW atomic 
(linearizable) boolean register. Now assume that we have a SRSW atomic boolean 
register. This register maintains a single bit and guarantees that in spite of concur- 
rent accesses by a single reader and a single writer, it will result only in linearizable 
concurrent histories. We now show that, using such registers, we can implement a 
multivalued SRSW register. The implementation shown in Figure 5.5 is useful only 
when maxVa1 is small because it uses an array of maxVa1 SRSW boolean registers 
to allow values in the range 0. . .maxVal-1. 
The idea is that the reader should return the index of the first, true bit. The 
straightforward solution of the writer updating the array in the forward direction 
until it reaches the required index and the reader also scanning in the forward 
direction for the first t.rue bit does not work. It can happen that the reader does 
not find any true bit. Come up with an execution to show this! So the first idea we 
will use is that the writer will first, set the required bit to true and then traverse the 
array in the backward direction, set,ting all previous bits to false. We now describe 
the setvalue method in Figure 5.5. To write the value 2 ,  the writer makes the zth 
bit true at line 19 and then makes all the previous values false at lines 20-21. The 
reader scans for the true bit in the forward direction at line 12. With this strategy, 

72 
C:HAPTER 5. WAIT-FREE SYNCHRONIZATION 
1 c l a s s  MultiValued { 
2 
i n t  n = 0; 
3 
boolean A[] = n u l l ;  
4 
public MultiValued(int InaxVal, i n t  initVal ) { 
5 
n = InaxVal; 
6 
7 
for ( i n t  i = 0 ;  i < n ;  i++) A[ i ]  = f a l s e  ; 
8 
A[ initVal ] = t r u e ;  
9 
A = new boolean [ n ]  ; 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 1 
public i n t  getvalue ( )  { 
i n t  j = 0; 
while ( !  A[ j 
j t i  ; // forward s c a n  
i n t  v = j ;  
for ( i n t  i = j 
- 1; i >= 0; i--) // backward s c a n  
r e t u r n  v ;  
if ( A [ i ] )  v =  i ;  
} 
public void s e t V a l u e ( i n t  x )  { 
A [ x ]  = t r u e ;  
f o r  ( i n t  i = x - 1: i >= 0; i--) 
A[ i ]  = f a l s e  : 
1 
Figurc 5.5: Construction of a rnultivalued register 

5.5. MRSW REGISTER 
the reader is guaranteed to find at least one bit to be true. Further, this bit would 
correspond to the most recent write before the read or one of the concurrent writes. 
Therefore, this will result in at least a regular register. 
However, a single scan by the reader does not result in a linearizable implernen- 
tation. To see this, assume that the initial value of the register is 5 and the writer 
first writes the value 1 and then the value 4. These steps will result in 
1. Writer sets All] to true. 
2. Writer sets A[4] to true. 
3. Writer sets A[l] to false. 
Now assume that concurrent with these two writes, a reader performs two read 
operations. Since the initial value of All] is false, the first read may read A[4] 
the first bit to be true. This can happen as follows. The reader reads A[1], A[2], 
and A[3] as false. Before the reader reads A[4], the writer sets A[1] to true and 
subsequently A[4] to true. The reader now reads A[4] 
true. The second read 
may happen between steps 2 and 3, resulting in the second read returning 1. The 
resulting concurrent history is not linearizable because there is an inversion of old 
and new values. If the first read returned the value 4 then the second read cannot 
return an older value 1. 
In our implementation, the reader first does a forward scan and then does a 
backward scan at line 14 to find the first bit that is true. Two scans are sufficient 
to guarantee linearizability. 
5.5 MRSW Register 
We now build a MRSW register from SRSW registers. Assume that there are n 
readers and one writer. The simplest strategy would be to have an array of n 
SRSW registers, V[n], 
one for each of the readers. The writer would write to all n 
registers, and the reader r can read from its own register V[r]. This does not result 
in a linearizable implementation. Assume that initially all registers are 5 ,  the initial 
value of the MRSW register, and that the writer is writing the value 3. Concurrent 
to this write, two reads occur one after another. Assume that the first read is by 
the reader i and the second read is by the reader j, where i is less than j. It is then 
possible for the first read to get the new value 3 because the writer had updated 
V[i] 
and the second read to get the old value 5 because the writer had not updated 
Vb] by t,hen. This contradicts linearizability. 
To solve this problem, we require a reader to read not only the value written 
by the writer but also all the values read by other readers so far to ensure that 

74 
CHAPTER 5. WAIT-FREE SYNCHRONIZATION 
a reader returns the most recent, value. How does the reader determine the most 
recent value'? We use a sequence number associated with each value. The writer 
maintains the sequence number and writes this sequence nuniber wit,h any value 
that, it writes. Thus we view our SRSW register 
consisting of two fields: the 
value and 
(for timestamp). 
Now t,hat our values are tiniestamped, we can build a MRSW register from 
SRSW registers using the algorithm shown in Figure 5.6. 
Since we can only use SRSW objects, we are forced to keep O(n2) 
registers 
for inforniing readers what other readers have read. 
[j] is used by t,lie 
reader i to inform the value it read t,o the rea,der j. 
The reader simply reads its own rcgister and also what. other readers have read 
and returns the latest value. It reads it3s own register at. line 18 in the local variable 
t s v  (timestamped value). It compares the timestamp of this value with the times- 
tamps of values read by other readers at line 22. After line 23, the reader has the 
latest value that is read by any reader. It informs other readers of this value at, lines 
The writer simply increments the sequence number at line 33 and writ,es t.he 
26-28. 
value in all n registers at. lines 34--35. 
5.6 MRMW Register 
The const,ruction of an MRMW register from MRSW registers is simpler than the 
previous construction. We use n MRSW regist,ers for n writers. Each writ,er writcs 
in it.s own regist,er. Tlie only problem for t,he reader to solve is which of the write it 
should choose for reading. We use the idea of sequence numbers as in the previous 
implenient,at.ion. The reader chooses the value with the highest sequence number. 
There is only one problem with this approach. Previously, there was a single writer 
and t,herefore we could guarantee that all writes had different sequence numbers. 
Now we have multiple writers choosing their numbers possibly concurrently. How 
do we assign unique sequence number to each write? We use the approach of the 
Bakery algorithm. The algorit(hn1 is shown in Figure 5.7. In the method setvalue, 
we require a writer to read all the sequence numbers and then choose its sequence 
nuniber to be larger than the maximum sequence number it read. Then, to avoid 
the problem of two concurrent writers coming up with the same sequence number, 
we at.t,arh the process identifier w with the sequence number. Now t.wo writes with 
t.he same sequence number can be ordered on the basis of process ids. Furthermore, 
we havc t,he following giiarant,ce: If one write completely precedes another write, 
then the scquence number associat,ed with the first write will be smaller t,han that. 
with t,he second write. Tlie reader reads all the values written by various writ,ers 

5.6. MRMW REGISTER 
75 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
int n = 0; 
SRSW V[] = null;// walue w r i t t e n  
r e a d e r  i 
SFSW 
= null ; // c o m m u n i c a t i o n  between r e a d e r s  
int seqNo = 0; 
public MRSW(int r e a d e r s ,  int i n i t V a l )  { 
n = r e a d e r s ;  
V = new SRSW[ n ] ;  
for ( i n t  i = 0 ;  i < n ;  i++) 
Comm = new SRSW[n] [ n ] ;  
for ( i n t  i = 0 ;  i < n ;  i++) 
V[ i 1 .  s e t v a l u e  ( i n i t V a l  , 0 ) ;  
for ( i n t  j = 0; j < n ;  j++) 
Corm[ i ] [ j 1 .  setVallie ( i n i t V a l  , 0 ) ;  
public int getvalue (int r )  { / / r e a d e r  r r e a d s  
1 
/ / r e a d  
y o u r  own r e g i s t e r  
SRSW tsv = V [ r ] ;  // t s v  i s  l o c a l  
// f i n d  t h e  v a l u e  w i t h  t h e  l a r g e s t  t i m e s t a m p  
for ( i n t  i = 0 ;  i < n ;  i++) 
if ( k n [  i ] [ r 
getTS () > tsv . getTS ( ) )  
tsv = Comni[i][ 1-1; 
// i n f o r m  o t h e r  r e a d e r s  
for ( i n t  i = 0 ;  i < n ;  i + + )  { 
&m[ r ] [ i 1 .  s e t v a l u e  ( tsv ) ; 
1 
return t s v .  getvalue ( ) ;  
1 
public void setVialue( int x )  { // a c c e s s e d  by t h e  w r i t e r  
// w r i t e  t h e  walue w i t h  a l a r g e r  t i m e s t a m p  
seqNo++; 
for ( i n t  i = 0 ;  i < n ;  i++) 
V[ i ] .  s e t V a l u e ( x ,  seqNo); 
1 
Figure 5.6: Construction of a multireader register 

76 
CHAPTER 5. WAIT-FREE SYNCHRONIZATION 
and chooses the one with the largest timestamp. 
:lass MultiWriter { 
int n = 0; 
MRswV[] = null; // v a l u e  u:ritten by the wri 
public MultiWriter ( i n t  writers , int initVal 
n = writers; 
V = new h m n ] ;  
for ( i n t  i = 0 ;  i < n ;  i++) 
V [ i ] .  setValue(initVa1, 0 ,  i ) ;  
1 
e r  
{ 
1 public int getvalue ( )  { 
MRSW tsv = V[O]; // t s v  
l o c a l  
for ( i n t  i = 1; i < n ;  i++) 
if ( ( t s v . t s  < V [ i ] .  t s )  1 1  
( ( t s v . t s  = = V [ i ] .  t s )  && (tsv.pid < V [ i ] . p i d ) ) )  
t s v  = V[ i 1 ;  
return t s v  . Val : 
1 
public void setValue(int w, int x )  { // w r i t e r  
int niaxseq = V[0]. ts ; 
for ( i n t  i = 1 ;  i < n ;  i++) 
if ( maxseq < V[ i 1 .  ts ) maxseq = V[ i 1 .  t s  ; 
V[w]. setvalue ( x ,  maxseq + 1, w); 
1 
~ 
Figure 5.7: Construction of a multiwriter register 
5.7 
Atomic Snapshots 
All our algorithms so far handled single values. Consider an array of values that we 
want. to read in an atomic fashion using an operation readArray. We will assume 
that there is a single reader and a single writer but while the array is being read, the 
writes t,o individual locations may be going on concurrently. Intxitively, we would 
like our readArray operation t,o behave as if it took place instantaneously. 
A simple scan of t,he array does not work. Assume that the array has three 
locations initially all with value 0 and that a readArray operation and concurrently 
t,wo writes take place one after the other. The first write updates the first location to 
1 and the second write updates the second location to 1. A simple scan may return 
the value of array as [0,1,0]. However, the array went through the transitions 
[0,0,0] to [1,0,0], and then to 
1,0]. Thus, t.he value returned by readArray 

5.7. ATOMIC SNAPSHOTS 
77 
is not consistent with linearizabilit~y. A construction that provides a readArray 
operation with consistent values in spite of concurrent writes with it is called an 
atomzc snapshot operation. Such an operation can be used in getting a checkpoint. 
for fault-tolerance applications. 
We first present a lock-free construction of atomic snapshots shown in Figure 
5.8. This construction is extremely simple. First, to determine whether a location 
in the array has changed, we append each value with the sequence number. Now, 
the readArray operation reads the array twice. If none of the sequence numbers 
changed, then we know that there exists a time interval in which the array did 
not change. Hence the copy read is consistent. This coristriiction is not wait-free 
because if a conflict is detected, the readArray operation has to start all over again. 
There is no upper bound on the number of times this may have to be done. 
public class LockFreeSnapshot { 
int n = 0; 
public void LockFreeSnapshot (int initN ) { 
SRsW[l V; 
n = i n i t N ;  
V = new SRSW[ n] ; 
1 
public void writeLoc(int k ,  int x )  { 
int seq = V[ k ] .  ts ; 
V [ k ] .  setValue(x, seq + 1); 
1 
public Srtsw[] readArray ( )  { 
SRSW[] W =  new SRsw[n]; // W i s  l o c a l  
boolean done = false; 
while (! done) { 
for ( i n t  i = 0 ;  i < n ;  i + + )  // copy V t o  W 
done = true; 
// check i f  
h a s  changed 
for ( i n t  i = 0 ;  i < 11; i++) 
\.11 i 1 .  setValiie (V[ i 1 .  value 
~ 
V[ i 1 .  ts ); 
if (vi]. 
ts != V [ i ] .  t s )  { 
done = false ; 
break ; 
1 
1 
return W; 
} 
Figure 5.8: Lock-free atomic snapshot algorithm 
This construction is not wait-free because a readArray operation may be "starved" 

78 
CIIAPTER 5. WAIT-FREE SYNCHRONIZATION 
by t,he update operation. We do not go into detail here, but this and many other 
lock-free constructions can be turned into wait-free constructions by using the notion 
of “helping” moves. The main idea is that a thread tries to help pending opera- 
tions. For example, if the thread wanting to perform an update operation helps 
another concurrent, thread that is trying tjo do a 
operation, then we call 
it a “helping” move. Thus one of the ingredients in constructing a wait-free atomic 
snapshot would require the update operation to also scan the array. 
5.8 
Consensus 
So far we have given many wait-free (or lock-free) constructions of a concurrent 
object using other simpler concurrent objects. The question that naturally arises is 
whet,her it is always possible to build a concurrent object from simpler Concurrent 
object,s. We mentioned that it is possible to construct a wait,-free algorithm for an 
object that allows atomic reads of multiple locations in an array 
and at.omic write to a single location. What, if we wanted to build an object that 
allowed both rcads and writes to multiple locat,ions in an atomic manner? Such 
a construction is not possible using atomic read--write registers. Another question 
that, arises concerns the exist.ence of universal objects, that is, whether there are 
concurrent objects powerful enough to implement all other concurrent objects. 
 it^ turns out. that consensus is a fundamental problem useful for analyzing such 
problems. The consensus problem requires a given set of processes to agree on an 
input value. For example, in a concurrent linked list if multiple threads attempt to 
insert a node, t,hen all t,he processes have t,o agree on which node is inserted first. 
The consensus problem is abstrackd as follows. Each process has a value input 
t,o it that it, can propose. For simplicity, we will restrict the range of input values to 
a single bit. The processes are requircd to run a protocol so that, they decide on a 
common value. Thus, any object that irriplenients consensus supports the interface 
shown in Fignre 5.9. 
public interface Consensus { 
public void propose (int pid , int value ); 
public int decide (int pid ); 
1 
I 
Figure 5.9: consensus Interface 
The requircments on any object implementing consensus are as follows: 
0 Agreement: Two correct processes cannot decide different, values. 

5.8. CONSENSUS 
79 
0 Validity: The value decided by a correct process must be one of the proposed 
values. 
0 Wait-free: Each correct process decides the value after a finite number of steps. 
This should be true without any assumption on relative speeds of processes. 
A concurrent object 0 is defined to have a consensus number equal to the largest 
number of processes that can use 0 to solve the consensus problem. If 0 can be 
used to solve consensus for any number of processes, then its consensus number is 
and the object 0 is called the universal object. 
Now if we could show that some concurrent object 0 has consensus number m 
and anot,her concurrent object has consensus number m’ > m, then it is clear that 
t,here can be no wait-free implementation of 0’ using 0. Surprisingly, the coriverse 
is true as well: If 0’ has consensus number m’ 5 m, then 0’ can be implemented 
using 0. 
We begin by showing that linearizable (or atomic) registers have consensus nuni- 
ber 1. Clearly, an atomic register can be used to solve the consensus problem for a 
single process. The process simply decides its own value. Therefore, the coriserisus 
number is at least 1. Now we show that there does not exist any protocol to solve 
the consensus using atomic registers. 
The argument for nonexistence of a consensus protocol hinges on the concepts of 
a bivalent state and a critical state. A protocol is in a bivalent state if both the values 
are possible as decision values starting from that global state. A bivalent state is a 
critical state if all possible moves from that state result in nonbivalent states. Any 
initial state in which processes have different proposed values is bivalent because 
there exist at least two runs from that state that result in different decision values. 
In the first. run, the process with input 0 gets t,o execute and all other processes are 
very slow. Because of wait freedom, this process must decide, and it, can decide only 
on 0 to ensure validity. A similar run exists for a process with its input 
1. 
Starting from a bivalent initial state, and letting any process move that keeps 
the statje as bivalent, we must hit a critical state; otherwise, the protocol can run 
forever. We show that even in a two-process system, atomic regist,ers cannot be 
used to go to nonbivalent states in a consistent manner. We perform a case analysis 
of events that can be done by two processes, say, P and Q in a critical state S. 
Let e be the event at P and event f be at Q be such that e(S) has a decision 
value different from that of 
We now do a case analysis (shown in Figure 5.10): 
0 Case 1: e and 
are on different registers. In this case, both ef and f e  are 
possible in the critical state S. Further, the state e f ( S )  is identical to f e ( S)  
and therefore cannot have different decision values. But we assumed that 

80 
C'HAPTER 5. WAIT-FREE SYNCHRONIZATION 
Case 1: 
e and fare operations on different registers 
case 2: 
e is a read 
Case 3: 
e and fare writes on the same register 
f
\
 
Figure 5.10: Impossibility of wait-free corisCnsus with atomic rcad write registws 

5.8. CONSENSUS 
81 
and e(S) have different decision values, which implies that e ( f ( S ) )  and 
f(e(S)) have different decision values because decision values cannot change. 
0 Case 2: Either e or 
is a read. Assume that, e is a read. Then the state of 
Q does not change when P does e. Therefore, the decision value for Q from 
and e(S), if it ran alone, would be the same; a contradiction. 
0 Case 3: Both e and f are writes on the same register. Again the st,ates f ( S )  
and f ( e ( S ) )  are identical for Q and should result, in the same decision value. 
This implies that there is no consensus protocol for two processes that, uses just 
atomic registers. Therefore, the consensus number for atsoniic registers is 1. 
Now let us look at a concurrent object that can do both read and write in one 
operation. Let us define a 
object 
one that provides the test and set 
instruction discussed in Chapter 2. The semantics of the object is shown in Figure 
5.11. 
public class TestAndSet { 
i n t  IiiyValue = -1; 
public synchronized int testAndSet ( i n t  newvalue) { 
int oldvalue = myvalue; 
myVallie = newVallie ; 
return oldvalue ; 
1 
1 
Figure 5.11: Test,AndSet class 
We now show that two threads can indeed use this object to achieve consensus. 
The algorithm is given in Figure 5.12. 
By analyzing the 
operations on the critical state, one can show that 
the TestAndSet registers cannot be used to solve the consensus problem on three 
processes (see Problem 5.1). 
Finally, we show universal objects. One such universal object is 
(Com- 
pare and Swap). The semantics of this object is shown in Figure 5.13. Note that 
such objects are generally provided by the hardware and we have shown the Java 
code only to specify how the object behaves. Their actual implementation does no 
use any locks. Therefore, 
can be used for wait-free synchronization. An 
object of type 
can hold a value 
It supports a single operation 
that takes two arguments: 
and 
It replaces the 
value of the object only if the old value of the object matches the 

82 
CHAPTER 5. WAIT-FREE SYNCHRONIZATION 
:lass TestSetConsensus implements Consensus { 
TestAndSet 
int proposed [ I  = ( 0 ,  O } ;  
// assumes p i d  
0 or 1 
public void propose(int p i d ,  int v )  { 
proposed [ pid ] = v ; 
} public i n t  d ecid e ( i n t  p i d )  { 
if ( x .  t,estAndSet ( p i d )  == -1) 
else 
return proposed [ pid ] ; 
r e t u r n  proposed 11 - pid 
1 
Figure 5.12: Consensus using TestAIidSet object 
public class CompSwap { 
int inyValue = 0; 
public ConipSwap( i n t  i n i t v a l u e  ) { 
myvalue = i n i t v a l u e  ; 
1 
public synchronized i n t  compSwapOp( int prevValue , i n t  newvalue) { 
i n t  oldVallie = myvalue; 
if ( myvalur == prevvalue ) 
myvalue = newvalue ; 
r e t u r n  oldvalue ; 
1 
1 
Figure 5.13: CompSwap object 

5.8. CONSENSUS 
Processes use a 
object 
for consensus as follows. It is assumed to 
have the initial value of -1. Each process tries to update 
with its own pid. They 
use the initial value -1 as prevvalue. It is clear that only the first process will 
have the right prevvalue. All other processes will get the pid of the first process 
when they perform this operation. The program to implement consensus is shown 
in Figure 5.14. 
public class CompSwapConsensus implements Consensus { 
CompSwap x = new CompSwap ( - 1) ; 
int proposed 
public ConipSwapConsensus( int 11) { 
proposed = new int [ n ] ;  
1 
1 
public void propose(int p i d ,  int v )  { 
proposed [ pid] = v ;  
public int decide ( i n t  p i d )  { 
int j = x.cornpSwapOp(-1, p i d ) ;  
i f  ( j  == -1) 
else 
return proposed [ pid ] ; 
return proposed [ j ] ; 
1 
1 
Figure 5.14: Consensus using CompSwap object 
We now describe another universal object called the load-linked and store- 
conditional (LLSC) register. An LLSC object contains an object pointer p of the 
following type. 
public class ObjPointer { 
public Object o b j ;  
public int v e r s i o n ;  
1 
It provides two atomic operations. The load-linked operation allows a thread 
to load the value of a pointer to an object. The store-conditional operation 
allows the pointer to an object to be updated if the pointer has not changed since 
the last load-linked operation. Thus the semantics of LLSC are shown in Figure 5.15. 
With each object, we also keep its version number. The method load-linked reads 
the value of the pointer p in the variable local. The method store-conditional 

84 
CHAPrER 5. WAIT-FREE SYNCHRONIZATION 
takes an object, newObj and 
pointer 
its parameters. It updates LLSC 
object only if the pointer 
is identical to p. 
The program for implementing consensus using LLSC is left as an exercise. 
Iublic class LLSC { 
ObjPointer p ;  
public LLSC(0bject x )  { 
p . o b j  = x ;  
p .  version = 0; 
I public synchronized void load-linkcd ( ObjPointer local ) { 
local . obj = p. obj ; 
local . version = p .  version ; 
1 public synchronized boolean 
store-conditional (ObjPoint,er local , Object ncwObj) { 
if ( (  p .  obj == local . obj) && ( p .  version == local . version ) )  { 
p .  obj = iiewObj; 
p .  version ++; 
return true ; 
1 
return false : 
Figure 5.15: Load-Linked and Store-Conditional object 
5.9 Universal Constructions 
Now t,kiat we have secn universal objects, let us show that they can indeed be used 
to build all other concurrent object,s. We first show 
construction for a concurrent 
queue that allows multiple processes to enqueue and dequeue concurrently. Our 
construction will almost be mechanical. We first begin with a sequential implemen- 
tation of a queue shown in Figure 5.16. 
Now wc use LLSC to iniplenierit 
concurrent queue using 
pointer-swingiy 
tecliniquc. In the ohject poiritm swinging t,echriique any thread that, wishes to 
perform an operation on an object, goes through the following steps: 
1. The thread makes 
copy of that object. 
2. It performs its operation on the copy of the object instead of the original 
object. 

5.9. UNIVERSAL CONSTRCJCTIONS 
85 
>ublic class SeqQueue { 
class Element { 
public String d a t a ;  
public Element next ; 
public Elenient ( S t r i n g  s ,  Element e )  { 
data = s ;  
next = e ;  
1 
1 
public Element head, t a i l  ; 
public SeqQueue ( )  { 
head = null; 
t a i l  = null; 
1 
public SeqQueue ( SeqQueue copy ) { 
Element node ; 
head = copy. head; 
t a i l  = copy. t a i l ;  
for (Element i = head; i != null ; i = i . n e x t )  
node = new Element ( i  . d a t a ,  i . next ); 
public void Enqueue( String d a t a )  { 
1 
Element temp = new Element ( data , 
if ( t a i l  == null) { 
t a i l  = temp; 
head = t a i l  ; 
t a i l  . n e x t  = temp; 
t a i l  = tcmp; 
null ) ; 
} else { 
} 
1 
public String Dequeue() { 
if (head == null) return null; 
String returnval = head. data ; 
head = head. next ; 
return returnval ; 
Figure 5.16: Sequential queue 

CHAPTER 
WAIT-FREE SYNCHRONIZATION 
3. It, swings the pointer of the object to the copy if the original pointer has not 
changed. This is done atomically. 
4. If the original pointer has changed, then some other thread has succeeded in 
performing its operation. This thread starts all over again. 
Using these ideas, we can now implerrient a concurrent queue class 
shown in Figure 
5.17. To Enqueue an item (of type string), we first save the pointer to the queue 
in 
t,he variable 
at line 
Line 11 makes a local copy new-queue of the queue 
accessed using 
pointer. It then inserts 
in new-queue at line 12. Line 13 
returns true if no other thread has changed 
If 
returns false, 
then this thread tries again by going to the line 9. The method Dequeue is similar. 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
14 
15 
16 
17 
18 
19 
20 
21 
22 
2:5 
24 
25 
26 
27 
28 
~ 
1 public class CQueue { 
Drivate 
public CQiieue() { 
= new 
SeqQueue 
1 
public void Enqueue( String data) { 
SeqQuene new-queue ; 
ObjPointer local = new ObjPointer (); 
while ( t r u e )  { 
x .  load-linked (local ); 
new-queue = new SeqQueue ( (  SeqQueue) local . obj ); 
new-queue . Enqueue (data ) ; 
if ( x .  store-conditional (local , new-queue)) 
return : 
1 public String Dequeue () { 
SeqQueiie new-queue ; 
ObjPointer local = new ObjPointer ( ) ;  
String returnval ; 
while 
{ 
x .  load-linked (local ); 
new-queue = new SeqQueue ( (  SeqQueue) local . 
returnval = new-queue. Dequeue ( ) ;  
if ( x .  store_conditional (local , new-queue) 
return returnval ; 
} 
1 
29 } 
Figure 5.17: Concurrent queue 
While the above riientioried technique ran be applied for lock-free construction of 

5.10. PROBLEMS 
87 
any concurrent object, it may be inefficient in practice for large objects because every 
operation requires a copy. There are more efficient algorithms for large concurrent 
objects. However, these algorithms are different for different data structures and 
will not be covered in this book. 
5.10 Problems 
Show that 
cannot be used to solve the consensus problem for 
three processes. (Hint: Show that TestAridSet by two processes in any order 
results in the same state and the third process cannot distinguish between the 
two cases.) 
5.2. Consider a concurrent FIFO queue class that allows two threads to concur- 
rently dequeue. Show that the consensus number of such an object is 2. (Hint: 
Assume that queue is initialized with two values 0 and 1. Whichever process 
dequeues 0 wins.) 
5.3. Consider a concurrent object of type 
that holds an integer. It supports a 
single method 
v) that sets the value with 
and returns the old 
value. Show that 
has consensus number 2. 
5.4. Show that LLSC is a universal object. 
*5.5. Give a lock-free construction of queue that does not make a copy of the entire 
queue for enqueue and dequeue. 
5.11 Bibliographic Remarks 
The notion of safe, regular, and a.tomic registers was first proposed by Lamport 
[Lam861 who also gave many of the constructions provided here. The notions of 
consensus number and universal objects are due to Herlihy [Her%]. The reader 
should also consult [AW98] (chapters 10 and 15). 

This Page Intentionally Left Blank

Chapter 6 
Distributed Programming 
6.1 Introduction 
In this chapter, we will learn primitives provided in the Java programming language 
for building distributed applications. We will see primarily two programming styles: 
sockets and remote method invocations. Sockets provide a lower-level interface for 
building distributed programs but, are more efficient and flexible. Remote method 
irivocat.ions (RMI) are easier to use. 
In this chapter we first describe the class 
which is useful for net- 
work programming no matter which style of primitives are used. Then we discuss 
primitives for programming using sockets. These sockets may use either the Uni- 
versal Datagram Protocol (UDP), or the Transmission Control Protocol (TCP). We 
give an example of an echo server using sockets based on the UDP protocol and a 
simple name server using sockets based on the TCP protocol. Finally, we discuss 
programming using remote method invocations. 
6.2 InetAddress Class 
For any kind of distributed application, we need the notion of an Internet address. 
Any computer connected to the Internet (called a host) can be uniquely identified 
by an address called an IP address. Since addresses are difficult to remember, 
each host also has a hostname. It is the task of a domain name system (DNS) 
server to provide the mapping from a hostname to its address. Java provides a 
class 
. 
which can be used for this translation. The relevant 
methods for the class Inet Address are given below: 
89 

90 
CHAPTER 6. DISTRIBUTED PlLOGRAhlhlING 
[] 
0 
) 
6.3 Sockets based on UDP 
Socket,s are useful in writing progranis based on communicat,ion using messages. A 
Socket is an object, that can be used to send and receive messages. There are pri- 
marily two protocols used for sending and receiving messages: Universal Datagram 
Protocol (UDP) and nansmission Control Protocol (TCP). The UDP provides a 
low-level connectionless protocol. This means that packets sent using UDP are not 
guaranteed to be received in the order sent. In fact, the UDP protocol does riot 
even guarantee reliability, that, is, packets niay get lost. The protocol does not use 
any handshaking mechanisms (such 
acknowledgments) to det.ect loss of packets. 
Why is UDP useful, then? Because, wen though UDP may lose packets, in practice, 
t,his is rarely t,he case. Since t,here are no overheads associated with error checking, 
UDP is an extremely efficient protocol. 
The TCP protocol is a reliable connection-oriented protocol. It also guarantees 
ordered delivery of packets. Needless to say, TCP is not as efficient' as UDP. 
6.3.1 Datagram Sockets 
The first class that we use is 
which is based on the UDP protocol. 
This class represents a socket for sending and receiving datagram packets. A data- 
gram. socket is t,he sending or receiving point for a connectionless packet delivery 
service. Each packet sent or received on a datagram socket is individually addressed 
and rout,ed. Multiple packets sent from a machine to another may be routed differ- 
ently, and may arrive in any order. This class provides a very low level interface for 
sending and receiving messages. There are few guarantees associated with data, nram 
sockets. An advant,age of datagram sockets is that it allows fast data t,ransniission. 
The details for the methods in this class are given below. To construct a Dat8a- 
gramsocket, we can use one of the following constructors: 

6.3. SOCKETS BASED ON UUP 
91 
The first constructor constructs a datagram socket and binds it to any available 
port on the local host machine. Optionally, a port may be specified 
in the second 
constructor. The last constructor creates a datagram socket, bound to the specified 
local address. These constructors throw 
if the socket could not 
be opened, or if the socket could not bind the specified local port. 
The other important methods of this class are as follows: 
1. 
This method closes a datagram socket. 
To get the information about the socket, one 
can use this method, which returns the port number on the local host to which 
this socket is bound. 
This method gets the local ad- 
dress to which the socket is bound. 
This method 
receives 
a dahgram packet from this socket. When this method returns, the Data- 
grampacket’s buffer is filled with the data received. The datagram packet also 
contains the sender’s IP address and the port number on the sender’s machine. 
Note that this method blocks until a datagram is received. The length field of 
the datagram packet object contains the length of the received message. If the 
message is longer than the buffer length, the message is truncated. It throws 
IOException if an 
error occurs. The blocking can be avoided by setting 
the timeout. 
This method sends a datagram packet 
from this socket. The Datagrampacket includes information indicating the 
data to be sent, its length, the IP address of the remote host, and the port 
number on the remote host. 
6.3.2 Datagrampacket Class 
The 
class required data to be sent as datagram packets. The class 
is used for that. Its definition is given below. 
{ 
[I , 
; 

92 
CHAPTER 6. DISTRIBUTED PROGRAhlhfING 
, 
[I 
; 
; 
(> ; 
) 
; 
3 
The first. constructor 
, 
constructs a Datagrampacket for receiving packets of length 
The pa- 
rameter 
is the buffer for holding the incoming datagram, and 
is the 
number of bytes to read. 
The constructor for creating a packet to be sent is 
It, constructs a Datagrampacket for sending packets of length 
to the speci- 
fied port number on the specified host. The parameters 
and 
are used for 
the destination address and the dcstination port. number, respectively. The method 
returns the IF address of the ma.chine to which this datagram is being 
sent, or from which the datagram was received. The method 
ret,iirns the 
dat,a received, or the data to be sent. The method 
returns the length 
of the data to be sent,, or the length of the dat,a received. Similarly, the rnet.hod 
returns the port number on the remote host t,o which this datagram is 
being sent, or frorn which the datagram was received. The 
methods are used to 
set, t,he IP address, port number, and &her elements appropriat,ely. 
6.3.3 Example Using Datagrams 
We givc a sirnplc example of a program that uses datagrams. This example consists 
of two proceses - a  server and a client. The client reads input from the user and 
sends it to the server. The server receives the datagram packet and then echoes 
back the same data. The program for the server is given in Figure 6.1. 

6.3. SOCKETS BASED O N  UDP 
93 
mport j ava . net . * ; 
mport java . io .*; 
,ublic class Datagramserver { 
public static void main( String [ I  
a r g s )  { 
Datagrampacket datapacket , ret urnpacket ; 
int port = 2018: 
int len = 1024; 
try { 
Datagramsocket datasocket 
byte 
buf = new byte [ len 
while ( t r u e )  { 
try { 
= new Datagramsocket ( port ) ;  
datapacket = new DatagraniPacket (buf , buf. length ); 
d a t a s o c k e t .  receive (datapacket ) ;  
returnpacket = new Datagrampacket ( 
d a t a p a c k e t .  getData (), 
datapacket. getLength ( ) ,  
d a t a p a c k e t .  getAddress ( ) ,  
d a t a p a c k e t .  getport 
d a t a s o c k e t .  send ( returnpacket ) ;  
System. e r r .  p r i n t l n  ( e ) ;  
} catch (IOException e )  { 
1 
1 
} catch (SocketException s e )  { 
System. err . p r i n t l n  (se ) ;  
1 
1 
Figure 6.1: A datagram server 

CHAPTER 6. DISTRIBIJTED PROGRAMMING 
The client process reads a line of input from 
It then creates a 
datagram packet and sends it to the server. On receiving a response from the server 
it displays the message received. The program for the client is given in Figure 6.2. 
6.4 Sockets Based on TCP 
The second style of interprocess communication is based on the notion of streams. 
In this style, a connection is set up between the sender and the receiver. This style 
allows better error recovery and guarantees on the delivery of packets. Thus, in a 
stream the packets are received in the order they are sent. 
The 
class in .Java extends the 
class. We will give only a subset 
of constructors and methods available for 
The constructor 
creates a stream 
socket and connccts it to the specified port number on the named host. It throws 
arid 
Here we have used t,he name of hhe host. Alternat,ively, IP address ca.n be used 
in t,he form of t,he class InetAddress as below: 
The methods for the socket are 
which returns the remote IP ad- 
dress to which this socket is connected. 
0, which returns t,he local address 
to which the socket is bound. 
which ret,urns the remote port to which this socket 
is connected. 
which returns an input stream for 
reading bytes from this socket. 
0 
which returns an output stream 
for writing bytes to this socket. 
which closes this socket. 
Note that marly of these methods throw 
if an 1/0 error occiirs when 
applying the method to the socket. 

6.4. SOCKETS BASED ON TCP 
95 
import j a v a .  net .*; 
import java . io . *; 
public class Datagramclient { 
public static void main(String [ I  
a r g s )  { 
String hostname ; 
int port = 2018; 
int lei1 = 1024; 
Datagrampacket sPacket , rPacket ; 
if ( a r g s .  length > 0) 
else 
hostname = args [ O ] ;  
hostnaine = "localhost "; 
InetAddress ia = InetAddress . getByName( hostname); 
Datagramsocket datasocket = new Datagramsocket ( ) ;  
BufferedReader stdinp = new BufferedReader ( 
new InputStreaniReader (System. in ) ) ;  
while ( t r u e )  { 
try { 
try { 
String echoline = s t d i n p .  readLine ( ) ;  
if ( echoline. equals ("done")) break; 
byte [ I  
buffer = new byte[ e c h o l i n e .  length 
buffer = echoline . getBytes ( ) ;  
sPacket = new Datagrampacket ( buffer , 
d a t a s o c k e t .  send (sPacket ); 
byte[] rbuffer = new byte[ l e n ] ;  
rPacket = new Datagrampacket( rbuffer , rbuffer . length 
d a t a s o c k e t .  receive (rPacket ); 
String r e t s t r i n g  = new String (rPacket . getData ( ) ,  0 ,  
rPacket . getLength 0); 
System. o u t .  p r i n t l n  ( r e t s t r i u g  ); 
} catch (IOException e )  { 
System. e r r .  p r i n t l n  ( e ) ;  
buffer . length , ia , port ); 
1 
} // while 
} catch ( UnknownHostException e )  { 
System. e r r .  p r i n t l n  ( e ) ;  
} catch ( SocketException s e )  { 
System. e r r .  p r i n t l n  ( s e ) ;  
1 
Figure 6.2: A datagram client 

96 
CHAPTER 6. DISTRIBUTED 
6.4.1 Server Sockets 
On the server side the class that is used is called 
way to create a 
server socket, is 
This call creat,es a server socket on a specified port. Various methods on a server 
socket are 
follows: 
which returns the address to which 
this socket is connected, or null if the socket is not yet connected. 
(), which ret.urns t.he port on which this socket is 
listening. 
0 
which listens for a connection to be made to this 
socket, and accepts it. The method blocks until a connection is made. 
(), which closes this socket. 
6.4.2 
We now give a simple name server implemented using server sockets. The name 
server maintains a table of 
to give a mapping 
from a proccss 
t,o the host and the port number. For simplicit,y, we assume 
t,hat t,he maxiiriuni size of the table is 100 and that, there are only two operations on 
the table: 
and 
This table 
kept by the object 
shown 
in Figure 6.3. 
Now let us look at the name server. The name server creates a server socket 
with the specified port. It then listens to any incoming connections by the niethod 
The 
niet,hod returns the socket whenever a connection is made. It 
then handles the request that arrives on that socket by the met,hod 
We call 
and 
to get input and output, streanis 
associakd with the socket. Now wc can simply use all niethods associated for reading 
and writing input streams t.0 read and write data from the socket. 
In our iniplement,ation of the name server shown in Figure 6.4, at most one client 
is handled at a time. Once a request is handled, the main loop of the name server 
ac:cept,s another connection. For many applications this may be unacceptable if the 
procedure to handle a request takes a long time. For these applications, it is quite 
common for the server to be multithreaded. The server accepts a connect,ion and 
then spawns a t,liread t.o handle the request. However, it must be observed that since 
the data for the server is shared among multiple threads, it is the responsibilit,y of 
t'he progranimer to ensure that the data is accessed in a safe manner (for example, 
by using 
methods). 
Example 1: A Name Server 
The client program in Figure 6.5 can be used to test this name server. 

6.4. SOCKETS BASED ON TCP 
97 
mport j a v a .  u t i l  .*; 
)ublic class NameTable { 
final int maxSize = 100; 
private String [ I  names = new String [ maxSize 
private String 
hosts = new String [ maxSize 
private int 
ports = new int [ maxSize]; 
private int d i r s i z e  = 0; 
int s e a r c h ( S t r i n g  s )  { 
for ( i n t  i = 0; i < d i r s i z e  ; i++) 
return -1; 
if ( names [ i 1 .  equals ( s ) )  return i ; 
} int i n s e r t  ( S t r i n g  s ,  String hostName, int portNumber) { 
int oldIndex = s e a r c h ( s ) ;  // 
i t  a l r e a d y  t h e r e  
if ( (  oldIndex == -1) && ( d i r s i z e  < maxSize)) { 
names[ dirsize 1 = s ;  
hosts [ d i r s i z e  ] = hostName; 
ports [ d i r s i z e  ] = portNuniber ; 
dirsize ++; 
return 1; 
return 0 ;  
} else // a l r e a d y  t h e r e ,  o r  t a b l e  f u l l  
1 
1 
I 
int getport (int index) { 
return ports [index 1 ;  
String getHostName( int index 
return hosts [ index ] ; 
Figure 6.3: Simple name table 

98 
CHAPTER 6. DISTRIBUTED PROGRAMMING 
import java . iiet . * ;  
import java . io . *; 
import j a v a .  u t i l  . * ;  
public class Nameserver { 
NameTable table ; 
public NarneServer ( )  { 
table = new NanieTable ( )  ; 
void handleclient (Socket t h e c l i e n t  ) { 
1 
try { 
BufferedReader din = new BufferedReader 
(new InputSt,reamReader ( t h e c l i e n t  . getInputStream 
Print,Writer pout = new P r i n t w r i t e r  ( t h e c l i e n t  . getOutputStream ( ) )
String getline = d i n .  readLine ( ) ;  
StringTokenizer s t  = new StringTokenizer ( getline ) ;  
String tag = s t .  nextToken ( ) ;  
if ( t a g .  equals ( ” s e a r c h ” ) )  { 
int index = t a b l e .  search ( s t .  nextToken 
if ( i n d e x  == -1) // not found 
pout. p r i n t l n  (-1 + ‘’ ” 
else 
pout. p r i n t l n  ( t a b l e .  getport ( i n d e x )  + 
+ table . getHostNarne ( index ) ) ; 
+ ! ‘ n u l l h o s t ” ) ;  
” ” 
} else if ( t a g . e q n a l s ( ” i n s f r t ” ) )  { 
String name = s t .  nextToken ( ) ;  
String hostName = s t .  nextToken ( ) ;  
int port = Integer . parseInt ( s t .  nextToken 0); 
int retValue = t a b l e .  i n s e r t  (name, hostName, port ); 
pout. p r i n t l n  ( r e t V a l u e  ); 
1 
pout. flush ( )  ; 
} catch ( IOException e )  { 
System. err . p r i n t l n  ( e ) ;  
} 
} public static void main(String [ I  
a r g s )  { 
Nameserver ns = new Nameserver ( )  ; 
Syst,em. o u t .  p r i n t l n  (”Nameserver s t a r t e d  :” ); 
try { 
ServerSocket l i s t e n e r  = new ServerSocket (Symbols. Serverport ) ;  
while ( t r u e )  { 
Socket &Client = l i s t e n e r  .accept (); 
ns. handleclient ( aClient ) ;  
aClient . close (); 
1 
} catch (IOException e )  { 
System. e r r .  p r i n t l n  (”Server aborted :” + e ) ;  
} 
1 
1 
I 
Figure 6.4: Name server 

6.4. SOCKETS BASED ON TCP 
99 
~~ 
~~ 
~ 
~ 
~~ 
~ 
~ 
mport j a v a .  lang . * ;  import j a v a .  u t i l  .*; 
mport j a v a .  net .*; import j a v a .  io .*; 
,ublic class Name { 
BufferedReader din ; 
Printstream pout; 
public void getsocket ( )  throws IOException { 
Socket server = new Socket (Symbols. nameserver, 
din 
new BufferedReader( 
pout = new Printstream ( s e r v e r .  getoutputstream 0); 
Symbols. Serverport ) ; 
new InputStreamReader ( s e r v e r .  getInputStream 0)); 
public int insertName( String name, String hname, int portnum) 
throws IOException { 
getsocket ( ) ;  
pout. p r i n t l n  ( ” i n s e r t  ” + name + ” ” + hname + ” ” + portnum ) ; 
pout. flush ( ) ;  
return I n t e g e r .  parseInt ( d i n .  readLine 0); 
1 
public PortAddr searcliName ( String name) throws IOException { 
getsocket ( ) ;  
pout. p r i n t l n  ( ” s e a r c h  ” + name); 
pout. flush ( ) ;  
String r e s u l t  = d i n .  readLine ( ) ;  
StringTokenizer st = new StringTokenizer ( r e s u l t  ); 
int portnum = I n t e g e r .  parseInt ( s t .  nextToken 
String hname = s t .  nextToken ( ) ;  
return new PortAddr (hname, portnum ) ; 
public static void main(String [I a r g s )  { 
1 
Name myclient = new Name(); 
try { 
myclient. insertName ( ”  h e l l o l ”  , ”oak. ece. utexas. edu” , 1000); 
PortAddr pa = myclient. searchName ( ”  hellol ” ) ; 
System. o u t .  p r i n t l n  (pa.getHostName() + ”:” + p a .  getport 0); 
System. e r r .  p r i n t l n  (”Server aborted :” + e ) ;  
} catch (Exception e )  { 
1 
} 
Figure 6.5: A client for name server 

100 
CHAPTER 6. DISTRIBUTED PROGRAMMING 
6.4.3 Example 2: A Linker 
We now show a java class Linker that, allows us t.0 link a given set of processes with 
each other. Assume that we want to start n processes PI, P2,. . . , P, in a distributed 
system and establish connections between them such that any of the process can 
send and receive messages with any other process. We would like to support direct 
naming to send and receive messages; that is, processes are unaware of the host 
addresses and port numbers. They simply use process identifiers (1 . . . n} to send 
and receive messages. 
We first read the topology of the underlying network. This is done by the method 
rearneighbors in t,he class 
shown in Figure 6.6. The list of neighbors of 
Pi are assumed to be enumerated in the file “topologyi.” If such a file is not found, 
then it is assumed that all other processes are neighbors. 
mport java . io . *; 
mport j a v a .  util .*; 
?ublic class Topology { 
public s t a t i c  void readNeighbors ( i n t  myId, int N, 
IntLinkedList neighbors ) { 
U t i l .  println (”Reading t o p o l o g y ” ) ;  
t r y  
BufferedReader dIn = new BufferedReadrr ( 
St,ringTokenizer st, = new StriiigTokenizer (dIn. readLine 0); 
while ( st . hasMoreTokens ()) { 
int neighbor = Integer. parseInt ( s t .  nextToken 0); 
neighbors. add( neighbor ); 
new FileReader (”topology” + inyId)); 
1 
} catch ( FileNotFoiindException e )  { 
for ( i n t  j = 0; j < N; j++) 
if ( j != myId) neighbors. add( j ) ;  
} catch ( IOException e )  { 
System. err . println ( e ) ;  
1 
U t i l .  println (neighbors. tostring 
1 
Figure 6.6: Topology class 
Now we discuss the Connecter class, which establishes connections between 
processes. Since processes may start at different times and at different locations, 
we use the Nameserver to help processes locate each other. Any process Pi that 
starts up first creates a Serversocket for it,self. It uses the Serversocket to listen 

6.5. REMOTE METHOD INVOCATIONS 
101 
for incoming requests for communication with all small numbered processes. It 
then contacts the 
and inserts its entry in that table. All the smaller 
numbered processes wait for the entry of PI to appear in the 
When 
they get the port number from the 
they use it to connect it to P,. 
Once P, has established a TCP connection with all smaller number processes, it 
tries to connect with higher-number processes. This class is shown in Figure 6.7. 
For simplicity, it is assumed that the underlying topology is completely connected. 
Once all the connections are established, the 
provides methods to send 
and receive messages from process P, to PJ. We will require each message to contain 
at least four fields: source identifier, destination identifier, message type (or the 
message tag), and actual message. We implement this in the Java class shown in 
Figure 6.8. 
The Linker class is shown in Figure 6.9. It provides methods to send and receive 
messages based on process identifiers. Different 
methods have been provided 
to facilitate sending messages of different types. Every message is assumed to have 
a field 
that corresponds to the message tag (or the message type). 
6.5 Remote Method Invocations 
A popular way of developing distributed applications is based on the concept of 
remote procedure calls (RPCs) or remote method invocations (RMIs). Here the 
main idea is that a process can make calls to methods of a remote object as if it 
were on the same machine. The process making the call is called a client and tjhe 
process that serves the request, is called the server. In RMI, the client may not even 
know the location of the remote object. This provides location transparency to the 
client. In Java, for example, the remote object may be located using 
Alternatively, references to remote objects may be passed around by the application 
references to local objects. 
A call to a method may have some arguments, and the execution of the method 
may return some value. The arguments to the method when the object is remote 
are sent via a message. Similarly, the return value is transmitted to the caller via 
a message. All this message passing is hidden from the programmer, and there- 
fore RMI can be viewed as a higher-level programming construct than sending or 
receiving of messages. 
Although the idea behind RMI is quite simple, certain issues need to be tackled 
in implementing and using Rh4I. Since we are passing arguments to the method, we 
have t,o understand the semantics of the parameter passing. Another issue is that 
of a failure. What happens when the messages get lost? We will look at such issues 
in this section. 

102 
CHAPTER 6. DISTRIBUTED PROGRAMMING 
import j a v a .  u t i l  . * ;  import j a v a .  net .*; import j a v a .  io . * ;  
public class Connector { 
Serversocket l i s t e n e r  ; 
Socket [ I  
link ; 
public void Connect ( S t r i n g  basename, int myId, int nurnProc, 
BufferedReader [ I  dataIn , P r i n t w r i t e r  [ I  dataout) throws Exception { 
Name myNameclient = new Name(); 
link = new Socket [ numProc]; 
int localport = getLocalPort (myId); 
l i s t e n e r  = new ServerSocket ( l o c a l p o r t  ) ;  
/* 
r e g i s t e r  i n  t h e  name s e r v e r  */ 
myNanieclient . insertName ( basename + myId, 
( InetAddress. getLocalHost 
getHostName ( ) ,  localport ) ;  
/* a c c e p t  c o n n e c t i o n s  from a l l  t h e  s m a l l e r  p r o c e s s e s  */ 
for ( i n t  i = 0 ;  i < myId; i + + )  { 
Socket s = l i s t e n e r  . accept ( ) ;  
BufferedReader dIri = new BufferedReader ( 
String g e t l i n e  = dIn. readLine ( ) ;  
StringTokenizer s t  = new StringTokenizer ( g e t l i n e  ) ;  
int hisld = Integer . parseInt ( s t .  nextToken 
int destId = I n t e g e r .  parseInt ( s t .  nextToken 
St.ring tag = s t .  riextToken ( ) ;  
if ( t a g .  equals ( ” h e l l o ” ) )  { 
new InputStreamReader ( s .  getInputStream 
; 
link [ hisId ] = s ;  
dataIn [ hisId ] = dIn; 
dataOut, [ hisId ] = new P r i n t w r i t e r  ( s .  getoutputstream 
1 
1 /* c o n t a c t  a l l  t h e  b i g g e r  p r o c e s s e s  */ 
for ( int i = myId + 1; i < numProc; i ++) { 
PortAddr addr ; 
addr = inyNameclient . searchName ( basename + i ); 
Thread. sleep (100) ; 
do { 
} while ( a d d r .  getPort () == -1); 
l i n k [ i ]  = new 
a d d r . g e t P o r t  
dataout [ i ]  = new P r i n t w r i t e r  ( l i n k  [ i ] .  getoutputstream 
dataIn [ i ] = new BufferedReader (new 
InputStreamReader( link [ i 1. getInputStream 
/* send a h e l l o  messnge t o  P-a */ 
dataout [ i 1. flush ( ) ;  
dat,aOut [ i 1 .  p r i n t l n  (myId +” ” + i +” ” + ” hello ” + 3) 
n 
+ x Ilulln ) ;  
1 
1 
int getLocalPort ( i n t  i d )  { return Symbols. ServerPort + 10 + id ; } 
public void closesockets ( ) {  
try { 
l i s t e n e r  . close (); 
for ( i n t  i=O;i<link. l e n g t h ;  i++) link [ i ] .  close ( ) ;  
} catch ( Exception e )  { System. err . p r i n t l n  ( e ) ; }  
1 
1 
Figure 6.7: Connector class 

6.5. REMOTE METHOD INVOCATIONS 
103 
mport j a v a .  u t i l  . * ;  
Jublic class Msg { 
int s r c I d ,  d e s t I d ;  
String tag ; 
String msgBuf; 
public Msg(int s ,  int t ,  String msgType, String buf) { 
t h i s .  srcId = s ;  
destId = t ;  
tag = nisgType; 
msgBuf = buf; 
I 
I 
I 
I 
I 
public int getSrcId ( )  { 
return srcId ; 
public int getDestId ( )  { 
return destId ; 
public String getTag() { 
return t a g ;  
public String getMessage ( )  { 
return msgBuf ; 
public int getMessageInt () { 
StringTokenizer st = new StringTokenizer (msgBuf); 
return I n t e g e r .  parseInt ( s t .  nextToken 
I public static Msg parsehlsg( StringTokenizer st ) {  
int srcId = I n t e g e r .  parseInt ( s t .  nextToken 0); 
int destId = I n t e g e r .  parseInt ( s t .  nextToken 
String tag = s t .  nextToken ( ) ;  
String buf = s t .  n e x t T o k e n ( ” # ) ;  
return new Msg( srcId , destId , t a g ,  b u f ) ;  
1 public String t o s t r i n g  (){ 
String s = S t r i n g .  valueof ( s r c I d ) + ”  ” + 
String . valueof ( destId )+ ” ” + 
tag + ” ” + msgBuf + ”#”; 
return s ;  
I 
t 
Figure 6.8: Message class 

104 
CIIAPTER 6. DISTRIBUTED PROGRAMMING 
import j a v a .  u t i l  .*; 
import java . io . * ; 
public c l a s s  Linker { 
PrintWriter [ ]  dataout: 
BiifferetlReader [ I  
data111 ; 
BuffrredReader dIn ; 
i n t  myId, N; 
connector connector ; 
public IritLinkedList neighbors = new IntLinkedList ( ) ;  
public Linker ( S t r i n g  basename, i n t  i d ,  i n t  niimProc) throws Exception { 
myId = id ; 
N = nuinProc; 
data111 = new BufferedReader [ nuniProc] ; 
dat,aOut = new PrintWriter [ numProc]; 
Topology. readNeighhors (inyId, N ,  neighbors ) ;  
conncctor = new Connector ( ) ;  
connect,or. Connect ( basename, niyId, nuniF’roc, dataIn , dat,aOut ) ;  
1 public void sendMsg(int d r s t I d  , String t a g ,  String nisg) { 
+ msg + ”#”); 
dataOut [ drstId 1 .  priiitln (rnyId + ” ’’ + destId + ” ” + 
tag + ” ” 
dataout [ dest,Id 1 .  flush ( ) ;  
} public void seridMsg( i n t  destId , String t a g )  { 
sendhfsg( destId , t a g ,  ’’ 0 ” ) ;  
public void multicast, ( IntLinkedList destIds , String t a g ,  String insg){ 
for ( i n t  i =0; i < d e s t I d s .  size ( ) ;  i++) { 
sendMsg( tlrstlds . getEntry ( i  ) ,  t,ag, msg); 
J 
1 public 
receiveMsg ( i n t  fromId ) throws IOExceptioii 
{ 
String getline = dataIn [ f r o i d d ] .  rcadLine ( ) ;  
U t i l .  priiitln ( ”  received message ’’ + getline ) ;  
StriiigTokenizer s t  = new StringTokenizer ( g e t l i n e  ) ;  
i n t  srcId = Integer . parse.Int ( s t .  nextToken 
i n t  destId = I n t e g e r .  parseInt ( s t , .  riextToken 
String tag = s t .  nextToken ( ) ;  
String insg = s t .  riextTokeii(”#); 
r e t u r n  new Msg( srcId , destId , t a g ,  msg); 
1 
public i n t  get.MyId ( )  { r e t u r n  myId; } 
public i n t  
{ r e t u r n  N; } 
public void close ( )  { connector. closeSockets 
1 
Figure 6.9: Linker class 

6.5. REMOTE METHOD INVOCATIONS 
An RMI is implemented as follows. With each remote object. there is an 
sociated object at the client side and an object at the server side. An invocation 
to a method to a remote object is managed by using a local surrogate object at 
the client called the stub object. An invocation of a method results in packing the 
method name and the arguments in a message and shipping it to the server side. 
This is called parameter marshaling. This message is received on the server side 
by the server skeleton object. The skeleton object is responsible for receiving the 
message, reconstructing the arguments, arid then finally calling the method. Note 
that a RMI class requires compilation by a RMI compiler t,o generate the stub and 
the skeleton routines. 
Remote Objects 
An object is called remote object if its methods can be invoked from another Java 
virtual machine running on the same host or a different host. Such an object is de- 
scribed using a 
interface. An interface is remote if it extends java.rmi.Rernote. 
The remote interface serves to identify all remote objects. Any object that is a re- 
mote object must directly or indirectly implement this interface. Only those meth- 
ods specified in a remote interface are available remotely. Figure 6.10 gives a remote 
interface for a name service. 
import java . rmi . * ; 
public interface Nameservice extends Remote { 
public int search ( String s )  throws RemoteException ; 
public int insert (String s , String hostName, int portNumber) 
public int getport ( int index ) throws RemoteException ; 
public String getHostName( int index ) throws RemoteException ; 
throws RemoteException ; 
1 
Figure 6.10: Remote interface 
Any object that implements a remote interface and extends 
is a remote object. Remote method invocation corresponds to invocation of one of 
the methods on a remote object. We can now provide a class that implements the 
shown in Figure 6.11. 
To install our server, we first compile the file 
Then, we 
need to invoke the RMI compiler to generate the stub and skeleton associated with 
t,he server. On a UNIX machine, one may use the following commands to carry out 
these steps: 

106 
CHAPTER 6. DISTRIBUTED PROGRAMMING 
~~ 
~ 
~ 
mport j a v a . r m i . i ;  
mport java . rnii. server . UnicastRemoteObject ; 
u b 1 i c c 1 ass Name S e r vi c e I mpl extends U ii i c a s t Re mo t e 0 b j e c t 
final int niaxSize = 100; 
private String [ I  nanies = new String [maxsize]; 
private String [ I  hosts = new String [niaxSize]; 
private int [ I  ports = new int [ maxSize]; 
private int d i r s i z e  = 0; 
public NameServiceImpl ( )  throws RenioteException { 
public int search ( S t r i n g  s )  throws RemoteException { 
implements Nameservice { 
> 
for ( i n t  i = 0 ;  i < d i r s i z e  ; i++) 
return -1; 
if (names[ i 1 .  equals ( s ) )  return i ; 
1 
public 
int 
if 
nt i n s e r t  ( String s , String IiostName, int portNuniber) 
throws RcmoteException { 
oldIndex = s e a r c h ( s ) ;  // zs rt 
a l r e a d y  there 
(oldlndex == -1) && ( d i r s i z e  < rriaxSize)) { 
names[ d i r s i z e ]  = s ;  
hosts [ dirsize ] = IiostName; 
ports [ dirsize ] = portNuiriber; 
dirsize ++; 
return 1; 
return 0: 
} else 
public int getport ( i n t  index) throws RemoteException { 
return ports [ index ] ; 
1 
1 
public String getHostName ( int index ) throws RemoteExceptioii { 
return hosts [ index ] ; 
public s t a t i c  void inain(String args 
{ 
// c r e a t e  s e c u r i t y  manager 
System. setSecurityMariager (new R.MISecurityManager 
NarneServiceImpl obj = new NarneServiceImpl ( )  ; 
Naming. rebind ( ”  MyNameServer” , obj ) ; 
System. o u t .  printlii (”MyNarneServer bound in r e g i s t r y ” ) ;  
System. o u t .  p r i n t l n  (“NamcServiceImpl err : ” + e .  getMessage 
try { 
} catch (Exception e )  { 
1 
1 
Figure 6.11: A name service implementation 

6.5. REMOTE METHOD INVOCATIONS 
107 
> 
> 
> 
& 
Now assuming that the 
service is running on the machine, we can 
start our server. There is just one last thing that we need to take care of security. 
We need to specify who can connect to the server. This specification is done by a 
security policy file. For example, consider a file called 
as follows: 
“*:1024-65535”, 
; 
“ * : 8 0 “ ,  
1; 
This policy allows downloaded code, from any code base, to do two things: (1) 
connect to or accept connections on unprivileged ports (ports greater than 1024) 
on any host, or ( 2 )  connect to port 80 [the port for HTTP(Hypertext Transfer 
Protocol)]. 
Now we can start the NameServiceImpl server as follows: 
> 
6.5.2 Parameter Passing 
If a local object is passed 
an argument to a local method on a local object, then 
in Java we simply pass the reference to the object. However, if the method is to a 
remote object, then reference to a local object is useless at the other side. Therefore, 
arguments to remote methods are handled differently. 
There are three ways of passing arguments (and returning results) in remote 
method invocations. The primitive types in Java (e.g., 
and 
are passed 
by values. 
Objects that are not remote are passed by value using object serialization, which 
refers to the process of converting the object state into a stream of bytes. Any 
object that implements the interface 
can be communicated over the 
Internet using serialization. The object is written into a stream of bytes at one 
end (“serialized”) and at the other end it is reconstructed from the stream of bytes 
received (“deserialized). An interesting question is what happens if the object has 
references to other objects. In this case, those objects also need to be serialized; 
otherwise references will be meaningless at the other side. Thus, all objects that 
are reachable from that object get serialized. The same mechanism works when 

108 
CHAPTER 6. DISTRIBUTED PROGRAR,IMING 
a nonremote object is returned from a remote method invocation. Java supports 
referential integrity, that is, if multiple references to the same object are passed froni 
one Java Virtual Machine (JVM) to the other, then those references will refer to a 
single copy of the object in the receiving JVM. 
Finally, references to objects that implement 
interface are passed as 
remote references. In this case, the stub for the reniote object is passed. 
6.5.3 Dealing with Failures 
One differcnce betwecn invoking a local method and a remote method is that more 
things can go wrong when a remote method is invoked. The machine that contains 
the remote object, may be down, the connection to that machine be down, or t'he 
message sent may get corrupted or lost. In spite of all these possible problems, 
Java system guarantees at-most-ome semantics for a remote method invocat'ion: 
any invocation will result, in execution of the remote method at most once. 
6.5.4 Client Program 
The client program first, nceds to obt,ain a reference for the remote object. The 
j 
class provides methods t,o do so. It is a mechanism for obtaining 
references to reniote objects based on Uniforrn Resource Locator (URL) syntax. The 
UR.L for a remot,e object, is specified iising the usual host, port, and name: 
where 
is the host, namc of registry (defaults to current host), 
is the port 
number of registry (defaults to the registry port, number), and 
is the name for 
the remote object. 
The key methods in this class are 
We now show how a client can use 
to get a reference of t,he remote object 
and then invoke methods on it (see the program in Figure 6.12). 

6.6. OTHER USEFUL CLASSES 
109 
import java . rmi . * ; 
public class NameRmiClient { 
public static void main(String args 
{ 
try { 
Nameservice r = ( Nameservice) 
int i = r .  insert ( ” p l ”  , ” t i c k .  ece” , 2058); 
int j = r . s e a r c h ( ” p 1 ” ) ;  
if ( j  != -1) 
Naming. lookup (”rrni :// linux02 /MyNameServer” ); 
System.out. 
+”:” + r.getPort 
} catch (Exception e )  { 
System.out. println ( e ) ;  
1 
} 
1 
Figure 6.12: A RMI client program 
6.6 Other Useful Classes 
In this chapter, we have focused on classes that allow you t.o write distributed 
programs. For cases when a process simply needs data from a remote location, Java 
provides the Uniform Resource Locator (URL) class. A URL consists of six parts: 
protocol, hostname, port, path, filename, and document section. An example of a 
URL is 
The java.net.URL class allows the programmer to read data from a URL by 
methods such as 
This method returns a 
from which one can read the data. For different 
types of data such 
images and audio clips there are methods such 
and 
We will not concern ourselves with these classes and methods. 
6.7 Problems 
6.1. Make the 
class fault-tolerant by keeping two copies of the server 
Assume that the client chooses a server at random. 
process at all times. 

110 
CHAPTER 6 .  DISTRIBUTED PROGRAMMING 
If t,hat server is down (i.e., after the timeout), the client contacts the other 
server. You may assume that at most one server goes down. When the server 
comes up again, it would need to synchronize with the other server to ensure 
consistency. 
6.2. Message passing can also be employed for communication and synchronizat*ion 
Implement a Java, monitor library that provides message 
among threads. 
passing primitives for threads in a single Java Virtual Machine (JVM). 
6.3. Develop a 
class that provides synchronous messages. A message is 
synchronous if the sender of the message blocks until the message is received 
by the receiver. 
6.4. Give advantages and disadvantages of using synchronous messages (see Prob- 
lem 6.3) over asynchronous messages for developing distributed applications. 
6.5. Write a Java program t,o maintain a large linked list on multiple computers 
connected by a message passing system. Each computer maintains a part of 
t,he linked list. 
6.6. List all the differences between a local method invocation and a remote method 
invocation. 
6.7. How will you provide semaphores in a distributed environment? 
6.8. Solve the producer consumer problem discussed in Chapter 3 using messages. 
6.9. Give advantages and disadvantages of using RMI over TCP sockets for devel- 
oping distributed applications. 
6.8 Bibliographic Remarks 
Details on t,he Transmission Control Protocol can be found in the book by Comer 
[ComOO]. Remote procedure calls were first implemented by Birrell and Nelson 
[BN84]. 

Chapter 7 
Models and Clocks 
7.1 Introduction 
Distributed software requires a set of tools and techniques different from that re- 
quired by the traditional sequential soft,ware. One of the most important issues in 
reasoning about a distributed program is the model used for a distributed com,puta- 
tion. It is clear that when a distributed program is execukd, at the most abstract 
level, a set, of events is generated. Some examples of events are the beginning and 
the end of the execution of a function, and the sending and receiving of a message. 
This set alone does not characterize the behavior. We also impose an ordering rela- 
tion on this set. The first relation is based on the physical time model. Assuming 
that all events are instantaneous, that no two events are simultaneous, and that a 
shared physical clock is available, we can t,otally order all the events in the system. 
This is called the interleaving model of computation. If there is no shared physical 
clock, then we can observe a total order among events on a single processor but 
only a partial order between events on different processors. The order for events on 
different processors is determined on the basis of the information flow from one pro- 
cessor to another. This is the happened-before model of a distributed computation. 
We describe these two models in this chapter. 
In this chapter we also discuss mechanisms called clocks that can be used for 
tracking the order relation on the set of events. The first relation we discussed 
on events imposes a total order on all events. Because this total order cannot 
be observed, we describe a mechanism to generate a total order that could have 
happened in the system (rather than the one that actually happened in the system). 
This mechanism is called a logical clock. The second relation, happened-before, 
111 

112 
CHAPTER 7. MODELS AND CLOCKS 
can be accurately tracked by a vector clock. A vector clock assigns timestamps to 
states (and events) such that the happened-before relationship between states can 
be determined by using t,he tiniestamps. 
7.2 Model of a Distributed System 
We take t,he following characteristics as tzhe defining ones for distributed systems: 
0 Absence of a shared clock: In a distributed system, it is inipossible t,o synchro- 
riixe the clocks of different processors precisely due to uncertainty in conimu- 
Iiication dclays bet,ween them. As a result, it is rare to use physical clocks 
for synchronizat$ion in distributed systems. In this book we will see how ttlie 
concept of causality is used instead of time to tackle this problem. 
0 Absen,ce of shared m,emory: In a distributed system, it is impossible for any 
o~ic processor to know the global stak of the system. As a result, it is difficult 
to observe any global property of the system. In this book we will see how 
efficient algorithms can be developed for evaluating a suitably restricted set of 
global properties. 
Absence of accurate failure detection: In an asyrichrorious distributed system 
(a distributed syst,em is asynchronous if there is no upper bound on message 
delays), it is inipossible to distinguish between a slow processor and a failed 
processor. This leads to many difficulties in developing algorithms for consen- 
sus, election, and so on. In t.his book we will see these problems, arid their 
so1ut)ions when synchrony is assumed. 
Our model for a distributed syst.ern is based on message passing, and all of our 
algorit,hms a,re h s e d  a.rourir1 that concept. Our algorit'hms do not assume any upper 
bound on the message delays. Thus we assume asynchronous systems. An advantage 
is t,hat all the algorithms developed in this model are also applicable to synchronous 
systenis. 
We model a distribut,ed system as an asyrichrorious message-passing system with- 
out, any shared nieniory or a global clock. A distributed program consists of a set 
of N processes denotcd by {PI, PL, ..., PN} and a set of unidirectional channels. A 
channel connects two processes. Thus the topology of a distributed system can be 
viewed as a directed graph in which vertices represent the processes and the edges 
represent the channels. Figure 7.1 shows the topology of a distribiit,ed system with 
three processes and four channels. Observe that a bidirectional channel can simply 
be modeled as two unidirectional channels. 

7.2. MODEL OF A DISTRIBUTED SYSTEM 
113 
Figure 7.1: An example of topology of a distributed system 
A channel is assumed to have infinite buffer and to be error-free. We do not make 
any assumptions on the ordering of messages. Any message sent on the channcl may 
experience arbitrary but finite delay. The state of the channel at any point is defined 
to be the sequence of messages sent along that channel but not received. 
A process is defined as a sct of states, an initial condition (i.e., a subset of states), 
and a set of events. Each event may change the state of the process and the state of 
at most one channel incident on that process. The behavior of a process with finite 
states can be described visually with state transition diagrams. Figure 7.2 shows 
the state transition diagram for two processes. The first process PI sends a token 
to P2 and then receives a token from P2. Process P2 first receives a token from PI 
and then sends it back to PI. The state s1 is the initial state for PI, and the state 
tl is the initial state for P2. 
send token to P, 
\ 
receive token from P ,  
receive token from P, 
send token to P, 
Figure 7.2: A simple distributed program with two processes 

114 
CHAPTER 7. MODELS AND CLOCKS 
7.3 Model of a Distributed Computation 
In this section, we describe the interleaving and the happendbefore models for 
capturing behavior of a distributed system. 
7.3.1 Interleaving Model 
In this model, a distvibuted computation or a run is simply a global sequence of 
events. Thus all events in a run are interleaved. For example, consider a system 
with two processes: a bank server and a bank customer. The program of the bank 
cust,orner process sends two request messages to t,he bank server querying the savings 
and the checking accounts. On receiving the response, it adds up the total balance. 
In t,he interleaving model, a run may be given as follows: 
PI sends “what is rny checking balance” to Pz 
PI sends “what is my savings balance” to Pz 
P2 receives “what is my checking balance” from PI 
PI sets total to 0 
P2 receives “what is my savings balance” from PI 
P2 sends “checking balance = 40” to PI 
PI receives “checking balance = 
from Pz 
PI sets total to 40 (t,otal + checking balance) 
Pl sends “savings balance = 70” to PI 
PI receives “savings balance = 70” from P2 
PI sets total to 110 (total + savings balance) 
7.3.2 Happened-Before Model 
In t,he interleaving model, there is a total order defined on the set of events. Lamport 
has argued t,hatt in a true distributed system only a partial order, called a happened- 
before relation, can be determined hct,ween events. In this section we define this 
relation formally. 
As before, we will tie concerned with a single computat,ion of a distributed pro- 
gram. Each process Pi in that computation generates a sequence of events. It, is 
clear how to order events within a single process. If event e occurred before f in 
the process, then e is ordered before f .  How do we order events across processes? 
If e is t,he send event of a message and 
is the receive event of the same message, 
then we can order e before 
Combining these two ideas, we obtain the following 
definition. 
Definition 7.1 (Happened Before Relation) The happened-before relation. (+) 
is the smallest relation that sa.tisfies 

7.4. LOGICAL CLOCKS 
1. If e occurred before f in the same process, then e --+ f .  
2. If e is the send event of a message and f is the receive event of the same 
message, then e -+ f .  
3. If there exists an event g such that (e -+ g )  and 
-+ f ) ,  then ( e  -+ f ) .  
In Figure 7.3, e2 -+ e4, e3 -+ f 3 ,  and el -+ g4. 
\ 
Figure 7.3: A run in the happened-before model 
A run or a computation in the happened-before model is defined 
a tuple ( E ,  -+) 
where E is the set of all events and + is a partial order on events in E such that all 
events within a single process are totally ordered. Figure 7.3 illustrates a run. Such 
figures are usually called space-time diagrams, process-time diagrams, or happened- 
before diagrams. In a process-time diagram, e + f iff it contains a directed path 
from the event e to event f .  Intuitively, this relation captures the order that can be 
determined between events. The important thing here is that the happened-before 
relation is only a partial order on the set of events. Thus two events e and 
may 
not be related by the happened-before relation. We say that e and 
are concurrent 
(denoted by ellf) if T ( e  -+ f )  A - ( f  
-+ e). In Figure 7.3, e211f2, and el((g3. 
Instead of focusing on the set of events, one can also define a computation 
based on the the set of states of processes that occur in a computation, say S. 
The happened-before relation on S can be defined in the manner similar to t,he 
happened-before relation on E. 
Logical Clocks 
We have defined two relations between events based on the global total order of 
events, and the happened-before order. We now discuss mechanisms called clocks 
that can be used for tracking these relations. 
When the behavior of a distributed computation is viewed as a total order, it 
is impossible to determine the actual order of events in the absence of accurately 

116 
CHAPTER 7. MODELS AND CLOCKS 
synchronized physical clocks. If the system has a shared clock (or equivalently, 
precisely synchronized clocks), then timestamping the event with the clock would 
be sufficient to determine the order. Because in the absence of a shared clock the 
total order betaween events cannot be determined, we will develop a mechanism that 
gives a total order that could have happened instead of the total order that did 
happen. 
The purpose of our clock is only to give us an order between events and not any 
other property associated with clocks. For example, on the basis of our clocks one 
could riot det,ermine the time elapsed between two events. In fact, the number we 
associate with each event will have no relationship with the tirne we have on our 
watches. 
As we have seen before, only two kinds of order information can be determined in 
a distributed systern-the 
order of events on a single process and the order between 
the send and the receive events of a message. On the basis of these considerations, 
we get, the following definition. 
A 
clock C is a map from the set of events E to N (the set of natural 
numbers) with the following constraint: 
Ve,f E E :  e +  + C(e) < C ( f )  
Sornetirnes it is more convenient to timestamp states on processes rat,her t,han 
events. The logical clock C also sa.tisfies 
vs:t E 
: s + t =+ C(s) < C(t) 
The constraint for logical clocks models the sequential nature of execution at, 
each process and the physical requirement, that any message transmission requires 
a nonzero amount of time. 
Availability of a logical clock during distributed computation makes it easier 
to solve marly distributed problems. An accurate physical clock clearly satisfies 
the above mentioned condition and therefore is also a logical clock. However, by 
definition of a distributed system there is no shared clock in the system. Figure 7.4 
shows an iniplementation of a logical clock that does not use any shared physical 
clock or shared memory. 
The 
algorithni is dcscribed by the initial conditions and the actions taken for each everit 
t,ype. The algorithm uses the variable c to assign the logical clock. The not,at.ion 
s.c denotes the value of c in the state s. Let s.p denote the process t,o which st.ate 
s belongs. 
For any send event, the value of t.he clock is sent with the message and then 
increment'ed at line 14. On receiving a message, a process takes the maximum of its 
It, is not required that message conimuriication be ordered or reliable. 

7.5. VECTOR CLOCKS 
117 
own clock value and the value received with the message at line 17. After taking the 
maximum, the process increments the clock value. On an internal event, a process 
simply increments its clock at line 10. 
1 public class LamportClock { 
2 
int c ;  
3 
public L amportclock() { 
4 
c = 1; 
5
1
 
6 
public int g e t v a l u e ( )  { 
7 
return c ;  
8
)
 
9 
public void t i c k  ( )  { // o n  i n t e r n a l  e v e n t s  
10 
c = c + l ;  
11 
1 
12 
public void sendAction ( )  { 
13 
// i n c l u d e  c in message 
14 
c = c + l ;  
15 
} 
16 
public void receiveA ction (int src , int s e n t v a l u e )  { 
17 
c = U t i l  .max(c, s e n t v a l u e )  + 1; 
18 
1 
19 I 
Figure 7.4: A logical clock algorithm 
The following claim is easy to verify. 
v s , t  E s :  s + t =+ s.c < t.c 
In some applications it is required that all events in the system be ordered t,otally. 
If we extend the logical clock with the process number, then we get a total ordering 
on events. Recall that for any state s, s.p indicates the identity of the process to 
which it belongs. Thus the timestamp of any event is a tuple (s.c, s.p) and the t,otal 
order < is obtained 
def 
(s.c, 
< (t.c,t.p) = (s.c < t.c) v 
= t.c) A (s.p < t.p)). 
7.5 Vector Clocks 
We saw that logical clocks satisfy the following property: 
s + t =+ s.c < t.c. 

118 
CHAPTER 7. MODELS AND CLOCKS 
However, the converse is not true; s.c < t.c does not imply that s - t. The 
computation (S, -+) is a partial order, but the domain of logical clock values (the 
set of natural numbers) is a total order with respect to <. Thus logical clocks 
do not provide coniplete information about the happened-before relation. In this 
section, we describe a mechanism called a vector clock that allows us to infer the 
happened-before relation completely. 
Definition 7.2 (Vector Clock) A vector clock v is a map from S to 
(vectors 
of n.atura1 numbers) with the following constraint 
Qs, t : s + t @ s.v < t.v. 
where s.v is the vector assigned to the state s. 
Because + is a partial order, it is clear that the timestamping mechanism should 
also result in a partial order. Thus the range of the timestamping function cannot 
be a total order like the set of natural numbers used for logical clocks. Instead, 
we use vectors of natural numbers. Given two vectors 
and y of dimension N, we 
compare them 
follows: 
z < y = (Yk : 1 5 k 5 N : z [ k ]  5 y[k]) A 
( 3 j  : 1 5 5 
: 5 [ j ]  < y[j]) 
z < y  = ( z < y ) V ( z = y )  
It is clear that this order is only partial for N 
2. For example, the vectors (2,3,0) 
and (0,4,1) are incomparable. A vector clock timestanips each event with a vector 
of natural numbers. 
Our implementation of vector clocks uses vectors of size N ,  the number of pro- 
cesses in the system. The algorithm presented in Figure 7.5 is described by the 
initial conditions and the actions taken for each event type. A process increments 
its own component of the vector clock after each event. Furthermore, it includes 
a copy of its vector clock in every outgoing message. On receiving a message, it 
updates its vector clock by taking a coniponent,wise maximum with the vector clock 
included in the message. This is shown in bhe method 
It is not 
required that message communication be ordered or reliable. A sample execution of 
the algorit,hm is given in Figure 7.7. 
Figure 7.6 extends the 
class (defined in Chapter 6) to automatically 
include the vector clock in all out,going messages and to take the 
when a message is received. The method 
prefixes the message with the 
tag “vect,or” and the vector clock. The method 
is useful for appli- 
cation messages that do not use vector clocks. The method 
determines 

7.5. VECTOR CLOCKS 
119 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
1 public class Vectorclock { 
public int [ ]  v ;  
int mvId: 
i
I
 
int N; 
public Vectorclock ( int numProc, int id ) { 
myId = i d ;  
N = numProc; 
v = new int [ numProc] ; 
for ( i n t  i = 0 ;  i < N; i++) v [ i ]  = 0; 
v[myId] = 1; 
public void tick () { 
} 
} 
v [ myId] + ; 
public void sendAction () { 
/ / i n c l u d e  the vector i n  the message 
v [ myId]++; 
} public void receiveAction ( i n t  [ I  sentvalue ) { 
v l i ]  = U t i l  . m a x ( v \ i ] ,  s e n t V a l u e [ i ] ) ;  
for ( i n t  i = 0; i < N; i++) 
v [  myId]++; 
1 
1 
1 
public int getValue(int i )  { 
return v [  i ] ; 
public String t o s t r i n g  ( ) {  
return U t i l .  w r i t e A r r a y ( v ) ;  
Figure 7.5: A vector clock algorithm 

120 
CHAPTER 7. MODELS AND CLOCKS 
,ublic c l a s s  VCLinker extends Linker { 
public VectorClock vc ; 
i n t  receiveTag 
= n u l l ;  
public VCLinker( String baseiianie, i n t  id , i n t  N) throws Exception { 
super ( basenanie , id , N) ; 
vc = new VectorClock(N, i d ) ;  
receiveTag = new i n t  
I public void seridMsg(int destId , String t a g ,  String nisg) { 
super.seridMsg(destId , " v e c t o r " ,  v c .  t o s t r i n g  
super. sendMsg( destId , t a g ,  msg); 
v c .  sendAction ( ) ;  
1 
public void simpleSendMsg( i n t  destId ! String t a g ,  String msg) { 
super.sendMsg( destId , t a g ,  irisg); 
1 
public Msg receiveMsg ( i n t  froniId ) throws 
. io . IOExreption { 
Msg ml = super. receiveMsg ( f r o i d d  ) ;  
if ( r r i l .  getTng(). equals ( " v e c t o r " ) )  { 
Util . readArray (ml. getMessage ( ) ,  
v c .  r e c c i v e A c t i o n  (receiveTag ) ;  
Msg 111 = s u p e r .  receiveMsg ( fromId ); / / a p p  
messuge 
r e t u r n  m; 
receiveTag ); 
1 
else r e t u r n  nil; 
I 
t 
Figure 
The 
class that extends the 
class 

7.5. VECTOR CLOCKS 
121 
Figure 7.7: A sample execution of the vector clock algorithm 
whether the message has a vector clock in it. If it does, the method removes the 
vector clock, invokes 
and then returns the application message. 
We now show that s -+ t iff s.v < t.v. We first claim that if s # t, then 
ft t =+ t.v[s.p] < s.71[s.p] 
(7.1) 
If t.p = s.p, then it follows that t occurs before s. Because the local component 
of the vector clock is increased after each event, t.u[s.p] < s.v[s.p]. So, we assume 
that s.p # t.p. Since s.v[s.p] is the local clock of P, 
could not have seen 
this value 
s f, t ,  it follows that t.u[s.p] < s.u[s.p]. Therefore, we have that 
f. t )  
implies i ( s . u  < t.v). 
Now we show that ( s  + t )  implies (s.w < t.71). If s + t, then there is a message 
path from s to t. Since every process updates its vector on receipt of a message 
and this update is done by taking the componentwise maximum, we know that the 
following holds: 
'dk : s.v[k] 5 t.v[k]. 
Furthermore, since t f, s, from Equation (7.1), we know that s.v[t.p] is strictly l e s  
than t.v[t.p]. Hence, ( s  + t )  + (s.v < t.v). 
It is left 
an exercise to show that if we know the processes the vectors came 
from, the comparison between two states can be made in constant time: 
and Pt 
s + t * (s .v[s.p] 5 t.u[s.p]) A (s.v[t.p] < t.w[t.p]) 

122 
CHAPTER 7. MODELS AND CLOCKS 
7.6 Direct-Dependency Clocks 
One drawback wit,h the vector clock algorithm is that it requires O ( N )  integers to 
be sent with every message. For many applications, a weaker version of the clock 
suffices. We now describe a clock algorithm that is used by many algorithms in 
distributed systems. These clocks require only one integer to be appended to each 
message. We call these clocks direct-dependency clocks. 
The algorithm shown in Figure 7.8 is described by the initial conditions and the 
actions taken for each event type. On a send event, the process sends only its local 
component in t,he message. It also increments its component. as in vector clocks. 
The action for internal events is the same as that for vector clocks. When a process 
receives a message, it updates two components--one for itself, and the other for 
the process from which it received the message. It updates its own component in a 
manner identical to that for logical clocks. It also updates the component for the 
sender by taking the maximum wit'h the previous value. 
public class Directclock l 
public int [I 
c l o c k ;  
int myId; 
public Directclock ( i n t  nuniProc, int id ) { 
myId = id ; 
clock = new int [numProc]; 
for ( i n t  i = 0; i < numProc; i + + )  clock [ i ]  = 0 ;  
clork [myId] = 1; 
1 
bublic int getValue(int i )  { 
return clock [ i ] ; 
public void tick ( )  { 
1 
1 
clock [ myId]++; 
public void seridAction ( )  { 
// s e n t v a l u e  = c l o c k  [myld]; 
t i c k  ( ) ;  
1 
public void receiveAction ( i n t  sender , int sentvalue ) { 
clock [ sender ] = Util . max( clock [ sender 1 ,  sentvalue ) ;  
clock [myId] = Util .max( clock [myId], sentvalue) + 1 ;  
I 
Figure 7.8: A direct-dependency clock algorithm 
An example of a distributed computation and its associated direct-dependency 

MATRIX CLOCKS 
123 
Figure 7.9: A sample execution of the direct-dependency clock algorit.hm. 
clock is given in Figure 7.9. 
We first observe that if we retain only the ith component for the ith process, then 
the algorithm above is identical to the logical clock algorithm. However, our interest 
in a direct-dependency clock is not due to its logical clock property (Lamport’s 
logical clock is sufficient for that), but to its ability to capture the notion of direct 
dependency. We first define a relation, directly precedes (--+d), a subset of -+, 
follows: s -d 
t iff there is a path from s to t that uses at most one message in 
the happened-before diagram of the computation. The following property makes 
direct-dependency clocks useful for many applications: 
The proof of this property is left 
an exercise. The reader will see an application 
of direct-dependency clock in Lamport’s mutual exclusion algorithm discussed in 
Chapter 8. 

124 
CHAPTER 7. MODELS AND CLOCKS 
3ublic class Matrixclock { 
int [ I  
M; 
int niyId; 
int N; 
public Matrixclock ( int numProc-, int id ) { 
rnyId = id ; 
N = numProc; 
M =  new int[N][N]; 
for ( i n t  i = 0 ;  i < N; i++) 
M [ i ] [ j ]  = 0 ;  
for ( i n t  j = 0 ;  j < N; j++) 
R4[ myId] [ myId ] = 1 ; 
public void tick () { 
pVl[ myId ] [ myId] + ; 
} 
1 
public void sendhction ( )  { 
/ / i n c l u d e  
the matrix an the message 
M[ myId ] [ myId] + + ; 
1 public void receiveAction (int [ I  I ]  W, int srcId ) { 
// component-wise mazimum of m a t r i c e s  
for ( i n t  i = 0; i < N; i++) 
if ( i  != myId) { 
for ( i n t  j = 0 ;  j < N; j++) 
M[ i ] [ j ] = U t i l  .max(M[ i ] [ j 1 ,  w[i ] [ j I ) ;  
I 
// update the v e c t o r  f o r  t h i s  process 
for ( i n t  j = 0 ;  j < 
j++) 
M[myId][ j ]  = Util.max(M[niyId][j], w [ s r c I d ] [ j ] ) ;  
M[ rnyId ] [ my Id] + + ; 
1 
} 
public int g e t v a l u e (  int i ,  int j )  { 
return M[ i ] [ j ] ; 
t 
Figure 7.10: The matrix clock algorithm 

7.7. MATRIX CLOCKS 
7.7 Matrix Clocks 
It is natural to ask whether using higher-dimensional clocks can give processes addi- 
tional knowledge. The answer is “yes.” A vector clock can be viewed as a knowledge 
vector. In this interpretation, s.w[i] denotes what process s.p knows about process 
in the local state s. In some applications it may be important for the process to have 
a still higher level of knowledge. The value s . u [ i , j ]  could represent what process s.p 
knows about what process 
knows about process j. For example, if s.v[i, s.p] > k 
for all 
then process s.p can conclude that everybody knows that, its state is strictly 
greater than k. 
Next, we discuss the matrix clock that encodes a higher level of knowledge than a 
vector clock. The matrix clock algorithm is presented in Figure 7.10. The following 
description applies to an N x N matrix clock in a system with N processes. The 
algorithm is easier to understand by noticing the vector clock algorithm embedded 
within it. If we focus only on row m y l d  for process Pmy~d, 
the algorithm presented 
above reduces to the vector clock algorithm. Consider the update of the matrix 
in the algorithm in Figure 7.10 when a message is received. The first step affects 
only rows differcnt from myId and can be ignored. When a matrix is received from 
process srcld, then we use only the row given by the index srcId of the matrix W for 
updating row m y I d  of Pmyld. Thus, from our discussion of vector clock algorithms, 
it is clear that 
v s , t  : s.p f t.p : s + t = s.M[s.p, .] < t.M[t.p, .] 
The ot,her rows of the matrix M keep the vector clocks of other processes. Note 
that initially M contains 0 vector for other processes. When it receives a matrix 
in W ,  it updates its information about the vector clock by taking componentwise 
maximum. 
We now show an application of matrix clocks in garbage collection. Assume 
that a process 
generated some information when its matrix clock value for M[i][i] 
equals k. Pi sends this information directly (or indirectly) to all processes and wants 
to delete this information when it is known to all processes. We claim that Pi can 
delete the information when the following condition is true for the matrix M :  
vj : M[j][i] k 
This condition implies that the vector clock of all other processes j have ith compo- 
nent at least k. Thus, if the information is propagated through messages, 
knows 
that all other processes have received the information that 
had when M[i][i] 
was 
k. 
We will later see another application of a variant of matrix clock in enforcing 
causal ordering of messages discussed in Chapter 12. 

126 
CHAPTER 7. MODELS AND CLOCKS 
7.8 Problems 
7.1. Give advantages and disadvantages of a parallel programming model over a 
distributed system (message based) model. 
7.2. Show t>hat “concurrent with” is not a transitive relation. 
7.3. Write a program that takes as input a distributed computation in the happened- 
before model and outputs all interleavings of events that are compatible with 
the happened-before model. 
7.4. We discussed a method by which we can totally order all events within a sys- 
tern. If two events have the same logical time, we broke the t,ie using process 
identifiers. This scheme always favors processes with smaller identifiers. Sug- 
gest a scheme that does not have this disadvantage. (Hint: Use the value of 
the logical clock in determining the priority.) 
7.5. Prove the following for vector clocks: s + t iff 
(s.v[s.p] 5 t.v[s.p]) A (s.v[t.p] < t.w[t.p]). 
7.6. Suppose that the underlying communication system guarantees FIFO ordering 
of messages. How will you exploit this feature to reduce the communication 
complexity of the vector clock algorithm? Give an expression for overhead 
savings if your scheme is used instead of the traditional vector clock algorit,hm. 
Assume that any process can send at most m messages. 
7.7. Assume that you have implemented the vector clock algorithm. However, some 
application needs Lamport’s logical clock. Write a function convert that takes 
input a vector timestamp and outputs a logical clock timestamp. 
7.8. Give a distributed algorithm to maintain clocks for a distributed program 
that has a dynamic number of processes. Assume that there are the following 
events in the life of any process: start-process, internal, send, receive, fork, 
join processid, terminate. It should be possible to infer the happened-before 
relation using your clocks. 
7.9. Prove the following for direct-dependency clocks: 
VS,t s.p # t.p : (s -)d t )  
(s.v[s.p] 5 t.v[s.p]) 
7.10. Show that for matrix clocks, the row corresponding to the index s.p is bigger 
than any other row in the matrix s.M for any state s. 

7.9. BIBLIOGRAPIHC REMARKS 
127 
7.9 Bibliographic Remarks 
The idea of logical clocks is from Lamport [Lam78]. The idea of vector clocks in pure 
form first appeared in papers by Fidge and Mattern [Fid89, Mat891. However, vec- 
tors had been used before in some earlier papers (e.g., [SY85]). Direct-dependency 
clocks have been used in mutual exclusion algorithms (e.g., [Lam78]), global prop- 
erty detect,ion (e.g., [GarSG]), and recovery in distributed systems. Matrix clocks 
have been used for discarding obsolete information [SL87] and for detecting rela- 
tional global predicates [TG93]. 

This Page Intentionally Left Blank

Chapter 8 
Resource Allocation 
8.1 Introduction 
In a distributed system mutual exclusion is often necessary for accessing shared 
resources such as data. For example, consider a table that is replicated on multiple 
sites. Assume that operations on the table can be issued concurrently. For their 
correct,ness, we require that all operations appear atomic in the sense that the effect 
of the operations must appear indivisible to the user. For example, if an update 
operation requires changes to two fields, z and y, then another operation should 
not read the old value of z and the new value of y. Observe that in a distribiit,ed 
system, there is no shared memory and therefore one could not use shared objects 
such as semaphores to implement the mutual exclusion. 
Mutual exclusion is one of the most studied topics in distributed systems. It 
reveals many important issues in distributed algorithms such as safety and liveness 
properties. We will study three classes of algorithms--timestamp-based 
algorithms, 
token-based algorithms and quorum-based algorithms. The timestamp-based algo- 
rithms resolve conflict in use of resources based on timestamps assigned to requests 
of resources. The token-based algorithms use auxiliary resources such as tokens to 
resolve the conflicts. The quorum-based algorithms use a subset of processes to 
get permission for accessing the shared resource. All algorithms in this chapter 
sume that there are no faults in the distributed system, that is, that processors and 
communication links are reliable. 
129 

130 
CHAPTER 8. RESOURCE ALLOCATION 
8.2 Specification of the Mutual Exclusion Problem 
Let a system consist of a fixed number of processes and a shared resource called 
the critical section. An example of a critical section is the opera.tion performed on 
the replicated table introduced earlier. The algorithm to coordinate access to t>he 
critical section must satisfy the following properties: 
Safety: Two processes should not have permission to use the critical section simul- 
t aneously. 
Liveness: Every request for the critical section is eventually granted. 
Fuirness: Different requests must be granted in the order they are made. 
We can abstract this problem as implementation of a lock in a distributed envi- 
ronment. The interface 
is as follows: 
public interface Lock extends MsgHandler { 
public void requestCS ( ) ;  //may 
block 
public void releaseCS ( ) ;  
1 
Any lock implementation in a distributed environment will also have to handle 
messages that are used by the algorithm for locking. For this we use the interface 
shown below. 
import java . io . * ;  
public interface MsgHandler { 
public void handleMsg(Msg m, int s r c I d ,  String t a g ) ;  
public Msg receiveMsg ( int fromId ) throws IOException ; 
1 
Any implementation of the lock can be exercised by the program shown in Figure 
8.1. Line 8 creates a Linker that links all the processes in the system. After instan- 
tiating a lock implementation at lines 10- 17, we start separate threads to listen for 
messages from all the other processes at lines 18-20. The class 
is 
shown in Figure 8.2. A 
is passed a 
on its construction. 
It makes a blocking 
call at line 12, and on receiving a message gives it 
t,o the 
at line 13. 
Most of our distributed programs in this book will extend the class 
shown in Figure 8.3. This will allow processes to have access to its identifier 
the tot,al number of processes 
and simple send and receive routines. The method 
is empty, and any class that extends 
is expected to override 
this method. 

8.2. SPECIFICATION OF THE MUTUAL EXCLUSION PROBLEM 
131 
1 public class LockTester { 
2 
public static void main( String [ I  
a r g s )  throws Exception { 
3 
Linker comm = null; 
5 
String baseName = args [ 01 ; 
6 
7 
int numProc = Integer . parseInt (args 121); 
8 
comm = new Linker (baseName , myId , numProc); 
9 
Lock lock = null; 
4 
try { 
int myId = Integer . parseInt ( args [ 11 ) ; 
10 
if ( args [ 31. equals ( ”  Lamport” ) )  
11 
lock = new LamportMutex (comm) ; 
12 
if ( args [ 3 ] .  equals (”RicartAgrawala”)) 
13 
lock = new RAMutex(comm); 
14 
15 
lock = new DinMutex(comm); 
16 
if ( args 131. equals (” CircToken”)) 
17 
18 
for ( i n t  i = 0 ;  i < numproc; i + + )  
19 
if ( i  != myId) 
20 
21 
while (true) { 
22 
System. o u t .  p r i n t l n  (myId + ” is not in CS”); 
23 
Util . mysleep (2000); 
24 
25 
Util . mysleep (2000); 
26 
System. o u t .  p r i n t l n  (myId + ” is in CS *****” ) ;  
27 
if ( args [ 31 . equals ( ”  DiningPhil ” ) )  
lock = new CircToken (comm,O); 
(new ListenerThread ( i  , ( MsgHandler) lock ) ) .  s t a r t  ( ) ;  
lock . requestCS ( )  ; 
lock . releaseCS ( )  ; 
28 
1 
29 
1 
30 
catch ( InterruptedException e )  { 
31 
if (comm != null) comm. close (); 
32 
1 
33 
catch (Exception e )  { 
34 
System. o u t .  p r i n t l n  ( e ) ;  
35 
e .  printStackTrace (); 
36 
1 
37 
1 
38 1 
Figure 8.1: Testing a lock implementation 

132 
CHAPTER 8. RESOURCE ALLOCATION 
1 import j a v a .  io *; 
2 public class ListenerThread extends Thread { 
3 
int channel; 
MsgHandler process ; 
5 
public ListenerThread ( i n t  channel , MsgHandler process ) { 
6 
t h i s .  channel = channel ; 
7 
t h i s .  process = process ; 
9 
public void run ( )  { 
8
1
 
10 
while ( t r u e )  { 
11 
12 
13 
14 
15 
16 
t r y  { 
Msg m = process . receiveMsg (channel ); 
process . 1iandleMsg (In, in. getSrcId ( ) ,  m. getTag ( ) )  ; 
System. e r r .  println ( e ) ;  
} catch (IOException e )  { 
1 
Figure 8.2: ListenerThread 
8.3 Centralized Algorithm 
There are many algorithms for niutual exclusion in a distributed system. How- 
ever, the least expensive algorithm for the mutual exclusion is the centralized algo- 
rithm shown in Figure 8.4. If we are required to satisfy just the safety and liveness 
properties! then this simple queue-based algorithm works. One of the processes is 
designated as the leader (or t.he coordinat,or) for the critical section. The variable 
haveToken is true for t8he process t,hat has access t30 the critical section. Any process 
that wants to enter the critical section sends a request message to the leader. The 
leader siriiply puts these requests in the pendingQ in the order it, receives then]. It 
also grants permission to t,he process that is at the head of the queue by sending an 
okay message. When a proccss has finished execiiting its critical section, it sends 
the release message to the leader. On receiving a release message, the leader sends 
the okay message to the next. process in its pending9 if the queue is nonempty. 
Ot'herwise, the leader sets haveToken to true. 
The cpntralized algorithm does not, satisfy the notion of fairness, which says that 
requests should be grarked in the order they are made and not in the order they are 
received. Assunie that the process Pi makes a request for the shared resource to t,he 
leader process 4. After making the request, Pi sends a message to the process Pj. 
Now, Pj sends a request to Pk that reaches 
earlier than the request made by the 

8.3. CENTRALIZED 
mport java . io . * ; import java . lang . * ; 
Iublic class Process implements MsgHandler { 
int N, myId; 
Linker comm; 
public Process (Linker initComm) { 
c o r n  = initComm; 
myId = comrn. getMyId ( )  ; 
N = corm. getNumProc ( ) ; 
1 
1 
public synchronized void handleMsg(Msg m, int s r c  , String t a g )  { 
public void sendMsg( int destId , String t a g ,  String msg) { 
U t i l .  p r i n t l n  (”Sending msg t o  ” + destId + ”:” +tag + ” ” + msg); 
comm. sendMsg( destId , t a g ,  msg); 
} 
1 
public void sendMsg(int destId , String t a g ,  int msg) { 
sendMsg( destId , t a g ,  S t r i n g .  valueOf(msg)+” ” ) ;  
public void seiidMsg(int d e s t I d ,  String t a g ,  int msgl, int msg2) { 
sendMsg( destId , t a g ,  S t r i n g .  valueof (msgl) 
+” ” +String . valueof (msg2)+” ” ) ; 
1 
1 
public void sendMsg(int d e s t I d ,  String t a g )  { 
sendMsg( destId , t a g ,  ” 0 ” ) ;  
public void broadcastMsg ( S t r i n g  t a g ,  int msg) { 
for ( i n t  i = 0 ;  i < N; i++) 
if ( i  != myId) sendMsg(i, t a g ,  msg); 
1 
public void sendToNeighbors ( S t r i n g  t a g ,  int msg) { 
for ( i n t  i = 0; i < N; i++) 
i f  ( isNeighbor ( i  ) )  sendMsg( i , t a g ,  msg); 
1 
public boolean isNeighbor ( i n t  i ) { 
if (comm. neighbors. contains ( i  ) )  return t r u e ;  
else return f a l s e ;  
1 
public Msg receiveMsg ( i n t  froniId) { 
try { 
return comm. receiveMsg (fromId ); 
System.out. p r i n t l n  ( e ) ;  
comm. close ( ) ;  
return null; 
} catch (IOException e){ 
1 
t r y  { 
1 
1 
public synchronized void mywait ( )  { 
wait ( ) ;  
} catch ( InterruptedException e )  {System. e r r .  p r i n t l n  ( e ) ;  
1 
Figure 8.3: Process.java 

CHAPTER 8. RESOURCE ALLOCATION 
public class CentMutex extends Process implements Lock { 
// a s s u m e s  t h a t  P-0 c o o r d z n a t e s  a n d  d o e s  n o t  r e q u e s t  l o c k s .  
boolean haveToken ; 
final int leader = 0; 
ImtLinkedList 
public CentMutex( Linker initComm) { 
pending$ = new IntLinkedList ( ) ;  
super (initComm ) ; 
haveToken = (myId == leader ); 
public synchronized void requestCS ( )  { 
} 
sendMsg ( leader , ” request ” ) ; 
while ( ! have’l’oken) myWait () ; 
1 
public synchronized void releaseCS ( )  { 
sendMsg( leader . ” r e l e a s e ” ) ;  
haveToken = false; 
1 
public synchronized void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals ( ” r e q u e s t ” ) )  { 
sendMsg ( src , ”okay” ) ; 
have’l’okeri = false; 
if ( haveToken){ 
1 
else 
pendingQ . add ( src ) ; 
} else if ( t a g .  equals ( ”  r e l e a s e ” ) )  { 
if ( ! pendirigQ . isEmpty ( ) )  { 
int pid = pendingQ . removeHead ( )  ; 
srndMsg ( pid , ” okay” ) ; 
haveToken = true ; 
} else if ( t a g .  equals ( ” o k a y ” ) )  { 
} else 
haveToken = true ; 
notify (); 
~ 
Figure 8.4: A centralized mutual exclusion algorithm 

8.4. LAMPORT’S ALGORITHM 
process 
This example shows that it is possible for the order in which requests 
are received by the leader process to be different from the order in which they are 
made. The modification of the algorithm to ensure fairness is left 
an exercise (see 
Problem 8.1). 
8.4 Lamport’s Algorithm 
In Lamport’s algorithm each process maintains a logical clock (used for timestamps) 
and a queue (used for storing requests for the critical section). The algorithm ensures 
that processes enter the critical section in the order of timestamps of their requests. 
It assumes FIFO ordering of messages. The rules of the algorithm are as follows: 
0 To request the critical section, a process sends a timestamped message to all 
other processes and adds a timestamped request to the queue. 
0 On receiving a request message, the request and its timestamp are stored in 
the queue and a timestamped acknowledgment is sent back. 
0 To release the critical sect.ion, a process sends a release message to all other 
processes. 
0 On receiving a release message, the corresponding request is deleted from the 
queue. 
0 A process determines that it can access the critical section if and only if (1) it 
has a request in the queue wit,h timestamp t, (2) t is less than all other requests 
in the queue, and (3) it has received a niessage from every other process with 
timestamp greater than t (the request acknowledgments ensure this). 
Figure 8.5 gives an implementation of this algorithm in Java. In this version, 
every process maintains two vectors. These two vectors simulate the queue used in 
the informal description given earlier. These vectors are interpreted at process 
as follows: 
q [ j ]  : the timestamp of the request by process Pj. The value 
signifies that 
does not have any record of outstanding request by process 
Pj . 
u[j] : the timestamp of the last message seen from Pj if j # 
The component s.u[i] 
is simply 
represents the value of the logical clock in state s. Thus the vector 
the direct-dependency clock. 

136 
CHAPTER 8. RESOURCE ALLOCATION 
To request the critical section (method 
Pi simply records its clock in 
q[i]. Because all other processes also maintain this information, “request” messages 
are sent to all processes indicating the new value of q[i]. It then siniply wait,s for 
the condit,ion 
to become true. 
To release t,he critical section (method 
Pi simply resets q[i] to 
and sends “release” messages to all proccsses. Finally, we also require processes 
to acknowledge any request message 
shown in the method 
Note 
that, every message is tiniestaniped and when it is received, the vector 
is updated 
according t,o the direct-dependency clock rules as discussed in Chapter 7. 
Process Pi has permission to access the critical section when there is a request 
from Pi with its timestamp less than all other requests and Pi has received a message 
from every other process with a timestamp great,er than the timestamp of its own 
request. Since two requests may have identical timestamps, we extend the set. of 
timestamps to a total order using process identifiers as discussed in Chapter 7. Thus, 
if two requests have the same timestamp, t,hen the request by the process witjli the 
smaller process number is considered smaller. Formally, Pi can enter the critical 
section if 
b : j # : ( q [ i ] , i )  < ( , ~ ~ [ j ] , j )  
A (q[i],i) < ( q [ j ] , j )  
This condition is checked in the method 
Lamport’s algorithm requires 3(N - 1) messages per invocation of the critical 
section: N - 1 request messages, N - 1 acknowledgrnent messages, and N - 1 
release messages. There is a time delay of two serial messages to get permission for 
the critical section- a request message followed by an acknowledgment. The space 
overhcad per process is tshe vectors q and 7) which is O(Nlogm), where m is t’lie 
niaximuin number of times any process enters the critical section. 
8.5 
Ricart and Agrawala’s Algorithm 
Ricart and Agrawala’s algorithm uses only 2(N - I) messages per invocation of 
the critical section. It does so by eonibining the functionality of acknowledgment 
and release messages. In this algorithm, a process does not always send back an 
ackriowledgnient on receiving a request. It may defer the reply for a later time. 
Another advantage of Ricart and hgrawala’s algorithm is that it does not require 
FIFO ordering of messages. 
The algorithm is statc,d by the following rules: 
0 To request a resource, the process sends a timestamped message to all pro- 
c’essts. 

8.5. RICART AND AGRAWALA’S ALGORI‘rHM 
137 
public class LamportMutex extends Process implements Lock { 
Directclock v ;  
int [ I  q ;  // request queue 
public LarnportMutex( Linker initcomrn) { 
super ( innitcomm 
v = new DirectClock(N, myId); 
q = new int [ N ] ;  
for ( i n t  j = 0 ;  j < N; j++) 
q [ j ]  = Symbols. I n f i n i t y ;  
1 
public synchronized void requestCS 
v .  tick ( ) ;  
q[myId] = v .  getValue(my1d); 
broadcastA4sg ( ” r e q u e s t  ” , q [  niyId 
while (! okayCS ( ) )  
mywait ( ) ; 
public synchronized void releaseCS ( )  { 
q[myId] = Symbols. I n f i n i t y  ; 
b r o a d c a s t M s g ( ” r e l e a s e ” ,  v .  getValue(my1d)); 
1 
boolean okayCS ( )  { 
for ( i n t  j = 0 ;  j < N; j++){ 
return false ; 
return false ; 
if ( isGreater ( q [  myId], myld, q [  j 1 ,  j ) )  
if ( i s G r e a t e r  (q[myId], myId, v . g e t V a l u e ( j ) ,  j ) )  
1 
return true ; 
1 
boolean isGreater (int e n t r y l  , int pidl , int e n t r y 2 ,  int pid2) 
if ( entry2 == Symbols. I n f i n i t y  ) return false ; 
return ( (  e n t r y l  > e n t r y 2 )  
I I ( (  e n t r y l  == e n t r y 2 )  && ( p i d l  > pid2 ) ) ) ;  
1 
public synchronized void handleMsg(Msg m, int src , String tag 
int timestamp = m. getMessageInt ( ) ;  
v .  receiveAction ( s r c  , timestamp); 
if ( t a g .  e q u a l s ( ” r e q u e s t ” ) )  { 
q [  s r c ]  = timestamp; 
sendMsg( src , “ a c k ” ,  v. getValue(my1d)); 
q [  src ] = Symbols. I n f i n i t y  ; 
} else if ( t a g .  equals ( ”  r e l e a s e ” ) )  
notify ( ) ;  // okayCS() may be true now 
1 
t 
Figure 8.5: Lamport’s mutual exclusion algorithm 

138 
CHAPTER 8. RESOURCE ALLOCATION 
0 On receiving a request. from any other process, the process sends an okay 
message if either the process is not interested in the critical section or its own 
request has a higher timestamp value. Otherwise, that process is kept in a 
periding queue. 
0 To release a resource, the process sends okay to all the processes in the pending 
qucue . 
0 The process is granted the resource when it has requested the resource and 
it. has received the okay message from every other process in response t'o itts 
request message. 
The algorit,hm is presented formally in Figure 8.6. There are two kinds of mes- 
sages in the system-request 
messages and okay messages. Each process maintains 
the logical time of its request, in the variable myts. In the method 
a prc- 
cess simply broadcasts a request message with its timestamp. The variable nuniOkay 
counts the number of okay messages received since the request was made. On re- 
ceiving any request with a timestamp lower than its own, it replies immediately 
with okay. Otherwise, it adds t,hat, process to pendingQ. 
The algorithm presented above sat,isfies safety, liveness, and fairness properties 
of mutual exclusion. To see the safety property, assume that Pi and Pj are in the 
critical section concurrently and Pi has the smaller value of the timestamp for its 
request. Pj can enter t,he critical section only if it received okay for its request. 
The request, made by Pj can reach Pi only after Pi has made its request; otherwise, 
t,he t,imcstamp of Pi's request would have been greater because of the rules of the 
logical clock. From the algorithm, Pi cannot send okay unless it has exited from the 
critical section cont,radicting our earlier assumption that Pj received okay from Pi. 
Thus the safet,y property is not violatled. The process with the least timestamp for 
its request, can never be deferred by any other process, and therefore the algorithm 
also satisfies liveness. Because processes enter the critical section in the order of the 
t,imestanips of the requests, t'he fairness is also hue. 
It, is easy t,o see t,hat every critical section execution requires N - 1 request 
messages and N - 1 okay messages. 
8.6 
Dining Philosopher Algorithm 
In the prcvious algorithm, every critical section invocation requires 2(N - 1) mcs- 
sages. We now show an algorithm in which 2(N - 1) messages are required only 
in the worst case. Consider a large distributed system in which even though N is 
large, the number of processes that request the critical section, say, n, is small. In 

8.6. DINING PHILOSOPHER ALGORITHM 
139 
import j a v a .  u t i l  .*; 
public class RAMutex extends Process implements Lock { 
int myts; 
Lamportclock c = new Lamportclock ( )  ; 
IntLinkedList pendingQ = new IntLinkedList (); 
int numOkay = 0; 
public RAMutex( Linker initComm) { 
super (initComm ) ; 
myts = Symbols. I n f i n i t y  ; 
1 
I public synchronized void requestCS 
c .  tick ( ) ;  
myts = c . getvalue ( )  ; 
broadcastMsg ( ”  request ” , myts) ; 
numOkay = 0; 
while (numOkay < N-1) 
myWait ( ) ; 
I
1
 
public synchronized void releaseCS ( )  { 
myts = Symbols. I n f i n i t y  ; 
while ( !  pendingQ . isEmpty ( ) )  { 
int pid = pendingQ . removeHead ( )  ; 
sendMsg(pid, ”okay”, c .  getvalue 0); 
1 
1 
public synchronized void handleMsg(Msg m, int src , String tag { 
int timestamp = m. getMessageInt ( ) ;  
c .  receiveAction ( s r c  , timestamp); 
if ( t a g .  equals ( ”  request ” ) )  { 
if (( myts == Symbols. I n f i n i t y  ) // not interested in CS 
1 1  ( timestamp < myts) 
I I ( (  timestamp == myts) && ( s r c  < myId))) 
sendMsg ( src , ” okay” , c . getvalue ( )  ) ; 
pendingQ . add ( src ) ; 
else 
} else if ( t a g .  equals ( ” o k a y ” ) )  { 
numOkay++; 
if (numOkay == N - 
notify ( ) ;  // okayCS() may be true now 
1 
} 
1 
Figure 8.6: Ricart and Agrawala’s algorithm 

140 
CHAPTER 8. RESOURCE ALLOCATION 
our next algorithm, processes that are not interested in the critical section will not 
be required to send messages eventually. 
The next algorithm will also solve more general problem, the dining philosopher 
problem, where 
resource may not be shared by all the processes. The dining 
philosopher problem, as discussed in Chapter 3, consists of multiple philosophers 
who spend their time thinking and eating spaghetti. However, philosopher requires 
shared resources, such as forks, to eat spaghetki. We are required to devise a protocol 
to coordinate access to the shared resources. 
There are two requirenieiits on the solution of the dining philosopher problem: 
(1) we require mutually exclusive use of shared resources, that is, 
shared resource 
should riot be used by more than one process at 
time; and (2) we want freedom 
from starvation. Every philosopher (process) should be able to eat (perform its 
operation) infinitely often. 
The crucial problem in resource allocation is that of resolving conflicts. If a set, 
of processes require resource and only one of them can use it at 
time, then there 
is a conflict that must be resolved in favor of one of these processes. We have already 
studied one conflict resolution method via logical clocks in Lamport’s and Ricart 
and Agrawala’s niut,uitl exclusion algorithms. The processes used logical clocks to 
resolve access to mutual exclusion. If two requests had the same logical clock value, 
t,hen process identity was used to break ties. Now we study another mechanism that 
resolves coIiflict,s based on locaticin of auxiliary resources. The auxiliary resources 
are used only for conflict resolution and are not actual resources. 
We niodel the problem as an undirected graph called 
conflict graph, in which 
each node represents a process and an edge between process 
and P’ 
denotes 
that one or more resources are shared between Pi and P’. Figure 8.7(a) shows the 
conflict graph for five philosophers. If 
process needs all the shared resources for 
performing its operation, then only one of any t,wo adjacent nodes can perform its 
operation in any step. The conflict graph for a simple mutual exclusion algorithm 
is a complete graph. 
Now consider the problem of five dining philosophers sitting around a table such 
t8hat. tjwo adjacent philosophers share a fork. The conflict graph of this problem is a 
ring on five nodes. 
An orientation of an undirected graph consists of providing direction to all edges. 
The edge between Pi and Pj points from Pi to P’ 
if 
has precedence over P’. We say 
that an orientation is acyclic if the directed graph that results from the orientfation 
is acyclic. Figure 8.7(b) shows an acyclic orientation of the conflict graph. In a 
directed graph, we call a node source if it does not have any incoming edge. Any 
finite-directed acyclic graph must have at least one source (see Problem 8.5). In 
Figure 8.7, processes P2 and P4 are sources. 
To maintain orientation of an edge, we use the notion of an auxiliary resource, a 

8.6. DINING PHILOSOPHER ALGORITHM 
Figure 8.7: (a) Conflict graph; (b) an acyclic orientation with P2 and P4 as sources; 
(c) orientation after Pz and P4 finish eating 
fork, associated with each edge. Process Pi is considered to have the fork associated 
with the edge ( i , j ) ,  if it has precedence over Pj in any conflict resolution. 
The algorithm for dining philosophers obeys the following two rules: 
0 Eating rule: A process can eat only if it has all the forks for the edges incident 
to it, t'hat is, a process can eat only when it is a source. 
0 Edge reversal On finishing the eating session, a process reverses orientations 
of all the outgoing edges to incoming edges. 
Now let us look at the rules for transmitting forks. We do not require that' once 
a philosopher has finished eating it sends all the forks to its neighbors. This is 
because its neighbors may be thinking and therefore not interested in eating. Thus 
we require that if a philosopher is hungry (interested in eating) and does not have 
the fork, t>hen it should explicitly request the fork. To request the fork, we use a 
request token associated with each fork. Although a fork is not transmitted after 
eating, we still need to capture the fact that. the other philosopher has priority over 
this fork to satisfy the edge reversal rule. Thus we need to distinguish the case when 
a philosopher has a fork but has not used it from the case when the philosopher 
has the fork and has used it for eating. This is done conveniently by associating a 
boolean variable dirty with each fork. Once a philosopher has eaten from a fork, it 
becomes dirty. Before a fork is sent to the neighbor, it is cleaned. 
Our solution is based on keeping an acyclic conflict resolution graph as mentioned 
earlier. Philosopher 
has priority over philosopher 
if the edge between u and 
points to 
The direction of the edge is from 
to if (1) holds the fork and it is 
clean, (2) 
holds the fork and it is dirty, or (3) the fork is in transit from 
to 

CHAPTER 8. RESOURCE ALLOCATION 
The forks are initially placed so that the conflict resolution graph is initially 
acyclic. The algorithm ensures that the graph stays acyclic. Observe that when a 
fork is cleaned before it is sent, the conflict graph does not change. The change in 
the conflict graph occurs only when a philosopher eats, thereby reversing all edges 
incident to it. The algorithm for the dining philosophers problem is given in Figure 
8.8. In this algorithm, we have assumed that the conflict graph is a complete graph 
for simplicity. 
We use the following boolean variables for each process Pi: 
0 f o ~ k [ j ] :  
Process Pi holds the fork that is shared with F'j 
0 requesf[j]: Process Pi holds the request t,oken for the fork that is shared with 
Pj . 
0 dirty[j]: The fork that is sha,red with Pj is dirty. 
It is easy to see that the conflict resolution graph is always acyclic. It is acyclic 
initially by our initialization. The only action that changes direction of any edge in 
t,he graph is eating (which dirties the fork). A philosopher can eat only when she 
has all the forks corresponding to the edges that she shares with other philosophers. 
By the act of eating, all those forks are dirtied and therefore all those edges point 
toward the philosopher after eating. This transformation cannot create a cycle. 
Observe t.hat when a fork is transmitted, it is cleaned before transmission and 
t,hus does not result in any change in the conflict resolution graph. 
The conflict graph for the mutual exclusion on 
processes is a complete graph 
on N nodes. For any philosopher to eat, she will need to request only those forks 
that she is missing. This can be at most N -  1. This results in 2(N - 1) messages in 
the worst case. Note that if a process never requests critical section after some time, 
it will eventually relinquish all its forks and will not be disturbed after that. Thus, 
the number of niessages in the average case is proportional only to the riiirnber of 
processes who are active in accessing the resource. 
8.7 Token-Based Algorithms 
Token-based algorithms use the auxiliary resource token to resolve conflicts in a 
resource coordination problem. The issue in these algorithms is how the requests for 
the token are made, maintained, and served. A centralized algorithm is an instance 
of a token-based algorithm in which the coordinator is responsible for keeping the 
token. All the requests for the token go to the coordinator. 
The token 
rirculates around tjhe ring. Any process that wants to enter the critical section 
In a token ring approach, all processes are organized in a ring. 

8.7. TOKEN-BASED ALGORITHMS 
)ublic class DinMutex extends Process implements Lock { 
private static final int thinking = 0, hungry = 1, eating = 2; 
boolean fork [ ]  = null, 
d i r t y  [] = null, request [ I  = null; 
int mystate = t h i n k i n g ;  
public DinMutex( Linker initConim) { 
super (initComm ) ; 
fork = new boolean[N]; 
request = new boolean[N]; 
for ( i n t  i = 0 ;  i < N; i++) { 
d i r t y  = new boolean[N]; 
if ( (  myId > i ) && (isNeiglibor ( i  ) ) )  { 
} else { fork [ i ]  = true; request [ i ]  = false ; } 
d i r t y  [ i ]  = true; 
fork [ i ]  = false ; request [ i ]  = true; 
1 
1 
public synchronized void requestCS () { 
mystate = hungry ; 
if ( haveForks ( ) )  mystate = eating ; 
else 
for ( i n t  i = 0 ;  i < N; i++) 
if ( request [ i ]  & &  ! fork [ i 1 )  { 
sendMsg( i , ” R e q u e s t ” ) ;  request [ i ]  = false ; 
} 
while (mystate != e a t i n g )  mywait(); 
} public synchronized void releaseCS ( )  { 
mystate = thinking ; 
for ( i n t  i = 0 ;  i < N; i + + )  { 
d i r t y  [ i 1 = true; 
if ( r e q u e s t  [ i ] )  { sendMsg(i, ” F o r k ” ) ;  fork [ i ]  = false; } 
1 
1 
boolean haveForks () { 
for ( i n t  i = 0 ;  i < N; i++) 
return true ; 
if ( !  fork [ i 1 )  return false ; 
} public synchronized void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals (”Request”)) { 
request [ src ] = true; 
if ( (  mystate != e a t i n g )  && fork [ s r c ]  && d i r t y  [ src 1 )  { 
sendMsg( src , ” Fork” ); 
fork [ src ] = false ; 
if ( mystate == hungry){ 
sendMsg( src , ” R e q u e s t ” ) ;  request [ src ] = false ; 
} 
} 
} else if ( t a g .  equals ( ” F o r k ” ) )  { 
fork [ src ] = true; d i r t y  [ src ] = false ; 
if ( haveForks ( ) ) { 
mystate = r a t i n g  ; notify (); 
} 
1 
Figure 8.8: An algorithm for dining philosopher problem 

144 
CHAPTER 8. RESOURCE A1,LOCATION 
waits for the token to arrive at that process. It, then grabs the token and enters the 
critical section. This algorithm is shown in Figure 8.9. The algorithm is initiat,ed 
by the coordinat.or who sends the token t,o the next process in the ring. The local 
stmate of a process is simply t,lie boolean variable 
which records whether 
the process has the token. By ensuring that a process enters the critical section oiily 
when it hns the token, the algorithm guarantees the safety pr0pcrt.y trivially. 
In t,his algorithm, the token is sent to t,lie next process in the ring after a fixed 
period of time. The reader is invited to design an algorithm in wliich t'he t,okeri 
moves only on receiving a request. 
8.8 Quorum-Based Algorithms 
Token-based algorit,hnis are vulnerable to failures of processes holding the t,oken. We 
now present quorum-based algorithms, which do not suffer from such single point, of 
failures. The main idca behind a quorum-based algorit,hm is that instead of asking 
permission to erit>er t,he crit,ical section from either just one process 
in token-based 
algorithms, or from all processes, as in t,imestamp-based algorithms in Chapt,er 2, 
the permission is sought from a subset of processes called the request set. If any t,wo 
request sets have nonempty intersect,ion, then we are guarant.eed t.hat at most m e  
process can have permission to enter t,he critical section. A simple example of this 
strategy is that of requiring permission from a majority of processes. In t,liis case, a 
request. sct, is any subset of processes with at least [Yl processes. 
Vot,ing systems and criinibling walls are some examples of quorum systems. In 
vot,ing systems, each process is assigned a number of votes. Let the tot,al number of 
votes in the system be V. A quorum is defined to be any subset of processes with a 
combined number of votes exceeding V/2. If each process is assigned a single vote, 
then such a quorum system is also called a majority voting system. 
When applications require r e d  or write accesses to the critical section, then 
the voting systenis can be generalized to two kinds of quorunis-read 
quorums and 
write quorums. These quorums are defined by two parameters R and IV such that 
R + W > V and W > V/2. For a subset of processes if the combined number of 
votes exceeds R, then it is a read quorum and if it exceeds W ,  then it is a write 
qiiorum. 
To obt.ain qiioriims for crumbZin,g w~iZls, processes are logically arranged in rows 
of possibly differont widt,hs. A qiioruni in a crumbling wall is the union of one full 
row and a represeiit,ative from every row below the full rows. For example, consider 
a syst'erii with 9 processes suc:l-1 that PI to P3 are in row 1, P4 to PG are in row 2 
and P7 tjo Pg are in row 3. In this system, 
P6, 
is a quorum because it, 
contains the entire second row arid a representative, 
from the t,hird row. Let, 

8.8. QUORUM-RASED ALGORITHMS 
mport j a v a .  u t i l  .Timer; 
)ublic class CircToken extends Process implements Lock { 
boolean haveToken ; 
boolean wantCS = false; 
public CircToken (Linker initComm, int coordinator ) { 
super (initComm) ; 
haveToken = (myId == coordinator ); 
1 
} 
public synchronized void i n i t i a t e  ( )  { 
if ( haveToken ) sendToken ( )  ; 
public synchronized void requestCS ( )  { 
waiitCS = true ; 
while ( ! haveToken ) myWait () ; 
} public synchronized void releaseCS ( )  { 
wantCS = false; 
sendToken ( )  ; 
1 
void sendTokeri ( )  { 
if (haveToken && !wantCS) { 
int next = (myId + 1) % N; 
Ut,il . p r i n t l n  (”Process ” + rnyId + ”has sent the token” 
sendMsg (next , ” token” ) ; 
liave‘roken = false ; 
1 
1 
public synchronized void handleMsg(Msg In, int src , String tag 
if ( t a g .  equals ( ” t o k e n ” ) )  { 
haveToken = true ; 
if (wantCS) 
notify ( ) ;  
else { 
Util . mysleep (1000); 
sendToken ( )  ; 
1 
1 
1 
1 
Figure 8.9: A token ring algorithm for the mutual exclusion problem 

146 
CHAPTER 8. RESOURCE ALLOCATION 
CW(nl,n2,. . . , n d )  be a wall with d rows of width n1, n2, . . . , n d ,  respectively. We 
assume that processes in the wall are numbered sequentially from left to right and 
top to bottom. Our earlier example of the crumbling wall can be concisely written as 
CW(3,3,3). CIV(1) denotes a wall with a single row of width 1. This corresponds 
t,o a centralized algorithm. The crumbling wall CW(1, N - 1) is called the wheel 
coterie because it has N - 1 “spoke” quorums of the form { 1 , i }  for i = 2, . . . , N 
arid one “rim” quorum ( 2 , .  . . , N } .  
a triangular quorum syst,em, processes are 
arrarigcd in a triangle such that the ith row has processes. If there are d rows, then 
each quorum has exactly d processes, In a grid quorum system, N ( =  d 2 )  processes 
are i-lrrangcd in a grid such t,hat there are d rows each with d processes. A quorum 
consists of the union of one full row and a representative from every row below t,he 
full rows. 
It is important to recognize that t,he simple strategy of getting permission to 
enter the critical section from one of the quorums can result in a deadlock. In 
the majority voting system, if two requests gather N/2 votes each (for an even 
value of N ) ,  then neither of the reqiiesh will be grant,ed. Quorum-based systems 
require additional messages to ensure that t,he system is deadlock-free. The details 
of ensuring deadlock freedoni are left to the reader (see Problem 8.9). 
8.9 Problems 
8.1. How will you modify the centralized niutual exclusion algoritjhni to cnsure 
fairness. (Hint: Use vector clocks modified appropriately.) 
8.2. The mut,ual exclusion algorithm by Lainport requires that any request, rnessage 
be acknowledged. Under what conditions does a process not’ need to send an 
ackiiowledgment message for a request niessage? 
8.3. Some applicat,ions require t,wo types of access to the critical sect,ion--read ac- 
cess and write access. For these applications, it is reasonable for two read 
accesses to happen concurrently. However, a write access cannot happen con- 
currently with eit,her a read access or a write access. Modify algorithms pre- 
sented in this chapter for such applications. 
8.4. Build a multiuser Chat application in Java that ensures that a user can type 
its message only in its critical section. Ensure that your system handles a 
dyriarnic number of users, that, is, allows users to join and leave a chat session. 
8.5. Sliow that. any finite directed acyclic graph has at least one source. 
8.6. Wlmi can you combine the request token message with a fork message? Wit,h 
t,liis optimization, show that a philosopher with d neighbors needs to send or 

8.10. BIBLIOGRAPHIC REMARKS 
receive at most 2d messages before making transition from hungry state to 
eating state. 
8.7. Show that the solution to the dining problem does not deny the possibility of 
simultaneous eating from different forks by different philosophers (when there 
is no conflict in requirements of forks). 
8.8. (due to Raymond [Ray89]) In the decentralized algorithm, a process is required 
to send the message to everybody to request the token. Design an algorithm 
in which all processes are organized in the form of a logical binary tree. The 
edges in the tree are directed as follows. Each node except the one with the 
token has exactly one outgoing edge such that if that edge is followed, it will 
lead to the node with the token. Give the actions required for requesting 
and releasing the critical section. What is the message complexity of your 
algorithm? 
8.9. (due to Maekawa [Mae85]) Let all processes be organized in a rectangular 
grid. We allow a process to enter the critical section only if it has permission 
from all the processes in its row and its column. A process grants permission 
to another process only if it has not given permission to some other process. 
What properties does this algorithm satisfy? What is the message complexit,y 
of the algorithm? How will you ensure deadlock freedom? 
8.10. Compare all the algorithms for mutual exclusion discussed in tjhis chapter 
using the following metrics: the response time and the number of messages. 
8.11. Discuss how you will extend each of the mutual exclusion algorithms t,o tolerate 
failure of a process. Assume perfect failure detection of a process. 
8.12. Extend all algorithms discussed in this chapter to solve k-mutual exclusion 
problem, in which at most k processes can be in the critical section concur- 
rently. 
8.13. (due to Agrawal and El-Abbadi [AEASl]) In the tree-based quorum system, 
processes are organized in a rooted binary tree. A quorum in the system is 
defined recursively to be either the union of the root and a quorum in one of 
the two subtrees, or the union of quorums of subtrees. Analyze this coterie 
for availability and load. 
8.10 
Bibliographic Remarks 
Lamport’s algorithm for mutual exclusion [Lam781 was initially presented as an 
application of logical clocks. The number of messages per invocation of the critical 

148 
CHAPTER 
RESOURCE ALLOCATION 
section in Lamport's algorithm can be reduced 
shown by Ricart and Agrawala 
[RA81]. The token-based algorithm can be decentralized as shown by Suzuki and 
Kasami [SK85]. The tree-based algorithm in the problem set is due to Raymond 
[Ray8S]. The use of majority voting systems for distributed control is due to Thomas 
[Tho79], arid the use of weighted voting systems with R and W parameters is due 
to Gifford [GiflS]. Maekawa (Mae851 introduced grid-based quorums and quorums 
based on finite projective planes. The tree-based quorum in the problem set is due 
to Agrawal and El-Abbadi [AEASI]. The triangular quorum systems are due to 
Lovasz [Lov73]. The notion of crumbling walls is due t,o Peleg and Wool [PW95]. 

Chapter 9 
Global Snapshot 
9.1 Introduction 
One of the difficulties in a distributed system is that no process has access to the 
global state of the system, that is, it is impossible for a process to know the current 
global state of the system (unless the computation is frozen). For many applica- 
tions, it is sufficient to capture a global state that happened in the past instead of 
the current global state. For example, in case of a failure the system can restart 
from such a global state. As another example, suppose that we were interested in 
monitoring the system for the property that the token in the system has been lost. 
This property is stable, that is, once it is true it stays true forever; therefore, we can 
check this property on an old global state. If the token is found to be missing in the 
old global state, then we can conclude that the token is also missing in the current 
global state. An algorithm that captures a global state is called a global snapshot 
algorithm. 
A global snapshot algorithm is a useful tool in building distributed systems. 
Computing a global snapshot is beautifully exemplified by Chandy and Lamport as 
the problem of taking a picture of a big scene such as a sky filled with birds. The 
scene is so big that it cannot be captured by a single photograph, and therefore 
multiple photographs must be taken and composed together to form the global 
picture. The multiple photographs cannot be taken at the same time instant because 
there is no shared physical clock in a distributed system. Furthermore, the act of 
taking a picture cannot change the behavior of the underlying process. Thus birds 
may fly from one part of the sky to the other while the local pictures are being taken. 
Despite these problems, we require that the composite picture be meaningful. For 

150 
CHAPTER 9. GLOBAL SNAPSHOT 
example, it should give us an accurate count of the number of birds. We next define 
what is meant by “meaningful” global state. 
Consider the following definition of a global state: A global state is a set of local 
states that occur simultaneously. This definition is based on physical time. We use 
the phrase “time-based model” to refer to such a definition. A different definition of 
a global state based on the “happened-before model” is possible. In the happened- 
before model, a global state is a set of local states that are all concurrent with 
each other. By concurrent, we mean that no two states have a happened-before 
relationship with each other. A global state in the time-based model is also a global 
state in the happened-before model; if two states occur simultaneously, then they 
cannot have any happened-before relationship. However, the converse is not true; 
two concurrent states may or may not occur simultaneously in a given execution. 
We choose to use the definition for the global state from the happened-before 
model for two reasons. 
1. It is impossible to determine whether a given global state occurs in the time- 
based model without access t,o perfectly synchronized local clocks. For exam- 
ple, the statement. ‘%here exists a global state in which more than two processes 
have access to the critical section” cannot be verified in the time-based model. 
In the happened-before model, however, it is possible to determine whether a 
given global stat2e occurs. 
2. Program properties that are of interest are often more simply stated in the 
happened-before niodel than in the time-based model, which makes them eas- 
ier to understand and manipulate. This simplicity and elegance is gained be- 
cause the happened-before model inherently accounts for different execution 
schedules. For example, an execution that does not violate mutual exclusion 
in the time-based model may do so with a different execution schedule. This 
problem is avoided in the happened-before model. 
It is instructive to observe that a consistent global state is not simply a product 
of local states. To appreciate this, consider a distributed database for a banking ap- 
plication. Assume for simplicity that there are only two sites that keep the accounts 
for a customer. Also assume that the customer has $500 at the first site and $300 
at the second site. In the absence of any communication between these sites, bhe 
tot,al money of the customer can be easily computed to be $800. However, if there 
is a transfer of $200 from site A to site B, and a simple procedure is used to add up 
the accounts, we may falsely report that the customer has a total of $1000 in his or 
her accounts (to the chagrin of the bank). This happens when tjhe value at the first 
site is used before the transfer and the value at the second site after the transfer. It 

9.2. CHANDY AND LAMPORT’S GLOBAL SNAPSHOT ALGORITHM 
151 
is easily seen that these two states are not concurrent. Note that $1000 cannot be 
justified even by the messages in transit (or, that “the check is in the mail”). 
GI 
G2 
\ 
\ 
I 
! 
I , 
I 
I 
I 
PI 
I 
m3 ; 
p2 
I 
> 
> 
p3 
I 
* 
, 
I 
I 
! 
I 
Figure 9. I: Consistent and inconsistent cuts 
Figure 9.1 depicts 
distributed computation. The dashed lines labeled GI and 
GP represent global states that consist of local states at 4, Pz, and P3, where G1 
and Gz intersect the processes. Because 
global state can be visualized in such 
figure 
a cut across the computation, the term, “cut” is used interchangeably with 
“global state.” The cut GI in this computation is not consistent because it records 
the message r n 2  as having been received but not sent. This is clearly impossible. 
The cut G2 is consistent. The message mg in this cut has been sent but not yet 
received. Thus it is 
part of the channel from process PI to P3. 
Formally, in an event-based model of 
computation (E, +), with total order < 
on events in a single process, we define 
cut as any subset F 
E such that 
f E F A e  4 f + e E F. 
We define a consistent cut, or 
global snapshot, as any subset F 
E such that 
9.2 Chandy and Lamport’s Global Snapshot Algorithm 
In this section, we describe an algorithm to take 
global snapshot (or 
consistent 
cut) of a distributed system. Our example of the distributed database in the pre- 
vious section illustrates the importance of recording only the consistent cuts. The 
computation of the snapshot is initiated by one or more processes. We assume that 
all channels are unidirectional and satisfy the FIFO property. Assuming that chan- 
nels are unidirectional is not restrictive because 
bidirectional channel can simply 

152 
CHAPTER 9. GLOBAL SNAPSHOT 
be modeled by using two unidirFctiona1 channels. The assumption that channels are 
FIFO is essential to the correctness of the algorithm as explained later. 
The interface that we study in this chapter is called Camera. It allows any 
application that uses a camera to invoke the method globalstate, which records a 
consistent global state of the syst,ern. 
interface Camera extends MsgHandler { 
void g l o b a l s t a t c  ( ) ;  
'The class Camera can be used by any applicat,ion that implement,s the interface 
CamUser. Thus, the application is required to implement the nicthod localstate, 
which records t,he local &ate of the application whenever invoked. 
interface CaniUser extends MsgHandler { 
void l o c a l s t a t e  ( ) ;  
The algorithm is shown in Figure 9.3. We associate with each process a variable 
called color that is either white or red. Intuitively, the computed global snapshot 
corresponds to t,he statc of the syst,ern just before the processes turn red. All pro- 
cesses are initially white. Aft,er recording the local state, a process t.urris red. 'Thus 
the st,ate of a local process is siniply t,he state just before it turned red. 
There are two difficulties in the design of rules for changing t.he color for the 
global snapshot algorithm: (1) we need t,o ensure that the recorded local states are 
mutually concurrent, and (2) we also need a mechanism to capture the state of the 
channels. To address these difficulties, the algorithm relies on a special message 
called a marker. Once a process turns red, it, is required to send a marker along all 
its outgoing channels before it sends out any message. A process is required to turn 
red on receiving a niarkcr if it has riot, already done so. Since channels are FIFO, the 
above mentioned rule guarantees that no white process ever receives a message sent 
by a red process. This in turn guarantees that, local states are mut,ually concurrent. 
Now let 11s turn our attention to the problem of computing states of the channels. 
Figure 9.2 shows that messages in the presence of colors can be of four types: 
1. 
niessuges: These are the messages sent by a white process to a white 
process. These messages correspond to the messages sent. and received before 
the global snapshot. 
2. r'r messages: These are the messages sent by a red process to a red process. 
These messages correspond to the messages sent and received after the global 
snapshot. 

9.2. CHANDY AND LAMPORT’S GLOBAL SNAPSHOT ALGORITHPI4 
ww \ 
‘. \\ 
\ 
\ 
I 
Figure 9.2: Classification of messages 
3. rw messages: These are the messages sent by a red process received by a white 
process. In the figure, they cross the global snapshot in the backward direction. 
The presence of any such message makes the global snapshot inconsistent. The 
reader should verify that such messages are not possible if a marker is used. 
4. WT messages: These are the messages sent by a white process received by a 
red process. These messages cross the global snapshot in the forward direction 
and form the state of the channel in the global snapshot because they are in 
transit when the snapshot is taken. 
To record the state of the channel, Pj starts recording all messages it receives from 
Pi after turning red. Since Pi sends a marker to Pj on turning red, the arrival of the 
marker at Pj from Pi indicates that there will not be any further white messages 
from Pi sent to P’. It can, therefore, stop recording messages once it has received 
the marker. 
The program shown in Figure 9.3 uses chan[k] t.o record the state of t.he kth 
incoming channel and closed 
to stop recording messages along that channel. In 
the program, we say tthat Pj is a neighbor of Pi if there is a channel from Pi to Pj. 
In our implementation, we have assumed that channels are bidirectional. 
Lines 10-17 initialize the variables of the algorithm. All channels are initialized 
to empty. For each neighboring process 4, closedCk1 is initialized to false. The 
method globalstate turns the process red, records the local state, and sends the 
marker message on all outgoing channels. Lines 25-34 give the rule for receiving 
a marker message. If the process is white, it turns red by invoking globalstate. 
Line 27 sets c l o s e d h c ]  to true because tjhere cannot be any message of type wr in 
that channel after the marker is received. The method isDone determines whether 

CHAPTER 9. GLOBAL SNAPSHOT 
the process has recorded it,s local state and all incoming channels. Lines 29-33 print 
all the riicssagcs recorded 
part of the channels. Lines 36-38 handle application 
messages. The condition on line 36 is true if the application message is of t,ype 
In t.lie algorithm, any change in the value of color must be reported to all 
neighbors. On receiving any such notification, a process is required to update its own 
color. This may result in additional messages because of the method 
The net result is that if one process turns red, all processes that can be reached 
directly or indirectly from that process also turn red. 
The Chandy-Lamport algorithni requires that a marker be sent along all chan- 
ncls. Thus it has an overhead of e messages, where e is the number of unidirectional 
channels in the system. We have not, discussed the overhead required t,o combine 
local snapshot,s into a global snapshot. A simple method would be for all processes 
to send their local snapshots to a predetermined process, say, Po. 
9.3 Global Snapshots for non-FIFO Channels 
We now describe an algorithm due to Mattern that works even if channels arc not 
FIFO. We cannot rely on the marker any more to distinguish between white and 
red messages. Therefore, we include the color in all the outgoing messages for any 
process besides sending the marker. Further, even after Pz gets a red message from 
PI or the marker, it cannot be sure that it will not receive a white message on that 
channel. A white message may arrive later than a red message due to the overtaking 
of messages. To solve this problem we include in the marker the total number of 
white messages sent by that process along that channel. The receiver keeps track 
of the total number of white messages received and knows that all white messages 
have been received when this count equals the count included in the marker. We 
leave the details of the algorithm to the reader 
an exercise. 
9.4 Channel Recording by the Sender 
Chandy and Lamport’s algorithm requires the receiver to record the state of the 
channel. Since messages in real channels may get lost, it may be advantageous for 
senders to record the state of the vhannel. We will assume that control messages 
can be sent over unidirectional channels even in the reverse direction. 
The mechanism to ensure that we do not record inconsistent global state is 
based on the coloring mechanism discussed earlier. A process sends white messages 
before it has recorded its local state and red messages after it has recorded the local 
state. By ensuring that a white process turns red before accepting a red message, 

9.4. CHANNEL RECORDING BY THE SENDER 
155 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
1 import j a v a .  u t i l  . * ;  
2 public class RecvCamera 
extends Process implements Camera { 
static final int white = 0 ,  red = 1; 
int mycolor = white; 
boolean closed [I; 
CamUser app; 
LinkedList chan[] = null; 
public RecvCamera( Linker initComm, CamUser app) { 
super (initComm) ; 
closed = new boolean[N]; 
chan = new LinkedList [N]; 
for ( i n t  i = 0; i < N; i++) 
closed [ i ]  = false ; 
chan [ i ] = new LinkedList ( )  ; 
if ( isNeighbor ( i  ) )  { 
1 else closed [ i ]  = true; 
this. app = app; 
public synchronized 
myColor = red; 
app. l o c a l s t a t e  ( 
sendToNeighbors 
} 
1 
J Dublic svnchronized 
void g l o b a l s t a t e  () { 
; // record local S t a t e ;  
”marker” , myId); // send Markers 
void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals ( marker” ) )  { 
if (mycolor == white) g l o b a l s t a t e  (); 
closed [ src ] = true ; 
if ( isDone (1) { 
System. o u t .  p r i n t l n  (”Channel S t a t e  : Transit Messages ” ); 
for ( i n t  i = 0 ;  i < N; i++) 
if ( isNeighbor ( i  ) )  
while ( !  chan [ i 1 .  isEmpty ( ) )  
System. o u t .  p r i n t l n  ( 
((Msg) chan[ i I .  removeFirst 0). t o s t r i n g  0); 
} 
} else 
if 
// application message 
( mycolor == r e d )  && (! closed [ src 
chan[ src 1 .  add(m); 
app. handleMsg(m, src , tag ) ;  // give 
t o  app 
} 
1 
boolean isDone () { 
if ( mycolor == white) return false ; 
for ( i n t  i = 0; i < N; i++) 
return true ; 
if ( !  closed [ i 1 )  return false ; 
Figure 9.3: Chandy and Lamport’s snapshot algorithm 

CHAPTER 9. GLOBAL SNAPSHOT 
we are guaranteed that there are no TW messages and therefore we will record only 
a consistent global snapshot. 
Now let us turn our attention to recording the state of channels. We assume 
t,hat the sender records all the messages that it sends out on any outgoing channel 
before it turned red. Whenever a process turns red, it sends a marker message 
on all its incoming channels (in the reverse direction) indicating the messages it 
has received on t,hat channel so far. The sender can now compute the state of the 
channel by removing from its buffer all messages that have been received according 
to the marker. 
In t,his scheme, the sender may end up storing a large nurnber of messages 
before the marker arrives. Assuming that all control messages (marker arid ac- 
knowledgment messages) follow FIFO ordering, we can reduce the storage burden 
at the sender by requiring the receiver to send acknowledgments. When the sender 
receives an acknowledgment and has not received the marker, it can delete the mes- 
sage from the storage. To identify each message uniquely, we use sequence numbers 
with messages as encapsulated by the class 
given below. 
~~~ 
~~~ 
~ 
public class SeqMessage { 
Msg in. 
int seqNo; 
public SeqMessage (Msg m, int seqNo) { 
this m =  m, 
this seqNo = seqNo, 
public int getSrqNo() { 
I 
1 
return seqNo, 
public h4sg gcthfrssitge ( )  { 
return in; 
‘rhus, the algorithm can be sumniarized by the following rules. 
1. Every process is white before recording its state and red after recording its 
state. A white process sends white messages and a red process sends a red 
message. 
2. A whitre process turns red before accepting a red message or a marker. 
3. On t,urning red, a process sends markers on all incoming channels in the reverse 
direction. 

9.5. APPLICATION: CHECKPOINrING A DISTRIBUTED APPLICATION 
4. A white process acknowledges a white message. 
5. A white process records any message sent. On receiving an acknowledgment, 
the corresponding message is removed from the record. 
Since this algorithm requires every application message to include the color and 
the sequence number, we extend the Linker class 
shown in Figure 9.4. The method 
works as follows. If the message is either a marker or an acknowledgment 
message, then no special action is required, and 
is invoked. If it 
is a white application message, then it is recorded as part of the channel history. 
The method 
also appends the tag “white” or “red” with the message and 
includes a sequence number. 
The algorithm for recording a global snapshot with channel states recorded by 
the sender is shown in Figure 9.5. For simplicity we have assumed a completely 
connected topology. 
The method 
is identical to Chandy and Lamport’s algorithm ex- 
cept that the markers are sent on the incoming channels in the reverse direc- 
tion. When a marker message is received, a white process invokes the method 
at line 
Also, t,hat incoming channel is closed at line 27. When 
acknowledgement message is received then the corresponding message is removed 
from the channel history at line 31. This is accomplished by the method 
When an application message is received, a white process sends an acknowledgment 
for a white message at line 37. If the message is red, then the process also turns red 
by invoking 
at line 
Note that this algorithm also does not require channels for application messages 
to be FIFO. If channels are known to be FIFO then the receiver only needs to 
record the sequence number of the last message it received before turning red. The 
algorithm does require the ability to send control messages in the reverse dircction for 
any application channel. Furthermore, it requires control messages to follow FIFO 
order. (Why?) If the underlying network does not support FIFO, then sequence 
numbers can be used to ensure FIFO ordering of messages. 
9.5 Application: Checkpointing a Distributed Applica- 
t ion 
As a simple example, let us try our snapshot algorithm on the circulating token 
algorithm discussed in the Chapter 2. Figure 9.6 gives a program that constructs a 
circulating token and a camera. The computation of the global snapshot is initiated 
by the method 

CHAPTER 9. GLOBAL SNAPSHOT 
mport j a v a .  u t i l  . * ;  import j a v a . n e t  . * ;  import j a v a . i o . * ;  
>ublic class CameraLinker extends Linker { 
static final int white = 0 ,  red = 1; 
int seqNo[] = null; 
Sendercamera cam: 
public CameraLinker ( S t r i n g  basename, int myId, int numProc) 
throws Exception { 
super ( basename , myId , numProc) ; 
seqNo = new int I numProc 1 : 
fo; ( int i = O ;  i < numkroc; i ++I 
seqNo[ i ] = 0 ;  
} 
1 
public void initcam ( Sendercamera cam) { 
this .cam = cam; 
public void sendMsg(int d e s t I d ,  String t a g ,  String msg 
if ( (  t a g .  equals (”marker” ) )  I I ( t,ag. equals ( ” a c k ” ) ) )  
super. srndMsg ( destId , tag , msg) ; 
else { // s e n d  s e q  numbers wzth upp msgs 
seqNo [ destId]++; 
Msg m = new 
Msg(myId, destId , t a g ,  msg); 
if (cam. nivColor == white l 
,
I
 
‘cam. recordMsg( destId , new SeqMessage(m, seqNo[ destId I ) ) ;  
super. sendMsg( destId , ” w h i t e ” ,  + ”  ”+in. t o s t r i n g ( ) + ”  ” ) ;  
} else 
super. sendMsg ( d e s t I d  , ” r e d ” ,  
String.valueOf(seqNo[ d e s t I d ] )  + ”  ” + m .  t o s t r i n g ( ) + ”  ” ) ;  
Figure 9.4: Linker extended for use with Sendercamera 

9.5. APPLICATION: CIIECKPOINTING A DISTRIBUTED APPLICATION 
159 
1 import j a v a .  u t i l  .*; 
2 public class Sendercamera extends Process implements Camera { 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 
43 
44 
45 
46 
47 
48 
49 
50 
51 
52 
static final int white = 0 ,  red = 1; 
public int mycolor = white; 
CamUser app; 
boolean closed 
; 
MsgList outChan[] = null; 
public Sendercamera ( CameraLinker initConim , CamUscr 
super (initComm); 
this. app = app; 
closed = new boolean[N]; outChan = new MsgList 
for ( int i = 0; i < N; i++) 
closed [ i ]  = false ; 
outChan [ i ] = new MsgList ( ) ;  
if ( isNeighbor ( i  ) )  { 
} else closed [ i ] = true; 
initComm. initcam (this ) ;  
1 
public synchronized void g l o b a l s t a t e  ( )  { 
mvColor = red: 
app. l o c a l s t a t e  0; // r e c o r d  l o c a l  S t a t e ;  
sendToNeighbors (”marker” , myId); // send Markers 
} public synchronized void handleMsg(Msg m, int src , String tag){ 
if ( t a g .  equals ( ” m a r k e r ” ) )  { 
if (mycolor == white) g l o b a l s t a t e  (); 
closed [ src ] = true; 
if (isDone ( ) )  System. o u t .  p r i n t l n  (”Done r e c o r d i n g ” ) ;  
int seqNo = m. getMessageInt ( ) ;  
outChan [ s r c  1. renioveM (seqNo ) ; 
S t r i n g T o ken i z e r s t =new S t ring T o ke n i z e r (m. get Message ( ) + ” #” ) ; 
int seqNo = I n t e g e r .  parseInt ( s t .  nextToken 
Msg appMsg = Msg. parseMsg ( s t  ) ;  
if ( (  mycolor == w h i t e )  && ( t a g .  equals ( ” w h i t e ” ) ) )  
sendMsg ( src , ” ack” , seqNo ) ; 
if ( (  mycolor == white) && ( t a g .  e q u a l s ( ” r e d ” ) ) )  
g l o b a l s t a t e  (); 
app . handleMsg (appMsg , src , appMsg. getTag ( ) ) ; 
} else if ( t a g .  equals ( ” a c k ” ) )  { 
} else { // a p p l i c a t i o n  message 
1 
} boolean isDone () { 
if ( mycolor == white) return false ; 
for ( i n t  i = 0 ;  i < N; i++) 
return true ; 
if (! closed [ i 1 )  return false ; 
1 
1 
public synchronized void recordMsg( int c - s t I d  , SeqMessage sm){ 
outChan[ destId 1 .  add(sm); 
1 
Figure 9.5: A global snapshot algorithm based on sender recording 

160 
CHAPTER 9. GLOBAL SNAPSHOT 
import j a v a .  u t i l  .Random; 
public class CameraTcster { 
public static void main( String 
a r g s )  throws Exception { 
String baseName = args [ O ] ;  
int myId = I n t e g e r .  parseIrit ( a r g s  [ 11); 
int riuniProc = I n t e g e r .  parseInt ( a r g s  [ 2 ] ) ;  
Camera camera = null; 
CamCircToken sp = null; 
if ( args [ 3 ] .  equals ("RecvCamera")) { 
Linker c o r n  = new Linker (baseNanie, myId, nuniproc) ; 
sp = new CaniCircToken(comm, 0 ) ;  
camera = new RecvCainera ( c o r n ,  sp ) ; 
1 
if ( args (31. equals ("SenderCamera" ) )  { 
CanieraLinker conm = new CarneraLinker ( a r g s  [ 0 ]  , myId, numProc); 
sp = new CaniCircToken(con~i, 0 ) ;  
camera = new SenderCainera (conm, sp ) ; 
1 
s p .  i n i t i a t e  ( ) ;  
for ( int i = 0 ;  i < nuinProc; i++) 
if (myId == 0) camera. g l o b a l s t a t e  (); 
if ( i  != myId) (new ListenerThread(i , camera)). s t a r t  ( ) ;  
} 
1 
Figure 9.6: Irivocation of the global snapshot algorithm 

9.6. PROBLEMS 
161 
The global snapshot algorithm can be used for providing fault tolerance in dis- 
tributed systems. On failure, the system can be restarted from the last snapshot. 
Global snapshots can also be used for distributed debugging. Inspection of interme- 
diate snapshots may sometimes reveal the source of an error. 
9.6 Problems 
9.1. Show that if G and H are consistent cuts of a distributed computation (E, +), 
then so are G U H and G fi H .  
9.2. The global snapshot algorithms discussed in this chapter do not freeze the un- 
derlying computation. In some applications it may be okay for the underlying 
application to be frozen while the snapshot algorithm is in progress. How can 
the snapshot algorithm be simplified if this is the case? Give an algorithm for 
global snapshot coniputation and it,s Java implementation. 
9.3. Extend the Java implementation of Chandy and Lamport’s algorithm to allow 
repeated computation of global snapshots. 
9.4. The original algorithm proposed by Chandy and Lamport does not require 
FIFO but a condition weaker than that. Specify the coadition formally. 
9.5. How can you use Lamport’s logical clock to compute a consistent global snap- 
shot? 
9.6. Give Java imp1ementat)ion of global snapshot algorithm when channels are not 
FIFO. 
9.7. Extend Chandy and Lamport’s algorithm to compute a tran,sitless global state. 
A consistent global state is transitless if there are no messages in any channel 
in that global state. Note that a process may have to record its local state 
multiple times until the recorded local state can be part of a transitless global 
state. Give Java implementation of your algorithm. 
9.8. Give an example of a distributed computation in the interleaving model (with 
the events of the superimposed global snapshot algorithm) in which the recorded 
global snapshot does not, occur in the computation. 
9.9. How will you use snapshot algorithms to detect that the application has 
reached a deadlock state? 

162 
CHAPTER 9. GLOBAL SNAPSHOT 
9.7 Bibliographic Remarks 
Chandy and Lamport [CL85] were the first to give an algorithm for computation 
of a meaningful global snapshot (a colorful description of this algorithm is given by 
Dijkstra [Dij85]). Spezialetti and Kearns have given efficient algorithms to dissern- 
inate a global snapshot to processes initiating the snapshot computation [SK86]. 
Bouge [Bou87] has given an efficient algorithm for repeated computation of snap- 
shots for synchronous computations. In the absence of the FIFO assumption, 
shown by Taylor [TayRS], any algorithm for a snapshot is either inhibitory (that is, 
it may delay actions of the underlying application) or requires piggybacking of con- 
trol information on basic messages. Lai and Yang [LY87] and Mattern [Mat931 have 
given snapshot algorithms that require only the piggybacking of control information. 
Helary [He1891 has proposed an inhibitory snapshot algorithm. 

Chapter 10 
Global Properties 
10.1 Introduction 
In this chapter, we introduce another useful tool for monitoring distributed compu- 
tations. A distributed computation is generally monitored to detect if the system 
has reached a global state satisfying a certain property. For example, a token ring 
system may be monitored for the loss of the token. A distributed database sys- 
tem may br monitored for deadlocks. The global snapshot algorithm discussed in 
Chapter 9 can be used to detect a stable predicate in a distributed computation. 
To define stable predicates, we use the notion of the reachability of one global state 
from another. For two consistent global states G and H ,  we say that G 5 H if H is 
reachable from G. A predicate B is stable iff 
' J G , H :  G 5 H : B(G) + B ( H )  
In other words, a property B is stable if once it becomes true, it stays true. Some 
examples of stable properties are deadlock, termination, and loss of a token. Once a 
system has deadlocked or terminated, it remains in that state. A simple algorithm 
to detect a stable property is as follows. Compute a consistent global state. If the 
property B is true in that global state, then we are done. Otherwise, we repeat the 
process after some period of time. It is easily seen that if the stable property ever 
becomes true, the algorithm will detect it. Conversely, if the algorithm detects that 
some stable property B is true, then the property must have become true in the 
past (and is therefore also true currently). 
Formally, if the global snapshot computation was started in the global state G,, 
the algorithm finished by the global state G f ,  and the recorded state is G,, then 

164 
CHAPTER 10. GLOBAL PROPERTIES 
the following is true: 
1. B(G*) =+ B ( G f )  
2. 7B(G,) + lB(Gi) 
Note that the converses of statements 1 and 2 may not hold. 
rithm for detection of global properties: 
At this point it is important to observe some limitations of the snapshot algo- 
0 The algorithm is not useful for unstable predicates. An unstable predicate 
may turn true only between two snapshots. 
0 In many applications (such as debugging), it is desirable to compute the least 
global state that satisfies some given predicate. The snapshot, algorithm cannot 
be used for this purpose. 
0 The algorithm may result in an excessive overhead depending on t,he frequency 
of snapshots. A process in Chandy and Lamport’s algorithm is forced to take a 
local snapshot on receiving a marker even if it knows that the global snapshot 
that includes its local snapshot cannot satisfy the predicate being detected. For 
example, suppose that the property being detected is termination. Clearly, if 
a process is not terminated, then the entire system could not have terminated. 
In this case, cornpiitmation of the global snapshot is a wasted effort. 
10.2 Unstable Predicate Detection 
In this section, we discuss an algorithm to detect unstable predicates. We will 
assume that the given global predicate, say, B, is constructed from local predicates 
using boolean connectives. We first show that B can be detected using an algorithm 
that can detect q, where y is a pure conjunction of local predicates. The predicate 
can be rewritten in its disjunctive normal form. Thus 
where each q, is a pure conjunction of local predicates. Next, observe that a global 
cut satisfies B if and only if it satisfies at least one of the qz’s. Thus the problem 
of detecting B is reduced to solving k problems of detecting q, where y is a pure 
conjunction of local predicates. 
As an example, consider a distributed program in which 5 ,  y. and z are in three 
different processes. Then, 
even(z) A ((y < 0) V ( z  > 6)) 

10.2. UNSTABLE PREDICATE DETECTION 
can be rewritten 
(even(z) A (y < 0)) V (even(z) A ( z  > 6)) 
where each disjunct is a conjunctive predicate. 
Note that even if the global predicate is not a boolean expression of local pred- 
icates, but is satisfied by a finite number of possible global states, it can also be 
rewritten 
a disjunction of conjunctive predicates. For example, consider the 
predicate (z = y), where z and y are in different processes. 
= y) is not a local 
predicate because it depends on both processes. However, if we know that 
and y 
can take values (0,l) only, we can rewrite the preceding expression 
follows: 
((z = 0) A (y = 0)) V ((z = 1) A (y = 1)). 
Each of the disjuncts in this expression is a conjunctive predicate. 
In this chapter we study methods to detect global predicates that are conjunc- 
tions of local predicates. We will implement the interface 
which abstracts 
the functionality of a global predicate evaluation algorithm. This interface is shown 
below: 
public interface Sensor extends MsgHandler { 
void 1ocalPredicateTrue (VectorClock vc ) ;  
1 
Any application that uses Sensor is required to call 
when- 
ever its local predicate becomes true and provide its 
It also needs to 
implement the following interface: 
public interface Sensoruser extends MsgHandler { 
void globalPredicateTrue ( i n t  G [ ] ) ;  
void globalPredicateFalse (int pid ); 
} 
The class that implements 
calls these methods when the value of the 
global predicate becomes known. If the global predicate is true in a consistent 
global state G, then the vector clock for the global state is passed 
a parameter to 
the method. If the global predicate is false, then the process id of the process that 
terminated is passed as a parameter. 
We have emphasized conjunctive predicates and not disjunctive predicates. The 
reason is that disjunctive predicates are quite simple to detect. To detect a disjunc- 
tive predicate 
V 12 V . . . V I N ,  where li denotes a local predicate in the process 

166 
CHAPTER 10. GLOBAL PROPERTIES 
it is sufficient for the process 
to monitor li. If any of the processes finds its local 
predicate true, then the disjunctive predicate is true. 
Formally, we define a weak conjunctive predicate (WCP) to be true for a given 
computation if and only if there exists a consistent global cut in that run in which all 
conjuncts are true. Intuitively, detecting a weak conjunctive predicate is generally 
useful when one is interested in detecting a combination of states that is unsafe. For 
example, violation of mutual exclusion for a two-process system can be written as 
‘‘PI is in the critical section and P2 is in the critical section.” It is necessary and 
sufficient, to find a set of incomparable states, one on each process in which local 
predicates are true, to detect a weak conjunctive predicate. We now present an 
algorithm to do so. This algorithm finds the least consistent cut for which a WCP 
is true. 
In this algorithm, one process serves as a checker. All other processes involved 
in detecting the WCP are referred to 
application processes. Each application 
process checks for local predicates. It also maint,ains the vector clock algorithm. 
Whenever the local predicate of a process becomes true for the first time since the 
most recently sent message (or the beginning of the trace), it generates a debug 
message containing it,s local timestamp vector and sends it to the checker process. 
Not,e t,hat a process is not required to send its vector clock every time the local 
predicate is detected. If two local states, say, s and t, on the same process are 
separated only by internal events, then they are indistinguishable to other processes 
so far as consistency is concerned, that is, if u is a local state on some other process, 
then sllu if and only if tl(u. Thus it is sufficient to consider at most one local state 
between two external events and the vector clock need not be sent if there has been 
no message activity since the last time the vector clock was sent. 
The checker process is responsible for searching for a consistent cut that satisfies 
the WCP by considering a sequence of candidate cuts. If the candidate cut either 
is not a consistent cut or does not satisfy some term of the WCP, the checker can 
efficiently e1iminat.e one of the states along the cut. The eliminated state can never 
be part of a consistent cut that satisfies the WCP. The checker can then advance 
the cut, by considering the successor to one of t,he eliminated states on the cut. If 
the checker finds a cut for which no state can be eliminated, then that cut satisfies 
the WCP arid the detection algorithm halts. The algorithm for the checker process 
is shown in Figure 10.1. 
The checker receives local snapshots from the other processes in t.he system. 
These messages are used by the checker to create and maintain data structures that 
describe the global state of the syst,eni for the current cut. The data structures are 
divided into two categories: queues of incoming messages and those data structures 
that describe the state of the processes. 
The queue of incoming messages is used to hold incoming local snapshots from 

10.2. ITNSTABLE PREDICATE DETECTION 
mport j a v a .  u t i l  .*; 
)ublic class CentSensor extends Process implements Runnable , Sensor { 
final s t a t i c  int red = 0 ,  green = 1; 
LinkedList q [ ] ;  // q [ i ]  s t o r e s  v e c t o r  timestamps f r o m  process i 
int cut [ I  [ I ,  
color 
g s t a t e  
boolean finished [ I ;  // process i f i n i s h e d  
Sensoruser app ; final int checker = Symbols. coordinator ; 
public CentSensor (VCLinker initComm, Sensoruser app) { 
super (initcomm) ; 
cut = new int [ N ]  [ N] ; 
color = new i n t [ N ] ;  g s t a t e  = new i n t [ N ] ;  finished = new boolean[N]; 
for ( i n t  i = 0; i < N; i++) { 
q = new LinkedList [ N ] ;  
q [  i ]  = new LinkedList ( ) ;  color [ i ]  = r e d ;  finished [ i ]  = f a l s e ;  
1 this .app = app; 
if (myId == checker) new Thread(this). s t a r t  ( ) ;  
1 
public synchronized void 1ocalPredicateTrue ( VectorClock vc){ 
if ( myId == checker ) 
else 
handleMsg(new Msg(O,O,”trueVC”, vc. t o s t r i n g  ( ) ) , O ,  ”trueVC”); 
( (  VCLinker)comm). simpleSendMsg( checker ,”trueVC”, vc. t o s t r i n g  
1 
public synchronized void run ( )  { 
int i = Util . searchArray (color , red ); 
while ( i  != -1) { 
while ( q [  i 1 .  isEmpty() & &  ! finished [ i 1 )  mywait(); 
if ( finished [ i I )  { 
app. globalPredicateFalse ( i  ) ;  
return ; 
cut [ i ] = ( int [ I )  
p a i n t s t a t e  ( i  ); 
i = Util . searchArray ( color , red ) ;  
} 
q [  i 1 .  removeFirst ( ) ;  
1 
for ( i n t  j = 0 ;  j < N; j + + )  g s t a t e [ j ]  = c u t [ j ] [ j ] ;  
app. globalPredicateTrue ( g s t a t e  ); 
public synchronized void handleMsg (Msg m, int src 
if ( t a g .  equals (”trueVC”)) { 
int 
receiveTag = new int [ N ] ;  
Util . readArray (m. getMessage ( ) ,  
q [  src 1 .  add( receiveTag ) ;  
finished [ src ] = t r u e ;  notify (); 
receiveTag 
notify ( ) ;  
} else if ( t a g .  equals ( ”  finished ” ) )  { 
1 
1 
void p a i n t s t a t e  ( i n t  i ) { 
String tag){ 
color [ i ] = green ; 
for ( i n t  j = 0; j < N; j++) 
if ( color [ j ] == green ) 
if ( Util . lessThan ( c u t  [ i 1 ,  cut [ j I ) )  color [ i ] = red ; 
else if ( Util . lessThan (cut [ j 1 ,  cut [ i I ) )  
color [ j ] = red ; 
1 
t 
Figure 10.1: WCP (weak conjunctive predicate) detection algorithm-checker 
pro- 
cess. 

168 
CHAPTER 10. GLOBAL PROPERTIES 
application processes. 
We require that messages from an individual process be 
received in FIFO order. We abstract the message-passing system as a set of N 
FIFO queues, one for each process. We use the notation q[1.. . N ]  to label these 
queues in the algorithm. 
The checker also niairitains information describing one state from each process 
Pi. cut [il represer1t.s t,he state from Pi using the vector clock. Thus, cut [il [jl 
denotes t,he j t h  component of the vector clock of cut 
. The color [il of a state 
cut [i] is either red or green arid indicates whether the state has been eliniiriat,ed in 
the current cut. A state is green only if it is concurrent with all other green states. 
A state is rod only if it cannot be part of a consist,ent cut t,hat sat,isfies the WCP. 
The aim of advancing the cut is to find a new candidate cut. However, we can 
advance the cut only if we have eliminated at least one state along the current 
cut and if a message can be received from the corresponding pro 
structures for the processes are updated to reflect t,he new cut. This is done by the 
procedure paintstate. The paraniet,er i is the index of the process from which a 
local snapshot was most recently received. The color of cut [il is tcrriporarily set 
to green. It may be necessary to change some green states to red to preserve the 
property that all green states are mutually concurrent. Hence, we must compare 
the vector clock of cut [il to each of the other green states. Whenever the states 
are comparable, the smaller of the t,wo is painted red. 
Let N denote the number of processes involved in the WCP arid m denote the 
maximum number of messages sent or received by any process. 
The main time complexity is involved in detecting the local predicates and t,ime 
required to maintain vector clocks. In the worst case, one debug message is generated 
for each program message sent or received, so the worst-case message comp1exit)y is 
O(m). In addition, program messages have to include vector clocks. 
The main space requirement of the checker process is the buffer for the local 
snapshots. Each local snapshot consists of a vector clock that requires O ( N )  space. 
Since here are at most O ( m N )  local snapshots, O(N2m) total space is required to 
hold the component of local snapshots devoted t30 vector clocks. Therefore, the total 
amount of space required by the checker process is O(N2m). 
We now discuss the time complexity of the checker process. Note that it takes 
only two comparisons to check whether two vectors are concurrent. Hence, each 
invocat.ion of pnir~tState requires at most N comparisons. This function is called 
at most orice for each state, and there are at most mN states. Therefore, at most 
N 2 m  comparisons are required by the algorithm. 

10.3. APPLICATION: DISTRIBUTED DEBUGGING 
169 
10.3 Application: Distributed Debugging 
Assume that a programmer is interested in developing an application in which there 
is a lender or a coordinator at all times. Since the leader has to perform more work 
than other nodes, the programmer came up with the idea of circulating a token in 
the network and requiring that whichever node has the token acts as the leader. 
We will assume that this is accomplished using the class CircToken discussed in 
Chapter 8. Now, the programmer wants to ensure that his program is correct. 
He constructs the bad condition 
“there is no coordinator in the system.” This 
condition can be equivalent.ly written 
‘‘PI does not have the token, and P2 does 
not have the token,” and so on for all processes. To see if this condition becomes 
true, the programmer must modify his program to send a vector clock to the sensor 
whenever the local condition “does not have the token” becomes true. Figure 10.2 
shows the circulating token application modified to work with the class 
Figure 10.3 shows the main application that runs the application with the sensor. 
This program has an additional command-line argument that specifies which sensor 
algorithm needs to be invoked as sensor--the centralized algorithm discussed in this 
section, or the distributed algorithm discussed in the next section. 
When the programmer runs the program, he may discover that the global con- 
dition actually becomes true, that is, there is a global state in which there is no 
coordinator in the system. This simple test exposed the fallacy in the programmer’s 
t,hinking. The token may be in transit and at that time there is no coordinator in 
t,he system. 
We leave it for the reader to modify the circulating token application in which 
a process cont.inues to act 
the leader until it receives an acknowledgment for the 
token. This solution assumes that the application work correctly even if there are 
two processes acting 
the leader temporarily. 
10.4 A Token-Based Algorithm for Detecting Predicates 
Up to t,his point we have described detection of WCP on the basis of a checker 
process. The checker process in the vector-clock-based centralized algorithm requires 
O ( N 2 m )  time and space, where m is the number of messages sent or received by any 
process and N is the number of processes over which the predicate is defined. We 
now introduce token-based algorithms that distribute the computation and space 
requirements of the detection procedure. The distributed algorithm has O(N2m) 
time, space, and message complexity, distributed such that each process performs 
O(Nm,) work. 
We introduce a new set of N monitor processes. One monitor process is mated 

170 
CHAPTER 10. GLOBAL PROPERTIES 
)ublic class SensorCircToken extends CircToken 
mplements MsgHaridler , Sensoruser { 
VCLinker c o m i ;  
Sensor checker ; 
int coordinator ; 
int algoCode ; 
public SensorCircTokeri ( VCLinker conm, int coordinat,or . int algoCode ) { 
super(comm, coordinator ); 
this .conun = corrm; 
t h i s .  coordinator = coordinator ; 
t h i s .  algoCode = algoCode ; 
1 
public void i n i t i a t e  ( )  { 
if (algoCode == 0) 
else 
i f  ( !  1iaveToken) checker. IocalPredicateTrue (conm. v c ) ;  
super. i n i t i a t e  ( ) ;  
checker = new Cent#Srrisor (conun, this ) ;  
checker = new DistSensor ( c o r n ,  t h i s ) ;  
} public synchronized void seridToken ( )  { 
super. sendToken ( )  ; 
if ( !  1iaveToken) c h e c k e r .  IocalPredicateTrue ( c o r n .  v c ) ;  
} public synchronized void handleMsg(Msg m, int src , String t,ag){ 
checker . handlehlsg (m, src , tag ) ; 
super. handleMsg (m, src , tag ) ;  
I 
public void globalPredicateTrue ( i n t  v [ I )  { 
Systcni , out . p r i n t  ln ( ” * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * ” ) ; 
System. o u t .  p r i n t l n  ( ” P r e d i c a t e  true a t  :” + Util . writeArray ( v ) ) ;  
1 
public void globalPredicateFalse ( i n t  pid){ 
System , out . p r i n t  In ( ” * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *” ) ; 
System. o u t .  p r i n t l n  ( ”  Predicate false . Proc ” + pid + ” finished ’‘ ) ;  
1 
- 
Figlire 10.2: Circulat,ing token with vector clock 

10.4. A TOKEN-BASED ALGORITHM FOR DETECTING PREDICATES 
171 
3ublic class SensorTester { 
public s t a t i c  void main( String 
args ) throws Exception { 
String baseName = args [ O ] ;  
int myId = Integer. parseInt (args [ 11); 
i n t  nuniproc = Integer. parseInt (args [ 2 ] ) ;  
VCLinker c o r n  = new VCLinker (baseName , myId , numProc) ; 
int algoCode = Integer. parseIrit (args [ 3 ] ) ;  
SensorCircToken sp = new SensorCircToken ( 
s p .  i n i t i a t e  ( ) ;  
for ( int i = 0 ;  i < niimproc; it-+) 
comm, Symbols. coordinator , algoCode); 
if ( i  != myId) (new ListenerThread(i, s p ) ) .  start ( ) ;  
1 
- 
t 
Figure 10.3: An application that runs circulating token with a sensor 
to each application process. The application processes interact according to the 
distributed application. In addition, the application processes send local snapshots 
to monitor processes. The monitor processes interact with each other but do riot, 
send any information to the application processes. 
The distributed WCP detection algorithm shown in Figure 10.4 uses a unique 
token. The token contains two vectors. The first vector is labeled G. This vector 
defines the current candidate cut. If G[i] has the value k, then state k from process 
is part of the current candidate cut. Note that all states on the candidate cut 
satisfy local predicates. However, the states may not be mutually concurrent, that 
is, the candidate cut may not be a consistent cut. The token is initialized with 
Vi : G[i] = 0. 
The second vector is labeled color, where color[i] indicates the color for the 
candidate state from application process Pi. The color of a state can be either red 
or green. If color[i] equals red, then the state 
G[i]) and all its predecessors have 
been eliminated and can never satisfy the WCP. If color[i] = green, then there is 
no st,ate in G such that 
G[i]) 
happened before that state. The token is initialized 
with 
: colorji] = red. 
The token is sent to monitor process Mi only when color[i] = red. When it 
receives the token, 
waits to receive a new candidate state from 
and then 
checks for violations of consistency conditions with this new candidate. This activity 
is repeated until the candidate state does not causally precede any other state on 
the candidate cut, that is, the candidate can be labeled green. Next, Mi examines 
the token to see if any other states violate concurrency. If it finds any j such that 
( j ,  G[j]) happened before 
G[i]), 
then it makes color[j] red. Finally, if all st,ates 

172 
CHAPTER 10. GLOBAL PROPERTIES 
// vector clock from the candidate state 
candidate: array[l..n] of integer initially 0; 
Upon receiving the token (G, color) 
while (color[i] = red) 
receive candidate from application process P 
if (candidate[i] > G[i]) then 
G[i] := candidate[i]; 
color[i] := green; 
endif; 
endwhile; 
for j := 1 to n, ( j  # i )  
if (candidate[j] G [ j ] )  then 
G[j] := candidate[j]; 
color[j] := red; 
endif 
endfor 
if 
: color[j] = red) then send token t o  Mj; 
else detect := true; 
Figure 10.4: Monitor process algorithm at Pi 

10.5. PROBLEMS 
173 
in G are green, that is, G is consistent, then Mi has detected the WCP. Otherwise, 
sends the token to a process whose color is red. 
It uses three 
types of messages. The trueVC message is sent by the application process to the 
monitor process whenever the local predicate becomes true in a message interval. 
This message includes the value of the vector clock when the local predicate became 
true. This vector is stored in the queue q. The Token message denotes the token 
used in the description of the algorithm. Whenever a monitor process receives 
the token, it invokes the method 
described later. For simplicity of 
implementation, we send the G vector and the 
vector separately. The finished 
message from the application process indicates that it has ended and that there will 
not be any more messages from it. 
Let us now look at the 
method. The goal of the process is to make 
the entry 
green. If there is no pending vector in the queue 
then the 
monitor process simply waits for either a tmie VC or a finished message to arrive. If 
there is no pending vector and the finished message has been received, then we know 
that the global predicate can never be true and thus it is declared to be false for this 
computation. If a vector, 
is found such that 
> 
, then 
the global cut is advanced to include 
. This advancement may result 
in 
[j] becoming red if 
[j] 
[jl . The method 
determines 
the first process that has red color. If the array 
is completely green, 
returns -1, and the global predicate is detected to be true. Otherwise, the token is 
sent to the process returned by 
It is easy to see that 
whenever a process receives the token, it deletes at least one local state, that is, it 
receives at least one message from the application process. Every time a state is 
eliminated, O ( N )  work is performed by the process with the token. There are at 
most mN states; therefore, the total computation time for all processes is O(N2m). 
The work for any process in the distributed algorithm is at most O ( N m ) .  The 
analysis of message and space complexity is left 
an exercise (see Problem 10.4). 
The implementation for the algorithm is given in Figure 10.5. 
Let us analyze the time complexity of the algorithm. 
Problems 
10.1. Show that it is sufficient to send the vector clock once after each message is 
sent irrespective of the number of messages received. 
10.2. Assume that the given global predicate is a simple conjunction of local pred- 
icates. Further assume that the global predicate is stable. In this scenario, 
both Chandy and Lamport’s algorithm and the weak conjunctive algorithm 

174 
CHAPTER 10. GLOBAL PROPERTIES 
mport j a v a .  u t i l  .*; 
,ublic c l a s s  DistSensor extends Process implements Runnable , Sensor { 
f i n a l  s t a t i c  i n t  red = 0 ,  green = 1; 
i n t  candidate [ I ,  color [ I ,  G [ ] ;  
boolean finished = f a l s e  , haveToken = f a l s e  ; 
LinkedList q = new LinkedList ( ) ;  
Sensoruser app; 
public DistSensor (VCLinker initCornm, Sensoruser app) { 
super (initComm ) ; t h i s  . app = app : 
candidate = new i n t  [ N ] ;  rolor = new i n t  [N]; G = new i n t  [ N ] ;  
f o r  ( i n t  j =O; j < N; j + + )  { color [ j ]  = r e d ;  G [ j ]  = 0 ; )  
if ( myId == Symbols. coordinator ) haveToken=true ; 
new T h r e a d ( t h i s ) .  s t a r t  ( ) ;  
1 
public synchronized void run ( ) {  
while (! finished ) { 
while ( ! haveToken ) myWait ( )  ; 
handleToken ( )  ; 
1 
1 
public synchronized void handleToken ( )  { 
while ( color [myId] == r e d )  { 
while ( q . i s E m p t y ( )  && ! finished ) mywait(); 
if ( 9 .  isEmpty() && finished ) { 
app. globalPredicateFalse (myId); r e t u r n ;  
t 
candidate = ( i n t  [ I )  q .  removeFirst ( ) ;  
if ( candidate [ myId] > G[ myId]) { 
G[ myId] = candidate [ niyId] ; color [ myId] = green ; 
t 
for ( i n t  j = 0 ;  j < N ;  j++) 
if ( (  j != myId) && ( c a n d i d a t e  [ j 1 >= G[ j 1 ) )  { 
G[ j ] = candidate [ j I ;  color [ j ] = r e d ;  
t 
i n t  j = Util . searchArray (color , red ); 
if ( j  I =  -1) sendToken(j); 
e l s e  { app. globalPredicateTrue ( G ) ;  finished = t r u e ;  ] 
public synchronized void handleMsg(Msg m, i n t  src , String 
if ( t a g .  equals (”TokenG” ) )  Util . readArray (m. getMessage 
e l s e  if ( t a g .  equals ( ” T o k e n c o l o r ” ) )  { 
LJtil . readArray (m. getMessage ( ) ,  
haveToken = t r u e  ; 
color ) ;  
} e l s e  if ( t a g .  equals ( ” f i n i s h e d ” ) )  finished = t r u e ;  
notifyAll ( ) ;  
} 
void sendToken(int i 
4 
( (  VCLinker) ‘corn): simpleSendMsg ( j  , ”TokenG” , Util . writeArray ( G ) )  ; 
(( VCLinker) comm). simpleSendMsg( j , ”Tokencolor”, Util . writeArray (color 
haveToken = f a l s e ;  
1 
1 
public synchronized void 1ocalPredicatcTrue (Vectorclock vc) { 
q .  a d d ( v c . v ) ;  notifyAll (); 
Figure 10.5: Token-based WCP detection algorithm. 

10.5. PROBLEMS 
can be used to detect the global predicate. What are the advantages and 
disadvantages of using each of them? 
10.3. Show that if the given weak conjunctive predicate has a conjunct from each 
of the processes, then direct dependency clocks can be used instead of the 
vector clocks in the implementation of sensors. Give an example showing that 
if there is a process that does not have any conjunct in the global predicate, 
then direct dependency clocks cannot be used. 
10.4. Show that the message complexit,y of the vector-clock-based distributed al- 
gorithm is O(mlv), the bit complexity (number of bits communicated) is 
O(N2m), and the space complexity is O(mN) entries per process. 
10.5. The main drawback of the single-token WCP detection algorithm is that it has 
no concurrency-a 
monitor process is active only if it has the token. Design an 
algorithm that uses multiple t,okens in the system. 
Partition the set. of 
monitor processes into g groups and use one token-algorithm for each group. 
Once there are no longer any red states from processes within the group, t,he 
token is returned to a predetermined process (say, Po). When Po has received 
all the tokens, it merges the informat,ion in the g tokens to identify a new 
global cut. Some processes may not satisfy the consistency condition for this 
ncw cut. If so, a token is sent into each group containing such a process.] 
10.6. Design a hierarchical algorithm to detect WCP based on ideas in t,he previous 
excrcise. 
10.7. Show the following properties of the vector-clock-based algorithm for WCP 
det,ect.ion: for any i, 
1. G[i] # 0 A color[i] = red + 3j : # i : (i, G[i]) 
-+ ( j ,  G[j]); 
2. color[i] = green + V k  : 
G[i]) ft ( k ,  G[k]); 
3. (cnlor[i] = green) A (colork] = green) + (i, G[i])I((j, 
G[j]). 
4. If (color[i] = red), then there is no global cut satisfying the WCP which 
includes 
G[i]). 
10.8. Show the following claim for the vector-clock-based distributed WCP detection 
algorithm: The flag detect is true with G if and only if G is the smallest global 
state that satisfies the WCP. 
*10.9. (due to Hurfin et, al.[HMRS95]) Assume that every process communicates with 
every other process directly or indirectly infinitely often. Design a distributed 
algorithm in which information is piggybacked on existing program messages 
to detect a conjunctive predicate under this assumption, that is, the algorithm 
does not use any additional messages for detection purposes. 

176 
CHAPTER 10. GLOBAL PROPERTIES 
10.6 Bibliographic Remarks 
Detection of conjunctive properties 
first discussed by Garg and Waldecker[GW92]. 
Distributed online algorithms for detecting conjunctive predicates were first pre- 
sented by Garg and Chase [GC95]. Hurfin et al.[HMRS95] were the first to give a 
distributed algorithm that does not use any additional messages for predicate de- 
t,ection. Their algorithm piggybacks additional informat,ion on program messages 
to detect conjunctive predicates. Distributed algoribhms for offline evaluation of 
global predicates are also discussed in Venkatesan arid Dathan [VD92]. Stoller and 
Schneider [SS95] have shown how Cooper and Marzullo’s algorithm can be inte- 
grat,ed with that of Garg and Waldecker to detect conjuriction of global predicates. 
Lower bounds on t,hese algorithms were discussed by Garg [Gar92]. 

Chapter 11 
Detecting Termination and 
Deadlocks 
11.1 Introduction 
Termination and deadlocks are crucial predicates in a distributed system. Gen- 
erally, compiit,ations are expected to terminate and be free from deadlocks. It is 
an important problem in distributed computing to develop efficient algorithms for 
termination and deadlock detection. Note that both termination and deadlock are 
stable properties and therefore can be detected using any global snapshot algorithm. 
However, these predicates can be detected even more efficiently than general stable 
predicates. The reason for this efficiency is that these predicates are not only stable 
but also locally stablethe state of each process involved in the predicate does not 
change when the predicate becomes true. We will later define and exploit the locally 
stable property of the predicates to design efficient distributed algorithms. 
To motivate termination detection, we consider a class of distributed computa- 
tions called digusing computations. We give a diffusing computation for the problem 
of determining the shortest path from a fixed process. The diffusing computation 
algorithm works except, that one does not know when the computation has termi- 
nated. 
11.2 Diffusing Computation 
Consider a computation on a distributed system that is st,arted by a special process 
called environment. This process start,s up the computation by sending messages t,o 
177 

178 
CHAPTER 11. DETECTING TERMINATION AND DEADLOCKS 
some of the processes. Each process in the syst,em is either passive or active. It is 
assumed that a passive process c,an become active only on receiving a message (an 
active process can become passive at any time). Furthermore, a message can be sent 
by a process only if it is in the active state. Such a cornputsation is called a diflusiag 
co~riputation. Algorithms for many problems such 
computing t,he breadth-first 
trch-spanning tree in an asynchronous net,work or determining the shortest pat,hs 
from a processor in a network can be structured as diffusing comput,ations. 
We use a distributed shortest-pat,h algorithm to illustrat,e the concepts of a 
diffusing coniput,ation. Assume that we are interested in finding the shortest path 
from a fixed proccss called a coordinator (say, 
to all other processes. Each 
process initially knows only t,he average delay of all its incoming links in the array 
A diffusing computation to compute the shortest path is quite simple. 
Every process 
maintains the following variables: 
represents the cost of the shortmest path from the coordinator to Pi as 
known to 
current,ly 
represents t,he predecessor of 
in the shortest, pat,h from t,he coor- 
dinator t,o Pi as known to 
currently 
The coordinator acts as the environment and starts up the diffusing coriiputat,ion 
by sending the cost of the shortest path to be 0 using a message type path. Any 
process 
that receives a message from 
of type path with cost c determines 
whet,her its current, cost is greater t,han the cost of reaching Pj plus t,he cost of 
reaching from Pj t,o Pi. If t,hat is indeed the case, then 
has discovered a path 
of short>er cost, and it u p d a b  the 
and 
variables. Further, any such 
iipdat,e results in messages to its neighbors about it.s new cost. The algorithm is 
shown in Figure 11.1. Each process calls the met,hod 
to start the program. 
This call results in the coordinator sending out messages with 
0. The method 
simply handles messages of type path. 
The algorit,hm works fine with one catch. No process ever knows when it is done, 
that is, the cost variable will not decrease further. In this chapter, we study how we 
can ext.end the computation to detect termination. Figure 11.2 shows the interface 
iniplemerited by the termination detect,iori algorithm. Any application which uses a 
must invoke 
at the beginning of the program, 
on sending 
message, arid 
on turning passive. 
From properties of a diffusing computat,ion, it follows that if all processes are 
passive in t,l-ic system and there are no messages in transit, then the computation has 
t,erminated. Our problem is to design a protocol by which the environment, process 
can deterniine whether the computation has terminated. Our solution is based on 
an algorithm by Dijkstra and Scholten. 

11.2. DIFFUSING COMPUTATION 
179 
,ublic class ShortestPath extends Process { 
int parent = -1; 
int cost = -1; 
int edgeweight [I = null; 
public ShortestPath (Linker initComm, int i n i t c o s t  
{ 
super (initcomm); 
edgeweight = i n i t c o s t  ; 
1 public synchronized void i n i t i a t e  ( )  { 
if ( myId == Symbols. coordinator ) { 
parent = myId; 
cost = 0; 
sendToNeighbors ( ” p a t h ”  , cost ); 
} 
1 
public synchronized void handleMsg(Msg m, int src , String tag){ 
if ( t a g .  equals ( ” p a t h ” ) )  { 
int d i s t  = m. getMessageInt ( ) ;  
if ( ( p a r e n t  == -1) 1 1  ( d i s t  + edgeweight[ s r c ]  < c o s t ) )  { 
parent = s r c ;  
cost = d i s t  + edgeweight [ src 1 ;  
System. o u t .  p r i n t l n  (”New cost is ” + cost ); 
sendToNeighbors ( ”  path” , cost ) ; 
} 
Figure 11.1: A diffusing computation for the shortest path 
public interface TermDetector { 
public void i n i t i a t e  ( ) ;  
. 
public void sendAction ( )  ; 
public void turnpassive ( ) ;  
public void handleMsg(Msg m, int s r c s I d ,  String t a g ) ;  
1 
Figure 11.2: Interface for a termination detection algorithm 

180 
CHAPTER 11. DETECTING TERMINATION AND DEADLOCKS 
11.3 Dijkstra and Scholten’s Algorithm 
We say that a process is in a green state if it is passive and all of its outgoing channels 
are empty; otherwise, it is in a red state. How can a process determine whether its 
outgoing channel is empty’! This can be done if the receiver of the channel signals 
the sender of t,he channel the number of messages received along that channel. If 
the sender keeps a variable D[i] (for deficit) for each outgoing channel i, which 
records the number of messages sent, minus the number of messages that have been 
acknowledged via signals, it can determine that the channel i is empty by checking 
whetsher D[i] = 0. Observe that D[i] 0 is always true. Therefore, if 0 is the set 
of all outgoing channels, it follows that 
vi E 
: D[i] = 0 
is ecluivalent, t,o 
C D[i] = 0. 
tEO 
Thus it is sufficient for a process to maintain just one variable D that represents the 
total deficit for the process. 
It is clear that if all processes are in the green state, then the computation has 
terminated. To check this condition, we will maintain a set T with the following 
invariant (10): 
(10) All red processes are part of the set 
Observe that green processes may also be part of T-the 
invariant is that t8here 
is no red process outside T .  When the set T becomes empty, termination is true. 
When the diffusing computation st,arts, the environment is the only red proccss 
initially (with no1iempt.y outgoing channels); the invariant is made true by keeping 
environment in the set T. To maintain the invariant that all red processes are in T ,  
we use the following rule. If p3 turns Pk red (by sending a message), and pk is riot 
in T ,  then we add Pk to T. 
We now induce a directed graph (T, E )  on the set T by defining the set of edges 
E as follows. We add an edge from Pj to 9, if Pj was responsible for addition of 
Pk to t,he set T. We say that Pj is {,he parent, of 4. From now on we use the ternis 
n.ode arid process interchangeably. Because every node (other than the enviromnerit) 
has exact,ly one parent and an edge is drawn from Pj t.0 Pk only when Pk is not 
part of T ,  t,he edges E form a spanning tree on T rooted at the environment. Our 
algorit.hm will mainhin this 
invariant: 

11.3. DIJKSTRA AND SCIIOLTEN’S ALGORITHM 
181 
(11) The edges E form a spanning tree of nodes in T root,ed at the environment. 
Up to now, our algorithm only increases the size of T. Because detection of termi- 
nation requires the set to be empty, we clearly need a mechanism to remove nodcs 
from T .  Our rule for removal is simple-a 
node is removed from T only if it is a 
green-leaf node. When a node is removed from T, the incoming edge to that node is 
also removed from E. Thus the invariants (10) and (11) are maintained by this rule. 
To implement this rule, a node needs to keep track of the number of its children in 
T .  This can be implemented by keeping a variable at each node numchild initial- 
ized to 0 that denotes the number of children it has in T .  Whenever a new edge is 
formed, the child reports this to tlie parent by a special acknowledgment that also 
indicates that a new edge has been formed. When a leaf leaves T ,  it reports this 
to the parent, who decrements the count,. If the node has no parent. (it must be 
the environment) and it leaves the set T, thcn termination is detected. By assum- 
ing that a green-leaf node eventually reports to its parent, we conclude that once 
the computation terminates, it is eventually detected. Conversely, if termination is 
detected, then the computation has indeed krminated on account of invariant (10). 
Observe that the property that a node is green is not stable and hence a node, my, 
F‘k, t,hat is green may become active once again on receiving a message. However, 
because a message can be sent only by an active process, we know that some active 
process (which is already a part of the spanning tree) will be now responsible for 
the node p k .  Thus tlie tree T changes with time but maintains tlie invariant that 
all active nodes are part of the tree. 
11.3.1 An Optimization 
The algorit,hm given above can be optimized for the number of messages by com- 
bining messages from the reporting process and the messages for detecting whether 
a node is green. To detect whether an outgoing channel is empty, we assumed a 
mechanism by which the receiver tells the sender the number of messages it has 
received. One implementation could be based on control messages called signal. For 
every message received, a node is eventually required to send a signal message to 
the sender. To avoid the use of report messages, we require that a node not send 
the signal message for the message that made it active until it is ready to report to 
leave T. When it is ready to report, the signal message for the message that made 
it active is sent. With this constraint we get an additional property that a node will 
not turn green unless all its children in the t,ree have reported. Thus we have also 
eliminated the need for maintaining numchlld: only a leaf node in the tree can be 
green. A node is ready to report when it has turned green, that is, it is passive and 
D = 0. The algorithm obtained after the optimization is shown in Figure 11.3. 

182 
CHAPTER 11. DETECTING TERMINATION AND DEADLOCKS 
The algorithm uses state to record the state of the process, D to record t,he 
deficit, and parent to record the parent of the process. There is no action required 
on i n i t i a t e .  The method handleMsg reduces deficit on receiving a signal message 
at line 15. If D becomes 0 for the environment process, then termination is detected 
at line 18. On receiving an application message, a node without parent sets the 
source of the message as the parent. In this case, no signal is sent back. This signal 
is sent at line 20 or line 38 when this node is passive and its D is 0. If the receiving 
node had a parent, then it simply sends a signal message back at line 29. The 
method sendAct i o n  increments the deficit and the method turnpassive changes 
state to passive and sends a signal to the parent if D is 0. 
Now we can solve the original problem of computing the shortest path 
shown 
in Figure 11.4. 
11.4 Termination Detection without Acknowledgment 
Messages 
Dijkstra and Scholten’s algorithm required overhead of one acknowledgment message 
per application message. We now present, an algorithm due to Safra 
described by 
Dijkstra which does not use acknowledgment messages. This algorithm is based on 
a token going around the ring. The token collects the information from all processes 
and determines whether the computation has terminated. The algorithm shown in 
Figure 11.5 requires each process to maintain the following variables: 
1. state: The state of a process is either active or passive as defined earlier. 
2 .  color: The color of a process is either white or black. If the process is white, 
then it has not rcccived any message since t,he last visit of the token. This 
variable is initialized to white. 
3. c: This is an integer variable maintained by each process. It records the value 
of the number of messages sent by the process minus the number of messages 
received by that process. This variable is initialized to 0. 
Process Po begins the detection probe by sending token to the next process when 
it. is passive. The token consists of two fields: c o l o r and count. The color simply 
records if the token has seen any black process. The count records sum of all c 
variables seen in this round. 
When a process receives the token, it keeps the token until it becomes passive. 
It then forwards the token to the next process, maintaining the invariants on the 
color of t.he token and the count of the token. Thus, if a black process forwards 
the token, the token turns black; otherwise t,he token keeps its color. The count 

11.4. TERMINATION DETECTION WITHOUT ACKNOWLEDGMENT MESSAGES 
183 
1 public c l a s s  DSTerm extends Process implements TermDetector { 
2 
f i n a l  s t a t i c  i n t  passive = 0 ,  active = 1; 
3 
i n t  s t a t e  = passive; 
4 
i n t  D = 0; 
5 
i n t  parent = -1; 
6 
boolean envtFlag ; 
7 
public DSTerm( Linker initconmi) { 
8 
super (initComm ) ; 
9 
envtFlag = (myId == Symbols. coordinator ); 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
21 
22 
23 
24 
25 
26 
27 
28 
29 
30 
31 
32 
33 
34 
35 
36 
37 
38 
39 
40 
41 
42 } 
public synchronized void i n i t i a t e  ( )  { 
public synchronized void handleMsg(Msg m, i n t  src , String t a g )  { 
} 
if ( t a g .  equals ( ”  s i g n a l ” ) )  { 
D = D -  
i f  ( D  == 0) { 
if ( e n v t F l a g )  
e l s e  if ( s t a t e  == passive ) { 
System. o u t .  p r i n t l n  (”Termination Detected”); 
sendMsg(parent , ” s i g n a l ” ) ;  
parent = -1; 
} 
1 
} else { // a p p l i c a t i o n  message 
s t a t e  = a c t i v e ;  
if ( (  parent == -1) && ! envtFlag ) { 
} e l s e  
parent = s r c ;  
sendMsg ( src , ” signal ” ) ; 
1 
} 
1 
public synchronized void sendAction ( )  { 
D = D + l ;  
public synchronized void turnpassive () { 
s t a t e  = p a s s i v e ;  
if ( ( D  == 0) && (parent != -1)) { 
sendMsg( p a r e n t ,  ” s i g n a l ” ) ;  
parent = -1; 
1 
} 
Figure 11.3: Termination detection algorithm 

184 
CHAPTER 11. DETECTING TERMINATION AND DEADLOCKS 
public c l a s s  TermShortestPath extends ShortestPath { 
TermDetector td = n u l l  ; 
public TerniShortestPath (Linker initcoinm, i n t  i n i t c o s t  
TerrnDetector td ) { 
super(iiiitComrn, i n i t c o s t  ) ;  
t h i s . t d  = t d ;  
} public synchronized void i n i t i a t e  () { 
s u p e r .  i n i t i a t e  ( ) ;  
t d .  iiiit.iate ( ) ;  
1 
public synchronized void sendhlsg( i n t  dest , String t a g ,  i n t  msg) { 
s u p e r .  sendMsg( dest , t a g ,  insg); 
t d .  sendAction ( ) ;  
public synchronized void liaiidlehlsg (Msg ni, i n t  src ~ 
String t,ag ) { 
tad. handleMsg (In, src , t a g  ) ; 
s u p e r .  handleMsg (m, src , tag ); 
t d .  t u r n p a s s i v e  ( ) ;  
1 
} 
Figure 11.4: A diffusing romputation for the shortest path with termination 

11.5. LOCALLY STABLE PREDICATES 
variable in the token is increased by c. The process resets its own color to white 
after sending the token. 
Process 
is responsible for detecting termination. On receiving the token, 
detects termination, if its own color is white? it, is passive, the token is white and 
the sum of token count and c is 0. If termination is not detected, then Po can start. 
a new round of token passing. The correctness of this algorithm will he apparent 
after the discussion of locally st.able predicates. 
11.5 Locally Stable Predicates 
We now show a technique that can be used for efficient detection of not only ter- 
mination hut many other locally stable predicates 
well. A stable predicate B 
is locally stable if no process involved in the predicate can change its state relative 
to B once B holds. In ot,her words, the values of all the variables involved in t,he 
predicate do not change once the predicate becomes true. The predicate B, “the 
distributed computation has terminated,” is locally stable. It is clear that if B is 
true, the states of processes will not change. Similarly, once there is a deadlock in 
the system the processes involved in the deadlock do not change their state. 
Now consider the predicate B, “there is at most one token in the system.” This 
predicate is stable in a system which cannot create tokens. It is not locally stable 
because the state of a process can change by sending or receiving a token even when 
the predicate is true. 
Since a locally stable predicate is also stable, one can use any global snapshot 
algorithm to detect it. However, computing a single global snapshot requires O(e) 
messages, where e is the number of unidirectional channels. We will show that for 
locally stable predicates, one need not compute a consistent global state. 
We first generalize the not,ion of a consistent cut to a consistent interval. An 
interval is a pair of cuts (possibly inconsistent) X and Y such that X 
Y .  We 
denote an interval by [X, Y ] .  
An interval of cuts [X,Y] is consistent if there exists a consistent cut G such 
that X 
G 
Y .  Note that [G,G] is a consistent interval iff G is consistent. We 
now show that an interval [X, Y ]  is consistent iff 
Ve,f : (f E 
A (e -+ f )  + e E Y 
(11.1) 
First assume that [X, Y ]  is a consistent interval. This implies that there exists 
a consistent cut G such that X 
G 
Y .  We need to show that Equation (11.1) 
is true. Pick any e, f such that f E 
and e -+ f .  Since f 
X and X 
G, we 
get that f E G. From the fact that G is consistent, we get that e E G. But e 
G 
implies that e 
Y because G 
Y .  Therefore Equation (11.1) is true. 

186 
CHAPTER 11. DECTECTING TERMINATION AND DEADLOCKS 
mport j a v a .  u t i l  .*; 
,ublic class TerrnToken extends Process implements TermDetertor 
final static int passive = 0 ,  active = 1, white = 0, black 
int s t a t e  = p a s s i v e ,  color == white; 
int c = 0; 
int n e x t ;  
boolean haveToken = false ; 
int tokencount = 0 ,  tokencolor = white; 
public TerrnToken( Linker initComm) { 
super ( initcornin) ; 
next = (myId + 1) % 
1 
{ = 1: 
I public synchronized void i n i t i a t e  () { 
if ( myId == Symbols. coordinator ) { 
i f  ( s t a t e  == passive ) sendToken (); 
else haveToken = true ; 
1 
} public synchronized void handleMsg (Msg ni, int src , String tag ) { 
if ( t a g .  equals ("termToken")) { 
haveToken = true ; 
StringTokenizer s t  = new StringTokenizer (m. getMessage 
tokencolor = I n t e g e r .  parseInt ( s t  . nextToken 
tokencount = I n t e g e r .  parseInt ( s t  .nextToken 
if ( myId == Symbols. coordinator ) { 
if ( (  c + tokencount == 0) && ( c o l o r  == white) && 
System. o u t .  p r i n t l n  ("Termination Detected"); 
1iaveToken = false; 
( s t a t e  == passive) && ( tokencolor == whit.e)) { 
1 
1 if ( (  s t a t e  == passive ) && haveToken) sendToken ( ) ;  
s t a t e  = a c t i v e ;  
color = black; 
} else { // application message 
1; 
c = c -  
1 
c = c + l ;  
1 
I 
public synchronized void seridAction ( )  { 
public synchronized void turnpassive ( )  { 
s t a t e  = p a s s i v e ;  
if ( haveToken ) sendToken ( ) ; 
} void sendToken () { 
if ( myId == Symbols. coordinator ) 
else if ( (  color == black ) I I ( tokenColor == black)) 
else 
haveToken = false; 
color = white; 
sendMsg(next , "term'roken", 
w h i t e ,  0 ) ;  
sendMsg ( next , " term'roken" , black , c + tokencount ); 
sendMsg( n e x t ,  "term'l'oken" , white, c + tokencount ); 
1 
Figure 11.5: Termination detection by token traversal. 

11.5. LOCALLY STABLE PREDICATES 
Conversely, assume that Equation (11.1) is true. We define the cut G as follows: 
G = {e E I 
X : (e -+ f )  V (e = f ) }  
Clearly, X E G from the definition of G and G 
Y because of Equation (11.1). 
We only need to show that G is consistent. Pick any c, d such that c -+ d and d 
G. 
From the definition of G, there exists f E X such that d = f or d + f .  In either 
case, c -+ d implies that c -+ and therefore c 
G. Hence, G is consistent. 
Our algorithm will exploit the observation presented above as follows. It repeat- 
edly computes consistent intervals [ X ,  Y ]  and checks if B is true in Y and the values 
of variables have not changed in the interval. If both these conditions are true, then 
we know that there exists a consistent cut G in the interval with the same values 
of (relevant) variables as 
and therefore has B true. Conversely, if a predicate 
is locally stable and it turns true at a global state G, then all consistent intervals 
[ X ,  Y ]  such that G 
X will satisfy both the conditions checked by the algorithm. 
Note that computing a consistent interval is easier than computing a consistent 
cut. To compute a consistent interval, we need to compute any two cuts X and 
Y ,  such t.hat X 2 Y and Equation (11.1) holds. To ensure that Equation (11.1) 
holds, we will use the notion of barrier synchronization. Let X and Y be any cuts 
such that X 
Y (i.e., [ X , Y ]  is an interval) and Y has at least one event on every 
process. We say that an interval [ X ,  Y ]  is barrier-synchronized if 
Vg 
X A h 
E - Y g -+ h 
Intuitively, this means that every event in X happened before every event that is 
not in Y .  If [ X , Y ]  are barrier synchronized, then t.hey form a consistent interval. 
Assume, if possible, that [ X , Y ]  is not a consistent interval. Then there exist e , f  
such that f 
X ,  e + f ,  but e 
Y .  But e $ Y implies that f + e which contradicts 
e - +  
f .  
Barrier synchronization can be achieved in a distributed system in many ways. 
For example 
1. Po sends a token to PI which sends it to the higher-numbered process until 
it reaches P N - ~ .  Once it reaches P N - ~ ,  
the token travels in the opposite 
direction. Alternatively, the token could simply go around the ring twice. 
These methods require every process to handle only 0(1) messages with total 
O ( N )  messages for all processes but have a high latency. 
2. All processes send a message to Po. After receiving a message from all other 
processes, Po sends a message to everybody. This method requires total O ( N )  
messages and has a low latency but requires the coordinator to handle O ( N )  
messages. 

188 
CHAPTER 11. DETECTING TERMINATION AND DEADLOCKS 
3. All processes send messages to everybody else. This method is symmetric and 
has low latency but requires O ( N 2 )  messages. 
Clearly, in each of t,hese methods a happened-before path is formed from every event 
before the barrier synchronization to every process after the synchronization. 
Now dekcting a locally stable predicate B is simple. The algorithm repeat,edly 
collects two barrier synchronized cuts [ X ,  Y ] .  If the predicate B is t,rue in cut Y arid 
t,he values of the variables in the predicate B have not changed during the interval, 
then 
is announced to be true in spite of the fact that B is cvaliiated only on 
possibly iricorisist,ent cuts X and Y .  
11.6 Application: Deadlock Detection 
We illustrate the technique for detecting locally stable predicates for deadlocks. 
A deadlock in a distributed system can he characterized using the wait-for graph 
(WFG): a graph with nodes as processes and an edge from Pi to 1% if P, is waiting 
for Pj for a resource to finish its job or transaction. Thus, an edge from 
to 
means that, there exist one or more resources held by Pj without which 
cannot, 
proceed. We have assumed that, it process needs all the resources for which it, is 
waiting t.o finish its job. Clearly, if there is a cycle in the WFG, then processes 
involved in the cycle will wait forever. This is called a deadlock. 
A simple approach to detecting deadlocks based on the idea of locally stable 
predicates is as follows. We use a coordinator to collect information from processes 
in the system. Each process Pi maintains its local WFG, that is, all the edges in the 
WFG that, are outgoing from Pi. It also maintains a bit charigedi, which records 
if its WFG has changed since its last report to the coordinator. The coordinator 
periodically sends a request message to all processes requesting their local WFGs. 
On receiving this request,, a process sends its local WFG if the clrangedi bit is true 
and “notmChariged” message if changedi is false. On receiving all local WFGs, the 
coordinator can combine them to form the global WFG. If this graph has a cycle, 
the coordinator sends a message to all processes to send their reports again. If 
chan.,qedi is false for all processes involved in the cycle, then t,he coordinat,or reports 
deadlock. 
In t>his algorit(hrr1, even though WFGs are constructed possibly on inconsistent 
global st,ates, we know, thanks to barrier synchronization, that there exists a con- 
sistent global state with the same WFG. Therefore, any deadlock reported actually 
happened in a consistent, global state. 
We leave the Java implementation of this algorithm as an exercise. 

11.7. PROBLEMS 
189 
11.7 Problems 
11.1. What is the message complexity of Dijkstra and Scholten’s algorithm? 
11.2. Give an algorithm based on diffusing computation to determine the breadth- 
first search tree from a given processor. 
11.3. Extend Dijkstra and Scholten’s algorithm for the case when there can be 
multiple initiators of the diffusing computation. 
11.4. Prove the correctness of the token-based algorithm for termination detection. 
11.5. Give a Java implementation of the two-phase deadlock detection algorithm. 
11.8 Bibliographic Remarks 
The spanning-tree-based algorithm discussed in this chapter is a slight variant of the 
algorithm proposed by Dijkstra and Scholt,en [DSSO]. The token-based termination 
algorit,hm is due to Safra as described by Dijkstra [Dij87]. The notion of locally 
stable predicates is due to Marzullo and Sabel [MSS4]. The not,ion of consistent 
interval and the algorithm of detecting locally stable predicates by using two cuts 
is due to Atreya, Mittal and Garg [AMGO3]. The two-phase deadlock detection 
algorithm is due to Ho and Ramamoorthy [HR82]. 

This Page Intentionally Left Blank

Chapter 12 
Message Ordering 
12.1 Introduction 
Distributed programs are difficult to design and test because of their nondetermin- 
istic nature, that is, a distributed program may exhibit multiple behaviors on the 
same external input. This nondeterminism is caused by reordering of messages in 
different executions. It is sometimes desirable to control this nondeterminism by 
restricting the possible message ordering in a system. 
Figure 12.1: A FIFO computation that is not causally ordered 
A fully asynchronous computation does not have any restriction on the message 
ordering. It permits maximum concurrency, but algorithms based on fully asyn- 
chronous communication can be difficult to design because they are required to 
work for all ordering of the messages. Therefore, many systems restrict message 
191 

192 
CHAPTER 12. MESSAGE ORDERING 
delivery to a FIFO order. This results in simplicity in design of distributed algo- 
rithms based on the FIFO assumption. For example, we used the FIFO assumption 
in Lamport's algorithm for mutual exclusion and Chandy and Lamport's algorithm 
for a global snapshot. 
A FIFO-ordered computation is impleniented generally by using sequence num- 
bers for messages. However, observe that by using FIFO ordering, a program loses 
some of its concurrency. When a message is received out of order, its processing 
must be delayed. 
A st,ronger requirement than FIFO is t,hat of causal o r d e k g .  Intuitively, causal 
ordering requires that a single message not, be overtaken by a sequence of messages. 
For example, the computation in Figure 12.1 satisfies FIFO ordering of niessagcs 
but, does not satisfy causal ordering. A sequence of messages from PI to P2 and 
from €3 to P.3 overtakes a message from PI to P3 in this example. Causal ordering 
of messages is useful in many contexts. In Chapter 8, we considered the problem 
of mutual exclusion. Assunie that we use a centralized coordinator for granting 
requests to the access of the critical section. The fairness property requires that 
the request,s be honored in t,he order they are made (and not in the order t,hey 
are received). It is easy to see that if t,he underlying system guaranteed a causal 
ordering of messagcs, then the order in which rcquests are received cannot violate 
the happened-before order in which they are made. For another example of the 
usefulness of causal ordering, see Problem 12.1. 
The relationship among various message orderings can be formally specified on 
the basis of the happened-before relation. For convenience, we denote the receive 
event corresponding to t,he send event si by 
and vice versa. The message is 
represenkd as ( s i , ~ ~ ) .  
We also use si -A ri to denote that 
is the receive event 
corresponding to the send event 
Finally, we use e 4 f to denote that e occurred 
before f in the same process. 
Now, FIFO and causally ordered computations can be defined as follows: 
FIFO: Any two messages from a process Pi to Pj are received in the same order as 
they were sent. Formally, let s1 arid s2 be any two send events and 
and r2 
be the corresponding receive events. Then 
Cnimdly Ordered: Let m y  two send events s1 arid 32 in a. distributed coinputation 
be related such that the first send happened before the second send. Then, 
the second message cannot be received before the first, message by any process. 
Formally, this can be expressed as 

12.2. CAUSAI, ORDERING 
12.2 Causal Ordering 
193 
. 
var 
M:array[l..N, 1..N] of integer initially V j ,  k : M [ j ,  k] = 0; 
To send a message to Pj: 
M [ i , j ]  := M [ i , j ]  + 1; 
piggyback M as part of the message; 
To receive a message with matrix W from Pj 
enabled if ( W [ j ,  = M [ j ,  + 1) A (Vk # j : M [ k ,  
21 
W[k, 
M := max(M, W ) ;  
Figure 12.2: An algorit,hm for causal ordering of messages at Pi 
We now describe an algorithm to ensure causal ordering of messages. We assunie 
that a process never sends any message to itself. Each process maintains a matrix 
M of integers. The entry M [ j , k ]  at Pi records the number of messages sent by 
process Pj to process 
as known by process Pi. The algorithm for process 
is 
given in Figure 12.2. Whenever a message is sent from Pi to Pj, first the entry 
M [ i , j ]  is incremented to reflect the fact that one more message is known to be 
sent, from Pi t,o Pj. The matrix M is piggybacked wit.h t.he message. Whenever 
messages are received by the communication system at Pi, they are first checked 
for eligibility before delivery t,o Pi. If a message is not eligible, it is simply buffered 
until it becomes eligible. A message rri from Pj is eligible to be received when 
1. The entry W[j, is one more than t.he entry M [ j ,  
21 that records the number 
of messages received by 
from Pj. 
2. The number of messages sent from any ot,her process Pk(k # j )  tjo 
indicated by the matrix W in the message, is less than or equal to the number 
recorded in the matrix M .  Formally, t,his condition is 
Vk # j : M [ k ,  
W[k, 
i ]  
If for some k ,  W[k,i] > M [ k , i ] ,  then there is a message that was sent in the 
causal history of the message and has not arrived yet. Therefore, P, must wait 
for that message to be delivered before it can accept the message m. 

194 
CHAPTER 12. MESSAGE ORDERING 
Whenever a message is accepted for delivery, the information at matrix M is 
updated with the matrix W received in the message. 
The structure of a causal message is shown in Figure 12.3, and the Java im- 
plementation of the causal ordering algorithm is shown in Figure 12.4. The causal 
ordering algorithm extends the class 
to include the matrix in outgoing mes- 
sages. The method 
increments the entry AT[rnyld] [destlrl] to account 
for t,his message and attaches the mat,rix M wit,h it. The method 
is 
used for sending a message to multiple sites. In this method, we first increment 
AT[myZd][destZd] for all destld in the list of destinations. It is this matrix that is 
sent with every message. 
The niethod 
determines whether a message can be delivered to 
the process. The met,hod 
uses two 
for storing messages. 
The 
stores all messages that are deliverable to the application layer. The 
stores all messages that are received but are not deliverable. When the 
application layer asks for message, the 
is traversed first to check whether 
some messages are deliverable. Deliverable messages are moved from the 
to the 
by the method 
If 
empty, then we 
wait for a message to arrive by calling the blocking method 
On 
receiving this message, it is put in t,he 
and tjhe method 
is invoked again. If 
is nonempty, the first message from that queue is 
delivered and the matrix M updated to record the delivery of this message. 
public class CausalMessage { 
Msg m; 
int N; 
public CausalMessage (Msg m, int N, int matrix [ I  [ I  ) { 
int Vl [ I ;  
this .m = m; 
this.N = N; 
W =  matrix; 
1 
1 
1 
public int [ I  
getMatrix ( )  { 
return W; 
public Msg getMessage () { 
return m; 
} 
Figure 12.3: Structure of a causal message 

12.2. CAUSAL ORDERING 
super. sendMsg ( destId , t a g ,  msg); 
1 
public synchronized void multicast ( IntLinkedList destIds , 
String t a g ,  String msg) { 
for ( i n t  i =O; i<destIds . size (); i++) 
M[ myId] [ destIds . getEntry ( i )I++; 
for ( int i = O ;  i<destIds . size ( ) ;  i ++) { 
int destId = destIds . getEntry ( i ) ; 
destId , " m a t r i x " ,  Matrix. write (M)); 
super. sendMsg( destId , t a g ,  msg); 
1 
1 
boolean okayToRecv( int W[] [ I ,  int srcId ) { 
if 
srcId ] [ myId] > 
s r r I d  ] [ myId]+l) return false ; 
for ( i n t  k = 0 ;  k < N; k++) 
return true ; 
if ( (  k!=srcId) && (W[k][ myId] > M[k][ myId])) return false; 
1 
synchronized void checkPendingQ ( )  { 
L i s t I t e r a t o r  i t e r  = pendingQ. l i s t I t e r a t o r  ( 0 ) ;  
while ( i t e r  . hasNext ( ) )  { 
CausalMessage cm = ( CausalMessage) i t e r  . next (); 
if (okayToRecv(cm. getMatrix ( ) ,  em. getMessage ( ) .  getSrcId ( ) ) ) {  
i t e r  .remove ( ) ;  deliveryQ. add(cm); 
1 
} 
1 // p o l l s  the channel given by fromld t o  add t o  the pendingQ 
public Msg receiveMsg (int fromId ) throws IOException { 
checkPendingQ ( )  ; 
while ( deliveryQ . isEmpty ( ) )  { 
Msg matrix = super. receiveMsg ( fromId ) ; // matrix 
int [ ] [ ] W =  new int[N][N]; 
Matrix. read ( matrix. getMessage ( )  , 
; 
Msg ml = super. receiveMsg (fromId ); //app message 
pendingQ . add (new CausalMessage (ml, N, 
) ; 
checkPendingQ ( )  ; 
1 
CausalMessage cm = ( CausalMessage ) deliveryQ . removeFirst (); 
Matrix. setMax(M, em. getMatrix 0); 
return em. getMessage ( ) ;  
} 
I }  
Figure 12.4: CausalLinker for causal ordering of messages 

196 
CHAPTER 12. MESSAGE ORDERING 
12.2.1 Application: Causal Chat 
To illust,rate an application of causal ordering, we consider a chat application in 
which a user can send messages to multiple other users. This simple program, shown 
in Figure 12.5, takes as input from t,he user a message and the list of destination 
process identifiers. This message is then multicast to all the process identifiers in 
the list,. 
The application takes 
an argument the message ordering to be used. The 
user can verib that, if the plain 
class were used in this application, t,lien the 
following scenario would be possible. If Po sends a query to bot,li PI and Pl, and 
PI sends a reply to the query t,o both Po and P2, then Pl may receive t,he reply 
before the query. On t,he other hand, if the class 
is used, then such 
a scenario is not possible. 
12.3 Synchronous Ordering 
Synchronous ordering is a stronger requirement than causal ordering. A computa- 
tion satisfies synchronous ordering of messages if it, is equivalent t,o a cornput,at,ion 
in which all messages are logically instantaneous. Figure 12.6 gives an example of a 
synchronously ordered computation and Figure 12.7, an example of a comput,ation 
t,hat does riot satisfy synchronous ordering. 
Alg~rit~hnis 
for synchronous systems are easier to design than those for causally 
ordered systems. Thc niodel of synchronous message passing lets us reason about 
a distribukd program under the assumpt.ion that messages are instantaneous or 
“points” rather then “int,ervals” (i.e., we can always draw the time diagrams for 
the dist,ributed programs with the message arrows being vertical). If we assume 
messages 
points instead of intervals, we can order the messages 
a partial order 
and t,lierefore, we can have vector clocks wit,h respect to messages. One applicat,ion 
for synchronous ordering of messages is that it enables us to reason about distributed 
object,s 
if they were centralized. Assume that a process invokes an operation on a 
remote object by sending a messagc. If synchronous ordering of messages is assumed, 
then all operations on the objects can be ordered according to when the messages 
are sent because messages can be considered instantaneous. 
coniputration is synchronous if its time diagram can be drawn such that, all 
message arrows are vertical, that is: all external events can be assigned a t,iniest,amp 
such that time increases within a single process, and for any message its send arid 
receive are assigned the same timestamp. Formally, let & be the set of all external 
events. Then, a cornputfation is synchronous iff there exists a mapping T from & t,o 

12.3. SYNCHRONOUS ORDERING 
197 
public s t a t i c  void main( String [ I  
args ) throws Exception { 
String baseName = args [ O ] ;  
int myId = I n t e g e r .  parseInt ( a r g s  [ 11) ; 
int iiumProc = I n t e g e r .  parseInt (args [ 2 ] ) ;  
Linker conm = null; 
if ( args [ 3 ] .  equals ( ” s i m p l e ” ) )  
else if ( args [ 3 ] .  equals ( ” c a u s a l ” ) )  
else if ( args 131. equals ( ” s y n c h ” ) )  
Chat c = new Chat(comm); 
for ( int i = 0; i < numProc; i++) 
BufferedReader din = new BufferedReader ( 
new InputStreainReader (System. in ) ) ;  
while ( t r u e )  { 
String cliatMsg = c .  getUserInput ( d i n ) ;  
if ( chatMsg. equals ( ”  quit ” ) )  break; 
IntLinkedList destIds = c .  getDest ( d i n ) ;  
if ( a r g s  [ 3 ] .  equals ( ” s y n c l i ” ) )  
else 
comm = new Linker (baseName, myId, nurnProc); 
c o r n  = new CausalLinker (baseName, myId, numProc); 
c o r n  = new SynchLinker (baseNanie, rnyId, numProc); 
if ( i  != myId) (new L i s t e n e r T h r e a d ( i ,  c ) ) .  s t a r t  ( ) ;  
comm. sendMsg( destIds . getEntry ( 0 ) ,  ” c h a t ” ,  chatlllsg); 
c o r n .  nitilticast ( d e s t I d s  , ” c h a t ”  , chatMsg); 
1 
1 
1 
Figure 12.5: A chat program 

198 
CHAPTER 12. MESSAGE ORDERING 
1 
3 
2 
p 2  
T 
3 
1 
2 
1 
2 
Figure 12.6: A coniputat,ion that is synchronously ordered 
Figure 12.7: A computa1,ion that is not synchronously ordered 

12.3. SYNCHRONOIJS ORDERING 
199 
the set of natural numbers such that for all s, T ,  e, f E & 
s --+ T =+ T(s) = T(r) 
and 
e < f + T(e) < T(f) 
We call this condition SYNC. It is easy to see that, for any two external events 
e and .f 
We show that the hierarchy associated witjh the various message orderings is 
Synchronous 2 causally ordered 
FIFO 5 asynchronous. 
FIFO 
because 
asynchronous is obvious. A causally ordered computation satisfies FIFO 
4 s p  =+ 
s1 + s2. 
We only need to show that if a computation is synchronous then it is also causally 
ordered. Because the communication is synchronous, there exists a function T 
satisfying SYNC. 
For any set of send events 
s2 and receive events T I ,  7-2 such that s1 --+ T I ,  
s 2  - 
7-2 and 
-+ 
s2: 
T ( s ~ )  
= T(rl), 
T ( s ~ )  
= T(T~), mld T ( s ~ )  
< T(s~). 
It follows that' T(r1) < T(r2). Therefore, (12.1) implies 
T(7-2 + 
The algorithm for synchronous ordering uses control messages. Note that control 
messages do not have to be synchronously ordered. Thus & includes the send and 
receive events only of application messages. It does not include send and receive of 
control messages sent by the algorithm to ensure synchronous ordering. 
The algorithm shown in Figure 12.8 is for the process Pi. All processes have the 
same algorithm. Observe that the protocol to implement synchronous message or- 
dering cannot be completely symmetric. If two processes desire to send messages to 
each other, then there is no symmetric synchronous computation that allows this- 
one of them must succeed before the other. To introduce asymmetry, we use process 
numbers to totally order all processes. We classify messages into big messages and 
small messages. A message sent from a (bigger) process to a smaller process is a 

200 
CHAPTER 12. hZESSAGE ORDERING 
big message and a message from a (smaller) process to a bigger process is called a 
small message. We assume that processes do riot send messages to themselves. 
In our algorithm, a process can be in two states- active or passive. Every process 
is initially active. We first consider the algorithm for a big message. A process is 
allowed to send a message to a smaller process only when it is active. After sending 
the message, it turns passive unt,il it receives an ack message from the receiver of 
the message. While passive, it cannot send any other message, nor can it accept any 
other message. Note that the protocol for a message from a bigger process requires 
only one control message (ack). 
l o  send a message t,o a bigger process, say, Pj, process Pi first, needs permission 
from Pj. 
can request t,he permission at any time. Pj can grant permission only 
when it is active. Furt,herrnore, a.ft,er granting the permission, Pj turns passive 
until it receives the message for which it has granted the permission. Thus the 
prot,ocol for a message from a smaller process requires two control messages (request 
arid permission). The irnp1enientat)ion of synchronous ordering in Java is shown in 
Figure 12.9. 
To prove correctness of the algorithm, we show that one can assign timestamps 
to all messages such that the timestamps of messages are in increasing order for any 
process. Each process maintains a local variable c that serves as the timestamping 
function for messages. The rules used by the algorithm are: 
1. Tiniestamp proposal: When a process sends a big message, it increments c and 
sends c as the proposed timestamp with the message. For a small message, the 
timestamp is proposed in the permission message sent from the bigger process. 
Again, to make the proposal c is incremented and sent 
a proposal. Thus 
the proposal of the timestamp is made by the bigger process for both types of 
messages. Note that as soon as a proposal is made, the process turns passive 
and cannot make any further proposals. A process can propose a timestamp 
only if it, is active. 
2. Timestamp finalization: When a process receives a proposal for a timestamp 
t, it can finalize the timestamp only if it is active. The timestamp assigned 
to this message is rnaz(c + 1, t). This timest'amp is sent with the ack message 
or the app message, depending on whether the message is big or small. The 
new value of c is set to the finalized timestamp. When the proposer receives 
the final timeshmp of the message, it assigns that timestamp to the message 
and sets its own clock to the maximum of the timestamp received and its own 
timestamp. 
It is easy to verify that 

12.3. SYNCHRONOUS ORDERING 
201 
.. 
2 '. 
V X  
state : {active,passive} initially wtive; 
To send m t o  Pj, ( j  < i )  
enabled if (state = active): 
send 
to Pj 
state := passive; 
Upon receive 
from Pjv (j > 
send ack t o  Pj; 
enabled if (state = 
Upon receive ack: 
state := active; 
To send a message (rnessage-id,rn) t o  Pj, ( j  > 
send request(message-id) to  Pj; 
Upon receive request(message-id) from Pj, (j < i )  
enabled if (state = active): 
send permission(message-id) t o  Pj 
state := passive; 
Upon receive permission(message-id) from Pj, ( j  > 
enabled if (state = active): 
send m t o  Pj; 
Upon receive m from Pj, ( j  < 
state := active: 
Figure 12.8: The algorithm at Pi for synchronous ordering of messages 

202 
CHAPTER 12. MESSAGE ORDERING 
mport java . io . * ;  
public class SynchLinker extends Linker { 
final static int passive =- 0, active = 1; 
int s t a t e  = a c t i v e ;  
private boolean granted ; 
public SynchLinker ( S t r i n g  basename, int id , int numProc 
throws Exception { 
super ( basename , id , nnmProc ) ; 
1 
public synchronized void sendMsg( int destId , Strillg tag 
if ( d e s t I d  < inyId) { // bzg message 
waitForActive ( ) ;  
super. sendMsg( destId , ”app” , ” ” ); 
super. sendMsg( destId . t a g .  msg); 
s t a t e  = passive; 
1 else { // s m a l l  messaqe 
String msg) { 
. , .  
granted = false ; 
super. sendMsg( destId , ” r e q u e s t ” ,  ” ” ) ;  
while ( !  granted ) Util . mywait( this ): // w a i t  
p e r m i s s i o n  
super. scndMsg( destId , ”app” , ” ” ); 
super. sendMsg( destId , t a g ,  msg); 
1 
1 
1 
1 
1 
synchronized void turnArtive ( )  { 
st,ate = a c t i v e  ; natifyAl1 ( ) ;  
synchronized void waitForActive ( )  { 
while ( s t a t e  != active ) Util .rnyWait( t h i s ) ;  
synchronized void grant. ( )  { 
grant,ed = true; notifyAll (); 
public Msg receiveMsg (int fromId ) throws IOException { 
boolean done = false; 
Msg m = null; 
while (! done) { // app msg r e c e i v e d  
In = super. rcceiveMsg (fromId ) ;  
String tag = m. get,Tag ( ) ;  
if ( t a g .  equals ( ” a p p ” ) )  { 
waitForActive ( ) ;  
= super. receivehfsg ( fromId ) ;  
super. sendhlsg ( frornId , ” ack” , 
} else { // s m a l l  message 
m = super. receiveMsg ( fromId ) ;  
tiirnActivc ( ) ;  
1 
if (m. getSrcId ( )  > niyId) { // b i g  message 
” ’‘ ) ; 
done = true; 
} else if ( t a g .  equals ( ” a c k ” ) )  turnActive ( ) ;  
else if ( t a g .  equals ( ” r e q u e s t ” ) )  { 
waitForActive (); 
super. sendMsg ( fromId , ’‘ permission ” , ” ’‘ ) ; 
} else if ( t a g .  equals ( ” p e r n i i s s i o n ” ) )  grant ( ) ;  
1 return m: 
Figiire 12.9: The algorithm for synclironous ordering of messages 

12.4. TOTAL ORDER FOR MULTICAST MESSA4GES 
203 
1. No process decreases its clock, and each process increases its clock by at least 
one for successive messages. 
2. The send and receive points of a message have the same timestamp. 
Total Order for Multicast Messages 
For synchronous ordering, we had assumed t,hat messages were point-to-point. In 
applications where a message may be sent to multiple processes, it is often desirable 
that all messages be delivered in the same order at all processes. For example, 
consider a server that is replicated at mukiple sites for fault tolerance. If a client 
makes a request to the server, then all copies of the server should handle requests in 
the same order. The total ordering of messages can be formally specified as follows: 
For all messages z and y and all processes P and Q, if z is received at P before 
y, then y is not received before z at Q. (Total Order) 
We require that y not be received before 2, rather than that z be received before 
y, to address the case where z is not sent to Q. Observe that we do not require that 
a message be broadcast to all processes. 
In this section we discuss algorithms for the total ordering of messages. Observe 
that the property of total order of messages does not imply causal or even FIFO 
property of messages. Consider the case when P sends messages ml followed by m2. 
If all processes receive m2 before ml, then the total order is satisfied even though 
FIFO is not. If messages satisfy causal order in addition to the total order, then we 
will call this ordering of messages causal total order. 
The algorithms for ensuring total order are very similar to mutual exclusion algo- 
rithms. After all, mutual exclusion algorithms ensure that all accesses to the critical 
section form a total order. If we ensure that messages are received in the “critical 
section” order, then we are done. We now discuss centralized and distributed algo- 
rithms for causal total ordering of messages. 
12.4.1 Centralized Algorithm 
We first modify the centralized algorithm for mutual exclusion to guarantee causal 
total ordering of messages. We assume that channels between the coordinator pro- 
cess and other processes satisfy the FIFO property. A process that wants to multi- 
cast a message simply sends it to the coordinator. This step corresponds to request- 
ing the lock in the mutual exclusion algorithm. Furthermore, in that algorithm, the 
coordinator maintains a request queue, and whenever a request by a process be- 
comes eligible, it sends the lock to that process. In the algorithm for total ordering 
of messages, the coordinator will simply multicast the message corresponding to the 

204 
CHAPTER 12. MESSAGE ORDERING 
request instead of sending the lock. Since all multicast messages originate from the 
coordinator, and the channels are FIFO, the total-order property holds. 
In this centralized algorithm, the coordinator has to perform more work than the 
other nodes. One way to perform load balancing over time is by suitably rotating 
the responsibility of the coordinator among processes. This can be achieved through 
t,he use of a token. The token assigns sequence numbers to broadcasts, and messages 
are delivered only in this sequence order. 
12.4.2 
We modify Lamport’s algorithm for mutual exclusion to derive an algorithm for 
total ordering of messages. As in that algorithm, we assume FIFO ordering of 
messages. We also assume that a message is broadcast to all processes. To simulate 
multicast, a process can simply ignore a message that is not meant for it. Each 
process maintains a logical clock (used for timestamps) and a queue (used for storing 
undelivered messages). The algorithm is given by the following rules: 
Lamport’s Algorithm for Total Order 
0 To send a broadcast message, a process sends a timestamped message to all 
processes including itself. This step corresponds to requesting the critical 
sect,ion in the mutual exclusion algorithm. 
0 On receiving a broadcast message, the message and its timestamp are stored 
in the queue, and a tirriestarnped acknowledgment is returned. 
0 A process can deliver the message with the smallest t#irnestamp, t, in the 
request queue if it has received a message with timestamp greater than t from 
every other process. This step corresponds to executing the critical section for 
the mutual exclusion algorithm. 
In this algorit,hm, the total order of messages delivered is given by the logical clock 
of send events of the broadcast mcssages. 
12.4.3 Skeen’s Algorithm 
Lamport’s algorithm is wasteful when most messages are multicast and not broad- 
cast. Skeen’s algorithm requires messages proportional to the number of recipients 
of a message and not the total number of processes in the system. 
The distributed algorithm of Skeen also assumes that processes have access to 
Lamport’s logical clock. The algorithm is given by the following rules: 
0 To send a multicast message, a process sends a timestamped message to all 
the destination processes. 

12.5. PROBLEMS 
205 
0 On receiving a message, a process marks it as undeliverable and sends the 
value of the logical clock as the proposed timestamp to the initiator. 
0 When the initiator has received all the proposed timestamps, it takes the 
maximum of all proposals and assigns that timestamp as the final timestamp 
to that message. This value is sent to all the destinations. 
0 On receiving the final timestamp of a message, it is marked as deliverable. 
0 A deliverable message is delivered to the site if it has the smallest timestamp 
in the message queue. 
In this algorithm, the tot,al order of message delivery is given by the final timestamps 
of the messages. 
12.4.4 Application: Replicated State Machines 
Assume that we are interested in providing a fault-tolerant service in a distributed 
system. The service is expected to process requests and provide outputs. We would 
also like the service to tolerate up to faults where each fault corresponds to a crash 
of a processor. We can build such a service using t + 1 processors in a distributed 
system 
follows. We structure our service as a deterministic state machine. This 
means that if each nonfaulty processor starts in the same initial state and executes 
the requests in the same order, then each will produce the same output. Thus, by 
combining outputs of the collection, we can get a t fault-tolerant service. The key 
requirement for implementation is that all state machines process all requests in the 
same order. The total ordering of messages satisfies this propert,y. 
12.5 Problems 
12.1. Assume that you have replicated data for fault tolerance. Any file (or a record) 
may be replicated at more than one site. To avoid updating two copies of the 
data, assume that a token-based scheme is used. Any site possessing the token 
can update the file and broadcast the update to all sites that have that file. 
Show that if the communication is guaranteed to be causally ordered, then 
the scheme described above will ensure that all updates at all sites happen in 
the same order. 
12.2. Let M be the set of messages in a distribut,ed computation. Given a message 
we use x.s to denote t,he send event and Z.T to denote the receive event. 
We say that a computation is causally ordered if 
tix, y E M : (x.s -+ y.s) =+ -(y.r + z.r). 

206 
CHAPTER 12. MESSAGE ORDERING 
We say that a computation is m.ysteriously ordered if 
Vz,y E M : ( 5 . 5  -+ y . ~ )  + -(y.s + z.r). 
(a) Prove or disprove that every causally ordered computation is also myste- 
riously ordered. 
(b) Prove or disprove that every mysteriously ordered computation is also 
causally ordered. 
12.3. Show the relationship between conditions (Cl), (C2), and (C3) on message 
delivery of a system. 
where s1 and s 2  are sends of any two messages and 
and 7-2 are the corre- 
sponding receives. Note that a comput.ation satisfies a delivery condition if 
and only if the condition is true for all pairs of messages. 
12.4. Assume that all messages are broadcast messages. How can you simplify the 
algorithm for guaranteeing causal ordering of niessages under this condition? 
12.5. Consider a system of N + 1 processes {PO, PI,. . . , Piv} in which processes Pi 
through PN can only scnd mmsages to PO or receive messages from Po. Show 
that if all channels in the system are FIFO, then any computation on this 
system is causally ordered. 
12.6. In this chapter, we have used the happened-before model for modeling the 
dependency of one message to the other. Thus all messages within a process 
are totally ordered. For some applications, messages sent from a process may 
be independent. Give an algorithm to ensure causal ordering of messages when 
the send events from a single process do not form a total order. 
12.7. Suppose that a system is composed of iionoverlapping groups such that any 
communicat,ion outside t,he group is always through the group leader, that, is, 
only a group leader is permitted to send or receive messages outside the group. 
How will you exploit this structure to reducc the overhead in causal ordering 
of messages? 

12.6. BIBLIOGRAPHIC REMARKS 
207 
12.8. Design an algorithm for synchronous ordering for point-to-point messages that 
does not use a static priority scheme. (Hznt: Impose an acyclic directed graph 
on processes. The edge from P, to P3 means that P, is bigger than P3 for the 
purpose of sending messages. Give a rule by which the direction of edges is 
reversed, such that acyclicity of the graph is maintained.) 
12.9. Prove the correctness of Lamport’s algorithm for providing causal total order- 
ing of messages. 
12.10. Prove the correctness of Skeen’s algorithm for providing total ordering of mes- 
sages. 
12.11. Build a multiuser Chat application in Java that guarantees that all users see 
all niessages in the same order. 
12.6 Bibliographic Remarks 
Causal ordering was first proposed by Birman and Joseph [BJ87]. The algorithm for 
causal ordering described in this chapter is essentially the same 
that described by 
Raynal, Schiper, and Toueg [RST91]. The algorithm for implementing synchronous 
ordering is taken from a paper by Murty and Garg [MG95]. For a discussion on total 
ordering of messages, see the article by Birnian and Joseph [BJ87]. The distributed 
algorithm for causal total ordering of messages is implicit in the replicated state 
machine constriiction described by Lamport [Lam78]. Skeen’s algorithm is taken 
from the rc,ference [Ske82]. 

This Page Intentionally Left Blank

Chapter 13 
Leader Election 
13.1 Introduction 
Many distributed systems superimpose a logical ring topology on the underlying 
network to execute control functions. An important control function is that of 
electing a leader process. The leader can serve as a coordinator for centralized 
algorithms for problems such as mutual exclusion. Electing a leader in a ring can 
also be viewed as the problem of breaking symmetry in a system. For example, once 
a deadlock is detected in the form of a cycle, we may wish to remove one of the 
nodes in the cycle to rcmove the deadlock. This can be achieved by electing the 
leader. 
We abstract the leader election problem using the interface Election shown 
below. 
public interface Election extends MsgHaridler { 
void startElection ( ) ;  
int getLeader ( ) ; / / b l o c k s  t i l l  the l e a d e r  is 
1 
Any implementation of Election should provide the method startElection, 
which is invoked by one or more processes in the system. The method getLeader 
returns the identity of the leader. If the identity of the leader is not known, then 
this niethod blocks until the leader is electfed. 
The leader election problem is similar to the mutual exclusion problem discussed 
in Chapter 8. In both problems, we are interested in choosing one of the processes 
209 

210 
CHAPTER 13. LEADER ELECTION 
as a privileged process. Coordinat,or-based or token-based solutions for mutual ex- 
clusion are not applicable for the leader election problem, because deciding which 
process can serve as the coordinator or has the token is precisely the leader election 
problem. If processes have unique identifiers and the underlying communication 
network is completely connect.ed, then we can apply Lamport’s mutual exclusion 
algorit,hni to determine the leader-the 
first process to enter the critical section is 
deerlied as the leader. However, t,his algorithm requires every process to commu- 
nicat.e with every other process in the worst case. We will explore more efficient 
algorithms for the ring t.opo1og-y. 
13.2 Ring-Based Algorithms 
A ring is considered anonymous if processes in the ring do not. have unique identifiers. 
Furthermore, every process has an identical state machine with the same initial state. 
It, is not. difficult. to see that there is no deterministic algorithm for leader election 
in an anonyiiious ring. The reason is that we have cornplet,e symmetry initially--no 
process is distingiiisliable from other processes. Because there is a unique leader, 
we know that the system can never terminate in a symmetric state. Thus the 
algorithm has not terminated in the initial state. We now show an execution that 
moves t.he system from one symmetric stat,e to the other. Assume that any process 
in the ring takes a step. By symmetry, this step is possible at all processes. Thus 
in the adversarial execiit,ion all processes take t.he same step. Since the algorit,lim 
is deterniinist,ic, the system must again reach a symmetric state. Therefore, the 
syst,ern could not have terniiriat,cd (i.e., t,he leader could not have been elected yet,). 
We can repeat this procedure forever. 
Observe that our argument, uses the fact, that the algorithm is deterministic. A 
randomized algorithm can solve the leader election problcm in expected finite time 
(see Problem 13.1). 
13.2.1 Chang-Roberts Algorithm 
Now assunie that each process has a unique identifier. In such a system, a leader 
can be elected in a ring by a very simple algorithm due to Chang and Roberts. The 
algorithm ensures t,hat the process with the maximum identifier gets elected as the 
leader. In t,he algorit,hm shown in Figure 13.1, every process sends messages only 
to it,s left neighbor and receives messages from its right neighbor. A process can 
send an election iriessagc along wit#h its identifier to its left, if it 
not seen any 
message with a higher ident,ifier than its own identifier. It, also forwards any message 
that has an identifier greater than its own; otherwise, it swallows that message. If 
a process receives its own message, then it declares itself as the leader by sending 

13.2. RING-BASED ALGORITHMS 
211 
a leader message. When a process receives its own leader message, it knows that 
everybody knows the leader. 
In the algorithm, one or more processes may spontaneously wake up and initiate 
the election using the method startElection. When a process wakes up on receiv- 
ing a message from a process with a smaller identifier, it circulates its own election 
message. 
Note that the algorithm does not require any process to know the total number 
of processes in the system. 
public class RingLeader extends Process implements Election { 
int number; 
int leaderId = -1; 
int next; 
boolean awake = false; 
public RingLeader (Linker initComm, int number) { 
super (initComm) ; 
this . number = number ; 
next = (myId + 1) % N; 
I public synchronized int getLeader ( ) {  
while (1eaderId == -1) mywait(); 
return leaderId ; 
1 
public synchronized void handleMsg(Msg m, int src , String t a g )  { 
int j = m. getMessageInt ( ) ;  // get the number 
if ( t a g .  e q u a l s ( ” e 1 e c t i o n ” ) )  { 
if ( j  > number) 
else if ( j  == number) // I 
else if ( (  j < number) && !awake) s t a r t E l e c t i o n  ( ) ;  
leaderId = j ;  
notify ( ) ;  
if ( j  != myId) sendMsg(next, ” l e a d e r ” ,  j ) ;  
sendMsg ( n e x t  , ‘’ election ” , j ) ; // forward the message 
sendMsg (next , ” leader ” , myId) ; 
} else if ( t a g .  equals ( ”  leader” ) )  { 
1 
} public synchronized void s t a r t E l e c t i o n  () { 
awake = true; 
sendMsg(next, ” e l e c t i o n ” ,  number); 
I 
Figure 13.1: The leader election algorithm 

212 
CHAPTER 13. LEADER ELECTION 
8 
1 
Figure 13.2: Configurations for t,he worst case (a) and the best case (b) 
The worst. case of this algorithm is when N processes with identifiers 1 . . . N are 
arranged clockwise in decreasing order (see Figure 13.2(a)). The message inittiatled 
by process j will travel j processes before it is swallowed by a larger process. Thus 
the total number of election messages in the worst, case is 
j=ZN 
j = O(N2). 
In addition, t,here are N leader messages. The best case is when the same identifiers 
are arranged clockwise in the increasing order. In that case, only O ( N )  election 
rnessages are required. On an avera.ge, the algorithm requires O(N log N )  messages 
(sce Problem 13.2). 
13.2.2 Hirschberg-Sinclair Algorithm 
In this section we assunie that the ring is bidirectional so that messages can he sent 
to the left or the right neighbor. The main idea of the algorithm is to carry out 
elections on increasingly larger sets. The algorithm works in asynchronous rounds 
such that, a process P, tries to elect itself in round T .  Only processes that win 
their elcct,ion in round T can proceed to round T + 1. The invariant satisfied by the 
algorithm is t,hat process Pi is a leader in round T iff Pi has the largest identifier 
of all nodes that are at distraIice 2T or less from Pi. It follows that any two leaders 
after round r must be at least 2T distance apart. In other words, after round T ,  
there are at' most N/(2T-1 + 1) leaders. With each round, the number of lcaders 

13.3. ELECTION ON GENERAL GRAPHS 
213 
decreases, and in O(1ogN) rounds there is exactly one leader. It can be shown by 
using induction that there are at most O ( N )  messages per round, which gives us a 
bound of O(N1ogN). The details of the algorithm and its proof of correctness are 
left as exercises. 
13.3 Election on General Graphs 
First assume that the graph is completely connected, that is, every process can 
talk to every other process directly. In this case, we can modify Lamport’s mutual 
exclusion algorithm for leader election. One or more processes start the election. 
Any process that enters the critical section first is considered the leader. 
Note that a process need not acknowledge another process’s request if it knows 
that there is a request with a lower timestamp. Moreover, there is no need for release 
messages for the leader election problem. As soon 
a process enters the critical 
section, it can inform all other processes that it has won the election. If c processes 
start the election concurrently, then this algorithm takes at most 2cN messages for 
“request” and “acknowledgment,” and N messages for the final broadcast of who 
the leader is. 
Now consider the case when the graph is not completely connected. We assume 
that every process initially knows only the identities of its neighbors. In this case, 
we can simulate the broadcast from a node v by constructing a spanning tree rooted 
at 
13.3.1 Spanning 
Construction 
We assume that there is a distinguished process root. Later we will remove this 
assumption. The algorithm shown in Figure 13.3 is initiated by the root process by 
sending an invite message to all its neighbors. Whenever a process Pi receives an 
invite message (from Pj) for the first time, it sends that message to all its neighbors 
except Pj. To Pj it sends an accept message, indicating that Pj is the parent of Pi. 
If Pi receives an invite message from some other process thereafter, it simply replies 
with a reject message. Every node keeps a count of the number of nodes from which 
it 
received messages in the variable numreports. When this value reaches the 
total number of neighbors, Pi knows that it has heard from all processes that it had 
sent the invite message (all neighbors except the parent). At this point, Pi can be 
sure that it knows all its children and can halt. 
This algorithm is also called the flooding algorithm because it can be used to 
broadcast a message m, when there is no predefined spanning tree. The algorithm 
for flooding a message is simple. Whenever a process Pi receives a message m (from 
Pj) for the first time, it sends that message to all its neighbors except Pj. 

214 
CHAPTER 13. LEADER ELECTION 
public class SpanTree extends Process { 
public int parent = 
// no p a r e n t  y e t  
public IntLinkedList children = new IntLinkedList (); 
int numReports = 0; 
boolean done = false; 
public SpanTree ( Linker initComm, boolean isRoot ) { 
super (initcornni); 
if ( i s R o o t )  { 
parent = myId; 
if (iiiitCoinin. neighbors. size () == 0) 
else 
done = true; 
seiidToNeighbors ( ” i n v i t e ” ,  niyId); 
1 
1 
1 
public synchronized void waitForDone() { // b l o c k  t z l l  c h r l d r e n  k n o w n  
while ( ! done ) niyWait ( )  ; 
public synchronized void handleMsg(Msg in, int src , String t a g )  { 
if ( t a g . e q u a l s ( ” i n v i t e ” ) )  { 
if ( p a r e n t  == -1) { 
numReports++; 
parent = s r c ;  
seiidhlsg ( src , ” accept ’’ ) ; 
for ( i n t  i = 0; i < N; i++) 
if ( (  i != inyId) && ( i  != s r c )  && isNeighbor ( i ) )  
sendMsg ( i , ” i n v i t e  ” ) ; 
} else 
sendMsg( s r c ,  ” r e j e c t ” ) ;  
} else if ( (  t a g . e q u a l s ( ” a c c e p t ” ) )  1 1  ( t a g . e q u a l s ( ” r e j e c t ” ) ) )  { 
if ( t a g .  equals ( ” a c c e p t ” ) )  c h i l d r e n .  add( src ) ;  
riuniReports++; 
if (numReports == comni. neighbors. size ( ) )  { 
done = true; 
Figure 13.3: A spanning tree construction algorithm 

13.4. APPLICATION: COMPUTING GLOBAL FUNCTIONS 
215 
What if therc is no distinguished process? We assume that each process has 
a unique id, but initially every process knows only its own id. In this case, each 
node can start the spanning tree construction assuming that it is the distinguished 
process. Thus many instances of spanning tree construction may be active concur- 
rently. To distinguish these instances, all messages in the spanning tree started by 
Pi contain the id for 
By ensuring that only the instance started by the process 
with the largest id succeeds, we can build a spanning tree even when there is no 
distinguished process. The details of the algorithm are left 
an exercise. 
13.4 Application: Computing Global Functions 
One of the fundamental difficulties of distributed computing is that no process has 
access to the global state. This difficulty can be alleviated by developing mechanisms 
to compute functions of the global state. We call such funct,ions global fmctions. 
More concretely, assume that we have 
located at process Pi. Our aim is t,o 
compute a function 
, 2 2 , .  . . , 
that depends on states of all processes. 
First, we present an algorithm for convergecast and broadcast on a network, 
assuming that there is a predefined spanning tree. The convergecast requires infor- 
mation from all nodes of the tree to be collectcd at the root of the tree. Once all 
t,he information is present at the root node, it can compute the global function and 
then broadcast the value to all nodes in the tree. Both the convergecast and the 
broadcast require a spanning tree on the network. 
The algorithms for convergecast and broadcast are very simple if we assume a 
rooted spanning tree. For convergecast, the algorithm is shown in Figure 13.4. Each 
node in the spanning tree is responsible to report to its parent the information of 
its subtree. The variable parent, for a node 
is the identity of the neighbor of 
which is the parent in the rooted spanning tree. For the root, this value is null. The 
variable numchildren keeps track of the total number of its children, and nurnreports 
keeps track of the number of its children who have reported. When the root node 
hears from all its children, it has all the information needed to compute the global 
function. 
The broadcast algorithm shown in Figure 13.5 is dual of the convergecast al- 
gorithm. The algorithm is initiated by the root process by sending the broadcast 
message to all its children. In this algorithm, messages t.raverse down the txee. 
We now combine the convergecast and the broadcast algorithms to provide a 
service that, can compute a global function. For simplicity, we assume that the global 
function is commutative and associative, such as min, max, sum, and product. This 
allows internal nodes to send intermediate results to the parent node during the 
convergecast process. The 
interface is shown below. 

216 
CHAPTER 13. LEADER ELECTION 
parent: process id;// initialized based on the spanning tree 
rlurnchildren: integer; // initialized based on the spanning tree 
nvrnreports: integer initially 0; 
on receiving a report from PJ 
numreports := numreports + 1; 
if (nunzreports = 
then 
if (parent = null) then // root node 
else send report to parent; 
compute global function; 
endif; 
Figure 13.4: A convergecast algorithm 
Proot :: 
send m to all children; 
:: 2 # root 
on receiving a message 'rn from parent 
send rri to all children; 
Figiirr 13.5: A hroadcast algorithm 
public interface GlobalService extends MsgHandler { 
public void i n i t i a l i z e  (int x ,  FiincUser prog); 
public int compilteGloba1 ( ) ;  
} 
I 
Any program that wants to compute a global function can invoke 
with its value and the global function to be computed 
arguments. The 
is required to have a binary function called func 
shown below. 

13.5. PROBLEMS 
217 
public interface FuncUser { 
public int func(int 
int y ) ;  
1 
Now we can give an implenientation for 
based on the ideas of 
convergecast and broadcast. The Java implementation is shown in Figure 13.6. 
The program uses two types of messages, subTreeVa1 and globalFunc, for con- 
vergecast, and broadcast respectively. The list 
keeps track of all the children 
that have not reported using the srLbTree Val message. Whenever a subTree Val mes- 
sage is received, it is combined with 
using 
Whenever the 
list becomes empty, that node has the value of the global function for its 
subtree. If the node is a root, it can initiate the broadcast; otherwise it sends its 
to its parent and waits for the globalFunc message to arrive. The final 
answer is given by the value that comes with this message. 
The class 
can be used to compute a global function 
illustrated 
by the class 
in Figure 13.7. 
Problems 
13.1. An algorithm on a ring is considered nonunzform if every process knows the 
total number of processes in the ring. Show that there exists a randomized 
nonuniform algorithm to elect a leader on an anonymous ring that terminates 
with probability 1. (Hznt: Consider an algorithm with rounds in which initially 
all processes are eligible. In each round, an eligible process draws at random 
from 0. . . m (where m > 0). The subset of processes that draw the maximum 
element from the set selected is eligible for the next round. If there is exactly 
one eligible process, then the algorithm terminates. Analyze the expected 
number of rounds as a function of N and m.] 
13.2. Show that the Chang Roberts algorithni requires O(N1ogN) messages on 
average. 
13.3. Modify the Chang- Roberts algorithm such that a process keeps track of maxzd, 
thc largest identifier it 
seen so far. It swallows any message with any 
identifier that is smaller than maxzd. What are the worst and the expected 
number of messages for this variant of thc algorithm? 
13.4. Give an O(N log N )  algorithm for leader election on a bidirectional ring. 

218 
CHAPTER 13. LEADER ELECTION 
import j a v a .  ut,il . * ;  
public class GlobalFunc extends Process implements GlobalService { 
FuiicUser prog ; 
SpanTree t r e e  = null ; 
IiitLinkedList 
int rnyValue ; 
int answer; 
boolean answerRecvd ; 
boolean pendingset = false : 
public GlobalFuric ( Linker initconirn, boolean isRoot ) { 
pending = new I n t L i n k e d L i s t  ( ) ;  
super ( initconim ) ; 
t r e e  = new SpanTree(corrun, isRoot ) :  
1 public void i n  i t i a 1 i z c ( int inyVa1ue , FuncUser prog ) { 
t h i s .  myvalue = myvalue; 
t h i s .  prog = prog: 
t r e e  . waitForDone ( ) ; 
U t i l .  p r i n t l n  (niyId + ”:” + t r e e .  c h i l d r e n .  t o s t r i n g  
1 
public synchronized int corriputeGloba1 ( )  { 
pending. addAll ( t , r r e .  c h i l d r e n  ) ;  
pendingset = true; 
n o t i f y A l l  ( ) ;  
while ( ! pending. isEiiipty ( )  ) niyWait ( )  ; 
if ( t r c e .  parent == myId) { // root node 
answer = niyValue; 
} else { //non-root 
node 
sendMsg( t r e e  . parent, , ”subTreeVal” , myvalue); 
ariswerRecvd = false ; 
while ( !  answerRecvd ) iiiyWait ( )  ; 
} 
s e n d c h i l d r e n  (answer ): 
return answer ; 
} void s e n d c h i l d r e n  (int value ) { 
L i s t I t e r a t o r  t = t r e e .  c h i l d r e n .  I i s t I t e r a t o r  ( 0 ) ;  
while ( t . hasNext ( ) )  { 
I n t e g e r  child = ( I n t e g e r  ) t .  next ( ) ;  
sendMsg( c h i l d .  i n t V a l u e  ( ) ,  ” g l o b a l F u n c ” ,  value ) ;  
} 
} public synchronized void haiidleMsg(hlsg m, int s r c  , S t r i n g  t a g )  { 
t r e e .  handleMsg(n1, s r c  , t>ag); 
if ( t a g .  equals (”subTreeVa1”)) { 
while ( !  pendingset ) myWait ( )  ; 
pending. remove(new I n t e g e r  ( s r c  ) ) ;  
myvalue = prog . func (myvalue, m. getMessageInt ( ) )  ; 
if ( pending. isEmpty ( ) )  notifyA11 ( ) ;  
answer = m. getMessageInt ( ) ;  
answerRecvd = true ; 
notifyAl1 ( ) ;  
} else if ( t a g .  e q u a l s ( ” g 1 o b a l F u n c ” ) )  { 
} 
1 
1 
Figure 13.6: Algorithm for computing a global function 

13.6. 
REMARKS 
219 
3ublic class GlobalFuncTester implements FuncUser { 
public int func( int x ,  int y )  { 
return 
+ y ;  
1 
public s t a t i c  void main( String 
a r g s )  throws Exception { 
int myId = I n t e g e r .  parseInt ( a r g s  [ 11); 
int numProc = I n t e g e r .  parseInt ( a r g s  [ 2 ] ) ;  
Linker corrmi = new Linker ( a r g s  [ 01, myId, nuniProc); 
GlobalFunc g = new GlobalFunc(comm, (myId == 0 ) ) ;  
for ( int i = 0 ;  i < numProc; i++) 
if ( i  != myld) 
int myvalue = I n t e g e r .  parseInt ( a r g s  [:<I); 
GlobalFuncTester h = new GlobalFuncTester ( ) ;  
g .  i n i t i a l i z e  (myvalue, h ) ;  
int globalsum = g .  computeGloba1 (); 
System. o u t .  p r i n t l n  ("The global sum is " + globalSum); 
(new ListenerThread ( i  , g ) ) .  s t a r t  ( ) ;  
1 
t 
Figure 13.7: Computing the global sum 
13.6 Bibliographic Remarks 
The impossibility result on anonymous rings is due to Angluin [Ang80]. The O ( N 2 )  
algorithm is due to Chang and Roberts [CR79]. The O(N1ogN) algorithm dis- 
cussed in the chapter is due to Hirschberg and Sinclair [HS80]. Dolev, Klawe and 
Rodeh [DKR82] and Peterson [Pet821 have presented an O(N log N )  algorithm for 
unidirectional rings. For lower bounds of R(N log N ) ,  see papers by Burns [Bur801 
and Pachl, Korach, and Rotem [PKR82]. 

This Page Intentionally Left Blank

Chapter 14 
Synchronizers 
14.1 Introduction 
The design of distributed algorithms is easier if we assume that the underlying 
network is synchronous rather than asynchronous. A prime example is that of 
computing a breadth-first search (BFS) tree in a network. In this chapter, we 
assume that the network has N nodes, E edges, and its diameter is D. Assume that 
we are given a distinguished node u and our job is to build a breadth-first search 
tree rooted at 
A synchronous algorithm for this task is quite simple. We build 
the tree level by level. The node u is initially at level 0. A node at level i is required 
to send messages to its neighbors at pulse a. A process that receives one or more of 
these messages, and does not have a level number assigned yet, chooses the source 
of one of these messages 
it,s parent, and assigns itself level number + 1. It is clear 
that if the graph is connected, then every node will have its level number assigned 
in at most D pulses assuming that any message sent at pulse i is received at pulse 
i + l .  
What if the underlying network is not synchronous? The corresponding problem 
on an asynchronous network is more difficult. This motivates methods by which 
a synchronous network can be simulated by an asynchronous network. We show 
that, in the absence of failures, this is indeed possible using a mechanism called a 
synchronizer. To simulate the synchronous algorithm on an asynchronous network, 
all we need is to use one of the synchronizers discussed in this chapter. 
A synchronous network can be abstracted with the notion of a pulse, which is 
a counter at each process with the property that any message sent in pulse 
is 
received at pulse i + 1. A synchronizer is simply a mechanism that indicates to a 
221 

222 
CHAPTER 14. SYNCHRONIZERS 
- 
public interface Synchronizer extends MsgHandler { 
public void i n i t i a l i z e  (MsgHandler initProg ) ;  
public void sendMessage ( i n t  destId , String t a g ,  int msg); 
public void next,Pulse ( ) ;  // block f o r  the nest pulse 
1 
process when it can generate a pulse. In this chapter we will study synchronizers 
and t,heir complexity. 
To define properties of synchronizers formally, we associate a pulse number with 
each stat,e s on a process. It is initialized t,o 0 for all processes. A process can 
go from pulse i t,o + 1 only if it knows that it, has received and acted on all the 
messages sent during pulse i - 1. 
Given the notion of a pulse, t,he execution of a synchronous algorithm can be 
modeled as 
sequence of pulses. In each pulse, a process first receives messages 
from neighbors that were sent in previous round. It then performs internal compu- 
tation based on the received messages. It also sends messages to its neighbors 
required by the application. It can execute the next pulse only when indicated by 
the sync:hronizer. Thus a synchronizer can be abstracted by the following interface: 
There are t,wo aspects of the complexity of a synchronizer-the 
message com- 
plexity and the time complexity. The message complexity indicates the additional 
number of messages required by the synchronizer to simulate a synchronous algc- 
rithm on t,op of an asynchronous network. The time complexity is the number of 
time units required to simulate one pulse, where a time unit is defined as the time 
required for an asynchronous message. 
Some synchronizers have a nontrivial initialization cost. Let Minit be the num- 
ber of messages and 
be the time required for initialization of the synchronizer. 
Let 
and 
respectively be the number of messages and the time required 
to simulate one pulse of a synchronous algorithm. If a synchronous algorithm re- 
quires Tsynch rounds and Msynch messages, then the complexity of the asynchronous 
protocol based on the synchronizer is given by 
Masynch = Minit + Msynch + 
* Tsynch 
We model the topology of the underlying network as an undirected, connected 
graph. We assume that processes never fail. It is not possible to simulate a syn- 
chronous algorithm on an asynchronous network when processes can fail. In Chapter 

14.2. A SIMPLE SYNCHRONIZER 
223 
15, we show algorithms that can achieve consensus despite process failures in syn- 
chronous systems and that consensus is impossible in asynchronous systems when 
even a single process may fail. This implies that process failures cannot be toler- 
ated in simulating synchrony. We also assume that all channels are reliable. Again, 
Chapter 15 shows that the consensus problem is impossible to solve when channels 
are unreliable. 
14.2 A Simple Synchronizer 
A simple synchronizer can be built using a rule stipulating that every process send 
exactly one message to all neighbors in each pulse. With this rule, a process can 
simply wait for exactly one message from each of its neighbors. To implement this 
rule, even if the synchronous algorithm did not require Pi to send any message to 
its neighbor Pj in a particular round, it must still send a “null” message to Pj. 
Furthermore, if the synchronous algorithm required Pi to send multiple messages, 
then these messages must be packed as a single message and sent t.0 Pj. 
The simple synchronizer generates the next pulse for process p at pulse 
when 
it has received exactly one message sent during pulse i from each of its neighbors. 
The algorithm is shown in Figure 14.1 and its Java implementation, in Figure 14.2. 
3 ’. 
pulse: integer initially 0; 
round i : 
pulse := pulse + 1; 
wait for exactly one message with (pulse = i )  from each neighbors; 
simulate the round i of the synchronous algorithm; 
send messages to all neighbors with pulse; 
Figure 14.1: Algorithm for the simple synchronizer at Pj 
The algorithm in Figure 14.2 ensures that a process in pulse i receives only the 
The implementation in Java assumes FIFO and uses the following variables: 
messages sent in pulse - 1. 
0 
list of neighbors who have not been sent any message in this pulse 

224 
CHAPTER 14. SY NCHRONI ZEKS 
~~~~~ 
~ 
rnport j a v a .  u t i l  . LinkedList ; 
,ublic class SimpleSynch extends Process implements Synchronizer { 
int pulse = 0; 
MsgHandler prog ; 
boolean rcvEnabled 
; 
JntLinkedList pendings = new J n t L i n k e d L i s t  ( ) ;  
I n t L i n k e d L i s t  peiidingR = new IntLiiikedList ( ) ;  
public SimpleSynch ( Linker iriitcomm) { 
super ( initComm ) ; 
rcvEnabled = new boolean [ 
; 
for ( i n t  i = 0 ;  i < N; i++) 
rcvEnabled [ i ] = false ; 
1 
public synchronized void i n i t i a l i z e  ( MsgHandler i n i t P r o g  ) { 
prog = i i i i t P r o g ;  
peiidingS. addAll (co~nm. neighbors ) ;  
n o t i f y A l l  ( ) ;  
} public synchronized void handleMsg(Msg 111, 
int s r c  , S t r i n g  t a g )  { 
while ( !  rcvEnabled [ s r c  1 ) mywait ( ) ;  
pendingR. removeobject ( s r c  ); 
if ( prndingR. isEinpty ( ) )  n o t i f y A l l  ( ) ;  
if ( !  t a g .  equals ( ”  synchNul1“ ) )  
prog . IiandleMsg (m, s r c  , tag ) ; 
rcvEiiitbled [ s r c  ] = false ; 
1 
public synchronized void sendMessage (int destJd , Striiig t,ag, int nisg) { 
if ( pendingS. c o n t a i n s  ( d e s t l d  ) )  { 
pentiingS . renioveobject ( d e s t I d  ); 
seridMsg( destId , t a g !  nisg); 
System. e r r .  p r i n t l n  (’.Error : seiiding two m e s s a g e s / p u l s e ” ) ;  
} else 
} public synchronized void nextPiilse ( )  { 
while ( !  pendingS. isEinpty ( ) )  { // finish l a s t  p u l s e  by sending ~ ~ u l l  
int dest = pendings . renioveHead ( )  ; 
seiidMsg( dest , ”syricliNul1” , 0 ) ;  
1 
pulse ++; 
Util . p r i n t l n  ( ” * * * *  new pulse ****:” + pulse ) ;  
pentliiigS . addAll ( c o r n .  neighbors ) ;  
pendingR . addAll (comn. neighbors ); 
for ( i n t  i = 0; i < N; i++) 
iiotifyAl1 ( ) ;  
while ( ! pendingfl . isEmpty () ) myWait ( ) ; 
rcvEnabled [ i ] = true ; 
} 
1 
Figure 14.2: Implementmation of the simple synchronizer 

14.2. A SIMPLE SYNCHRONIZER 
225 
list of neighbors from which no message has been received in this 
pulse 
whether the process can receive a message from Pj in this 
round. 
The method 
sets 
and 
for all neighbors and the 
variable 
to 0. We have assumed that t,he communication topology is given 
by an undirected graph and that 
has the list of all neighbors. 
The method 
is implemented as follows. When a message is received 
at the application, it is determined whether any message has already been received 
from the source in the current, pulse. If there is such a message, then this message 
belongs to the next pulse and the process waits for rcvEnabled[src] to become 
true. Otherwise, this message is meant for this pulse and 
is removed from 
the list 
At this point, the tag of the message is checked to see if it is a null 
message (of type synchNull) used only for the synchronizer. If it is not, the message 
is passed on to the application. If a message hns been received in this pulse from 
each of the neighbors, that is, 
is empt,y, then the application can continue 
to the next pulse and the thread that may be blocked in 
is signaled. 
send a message, we simply remove t,he destination from the list 
Whenever t.he application layer calls 
t,he synchronizer first. ensures 
that, every neighbor is sent exactly one message in the last, pulse. After increnienting 
t,he pulse number, it waits to receive exactly one message from every neighbor. This 
is achieved bu waiting for t,he list 
to be empty. When this condit,ion 
becomes true, it is ready for the next pulse. 
Note t,hat there is no special requirement for initialization of this synchronizer. 
When any process starts pulse I, within D t,ime units all other processes will also 
start pulse 1. Therefore, the complexity of initializing the simple synchronizer is 
Because each pulse requires a rncssage along every link in both directions, we 
get the complexity of simulating a pulse as 
14.2.1 Application: BFS Tree Construction 
Let us use the simple synchronizer for computing the BFS tree in a network. Figure 
14.3 gives an algorithm that will compute the BFS tree on a synchronous network, 
but not necessarily the BFS tree on an asynchronous network. The algorithm has 

226 
CHAPTER 14. SYNCHRONIZERS 
t,wo methods: initiate and 
The method initiate is invoked by the 
root, from which we would like to compute the tree. In this method, the root sends 
an invite message to all its neighbors. Any node that receives an invite message 
for the first t.ime becomes part of the tree with its parent 
the node that sent 
the invitation. 
In an 
asynchronous net,work, this algorithm niay not produce a BFS tree. Figure 14.4 
gives an algorithm that runs wit,h the simple synchronizer to ensure that the tree 
cornputled is the BFS tree wen on asynchronous net.works. 
This node in turn sends invitaations to all its neighbors. 
Bublic class Tree extends Process { 
int parent = -1; 
int l e v e l ;  
public Tree (Linker iriitConim , boolean isRoot ) { 
super ( initConim ) ; 
if ( i s R o o t )  i n i t i a t e  ( ) ;  
1 
public void i n i t i a t e  ( )  { 
parent = myId; 
level = 0; 
for ( i n t  i = 0 ;  i < N; i++) 
if ( isNeiglibor ( i  ) )  
sendMsg( i , " i n v i t e " ,  level + 1 ) ;  
} public synchronized void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals ( " i n v i t e " ) )  { 
if (parent == - 1 )  { 
parent = s r c ;  
level = m. getMessageInt ( ) ;  
for ( i n t  i = 0 ;  i < N; i++) 
if ( isNeighbor ( i )  && ( i  != src ) )  
sendMsg(i, " i n v i t e " ,  level + 1 ) ;  
1 
1 
} 
~~~ 
~ 
~ 
~ 
~ 
Figure 14.3: An algorithm that generates a tree on an asynchronous network 
14.3 Synchronizer a 
The synchronizer u is very similar to the simple synchronizer. We cover this syn- 
chronizer because it is a special case of a more general synchronizer 
that will be 
covered later. All the synchronizers discussed from now on are based around the 

14.3. SYNCHRONIZER 
227 
)ublic class SynchBfsTree extends Process { 
int parent = -1; 
int l e v e l ;  
Synchronizer s ; 
boolean isRoot ; 
public SynchBfsTree ( Linker initComm, 
Synchronizer i n i t S  , boolean isRoot ) { 
super (initcomm) ; 
s = i n i t S ;  
this. isRoot = isRoot : 
I public void i n i t i a t e  ( )  { 
if ( i s R o o t )  { 
parent = myld; 
level = 0; 
s .  i n i t i a l i z e  ( t h i s ) ;  
for ( i n t  pulse = 0; pulse < N; pulse++) { 
1 
if ( (  pulse == 0) && isRoot ) { 
for ( i n t  i = 0; i < N; i++) 
if ( isNeighbor ( i  )) 
s . s e n d M e s s a g e ( i ,  ” i n v i t e ” ,  level + 1 ) ;  
} s .  nextpulse (); 
I 
I public void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  e q u a l s ( ” i n v i t e ” ) )  { 
if ( p a r e n t  == -1) { 
parent = s r c ;  
level = m. getMessageInt ( ) ;  
U t i l .  println (myId + ” is a t  level ” + level ); 
for ( i n t  i = 0; i < N; i++) 
if ( isNeighbor ( i )  && ( i  != src )) 
s . s e n d M e s s a g e ( i ,  ” i n v i t e ” ,  level + 1); 
} 
I 
I 
t 
Figure 14.4: BFS tree algorithm using a synchronizer 

228 
CHAPTER 14. SYNCHRONIZERS 
concept of 
of a process. Process P is safe for pulse 
if it, knows that all 
messages sent from P in pulse have been received. 
The 
synchronizer generates t,he next pulse at process P if all its neighbors are 
safe. This is because if all neighbors of P are safe, then all messages sent to process 
P have been received. 
To iniplernerit, the Q synchronizer, it is sufficient for every process to inform all 
its neighbors whenever it is safe for a pulse. How can a process determine whether 
it is safe? This is a simple matter if all messages are required to be acknowledged. 
The algorithm for 
synchronizer is given in Figure 14.5. We have assumed FIFO 
ordering of messages. The algorithm maintains a variable 
that records 
the niiiiiber of unacknowledged messages for the current pulse. It also maintains 
the list of neighbors that are unsafe for this node for the current pulse. At 
the beginning of each pulse, 
is initialized to 0 and 
to the list of 
all neighbors. 
The 
synchAck message acknowledges an application message and 
is decre- 
ment,ed whenever the synchAck message is received. The safe message is handled 
by removing the source of the message from the 
list. When an application 
message is received, it is checked whether a safe message has been received from that 
neighbor. Since a process sends safe messages only at the end of the pulse, if a safe 
message has already been received. then this message is meant for the next pulse and 
is recorded in 
as in 
Otherwise, an acknowledgment 
is sent arid t,he message is delivered to the application layer. 
The method 
is implemented as follows. First, the node waits for all 
pending acknowledgments. Once all acknowledgments are received, it knows that it 
is safe arid sends the safe message tmo all neighbors. It then waits for all its neighbors 
to be safe. When that condition bc:c:omes true, the node is ready for t,he next pulse. 
At, the beginning of the pulse all the messages iii 
are delivered. 
The sync:hronizer handles two types of messages: synchAck and sufe. 
The complexity of synchronizer a is given below: 
14.4 Synchronizer /3 
Although the synchronizers discussed so far appear to be efficient, they have high 
message coniplexity when the topology of the underlying network is dense. For 
large networks, where every node may be connected to a large number of nodts, it 
may be impractical to send a message to all neighbors in evrry pulse. The message 

14.4. SYNCHRONIZER 
229 
mport j a v a .  u t i l  . LinkedList ; 
Iublic class AlphaSynch extends Process implements Synchronizer { 
int pulse = 
int acksNeeded = 0; 
IntLinkedList unsafe = new IntLinkedList ( ) ;  
LinkedList nextPulseMsgs = new LinkedList ( ) ;  //msgs 
n e x t  p u l s e  
boolean meSafe ; 
MsgHandler prog ; 
public AlphaSynch ( Linker initComm) { 
super ( initComm ) ; 
1 
public synchronized void i n i t i a l i z e  (MsgHandler initProg ) { 
prog = i n i t P r o g ;  
s t a r t p u l s e  ( ) ;  
notifyAll ( ) ;  
1 
void s t a r t P u l s e  ( ) {  
unsafe . addAll ( c o r n .  neighbors ) ; 
meSafe = false; 
pulse ++; 
U t i l .  println ( ” * * * *  new pulse ****:” + pulse ) ;  
1 
public synchronized void handleMsg(Msg m, int src , String t a g )  { 
while ( pulse < 0) mywait(); 
if ( t a g .  equals (”syrichAck”)) { 
acksNeeded --; 
if (acksNeeded == 0) notifyAll ( ) ;  
} else if ( t a g .  equals ( ” s a f e ” ) )  { 
while (! unsafe. contains ( s r c  ) )  mywait(); 
unsafe. removeobject ( s r c  ) ;  
if ( unsafe. isEmpty ( ) )  notifyAll ( ) ;  
} else { // a p p l i c a t i o n  m e s s a g e  
sendMsg( src , ”synchAck” , 0 )  ; 
while ( !  unsafe . contains ( src ) )  mywait ( ) ;  
if ( mesafe) nextPulseMsgs . add(m); 
else prog . handleMsg (m, src , tag ) ; 
1 
1 
public synchronized void sendMessage (int destId , String t a g ,  int msg) { 
acksNeeded++; 
sendMsg( destId , t a g ,  msg); 
1 
public synchronized void nextpulse ( )  { 
while ( acksNeeded != 0 )  mywait(); 
meSafe = true; 
sendToNeighbors ( ”  safe ” , 0 )  ; 
while ( !  unsafe.isEmpty()) mywait(); 
s t a r t p u l s e  ( ) ;  
while ( !  nextPulseMsgs. isEmpty ( ) )  { / / a c t  on msgs r e c e i v e d  e a r l i e r  
Msg in = (Msg) nextPulseMsgs . removeFirst (); 
prog. handleMsg(m, m. getSrcId ( ) ,  m. getTag 
notifyAl1 ( ) ;  
1 
1 
Figure 14.5: Alpha synchronizer 

230 
CHAPTER 14. SYNCHRONIZERS 
complexity can be reduced at the expense of time complexity as illustrated by the 
/3 synchronizer. 
The /? synchronizer assumes the existence of a rooted spanning tree in the net- 
work. A node in the tree sends a message subtree 
when all nodes in its subtree 
are safe. When the root of the tree is safe and all its children are safe, then we can 
conclude that all nodes in the tree are safe. Now a simple broadcast of this fact via 
a pulse message can start the next pulse at all nodes. The broadcast can be done 
using tthe rooted spanning tree. 
The initializat,ion phase of this synchronizer requires a spanning tree to be built. 
This can be done using O ( N  log N + E )  messages and O ( N )  time. For each pulse, 
we require messages only along the spanning tree. Thus the message comp1exit)y for 
each pulse is O ( N ) .  Each pulse also takes t,inie proportional to the height of the 
spanning t,ree, which in the worst case is O ( N ) .  In summary, the complexity of the 
/3 synchronizer is 
hftznzt 
= O(N log N + E )  
Synchronizer 
We have seen that the 01 synchronizer takes 0(1) time but has high message com- 
plexity O ( E ) ,  and the p synchronizer has low message complexity O( N )  but, requires 
O ( N )  time per pulse. We now describe the 
synchronizer which is a generalization 
of both n and 
synchronizers. It takes a parameter k such that when k is N - 1, 
it reduces to the Q synchronizer arid when k is 2, it reduces to the p synchronizer. 
The 
synchronizer is based on clustering. In the initialization phase, the net- 
work is divided into clusters. Within each cluster the algorithm is similar to the 
/3 synchronizer and between clusters it is similar to the Q synchronizer. Thus each 
cluster has a cluster spanning tree. The root of the cluster spanning tree is called 
the cluster leader. We say that t,wo clusters are neighboring if there is an edge con- 
nect,ing t,hem. For any two neightioring clust,ers, we designate one of the edges as 
the preferred edge. 
The algorithm works as follows. There are two phases in each pulse. In both 
phases, the messages first, travel upward in the cluster tree and then travel downward. 
The goal of the first phase is to determine when the cluster is safe arid inform all 
clust,er nodes when it is so. In this phase, subtree 
messages first propagate up 
the clust,er tree. When the root of the cluster gets messages from all its children and 
it is safe itself, it propagates the cluster 
message down the cluster tree. This 
phase corresponds to the ,B synchronizer running on the cluster. We also require 

14.5. SYNCHRONIZER Y 
231 
that t,he nodes that are incident on preferred edges also send out our cluster safe 
(ocs) messages over preferred edges. 
The goal of the second phase is t,o determine whether all neighboring clusters are 
safe. In this sense, it is like an a synchronizer. It uses two additional message types: 
neighboring cluster safe (ncs) and pulse. When a leaf in the cluster t>ree receives 
the our cluster safe message from all preferred edges incident on it, it sends n m  to 
its parent. Now consider an internal node in the cluster tree that has received ncs 
messages from all its children and has received ocs on all preferred edges incident 
on it. If it is not the cluster leader, then it propagates the ncs message upward; 
otherwise, it broadcasts the pulse message in its group. 
For any clustering scheme c, let E, denote the number of tree edges and preferred 
edges and H, denote the maximum height of a tree in c. The complexity of the 
synchronizer is given by 
A&ulse = O(EC) 
Tpulse = O(&) 
We now show that any graph can be decomposed into clusters so that there 
is an appropriate tradeoff between the cluster height and the number of tree and 
preferred edges. In particular, we claim that for each k in the range 2 5 k < 
there exists a clustering c such that 
5 kN and H, 5 log N /  log k .  
We give an explicit, construction of t,he clustering. In this scheme, we add clust,ers 
one at a time. Assume that we have already constructed T clusters and there are 
still some nodes left that are not part of any cluster. We add the next cluster as 
follows. 
Each cluster C consists of multiple layers. For the first layer, any node that is 
not part of any cluster so far is chosen. Assume that layers 
I) of the cluster 
C have already been formed. Let S be the set of neighbors of the node in layer 
that are riot part of any cluster yet. If the size of S is at least ( k  - 1) times the size 
of C, then 
is added as the next layer of the cluster C ;  otherwise, C’s construction 
is finished. 
Let us compute Hc and Ec for this clustering scheme. Since each cluster with 
level has at least kip’ nodes, it follows that Hc is at most log N /  log k .  
has two 
components-tree 
edges and preferred edges. The tree edges are clearly at most N .  
To count the preferred edges, we charge a preferred edge between two clusters to 
the first cluster that is created in our construction process. Note that for a cluster 
C ,  its construction is finished only when there are at most ( k  - 1)jCI neighboring 
nodes. Thus, for the cluster C, there can be at most ( k  - 1)lC preferred edges 
charged to it. Adding up the contribution from all clusters, we get that the total 
number of preferred edges is at most ( k  - 1)N. 

232 
14. SYNCHftONIZEltS 
14.6 Problems 
14.1. Give the Java code for the p synchronizer. 
14.2. Give t,he Java code for the y synchronizer. 
14.3. What, is t,he message complexity of t,he asynchronous algorithm for construct- 
ing a breadt,h-first search tree when it is obtained by combining the syn- 
chronous algorithm with (a) the Q synchronizer, (b) the 
synchronizer, and 
(c) t,he y ( k )  synchronizer? 
14.4. Show how synchronizers can be used in a distribut>ed algorit,hni to solve a set 
of simultaneous equations by an iterative method. 
*14.5. (due to Awerbuch[Awe85]) Give a distributed algorithm t,o carry out, the clus- 
tering used by the y synchronizer. 
*14.6. (due to Luby[Lub85]) Let G = (V, E )  be an undirected graph corresponding to 
t,lie t,opology of a network. A set. V’ 
V is said to be independent if there is no 
edge between any two vertices in V’. An independent set is rnazimal if thew is 
no independent set that strictly contains V’. Give a distributed synchronoiis 
randomized algorithm that terminates in O(1og IV]) rounds. Also, give an 
algorit,hm that works on asynchronous networks. 
14.7 Bibliographic Remarks 
The concept of synchronizers, and the synchronizers a, /3, and 
were introduced 
by Awerbuch [Awe85]. The reader is referred l o  the books by Raynal and Helary 
[RHSO] and Tcl [TclS4] for more details on synchronizers. 

Chapter 
Agreement 
15.1 Introduction 
Consensus is a fundamental problem in distributed computing. Consider a dis- 
tributed dat,abase in which a transaction spans multiple sites. In this application it 
is important that either all sites agree to commit or all sites agree to abort the trans- 
action. In absence of failures, this is a simple task. We can use either a centralized 
scheme or a quorum-based scheme. What if processes can fail? It may appear that 
if links are reliable, the system should be able to tolerate at least failure of a single 
process. In t,his chapter, we show the surprising result that even in the presence of 
one unannounced process death, the consensus problem is impossible to solve. This 
result (FLP) is named after Fischer, Lynch and Paterson who first, discovered it. 
The FLP result for consensus shows a fundamental limitation of asynchronous 
computing. The problem itself is very basic-processes 
need to agree on a single 
bit. Most problems we have discussed such 
leader election, mutual exclusion, 
and computation of global functions are harder than the consensus problem because 
any solution to these problems can be used to solve the consensus problem. The 
inipossibility of consensm implies that all these problems are also impossible to solve 
in the presence of process failures. 
The FLP result is remarkable in another sense. It assumes only a mild form of 
failures in the environment. First, it assumes only process failures and not link fail- 
ures. Any message sent takes a finite but unbounded amount of time. Furthermore, 
it assumes that. a process fails only by crashing and thus ceasing all its activities. 
Thus it does not consider failures in which the process omits certain steps of the 
protocol or colludes with other processes to foil the protocol. Since the impossibility 
233 

234 
CHAPTER 15. ACREELIEN'T 
rcsrilt holds under weak rnodcls of failure, it is also true for stronger fidure models. 
15.2 Consensus in Asynchronous Systems (Impossibil- 
ity 1 
The consensus problem is as follows. We assume that there are N ,  ( N  
2) processes 
in t,he syst,ern and that. t,he value of N is known t,o all processes. Each process st,arts 
with itri iriit,ial value of {0,1}. This is modeled 
a one bit input register 5 .  A 
rionfault,y process decides by mitering a decision state. We require that some process 
evrrit,uitlly make a decision. Making a decision is modeled by output registers. Each 
proccss also 
an output, registc,r y that indicat,es the value decided or comniit>ted 
on by the process. The value of 0 in y indicat,es t,hat t.he process has decided on the 
v;zlur 0. Thc same holds for the value 1. The value 
indicates that the process has 
riot agreed on any value. Initmially, we require all processes to have 
in their register 
y. We require t,hiit once a process has decided, it, does not change its value, that is, 
out,put, regiskrs are write-once. We also assume t,hat each process has iinhounded 
st.oragc. 
e -receiv 
event i"' ) 
(b) 
Figure 15.1: (a) Comniutativit,y of disjoint events; (b) asynchrony of messages 
Before we list formal requirements of the consensus problem, we discuss our 
assumptions on the environment. 
Inatzal zndependence: We allow processes to choose their input in an indepen- 
dent manner. Thus, all input vectors are possible. 

15.2. CONSENSUS IN ASYNCHRONOUS SYSTEMS (IMPOSSIBILITY) 
0 Commute property of disjoint events: Let G be any global state such that two 
events e and f are enabled in it. If e and f are on different processes, then 
they commute. This is shown in Figure 15.1(a). We use the notation e(G) for 
the global state reached after executing event e at G. Similarly, we use s(G) 
to denote the global state reached after executing a sequence of events s at the 
global state G. The global state reached after executing 
at G is identical 
to the one reached after executing f e  at G. 
0 Asynchrony of events: The asynchronous message system is modeled 
a 
buffer with two operations. The operation send(p,m) by process p places 
(p, m) in the buffer. The operation receive() from p by any process q deletes 
(p, m) and returns m or returns null. The system may return null to model the 
fact that the asynchronous messages may take an unbounded amount of time. 
The condition we impose on the message system is that if the receive() event 
is performed an unbounded number of times, then every message is eventually 
delivered. The asynchrony assumption states that any receive event may be 
arbitrarily delayed. In Figure 15.1(b), t.he event e is an enabled event after 
executing a sequence of events s at state G because e @ s. Note that this 
assumption does not state that se(G) = es(G). The event e commutes with 
s only when the process on which e is executed is completely disjoint from 
processes that have events in s. 
Our model of a faulty process is 
follows. We only consider infinite runs. A 
faulty process is one that takes only a finite number of steps in that run. A run is 
admissible if at most one process is faulty. Since the message system is reliable, all 
messages sent to nonfaulty processes are eventually delivered. A run is deciding if 
some process reaches a decision state in that run. 
The requirements of the protocol can be summarized 
0 Agreement Two nonfaulty processes cannot commit on different values. 
0 Nontriviality: Both values 0 and 1 should be possible outcomes. This require- 
ment eliminates protocols that return a fixed value 0 or 1 independent of the 
initial input. 
0 Termination: A nonfaulty process decides in finite time. 
We now show the FLP result--there is no protocol that satisfies agreement, 
nontriviality, and termination in an asynchronous system in presence of one fault. 
The main idea behind the proof consists of showing that there exists an admissible 
run that, remains forever indecisive. Specifically, we show that (1) there is an initial 

236 
CHAPTER 15. AGREEMENT 
global state in which the system is indecisive, and (2) there exists a method to keep 
the system indecisive. 
To formalize the notion of indecision, we use the notion of valences of a global 
state. Let G.V be the set of decision values of global state reachable from G. Since 
the protocol is correct, G.V is nonempty. We say that G is bivalent if JG.VI = 2 
and univalent if JG.VJ = 1. In the latter case, we call G 0-valent if G.V = (0) and 
1-valent if G.V = (1). The bivalent state captures the notion of indecision. 
We first show that every consensus protocol has a bivalent initial global state. 
Assume, if possible, that the protocol does not have any bivalent initial global state. 
By the nontriviality requirenient, the protocol must have both 0-valent and 1-valent 
global states. Let us call two global states adjacent if they differ in the local state of 
exactly one process. Since any two initial global states can be connected by a chain 
of initial global states each adjacent to the next, there exist adjacent 0-valent and 
1-valent global states. Assume that they differ in the state of p. We now apply to 
both of these global states a sequence in which p takes no steps. Since they differ 
only in the state of p ,  the system mist reach the same decision value, which is a 
contradiction. 
Our next step is to show that we can keep the system in an indecisive state. Let 
G be a bivalent global state of a protocol. Let everit e on process p be applicable 
t,o G, and 6 be the set of global stat,es reachable from G without applying e. Let 
= e(6). We claim that ‘H contains a bivalent global state. Assume? if possible, 
that ‘H cont,ains no bivalent global states. We show a contradiction. 
We first claim that ‘H contains both 0-valent and 1-valent states. Let Ei (i 
{0..1}) be an i-valent global state reachable from G. If Ei E 6, then define Fi = 
e(E,). Ot,herwise, e was applied in reaching Ei. In this case, there exists Fi 
‘H 
from which Ei is reachable. Thus ‘H corit,ains bot,h 0-valent and 1-valent states. 
We call two global st,ates neighbors if one results from the other in a single step. 
We now claim that t,here exist neighbors Go, GI such that Ho = e(G0) is 0-valent, 
and 131 = e(G1) is I-valent. Let, t be the smallest, sequence of events applied to G 
without applying e such that, ef,(G) has different valency from e(G). To see that’ 
such a srquence exists, assunie t.li;tt, e(G) is 0-valent. From our earlier claim about 
there exist,s a global state in ‘H which is 1-valent. Let t be a minimal sequence 
that leads t,o a 1-valent state. The case when e(G) is 1-valent is similar. The last 
t,wo global stmates reached in this sequence give us the required neighbors. 
Without loss of generality let GI = f(GO), where 
is an event on process q. We 
now do a case analysis: 
Case 1: p is different from q [see Figure 15.2(a)] 
This implies that f is applicable to Ho, resulting in H I .  This is a contradiction 
because Ho is 0-valent, arid H I  is I-valent. 
Case 2: p = q [see Figure 15.2(b)] 

15.2. CONSENSUS IN ASYNCHRONOUS SYSTEMS (IMPOSSIBILITY) 
237 
Consider any finite deciding run from Go in which p takes no steps. Let s be the 
corresponding sequence. Let K = s(G0). From the commute property, s is also 
applicable to Hi and leads to i-valent global states Ei = s(Hi). Again, by the 
commute property, e ( K )  = EO and e ( f ( K ) )  = El. Hence K is bivalent, which is a 
contradiction. 
I 
I 
I 
I 
S
I
 
Figure 15.2: (a) Case 1: proc(e) # proc(f); (b) case 2: proc(e) = proc(f) 
The intuition behind the case analysis above is as follows. Any protocol that goes 
from a bivalent global state to a univalent state must have a critical step in which 
the decision is taken. This critical step cannot be based on the order of events done 
by different processes because execution of events at different processes conimutes. 
This observation corresponds to case 1. This implies that the critical step must be 
taken by one process. But this method also does not work because other processes 
cannot distinguish between the situation when this process is slow and the situation 
when the process has crashed. This observation corresponds to case 2. 
We are now ready for the main result that no consensus protocol satisfies agree- 
ment, nontriviality, and termination despite one fault. To show this result, we 
construct an admissible nondeciding run as follows. The run is started from any 
bivalent initial global state Go and consists of a sequence of stages. We ensure that 

238 
CHAPTER 15. AGREEMENT 
every stage begins from a bivalent global state G. We maintain a queue of processes 
and maintain the message buffer as a FIFO queue. In each stage the process p at 
the head of the queue receives the earliest message m (if any). Let e be the event 
corresponding to p receiving the message m. From our earlier claims, we know that 
there is a bivalent global state H reachable from G by a sequence in which e is 
the last event applied. We then move the process to the back of the queue and 
repeat this set of steps forever. This method guarantees that every process execut’es 
infinitely oft,en and every message sent is eventually received. Thus, the protocol 
stays in bivalent global states forever. 
15.3 Application: Terminating Reliable Broadcast 
Impossibility of consensus in the presence of a failure implies that niariy other inter- 
&ing problems are impossible to solve in asynchronous systems 
well. Consider, 
for exaniple, the problem of the terminating reliable broadcast specified as follows. 
Assume t,hat t,here are N processes in the system Po, . . . , P N - ~  
and that 1’0 wishes 
t,o broadcast a single message to all processes (including itself). ‘The terminating 
reliable broadcast’ requires t.hat8 a correct process always deliver a message even if 
the sender is faulty and crashes during the protocol. The message in that case may 
be “sender faulty.” The requirements of the problem are 
0 Termination: Every correct process eventually delivers some message. 
0 Validity: If the sender is correct and broadcasts a message m, then all correct 
processes eventually deliver m. 
0 Agreem,ent: If a correct, process delivers a message m, then all correct processes 
deliver m. 
0 Integrzty: Every correct process delivers at most one message, and if it delivers 
nz different from “scnder faulty,” then the sender must have broadcast m. 
We now show that the terminating reliable broadcast (TRB) is impossible to 
solve in an asynchronous system. We show this by providing an algorithm for con- 
sensus given an algorithm for TRB. The algorithm for consensus is simple. Process 
Po is required to broadcast its input bit using the TRB protocol. If a correct process 
receives a message different from “sender faulty” it decides on the bit received; other- 
wise, it, decides on 0. It is easy to verify that this algorithm satisfies the termination, 
agreement, and nontriviality requirements of the consensus problem. 

15.4. CONSENSUS IN SYNCHRONOUS SYSTEMS 
239 
Consensus in Synchronous Systems 
We have seen that consensus is impossible to solve in asynchronous systems even in 
the presence of a single crash. We show that the main difficulty in solving consensus 
lies in the synchrony assumption. Thus there exist protocols to solve consensus 
when the system is synchronous. A system is synchronous if there is an upper 
bourid on the message delay and on the duration of actions performed by processes. 
We show that under suitable conditions not only crash failures but also nialevolent 
faults in which faulty processes can send arbitrary messages can be tolerated by 
consensus algorithms. 
In general, we can classify the faults in a distributed system as follows: 
0 Crash In the crash model, a fault corresponds to a processor halting. When 
the processor halts, it does not perform any other action and stays halted 
forever. The processor does not perform any wrong operation such 
sending 
a corrupted message. As we have seen earlier, crashes may not be detectable 
by other processors in asynchronous systems, but they are detectable in syn- 
chronous systems. 
0 Crashtlink In this model, either a processor can crash or a link may go down. 
If a link goes down, then it stays down. When we consider link failures, it 
is somet,imes important t,o distinguish between two cases-one 
in which t.he 
network is partitioned and the second in which the underlying communication 
graph stays connected. When the network gets partitioned, some pairs of 
nodes can never communicate with each other. 
Omission: In this model, a processor fails either by sending only a proper 
subset of messages that it is required to send by the algorithm or by receiving 
only a proper subset of messages that have been sent to it. The fault of the 
first kind is called a send omission, and that of the second kind is called a 
receive omission. 
0 Byzantine failure: In this model, a processor fails by exhibiting arbitrary be- 
havior. This is an extreme form of a failure. A system that can tolerate a 
Byzantine fault can tolerate any other fault. 
In this chapter, we will consider only the crash and Byzantine failures. We 
assume that links are reliable for crash failures. A processor that is not faulty is 
called a correct processor. 

240 
CHAPTER 15. AGREEMENT 
15.4.1 Consensus under Crash Failures 
In this section, we will be concerned mainly with algorithms for synchronous systems. 
It, is generally advantageous to prove impossibility results with as weak a specifi- 
cation of t.he problem as possible because the same proof will hold for a stronger 
specification. However, when designing algorithms it is better for the algorithm t,o 
satisfy a st,rong specification because the same algorithm will work for all weaker 
specificat ions. 
We first generalize the set of values on which consensus is required. Instead of 
a single bit, the set of values can be any t,otally ordered set. We will assume that 
each process Pi has 
its input, a value 
from this set. The goal of the protocol is 
t,o set the value y at each process such that t,he following constraints are met. The 
value y can be set, at most once and is called the value decided by the process. Thus 
the requirements of the protocol are: 
0 Agreement: Two nonfaulty processes cannot decide on different values. 
0 Vulzdity: If all processes propose the same value, then the decided value should 
be that proposed value. It is easy to verify that this condition implies the 
Iiontriviality condition discussed in Section 15.2. 
0 Termination: A nonfaulty process decides in finite time. 
An algorit,hni for achieving consensus in the presence of crash failures is quitme 
simple. In the algorit,hm we use the parameter f to denote the maximum number of 
processors that can fail. The algorithm shown in Figure 15.3 works based 011 rounds. 
Each process maintains V ,  which contains the set of values that it knows have been 
proposed by processors in the system. Initially, a process 
knows only the value 
it, proposed. The algorit,hm takes f + 1 roilrids for completion; thus the algorithm 
assumes that the value of f is known. In each round, a process sends to all other 
processes, values from V that it, has riot sent before. So, initially the process sends 
it,s own vdue and in later rounds it sends only those values that, it learns for the 
first time in the previous round. In each round, the processor Pi also receives the 
values sent by Pj. In t.his st,ep, we liave used the synchrony assumption. Pi waits for 
a message from Pj for some fixed predetermined time after which it assumes that 
PI 
c:r<a.sheti. After f + 1 rounds, each process decides on the minimum value in 
it,s set, V .  
‘rhc algorit.lim present,ed above satisfies krmination because each correct process 
t#erniiiiat,es in exactly f + 1 rounds. It satisfies validity because the decided value 
is chosen from the set V ,  which contains only the proposed values. We show the 
agreement. property: All correct processors decide on the same value. 

15.4. CONSENSUS IN SYNCHRONOUS SYSTEMS 
24 1 
V :  set of values initially 
for k := 1 t o  + 1 do 
send 
E V I Pi has not already sent 
t o  all; 
receive Sj from all processes Pj,j # 
:= u sj; 
endfor; 
y := min(V); 
Figure 15.3: Algorithm at Pi for consensus under crash failures 
Let V, denote the set of values at Pi after the round f + 1. We show that if any 
value x is in the set V, for some correct processor Pi, then it is also in the set Vj for 
any other correct processor Pj. 
First assume that the value 
was added to V, in 
round k < f +  1. Since Pi and 
Pj are correct processes, Pj will receive that value in round k + 1 and will therefore 
be present in V, after round f + 1. 
Now assume that the value x was added to V, in the last round (round number 
f + 1). This implies that there exists a chain of f + 1 distinct processors that 
transferred the value from one of the processors that had z 
its input value to Pi. 
If all the processors in the chain are faulty, then we contradict the assumption that 
there are at most f faulty processors. If any processor in the chain is nonfaulty, 
then it would have succeeded in sending 5 to all the processors in that round. Hence 
x was also added to 
by at most f + 1 rounds. 
The preceding algorithm requires O((f + 1)N2) messages because each round 
requires every process to send a message to all other processes. If each value requires 
b bits, then the total number of communication bits is O(bN3) bits because each 
processor may relay up to N values to all other processors. 
The implementation of the above algorithm in Java is given in Figure 15.4. In 
this implementation we require every process to simply maintain the smallest value 
that it knows in the variable myvalue. The variable changed records whether the 
minimum value it knows changed in the last round. The process broadcasts its value 
only if changed is true. 
A simple program that calls the Consensus object is shown in Figure 15.5. 

242 
CHAPTER 15. AGREEMENT 
mport j a v a .  u t i l  . * ;  
)ublic class Consensus extends Process { 
int myvalue : 
int f ;  // mammurn nuniher of f a u l t s  
boolean changed = true; 
boolean 1iasProposed = false ; 
public Consensus ( Linker initComm , int f ) { 
super (iiiitConim) ; 
t h i s . f  = f ;  
1 
public synchronized void propose (int value ) { 
iriyValuc = value ; 
1iasProposed = true ; 
notify ( ) ;  
} public int decide ( )  { 
for ( i n t  k = 0 ;  k <= f :  k + + )  { // f + l  roun,ds 
synchronized ( t h i s )  { 
if ( changed ) broadcastMsg ( ”  proposal ’’ , myvalue) ; 
// s l e e p  enough t o  r e c e i v e  messages 
t h i s  round 
Util . rnySleep( Svinbols. rourid‘lhie); 
1 
1 
synchronized ( t h i s )  { 
return lilyvalue; 
1 
I public synchronized void haiidleMsg (hlsg in, int s r c  
while ( ! hasproposed ) mywait, ( ) ; 
if ( t a g .  equals ( “  proposal” ) )  { 
int value = in. getMessageInt ( ) ;  
if ( value < myvalue) { 
myValiie = value ; 
changed = true ; 
changed = false ; 
} else 
1 
String t a g )  { 
Figure 15.4: Consensus in a synchronous environment 

15.4. CONSENSUS IN SYNCHRONOUS SYSTEMS 
public class ConsensusTester { 
public static void main( String [ I  args) throws Exception { 
String baseName = args [ O ] ;  
int myId = Integer. parseInt (args [ 11); 
int numProc = Integer. parseInt (args [ 2 ] ) ;  
Linker c o r n  = new Linker (baseName , myId, numProc); 
Consensus sp = new Consensus(comm, 3 ) ;  
for ( i n t  i = 0 ;  i < numProc; i++) 
if ( i != myId) (new ListenerThread ( i  , s p ) ) .  start ( ) ;  
sp. propose (myId); 
System. o u t .  println (”The value decided :” + sp. decide 
I 
I 
Figure 15.5: Consensus tester 
15.4.2 Consensus under Byzantine Faults 
Byzantine fau1t.s allow for malicious behavior by the processes. The consensus prob- 
lem in this model can be understood in the context of the Byzantine General Agree- 
ment problem, which is defined 
follows. There were N Byzantine generals who 
were called out to repel the attack by a Turkish Sult,an. These generals camped near 
the Turkish army. Each of the N Byzantine generals had a preference for whether 
to attack the Turkish army or to retreat. The Byzantine armies were strong enough 
that t,he loyal generals of the armies knew that if their actions were coordinated 
(either attack or retreat), then they would be able to resist the Sultan’s army. The 
problem was that some of the generals were treacherous and would try to foil any 
protocol that loyal generals might devise for the coordination of the attack. They 
might, for example, send conflicting messages to different generals, and might even 
collude to mislead loyal generals. The Byzantine General Agreement (BGA) prob- 
lem requires us to design a protocol by which the loyal generals can coordinate their 
actions. It is assumed that generals can communicate with each other using reliable 
messengers. 
The BGA problem can easily be seen 
the consensus problem in a distributed 
system under Byzantine faults. We call a protocol f-resilient if it can tolerate f 
Byzantine faulty processors. It has been shown t.hat there is no f-resilient protocol 
for BGA if N 
3 f .  
In this section we give an algorithm that takes f + 1 rounds, each round of two 
phases, to solve the BGA problem. This algorithm uses constant-size messages but 
requires that N > 4f. Each processor has a preference for each round, which is 
initially its input value. 

244 
CHAPTER 15. AGREEhIENT 
The algorithm is shown in Figure 15.6. The algorithm is based on the idea of 
a rot,ating coordinator (or king). Processor Pi is assumed to be the coordinat,or or 
the king for roilrid k. In the first phase of a round, each processor exchanges its 
value with all other processors. Based on its V vector, it determines it,s estimat,e in 
the variable myvalue. In the second phase, t.he processor receives the value from 
the coordinator. If it receives no value (because the coordinator has failed), then it, 
assumes 7
~
1
 
(a default value) for the king value. Now, it decides whether to use its 
own value or the kingualue. This decision is based on the multiplicity of myvalue 
in bkie vector V .  If V has more than N/2 + f copies of myvalue, then myuulue is 
chosen for V[i]; 
otherwise, kingvalue is used. 
We first, show that agreement persish, that is, if all correct processors prefer a 
value u at the beginning of a round, then thcy continue to do so at t,he end of a 
round. This property holds because 
N > 4f 
N - N/2 > 
= N - f > N / 2 + f .  
Since the number of correct. processors is at, least N - f ,  each correct, processor will 
receive niore t,han N/2 + f copies of 
and hence choose that at the end of second 
phase. 
We now show that the algorithm in Figure 15.6 solves the agreement problem. 
The va1idit.y property follows from the persistence of agreement. If all processors 
st,art wit,h the same value 
t,fien 7) is the value decided. Termination is obvious 
because the algorithm takes exactly + 1 rounds. We now show the agreement 
propert,y. Since there are + 1 rounds and at most 
faulty processors, at, least one 
of the rounds has a correct king. Each correct processor decides on either t,he value 
sent, by the king in that round or its own value. It chooses its own value w only 
if its multiplicity in V is at least N/2 + + 1. Therefore, the king of that round 
must have at, least N/2 + 1 multiplicit,y of w in its vector. Thus the value chosen by 
t,he king is also w. Hence, each processor decides on the same value at the end of a 
round in which t,he king is nonfaulty. From persistence of agreement, the agreement 
property at, the end of t,l-ie algorithm follows. 
15.5 
Knowledge and Common Knowledge 
Many problems in a distributed system arise from the lack of global knowledge. 
By sending and receiving messages, processes increase the knowledge they have 
about the system. However, there is a limit to the level of knowledge that can be 
attained. We use the notion of knowledge to prove some fundamental results about 
distributed systems. In particular, we show that agreement is impossible to achieve 

15.5. KNOWLEDGE AND COMMON KNOWLEDGE 
245 
mport java. u t i l  . * ;  
>ublic class KingBGA extends Process { 
final s t a t i c  int defaultvalue = 0; 
int f ;  // maxamum number of f a u l t s  
int V [ ] ;  // s r t  of v a l u e s  
int kingvalue , myvalue; 
public KingBGA(Linker initComm, int f )  { 
super ( initComrn ) ; 
t h i s .  f = f ;  
V =  new int[N]; 
1 
public synchronized void propose ( i n t  val ) { 
for ( i n t  i = 0 ;  i < N; i + + )  V[ i ]  = defaultvalue ; 
V[myId] = v a l ;  
public int decide ( )  { 
1 
for ( int k = 0 ;  k <= f ;  k + + )  { // f + 1  rounds 
broadcastMsg ( ” p h a s e l ”  , V[myId]); 
Util . mysleep ( Symbols. roiindTime); 
synchronized ( t h i s )  { 
niyValue = getMajority (V); 
i f  ( k  == niyId) 
broadcastMsg ( ”  king” , myvalue) ; 
1 
U t il . rnySleep ( Symbols. roundTime ) ; 
synchronized ( t h i s )  { 
if ( nuinCopies(V, myvalue) > N / 2 + f )  
else 
myId] = myvalue ; 
V[ myId] = kingvalue ; 
1 
I return V[ rnyId ] ; 
1 
public synchronized void handleMsg (Msg m, int src , String tag ) { 
if ( t a g .  cquals ( ” p h a s e l ” ) )  { 
} else if ( t a g .  equals ( ” k i n g ”  ) )  { 
V[ s r c ]  = m. getMessageInt ( ) ;  
kingvalue = m. getMessageInt ( ) ;  
1 
} int g e t M a j o r i t y ( i n t  V[]) { 
return 0; 
return 1; 
return defaultvalue ; 
if (numCopies(V, 0 )  > N / 2 )  
else if (nuniCopies(V, 1) > N / 2 )  
else 
1 int numCopies(int V [ ] ,  int v )  { 
int count = 0; 
for ( i n t  i = 0 ;  i < V. l e n g t h ;  i++) 
return count ; 
if 
i ]  == v )  count++; 
I 
t 
Figure 15.6: An algorithm for Byzantine General Agreement 

246 
CHAPTER 15. AGREEMENT 
in an asynchronous system in the absence of reliable communication. 
The riotion of knowledge is also useful in proving lower bounds on the rues- 
sage comp1exit)y of distributed algorithms. In particular, knowledge about remote 
processes can be gained in an asynchronous distributed system only by message 
For exarriple, consider t,he mutual exclusion problem. It is clear that if 
process Pi enters the critical section and later process Pj enters the crit,ical section, 
then there must be sonie knowledge gained by process 1% before it can begin c:at,irig. 
‘rhis gain of kriowledge can happen only through a message transfer. Observe t,hat, 
iinipt,ion of asynchrony is criicial in requiring the message transfer. In a syn- 
n with a global clock, the knowledge can indeed be gained simply by 
. Thus for a mutual rxclusion algorithm, one may have time-division 
multiplexing in which processes enter the critical sect,ion on t,heir preassigned slots. 
‘rhus mutual exclusion can be achieved without, any message transfers. 
t,ern. We use Ki(b) to denok t’hat the 
proccss 
in t,he group G knows the pretlicat,e 6. We will assume that a process can 
know only trur prctlicat.cs: 
Let G be a group of processes in a 
Ki(6) * b 
The converse riiay iiot be true. A predicate b niay be true, but, it, niay not be 
known t,o process 
For example, let, b be t,hat, t,here is a deadlock in t,he syst.em. It 
is quit,(: possiblr t,liat b is true hut, process 
does not, know about, it,. 
Now, it, is easy t.o tlc!firie t,l-ie riieaning of “someone in the group knows b,” denot,ed 
t,y S(h). as follows: 
S(b) Sf v KZ(6) 
iEG 
Similarly, WP d t h e  “evc,ryoiie in tlin group knows b,” denoted by E(b), 
It, is iniport,;tnt, to realize t,hat S(b) arid E(b) are also predicates--in any syst,ern 
st&! t,licy evaluate to true or false. Thus it makes perfect sense to use E(6) or S(6) 
for a predic:at,e. In p a r t i d a r ,  E(E(b)) means that, everyone in the group knows that 
everyolie in the group knows 
This otxxrvation allows 11s to d&ne Ek((b), for I; 1 0, inductively its follows: 
Eo(b) = b 
E“” 
(b) = E(E’(6)) 
It, is important. t,o realize that, although 

15.5. KNOWLEDGE AND COMMON KNOWLEDGE 
247 
the converse does not hold in general. To appreciate this fact, consider the following 
scenario. Assume that there are n 
1 children who have gone out to play. These 
children were told before they went for play, that they should not get dirty. However, 
children being children, k 
1 of the children have dirty foreheads. Now assume 
that the children stand in a circle such that every child can see everyone else but 
cannot see his or her own forehead. Now consider the following predicate b: 
def 
b = there is at least one child with a dirty forehead 
In this case E”’(0) is true but Ek(b) is not. For concreteness, let n be 10 and 
k be 2. It is clear t,hat since k is 2, b is true. Furthermore, since every child can 
see at least one other child with a dirty forehead, E(b) is also true. Is E2(b) true? 
Consider a child, say, child 
with a dirty forehead. That child can see exactly one 
other child, say, child j ,  with a dirty forehead. So from child 
perspective, there 
may be just one child, namely, child j ,  who has a dirty forehead. However, in that 
case child j would not know that b is true. Thus Ki(E(b)) does not hold; therefore, 
E2(b) is also false. 
The next higher level of knowledge, called common knowledge and denoted by 
C(b), is defined as 
C(b) ef Qk : Ek(b). 
It is clear that for any k ,  
C(b) =+ E’C(b). 
In the example of the children with a dirty forehead, assume that one of the 
parents walks to the children and announces “At least one of you has a dirty fore- 
head.” Every child hears the announcement. Not only that; they also know t.hat 
everybody else heard the announcement. Furthermore, every child knows that every 
other child also knows this. We could go on like that. In short, by announcing b, 
the level of knowledge has become C(b). 
Now, assume that t,he parent repeatedly asks the question: “Can anyone prove 
that his or her own forehead is dirty?” Assuming that all children can make all 
logical conclusions and they reply simult.aneously, it can be easily shown using in- 
duction that all children reply “No” to the first k - 1 questions and all the children 
with a dirty forehead reply “Yes” to the kth question (see Problem 15.11). 
To understand t,he role of conimon knowledge, consider the scenario when k 
2. 
At first, it may seem that the statement made by the parent “At least one of you 
has a dirty forehead.” does not add any knowledge because every child can see at 
least one other child with a dirty forehead and thus already knew b. But this is 
not true. To appreciate this the reader should also consider a variant in which tht: 
parent repeatedly asks: “Can anyone prove that his or her own forehead is dirty?” 

248 
CHAPTER 15. AGREEMENT 
witshout first announcing b. In this case, the children will never answer “Yes.” By 
announcing b, the parent gives conimon knowledge of b and therefore Ek(b). Ek(b) 
is required for the children to answer “Yes” in the kth round. 
15.6 Application: Two-General Problem 
We now prove a fundarnent,al result about common knowledge- it cannot be gained 
in a dist,ributed syst,em with unreliable messages. We explain the significance of t,he 
result in t,he context of t,he coordinating general problem under unreliable commu- 
nication. Assume t,hat there are two generals who need to coordinate an at,tack on 
t,he enemy army. The armies of tlie generals are camped on tlie hills surrounding 
a valley, which has the eneniy army. Both the generals would like to at,tack the 
enemy army simukaneously because each general’s army is outnumbered by the en- 
<:my army. They had no agreed-on plan beforehand, and on some night they would 
like t.o coordinate with each other so that both attack the enemy the next day. The 
generals are assumed to behave correct,ly, but. the communication between them is 
unreliable. Any messenger sent from one general to the other may be caught by the 
enemy. The question is whether taliere exists a prot,ocol that allows the generals to 
agree on a single bit denoting attack or retreat. 
It is clear tjliat in the presence of urirelirtble messages no protocol can guarant,cv 
ag~eerrierit for ull runs. None of the messagrs sent by any general may rcach t.he 
other side. The real quest.ion is whether there is some protocol that can guarantee 
agreement for ,some run (for example, when some messages reach their destination). 
Unfortunately, even in a siniple distributed system with just t,wo processors, P and 
Q ,  that communicate with unreliable messages, there is no protocol that allows 
common knowledge to be gained in any of its run. 
If not, let T be a run with the smallest number of messages that achieves common 
knowledge. Let rn be t,he last, message in the run. Assume without. loss of generalit,y 
that t,he last message was sent, from the processor P to processor Q. Since messages 
are unreliable, processor P does riot. know whet,her Q received the nirssage. Thus, 
if P can assert C(b) aft,er rn messages, then it, can also do so after m - 1 messages. 
But C(6) at P also implies C(b) at Q. Thus C(b) is true after m - 1 messages, 
violating rninimality of tlie run T .  
In cont,rast,, t,he lower levels of knowledge are attainable. Indeed, to go from S(b) 
t,o E(b), it, is sufficient for the processor with the knowledge of b to send messages to 
all ot’her processors indicating the truthness of b. In the run in which all messages 
reach their destination, E(b) will hold. The reader should also verify that E2(b) will 
not hold for any run after the protocol. The reader is asked to design a prot,ocol 
that guarantees E2(b) from S(b) in one of its runs in Problem 15.12. 

15.7. PROBLEMS 
249 
Problems 
15.1. Why does the following algorithm not work for consensus under FLP assunip- 
tions? Give a scenario under which the algorithm fails. It is common knowl- 
edge that there are six processes in the system numbered Po to Ps. The algo- 
rithm is 
follows: Every process sends its input bit to all processes (including 
itself) and waits for five messages Every process decides on the majority of 
the five bits receivcd. 
15.2. Show that all the following problems are impossible to solve in an asynchronous 
system in the presence of a single failure. 
(a) Leader Electzon: Show that the special case when the leader can be only 
from the set {Po, Pi} is equivalent to consensus. 
(b) Computatzon of a global functzon: Show that a deterministic nontrivial 
global function such as man, 
and addition can be used to solve 
consensus. 
15.3. Atomic broadcast requires the following properties. 
0 Vulidity: If the sender is correct and broadcasts a message m, then all 
correct processes eventually deliver m. 
0 Agreement If a correct process delivers a message m, then all correct 
processes deliver m. 
0 Integrity: For any message m, q receives m from p at most once anti ody 
0 Order: All correct processes receive all broadcast messages in the same 
if p sent m to q. 
order. 
Show that atomic broadcast is impossible to solve in asynchronous systems. 
*15.4. (due to Fischer, Lynch and Paterson[FLP85]) Show that if it is known that 
processes will not die during the protocol, then consensus can be reached 
(despite some initially dead processes). 
*15.5. Give a randomized algorithm that achieves consensus in an asynchronous dis- 
tributed system in the presence off crash failures under the assumption that 
N > 2 f + 1 .  
15.6. Show by an example that if the consensus algorithm decided the final value 
after f rounds instead of f + 1 rounds, then it might vio1at.e the agreement 
property. 

250 
CHAPTER 15. AGREEhIENT 
15.7. Give an example of an execution of a system with six processes, two of which 
are faulty in which the Byzantine general agreement algorithm does not work 
correctly. 
15.8. Give an algorit,hm that solves BGA problem whenever N 
+ 1. 
*15.9. [due to Dolev and Strong[DS83]] In the Byzantine failure niodel a faulty pro- 
cess could forward incorrect information about niessages received from other 
processes. A less nialevolent model is called Byzantine failure with mutual 
authentication. In this model, we assume that a message can be signed digi- 
tally for authentication. There exist many cryptographic algorithms for digital 
signatures. Give an algorithm for Byzantine General Agreement assuming au- 
thentication that is f-resilient for 
< N, requires only f + 1 rounds, arid uses 
a polynomial nuniber of niessages. 
*15.10. Show t,hat t,he number of rounds required to solve consensus under t,he c:rash 
riiodcl is at least f + 1 in the worst case when f 5 N - 2. 
15.11. Show using induckion on k t,hat when the parent repeatedly asks the quest,ion 
all children reply “NO” to the first k - 1 questions and all the children with a 
dirty forehead reply “Yes” to t,he kt,h question. 
15.12. Design a protocol t,hat, guarant,ees E2(b) from S(b) in one of its runs. 
15.13. Consider a ganie in which two people are asked to guess each other’s number. 
They are told that they have consecutive nat.ural nunibers. For example, the 
person with nuniber 50 can deduce that, the other person has either 51 or 
49. Now they are repeatedly asked in turn “Can you tell the other person’s 
nuniber?” Will any of thein ever be able t,o answer in the affirmative? If yes, 
how? If no, why not,? 
15.8 Bibliographic Remarks 
’The t,hcory of the consensus problcm and its generalizations is quit,e well developed. 
We have covered only the very basic ideas from the literature. The reader will 
find many results in t,he book by Lynch [Lyn96]. The impossibility of achieving 
consc’nsus in asynchronous system is duc to Fischer, Lynch, and Paterson [FLP85]. 
The consensus problem with Byzant,ine fault,s 
first introduced arid solved by 
Lamport, Shost,ak and Pease [LSP82, PSLSO]. The lower bound on the nuniber of 
hounds needed for solving the problem under Byzantine faults was given by Fischer 
arid Lynch [FL82] and under crash failures by Dolev and Strong [DS83]. 

15.8. BIBLIOGRAPHIC REMARKS 
The discussion of knowledge and common knowledge is taken from a paper by 
The “two-generals problem” was first described by 
Halpern and Moses[HM84]. 
[Gra78]. 

This Page Intentionally Left Blank

Chapter 16 
Transact ions 
16.1 Introduction 
The concept of a transaction has been very useful in allowing concurrent processing 
of data with consistent results. A transaction is a sequence of operations such that 
that entire sequence appears as one indivisible operation. For any observer, it ap- 
pears as if the entire sequence has been executed or none of the operations in the 
sequence have been executed. This property of indivisibility is preserved by a trans- 
action despite the presence of concurrency and failures. By concurrencg, we mean 
that multiple transactions may be going on at the same time. We are guaranteed 
that the transactions do not interfere with each other. The concurrent execution 
of multiple transactions is equivalent to a serial execution of those transactions. 
Further, if the transaction has not committed and there is failure, then everything 
should be restored to appear as if none of the operations of the transaction hap- 
pened. If the transaction has committed, then the results of the transaction must 
become permanent even when there are failures. 
As an example of a transaction, consider transfer of money from account A to 
account B. The transaction, say TI can be written as 
begin-transaction 
withdraw 
from account A; 
deposit 
to account B; 
end-transact ion 
Once the t,wo operations of withdrawing and depositing have been grouped as 

254 
16. TRANSACTIONS 
a transaction, they become atomic to the rest of the world. Assume, for example, 
that another transaction Tz is executed concurrently with this transaction. T2 simply 
adds t,he balances in accounts A and B. The semantics of t,he transact,ion guarantees 
that, T2 will not, add the balance of account A after the withdrawal and the balance of 
account B before the deposit. The all-or-nothing property is also guaranteed when 
t.here is a failure. If the second operation cannot be performed for some reason 
(such 
account B does not exist.), t,hen the effects of the first operation are also 
not visible, i.e., the balance is restored in account A. 
A t,ransaction can be viewed 
implementing the following primitives: 
1. begin-transaction: This denotes the beginning of a transaction. 
2. end-transaction: This denotes the end of a transaction. All the statements 
between the beginning and the end of t.he transaction constitute the transac- 
tion. Execution of this primitive results in committing the transaction, and 
its effects must persist. 
3. abort-transaction: The user has the ability to call abort-transaction in 
the middle of a transaction. This requires t,hat all values prior to the transac- 
tjion are restored. 
4. read: Within a tmnsaction, t,he program can read objects. 
5. write: Within a transaction, the program can also write objects. 
16.2 ACID Properties 
Sometimes the guarantees provided by a transaction are called ACID properties, 
where ACID stands for atomicity, consistency, isolation, and durabilit,y. These terms 
are explained next. 
0 Atomicity: This property refers to all-or-nothing property explained earlier. 
0 Consisten,cy: A transaction should not violate integrity constraints of the sys- 
tem. A typical example is that of a financial system where the transaction of 
money transfer from one account to the other should keep the total amount 
of money in the system constant. It is the responsibility of the programmer 
writing the transaction to ensure that such constraints are not violated after 
t,he transaction has taken place. 
0 Isolation: This means that transactions are isolated from effects of concurrent 
transactions. Thus, in spite of concurrency, the net effect is that it appears 
that all transactions executed sequentially in some order. 

16.3. CONCURRENCY CONTROL 
255 
0 Durability: This property refers to the aspect of committing a transaction. 
It says that once a transaction has been committed its effects must become 
permanent even if t,here are failures. 
16.3 Concurrency Control 
The isolation property of transaction is also called the seraalizability condition. A 
concurrent history H of transactions T I ,  T2, . . . , 
is serializable if it is equivalent 
to a serial history. As an example, suppose that there are two transactions: TI 
and T2. 
transfers $100 from account A to account B and T2 transfers $200 from 
account B to account C. Assuming that each account has $1000 initially, the final 
balances for A, B, and C should be $900, $900 and $1200, respectively. TI could be 
implemented as follows: 
= 
= 
= 
= x+ioo; 
could be implemented 
follows: 
= 
= 
y = y-200; 
y = y+200; 
It is clear that all concurrent histories are not serializable. In the example above, 
assume that the following history happened: 
= 
= x-100; 
= 

256 
CHAPTER 16. TRANSACTIONS 
y = 
y = 
y 
y = y-200; 
y = y+200; 
= 
In t,his case, t,he final values would be 900> 1100, and 1200, which is clearly wrong. 
One way to ensure serializability would be by locking the entire &abase whcm a 
transaction is in progress. However, this will allow only serial histories. 
A niorc’ practical technique frequentsly employed is called two-phase locking. In 
this technique, a transaction is viewed 
consisting of two phases: the locking 
pliase arid the unlocking phase. In the locking phase (sometimes called a “growing” 
phase), a transact.ion can only lock d&a items arid in the unlocking phase (sometimes 
called a “shrinking” phase) it, can only unlock them. With this kchnique, the 
irnplenient,ation of TI would he 
; 
= 
= 
; 
= 
= 
; 
; 
16.4 Dealing with Failures 
There are primarily t,wo techniques used for dealing with failures c:allrd primte 
rriorkspuc:r. arid logging. In t.he privak workspace approach, a transaction does not 
c:hange the original primary copy. Any object, t.liat is affected by the transaction is 
kept in a separat,e copy called a shadow copy. If t:he t,ransaction abort,s, t,hen private 
or shadow copies are discarded and nothing is lost. If the transaction c:onimit,s, 
then all the sliadow copies bccorne the primary copies. Note that this technique is 

16.5. DISTRIBIJTED COMMIT 
257 
different from the technique for nonblocking synchronization, which we studied in 
Chapter 5 .  In the private workspace approach, we do not make copy of the entire 
database before a transaction is started. Instead, a copy of only those objects (or 
pages) are made that have been updated by the transaction. This technique can be 
implemented as follows. Assume that object>s can be accessed only through pointers 
in the index t,able. Let S be primary index table of objects. At the beginning of a 
transaction, a copy of this table, S’, is made and all read and write operations go 
through S’. Since reads do not, change the object, both S and S’ point to the same 
copy, and thus all the read operations still go to the primary copy. For a write, 
a new copy of that object is made and the pointer in the table S’ is changed to 
the updated version. If the transaction aborts, then the table S’ can be discarded; 
otherwise, S’ becomes the primary table. Observe that this scheme requires locking 
of the objects for transactions to be serializable. 
In the logging scheme, all the updates are performed on a single copy. However, 
a trail of all the writes are kept, so that in case of a failure, one can go to the log and 
undo all the operations. For example, if an operation changed the value of object. 
from 5 to 3, then in the log it is maintained that 
is changed from 5 to 3. If the 
transaction has to abort, then it is easy to undo this operation. 
16.5 Distributed Commit 
When a transaction spans multiple sites, we require that either all sites commit the 
transaction or all abort it. This problem is called the distributed commit problem. 
The problem is, of course, quite simple when there are no failures. In this section, 
we address how to solve the problem when processes may fail. We assume that links 
are reliable. 
The requirements of the problem are 
follows: 
0 Agreement: No two processes (failed or unfailed) decide on different outcome 
of the transaction. 
0 Validity: If any process starts with abort, then abort is the only possible final 
outcome. If all processes start with commit and there are no failures, then 
commit is the only possible outcome. 
0 Weak termination: If there are no failures, then all processes eventually decide. 
0 Non-blocking: All nonfaulty processes eventually decide. 
We now give a two-phase commit protocol that satisfies the first t,hree conditions. 
The steps in the protocol are: 

258 
CHAPTER 16. TRANSACTIONS 
0 The coordinator sends a request message to all participants. 
0 On receiving a request message, each participant replies with either a “yes” or 
a “no“ message. A “yes” message signifies that t,he participant can commit all 
the actions performed at its sitme. This finishes the first phase of the algorithm. 
0 The coordinator waits to receive messages from all participants. If all of t,hem 
are “yes,” then the coordinat,or sends the finalCommit message. Otherwise, it 
sends a finmlAbort message. 
The participant carries out tlhe act,ion associated with the message received 
from t,he coordinator. 
Thus there are t,wo phases: t,he voting phase and the decision phase. In the 
voting phase, the coordinator collects all the votes arid in the decision phase it 
cornrnunicat.es t,he final decision to all the participants. 
The algorithm for the coordinator is shown in Figure 16.1. The coordinator 
invokes t,he method 
to carry out the protocol. In the first phase, 
the coordinat,or waits until the flag 
beconies tnie. This flag becomes 
t,rue if all participants have replied with “yes” messages or when one of the partici- 
pant has replied wit,h a “no” mcssage. These messages are handled in the niethod 
which make call to 
i f  y 0  appropriately. 
The algorit,hni for a participant is shown in Figure 16.2. A participant irnple- 
rnents t,he consensus interface wit,h t,he methods 
and 
When a 
participant invokes 
it is blocked until it gets a finalCommit or a finalAbort 
message from the coordinat,or. We have not shown the actions that the coordinator 
and participants need to take when they tirneout waiting for messages. We have 
also not shown the actions t.hat processes nerd t,o take on recovering from a crash. 
This part. is left as an exercise for the readcr (see Problem 16.6). 
Let, 11s now analyze the protocol from the perspective of the coordinator. If it 
does not hear from any of t,he participants in the first phase, t,hen it can abort, t,hc 
entire transaction. Therefore, if a participant fails before it sends out its vote in 
the first phase, the failure is easily handled. What if the participant fails after it 
has sent, out its vote as conimit? Since the transaction may have commit,ted, when 
the process recovers, it niust find out the state of t2he transaction and conimit all 
t,he chaxiges. This observat,ion iniplies that a part,iciparit can send a “yes” message 
only if it can make all t,he changes for committing a trarisact>ion despite a fault. In 
ot,lier words, the part,icipant must have logged onto its stable st,oragc the necessary 
iriforrnet,ion required to commit, the t.ransact,ion. 
Let us now analyze t8he protocol from the perspective of a participant. Initially 
it. is expecting a request message t,hat may not, arrive for the predetermined timeout 

16.5. DISTRIBUTED COMMIT 
public class TwoPhaseCoord extends Process { 
boolean globalCommit = false ; 
boolean donephase1 = false ; 
boolean noReceived = false ; 
int numparticipants ; 
int numReplies = 0 ;  
public TwoPhaseCoord ( Linker iiiitComm) { 
super ( initComm ) ; 
numparticipants = N - 1; 
1 public synchronized 
void doCoordinator () { 
// Phase 1 
broadcastMsg ( ”  request ” , myId); 
while ( !  donephase1 ) 
I 
myWait ( )  ; 
// Phase 2 
if ( noReceived ) 
else { 
broadcastMsg(”fina1Abort” , myId); 
globalCommit = true ; 
broadcastMsg (”finalCommit” , myld); 
1 
1 public synchronized void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals ( ” y e s ” ) )  { 
numReplies ++; 
if ( nurnReplies == numparticipants ) { 
donephase1 = true ; 
notify ( ) ;  
1 
} else if ( t a g .  equals ( ” n o ” ) )  { 
259 
Figure 16.1: Algorithm for the coordinator of the two-phase commit protocol 

260 
CHAPTER 16. TRANSACTIONS 
public class Tu.oPhaseParticipaiit extends Process { 
boolean IocalComniit ; 
boolean globalCorrimit ; 
boolean done = false; 
boolean hasProposed = false ; 
public TwoPliasrParticipant (Linker initComm) { 
super ( initConim ) ; 
I public synchronized void propose (boolean v o t e  ) { 
localcommit = v o t e ;  
hasproposed = true; 
notify ( ) ;  
I 
public synchronized boolean decide ( )  { 
while ( ! done ) mywait ( )  : 
return globalCoinmit ; 
I public synchronized void handleblsg(Msg in, int src , String t a g )  { 
while ( ! hasProposed ) inyWait ( )  ; 
if ( t a g .  e q u a l s ( ” r e q u e s t ” ) )  { 
if ( IocalCoinmit) 
sendMsg ( src , ’‘ yes” ) ; 
else 
sendMsg ( src , ’’ no’’ ) ; 
globalCoininit = true ; 
done = true; 
not,ify ( ) ;  
globalConimit = false : 
done = true; 
iiot,ify ( ) ;  
} else if ( t a g .  equals ( ”  finalConiiriit” ) )  { 
} else if ( t a g .  e q u a l s ( ” f i n a l A b o r t ” ) )  { 
1 
} 
I 
Figure 16.2: Algorit,hm for t,he part,icipants in the two-phase commit protocol 

16.6. PROBLEMS 
261 
interval. In this case, the participant can simply send a “no” message to the coor- 
dinator and can assume that the global transaction had aborted. The coordinator 
can also crash in the second phase. What if the participant has replied with a “yes” 
message in the first phase and does not hear from the coordinator in the second 
phase? In this case, it does not know whether the coordinator had committed the 
global t,ransaction. In this case, the participant should inquire other participants 
about the final decision. If the coordinator crashed after sending finalcommit or 
finalAbort message to any participant who does not crash, then all participants will 
learn about the final decision. However, this still leaves out the case when the 
coordinator crashed before informing any participant (or the participants that it 
informed also crashed). In this case, all the participants have no choice but to wait 
for the coordinat,or to recover. This is the reason why two-phase commit protocol 
is called blocking. 
16.6 Problems 
16.1. A contracted two-phase locking scheme is one in which the second phase is 
contracted, that is, all locks are unlocked at the same time. What are the 
advant,ages and disadvantages of this scheme compared with the ordinary two- 
phase locking scheme ? 
16.2. Write a class that provides the following services:(a) lock(String varname; 
i n t  pid); returns 1 if allowed by two-phase locking scheme, ret,urns 0 other- 
wise, and (b) unlock(String varname, i n t  pid); returns 1 if locked by the 
process, returns 0 otherwise. Assume that t.he processor never crashes. 
16.3. Which of the following schedules are serializable ? 
(a) TI (a, b); w1 (b); ~ 2 ( a ) ;  ( a ) ;  ,ul2(a). 
(b) ~i (a, b); w (a); ~ 2 ( a ) ;  
~ 1 2 ( a ) ;  
~1 (b). 
(c) ~2 (a); ri (a, b); ~ 1 2  
(c) ; 
(a); 
W I  (c). 
(4 T1 (a), T 2 ( b ) ,  w1 ( a ) ,  
U12 (b), T1 (b), ‘Iu1 (b), .2(c), u2(c) 
For each of the serializable schedules, show a possible two-phase locking liis- 
tory. 
16.4. Assume that you have two floats representing checking balance and savings 
balance stored on disk. Write a program that transfers $100 from the check- 
ing account to the savings account. Assume that the processor can crash at 
anytime but disks are error-free. This means that you will also have to write 
a crash recovery procedure. You are given the following primitives: 

262 
CHAPTER 16. TRANSACTIONS 
class Stable { 
// t o  copy disk object val t o  memory object 
use 
// t o  copy memory object 
t o  disk object V a l ,  use 
3 
float Val; 
synchronized void get (f loat 
synchronized void set (f loat 
16.5. Explain why the two-phase conimit protocol does not violate the FLP impos- 
sibility result. 
16.6. Complete the code for the participants (Figure 16.2) and the coordinator (Fig- 
ure 16.1) by specifying actions on timeout and crash recovery. 
16.7 Bibliographic Remarks 
‘The reader is referred to t,he book by Gray and Reuter [GR93] for a comprehensive 
treatment, of transactions. 

Chapter 17 
Recovery 
17.1 Introduction 
In this chapter, we study methods for fault tolerance using checkpointing. A check- 
point can be local to a process or global in the system. A global checkpoint is simply 
a global state that is stored on the stable storage so that in the event of a failure the 
entire system can be rolled back to the global checkpoint and restarted. To record 
a global state, one could employ methods presented in Chapter 9. These methods, 
called coordinated checkpointing, can be efficiently implemented. However, there 
are two major disadvantages of using coordinated checkpoints: 
1. There is the overhead of computing a global snapshot. When a coordinated 
checkpoint is taken, processes are forced to take their local checkpoints when- 
ever the algorithm for coordinated checkpoint requires it. It is better for this 
decision to be local because then a process is free to take its local checkpoint 
whenever it is idle or the size of its state is small. 
2. In case of a failure, the entire system is required to roll back. In particular, 
even those processes that never communicated with the process t.hat failed 
are also required to roll back. This results in wasted computation and slow 
recovery. 
An alternative method is to let processes take their local checkpoints at their 
will. During a failure-free mode of computation, this will result in an overhead on 
computation lower than that for coordinated checkpointing. In case of a failure, 
a suitable set of local checkpoints is chosen to form a global checkpoint. Observe 
263 

264 
CtIAPlER 17. RECOVERY 
t,liat processes that have not failcd have t,heir current states available, and t.hose 
st,ates can also serve as checkpoints. There are some disadvantages of uncoordi- 
nated checkpointing compared wit,h coordinat.ed checkpointing schemes. First, for 
coordiiiated checkpointing it is sufficieiit. tlo keep just the niost recent global snap- 
shot in the st,able st,orage. For uncoordinated checkpoints a more complex garbage 
co1lec:tion schrnic, is requircd. Moreover, in the case of a failure t,he 
metshod 
for c:oordiiiated checkpointing is simpler. There is no need to compute a c:onsistorit 
global ch:ckpoint. Finally, but, rriost importantly, simple uncoordinat,ed clieckpoirit- 
ing docs not. giiaraiitee any progress. If local checkpoint,s are takrri at inopportiiiie 
t,irnes! t,ho only consist,cnt global state rnay be the initial one. This problem is called 
t,he dom,ino eflect, and an example is shown in Figure 17.1. Assume that process PI 
crashes and t,lierefore niust, roll back to c 1 , 1 ,  its last checkpoint. Because a message 
was sent. between c1,1 and c1,2 t,hats is received before 02.2, process P2 is in an incon- 
sist,ent, st,at,e at c2,2 wit,h respect, t,o 
Therefore, P2 rolls back to c 2 , ~ .  But. t,his 
forces 
to roll back. Continuing in this manner, we find t,hat, t.he only consist,ent, 
global clieckpoiiit, is t,he initial one. Rolling back t,o t.he init,ial global checkpoint, 
rcsiilt,s in wasting the erit ire coniput,at,ion. 
I 
I 
p2 I 
p3 I 
c2.0 
5, 
I I 
“3, 
c3. I I 
Figurc 17.1: An example of t.he domino effect 
A hybrid of t,he completely coordinated arid the completely iincoordinat,ed schenies 
is called 
checkponoiat.in,g. In this method, processes are free to 
t,ake t,heir 1oc:al checkpoints whenever desired, but on the basis of the cornniunication 
pat,tern. they may be forced to t,ake additional local checkpoints. These met,hotls 
guarantee that recovery will not suffer from the domino effect. 
The characteristics of t,he application, the probability of failures, and t,echnolog- 
i(x1 fact,ors rnay dictate which of the above nient,ioned choices of checkpointing is 
best. for a given sit,uation. In this chapter, we will st.udy issues in uncoordinated 
checkpointing and communication-induced checkpointing. 

17.2. ZIGZAG RELATION 
265 
17.2 Zigzag Relation 
Consider any distributed coniputation with N processes P I , .  . . , PN. Each process 
Pi checkpoints its local state at some intermittent interval, giving rise to a sequence 
of local checkpoints denoted by 
We will assume that the initial state and the final 
state in any process are checkpointed. For any checkpoint c we denote by pred.c, 
the predecessor of the checkpoint c in the sequence 
whenever it exists, that is, 
when c is not the initial checkpoint. Similarly, we use succ.c for the successor of the 
checkpoint c. 
Given a set of local checkpoints, X ,  we say that X is consistent iff Vc, d E X : clld. 
A set of local checkpoints is called global if it contains N checkpoints, one from each 
process. 
Let the set of all local checkpoints be S: 
s = usi. 
We first tackle the problem of finding a global checkpoint that contains a given set 
of checkpoints X 
S. A relation called zigzag precedes, which is weaker (bigger) 
than +, is useful in analysis of such problems. 
Definition 17.1 T h e  relation zigzag precedes, denoted by 
is the smallest relation 
that satisfies 
(Z1) 
(Z2) 
c + d implies c 
d .  
3 e  E S : (c + e )  A (pred.e 
d )  implies c 
d. 
The following property of the zigzag relation is easy to show: 
(c f e) A (pred.e 
d )  implies (c f d). 
On the basis of this relation, we say that a set of local checkpoints X is z-consistent 
iff Vc,d E X : c $  d. 
Observe that all initial local checkpoints c satisfy 
Vs E 
: s $ c. 
Similarly, if c is a final local checkpoint, then 
Vs E s : c 
s. 
Alternatively, a zigzag path between two checkpoints c and d is defined as follows. 
There is a zigzag path from c to d iff 
1. Both c and d are in the same process and c 
d ;  or, 

266 
CHAPTER 17. RECOVERY 
2. there is a sequence of messagcs ml, . . . , mt such that 
(a) ml is sent, after the checkpoint, c. 
(b) If rrik is received by process r, then m k + l  is sent by process r in the same 
or a later checkpoint interval. Note that the message m k + l  may be sent 
before m k .  
(c) mt is received before the checkpoint d. 
In Figure 17.2, there is a zigzag path from c1,1 to c 3 , ~  even though there is 110 
happened-before path. This path corresponds to the messages m,3 and m4 in the 
diagram. The message m4 is sent in the same checkpoint interval in which m3 is 
received. Also not,e that t,here is a zigzag path from c2,2 to itself because of messages 
nz5 and m a .  Such a path is called a zigzag cycle. 
Figure 17.2: Examples of zigzag paths 
We leave it, as an exercise for t,he reader to show that c 
d iff there is a zigzag 
path from c to d. 
We now show that given a set of local checkpoints X ,  there exists a consistent 
global checkpoint G containing X iff X is z-consist,ent. 
We prove the contrapositive. Givcn c and d in X (possibly c = d), we show 
that c 
d implies t,hat there is no consistent global state G containing c and d. 
We prove a slightly stronger claim. We show that c 
d implies that there is no 
consistent, global st,ate G containing c or any checkpoint preceding it and d. 
The proof is based on induction on k ,  the minimum number of applications 
of rule (22) t,o derive that c 
d. When 
= 0, we have c --t d. Thus c or any 
checkpoint preceding c and d cannot be part of a consistent state by the definition of 
consistency. Now consider the case when c 
d because 3e : (c -+ c )  A (prrd.e 
d). 
We show that any consistent set of states 
containing c and d cannot have any 

17.3. COMMUNICATION-INDUCED CHECKPOINTING 
267 
checkpoint from the process containing the Checkpoint e. Y cannot contain e or 
any state following e because c --t e would imply that c happened before that state. 
Furthermore, Y cannot contain any checkpoint previous to e because pred.e f d 
and the induction hypothesis would imply that Y is inconsistent. The induction 
hypothesis is applicable because pred.e 
d must have fewer applications of (Z2) 
rule. Since any consistent set of checkpoints cannot contain any checkpoint from 
the process e.p, we conclude that there is no global checkpoint containing c and d. 
Conversely, it is sufficient to show that if X is not global, then there exists Y 
strictly containing X that is z-consistent.. By repeating the process, the set can 
be made global. Furthermore, the set is always consistent because 
4. y implies 
f .  y. For any process 
which does not have a checkpoint in X ,  we define 
e = min{f E 
\ V Z  E x : f & 
where min is taken over the relation 4. Note that the set over which min is taken 
is nonempty because the final checkpoint on process 
cannot zigzag precede any 
other checkpoint. We show that Y = XU { e }  is z-consistent. It is sufficient to show 
that e 
e and c 
e for any c in X .  If e is an initial local checkpoint, then e 
e 
and c 
e for any c in X clearly hold. Otherwise, pred.e exists. Since e is the 
minimum event for which 
E X : e f ,  we see that there exists an event, say, 
d E X ,  such that pred.e 
d. Since e 
e and pred.e 
d imply that e 
d, we 
know that e f e is not possible. Similarly, c 5 e and pred.e f d imply c 
d, 
which is false because X is z-consistent. 
This result implies that if a checkpoint is part of a zigzag cycle, then it cannot 
be part of any global checkpoint. Such checkpoints are called useless checkpoints. 
17.3 Communication-Induced Checkpoint ing 
If a computation satisfies a condition called rollback-dependency trackability (RDT), 
then for every zigzag path there is also a causal path. In other words, rollback 
dependency is then trackable by tracking causality. Formally, 
Definition 17.2 (RDT) A computation 
checkpoints satisfies rollback-dependency 
trackability 
for all checkpoints c, d: c + d = c 
d. 
Because there are no cycles in the happened-before relation, it follows that if a 
computation satisfies RDT, then it does not have any zigzag cycles. This implies 
that no checkpoint is useless in a computation that satisfies RDT. We now develop 
an algorithm for checkpointing to ensure RDT. 
The algorithm takes additional checkpoints before receiving some of the messages 
to ensure that the overall computation is RDT. The intuition behind the algorit,hm 

268 
CIIAP'TEH 17. RECOVERY 
is t,hat, for every zigzag path there should be a causal path. The difficulty arises 
when in a checkpoint irit,erval a message is sent before another message is received. 
For example, in Figure 17.2 m4 is sent before m3 is received. When m3 is received, 
a zigzag pat,h is fornied from c1,1 to c 3 , ~ .  
The message m3 had dependency on c l , ~ ,  
which was not, sent 
part of m4. To avoid this situation, we use the following rule: 
Fixed dependency ufter send (FDAS): A process takes additional checkpoints t,o 
guararit,ee t,hat the transit,ive dependency vector remairis unchanged after any send 
event (tintmil the next checkpoint). 
Thus a process takes a checkpoint, before a receive of a message if it has sent a 
message in that c:lieckpoint, irit.erva1 and the vect,or clock changes when t,he message 
is received. 
A corriputatiori that uscs FDAS is guaranteed to satisfy RDT because ariy zigzag 
path from (:heckpoints c t,o d implies the existence of a causal path from c t,o d. 
'Tliere art' two main advantages for a computat,ion to be RDT: (1) it' allows us to 
calculate efficiently the niaxiniuni recoverable global st.ate containing a given set of 
checkpoints (see Problem 17.2), and (2) every zigzag path implies t,he exist,ence of 
a happened-hefore path. Since there are 110 cycles in t,he happened-before relat#ion, 
it follows that the RDT graph does riot have any zigzag cycles. Hence, using FDAS 
we can guarantee that there are no useless checkpoints in the computation. 
17.4 Optimistic Message Logging: Main Ideas 
In chec~~oin~i7~gbased 
rnet,liods for recovery, afkr a process fails, some or all of t,he 
processes roll back to their last checkpoints such that the resulting system st,at.e 
is consistent. For large systems, the cost of t,his synchronization is prohibitive. 
Furthermore, these protocols may not restore the maximum recoverable stsate. 
If along wit,h checkpoints, messages are logged to the stable stsorage, then t>he 
niaxinium recoverable state can always be rest,ored. Theoretically, message logging 
alone is sufficient (assuming deterministic processes), but checkpointing speeds up 
thc recovcry. 
In 
pessimistic logging, niessages are logged either 
soon as they are received or before 
the receiver sends a new message. When a process fails, its last checkpoint is restored 
and the logged messages that were received after the checkpointed state are replayed 
in the order t,hey were received. Pessimism in logging ensures that no other process 
needs to be rolled back. Although this recovery mechanism is simple, it reduces the 
specd of thc computat,ion. Therefore, it, is not a desirable scheme in an eriviroriment 
where failures are rare arid message activity is high. 
Messages can be logged by either the sender or the receiver. 

17.4. OPTIMISTIC MESSAGE LOGGING: MAIN IDEAS 
269 
In optimistic logging, it is assumed that failures are rare. A process stores the 
received messages in volatile memory and logs them to stable storage at infrequent 
intervals. Since volatile memory is lost in a failure, some of the messages cannot be 
replayed after the failure. Thus some of the process states are lost in the failure. 
States in other processes that depend on these lost states become orphans. A recov- 
ery protocol must roll back these orphan states to nonorphan states. The following 
properties are desirable for an optimistic recovery protocol: 
0 Asynchronous recovery: A process should be able to restart immediately after 
a failure. It should not have to wait for messages from other processes. 
0 Minimal amount of rollback In some algorithms, processes that causally de- 
pend on the lost computation might roll back more than once. In the worst 
case, they may roll back an exponential number of times. A process should 
roll back at most once in response to each failure. 
0 No assumptions about the o r d e k g  of messages: If assumptions are made 
about the ordering of messages such as FIFO, then we lose the asynchronous 
character of the computation. A recovery protocol should make as weak as- 
sumptions as possible about the ordering of messages. 
0 Handle concurrent failures: It is possible that two or more processes fail con- 
currently in a distributed computation. A recovery protocol should handle 
this situation correctly and efficiently. 
0 Recover maximum recoverable state: No computation should be needlessly 
rolled back. 
We present an optimistic recovery protocol that has all these features. Our protocol 
is based on two mechanisms-a 
fault-tolerant vector clock and a version end-table 
mechanism. The fault-tolerant vector clock is used to maintain causality informa- 
tion despite failures. The version end-table mechanism is used to detect orphan 
states and obsolete messages. In this chapter, we present necessary and sufficient 
conditions for a message to be obsolete and for a state to be orphan in terms of the 
version end-table data structure. 
17.4.1 Model 
In our model, processes are assumed to be piecewise deterministic. This means 
that when a process receives a message, it performs some internal computation, 
sends some messages, and then blocks itself to receive a message. All these actions 
are completely deterministic, that is, actions performed after a message receive 

270 
CHAPTER 17. RECOVERY 
and before blocking for another message receive are determined completely by the 
contents of the message received and the state of the process at the time of message 
receive. A nondeterministic action can be modeled by treating it as a message 
receive. 
The receiver of a message depends on the content of the message and therefore 
on the sender of the message. This dependency relation is transitive. The receiver 
becomes dependent only after the received message is delivered. From now on, 
unless otherwise stated, receive of a message will imply its delivery. 
A process periodically takes its checkpoint. It also asynchronously logs to the 
stable storage all messages received in the order they are received. At the time of 
checkpointing, all unlogged messages are also logged. 
A failed process restarts by creating a new version of itself. It restores its last 
checkpoint and replays the logged messages that were received after the restored 
st,at,e. Because some of the messages might not have been logged at the time of 
t,he failure, some of the old states, called lost states, cannot be recreated. Now, 
consider the states in other processes that depend on the lost states. These states, 
called orphan states, must be rolled back. Ot,her processes have not failed, so before 
rolling back, they can log all the unlogged messages and save their states. Thus no 
inforniat,ion is lost in rollback. Note the distinction between restart and rollback. A 
failed process rest,arts, whereas an orphan process rolls back. Some information is 
lost in restart but not in rollback. A process creates a new version of itself on restart 
but not on rollback. A message sent by a lost or an orphan state is called an obsolete 
message. A process receiving an obsolete message must discard it. Otherwise, the 
receiver becomes an orphan. 
In Figure 17.3, a distributed computation is shown. Process 
fails at state 
f 1 0 ,  restores stat,e s11, takes some actions needed for recovery, and restarts from 
st,ate r10. States s12 and 
are lost. Being dependent on 512, state s22 of P2 is 
an orphan. P2 rolls back, restores state s21, takes actions needed for recovery, and 
restarts from state r20. Dashed lines show the lost computation. Solid lines show 
the useful computation at the current point. 
17.4.2 Fault-Tolerant Vector Clock 
Recall t.hat a vector clock is a vector whose number of components equals the number 
of processes. Each entry is the timestamp of the corresponding process. To maintain 
causality despite failures, we extend each entry by a version number. The extended 
vector dock is referred to as the fault-tolerant vector clock (FTVC). We use the 
term "clock" and the acronym FTVC interchangeably. Let us consider the FTVC 
of a process Pi. The version number in the ith entry of its FTVC (its own version 
number) is equal to the number of times it has rolled back. The version number 

17.4. OPTIMISTIC MESSAGE LOGGING: MAIN IDEAS 
271 
4 

272 
CHAPTER 17. ItECOVERY 
in t,lie jt.h entry is equal t,o t,lie highest version number of Pj on which Pi depends. 
Let erit,ry e corresporid to a tuple(version 
timestamp ts). Then, el < e2 = (v1 < 
A process P, sends its FTVC along with every outgoing message. After sending a 
message, Pi increments its timestamp. On receiving a message, it' updat.es it,s FTVC 
with the message's FTVC by taking the componentwise niaxiniuin of entries and 
incrementirig its own timestamp. To t,ake the maximum, the entry with t,he higher 
version number is chosen. If both ent,ries have the same version number, t.hen the 
entry with the higher t,iriiestanip value is chosen. 
When a process restmarts after a failure or rolls back because of failure of sonie 
other process, it, iricrernerits its version nurnber arid set,s its timestanip t.0 zero. Not>e 
t,hat t.his operat,ion does not, require access to previous timestamps that may be lost 
on a failure. It, requires only its previous version number. As explained in Section 
17.5.2, t,hc version number is not lost in a failure. A formal description of the FTVC 
algorithm is given in Figure 17.4. 
An example of FTVC is shown in Figure 17.3. The FTVC of each st>ate is shown 
in a rectangular box near it. 
0 2 )  V [(Vl = ~ 2 )  
A (t91 < t s p ) ] .  
17.4.3 Version End Table 
Orphan st~at,es arid result,ing obsolet,e messages are detected with t,he version end- 
table nieclianism. 'This nirt,hod requires that, after recovering from a failure, a 
process notify ot.her processes by broadcasting a token. The token contains the ver- 
sion number t,hat, failed and t,he t,iniestamp of that version at the point, of restorat,ion. 
We do not, niake any assumption about the ordering of tokens among themselves or 
with respect, to the messages. We assunie that, tokens are delivered reliably. 
Every process maintains some information, called vtable, about other processes 
in its stable storage. In ufable of Pi, t,here is a record for every known version of 
processes that ended in a failure. If Pi has received a token about kt,h version of 
Pi, tlieri it keeps that, token's timest,amp in the corresponding record in vtable. The 
routirie inswt(i.rtable[j], 
f o h n )  insertas the token in t>hat part of the ?!table of Pi that 
keeps t,rack of Pj. 
A formal descript,ion of t,hc version end-table manipulation algorit'lirri is given in 
Figure 17.5. 
17.5 An Asynchronous Recovery Protocol 
Our protocol for asynchronous recovery is shown in Figure 17.6. We describe t,he 
actions taken by a process, say, Pz, on the occurrence of different, events. We assunie 
that each action taken by a process is atomic. This means that any failure during 

17.5. AN ASYNCHRONOUS ItECOVEItY PROTOCOL 
273 
type entry = (integer ver, integer ts); // version, timestamp 
var clock : array [1..N] o f  entry initially 
v j  : clock[j].ver = 0 ; 
-
-
-
-
-
-
-
 
v j  : j # i : clock[j].ts = O;clock[i].ts = 1; 
To send message : 
send (data,clock) ; 
clock[i].ts := clock[i].ts + 1; 
Upon receive of a message (data, mclock) : 
// P, receives vector clock ’mclock’ in incoming message 
vj : clock[j] = maz(clock[j], 
mclock[j]); 
clock[i].ts := clock[i].ts + I; 
clock = s.clock; 
clock[i].ver := clock[i].ver + 1; 
clock[i].ts = 0; 
Upon Restart (state s restored) : 
Upon Rollback(state s restored) : 
clock = s.clock; 
Figure 17.4: Formal description of the fault-tolerant vector clock 
p.. . 
var 
vtable : array[l..N] of set of entry initially empty; 
token : entry; 
Receive-token ( u 1 , t l )  from Pj : 
insert(vtable[j], 
; 
Upon Restart 
insert(vtable[i], (v, clock[i] .ts)) ; 
L 
I 
Figure 17.5: Formal description of the version end-table mechanism 

2 74 
CHAPTER 17. REXOVEHY 
the execution of any action may be viewed 
a failure before or after the execution 
of t,he e h r e  action. 
17.5.1 Message Receive 
On receiving a message, Pi first checks whether the message is obsolete. This is done 
as follows. Let, e j  rcfer to t,he j t h  entry in the message’s FTVC. Recall that each 
entry is of the form 
where 
is the version number and t is the timestamp. If 
there exists an entry ej, such that ej is (u, t )  and (u, t’) belongs to inkzble[j] of Pi 
and t > t’, t,hen the message is obsolete. This is proved later. 
If the niessage is obsolete, then it is discarded. Otherwise, Pi checks whether 
the message is deliverable. The message is not deliverable if its FTVC contains a 
version number k for any process Pj, such that Pi has not received all the t,okens 
from Pj with the version number 1 less t,han k .  In this case, delivery of the message 
is post,poned. Since we assume failures to be rare, this should not affect t,he speed 
of t~he coniputat,ion. 
If t,he message is delivered, then the vector clock and the version end-table are up- 
dat,ed. P, updates its FTVC with the message’s FTVC 
explained in Section 17.4.2. 
The message and its FTVC are logged in the volatile storage. Asynchronously, the 
volatile log is flushcd to the stable storage. The version end-table is updated as 
explained in Section 17.4.3. 
17.5.2 On Restart after a Failure 
After a failure, Pi restores its last checkpoint from the stable storage (including 
the version end-table). Then it replays all the logged messages received after the 
restored st,ate, in the receipt order. To inform other processes about its failure, it 
broadcasts a token containing its current version number and timestamp. After that, 
it increments its own version number and resets its own timestamp to zero. Finally, 
it updates its version end-table, takes a new checkpoint, and starts computing in 
a riorrnal fashion. The new checkpoint is needed to avoid the loss of the current 
version number in another failure. Note t,hat the recovery is unaffected by a failure 
during this checkpointing. The entire event must appear atomic despite a failure. If 
the failure occurs bcfore the new checkpoint is finished, then it should appear that 
t,he rest.art never happened and the restart event can be executed again. 
17.5.3 On Receiving a Token 
We require all tokens to be logged synchronously, that is, the process is not allowed 
to compute further until the information about the token is in stable storage. This 
prevents the process from losing the information about the token if it fails after 

17.5. AN ASYNCHRONOUS RECOVERY PROTOCOL 
275 
D.. . 
Receive-message (data, mclock) : 
// Check whether message is obsolete 
Vj:if ((mclock[j].ver, 
t )  E wtable[j]) and (t < mclock[j].ts) then 
if 3j,l s.t. 1 < mclock[j].uer A 
has no token about lth version of Pj then 
discard message ; 
postpone the delivery of the message until that token arrives; 
Restart (after failure) : 
restore last checkpoint; 
replay all the logged messages that follow the restored state; 
insert(vtabZe[i], cZock[i] .ts)); 
broadcast-token( cloclc[i]) ; 
Receive-token (v,t) from Pj : 
synchronously log the token to the stable storage; 
if FTVC depends on version 
of Pj with timestamp t' 
// Regardless of rollback, following actions are taken 
update vtable; 
deliver messages that were held for this token; 
and (t < t') then Rollback; 
Rollback (due to token ( v , t )  from Pj ) : 
log all the unlogged messages t o  the stable storage; 
restore the maximum checkpoint such that 
discard the checkpoints that follow; 
replay the messages logged after this checkpoint 
discard the logged messages that follow; 
it does not depend upon any timestamp t' > t of version v for Pj..(I) 
until condition ( I )  holds; 
Figure 17.6: An optimistic protocol for asynchronous recovery 

276 
CHAPTER 17. RECOVERY 
acting on it. Since we expect the number of failures t,o be small, this would incur 
only a small overhead. 
The t,oken enables a process to discover whether it has become an orphan. To 
check whether it has beconie an orphan, it proceeds as follows. Assume that it 
received t,he token ( w ,  t )  from Pj. It checks whether its vector clock indicates that it 
depends on a state (v,t’) such that. t < t’. If so, then Pi is an orphan and it needs 
to roll back. 
Regardless of the rollback, P, ent,ers the record (v,t) in version end-table [ j ] .  
Finally, messages that were held for this token atre delivered. 
17.5.4 On Rollback 
On a rollback due to t,oken 
t )  from Pj, P, first logs all the unlogged me 
t,o the st,able storage. Then it restmores the niaximiim checkpoint s such t,liat. s does 
not, depend on any state on Pj with version number w and timestamp greater t,lian 
t. Then loggd rnessages t,liat, were received after s are replayed 
long 
messages 
are riot, obsolete. It discards the checkpoints and logged messages t>liat follow this 
state. Now t,he FTVC is updated by incrementirig its timestamp. Not.e t’hat it does 
not inc:rcment its version number. After this step, Pi restarts computing as normal. 
This prot.oco1 has t,hc following propert,ies: 
0 Asy~~chron,ous 
After a failure, a process restores it,self and starts 
coniputing. It broadcasts a t,oken about, its failure, but it does not require any 
response. 
0 Minimul rollback In response to the failure of a given version of a given pro- 
cess! other processes roll back at most once. This rollback occurs on receiving 
the corresponding token. 
0 Haridling concurrent failwe.s: In response t,o niiilt,iple failures, a process rolls 
back in t,he order in which it receives iriformation about different failures. 
Conc:urrent failures have the same effect 
t,hat of multiple rionconcurrent 
faihires. 
0 Recovering maximum recovernhle stute: Only orphan states are rolled back. 
We now do the overhead analysis of the prot,ocol. Except for application nies- 
sages, the protrocol causes no extra messages to he sent during failure-free run. It 
tags a FTVC t,o every application message. Let, the rnaxiniiim riurnber of failures 
of any process be f .  The prot,ocol adds log 
bit,s to each timestamp in the vector 
clock. Since we expect t,he number of failiires t,o be srnall, logf should be small. 

17.G. PROBLEMS 
Thus the total overhead is O(N log 
bits per message in addition to the vector 
clock. 
A token is broadcast only when a process fails. The size of a token is equal to 
just one entry of the vector clock. 
Let the number of processes in the system be n. There are at most f versions 
of a process, and there is one entry for each version of a process in the version 
end- t able. 
17.6 Problems 
17.1. Show that the following rules are special cases of FDAS. 
(a) A process takes a checkpoint before every receive of a message. 
(b) A process takes a checkpoint after every send of a message. 
(c) A process takes a checkpoint before any receive after any send of a mes- 
sage. 
17.2. Assume that a computation satisfies RDT. Given a set of checkpoints X from 
this computation, show how you will determine whether there exists a global 
checkpoint containing X .  If there exists one, then give an efficient algorithm 
to determine the least and the greatest global checkpoints containing X .  
17.3. (due to Helary et al. [HMNRH7]) Assume that all processes maintain a variant 
of logical clocks defined as follows: The logical clock is incremented on any 
checkpointing event,. The clock value is piggybacked on every message. On 
receiving a message, the logical clock is computed 
the maximum of the local 
clock and the value received with the message. Processes are free to take their 
local checkpointh whenever desired. In addition, a process is forced to take a 
local checkpoint on receiving a message if (a) it has sent out a message since its 
last checkpoint, and (b) the value of its logical clock will change on receiving 
the message. Show that this algorithm guarantees that there are no useless 
checkpoints. Will this protocol force more checkpoints or fewer checkpoints 
than the FDAS protocol? 
17.4. In many applications, the distributed program may output to the external 
environment such that the output message cannot be revoked (or the environ- 
ment cannot be rolled back). This is called the output commit problem. What 
changes will you make to the algorithm to take care of such messages? 
17.5. Give a scheme for garbage collection of obsolete local checkpoints and message 
logs. 

278 
CHAPTER 17. RECOVERY 
17.7 Bibliographic Remarks 
The zigzag relation was first defined by Netzer and Xu [NX95]. The definition we 
have used in this chapter is different from but equivalent to their definition. The 
notioii of the R-graph, RDT computation, and the fixed-dependency-after-send rule 
was introduced by Wang [Wan97]. 
Strom arid Yemini [SY85] init,iated t.he area of optimistic message logging. Their 
schenic:, however, suffers from the exponential rollback problem, where a single failure 
of a process can roll back another process an exponential mimber of times. The 
algorit,hm discussed in this chapter is taken from a paper by Damani and Garg 
[ DG 961 . 

Chapter 18 
Self- S t abilizat ion 
18.1 Introduction 
In this chapter we discuss a class of algorithms, called self-stabilizing algorithms, that 
can tolerate many kinds of “data” faults. A ‘Ldatal’ fault corresponds to change in 
value of one or more variable of a program because of some unforeseen error. For 
example, a spanning tree is usually implemented using parent variables. Every node 
in the computer network mainhins the parent pointer that points to the parent in 
the spanning tree. What if one or more of the parent pointers get corrupted? Now, 
the parent pointers may not form a valid spanning tree. Such errors will be called 
“data” faults. We will assume that the code of the program does not get corrupted. 
Because the code of the program does not change with time, it can be periodically 
checked for correctness using a copy stored in the secondary storage. 
We assume that the system states can be divided into legal and illegal states. 
The definition of the legal state is dependent on the application. Usually, system 
and algorithm designers are very careful about transitions from the legal states, but 
illegal states of the system are ignored. When a fault occurs, the system moves to 
an illegal state and if the system is not designed properly, it may continue to execute 
in illegal states. A system is called self-stabilizing if regardless of the initial state, 
the system is guaranteed to reach a legal state after a finite number of moves. 
We will illustrate the concept of self-stabilizing algorithms for two problems: 
mutual exclusion and spanning tree construction. 
279 

280 
18. SELF-S'TAUILIZAI'ION 
18.2 Mutual Exclusion with K-State Machines 
We will niotiel t,he mutila1 exclusion problem as follows. A machine can enter t.he 
critical section only if it has a privilege. Therefore, in the case of niutual exclusion, 
legal sthtes are those global st,at,es in which exaczctly one machine has a privilege. 
The goal of the self-stabilizing mutual exclusion algorithm is to determine who has 
the privilcgc aiid how the privileges move in the network. 
if ( L  = 
:= + 1 mod K ; 
For other machines: 
if ( L  # 
:= L ; 
Figure 18.1: K-state self-stabilizing algorithm 
4 
4 2o
2 
3 
2 
3 
Figure 18.2: A move by the bot,tom machine in the K-state algorithm 
We aSsiinie that there are N machines numbered 0 . .  . N - 1. The state of any 
machine is determined by its label from the set (0.. . K - 1). We use L ,  S, and 
R to denote the labels of the left neighbor, itself, and the right neighbor for any 
machine. Machine 0, also called the bottom machine, is t,reated differedy from all 
ot,her macliirics. The program is given in Figure 18.1, arid a sample execut,iori of the 
algorit.hm is shown in Figure 18.2. The bottom machine has a privilege if its label 
t,lie same value 
its left neighbor, (i.e., L = S). In Figure 18.2, the bott,om 
machine aid its left rieighbor have labels 2, and therefore the bottom rriacliine has 
a privilege. Once a machine possessing a privilege executes its critical sectiori, it 
should execute t,he transition given hy the program. In Figure 18.2, on exit.ing from 

18.2. MUTUAL EXCLUSION WITH K-STATE MACHINES 
281 
the critical section, the bottom machine executes the statement S := + 1 mod K 
and acquires t,he label 3. 
A normal machine has a privilege only when L # S. On exiting the critical 
section, it executes S := L and thus loses its privilege. In Figure 18.3, P5 moves 
and makes it label as 4. 
4 
4 
3 
3 
Figure 18.3: A move by a normal machine in the K-state algorithm 
In this algorithm, the system is in a legal state if exactly one machine has the 
privilege. It is easy to verify that 
~ 1 , .  
. . , Z N - ~ )  is legal if and only if either all 
zi values are equal or there exists m < N - 1 such that all 
values with i 5 m are 
equal to some value and all other zi values are equal to some other value. In the 
first case, the bottom machine has the privilege. In the second case the machine 
Pm+l has the privilege. It is easy to verify that if the system is in a legal state, then 
it will stsay legal. 
It is also easy to verify that in any configuration, at least one move is possible. 
Now we consider any unbounded sequence of moves. We claim that a sequence 
of moves in which the bottom machine does riot move is at most O ( N 2 ) .  This is 
because machine 1 can move at most once if the bottom machine does not move. 
This implies that the machine 2 can move at most twice and so on. Thus, the total 
number of moves is bounded by O ( N 2 ) .  
We now show that given any configuration of the ring, either (1) no other machine 
has the same label 
the bottom, or (2) there exists a label that is different from all 
machines. We show that if condition (1) does not hold, then condition ( 2 )  is true. 
If there exists a machine that has the same label 
that of bottom, then there are 
now K - 1 labels left to be distributed among N - 2 machines. Since K 
N ,  we 
see that there is some label which is not used. 
Furthermore, within a finite number of moves, condition (1) will be true. Note 
that if some label is missing from the network, it can be generated only by the bottom 

282 
CHAPTER 18. SELF-STAI3ILIZA'rION 
machine. Moreover, the bottom machine simply cycles among all labels. Since the 
bottom machine moves after some finite number of moves by normal machines, the 
bot,t,om machine will eventually get the missing label. 
We now show that if the system is in an illegal state, then within O ( N 2 )  moves, 
it reac:hcs a legal state. It is easy to see that once the bott,om machine gets the 
unique label, the system stabilizes in O ( N 2 )  nioves. The bottom machine can move 
at most N tirnes before it acquires the missing label. Machine 1 therefore can move 
at. most, N + 1 times before the bott,om acquires the label. Similarly, machine 
can 
move at most, N + times before the bottom get,s the label. By adding up all the 
moves, we get N + ( N  + 1) + . . . -t ( N  + N - 1) = O ( N 2 )  moves. 
In t.his algorithm, the processes could read the state of their left neighbors. How 
do we implement this in a dist,ributed syst,ern? One possibility is for a niachine t,o 
periodic:ally query it,s left neighbor. However, this may generate a lot of niossages 
in t.he net,work. A more message-efficient solution is for a token to go around the 
system. The token carries with it the st>ate of the last machine that it, visited. But 
now we have to worry about the token get,ting lost and the presence of multiple 
tokens in the system. To guard against the token gett,ing lost, we will require t,he 
bot,t,oni machine to generate a new tjoken if it does not see a token in a cert,ain 
timeout interval. To accomplish t.his, we use the following Java object' as a periodic 
t,ask. On invocation, this task sends a restart message to the bottom machine. To 
handle this message, the bottom machine calls the sendToken method. 
import j a v a  . u t i l  . TimerTask ; 
public class RestartTask extends TinierTask { 
hlsgHaridler app ; 
public RestartTask ( MsgHandler app) { 
= app; 
1 
1 
public void r u n ( )  { 
app. handleMsg( n u l l ,  0 ,  " r e s t a r t  " ) ;  
} 
We use a boolean tokensent to record if the token has been sent by the bottom 
machine in the last timeout interval. If not, a token is sent out. 
Multiple t,okens do not pose any problem in this scheme because that only means 
that miiltiple processes will read the st,at,e of their left neighbors. The algorithm 
is shown in Figure 18.4 for t,he bot,tom machine and in Figure 18.5 for the normal 
machine. 

18.2. MUTUAL EXCLUSION WITH K-STATE MACHINES 
283 
mport j a v a .  u t i l  .Timer; 
,ublic c l a s s  StableBottom extends Process implements Lock { 
i n t  mystate = 0 ;  
i n t  l e f t s t a t e  = 0; 
i n t  next; 
Timer t = new Timer ( ) ;  
boolean tokensent = f a l s e  ; 
public StableBottom ( Linker iriitcomm) { 
super ( initComm ) ; 
next = (myId + 1) % N; 
} 
1 
1 
1 
public synchronized void i n i t i a t e  ( )  { 
t .  schedule(new R e s t a r t T a s k ( t h i s ) ,  1 0 0 0 ,  1 0 0 0 ) ;  
public synchronized void requestCS ( )  { 
while ( l e f t s t a t e  != mystate) mywait(); 
public synchronized void releaseCS ( )  { 
mystate = ( l e f t s t a t e  + 1) % N; 
public synchronized void sendToken ( )  { 
sendMsg ( next , ‘’ token” , mystate ) ; 
tokeiiSent = t r u e ;  
if (! tokensent ) { 
} e l s e  tokensent = f a l s e ;  
1 
public synchronized void handleMsg(Msg m, i n t  src , String t a g )  { 
if ( t a g .  equals ( ” t o k e n ”  ) ) 
I I 
l e f t s t a t e  = m. getMessageInt ( ) ;  
notify ( ) ;  
Util .mySleep(1000); 
sendMsg ( next , ” token” , mystate ) ; 
tokensent = true; 
sendToken ( )  ; 
} e l s e  if ( t a g . e q u a l s ( ” r e s t a r t ” )  ) 
1 
1 
Figure 18.4: Self-stabilizing algorithm for mutual exclusion in a ring for the bottom 
machine 

284 
CHAPTER 18. 
mport j a v a .  u t i l  .Tinier; 
>ublic class StableNorma1 extends Process implements Lock { 
int mystate = 0; 
int l e f t s t a t e  = 0; 
public StableNorma1 (Linker initConim) { 
super ( initCoirim ) ; 
1 
1 
public synchronized void requestCS ( )  { 
while ( l e f t s t a t e  == mystate) myWait(); 
public synchronized void releaseCS ( )  { 
mvState = l e f t s t a t e  ; 
sekdToken ( ) ; 
1 
p u b 1 i c sync h r o n i z ed void se 11 d T o ke n 
int next = (myId + 1) O/o 
srndMsg( next , "token" , mystate 
1 
I public synchronized void IiandleMsg 
if ( t a g .  equals ( " t o k e n " ) )  { 
l e f t s t a t e  = m. getMessageInt ( ) ;  
notify ( ) ;  
Util . inySleep (1000); 
sendToken ( )  ; 
Msg in, int s r c  , String t a g )  { 
Figure 18.5: Self-stabilizing algorithm for mutual exclusion in a ring for a normal 
machine 

18.3. SELF-STABILIZING SPANNING TREE CONSTRUCTION 
285 
18.3 Self-stabilizing Spanning Tree Construction 
Assume that the underlying topology of a communication network is a connected 
undirected graph. Furthermore, one of the node is a distinguished node called root. 
Our task is to design a self-stabilizing algorithm to maintain a spanning tree rooted 
at the given node. Note that determining parent pointers of the spanning tree 
once is not enough. Any data fault can corrupt these pointers. We would need to 
recalculate parent pointers periodically. To calculate the parent pointers, we also 
use a variable d i s t  that indicates the distance of that node from the root. 
Root: (execute periodically) 
dist := 0; 
parent := -1; 
For other node: (execute periodically) 
read dist of all neighbors; 
Let j be the neighbor with a minimum distance distj; 
parent := j ;  
d i d  := distj + 1; 
Figure 18.6: Self-stabilizing algorithm for (BFS) spanning tree 
The algorithm shown in Figure 18.6 is quite simple. In this version, we asume 
that a node can read the value of variables of its neighbors. Later, we will t'rarislate 
t,his program into a distributed program. Each node maintains only two variables- 
parent and dist. The root node periodically sets parent to -1 and d i s t  to 0. If 
any of these values get corrupted, the root node will reset it to the correct value 
by this mechanism. A nonroot node periodically reads the variable d i s t  of all its 
neighbors. It chooses the neighbor wit>h the least distance and makes that neighbor 
its parent. It also set,s its own distsance to one more than the distance of that, 
neighbor. 
It is easy to verify that no matter what the values of parent and dist initially 
are, the program eventaually converges to valid values of parent and d i s t .  
Let us now translate this program into a dist,ributed program. The program for 
thc root node shown in Figure 18.7 is identical to that in Figure 18.6. For periodic 
recalculation we use a timer that schedules RestartTask after a fixed time interval. 
The algorithm for a nonroot node is shown in Figure 18.8. A nonroot node reads 
the values of the neighboring nodes by sending them a query message of type Q.dist. 
Whenever a node receives a message of type Q.dist it responds with a message of 

286 
C H A P T E R  18. SELF-STABILIZATION 
type A.dr,st with the value of its 
variable. The variable 
indicate the 
riuniber of A.dzst messages that node is expecting. Whenever 
become 
0, it knows that it has heard from all its neighbors and therefore it knows the 
neighbor with the least distance. The variable 
is used for recalculation of 
the distance. 
The main program that invokes 
and 
is 
shown in Figure 18.9. 
import j a v a .  u t i l  .Timer; 
public c l a s s  StableSpanRoot extends Process { 
i n t  parerit = -1; 
i n t  dist = 0; 
Tinier t = new Timer ( ) ;  
public StnbleSpariRoot ( Linker initComm) { 
super (initComm ) ; 
t .  schedule (new R e s t a r t T a s k ( t h i s  ) ,  1000, 1 0 0 0 ) ;  
1 
public synchronized void r e c a l c u l a t e  ( ) {  
parent = -1; 
dist = 0; 
System. o u t .  println ( ” p a r e n t  of ’’ + myId + ” is ’’ + parent ); 
System. o u t .  p r i n t l n  ( ” d i s t  of ” + myId + ” is ” + dist ) ;  
1 
public synchronized void Iiaridlehlsg(Msg m, i n t  s r c  , Striiig t a g )  { 
if ( t a g .  equals (”Q. d i s t ” ) )  { 
} else if ( t a g . e q u a l s ( ” r e s t a r t ” ) )  { 
sendMsg( s r c  , ”A. d i s t ” ,  0 ) ;  
r e c a l c u l a t e  ( ) ;  
1 
1 
} 
Figure 18.7: Self-stabilizing spanning t.ree algorithm for the root 
18.4 Problems 
18.1. Show that, a system with four machines may not stabilize if it uses the h’-st,ate 
machine algorithm with K = 2. 
18.2. Show t,liat, the K-state machine algorithm converges to a legal state in at, most 
O ( N 2 )  moves by providing a norm function on the configuration of the ring 
that. is at, most O(N2)), 
decreases by at least 1 for each move, and is always 
nonnegative. 

18.4. PROBLEMS 
287 
mport j a v a .  u t i l  .Timer; 
,ublic class StableSpanNonroot extends Process { 
int parent = -1; 
int d i s t  = 0; 
int newDist = 0; / / d i s t a n c e  
r e c a l c u l a t i o n  
Timer t = new Timer ( ) ;  
int numReports ; 
public StableSpanNonroot, ( Linker initComm) { 
super ( initComm ) ; 
t .  schedule (new RestartTask (this ) ,  1 0 0 0 ,  1 0 0 0 ) ;  
1 public synchronized void r e c a l c u l a t e  ( ) {  
newDist = N ; / / i n i t  newDist t o  max p o s s i b l e  
sendToNeighbors (”Q. d i s t ”  , O ) ;  / / q u e r y  
n e i g h b o r s  
t h e i r  d i s t  
numReports = coinni. neighbors. s i z e  ( ) ;  
while ( numReports > 0) myWait ( )  ; / / w a i t  
a l l  r e s p o n s e s  
d i s t  = newDist; 
Systern.out.print1n ( ” p a r e n t  of ” + myId + ” is ” + p a r e n t ) ;  
System.out, println ( ” d i s t  of ” + myId + ” is ” + dist ) ;  
1 public synchronized void handleMsg(Msg m, int src , String t a g )  { 
if ( t a g .  equals (”Q. d i s t ” ) )  { 
} else if ( t a g .  equals ( ” A .  dist ’‘ ) )  { 
sendMsg( s r c  , ”A. d i s t ” ,  d i s t  ) ;  / / r e p l y  
w i t h  my d i s t  
int hisDist = m. getMessageInt ( ) ;  
if ( (  hisDist >= 0) && (newDist > hisDist ) )  { 
newDist = hisDist +l; 
parent = s r c ;  
1 nuniReports --; 
notifyAll ( ) ;  
r e c a l c u l a t e  ( ) ;  
} else if ( t a g . e q u a l s ( ” r e s t a r t ” ) )  { 
1 
I 
t 
Figure 18.8: Self-stabilizing spanning tree algorithm for nonroot nodes 

288 
CHAPTER 18. 
public class S t a b l e T r e e T e s t e r  { 
public static void main( String [ ]  args) throws Exception { 
S t r i n g  baseNairie = args [ O ] ;  
int iriyId = I n t e g e r .  parseIiit ( a r g s  [ 11); 
int riiimproc = I n t e g e r .  p a r s e I n t  (args [ 2 ] ) ;  
Linker coimn = new L i n k e r  (baseName, rnyId, nuniproc); 
if (niyId==O) { 
StableSpanRoot bot = new StableSpanRoot (co~nm); 
for ( i n t  i = 0 ;  i < iiumProc; i++) 
if ( i  != inyId) 
(new Lis teiier ?' h r ead ( i, b o t ) ) .  s t a r t  ( ) ;  
} else { 
StableSpanNoriroot normal = new StableSpanNonroot (co~nm); 
for ( int i = 0; i < riumProc; i++) 
if ( i !:= myId) 
(new ListenerTliread ( i ,  normal)). s t a r t  ( ) ;  
1 
Figure 18.9: A .Java program for spanning tree 
18.3. In our K-stat,e machine algorithni we have assiinied that a macliine can w a d  
the value of the state of its left, machine and write its own &ate in one at,oniic 
a h o n .  Give a self-stabilizing algorithm in which a processor can only read a 
rernok value or write a local value in one step, but not both. 
*18.4. (duc t,o Dijkst,ra [Dij74]) Show t h t  the following four-state machine algorithm 
is self-stabilizing. The state of wch machine is represented by t,wo booleans 
:rS and ups. For t,he bottom machine ups = true arid for the top machine 
I L ~ S  
= false always hold. 
Bott,om: 
if (xS = x R )  and 71ipR then 
:= 
Normal: 
if XS # .EL then 
:= 7xS; ups := t r w ;  
if SS = .TR and UPS and 7upR then ups := false; 
'l'op: 
if 
# xL) then ZS := 7zS; 

18.5. BIBLIOGRAPHIC REMARKS 
289 
*18.5. Assume that each process P, has a pointer that is either null or points to one 
of its neighbors. Give a self-stabilizing, distributed algorithm on a network of 
processes that guarantees that the system reaches a configuration where (a) 
if P, points to PI, then PI points to P,, and (b) there are no two neighboring 
processes such that both have null pointers. 
18.5 Bibliographic Remarks 
The idea of self-shbilizing algorithms first appeared in a paper by Dijkstra [Dij74], 
where three self-stabilizing algorithms were presented for mutual exclusion in a ring. 

This Page Intentionally Left Blank

Appendix A 
Various Utility Classes 
Algorithms for several utility classes are shown in Figures A.l-A.6. 
29 

292 
APPENDIX A. VARIOUS U'TILITY CIASSES 
mport j a v a .  u t i l  . * ;  
iublic c l a s s  Ut,il { 
public s t a t i c  i n t  max(int a ,  i n t  b) { 
if ( a  > ti) r e t u r n  a ;  
r e t u r n  b ;  
1 
public s t a t i c  void InySleep ( i n t  time) { 
t r y  { 
1 
Thread. sleep ( time ) ;  
} c a t c h  ( InterruptedException e )  { 
I 
public s t a t i c  void rnyWait(0bjrct o b j )  { 
p r i n t l n  ( " w a i t i n g " ) ;  
o b j .  wait ( ) ;  
t r y  { 
1 
} catch ( Interriipt,edExccption e )  { 
1 
public s t a t i c  boolean l e s s T h a n ( i n t  A [ ] ,  i n t  B [ ] )  { 
for ( i n t  j = 0; j < A. length ; j++) 
if ( A [ j ]  > B [ j ] )  r e t u r n  f a l s e ;  
for ( i n t  j = 0 ;  j < A. lrngth ; j++) 
if ( A [ j ]  < B [ j ] )  r e t u r n  t r u e ;  
r e t u r n  f a l s e  ; 
1 
public s t a t i c  i n t  maxArray(int A [ ] )  { 
i n t  v = A [ O ] ;  
for ( i n t  i = O ;  
i d .  length ; i++) 
if ( A [ i ]  > v )  v = A [ i ] ;  
r e t u r n  v ;  
1 
public s t a t i c  String writeArray ( i n t  A [ ] ) {  
StringBuffer s = new StringBuffer ( ) ;  
for ( i n t  j = 0 ;  j < A. 1engt.h ; j +t- )  
s.apperid(String .valiieOf(A[j]) + 
r e t u r n  new String ( s .  t.oStriiig 
" " ); 
} 
public s t a t i c  void rradArray(Stri1ig s ,  i n t  A [ ] )  { 
St,riiigTokenizt.r s t  = new St,ring'l'okenizer ( s  ); 
for ( i n t  j = 0 ;  j < A. l e n g t h ;  j++) 
A[ j ] = Int,c,ger. parseInt ( s t .  IiextToken 
1 
public s t a t i c  i n t  s e a r c h A r r a y ( i n t  A [ ] ,  i n t  x )  { 
for ( i n t  i = 0 ;  i < A. l e n g t h ;  i++) 
r e t u r n  -1; 
if ( A [  i ]  == 
r e t u r n  i ; 
1 
public s t a t i c  void priiitlii ( S t r i n g  
Systerri.out. priiitlii ( s ) ;  
System. o u t  . flush ( )  ; 
if ( Symbols. debugFlag ) { 
} 
1 
Figure A.l: Util.java 

293 
APPENDIX A. VARIOUS UTILITY CLASSES 
public c l a s s  Symbols { 
public s t a t i c  f i n a l  i n t  I n f i n i t y  = -1; 
// i n t e r n e t  r e l a t e d  
public s t a t i c  f i n a l  String nameserver = 
public s t a t i c  f i n a l  int Serverport = 7033; 
public s t a t i c  f i n a l  i n t  coordinator = 0; 
// time bounds on messages f o r  synchronous algorithms 
public s t a t i c  f i n a l  i n t  roundTime = 500: // ms 
public s t a t i c  f i n a l  boolean debugFlag = t r u e :  
” linux02 . ece . utexas . edu” ; 
1 
Figure A.2: Syrnbols.java 
mport j a v a .  u t i l  . * ;  
3ublic c l a s s  Matrix { 
public s t a t i c  String write ( i n t  A[] [ I )  { 
StringBuffer s = new StringBuffer ( ) ;  
f o r  ( i n t  j = 0 ;  j < A. length ; j++) 
r e t u r n  new String ( s .  t o s t r i n g  
public s t a t i c  void read(String s ,  i n t  A [ ] [ ] )  { 
s.append(Uti1. w r i t e A r r a y ( A [ j ] )  + ” ” ) ;  
1 
StringTokenizer st = new StringTokenizer ( s ) ;  
for ( i n t  i = 0 ;  i < A. length ; i + + )  
for ( i n t  j = 0 ;  j < A[ i ] .  length ; j++) 
A[ i ] [ j ] = Integer . parseInt ( s t .  nextToken 
; 
1 
public s t a t i c  void s e t Z e r o ( i n t  A [ ] [ ] )  { 
for ( i n t  i = 0 ;  i < 
i++) 
for ( i n t  j 
= O ;  j < A [ i ] . l e n g t h ;  j++) 
A [ i ] [  j ]  = 0; 
1 , 
public s t a t i c  void setMax(int A [ ] [ ] ,  i n t  B [ ] [ ] )  { 
for ( i n t  i = 0 ;  i < A.lengtli; i++) 
f o r  ( i n t  j = 0; j < A[ i ] .  length ; j++) 
A [ i ] [ j ]  = U t i l . m a x ( A [ i ] [ j ] ,  B [ i ] [ j ] ) ;  
1 
1 
Figure A.3: Matrix.java 

294 
APPENDIX A. VARIOIJS UTILITY CLASSES 
import java. u t i l  . * ;  
public class MsgList extends LinkedList { 
public Msg removeM( int seqNo) { 
SeclMessage sni; 
L i s t I t e r a t o r  i t e r  = super. I i s t I t e r a t o r  ( 0 ) ;  
while ( i t e r  . hasNext ( ) )  { 
sm = ( SeqMessage) i t e r  , next ( ) ;  
if (sm. getSeqNo () == seqNo) { 
it,er .remove ( ) ;  
return sin. get,Message ( ) ;  
1 
return n u l l ;  
I/ 
Figure A.4: MsgList.java 
import j a v a .  u t i l  . LinkedList ; 
public class IntLirikedList extends LinkedList { 
public void add( int i ) { 
super.add(new I n t e g e r ( i ) ) ;  
} 
1 
public boolean c o n t a i n s  (int i ) { 
return super. c o n t a i n s  (new I n t e g e r  ( i  ) ) ;  
public int renioveHead0 { 
I n t e g e r  j = ( I n t e g e r  ) super. removeFirst ( ) ;  
return j .  i n t V a l u e  ( ) ;  
1 
I 
public boolean rernoveObject (int i ) { 
return super. remove(new I n t e g e r  ( i  ) ) ;  
public int gptEmtry (int index ) { 
I n t e g e r  j = ( I n t e g e r  ) super. get ( i n d e x ) ;  
return j .  intValue ( ) ;  
Figure A.5: IntLinkedLikjava 

APPENDIX A. VARIOUS UTILITY CLASSES 
295 
public class PortAddr { 
S t r i n g  hostname; 
int portiium ; 
public P ortA ddr( String s ,  int i ) { 
hostname = new S t r i n g ( s ) ;  
portnum = i ;  
} 
} 
} 
public String getHostName () { 
return hostname ; 
public int g e t p o r t  ( )  { 
return portnurri ; 
1 
Figure A.6: PortAddr.java 

This Page Intentionally Left Blank

Bibliography 
[AEASl] 
[AM GO31 
IAng801 
[AW98] 
(Awe851 
[Bar961 
[BJ87] 
[BN84] 
[Bou87] 
D. Agrawal and A. El-Ahbadi. An efficient and fault-tolerant solution 
for distributed mutual cxclusion. ACM Trans. Comput. Syst., 9(1):1-20, 
February 1991. 
R. Atreya, N. Mit,tal, and V. K. Garg. Detecting locally stable predicat,es 
without modifying applicat,ion messages. In Proc. Intnatl. Con$ on 
Principles of Distributed Systems, La Martinique, France, December 
2003. 
D. Angluin. Local arid global properties in networks of processors. In 
Proc. of the 12th ACM Symp. on Theory of Computing, pages 82 
93, 
1980. 
H. Attiya and J. Welch. Distributed Computing - Fundamentals, Simula- 
tions and Advanced Topics. McGraw Hill, Berkshire, SL6 2QL, England, 
1998. 
B. Awerbuch. Complexity of network synchronization. Joiirnal of the 
ACM, 32(4):804- 823, October 1985. 
V. Barbosa. An Introduction to Distributed Algorithms. The MIT Press, 
Cambridge, MA, 1996. 
K. P. Birman and T. A. Joseph. Reliable communication in the presence 
of failures. ACM Trans. Comput. Syst., 5(1):47-76, 1987. 
A. D. Birrell and B. J. Nelson. Implementing remote procedure calls. 
ACM Trans. Comput. Syst., 2(1):39-59, February 1984. 
L. Bouge. Repeated snapshots in distributed systems with synchronous 
communication and their implementation in CSP. Theoretical Computer 
Science, 49:145-169, 1987. 
297 

298 
BIBLIOGRAPHY 
[Bur801 
[CDK94] 
[CJ97] 
[CL85] 
jCb1891 
[conloo] 
[CR79] 
[DGSG] 
[Dij65a] 
[DijfSb] 
[ Dij 741 
[Dij85] 
[Dij 871 
J .  Burns. A formal model for message passing systems. Technical Report 
TR-91, Indiana University, 1980. Department of Computer Science. 
G. Couloris, J. Dollimore, and T. Kindberg. Distributed Systcms: Con- 
cepts and Design. Addison-Wesley, Reading, MA, 1994. 
R.. Chow arid T. Johnson. 
~ithms. Addison-Wesley Longman, Reading, MA, 1997. 
K. M. Charitly and L. Lamport. Distributed snapshots: Dekrmin- 
ing global states of distributed systems. ACM Tram. Comput. Syst., 
3(1):63-75, February 1985. 
K. M. Chandy arid J. Misra. Parallel Program Design: A Foundation. 
Addison-Wesley, Reading, MA, 1989. 
D. E. Corner. Internetworking with, TCP/IP: Volume 1. Principles, 
Protocols, and Architectures. Prentice-Hall, Upper Saddle River, NJ 
07458, USA, fourth edition, 2000. 
E. J. H. Chang and R. Roberts. An improved algorithm for decentralized 
extrema-finding in circular configurations of processes. Comm.un. 
the 
Distributed Operating Systems and Alyo- 
ACM, 22(5):281-283, 1979. 
0. P. Damani arid V. K. Garg. How to recover efficiently and asyn- 
chronously when optimism fails. In ZCDCS '96; Proc. 
the 16th Intnatl. 
Conf. on Distributed Computing Systems; Hong Kon,g, pages 108-1 15. 
IEEE, May 1996. 
E. W. Dijkst,ra. Co-operating Sequential Processes. In F. Genuys, editor, 
Programming Languages. Academic Press, London, 1965. 
E. W. Dijkstra. Solution of a problem in concurrent programming con- 
trol. Commun. 
the ACM, 8(9):569, September 1965. 
E. W. Dijkstra. Self-stabilizing systems in spite of distributed control. 
Comm.un. of the ACM, 17:643--644, 1974. 
E. W. Dijkstra. The distributed snapshot of K.M. Chandy and L. Lam- 
port. In M. Broy, editor, Control Flow and Data Flow: Concepts 
Dis- 
tributed Programming, volume F14. NATO AS1 Series, Springer-Verlag, 
New York, NY, 1985. 
E. W. Dijkstra. Shmuel Safra's version of termination detection. Report 
EWD998-0, University of Texas at Austin, January 1987. 

BIBLIOGRAPHY 
299 
[DKR82] 
[DSXO] 
[DS83] 
[Far 981 
[Fid89] 
[FL82] 
[FLP85] 
[Gar921 
[Gar961 
[Gar021 
[GC95] 
[Gin91 
[GosSl] 
[GR93] 
D. Dolev, M. Klawe, and M. Rodeh. An O(n1ogn) unidirectional dis- 
tributed algorithm for extrema finding in a circle. Journal of Algorithms, 
3:245-260, 1982. 
E. W. Dijkstra and C. S Scholten. Termination detection for diffusing 
computations. Information Processing Letters, 11 (4):1--4, August 1980. 
D. Dolev and H. R. Strong. Authenticated algorithms for Byzantine 
agreement. SIAM Journal on Computing, 12(4):656-666, 1983. 
J. Farley. Java Distributed Computing. O’Reilly, Sebastopol, CA, 1998. 
C. J. Fidge. Partial orders for parallel debugging. PTOC. of the ACM 
SIGPLA N/SIGOPS Workshop on Parallel and Distributed Debugging, 
(ACM SIGPLAN Notices), 24(1):183-194, January 1989. 
M. J. Fischer and N. A. Lynch. A lower bound on the time to assure 
interactive consistency. Information Processing Letters, 14(4):183-186, 
1982. 
M. J. Fischer, N. Lynch, and M. Paterson. Impossibility of distributed 
consensus with one faulty process. Journal of the ACM, 32(2):374-382, 
April 1985. 
V. K. Garg. Some optimal algorithms for decomposed partially ordered 
sets. Information Processiag Letters, 44:39-43, November 1992. 
V. K. Garg. Principles of Distributed Systems. Kluwer Academic 
lishers, Bost,on, MA, 1996. 
V. K. Garg. Elements of Distributed Computing. Wiley, New York, NY, 
2002. 
V. K. Garg and C. Chase. Distribut.ed algorithms for detecting con- 
junctive predicates. In Proc. of the IEEE Intnatl. Conf. on Distributed 
Computing Systems, pages 423-430, Vancouver, Canada, June 1995. 
D. K. Gifford. Weighted voting for replicated data. Proc. 7th Symp. on 
Operating Syst. Princzples,, 13(5):150-162, December, 1979. 
A. Goscinski. 
Distributed Operating Systems, The Logical Design. 
Addison-Wesley, Reading, MA, 1991. 
J. Gray and A. Reuter. Transaction Processing. Morgan Kaufmann 
Publishers, San Mateo, CA, 1993. 

300 
[Gra78] 
[GW92] 
[HA901 
[Ha11721 
[Har98] 
[ He1891 
[Her881 
[HM84] 
BIBLIOGRAPHY 
J .  N. Gray. 
In G. Goos and 
J. Hartmanis, editors, Operating Systems: A n  Advance Corirse, vol- 
ume 60 of Lecture Notes in Computer Science, pages 393-481. Springer- 
Verlag, 1978. 
V. K. Garg and B. Waldecker. 
Detection of unst.able predicates in 
dist,ributed programs. In Proc. of 12th Conf. on the Foundations of 
Software Technology & Theoretical Computer Science, pages 253-264. 
Springer Verlag, December 1992. Lecture Notes in Comput>er Science 
652. 
Notes on database operating systems. 
P. Hutto and M. Ahaniad. Slow memory : Weakening consistency to 
enhance concurreny in distributed shared memories. Proc. of Tenth 
Intnatl. Conf. on Distributed Computing Systems, May 1990. 
P. Brinch Hansen. Structured multi-programming. CACM, 15(7):574- 
578, July 1972. 
S. J .  Hartley. Concurent Programming: Th,e Java Prograrrinzinq Lan- 
guuge. Oxford, New York, NY, 1998. 
J .  Helary. Observing global states of asynchronous distributed applica- 
tions. In Workshop on Distributed Algorithms, pages 124-135. Springer 
Verlag, LNCS 392, 1989. 
M. Rerlihy. Impossibility and universality results for wait-free synchro- 
nization. Technical Report TR-CS-8, Carnegie-Mellon University (Pit,ts- 
burg PA),, May 1988. 
J.Y. Halpern and Y. Moses. Knowledge and common knowledge in a 
distributed environnient. In Proc. of the ACM Symp. on Principles of 
Distributed Computing, pages 50 - 61, Vancouver, B.C., Canada, 1984. 
[HMNR.97] J. Helary, A. Mostefaoui, R.. H. B. Netzer, and M. Raynal. Preventing 
useless checkpoints in distributed computat,ions. In Symp. o n  Reliable 
Distributed Systems, pages 183--190, Durham, NC, 1997. 
[HMRS95] M. Hurfin, M. Mizuno, M. Raynal, and M. Singhal. Efficient distributed 
detection of conjunction of local predicates. Technical Report 2731, 
IRISA, Renries, France, November 1995. 
C. A. R. Hoare. Monitors: An operating system structuring concept. 
Commun. of the ACM, 17(10):549-557, October 1974. Erratum in Com- 
mun. ofthe ACM, Vol. 18, No. 2 (February), p. 95, 1975. 
[Hoa74] 

BIRLIOGIiAPHY 
30 1 
[HR82] 
[HS80] 
[HW90] 
[Lam 741 
[Lam 781 
[Lam 791 
[Lam861 
[Lea991 
[Lov73] 
[LSP82] 
[Lub85] 
[ LY 871 
G. S. Ho and C. V. Ramamoorthy. Protocols for deadlock detection in 
distributed database systems. IEEE Trans. on Software Engineering, 
8(6):554-557, November 1982. 
D. S. Hirschberg and J. B. Sinclair. Decentralized extrema-finding in 
circular configurations of processors. Commun. of the ACM, 23( 11):627- 
628, 1980. 
M. P. Herlihy and J. M. Wing. Linerizabilit,y: A correctness condition 
for atomic objects. ACM Trans. Prog. Lang. Syst., 12(3):463-492, July 
1990. 
L. Lamport. A new solution of dijkstra’s concurrent programming pro- 
gram. Commun. of the ACM, 17(8), August 1974. 
L. Lamport. Time, clocks, and the ordering of evenk in a distributed 
system. Commun. of the ACM, 21(7):558-565, July 1978. 
L. Lamport. How to make a correct multiprocess program execute cor- 
rectly on a multiprocessor. IEEE Trans. on Computers, 28(9):690-691, 
1979. 
L. Lamport. On interprocess communication, part 11: Algorithms. Dis- 
tributed Computing, 1:86-101, 1986. 
D. Lea. Concurrent Programming in Java: Design principles and Put- 
terns. The Java Series. Addison Wesley, Reading, MA, 2nd edition, 
1999. 
L. Lovasz. Coverings and colorings of hypergraphs. In 4th Southeastern 
Conf. on Combinatorics, Graph Theory, and Computing, pages 3-12, 
1973. 
L. Lamport, R. Shostak, and M. Pease. The Byzantine generals problem. 
ACM Trans. on Programming Languages and Systems, 4(3):382-401, 
July 1982. 
M. Luby. A simple parallel algorithm for the maximal independent set 
problem. In ACM, editor, Proc. of the 17th annual ACM Symp. on 
Theory of Computing, Providence, RI, pages 1-10, May 1985. 
T. H. Lai and T. H. Yang. 
Processing Letters, pages 153-158, May 1987. 
On distributed snapshots. Information 

302 
[LyngG] 
[Mae851 
[ Mat891 
[Mat 931 
[ hiG 951 
[ M G 981 
[bIS94] 
[NX95] 
[Pet 8 11 
[Pet,82] 
[PKR82] 
BIBLIOGRAPHY 
N. A. Lynch. Distributed Algorithms. Morgan Kaufmann series in data 
management systems. Morgan Kaufmann Publishers, Los Altos, CA 
94022, USA, 1996. 
M. Maekawa. A square root N algorithm for mutual exclusion in de- 
centralized systems. ACM Trans. Comput. Syst., 3(2):145- 159, May 
1985. 
F. Mattern. Virtual time arid global states of distributed systems. In 
Pa.ralle1 and Distributed Algorithms: Proc. of th,e Intnatl. Workshop on 
Parallel and Distributed Algorithms, pages 215-226. Elsevier Science 
Publishers B.V. (North-Holland), 1989. 
F. Mattern. Efficient algorithms for distributed snapshots and global 
virtual time approximation. Journal of Parallel and Distributed Com- 
puting, pages 423--434, August 1993. 
V. V. Murty arid V. K. Garg. An algorithm to guarantee synchronous 
ordering of nirsages. In Proc. of Second Intnatl. Symp. on Autonomous 
Decentralized Systems, pages 208 214. IEEE Computer Society Press, 
1995. 
N. Mittal and V. K. Garg. Consistency conditions for multi-object dis- 
tributed operations. In Proc. of the 18th Int’l Conf. on Distributed 
Computing Systems (ICDCS-18), pages 582-589, May 1998. 
K. Marzullo and L. S. Sabel. Efficient detection of a class of stable 
properties. Distributed Computirzg, 8(2):81-91, 1994. 
R. H. B. Netzer and J. Xu. Necessary and sufficent conditions for consis- 
tent global snapshots. IEEE Trans. on Parallel and Distributed Systems, 
6(2):165-~ 
169, February 1995. 
G. L. Peterson. Myths about the mutual exclusion problem. Information 
Processing Letters, 12( 3): 1 15--116, June 1981. 
G. Peterson. unidirectional algorithm for the circular extrema problem. 
ACM Trans. on Programming Languages and Systems, 4:758-762, 1982. 
J .  K. Pachl, E. Korach, and D. Rotem. A technique for proving lower 
bounds for distributed maximum-finding algorithms. In ACM Symp. on 
Theory 
Computing, pages 378-382, 1982. 

BIBLIOGRAPHY 
303 
[PSLSO] 
[PW95] 
[RAN] 
[Ray 881 
[Ray891 
[RHSO] 
[RSl91] 
[SKSS] 
[SK86] 
[Ske82] 
[SL87] 
[SM94] 
[SS94] 
M. Pease, R. Shostak, and L. Lamport. Reaching agreements in the 
presence of faults. Journal 
the ACM, 27(2):228 -234, April 1980. 
D. Peleg and A. Wool. Crumbling walls: a class of practical and efficient 
quorum systems. In Proc. 
the 14th Annual ACM Symp. on Princi- 
ples 
Distributed Computing (PODC '95), pages 120-129, New York, 
August 1995. ACM. 
G. Ricart and A. K. Agrawala. An optimal algorithm for mutual exclu- 
sion in computer networks. Commun. 
the ACM, 24(1):9 - 17, 1981. 
M. Raynal. Distributed Algorithms and Protocols. John Wiley & Sons, 
1988. 
K. Raymond. A tree-based algorithm for distributed mutual exclusion. 
ACM Trans. Comput. Syst., 7(1):61-77, February 1989. 
M. Raynal and J. M. Helary. Synchronization and Control 
Distributed 
Systems and Programs. Wiley, Chichester, UK, 1990. 
M. Raynal, A. Schiper, and S. Toueg. The causal ordering abstrac- 
tion and a simple way to implement it. Information Processing Letters, 
39(6):343-350, July 1991. 
I. Suzuki and T. Kasami. A distributed mutual exclusion algorithm. 
ACM Trans. Comput. Syst., 3(4):344-349, November 1985. 
M. Spezialetti and P. Kearns. Efficient distributed snapshots. In Proc. 
the 6th Intnatl. Conf. on Distributed Computing Systems, pages 382- 
388, 1986. 
D. Skeen. Crash Recovery in Distributed Database System. PhD Disser- 
tation, EECS Department, University of California at Berkeley, 1982. 
S. Sarin and N. A. Lynch. Discarding obsolete information in a replicated 
database system. IEEE Trans. on Software Engineering, SE-13( 1):39- 
47, January 1987. 
ed. S. Mullender. Distributed Systems. Addison-Wesley, Reading, MA, 
1994. 
M. Singhal and N. G. Shivaratri. Advanced Concepts in Operating Sys- 
tems. McGraw Hill, New York, NY, 1994. 

304 
BIBLIOGRAPHY 
[SS95] 
[Te194] 
[TG93] 
[Tho79] 
[TVS02] 
[VD92] 
[Wan 971 
[YM94] 
S. D. Stoller and F. B. Schneider. Faster possibility detection by combin- 
ing two approac:hes. In Proc. of the 9th Intnatl. Workshop on Distributed 
Algorithms, pages 318-332, France, September 1995. Springer-Verlag. 
R. €2. Strom and S. Yemeni. Optiniist,ic recovery in distributed systems. 
ACM Trans. Comput. Syst., 3(3):204-226, 1985. 
R.. N. Taylor. Coniplexit,y of analyzing the synchronization structure of 
concurrent programs. Acta Informatica, 19( 1):57-84, April 1983. 
K. Taylor. The role of inhibition in asynchronous consistent-cut protv 
cols. In Workshop on Distributed Algorithms, pages 280 ~291. Springer 
Verlag, LNCS 392, 1989. 
G. Tel. Introduction to Distributed Algorithms. Cambridge IJniversity 
Press, Cambridge, England, 1994. 
A. I. Tonilinson and V. K. Garg. Detecting relational global predicates 
in distributed systcnis. In PTOC. of the Workshop on Parallel and Dis- 
tributed Debugging, pages 21--31, San Diego, CA, May 1993. 
R. H. Thomas. A majority consensus approach to concurrency con- 
trol for niultiple copy databases. ACM Tram. on Database Systems, 
4(2):180-209, .June 1979. 
A. S. Tanenbaum and M. van St,een. Distributed systems: principles arid 
paradigms. Prcnt2ice Hall, 2002. 
S. Venkatesaii and B. Dathan. Testing and debugging distributed pro- 
grams using global predicates. In 30th Annual Allerton Conf. on Com- 
mun., Control and Computing, pages 137-146, Allerton, Illinois, Octo- 
ber 1992. 
Y. M. Wang. Consist.ent global clieckpoints that cont,ain a given set of 
local checkpoints. IEEE Transactions on Computers, 46(4), April 1997. 
Z. Yang and T. A. Marsland. Introduction. In Z. Yang and T. A. 
Marsland, editors, Global State and Time in Distributed Systems. IEEE 
Coniput,er Socicty Press, 1994. 

Index 
a synchronizer, 226 
synchronizer, 230 
synchronizer, 230 
abort, 233 
ACID properties, 254 
agreement, 240, 248 
AlphaSynch.java, 229 
anonymous ring, 210 
asynchronous recovery, 272 
asynchrony of events, 235 
atomic, 66 
atomic snapshots, 76 
at,omicit,y, 254 
Attemptl.java, 20 
Attempt2.java, 21 
At tempt 3 .java, 2 1 
bakery algorithm, 24 
Bakery.java, 25 
barrier synchronization, 187 
BCell.java, 49 
binary semaphore, 31 
BinarySemaphore.java, 32 
bivalent, 236 
bivalent state, 79 
BoundedBuffer.java, 35 
BoundedBufferMonitor .j ava, 45 
broadcast, 213, 215 
busy wait, 31 
Byzantine failure, 239 
Byzantine General Agreement, 243 
Camera.java, 152 
CameraLinker .java, I58 
CameraTest,er .java, 160 
CamUser.java, 152 
causal consistency, 60 
causal ordering, 192, 193 
causal total order, 203 
CausalLinker.java, 195 
CausalMessage.java, 194 
CelLjava, 50 
Cent Mutex. j ava, 134 
centralized algorithm, 203 
Cent Sensor. j ava, 167 
C hang-Roberts algorithm, 2 10 
Chat.java, 197 
checker process, 166 
checkpoint, 263 
checkpointing, 268 
CircToken.java, 145 
clocks, 111, 115 
clustering, 230 
commit, 233 
common knowledge, 247 
communication-induced checkpointing, 
264, 267 
commute property, 235 
CompS wap. j ava, 82 
CompS wapconsensus .j ava, 83 
concurrent, 11 5 
concurrent object, 53 
concurrent queue, 86 
concurrent system, 54 
305 

306 
INDEX 
condition variables, 42 
condit,iorial synchronization, 33 
coriflict, graph, 140 
Conriec t.or. j ava, 1 02 
consensus, 78, 233, 239 
consensus number, 79 
Consensus.java, 78, 242 
ConsensiisTester.java, 243 
consistency, 254 
consistent cut, 166 
consistent inkrval, 185 
convergecast, 215 
coordinated checkpoints, 263 
coordinating general problem, 248 
counting semaphores, 32 
CountingSemaphore.java, 33 
CQueuejava, 86 
crash, 239 
critical region, 18 
crit,ical section, 18, 130 
critical st,at,e, 79 
crumbling wall, 144 
Datagram Client,. j ava, 95 
datagrams, 92 
DatagraniServer.java, 93 
Datagramsocket, 90 
deadlock, 49, 209 
deadlocks, 188 
debugging, 164 
Dekker.java, 29 
diffusing computation, 180 
Dijkstra and Scholten’s algorithm, 180 
dining philosopher, 39 
DiningMonitor.java, 47 
DiningPhilosopher.java, 41 
DinMutex.java, 143 
direct-dependency clocks, 122 
DirectClock.java, 122 
directly precedes, 123 
disjunctive normal form, 164 
disjunctive predicate, 165 
distributed commit, 257 
distributed Computation, 11 1, 114 
distributed database, 233 
distributed objects, 196 
distributed systems, 1 
DistSensor.java, 174 
domain name system, 89 
domino effect, 264 
DS’I’erm.java, 183 
durability, 255 
Election.java, 209 
cvents, 114 
failure detection, 112 
fairness, 130 
fault-tolerant vector clock, 270 
Fibonacci.java, 14 
FIFO consistency, 62 
Fixed dependency after send, 268 
flooding algorithm, 213 
FooBar.java, 12 
fork, 9 
FuncUser.java, 21 7 
global checkpoint, 263 
global functions, 215 
global properties, 164 
global snapshot, 151 
global snapshot algorithm, 149 
global state, 150 
GlobalFunc.java, 218 
GlobalFuncTester.java, 21 9 
GlobalService.java, 216 
grid quorum system, 146 
happened-before diagrams, 115 
happened-before model, 150 
happened-before relation, 114 

INDEX 
307 
HelloWorldThread.java, 11 
history, 54 
HWMutex.java, 28 
InetAddress, 89 
initial independence, 234 
interface definition language, 8 
interleaving model, 114 
invocation, 54 
isolation, 254 
KingBGA.java, 245 
knowledge, 244 
Larnport's Algorithm for Total Order, 
LaniportClock.java, 117 
LamportMutex.java, 137 
leader election, 209 
legal, 54 
lightweight processes, 9 
linearizable, 57 
Linker, 100 
Linker.java, 104 
ListenerThread.java, 132 
ListQueiie.java, 48 
liveness, 130 
LLSC.java, 84 
locally stable, 185 
location transparency, 101 
lock-free, 65 
Lock.java, 18, 130 
LockFreeSnapshot .java, 77 
LockTest er.java, 131 
logging, 256 
logical clock, 116 
204 
majority voting system, 144 
marker, 152 
matrix clock, 125 
MatrixClock.java, 124 
maximum recoverable state, 276 
message ordering, 191 
minimal rollback, 276 
monitor, 42 
MRMW Register, 74 
MRSW Register, 73, 74 
MRSW.java, 75 
Msg.java, 103 
MsgHandler.java, 130 
multicast messages, 203 
MultiValued. j ava, 72 
MultiWriter .j, 76 
mutual exclusion, 18, 129, 203, 280 
MyThread.java, 19 
name server, 96 
Name.java, 99 
NameRmiClient.java, 109 
NameServer.java, 98 
NameService.java, 105 
NameServiceImpl.java, 106 
NameTable.java, 97 
nondeterminism, 191 
normal, 62 
object serialization, 107 
ObjPointer.java, 83 
occurred before, 54 
omission model, 239 
optimistic logging, 269 
orphan, 270 
parallel system, 1 
pessimistic logging, 268 
Peterson's algorithm, 21 
Peterson Algorit hm. j ava, 22 
Philosopher.java, 40 
pointer-swinging, 84 
private workspace, 256 
process, 8 
process-time diagrams, 115 

308 
INDEX 
Process.java, 133 
producer-consumer problem, 33 
ProducerConsumer.java, 37 
progress. 22 
pulse, 221 
quorum, 144 
R.AMu tex. j 
1 39 
reader-writer problem, 36 
ReaderWriter.java, 38 
receive omission, 239 
RecvCaniera.java, 155 
regular, 66 
Regular SRSW Register, 70 
RegularBoolean.java, 71 
reliable cornmiinicat,ion, 246 
reniok rnethod invocat,ions, 101 
remote procedure calls, 101 
replicat’ed state machines, 205 
Resource.java, 4 1 
response, 54 
restarts, 270 
Rest.art,Task.java, 282 
Ricart and Agrawala’s algorithm, 136 
ring, 209 
RingLeader.java, 21 1 
rrriirq$hy, 107 
rollback, 270 
rollback-dependency trackability, 267 
run. 115 
safe, 66 
Safe SRSW Register, 70 
SafeBoolean java, 70 
safety, 130 
self-stabilizing, 279 
semaphore , 3 1 
send omission, 239 
SenderCanrera.java, 159 
Sensor.java, 165 
SensorCircToken.java, 170 
SensorTester.java, 171 
St:nsorUser.java, 165 
SeqMessage.java, 156 
SeqQueue.java, 85 
sequential consistency, 55 
sequential history, 54 
serializabilit,y, 255 
shared clock, 11 2 
shared memory, 112 
Shortest.Path.java, 179 
siniple synchronizer, 223 
SimpleSynch.java, 224 
Skeen’s algorithm, 204 
space-time diagrams, 115 
spanning tree, 181, 216, 285 
SpanTree .j ava, 2 14 
SRSW Boolean Register, 71 
SRSW Multivalued Register, 71 
SRSW Register, 73 
st,able predicate, 163 
StableBottom.java, 283 
StableNormaLjava, 284 
StableSpanNonroot .jam, 287 
StableSpanRoot.java, 286 
S tableTreeTest er. j ava, 288 
starvation-freedom, 22 
symmetry, 210 
Synch.java, 28 
SynchBfsTree.java, 227 
SynchLinker.java, 202 
synchronizer, 221 
Synchronizer.java, 222 
synchronous ordering, 196 
Tc>rmDet ector .java, 179 
terminating reliable broadcast, 238 
temiination, 181, 240 
TmnS hort &Pat h .j ava, 1 84 
TermToken. java, 186 

INDEX 
309 
TestAndSet.java, 27, 81 
TestSetConsensus.java, 82 
threads, 9 
time-based model, 150 
Topology.java, 100 
total order, 203 
transaction, 233, 253 
Transniission Control Protocol, 90 
tree-based quorum system, 147 
Tree.java, 226 
triangular quorum system, 146 
two-phase commit, 257 
two-phase locking, 256 
TwoPhaseCoord.java, 259 
TwoPhaseParticipant.java, 260 
universal, 78 
universal construction, 84 
Universal Datagram Protocol, 90 
universal object, 79 
validity, 240 
VCLinker.j ava, 120 
vector clock, 118 
VectorClock.java, 119 
version end-table, 272 
wait, 9 
wait-for graph, 188 
wait-free, 65 
weak conjiinctive predicate, 166 
wheel coterie, 146 
z-consistent, 265 
zigzag relation, 265 


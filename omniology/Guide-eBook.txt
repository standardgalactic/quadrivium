Professional
Topic
Thinking Skills
Subtopic
Professor Steven Novella
Yale School of Medicine
Your Deceptive Mind: 
A Scientific Guide  
to Critical Thinking Skills 
Course Guidebook

PUBLISHED BY:
THE GREAT COURSES
Corporate Headquarters
4840 Westﬁ elds Boulevard, Suite 500
Chantilly, Virginia 20151-2299
Phone: 1-800-832-2412
Fax: 703-378-3819
www.thegreatcourses.com
Copyright © The Teaching Company, 2012
Printed in the United States of America
This book is in copyright. All rights reserved. 
Without limiting the rights under copyright reserved above,
no part of this publication may be reproduced, stored in 
or introduced into a retrieval system, or transmitted, 
in any form, or by any means 
(electronic, mechanical, photocopying, recording, or otherwise), 
without the prior written permission of
The Teaching Company.

i
Steven Novella, M.D.
Academic Neurologist
Yale School of Medicine
P
rofessor Steven Novella is an Academic 
Neurologist at Yale School of Medicine. 
He is active in medical education at every 
level of experience, including patients, the public, 
medical students, and continuing education for 
medical professionals. He also performs clinical 
research in his specialty area, publishing on 
amyotrophic lateral sclerosis, myasthenia gravis, and neuropathy. 
Dr. Novella received his M.D. from Georgetown University and went on 
to complete residency training in neurology at Yale School of Medicine. 
He is also trained and board certi¿ ed in the subspecialty of neuromuscular 
disorders, which continues to be a focus of his practice. Although he treats 
all types of neurological disorders, his clinical focus includes headaches and 
diseases of nerves and muscles. 
Dr. Novella is the president and cofounder of the New England Skeptical 
Society, a nonpro¿ t educational organization dedicated to promoting the 
public understanding of science. He is also the host and producer of their 
popular weekly science podcast, The Skeptics’ Guide to the Universe. This 
award-winning science show (winner of the People’s Choice Podcast Award 
in education for 2009 and in science for 2010–2011) explores the latest 
science discoveries, the presentation of science in the mainstream media, 
public understanding and attitudes toward science, philosophy of science, 
and critical thinking. Dr. Novella has also recorded Medical Myths, Lies, 
and Half-Truths: What We Think We Know May Be Hurting Us with The
Great Courses.
Dr. Novella was appointed in 2009 as a fellow of the Committee for Skeptical 
Inquiry, an international organization dedicated to the promotion of science 
and reason; he writes a regular column for their publication, the Skeptical 
Inquirer. Dr. Novella was also appointed in 2011 as the senior fellow of the 

ii
James Randi Educational Foundation and director of their Science-Based 
Medicine project. Dr. Novella maintains a personal blog, the award-winning 
NeuroLogica Blog, which is considered one of the top neuroscience blogs. 
On NeuroLogica Blog, he covers news and issues in neuroscience but 
also general science, scienti¿ c skepticism, philosophy of science, critical 
thinking, and the intersection of science with the media and society.
Dr. Novella is the founder and senior editor of Science-Based Medicine—a 
group medical and health blog with contributions from dozens of physicians 
and scientists. Science-Based Medicine is dedicated to promoting the highest 
standards of both basic and clinical science in medical practice. This proli¿ c 
health blog is geared toward both the general public and health professionals. 
Science-Based Medicine is recognized as a top health blog and is increasingly 
inÀ uential in the ongoing discussion of the role of science in medicine. Ŷ

iii
Table of Contents
LECTURE GUIDES
INTRODUCTION
Professor Biography ............................................................................i
Course Scope .....................................................................................1
LECTURE 1
The Necessity of Thinking about Thinking ..........................................4
LECTURE 2
The Neuroscience of Belief ..............................................................12
LECTURE 3
Errors of Perception..........................................................................20
LECTURE 4
Flaws and Fabrications of Memory...................................................28
LECTURE 5
Pattern Recognition—Seeing What’s Not There ..............................36
LECTURE 6
Our Constructed Reality ...................................................................44
LECTURE 7
The Structure and Purpose of Argument ..........................................53
LECTURE 8
Logic and Logical Fallacies ..............................................................61
LECTURE 9
Heuristics and Cognitive Biases  ......................................................69

Table of Contents
iv
LECTURE 10
Poor at Probability—Our Innate Innumeracy ....................................77
LECTURE 11
Toward Better Estimates of What’s Probable ...................................85
LECTURE 12
Culture and Mass Delusions.............................................................92
LECTURE 13
Philosophy and Presuppositions of Science...................................100
LECTURE 14
Science and the Supernatural ........................................................108
LECTURE 15
Varieties and Quality of Scienti¿ c Evidence ...................................116
LECTURE 16
Great Scienti¿ c Blunders ................................................................124
LECTURE 17
Science versus Pseudoscience ......................................................132
LECTURE 18
The Many Kinds of Pseudoscience ................................................140
LECTURE 19
The Trap of Grand Conspiracy Thinking .........................................148
LECTURE 20
Denialism—Rejecting Science and History ....................................156
LECTURE 21
Marketing, Scams, and Urban Legends .........................................164
LECTURE 22
Science, Media, and Democracy ....................................................172

Table of Contents
v
LECTURE 23
Experts and Scienti¿ c Consensus ..................................................180
LECTURE 24
Critical Thinking and Science in Your Life ......................................187
Glossary .........................................................................................195
Additional References by Lecture...................................................209
Bibliography ....................................................................................218
SUPPLEMENTAL MATERIAL

vi

1
Your Deceptive Mind:
A Scienti¿ c Guide to Critical Thinking Skills 
Scope:
M
uch of what we remember and believe is À awed or simply wrong. 
Our brains seem to constantly generate false observations, 
memories, and beliefs—and yet we tend to take the truth of our 
experiences for granted. In this course, you will learn the many ways in 
which our human brains deceive us and lead us to conclusions that have 
little to do with reality. You will also learn strategies that can be used to 
combat the mind’s many deceptions. This course explores what is called 
metacognition: thinking about thinking itself. 
The ¿ rst part of the course will cover the way we perceive the world 
around us. Everything we think we see, hear, and experience is not a direct 
recording of the outside world; instead, it is a construction. Information is 
¿ ltered, distorted, compared, and confabulated—ultimately to be woven into 
a narrative that ¿ ts our assumptions about the world. Our experiences and 
thoughts are also ¿ ltered through our egos and the many emotional needs 
humans constantly feed. 
Furthermore, everything we think and experience becomes a memory, which 
is further constructed, altered, and fused. We rely upon our memories as if 
they were accurate recordings of the past, but the evidence shows that we 
should be highly suspicious of even the most vivid and con¿ dent memories. 
We don’t recall memories as much as we reconstruct and update them, 
altering the information every time we access it. Our brains also ¿ ll in gaps 
by making up information as needed. 
Additionally, a host of logical À aws and cognitive biases plague our thinking, 
unless we are speci¿ cally aware of and avoid those fallacies. In this course, 
you will explore logical fallacies and cognitive biases in detail, learning how 
they affect thinking in often subtle ways. You will also learn about heuristics, 

2
Scope
which are mental shortcuts we tend to take in thinking; these shortcuts may 
be ef¿ cient in most circumstances, but they can also lead us astray. 
Our brains have other interesting strengths and weaknesses that can further 
inform our thinking. We are generally very good at pattern recognition—so 
good that we often see patterns that are not actually there. However, many of 
us are inherently poor at probability and statistics, and this innumeracy opens 
us up to deception and errors in thinking. Perhaps our greatest weakness 
is our susceptibility to delusion, the ability to hold a false belief against
all evidence. 
The second part of the course goes beyond how our brains distort reality 
to discuss how you can speci¿ cally use critical thinking skills and tools to 
combat the deceptions of your mind. The philosophy and practice of critical 
thinking and science are the tools that humans have slowly and carefully 
honed over many millennia to compensate for the many À aws in our brains.
In addition, the second section covers the history of science and discusses 
how to tell the difference between good science, bad science, and 
pseudoscience that is so À awed that it’s not real science. In this section, 
you will encounter many examples of pseudoscience in which various 
attempts at new discoveries went wrong. The lecture on scienti¿ c blunders 
also discusses great scienti¿ c mistakes in history and the lessons that can be 
learned from them. 
In the ¿ nal section of the course, you will learn how to apply critical thinking, 
knowledge of science, and knowledge of the mechanisms of self-deception 
to everyday practice. Then, you will discover the role of science and critical 
thinking in democracy, the need for high-quality science education, and how 
to skeptically approach the media. This section will partly be a primer on 
how not to get scammed or fooled. 
By the end of the course, you will have a thorough understanding of what 
constitutes critical thinking and why we all so desperately need it. Left to our 
own devices—what psychologists call the default mode of human thinking—
we will be subject to the vagaries of perception and memory and slaves to 
our emotional needs and biases. 

3
The skills taught in this course will help you operate on the metacognitive 
level so that you are able to think about the process of your own thinking. 
The human brain is the universal tool by which we understand ourselves 
and the universe in which we live. By understanding the nature of human 
cognition and the methods of thinking clearly and critically, we can avoid 
common errors and make the best use of our minds. Ŷ

4
Lecture 1: The Necessity of Thinking about Thinking
The Necessity of Thinking about Thinking
Lecture 1
T
his course focuses on metacognition, or thinking about thinking 
itself, and it endeavors to give you the skills of critical thinking. 
Developing critical thinking skills is empowering and liberating, 
and it is a defense mechanism against the world that we live in. In this 
introductory lecture, the concept of metacognition will be introduced, and 
you will learn why it is necessary. In addition, an overview of the purpose of 
this course will be given with examples of the importance of critical thinking 
in everyday life.
Logic and Critical Thinking
x
Science and belief permeate our lives; they permeate our culture and 
our civilization. We buy products every day that involve claims—
either explicit or implicit—and we need to be able to evaluate those 
claims in order to make good purchasing decisions. 
x
We use critical thinking in order to think about how we run our 
civilization. We have to purchase health-care products and decide 
what foods to eat and what lifestyle changes to make in order to 
stay healthy. These claims are based upon evidence and logic, and 
we need critical thinking to be able to evaluate them properly. 
x
One of the premises of this course is that we are our brains. In 
essence, the brain is an organ that can think and is self-aware. It is 
not only the most complicated organ that we know about, but it may 
in fact be the most complicated thing in the universe that we know 
about. The brain can remember, feel, believe, calculate, extrapolate, 
infer, and deduce. It does everything that we think of as thinking.
x
The brain is our universal tool and greatest strength. Most people 
believe that our intelligence is our greatest advantage over all the 
other creatures on this planet. However, the brain is also strangely 
deceptive and is the root of many of our À aws and weaknesses. 

5
x
This course will also explore human nature. Humans possess logic, 
but we are not inherently logical creatures. In addition to being 
logical, we are also highly emotional creatures; we tend to follow our 
evolved emotions and rationalizations. Our thoughts tend to follow a 
pathway of least resistance, which is not always the optimal pathway.
x
Logic and critical thinking are, therefore, learned skills. While 
we have some inherent sense of logic, we are overwhelmingly 
emotional creatures. We have the capacity for logic, but logic 
and critical thinking are skills. We’re not born as master critical 
thinkers—just as we’re not born as violinists. Both are skills that 
need to be developed and practiced over many years. 
Flaws in Human Thinking
x
The inherent tendency of humans is to make many errors in thinking. 
One example is À aws in logic, which are called logical fallacies, in 
which we tend to make logical connections that are not valid, or real.
x
Our thinking is also plagued with many false assumptions. Our 
heads are ¿ lled with knowledge that we think is true but is, in fact, 
false. Either these bits of knowledge are simply wrong, or they’re 
assumptions that fall short of the truth.
x
Our memories are also massively À awed. We tend to naively assume 
that our memories are an accurate, passive recorder of what has 
happened, but our memories are actually plagued with numerous 
À aws that make them highly unreliable.
x
In psychology, heuristics are patterns of thinking. They’re mental 
shortcuts that we tend to take that may be right much of the time 
but are wrong often enough that they quite frequently lead us astray. 
x
We compensate for all of these À aws in our brain’s functioning 
by using metacognition, or thinking about thinking itself. A 
process called scienti¿ c skepticism involves systematic doubt—
questioning everything that you think, the process of your thinking, 
and everything that you think you know.

6
Lecture 1: The Necessity of Thinking about Thinking
x
One component of critical thinking is basing your beliefs on actual 
evidence as opposed to wishful thinking, for example. The goal is 
to arrive at conclusions that are likely to be reliable as opposed to 
conclusions that are unreliable, but we also want to have a sense of 
how reliable our conclusions are.
x
The scienti¿ c method is scienti¿ c skepticism—not just doubt, but a 
positive set of methods for examining reality. Essentially, science is 
a systematic way of comparing our ideas to external, objective data.
x
In short, the goal of science is to lead us to conclusions that are 
actually true as opposed to conclusions that we simply wish are 
true. However, not all science is valid. Some science is so À awed 
that we call it pseudoscience.
x
Science follows scienti¿ c methodology. It is not a set of beliefs, but 
it is a set of methods, and there are ways of de¿ ning that as well as 
distinguishing good science from bad science.
x
The scienti¿ c method is based upon methodological naturalism, 
which is the philosophical term for the notion that natural effects 
have natural causes. In trying to model and understand the world, 
you cannot refer to supernatural or miraculous causes that don’t 
have any testable cause in the natural world.
x
All conclusions in science are provisional; there is no such thing 
as absolute metaphysical certitude. Not only do we have to assess 
what is likely to be true but also how con¿ dent we can be about that 
belief, knowing that we’ll never quite reach absolute certainty. 
x
All of our beliefs are open to revision. When new data comes in, or 
maybe just a better way of interpreting data, we have to be open to 
revising what we thought we knew. 
x
Human beings are subject to delusions. Sometimes our thinking 
goes so far awry that we can invent our own reality or become 

7
swept up in the beliefs of others. One common manifestation of this 
is a public panic.
x
It’s helpful to consider thinking as a process and to focus on the 
process rather than on any particular conclusion. Once we emotionally 
invest in a conclusion, humans are very good at twisting and 
rationalizing 
facts 
and logic in order 
to ¿ t that desired 
conclusion. Instead, 
we should invest in 
the process and be 
very À exible when 
it comes to any 
conclusions.
x
In addition, we are 
currently living not 
only in the age of 
information 
with 
the Internet, but we 
are living in the age 
of misinformation. 
There are many rumors that now spread faster than wild¿ re; they 
spread with the speed of electrons through the Internet.
x
Whether they’re innocent or malicious, myths are spread through 
the Internet in order for the people behind the myths to try to steal 
other people’s money, lure them into a scam, or even inÀ uence
their voting. 
x
We live in a capitalistic society, which means that every day 
we’re subject to marketing claims that are highly motivated to 
misrepresent the facts or to give us a very speci¿ c perspective. Such 
claims try to inÀ uence our thoughts and behavior by engaging in 
persuasive speech and maybe even deception.
On a daily basis, we need to use critical 
thinking to evaluate our decisions—such as 
which products to purchase.
© Hemera/Thinkstock.

8
Lecture 1: The Necessity of Thinking about Thinking
x
As consumers, every day we have to sort through deliberately 
deceptive claims to ¿ gure out which ones are reliable and which 
ones aren’t. Furthermore, many companies use pseudoscience or 
even antiscienti¿ c claims to back up their marketing and products, 
and that can seem very persuasive to someone who isn’t skilled in 
telling real science from pseudoscience. 
Thinking Critically
x
Thinking critically is a process, and the ¿ rst component is to 
examine all of the facts that you are assuming or that you think 
are true. Many of them may not be reliable, or they may be 
assumptions. You may not know whether they’re true, but you’re 
assuming they’re true. 
x
You also need to examine your logic. Is the logic you’re using 
legitimate, or is it À awed in some way? Perhaps it’s systematically 
biased in a certain direction.
x
In addition, you should try to become aware of your motivations. 
People are extremely good at rationalizing beliefs when they are 
motivated by a desire to believe a certain conclusion. Understanding 
your motivations will help you deconstruct that process and will 
give you the skills to discover conclusions that are more likely to be 
true, as opposed to the ones that you just wish to be true.
x
Critical thinking also means thinking through the implications 
of a belief—that different beliefs about the world should 
all be compatible with each other. We have a tendency to 
compartmentalize, to have one belief walled off from all of our 
other beliefs, and therefore we insulate it from refutation. If you 
think about what else has to be true if a certain belief is true and 
whether both make sense, that is a good way to tell how plausible 
or how likely to be true a belief is. 
x
Additionally, you should check with others: No matter how 
developed your critical thinking skills are, you’re still only one 
person, whose thinking is quirky and individual.

9
x
You have a limited fund of knowledge and a limited perspective. In 
fact, your knowledge and perspective may be limited in ways that 
you’re not aware. You don’t know what you don’t know. Therefore, 
if you check your beliefs with others, it increases the probability 
that any holes in your thinking will be covered up.
x
When a large consensus on a speci¿ c claim is achieved, there’s a 
greater chance that the consensus reÀ ects reality than the process 
of an individual. A consensus may be systematically biased as well, 
but at least you’re stepping out of the limitations of your knowledge.
x
It’s also important to be humble, which means knowing your limits. 
We tend to get into trouble when we assume we have expertise or 
knowledge that we don’t have or when we don’t question the limits 
of our knowledge.
x
In addition, be comfortable with uncertainty. There are some things 
that we simply cannot know or that we currently do not know. There 
may be times when, after reviewing all the logic and evidence, our 
only conclusion is that we currently don’t know.
x
Critical thinking is a skill that can be learned and that can be 
reinforced by habit. The scienti¿ c approach to critical thinking is 
empirical. It is a way of testing our beliefs systematically against 
the real world. Once we develop our critical thinking skills
and begin to examine our beliefs systematically, it can be
extremely empowering. 
x
Critical thinking is, in fact, a defense mechanism against all the 
machinations that are trying to deceive us—whether for ideological, 
political, or marketing reasons. Critical thinking also liberates us 
from being weighed down by the many false beliefs, and perhaps 
mutually incompatible beliefs, that we tend to hold because of our 
emotional makeup. 

10
Lecture 1: The Necessity of Thinking about Thinking
critical thinking: Applying systematic logic and doubt to any claim or 
belief; thinking carefully and rigorously.
delusion: A ¿ xed, false belief that is vigorously held even in the face of 
overwhelming contradictory evidence. 
heuristic: A cognitive rule of thumb or mental shortcut that we 
subconsciously make that may be true much of the time but is not
logically valid. 
logic: A formal process or principle of reasoning.
metacognition: Thinking about thinking; examining the processes by which 
we think about and arrive at our own beliefs. 
methodological naturalism: The philosophical assumptions that underlie 
scienti¿ c methodology; speci¿ cally, the assumption that all effects have 
natural causes. 
pseudoscience: A practice that super¿ cially resembles the process of science 
but distorts proper methodology to the point that it is fatally À awed and does 
not qualify as true science. 
scienti¿ c skepticism: A comprehensive approach to knowledge that 
emphasizes critical thinking and science. Skepticism combines knowledge 
of philosophy of science, scienti¿ c methods, mechanisms of self-
deception, and related ¿ elds to approach all claims to truth in a provisional
and systematic way. 
valid: An argument in which the logic is proper and not fallacious. 
    Important Terms

11
Gilovich, How We Know What Isn’t So. 
Sagan, The Demon-Haunted World. 
Shermer, Why People Believe Weird Things. 
1. Why is critical thinking important to the average person—and to society 
as a whole?
2. What are the neurological, psychological, and cultural barriers to
critical thinking?
    Suggested Reading
    Questions to Consider

12
Lecture 2: The Neuroscience of Belief
The Neuroscience of Belief
Lecture 2
T
his lecture will cover why people believe what they do. Humans 
are emotional creatures, and this has a powerful effect on our 
reasoning. In this lecture, you will learn about the neurological 
organization of the brain and how that relates to how you rationalize beliefs 
and are inÀ uenced by basic human desires and emotions. Additionally, 
you will learn what drives this human desire for belief and for the speci¿ c 
things you believe in. The hope is that by understanding what motivates 
humans, you will be able to transcend or at least mitigate the inÀ uence of
those motivations.
Belief, Motivation, and Reason
x
Our brains are belief machines. We are motivated to believe, 
especially those things that we want to believe.
x
The default mode of human psychology is to arrive at beliefs for 
largely emotional reasons and then to employ our reason—more 
to justify those beliefs than to modify or arrive at those beliefs in 
the ¿ rst place. Therefore, in many ways, we are slaves to our own 
emotions if we let ourselves be. 
x
It is helpful to try to understand this interaction between belief, 
motivation, and reason in the context of microanatomy, or 
understanding the way our brains are organized.
x
The most recently evolved parts of our brain, speci¿ cally the 
frontal lobe portion of the neocortex, hierarchically can modify and 
control the earlier evolved, more primitive parts of our brain. The 
brain stem is the area associated with the most primitive functions.
x
In addition to the most basic functions, such as breathing and 
maintaining balance while we walk, much of our cognition takes 

13
place in our subconscious, or in the more primitive parts of our 
brain, which are also where our emotions are housed.
x
Emotions essentially make quick decisions for us that are mostly 
adaptive, evolved strategies, including fear, lust, hunger, anxiety, 
disgust, happiness, and sadness. The idea is that emotions provide 
a direct behavioral motivation so that we don’t have to calculate 
the risks of encountering a predator versus À eeing, for example. 
We simply experience the emotion of fear, and then we act upon
that emotion. 
x
Psychologist Abraham Harold Maslow made perhaps the ¿ rst 
attempt to classify the different emotional needs that people have, 
which are now known as Maslow’s hierarchy of needs. In addition 
to basic emotions, we also have a set of higher psychological needs 
that we seek: We desire to be safe, to be loved, to have self-esteem, 
and to experience what Maslow called self-actualization. 
x
The primitive parts of our brain can experience hunger, but only our 
much more evolved neocortex can experience an emotion like the 
need for self-actualization. 
x
When we meet our psychological needs, our brain gives us a 
reward: It makes us feel good, which is another emotion. There is 
a basic punishment-and-reward system built into the hardwiring of 
our brain. When we do something that is likely to be advantageous 
evolutionarily, we feel good—we get a shot of dopamine to our 
reward centers.
Needs That Motivate
x
The desire for control, or at least for the illusion or sense of control, 
is one need that motivates us. We don’t like to feel as if we are 
victims of a capricious universe or as if we are helpless in the face 
of unseen forces or randomness. We like to think that we exert some 
control over ourselves, over the events that happen to us, and over 
our environment.

14
Lecture 2: The Neuroscience of Belief
x
One manifestation of this desire for control is belief in superstitions. 
We tend to develop beliefs that if we engage in a certain activity, 
it will protect us or enable us to succeed. Superstitious practices 
give us the illusion that we can exert some control over otherwise 
random events. 
x
We also have a desire for simplicity because the simpler things 
are, the more control we can have over them. Therefore, we are 
motivated to oversimplify the things that we are confronted with. 
x
We stereotype because it enables us to boil down a very complicated 
set of data into some simple rule. This can be helpful and adaptive 
when we understand that the rule is just a schematic, or an 
oversimpli¿ ed representation of a much more complicated reality. 
However, accepting our oversimpli¿ ed versions of reality as reality 
leads to bigoted mindsets.
x
We also have a desire for the universe, and our lives, to have 
meaning. We want there to be an overarching meaning to our 
existence because it gives us a sense of purpose. We want to believe 
that things happen for a reason, but the reality is probably closer to 
the fact that stuff just happens. 
x
Related to this is our desire to believe that big effects must have 
big causes. We don’t like to think that there could be a massive 
consequence to a very innocuous or innocent cause.
x
Another need that we have is the need for self-esteem—the need to 
not only feel good about ourselves, but to feel that the others in our 
community respect us. This has largely to do with the fact that we 
are intensely social animals.
x
The need for self-esteem is often referred to as having an ego, and 
a certain amount of ego is very adaptive, but it also powerfully 
motivates us to interpret the world in a way that is favorable to
our ego.

15
x
In 
psychology, 
the fundamental 
attribution error 
is our tendency to 
look for external 
causes to explain 
our 
behavior. 
When we do this, 
we’re very good 
at 
rationalizing 
our 
behavior 
in 
order to protect our
self-esteem.
x
We also act to 
avoid 
social 
embarrassment or stigma. For example, we may avoid appearing 
inconsistent. We always want to make our behavior and beliefs 
seem consistent to others. 
x
We also have a very strong resistance to admitting error. We don’t 
like to admit that we’re wrong or to admit that we have À aws 
because that is a threat to our self-esteem and ego. 
x
Much of how various motivations affect our thoughts and behavior 
can be explained with a theory known as cognitive dissonance, 
which is a state of mind that is caused by the act of holding two 
beliefs at the same time that are mutually exclusive, or that conÀ ict 
with each other. We don’t like the feeling of cognitive dissonance, 
so it motivates us to resolve the conÀ ict. 
x
Initially, 
we 
may 
avoid 
cognitive 
dissonance 
through 
compartmentalization, in which we simply keep conÀ icting beliefs 
separate from each other, but when they are forced into conÀ ict, we 
need to resolve that conÀ ict somehow.
In addition to basic emotions, we also have a 
set of higher psychological needs, including 
the desire for self-actualization. 
© altrendo images/Stockbyte/Thinkstock.

16
Lecture 2: The Neuroscience of Belief
x
An adaptive way to resolve this conÀ ict is to update one or both 
beliefs, but what we tend to do instead is to rationalize the belief 
that we want to hold. We engage in a logical process called special 
pleading, in which we invent reasons to resolve the apparent conÀ ict 
between different beliefs. Humans are very good at inventing 
reasons to justify our beliefs. 
Senses That Motivate
x
Our sense of justice is hardwired into our brains. It’s not a learned 
sense; it is innate. In fact, animals also have been shown to have an 
innate sense of justice. For example, there is a species of birds that 
defends each other from predators, and they seem to have an innate 
sense of reciprocity, or of justice. 
x
Another sense that is inherent in humans is the sense of essence, 
or the notion that inanimate objects can carry the essence of their 
history. In fact, most cultures have a concept of spirituality. This 
innate sense of essence goes beyond inanimate objects, and we 
think that there is a spiritual force—whether it’s called animus, chi, 
or prana—that separates us from things that are nonliving.
x
In fact, we tend to categorize living from nonliving things in our 
brains, and we process information about living things differently 
than nonliving things. Speci¿ cally, living things are processed 
through the emotional centers of our brain; we imbue meaning and 
feeling to things that we think are alive in ways that we don’t do 
neurologically to things that are inanimate.
x
This division is not exactly between living and nonliving things. It’s 
actually between things that we think of as having agency, which 
means that something acts as if it has a will of its own, and things 
that we think of as not having agency. This explains, for example, 
why we so easily imbue meaning and emotions onto cartoon 
characters—because they’re acting as if they are people, which 
triggers the emotional hardwiring in our brains. 

17
x
We also have a sense of the supernatural—the belief that there is 
more to the world than what is immediately apparent. This ties 
back further to our need for a sense of connection, for there to be 
meaning, and for the profound. When we have a sense that we are 
connected to something that is profound, it feels good. This can 
then be reinforced by con¿ rmation bias, which involves seeking 
out data that seems to con¿ rm our beliefs. 
Motivation, Emotion, and Behavior
x
Psychologists have looked for ways to inÀ uence people’s behavior. 
One of many practical reasons that we might want to do this is for 
public service campaigns. 
x
When it comes to inÀ uencing others’ behavior, our initial instinct is 
to give people information, assuming that they will arrive through 
reason at the correct decision and behavior—but research has 
shown that this is not very effective.
x
It’s very dif¿ cult to change people’s behavior by making a rational 
argument to them because their behavior is still overwhelmed by 
their beliefs and by their emotions. However, if you address the 
individual’s emotions, that is much more effective.
x
It’s still dif¿ cult to get people to change old habits, but if you 
convince them by using social pressures, then this utilizes a 
technique called social norming. If you tell people, for example, 
that other people don’t drink and drive, that will have more of an 
impact on their behavior than telling them the reasons why they 
shouldn’t drink and drive. 
x
Children are very socially inept. Their brains have not fully 
developed, speci¿ cally their frontal lobes, which give us the 
ability to socialize—to plan our activity and to think about how 
our behavior will be perceived by others. Children have many of 
the same basic motivations and emotions that adults do, but they 
don’t have the social ¿ lter that we have in place, so their motives 

18
Lecture 2: The Neuroscience of Belief
are much more transparent. Therefore, children serve as a window 
into human emotions and motivations. 
x
Adults are better at hiding emotions and motivations. We’re better 
at rationalizing what we want to believe, and we’re better at 
putting a socially acceptable spin on our behavior. The underlying 
psychological motivations, however, are largely the same for adults 
as for children.
x
Ironically, highly intelligent people may not necessarily be better at 
making decisions, but they are better at rationalizing the decisions 
that they do make. Psychologists now recognize a separate skill 
set called emotional intelligence, which involves understanding 
the relationship between our motivations and our decisions—the 
tendency to relieve cognitive dissonance with rationalization.
cognitive dissonance: An unpleasant emotion generated by the simultaneous 
existence of mutually exclusive beliefs.
con¿ rmation bias: A cognitive bias to support beliefs we already hold, 
including the tendency to notice and accept con¿ rming information while 
ignoring or rationalizing discon¿ rming information. 
default mode: A common behavior that results from evolved emotions and 
subconscious processes without metacognitive insight. 
fundamental attribution error: A psychological term that refers to the 
tendency to ascribe the actions of others to internal motives and attributes 
rather than external situational factors. 
hierarchy of needs: The term coined by Abraham Maslow that describes the 
relationship among the basic and higher human needs—from physiological 
needs like food to emotional needs like self-actualization. 
    Important Terms

19
neocortex: The neocortex is the most recently evolved portion of the human 
brain—speci¿ cally, the frontal lobes, which provide executive function, 
among other things. 
subconscious: Brain processing that occurs without conscious awareness. 
Gazzaniga, The Mind’s Past.
Hood, Supersense.
Shermer, The Believing Brain.
Wiseman, 59 Seconds. 
1. How does knowledge of brain anatomy and function inform the process 
of our decision making?
2. Do humans have free will, or are all of our thoughts and behaviors the 
result of neurological destiny?
    Suggested Reading
    Questions to Consider

20
Lecture 3: Errors of Perception
Errors of Perception
Lecture 3
I
n order to understand the nature of human thinking and belief, it is 
necessary to understand how our brains acquire and process information. 
This lecture will begin this section on exploring the ways in which our 
underlying brain activity is deceptive and constructed by examining the 
nature of perception and all the ways in which humans can be deceived 
by what we think we perceive and by what we miss. This lecture will
cover phenomena such as attentional blindness, change blindness, and 
optical illusions.
Deceiving the Brain
x
You cannot trust anything you think you see or perceive. There 
are simply too many À aws in the ways our brain constructs
these perceptions.
x
Our perceptions are not passive. In fact, our brains actively 
construct a picture of what is going on around us based on a tiny 
fraction of all the sensory information that’s coming in, which 
introduces many opportunities for distortions and error.
x
Over centuries, magicians have learned how to exploit the foibles 
of the brain’s sensory processing. They have developed a practical 
knowledge of how to do things right in front of your face without 
you seeing or perceiving that these things occurred. They use this 
knowledge for entertainment, but it is possible for these same tricks 
to be used for nefarious purposes. 
x
Artists have also learned how to exploit the ways in which our 
brain processes information in order to create speci¿ c effects. 
For example, they have developed laws of perspective and 
dimensionality, resulting in the ability to create a three-dimensional 
image on a two-dimensional canvas. 

21
Optical Illusions
x
Perhaps the simplest demonstration of the constructed aspect of 
our sensory input is optical illusions. By de¿ nition, all types of 
illusions occur when the brain constructs sensory perception in an 
incorrect way, causing a misperception of reality.
x
Generally, objects in our world appear to be stable and accurate with 
respect to reality. Psychologists refer to this property as constancy.
x
Optical illusions, by de¿ nition, represent an exception to 
constancy—a time when objects either are not stable or do 
not accord with reality. This is because the brain has to make 
assumptions about what’s likely to be true, and then it processes the 
sensory information based on those assumptions. Most of the time, 
those assumptions are correct; when they’re not correct, that results 
in an optical illusion.
x
Constructed optical illusions are designed to speci¿ cally exploit 
known ways in which the brain processes information. There are 
different types of optical illusions, including perspective illusions, 
which exploit the ways in which our brains can construct three-
dimensional images out of two-dimensional input. 
x
There are optical illusions that are based on the relative shade and 
size of objects. There are also illusions that are based on ambiguous 
stimuli, in which the brain can construct an image in more than one 
way and switches back and forth between different constructions.
x
Additionally, there are afterimage optical illusions that the visual 
system will adapt to lighting and color, for example, and then when 
those are changed, the adaptation causes an afterimage that’s not 
really there. 
x
Other three-dimensional illusions that the brain makes assumptions 
about are size, distance, and relative position. These assumptions 
are based on what is usually correct, but when one or more of those 

22
Lecture 3: Errors of Perception
assumptions are incorrect, the brain can construct a false three-
dimensional image of reality.
x
There are also perceptual reversals, which have to do with 
ambiguous stimuli and your brain constructing images in different 
ways at different times. There is the classic cube illusion, for 
example, in which there is a two-dimensional drawing of a three-
dimensional cube and either face can be perceived as pointing 
toward you.
Seeing What’s Not There
x
Nobody is immune to optical illusions, so even being a trained pilot 
or observer does not make your brain construct things differently. 
x
For example, in Stephenville, Texas, on January 8, 2008, witnesses 
saw a UFO, which stands for unidenti¿ ed À ying object, that they 
reported was one mile long. Investigations revealed, however, that 
what they were seeing were actually À ares and that the witnesses 
constructed the UFO by connecting the dots of light.
x
When our brain is constructing our perception of the world around 
us, it will ¿ ll in lines or connections where it thinks they should be 
based on assumptions about what it thinks it’s seeing. 
x
Another example is the air raid over Los Angeles, California, 
that took place on February 25, 1942, during which over 1,400 
antiaircraft shells were ¿ red. Eventually, it was concluded that 
the air raid might have been triggered by something as innocuous 
as a weather balloon. Because it was the beginning of World War 
II, soldiers that were ¿ ring these shells thought they were seeing 
planes invading the United States. 
x
Another dramatic example of how our brain constructs a picture in 
our heads from available sensory input is the fact that the different 
senses can actually inÀ uence each other. Our brains will compare 
different types of sensory input in order to construct one seamless 

23
picture, and it will adjust one sense or the other in order to make 
things match.
x
Gender perception is also a combination of different types of 
sensory input. There are numerous studies that indicate that what 
we see in terms of gender is affected by the voice that we hear, 
especially when the visual input is ambiguous or androgynous.
x
This same research also shows that we respond better to congruous 
sensory input rather than incongruous input when the voice matches 
the face we see the sound coming from and the movements that 
are being made. Sometimes our brain will have dif¿ culty putting 
conÀ icting sensory input together. 
x
Another example of how our brain uses different types of sensory 
input in order to construct its picture is the McGurk effect, which 
describes the fact that the consonants we hear are affected by the 
mouth movements we see. In other words, our brain adjusts the 
sound in order to make 
it congruous with the lip 
movements that you see, so 
you hear different consonants.
x
There 
is 
also 
temporal 
synchronization. 
Activities 
that combine sounds and 
sights, such as clapping hands, 
seem to be simultaneous, 
but the light traveling from 
someone clapping his or 
her 
hands 
travels 
much 
faster than the sound waves 
propagate. Therefore, the two 
sensory inputs do not arrive 
at your sensory organs at 
exactly the same time. It also takes slightly different amounts of 
time for your brain to process those two types of information.
There are many À aws in the ways 
our brains construct what we think 
we see or perceive.
© iStockphoto/Thinkstock.

24
Lecture 3: Errors of Perception
x
However, when we look at someone clapping his or her hands, the 
two events appear to be simultaneous. This is a constructed illusion 
in your brain, which synchronizes the two events because it knows 
that they’re supposed to occur at the same time. In fact, as long as 
the visual and auditory information are within 80 milliseconds of 
each other, your brain will perceive them as being simultaneous. 
x
Synesthesia is a pathological phenomenon, which means that it 
occurs in some people but not in others. Synesthesia occurs when 
one sensory modality is interpreted by the brain as if it were a 
different sensory modality. Therefore, people with synesthesia 
may see numbers, hear light, or smell colors. It’s not clear whether 
synesthesia represents a bleeding over of information from one 
sensory area to an adjacent sensory area in the brain or if this is an 
alternate type of wiring in the brain. 
Paying Attention to Sensory Information
x
Attention is immensely important to the notion of perception 
because we are constantly bombarded with an overwhelming 
amount of sensory information, and we cannot possibly pay 
attention to even a signi¿ cant fraction of it—let alone all of it—at 
the same time. We ¿ lter out most sensory information that reaches 
us and pay attention to only what our brains deem to be important. 
x
In some experiments of attention, subjects watch a video and are 
instructed to focus on one type of activity that’s occurring in the 
video while at the same time, something else very dramatic is 
happening in the video. About 60 percent of people who watch the 
video will be completely unaware of dramatic events occurring 
right before their eyes simply because they were instructed to attend 
to a different part of the video. 
x
The amount of focus that we have can shift around. We can be 
focused on just one part of our visual ¿ eld, completely ignoring the 
peripheral vision, or we can be on the lookout, trying to diffuse our 
attention among a greater percentage of our visual ¿ eld.

25
x
We tend to shift around to different parts of our peripheral vision 
at different times, and when we do, the objects in that part of the 
visual ¿ eld are enhanced. Your brain will process more of the 
information coming from that part of the visual ¿ eld and will 
suppress information coming from other parts of the visual ¿ eld in 
order to prevent getting distracted by what it deems to be irrelevant 
information. Inattentional blindness describes the notion that we 
are blind to things that we are not attending to.
x
With a closely related phenomenon called change blindness, we do 
not notice sometimes even dramatic changes to our environment. 
For example, several experiments show that when interacting 
with an unfamiliar person, the person can change outside of your 
view and about 60 percent of the time subjects won’t notice that 
they’re talking to an entirely different person than they were before
the swap. 
x
An example of our ability to only attend to a small number of things 
at the same time is multitasking. The term multitasking is used to 
de¿ ne doing more than one thing at the same time, but research 
shows that people essentially cannot attend to more than one thing 
at the same time. 
x
In some experiments, it’s been demonstrated that people who think 
that they are good at multitasking actually suffer from multitasking 
the most; their performance decreased the greatest when they 
attempted to multitask. 
x
Eyewitness testimony is notoriously unreliable. Eyewitnesses are 
subject to suggestion; they have a false con¿ dence in their own 
accuracy and are subject to confabulation, or making up details. 
Our brains construct a consistent reality out of what it thinks it 
perceives that involves subconsciously ¿ lling in missing pieces. 
x
Perception is a construct—it is something that is happening inside 
your brain. Each individual sensory stream is interpreted and 
modi¿ ed: The different streams are combined, compared, and 

26
Lecture 3: Errors of Perception
then altered based on that comparison. We attend to a very small 
subset of that information, which we weave into a complete story 
by adding confabulating pieces as needed. The end result is a story 
that is largely ¿ ction.
x
Sensory input is constructed into meaningful patterns. Not only 
are the components of what we perceive constructed, but also how 
we put our perceptions together into a meaningful way is also 
constructed. This will often result in unnecessary arguments from 
an irrational faith in the ¿ delity of our perceptions. If, however, we 
understand the limits of our perceptions, then we will not overly 
rely upon them.
change blindness: The experimentally veri¿ ed tendency of humans not to 
notice changes in their environment.
confabulation: The ¿ lling in of details missing from either perception or 
memory. The brain invents the missing details to construct a consistent 
narrative. 
constancy: The fact that our brains construct a constant and consistent model 
of what we perceive that generally matches reality.
inattentional blindness: This phenomenon refers to the lack of attention 
to sensory information, especially while attending to other sensory input. 
Signi¿ cant information right before our eyes can be completely missed and 
is simply not processed.
McGurk effect: The phenomenon that the consonant sounds we hear are 
affected by the lip movements we see. 
multitasking: Dividing attention between two or more tasks or
sensory inputs. 
    Important Terms

27
optical illusion: The common term for the failure of constancy, or a 
breakdown in the process of creating a constant and consistent view of 
reality. Illusions occur when what our brain constructs does not match reality 
or when there is an inherent contradiction or ambiguity in the way perceptual 
information is constructed.
synesthesia: When more than one sensory modality is combined or when 
one sensory modality is interpreted as another, such as smelling colors. 
Fineman, “Sightings, UFOs, and Visual Perception.”
Macknik, Martinez-Conde, and Blakeslee, Sleights of Mind.
Novella, “The Spinning Girl Illusion Revisited.”
Seckel, The Great Book of Optical Illusions.
1. How does the brain process sensory information, and how does this 
affect what we think we perceive?
2. What are the various factors that make eyewitness testimony unreliable?
    Suggested Reading
    Questions to Consider

28
Lecture 4: Flaws and Fabrications of Memory
Flaws and Fabrications of Memory
Lecture 4
T
here are numerous ways in which human memory is À awed. Far from 
being a passive recording of events, memory is constructed, ¿ ltered 
through our beliefs, and subjected to contamination and morphing 
over time. Memories can even be fused or entirely fabricated. It’s naive to 
implicitly trust our memories, and it’s important to recognize that we need 
to be realistic and humble about the limitations and À aws of human memory. 
Without external, objective veri¿ cation, we can’t know how accurate the 
details of our memories are. Recognizing the fallibility of human memory is 
an important step toward true critical thinking. 
Human Memory
x
Human memory is utterly À awed. Like perceptions, memory is not 
a passive recording; instead, our memories are constructed entirely 
by our brains. In fact, they’re tied together with everything that we 
think and believe with our internal model of reality. 
x
There are various different types of memory. Short-term memory
is a several-minute window of memory that is stored in the 
hippocampus of the brain. Working memory is our ability to hold 
a few bits of information in our minds and manipulate it in some 
way. Long-term memory is stored more diffusely throughout the 
brain for a long period of time. 
x
Human memory can be incredibly powerful in terms of raw capacity 
and being familiar with objects that we have seen, especially 
in terms of visual memory, but there are many ways in which it
is limited. 
Limitations of Human Memory
x
There is a false assumption that all problems with memory are 
associated with recall. Some memories never form; in other words, 
we may experience something but never consolidate it from short-

29
term into long-term memory. Memories also degrade, fuse, and 
morph over time.
x
A À ashbulb memory is a type of long-term memory that we have 
for an unexpected emotional event—as opposed to everyday, 
mundane events of our lives. Flashbulb memories tend to be vivid, 
long-lasting memories, and they are reinforced by the emotion of 
the event. In fact, a strong emotional experience, such as a traumatic 
event, strongly reinforces a memory.
x
Jennifer Talarico and David Rubin did a À ashbulb memory recall 
study about the September 11, 2001, terrorist attacks and found that 
the accuracy and consistency of everyday and À ashbulb memories 
degrade equally over time. Additionally, for everyday memories, 
subjects’ con¿ dence also decreased at about the same rate as their 
accuracy did, but for À ashbulb memories, their con¿ dence remained 
high and the memories remained vivid and emotional. 
x
Interestingly, con¿ dence in a memory is not a good predictor of the 
accuracy of a memory. We tend to naively assume that if we are 
very con¿ dent in a memory—if it feels vivid to us and if it can be 
easily recalled—it must therefore be accurate, but the research does 
not support this. 
x
It’s still not certain whether the way we assess our con¿ dence in 
our memories and the way we form those memories are the same 
process or if there’s a distinct neurological process somewhere else 
in the brain where we assess our con¿ dence in our memories. 
x
Memories of details tend to increase con¿ dence, but having a vivid 
memory for detail does not necessarily predict accuracy. Overall, 
familiarity does not increase con¿ dence much, but it is found to be 
better correlated with accuracy in some experiments.
x
Source amnesia is an example of the disconnect between accuracy 
and con¿ dence. We have a particularly bad memory for the source 
of information, even when we can recall the information itself; 

30
Lecture 4: Flaws and Fabrications of Memory
our brains simply do not dedicate many memory resources to
source information.
x
In our complicated civilization, we often have to assess the 
reliability of multiple sources of information in order to note 
something meaningful about the information, so source amnesia is 
a major problem in our society. 
x
Similarly, truth amnesia involves remembering a claim much 
more easily than remembering the distinct fact of whether that 
claim is a myth. In 2007, a study conducted by Ian Skurnik and his 
collaborators showed that as many as 27 percent of young adults 
misremembered a false statement as being true only three days after 
they were told, and 40 percent of older adults misremembered a 
false statement as being true. They remembered that they had heard 
the statement before, but they didn’t remember that it was false. 
x
Truth status appears to fade faster than familiarity, as does source 
status. Therefore, familiarity leads to a truth bias, which gets 
reinforced with repetition. The effect of not remembering the truth 
status of a claim is worsened when the truth status is not revealed 
until the end. This has many implications for information campaigns 
that involve myth busting. 
x
Psychologists also distinguish between thematic and detail memory, 
which are different types of memory that we have for an event. 
Thematic memory is responsible for the overall emotional content 
of an event; detail memory is responsible for the details of an event.
x
There are different brain patterns for each of these types of memory. 
Functional magnetic resonance imaging (fMRI), which is a way 
of imaging real-time brain activity, shows that different parts of 
the brain activate during thematic versus detail memory tasks. In 
other words, remembering details is neurologically different than 
remembering the theme of a memory. 

31
x
We also tend to focus 
on 
stimuli 
and 
ignore 
peripheral 
details. 
For 
example, witnesses will 
tend to remember a weapon 
that an alleged assailant 
was holding; they tend 
to focus on emotionally 
laden 
detail. 
However, 
when the police ask them 
to remember such details 
as eye color and clothing, 
they may not remember 
those kinds of details at all.
Altering Memory Details
x
What’s 
much 
worse 
than not having a good 
memory for details is the 
fact that we actually alter 
the details. We construct 
a narrative, which has emotions and themes attached to it, and 
we alter details in order to be in line with our thematic narrative. 
Therefore, details are also biased thematically.
x
In this way, past events become contaminated; memory 
contamination occurs when we incorporate details that we are 
exposed to after an event into the memory of the event itself.
x
Researcher Elizabeth Loftus and her collaborators found that people 
incorporate misleading details from questions or other accounts 
into their own visual memory. These are false memories that are 
constructed by someone asking a leading question, such as, “How 
fast were you going when you slammed into that car?”—implying 
that you were speeding. 
Memory contamination occurs when 
details that are introduced after 
an event are incorporated into the 
memory of the event itself.
© Jupiterimages/Thinkstock.

32
Lecture 4: Flaws and Fabrications of Memory
x
There is also a need to conform to what we think we know about 
the event we’re trying to recall. Witnesses, therefore, tend to 
contaminate each other’s accounts, bringing them in line with 
each other. This is not deliberate deception on the part of the 
witnesses; our brains ¿ ll in gaps in our narratives, glossing over
any inconsistencies.
x
Additionally, we tend to invent details to ¿ ll in the gaps in order 
to create a consistent story and reinforce the emotional themes.
This relates to the fact that memory is a construction—not a
passive recording.
x
For example, when we blink, we miss a tiny bit of visual 
information, and our brains stitch together the visual information 
so that we have one continuous stream. We do this cognitively as 
well; we stitch together different bits of things we perceive about an 
event in the process of confabulation.
x
Forced confabulation occurs when leading questions are asked 
about a ¿ lm that was viewed, for example. In studies of forced 
confabulation, subjects can remember seeing a nonexistent scene 
that they were asked about. A memory of an event we confabulate 
feels like a memory of something we actually experienced, and we 
don’t have a way of telling the difference. 
x
This is sometimes referred to as the Rashomon effect, which is 
based on a 1950 Akira Kurosawa ¿ lm about a rape and murder in 
which the same story was told from four different accounts, and 
it was a very artistic depiction of how different accounts of the 
same event can be. Everyone has their own perspective, ¿ lter, and 
memory, and therefore, when they compare stories after the event, 
there may be striking differences in how each person constructs the 
same event that they all experienced at the same time.
Inventing False Memories and Experiences
x
The ability to confabulate can lead to what are called false 
memories. In 1988, Ellen Bass and Laura Davis wrote a book called 

33
The Courage to Heal, in which they promoted the idea that many 
people, mostly women, were abused as children, and they felt that 
the memories of these highly traumatic events were repressed and 
later manifested as adults as anxiety, depression, eating disorders, 
and other problems.
x
Bass and Davis created an epidemic of what is now known as false 
memory syndrome. Therapists who followed their prescription 
encouraged clients to remember details of abuse, which led to many 
thousands of people manufacturing memories of intense abuse as 
children and believing that they were real. In fact, there are cases of 
people who were sentenced to prison for committing abuses when 
the only evidence against them was from false memories. 
x
It’s so easy to create false memories that you could do so by giving 
people a simple word list and having them memorize it. You can 
give them very subtle suggestions by having a theme to the words 
and then by showing them another list that included words that were 
not, in fact, on the ¿ rst list but are similar or on the same theme. 
They will construct a false memory of seeing those words because 
they ¿ t with the theme of the words that they were given. 
x
There are often social demands and other motivational factors 
that may cause us to bring our memories in line with what’s 
being suggested to us. Children are particularly susceptible to 
suggestibility and creating false memory.
x
In addition to false memories, false experiences are very easy to 
generate. In one study, participants read an advertisement for a new 
but false brand of popcorn that vividly described both the taste and 
feel of the popcorn. A week later, participants were asked whether 
they were actually given the popcorn to taste or whether it was just 
described to them, and a certain percentage of the people who were 
never exposed to the popcorn remembered having eaten it. In other 
words, they inserted themselves into the memory of an event they 
only read about. 

34
Lecture 4: Flaws and Fabrications of Memory
x
This is a caution for any profession that must solicit a history from 
another person, such as police of¿ cers, physicians, and therapists. 
People in these professions need to remember the incredible 
tendency for suggestibility and creating false memories.
x
Furthermore, not only do we need to avoid encouraging people 
to invent memories, but we also need to make sure there’s always 
some external, objective veri¿ cation. Courts, in fact, have been 
using more caution about the validity of eyewitness testimony and 
recognition of the needs to validate any testimony with some kind 
of objective evidence. 
false memory syndrome: The implantation of false memories that are 
thought to be real by the possessor of the memory, often resulting from 
strong suggestion, imagining, or hypnosis. 
À ashbulb memory: A detailed memory or snapshot of a sudden, unexpected, 
and emotionally signi¿ cant event. 
functional magnetic resonance imaging (fMRI): Application of magnetic 
resonance imaging, a type of medical scanner, that can be used to image the 
degree to which different parts of the brain are active. 
long-term memory: Memories that have been consolidated into long-
term storage. 
recall: The act of bringing to the conscious mind memories stored in long-
term memory. 
short-term memory: Memory for immediate sensory or internal information 
that lasts from seconds to a few minutes. 
source amnesia: The tendency to forget the source of information more 
easily than the information itself. 
    Important Terms

35
working memory: A type of memory that is distinct from short-term 
memory because it consists of information that the brain can manipulate, 
such as performing mental calculations. 
Neimark, “It’s Magical, It’s Malleable, It’s ... Memory.”
Novella, “A Neurological Approach to Skepticism.”
———, “Memory.”
Shreve, The Fallibility of Memory in Eyewitness Testimony.
Winograd and Neisser, eds, Affect and Accuracy in Recall. 
1. What are the various processes that occur to long-term memories that 
may alter their content?
2. What are the implications for investigatory professions of the extreme 
fallibility of human memory?
    Suggested Reading
    Questions to Consider

36
Lecture 5: Pattern Recognition—Seeing What’s Not There
Pattern Recognition—Seeing What’s Not There
Lecture 5
H
umans have a well-documented tendency for pattern recognition. It 
is both a great cognitive strength but also can be a weakness because 
we may see patterns that do not actually exist. We constantly 
recognize illusory patterns, which are manifested in pareidolia, data mining, 
hyperactive agency detection, and superstitious thinking. Additionally, we 
have a sense of what we think is real. Critical thinking skills are a way of 
sifting through all of the randomness—transcending above our evolved 
tendency to detect patterns and agency too often—to sort out what’s actually 
real from what only appears to be real. 
Finding Meaning in Patterns
x
Humans are so good at seeing patterns that we sometimes see 
patterns that are not even there. Brain processing is based largely 
on pattern recognition—probably because our brains are massive 
parallel processers with many connecting neural networks. One of 
our strengths as cognitive beings is making connections between 
different ideas, visual patterns, words, events, and objects.
x
Our brains are able to process representative and abstract thinking, 
such as metaphors. Art is a good example of this human talent for 
making abstract connections, or thinking creatively. 
x
Additionally, we imbue meaning and emotion into the abstract 
patterns that we see, which makes the patterns seem real and 
meaningful. In fact, our brains are wired to assign meaning and 
emotions to things.
x
Pattern recognition is ¿ ltered through a different module of the 
brain that undertakes what is called reality testing. We see many 
apparent patterns in the world around us, and then we run those 
patterns through a reality-testing algorithm to decide whether it 
agrees with our internal model of reality. 

37
x
Interestingly, while we’re dreaming, the reality-testing module is not 
as active as when we are fully awake. This is why bizarre things can 
happen in our dreams that our dreaming selves accept and why upon 
awakening and remembering dreams, they no longer make sense. 
x
A pathological condition known as psychosis is essentially a lack 
of reality testing—a decreased ability to test the patterns that we 
see against reality. Psychosis makes the patterns that are seen, even 
the most bizarre ones, seem far more real and compelling than they 
actually are. 
Visual Pareidolia
x
There is a phenomenon known as visual pareidolia, which is a 
tendency to see a pattern in randomness. The most familiar example 
of this is seeing faces, animals, or common objects in the random 
shapes of clouds. We recognize that these patterns are completely 
random, but they still look familiar to us. 
x
Visual pareidolia is the most obvious example of the more general 
phenomenon of pareidolia, in which the brain seeks patterns to ¿ t 
the stimuli. As our brain is constructing what we think we see and 
remember, it’s also trying to make a pattern ¿ t to patterns it’s already 
familiar with. It’s also interesting that once you see a pattern in 
random stimuli, it’s very dif¿ cult, or even impossible, to not see it.
x
Many so-called ghost photographs are based on pareidolia. For 
example, you may be taking a picture of a wispy cloud or bouncing 
a À ash off of some dust in the room that results in an overexposed, 
cloudy image on the ¿ lm; our brains are very good at looking at 
the randomness of that cloudy photograph and seeing a face or the 
outline of a person. 
x
The human face is the most familiar pattern that we will tend to 
impose—perhaps because a large portion of our visual cortex is 
dedicated to seeing faces. It’s obviously important to us as social 
creatures to be able to recognize individuals very easily and
very quickly. 

38
Lecture 5: Pattern Recognition—Seeing What’s Not There
x
From the earliest age that we can detect it, there seems to be an 
inborn preference for looking at the human face. This preference for 
the pattern of the human face results in pareidolia that frequently 
results in a face.
x
For example, low-resolution pictures taken by NASA probes 
showed a particular formation on the surface of Mars that vaguely 
resembled a human face, especially when it was suggested to 
someone. Subsequent high-resolution pictures of the same area 
showed a normal geological formation. 
x
There is a lot of hardwiring in our brain that is dedicated to inferring 
emotion from minimalist visual cues. We understand, for example, 
the emotion that a stick ¿ gure or cartoon character is supposed to 
have; we can personify animals and cartoon characters and imbue 
them with the full range of human emotions. 
An example of visual pareidolia is seeing faces, animals, or common objects in 
the random shapes of clouds.
© Ingram Publishing/Thinkstock.

39
x
When asked to tell what emotion a picture of a person is displaying, 
people in eastern cultures spend more time looking at the eyes 
whereas westerners spend more time looking at the mouth. 
This is reÀ ected in emoticons, which are electronic expressions 
comprised of keyboard characters that resemble faces, in which 
western emoticons tend to vary the mouth shape whereas eastern 
emoticons tend to vary the eyes in order to alter the emotion that is
being represented. 
Audio Pareidolia
x
In addition to visual pareidolia, the tendency to see patterns in audio 
stimuli is called audio pareidolia. The brain has a limited number 
of phonemes, which are components of spoken speech, that it can 
distinguish. We mostly learn these by age four; after which, we tend 
to sort anything that we are trying to understand as speech into one 
of our learned phonemes.
x
Audio pareidolia is partially responsible for accents. People who 
learn one language may not be sensitive to phonemes that are not 
present in their language but are present in a different language, 
and therefore, they won’t be able to make the proper distinctions 
when speaking that other language, which will give them a
recognizable accent.
x
Believers in a phenomenon called electronic voice phenomenon 
(EVP), think that they are able to tap into the spirit world by 
listening to essentially static on tape recordings. They often visit 
an alleged haunted location and listen through recorded noise for 
words to pop out. 
x
Essentially, this is a form of data mining, which involves analyzing 
large amounts of data and looking for random patterns that occur by 
chance. Believers in EVP make the false assumption that a phrase 
they hear is real and that some spirit or ghost is actually saying 
those words. Psychologically, it has been demonstrated that your 
brain is imposing those words onto a random pattern of noise. 

40
Lecture 5: Pattern Recognition—Seeing What’s Not There
x
A similar phenomenon is backward masking on music, or listening 
for secret messages in music that is played backward. After this 
phenomenon became popular, some musicians deliberately put 
backward messages in their music, but most of the time, it was 
random noise in the backward music that the brain ¿ t to a familiar 
audio pattern.
x
It has been demonstrated experimentally that we also tend to be 
highly suggestible in this regard. For example, it’s easier to hear 
what you’re supposed to hear after you are told that that’s what 
you’re supposed to hear. Your brain will make the connection and 
¿ t your perception to the words. 
Data Mining and Hyperactive Agency Detection
x
Data mining is a legitimate exercise for generating hypotheses. 
When scientists have large amounts of data, they may mine the 
data using computer models or other algorithms to look for any
apparent patterns.
x
The problem that occurs is when scientists assume that the patterns 
they see are a way of con¿ rming an idea rather than a way of 
generating a hypothesis. After ¿ nding patterns, scientists need 
to con¿ rm that the patterns are real by testing them against an 
independent set of data because if the pattern was initially there by 
random chance, they’re going to propagate that random chance into 
future analyses unless they use new data. 
x
It is most dangerous when data mining is subconscious rather than 
deliberate. Because people only tend to see the pattern, they don’t 
see all the data that the pattern is buried in.
x
For example, astrological researchers look for any pattern in the 
data related to an astrological sign, but what they don’t realize is 
that they are looking for any possible match—any outcome to any 
astrological sign. By odds alone, there’s going to be randomness in 
the data and some patterns that emerge.

41
x
When astrological researchers use a completely independent set of 
data, there is no consistent pattern with astrological signs. Astrology 
is based almost entirely on this false pattern recognition and
data mining. 
x
A phenomenon called hyperactive agency detection describes the 
tendency to assume agency even when randomness is suf¿ cient. We 
tend to err on the side of feeling as if there’s agency even when 
there isn’t. 
x
Evolutionary explanations might suggest that we evolved from 
ancestors who had hyperactive agency detection—who assumed 
that a rustling in the bushes was a predator versus just the wind—
because it was a survival strategy. We didn’t evolve from those who 
investigated which option it actually was.
x
Hyperactive agency detection is reÀ ected in our neuroanatomy and 
in the way that we process visual information. Our brains do not 
decide whether something is alive and then assign emotion to things 
that are alive; instead, they decide whether something is acting as 
if it has agency and then assign emotional meaning to it as if it’s 
an agent. This is how we see agency in nonliving things, such as 
cartoon characters.
x
In addition to hyperactive agency, we have a tendency to detect 
the essence of various things, which make them what they are. 
For example, psychological research has found that how a child 
feels about a toy will depend on whether they imbue that toy with 
essence. For most toys, children understand that they’re just things 
and will happily accept an exact duplicate of them. However, if 
a child has a favorite toy, then he or she thinks there’s something 
essential about that toy that would be missing if they were given an 
exact replica of the toy. 
x
Evolutionarily, it makes sense that, for example, a parent would not 
accept an apparent exact replica or duplicate of their child; instead, 
they would want to have their own child. Therefore, there seems 

42
Lecture 5: Pattern Recognition—Seeing What’s Not There
to be an adaptive advantage to having this sense of individuality
and essence. 
x
Problems can arise when we see agency where it does not exist. For 
example, we may see an invisible hand that is controlling events, 
which leads to conspiracy thinking. When we see a pattern of 
events that we think can’t be a coincidence, then we further assume 
that there must be an agent behind that pattern—some organization, 
power, magical agent, or force of nature that is making things 
happen the way they are. 
x
We need to ¿ lter our tendencies for hyperactive pattern recognition 
and agency detection through our reality-testing ¿ lter, but we also 
need to understand that pattern recognition, agency detection, belief 
in essence, and data mining conspire together to create the powerful 
illusion that we are seeing something real in the world when it’s just 
randomness. Emotionally, an illusion might be very compelling, but 
we need to use critical thinking skills to systematically test apparent 
patterns to truly know if an illusion is real or not. 
data mining: The process of sifting through large sets of data and looking 
for apparent patterns. This is a legitimate way to generate hypotheses—
but not of con¿ rming them—because this process lends itself to ¿ nding
illusory patterns. 
electronic voice phenomenon (EVP): The phenomenon of apparent words 
or phrases being found in audio recordings of allegedly haunted locations. 
Believers ascribe EVP to ghost phenomena, but they are more easily 
explained as audio pareidolia. 
hyperactive agency detection: The human tendency to detect a conscious 
agent behind natural or random behavior or events—for example, believing 
that random events are a conspiracy to punish us. 
    Important Terms

43
pareidolia: The tendency to see patterns in random noise—for example, 
seeing a human face in the random features of a cloud. 
pattern recognition: The phenomenon of perceiving patterns—whether in 
visual information, other sensory information, or even events or behavior. 
Humans generally have a great ability to recognize patterns and a tendency 
to see patterns even when they are illusory. 
psychosis: A psychiatric condition characterized by impaired reality testing. 
reality testing: A cognitive process by which the brain compares any new 
information to its internal model of reality to see if the new information 
makes sense.
Ariely, Predictably Irrational, Revised and Expanded Edition.
Blackmore, Consciousness.
Kahneman, Thinking, Fast and Slow. 
Novella, “Body Snatchers, Phantom Limbs, and Alien Hands.”
1. How much of what we think and feel derives from subconscious, 
evolved brain processing—of which we are not consciously aware?
2. How is our experience of reality a constructed process of the brain?
    Suggested Reading
    Questions to Consider

44
Lecture 6: Our Constructed Reality
Our Constructed Reality
Lecture 6
T
he goal of this ¿ rst section of the course is to give you an appreciation 
for the extent to which what we perceive of as reality is actually an 
illusion constructed by our brains. In previous lectures, you learned 
how perception is not only highly ¿ ltered but also constructed. Our brains 
assign patterns to what we perceive and then assign meaning to those 
patterns; our very sense of self and what we perceive of as reality is also a 
constructed illusion by our brains. 
Brain Function
x
Perhaps the most persistent illusion that is constructed by our brain 
is that we are one cohesive consciousness. The reality is that various 
regions of our brain are communicating with each other and are in 
frequent conÀ ict as they each undergo their purpose.
x
The neocortex, or the frontal part of our brain, has executive 
function whereas more primitive parts of the brain are the seats 
of emotion and instinct. The neocortex is involved with social 
behavior, long-term planning, and inhibition.
x
Often, rather than imposing these functions on our more primitive 
desires, the neocortex may rationalize decisions that are made by 
more primitive regions. These two brain regions come into conÀ ict, 
and once the conÀ ict is resolved, our brains give us a small amount 
of dopamine, the reward neurotransmitter, to make us feel good.
x
To investigate brain function, we use functional magnetic 
resonance imaging (fMRI), which scans the brain using a magnetic 
resonance imager to not only look at the brain, but to also see how 
the brain is functioning in real time. Additionally, transcranial 
magnetic stimulation (TMS) is a technique for either increasing 
or decreasing different regions or circuits within the brain. This 

45
way, we can tell what they contribute to the overall functioning of
the brain.
x
In some cases, modern neuroscience researchers are ¿ nding 
the neurological correlates to what psychologists have been 
demonstrating for decades: different brain regions in conÀ ict.
x
Our decisions seem to be conscious, but they are often made 
subconsciously by an evolutionary neurobiological calculus that we 
are not aware of.
x
In addition to decision making, intuition is a form of subconscious 
processing. For example, emotional processing, social cues, and the 
monitoring of our internal state are largely subconscious processes.
x
Emotions are involuntary and subconscious. We don’t choose to 
feel angry; we just feel angry and then invent a reason to explain 
why we feel angry—with varying degrees of insight. In addition, 
explanations we invent for our feelings and behavior are typically 
highly self-serving. 
x
Subconscious processing also extends to other types of processing, 
such as searching for a particular memory or even problem 
solving. For example, your brain continues to try to search your 
memories for the location of your car keys, even though you aren’t 
consciously paying attention to it, and then when the match is made, 
the information pops into your conscious awareness.
x
The global workspace model of consciousness states that while 
various brain regions act like they are each their own consciousness, 
they all report to some central, broadly distributed network within 
the brain called the global workspace.
x
However, recent evidence argues against the existence of a global 
workspace; there appears to be no central location that the various 
regions report to. Consciousness, therefore, is the aggregate 
behavior of all of these various components acting together.

46
Lecture 6: Our Constructed Reality
Brain Research
x
Research shows that we could be conscious of visual perception 
without the visual cortex having to report that information to any 
other part of the brain. In addition, decisions can be altered by 
altering the function of different speci¿ c brain regions.
x
For example, researchers have inhibited the right temporal 
parietal junction using TMS. That part of the brain is involved 
with the theory of mind, which is the ability to think about 
the intentions of other people. When that part of the brain 
was inhibited, it impaired the ability of subjects to make
moral judgments that speci¿ cally require a theory of mind in
the calculus.
Various regions of the brain communicate with each other and frequently 
conÀ ict as each region undergoes its purpose.
© John Foxx/Stockbyte/Thinkstock.

47
x
A study showed that patients with damage to their insular cortex, 
which is often responsible for translating physical sensations into 
emotions, lost cravings, or the emotional desire to engage in drug 
seeking, based on their physical withdrawal symptoms—even in 
cases of lifetime smokers. 
x
A split-brain experiment is a classic type of neuroscience 
experiment that demonstrates the manner in which different parts of 
the brain construct our aggregate consciousness. Such experiments 
were ¿ rst done by Roger Sperry and Ronald Meyers in the late 
1950s and then were reproduced later by many other researchers.
x
Split-brain research involves patients who have had the right and 
left hemispheres of their brain separated surgically to prevent 
seizures from spreading from one part of the brain to another and 
to decrease the number and severity of seizures. Because about 
90 percent of the communication between the two halves of their 
brain is separated, these patients provide an opportunity to test 
the aggregate consciousness of one hemisphere separate from the 
aggregate consciousness of the other hemisphere.
x
The right hemisphere in most people is the nondominant hemisphere 
and the one that is not engaged in language. The left hemisphere 
has the ability to speak and understand language.
x
Because the two hemispheres of the brain are separated in epilepsy 
patients, researchers are able to show only the right hemisphere of 
the brain an object and then see how the individual responded to 
that information.
x
In a classic experiment, the experimenters showed the subject’s 
right hemisphere a bottle of soda, and then they immediately 
showed the subject a series of objects, one of which was the bottle 
of soda. When asked to choose one of the objects, the subject would 
pick the bottle of soda, but they wouldn’t know why. 

48
Lecture 6: Our Constructed Reality
x
We feel as though we are a self-contained entity that is separate 
from the rest of the universe. When brain regions that are involved 
in this feeling of separateness are inhibited, that can give an 
individual a powerful sense of being one with the universe, which is 
often interpreted as a profound spiritual experience. 
x
Usually, we feel as if not only do we occupy our bodies, but we exist 
somewhere behind the eyes. That is also a speci¿ cally constructed 
experience inside the brain. When these parts of the brain are 
inhibited by TMS, subjects have an out-of-body experience, in 
which they feel like they are À oating somewhere above their body.
Consciousness and Motor Control
x
The ownership module creates the sense that we own and control 
the different parts of our body. These networks in the brain compare 
sensory information that is both visual and proprioceptive and give 
muscle feedback to our intentions. 
x
If there is a disruption in this circuit by a stroke, for example, then 
we do not feel as if we own or control certain limbs, which will 
give us the sense that an arm is moving of its own will—called 
alien hand syndrome.
x
This is very similar to the phantom limb phenomenon, in which 
the ownership module still owns a limb, but the limb is either dead 
or is even not there. Amputees often experience this; they perceive 
a limb that’s not there. 
x
The perception of a supernumerary phantom limb is a very rare 
syndrome in which the ownership module still functions, but it 
doesn’t have a limb to control, so it manufactures a virtual limb 
that it can control by creating the illusion of an extra arm, perhaps 
emerging from the middle of one’s body. 
x
The ideomotor effect is essentially subconscious motor control: 
small motor movements that serve a purpose but that we are not 
fully conscious of. Probably one of the most common examples of 

49
this is arriving at a destination and not remembering driving there; 
you went through all of the motor movements to drive, which can 
be fairly complicated, without expending any conscious effort to 
do so. 
Altered States of Consciousness and Free Will
x
We gain insight into the fact that our brain is comprised of 
many different regions when we experience altered states of 
consciousness, such as dreaming. When we are dreaming, the 
reality-testing part of our brain is less active, but we’re still 
conscious. It’s just a different consciousness than we’re used to 
when we are fully awake. 
x
In addition, we may have experienced extreme sleep deprivation 
or the intoxicating effects of certain drugs. Alcohol, for example, 
inhibits all brain function, but especially the frontal lobes—mainly 
because they are a very active, energy-demanding part of the brain, 
so they feel the effects of alcohol more acutely than perhaps other 
parts of the brain do. As a result, judgment and the ability to inhibit 
our behavior decrease signi¿ cantly.
x
Hypnosis is another example of an altered state. In fact, when 
under hypnosis, we’re not in a trance—it’s actually a state of 
hyperawareness. The goal of hypnosis is to give the subject some 
sensory information that they’re not fully conscious of; they’re in 
a highly suggestible state, and some sensory information is being 
processed subconsciously. 
x
These various brain regions perform very complex processing, 
which results in our desires, motivations, and decisions. The frontal 
lobes are involved in decision making but also in rationalizing those 
decisions in order to remove cognitive dissonance, but that is also 
undergoing its own calculus. However, to varying degrees, we can 
enact upon our will—upon our frontal lobe function—or even turn 
off our critical thinking.

50
Lecture 6: Our Constructed Reality
x
Research has demonstrated that a very charismatic person may be 
able to hypnotize a crowd by turning off subjects’ critical thinking, 
making them susceptible to suggestion in the same way that a 
hypnotist does. 
x
This brings up the tricky philosophical question of free will. There 
are people who use all of this information to question the very 
concept of what they call noncausal free will. Essentially, what 
they believe is that everything we do is deterministic. Our brains 
are physical, and they are just as susceptible to the laws of physics 
as anything else in the world. Therefore, everything that we think, 
believe, and do is the result of these mechanical processes in the 
brain—not noncausal free will.
x
Research overwhelmingly shows that human beings generally have 
poor self-control. Generally speaking, about 95 percent of the time, 
people will fail to alter their own behavior through conscious effort 
alone to quit a bad habit, for example. It simply takes too much 
mental energy and vigilance. 
x
The brain is plastic, and habits of thought can change. Therefore, if 
you practice and make a concerted effort to behave in certain ways, 
those behaviors will become ingrained and will become easier
over time. 
x
Practicing the habit of exhibiting executive control or executive 
function over your more primitive parts of the brain can be a 
learned skill. This is why we need and can bene¿ t from formal 
reality testing, or critical thinking. We need formal logic and 
the methods of science to carefully and reliably sift through the 
myriad potential patterns that we see. This is the central concept to
critical thinking.

51
alien hand syndrome: A neurological syndrome in which a person’s limb, 
such as a hand, feels as if it is acting on its own—without conscious control. 
This results from damage to the brain pathways that compare the intention to 
move with actual movements. 
executive function: A function of the frontal lobes of the brain, speci¿ cally 
the ability to control and plan one’s behavior to meet long-term self-interest 
and social integration. 
free will: The ability of a sentient being to make voluntary choices and 
decisions. Philosophers argue about whether humans have true free will or 
just the illusion of free will. 
global workspace: A controversial theory (disputed by recent research) that 
posits that a distributed network in the brain is the common pathway for all 
conscious experience. 
hypnosis: Although not a trance, hypnosis is a state of mind characterized by 
alertness but also by high suggestibility. 
ideomotor effect: Subconscious muscle movements that conform
to expectations. 
intuition: Decision making or feelings, such as responses to social cues, that 
derive from subconscious brain processes.  
ownership module: The part of the brain that creates the sensation that we 
own the various parts of our body. 
phantom limb: An illusory limb that does not exist but that the subject can 
feel and even have the sense that they can move. It is commonly, but not 
exclusively, the result of amputation. 
    Important Terms

52
Lecture 6: Our Constructed Reality
split-brain experiment: An experiment on a subject who had the connection 
between their two brain hemispheres surgically cut that helped reveal the 
functions of the two hemispheres and how they work together. 
supernumerary phantom limb: A phantom limb that is not simply a 
replacement for a missing limb but is experienced in addition to the four 
natural limbs. 
theory of mind: A psychological term that refers to the ability to understand 
and think about the fact that other people have their own conscious existence 
with their own feelings and motivations. 
transcranial magnetic stimulation (TMS): Technology that uses magnetic 
¿ elds to either increase or decrease activity in speci¿ c regions of the brain. 
Novella, “Data Mining—Adventure in Pattern Recognition.”
———, “Hyperactive Agency Detection.”
———, “Pareidolia in the Brain.”
Taleb, Fooled by Randomness.
Wiseman, Paranormality.
1. Humans have a heightened ability to detect patterns, but how can we 
know which apparent patterns are real and which are just illusions?
2. What effect does the tendency to see agency where it may not exist have 
on human decision making?
    Suggested Reading
    Questions to Consider

53
The Structure and Purpose of Argument
Lecture 7
T
he ¿ rst section of this course examined how massively À awed our 
brains are as a tool for understanding the universe, but we do have the 
ability to reason. This next section of the course will address how to 
use that reasoning ability to override the À aws in our neurological function. 
This lecture will begin with a discussion of the structure and purpose of 
argument itself. Then, you will learn about speci¿ c logical fallacies with 
the goal of avoiding such fallacies in your thinking and recognizing these 
fallacies in the arguments of others.
Logic and Arguments
x
The purpose of an argument for a critical thinker is not to win, 
although that is often the default mode of how we behave. Often, 
we pick a side and then defend that side at all costs, marshaling 
whatever arguments we think can defend that position.
x
The critical thinking approach to argumentation is to value the 
process of developing your arguments and reaching conclusions; a 
critical thinker should be willing to change any conclusion when 
new information or a better argument is presented. 
x
Rationalizing is the process of starting with the conclusion and 
then ¿ guring out which arguments can be marshaled in order to 
defend that conclusion. On the other hand, reasoning focuses on the 
process going forward, where the conclusion À ows from the logic 
and not the other way around.
x
Often, we use the rationalizing process, which can seem very 
super¿ cially similar to reason. In fact, the point of rationalizing is 
to make it seem as if our decisions and conclusions are reasonable.
x
When we have a conclusion that is discordant—that is not in line 
with the facts or is challenged by a new argument or new piece of 

54
Lecture 7: The Structure and Purpose of Argument
information—our tendency is to rationalize that new information in 
order to resolve the cognitive dissonance that results, and then our 
brains reward us for doing that.
x
To avoid cognitive dissonance, we should focus on the process 
instead. In other words, if we don’t tie ourselves ¿ rmly to a 
conclusion, then we won’t feel any emotional dissonance when new 
data is encountered that shows that the conclusion is wrong.
x
Speci¿ cally, in logic, the term “argument” is a set of statements 
used to support a conclusion. An argument must start with speci¿ c 
premises and then logically derive a conclusion from those 
premises. Explanations and assertions are not arguments.
x
A premise is a starting point; it is a fact or assumption that we take 
as a given at the beginning of an argument. If a premise is false, 
then any argument that is based upon that premise is not sound. 
Therefore, it’s very important to examine all of the premises and 
to recognize what the premises of an argument actually are. Often, 
that is a missing step in an argument. 
x
Assumptions can be used as premises, but we need to recognize 
when we are starting with a statement that is not an established 
fact. It is not known whether an assumption is true; it might 
be incomplete and might not fully capture the whole situation. 
Assumptions or incomplete premises weaken an argument because 
the argument is only as good as the premises on which it is based.
x
When two or more people disagree over a factual statement, one 
side or both sides must be wrong in some way. By de¿ nition, 
two mutually exclusive conclusions can’t both be correct at the
same time. 
x
The goal should be to examine both of your arguments to ¿ nd out 
where the assumptions are in the premises, where the false premises 
are, or where the errors in logic are. If you can work together to 
discover the errors and assumptions of logic, then you should be 

55
able to resolve your 
differences and come to 
a better conclusion.
Logic Jargon
x
If a claim is true, it 
is factually correct; it 
is in line with reality. 
Arguments themselves 
are not true or false; 
their conclusions are 
true or false.
x
In most cases, we do 
not have access to 
truth with metaphysical 
certitude, so what we’re really looking for is something that is 
established to a suf¿ cient degree that we can treat it as if it were a 
fact, although we know that all true statements are tentative in some 
way because they are dependent upon our incomplete current level 
of knowledge.
x
If the logic works, then an argument is valid. One or more premises 
may still be wrong or unjusti¿ ed, but an argument fails when it is 
impossible for the conclusion to be false if the premises are true.
x
An argument is sound when all of the premises are true and the 
logic is valid. The conclusion of a sound argument, therefore, must 
be true by de¿ nition; if the premises are true, then the conclusion 
must also be true.
x
The conclusion of an unsound or invalid argument, however, may 
or may not be true. Demonstrating that an argument is unsound or 
invalid does not prove the conclusion to be false—it just removes 
that argument as justi¿ cation for the conclusion. 
The goal of an argument between two 
people should be to uncover assumptions, 
false premises, and errors in logic.
© Pixland/Thinkstock.

56
Lecture 7: The Structure and Purpose of Argument
x
Deductive reasoning uses premises to connect to a conclusion. 
Deduction goes from the general to the speci¿ c. You start with 
some general statements, and then you deduce a speci¿ c instance 
that must be true if those general statements are themselves true.
x
For example, if premise one is that all men are mortal and premise 
two is that Socrates is a man, then we can arrive at the conclusion 
that Socrates is therefore mortal.
x
Conclusions of deductive reasoning are positive assertions. 
They are truth statements—not value judgments, which involve 
subjectivity regarding what we believe. For example, there is 
no way to prove that Beethoven’s music is better than Mozart’s. 
Oftentimes, arguments involve one or more value judgments in 
the premises, and when you identify them, you can at least agree
to disagree. 
x
In contrast to deductive reasoning, inductive reasoning goes from 
a speci¿ c observation to a general principle. Inductive reasoning is 
used to decide what is probably true based on observations. Science 
is largely based on inductive reasoning.
x
For example, if premise one is that every swan that has ever been 
observed is white, we may come to the inductive conclusion that 
all swans are white. However, this statement is falsi¿ able; the 
observation of a single black swan will render the statement untrue. 
Then, the statement might have to be modi¿ ed to the notion that 
most swans are white or, perhaps, that all swans except for the 
species of black swans are white. 
Types of Logical Fallacies
x
Logical fallacies are arguments in which the conclusion does not 
have to be true if the premises are true. The generic term for this 
type of invalid logic is the non sequitur, which literally means 
“it does not follow”—or the conclusion does not follow from the 
premises. Essentially, logical fallacies are our mechanisms for 
rationalizing conclusions.

57
x
The argument from authority is a logical fallacy that typically 
suggests that a conclusion is correct because an authority ¿ gure 
asserts that it is correct. The more general form is that a conclusion 
is correct because a person or a group making the claim have some 
positive or admirable attribute. For example, John says it’s true, and 
he’s a nice man; therefore, his conclusion must be correct. 
x
We have this evolved desire to get along with the social group and 
to follow a leader of some sort, and we therefore have this tendency 
to respect celebrities, of¿ cials, experts, and professionals—people 
who have some claim to authority.
x
The argument from authority can also be overapplied, meaning 
that we can dismiss legitimate arguments by the claim that they are 
arguments from authority. In other words, because the argument 
from authority is a logical fallacy, that doesn’t mean that we should 
be dismissive of the value of the consensus of expert opinion.
x
Another logical fallacy is the argument from ¿ nal consequences. 
The form that this fallacy typically takes is that some claim cannot 
be true if it results in consequences that you ¿ nd abhorrent; the 
opposite of this fallacy is that something is true because it serves a 
positive purpose.
x
We often see this argument in conspiracy theories, for example. 
The logic that often goes into conspiracy theories is the principle 
of cui bono, or “who bene¿ ts.” If an event occurs, the conspiracy 
theorist might ask, “Who bene¿ ted from this event?” Then, they 
would argue that, therefore, that person must have caused the event 
through some hidden conspiracy. 
x
A very common logical fallacy is called post hoc ergo propter 
hoc: “after which hence by which.” We tend to assume that if B 
follows A, therefore A must have caused B.
x
This logical fallacy derives from innumeracy, a naivety about 
statistics and probability. We are impressed with the pattern that B 

58
Lecture 7: The Structure and Purpose of Argument
followed A, and we instinctively don’t like the explanation that the 
occurrence is a statistical À uke or just random chance. We like to 
impose meaning on the patterns that we see.
x
Very similar to this is confusing correlation with causation, in which 
the form of the argument is not just that B follows A, but that B 
correlates with A. Where we see B, we also see A. The assumption, 
therefore, is that A must cause B. However, this is not logically the 
case. It’s possible that B causes A. It’s also possible that some other 
factor, C or D, causes both B and A. This also assumes that the 
correlation is real and not a coincidence.
x
Similar to the argument from authority, the logical fallacy of 
assuming causation from correlation can be overapplied to dismiss 
the very legitimate signi¿ cance of a correlation. Once we establish 
that a correlation is real, that doesn’t mean that there’s no causal 
relationship between the two things. It means that there are a 
number of possible causal relationships, one of which may be the 
simplest one: that A causes B.
x
Another logical fallacy is special pleading, which is also called 
ad-hoc or post-hoc reasoning, meaning that we invent reasons as 
needed in order to explain certain aspects of the evidence. This 
logic is not formally invalid. The fallacy occurs in the process of 
invoking these arguments after we know that they’re needed to 
crudely construct a problematic conclusion.
x
This kind of logic may be a way of generating a hypothesis but 
can’t be used as a premise or conclusion to dismiss inconvenient 
evidence or the absence of evidence that should be there. 
x
The special pleading fallacy is related to the fallacy of limited 
scope, which involves introducing a new element that is not a 
broadly applicable principle. It narrowly addresses a single À aw in 
evidence or argument.

59
x
This type of argument has been invoked frequently to explain 
why there is no evidence of the existence of Bigfoot. Some have 
argued that perhaps he can turn invisible, travel through other
dimensions, or disappear when needed to—and that’s why you can 
never trap him. 
argument: A statement that is used to support a conclusion or belief, often 
following deductive reasoning. 
deductive reasoning: Reasoning that begins with one or more general 
statements that are taken as premises and then concludes what must be true 
if the premises are true. 
inductive reasoning: Inductive reasoning begins with observations of the 
world and then derives general statements about what is probably true from 
those observations. 
innumeracy: A lack of working knowledge of mathematics, probability,
and statistics. 
logical fallacy: A logical operation that is not valid. 
non sequitur: A Latin term referring to an invalid argument in which the 
conclusion does not logically follow from the premises. 
post hoc ergo propter hoc: Literally meaning “after which hence by which,” 
a logical fallacy in which it is assumed that B is caused by A simply because 
B follows A. 
premise: A fact that is assumed to be true, or treated as if it is true, as a 
starting point for an argument. 
sound: In logic, this describes an argument that has both true premises and 
valid logic, and therefore, the conclusion must be true. 
    Important Terms

60
Lecture 7: The Structure and Purpose of Argument
Novella, “How to Argue.”
———, “Beware the Nobel Laureate Argument from Authority.”
Tarski, Introduction to Logic.
1. What is a logical fallacy, and how can we recognize these fallacies in 
the arguments of others and of ourselves?
2. Are arguments based on the authority of the source of information
ever valid?
    Suggested Reading
    Questions to Consider

61
Logic and Logical Fallacies
Lecture 8
T
he last lecture addressed the structure of argument, including the 
premises and the logical connection leading to a conclusion. This 
lecture will delve further into logical fallacies, which are sometimes 
used innocently but often are used as motivated reasoning to get to a 
desired—even if invalid—conclusion. As a critical thinker, the goal is to 
arrive at conclusions that are more reliable and more likely to be true by 
avoiding logical fallacies, using legitimate logic, and examining premises. 
More Types of Logical Fallacies
x
The ad hominem logical fallacy, or speaking “against the person,” 
occurs when you attempt to counter the argument made by someone 
by focusing on the person making the argument rather than the 
argument itself. In a way, this is the opposite of the argument
from authority.
x
An example of the ad hominem argument involves rejecting 
scienti¿ c consensus as if it’s arrogant to think that we can know 
anything. This is often combined with an appeal to elitism—that it 
is elite professionals who are making a speci¿ c claim, and therefore 
they can be dismissed because they’re not regular people.
x
A closely related logical fallacy to ad hominem is called poisoning 
the well, in which you’re not necessarily addressing the person 
directly, but you’re trying to taint their argument by tying it to 
something else that is unsavory or unpopular.
x
A classic example of this is called Godwin’s argument, an example 
of which is if Adolf Hitler believed something, it’s tainted and, 
therefore, wrong to believe in that claim.
x
This fallacy can be overapplied. Pointing out that someone who 
is offering you a too-good-to-be-true investment opportunity 

62
Lecture 8: Logic and Logical Fallacies
has a prior conviction of fraud is not necessarily an attempt to 
poison the well of their new claims so much as to put them in a
realistic perspective. 
x
Another logical fallacy is the tu quoque, or “you too,” logical 
fallacy, which is an attempt to counter a legitimate criticism 
by pointing out that the other person is also guilty of the same 
thing. In other words, your claims may lack evidence, but so do 
the claims of these other people. Lacking evidence is lacking 
evidence—regardless of which claim is guilty of that failing. This 
frequently comes up in the promotion of unscienti¿ c or implausible
medical modalities. 
x
A very pernicious logical fallacy, one that is very common in 
the promotion of paranormal, or supernatural, beliefs is ad 
ignorantiam—an appeal “to ignorance”—which involves using a 
lack of evidence or knowledge as if it were a positive argument for 
a speci¿ c conclusion.
People with paranormal beliefs often confuse the notion that something is 
currently unexplained with the idea that it is unexplainable.
© iStockphoto/Thinkstock.

63
x
For example, if you cannot identify a light in the sky, by de¿ nition, 
it is an unidenti¿ ed À ying object. Therefore, it must be an alien 
spacecraft. What the person knows is they can’t identify the object, 
but they then make the argument from ignorance that, therefore, it is 
something speci¿ c rather than simply concluding that it’s unknown.
x
Intelligent design is an argument from ignorance that is used 
to describe what is presented as an alternate theory to the theory 
of organic evolution. The notion is that life arose on this planet 
through the work of an intelligent agency, the intelligent designer. 
The arguments that are used in support of intelligent design are that 
we can’t explain how everything evolved; therefore, the inability to 
explain that becomes an argument for this alternate hypothesis.
x
Conspiracy theories also frequently rely on the argument from 
ignorance. The process of supporting a conspiracy theory often takes 
the form of shooting holes in the standard explanation of events and 
emphasizing those facts that seem anomalous or conÀ icting—often 
called anomaly hunting. Despite the fact that there is often a lack of 
positive evidence, speci¿ cally for their conspiracy theory, they’re 
basing it entirely on the argument from ignorance—the inability to 
explain every detail of an event. 
x
Another logical fallacy is the act of confusing the notion that 
something is currently unexplained with the idea that it is 
unexplainable. This is often combined with anomaly hunting to 
create an argument from ignorance. It assumes that our current 
knowledge represents the ultimate limits of our knowledge and 
that because we don’t currently know something, it’s therefore 
unknowable. This is often used to support supernatural or 
paranormal beliefs.
x
Another common logical fallacy is the false dichotomy, or false 
choice. This results from prematurely or arti¿ cially reducing the 
number of possible outcomes or possibilities down to just two and 
then making the false choice between those two. If not A, then the 
answer must be B, but this ignores that maybe C or D could be 

64
Lecture 8: Logic and Logical Fallacies
the answer. Creationists often make the argument that if we didn’t 
evolve, then we must have been created—as if that is the only
other possibility.
x
The false dichotomy could be a false choice between two or more 
possibilities that are not mutually exclusive, and it could also reduce 
a continuum to its extremes. 
x
The À ip side to the false dichotomy logical fallacy is the false 
continuum fallacy, which denies the existence of extremes because 
there is a continuum in between. Just because there is no sharp 
demarcation line between tall and short, that doesn’t mean that 
we cannot reasonably de¿ ne some people as tall and other people
as short. 
x
Another logical fallacy is inconsistency, or applying different 
criteria to similar situations when the same criteria really should 
apply. This may create mutually exclusive conclusions, or this may 
be a manifestation of compartmentalization, in which we apply one 
set of criteria to one claim and a completely distinct set of criteria 
to another claim that we also wish to believe, not realizing that the 
two are mutually exclusive. This often involves an area where value 
judgments are being made because it’s easy to alter or tweak those 
value judgments in order to get to a desired conclusion.
x
An example of the application of inconsistency is the kettle defense, 
which comes up frequently in legal trials. The name comes from the 
accusation that someone borrowed a neighbor’s kettle and broke it, 
and then the person who is accused might claim simultaneously that 
they were never given the kettle, that it was broken from the start, 
and that it was in perfect condition when it was returned. All three 
of these claims are mutually exclusive, but if any one of them is 
correct, then that person is not guilty of breaking the kettle. 
x
Similar to inconsistency is a fallacy called moving the goal posts, 
which stems from changing the criteria for acceptance of a claim to 
keep the criteria just out of reach of whatever evidence is presented 

65
or is currently available. This is a method for denying a claim that 
someone does not wish to accept or believe in.
x
For example, someone might keep moving the goal post back so 
that no matter what evidence is presented that the United States 
landed on the moon, it’s still not convincing enough. 
x
The reductio ad absurdum is actually a legitimate way of 
demonstrating that an argument is not sound by showing an 
absurdity to which it leads when carried to its logical conclusion. 
However, the logical fallacy occurs when this legitimate form of 
argument is abused by forcing an absurd conclusion that doesn’t 
À ow from the premises.
x
For example, you don’t believe in aliens or Bigfoot because you’ve 
never seen them, but you’ve also never personally seen the Great 
Wall of China, so therefore, you should be equally skeptical of the 
existence of the Great Wall of China. 
x
The slippery slope argument is similar to the false reductio ad 
absurdum in that it argues that if a position is accepted, then the 
most extreme version of that position must also be accepted
or will inevitably occur. This is often used in political or
persuasive arguments.
x
For example, if assault riÀ es are banned, it will inevitably lead 
to banning sports or hunting weapons and to imposing further 
restrictions on our rights. However, it is possible to ban one type of 
weapon without banning all weapons or imposing further on rights. 
x
In formal logic, a tautology is a statement that must be true in all 
instances. The logical fallacy is the rhetorical tautology, which 
is the needless repetition of an idea in an argument. It becomes a 
fallacious form of argument when it’s used to justify a conclusion. 
In other words, your conclusion and premise are the same, although 
the tautology may be disguised by stating the premise and the 
argument in slightly different ways. 

66
Lecture 8: Logic and Logical Fallacies
x
The petitio principii is the logical fallacy that is also called begging 
the question, which is often misused in common usage to mean that 
something raises the question. In logic, begging the question is 
assuming the conclusion in the premise. In other words, it’s a form 
of circular reasoning.
x
Another form of circular reasoning is the no true Scotsman 
argument, which is a semantic form of circular reasoning. For 
example, if someone makes the claim that all Scotsmen are brave, 
the counter argument might be an example of a Scotsman who is 
a coward. However, the original person making the claim might 
say, “Well, then he’s no true Scotsman.” This is also referred to as 
making a semantic argument. 
x
A false analogy is a form of logical fallacy that makes a point by 
analogy, in which the things compared are not similar in the ways 
that are being compared, rendering the analogy misleading or false. 
x
For example, the evidence for ESP, or extrasensory perception, is 
as statistically robust as the evidence that aspirin prevents strokes, 
so the argument is that, therefore, both claims should be accepted 
equally. However, the claim that we have ESP is not as plausible 
as the claim that aspirin, which has known physiological effects, 
decreases strokes.
x
The genetic fallacy is confusingly named because we use that term 
now to refer to genes or inheritance, but the semantic roots simply 
imply the history that something has. The genetic fallacy involves 
assuming the historical use of something must still be relevant to 
the current use, even when it has changed.
x
For example, the history of the word “sunrise” comes from the 
outdated notion that the Sun rises in the sky—as opposed to the fact 
that Earth rotates on its axis to cause this effect. However, when 
you say that the Sun rises in the morning, that doesn’t mean that 
you are supporting a geocentric view of the universe.

67
x
A straw man argument is extremely common and involves 
responding to an altered version of an opponent’s argument rather 
than the actual position that they’re taking. This is used to set up a 
version of the argument that is easy to knock down—a straw man.
x
There is also what is called the fallacy fallacy, which involves 
assuming that because an argument for a conclusion is unsound, 
that the conclusion must be false. In fact, this is not true. Just by 
pointing out that an argument is fallacious doesn’t mean that you 
can therefore arrive at the conclusion that the conclusion is false—
you have to come up with other reasons.
ad hominem: A logical fallacy in which an assertion is said to be false or 
unreliable because of an alleged negative attribute of the person making the 
assertion; arguing against the person rather than the claim itself. 
anomaly: A phenomenon that is incompatible with or cannot be explained 
by current scienti¿ c theories. 
false continuum: A logical fallacy in which the fact that a characteristic 
varies along a continuum is used to argue that the extreme ends of the 
continuum do not exist or cannot be meaningfully identi¿ ed. 
false dichotomy: A logical fallacy in which multiple choices are reduced 
arti¿ cially to only a binary choice, or where a continuum is reduced to its 
two extremes. 
intelligent design: The term used to self-describe a new school of 
creationism that holds that life is too complex to have arisen from natural 
processes alone. 
paranormal: Any belief or phenomenon that allegedly is outside the 
naturalistic laws of science. 
    Important Terms

68
Lecture 8: Logic and Logical Fallacies
petitio principii: A Latin term for begging the question, or assuming one’s 
conclusion in the premise of an argument.
reductio ad absurdum: A Latin term that refers to a legitimate logical 
argument in which a premise is taken to its logical, although absurd, 
conclusion. This can be a fallacious argument if the absurd conclusion is 
forced and does not follow inevitably from the premise. 
tautology: In logical terms, this is an argument in which the conclusion 
simply repeats the premise and is, therefore, not a true argument. 
Flew, How to Think Straight.
Novella, “Holmesian Deduction.”
1. In what ways can the overapplication of identifying logical fallacies be 
a logical fallacy itself? 
2. What is the proper and fallacious use of analogy in argument?
    Suggested Reading
    Questions to Consider

69
Heuristics and Cognitive Biases 
Lecture 9
T
his lecture explores cognitive biases, the many ways in which our 
thinking is inclined or biased—often in very subtle or subconscious 
ways. An example of a cognitive bias is a heuristic, which is a mental 
shortcut that works most of the time but that biases thinking in speci¿ c ways 
that can lead to making erroneous conclusions. The worst kind of bias is 
the one of which you’re not aware. However, when you’re aware of biases, 
you can engage in metacognition, which loosens the grip that they have on
your thinking.
Types of Heuristics
x
Cognitive biases affect the way we argue and the way we think. 
Our minds will tend to take a path of least resistance unless we 
make a speci¿ c high-energy effort to step out of these processes and 
think in a more clear and logical manner.
x
Cognitive biases are often related to logical fallacies: They lead us 
into invalid or fallacious thinking rather than into formal logical 
ways of thinking. These biases are numerous, pervasive, and can 
have a very powerful inÀ uence on how we think.
x
A heuristic is a type of cognitive bias that is de¿ ned as a rule of 
thumb that we subconsciously apply—a mental shortcut. The 
conclusions that we arrive at through these heuristic methods may 
be correct much of the time; only a little accuracy is sacri¿ ced for 
the ef¿ ciency of decision making.
x
We often think of heuristics as simple common sense, but they are 
not strictly logically correct, which means that some of the time 
they can lead us to incorrect conclusions. 
x
You could also think of them as rules for making decisions, making 
judgments, or solving problems. They may be practical; the trial-

70
Lecture 9: Heuristics and Cognitive Biases
and-error approach is a common heuristic. However, in our 
complex modern world, heuristics of which we are not aware can 
lead us astray. 
x
Psychologists have theorized that heuristics are a way of 
substituting a simple computational problem for a more complex 
one without being aware. This could be very practical and useful—
as long as we’re aware that it’s a ¿ rst approximation of the truth and 
not strictly true.
x
A heuristic called anchoring involves the tendency to focus on 
a prominent feature of an object, person, or event and then make 
decisions or judgments based on that single feature alone. This is a 
way of oversimplifying the complexity that we’re confronted with. 
x
For example, the number of options that exist when buying a 
camera can be dizzying; therefore, advertisers typically boil it all 
down to megapixels, ignoring other very important features—such 
as the quality and size of the lens. 
x
One 
aspect 
of 
the 
anchoring 
heuristic is called 
anchoring 
and 
adjustment, 
in 
which 
the 
¿ rst 
number 
we 
encounter 
tends 
to subconsciously 
bias all of our
later thinking on 
that subject.
x
Anchoring 
is 
used 
widely 
in 
negotiation 
and 
marketing. 
For 
Marketers often use the existence of heuristics 
and cognitive biases to their bene¿ t when 
selling products.
© Creatas Images/Thinkstock.

71
example, marketers may sell products labeled as three for $19.95. 
This subtly encourages shoppers to buy three of the item, even 
though they don’t have to. They could purchase one or two at the 
sale price, but they anchor to the three for $19.95 because that’s 
what they were initially offered.
x
The availability heuristic could be very subtle and powerful in 
its inÀ uence over our thinking. Essentially, what is immediately 
accessible to us—what we can think of—we assume must be 
important and inÀ uential. The assumption is that if we can think 
of an example of something, then that thing must be common or 
representative. It also gives weight to events that are recent, vivid, 
personal, and emotional. 
x
Medical students tend to remember their recent patients—
especially recent, dramatic cases—and they shouldn’t assume that 
what happened with that patient is representative of patients with 
that condition in general because it is a biased dataset.
x
This is related to anecdotal evidence. Essentially, anecdotes are 
experiences that we personally have in our everyday lives that are 
not part of a controlled or experimental condition, but we use them 
as a method for estimating probability.
x
For example, if we’re trying to estimate how common allergies to 
strawberries are, we will tend to think of examples of people who 
have allergies to strawberries and conclude that strawberry allergies 
must be common. However, it could be a minor coincidence that 
you happen to know someone with a strawberry allergy. 
x
Similarly, if the media shows us stories of disasters and crimes 
repeatedly, we will tend to think that they are more common, even 
if they are showing rare events. This has huge implications for 
viewers watching news outlets that have particular biases.
x
Exemplars—which are cases that represent a phenomenon and 
are vivid, dramatic examples—are a reÀ ection of the availability 

72
Lecture 9: Heuristics and Cognitive Biases
heuristic. Exemplars tend to have a greater inÀ uence on our 
judgments than statistical information about the statistical rate at 
which things occur.
x
Exemplars also reÀ ect our storytelling bias. We are social creatures 
programmed to respond to stories, especially emotional ones. This 
is why marketers will use a dramatic story to make a point rather 
than give you dry, statistical information. 
x
Another heuristic is the escalation of commitment. Once we have 
committed to a decision, we tend to stick to that commitment. 
We feel like we have invested in it, and therefore, that feeling 
biases all later judgments about that commitment. We’re overly 
inÀ uenced by what we have already committed to, even if further
commitment is a losing proposition—including money, time, or 
even soldiers’ lives. 
x
The representativeness heuristic is the assumption that, typically, 
causes must resemble effects. Emotionally charged effects, for 
example, we assume must have emotionally charged causes. The 
assassination of John F. Kennedy is an example. Could it be that one 
deranged man acting on his own caused something so momentous? 
That doesn’t seem right. 
x
The effort heuristic is similar to the escalating investment heuristic 
and tells us that we value items more if they require greater effort 
to obtain. This is an example of a mental shortcut, but it is not 
necessarily logically true. We may obtain something easily that 
happens to be highly valuable. 
x
For example, we will spend $100 that we earned through hard 
work much more carefully than we will spend $100 that we found 
lying on the sidewalk. This is what psychologists call the found
money effect. 

73
Types of Con¿ rmation Biases
x
A con¿ rmation bias is one of the most pervasive biases in our 
thinking and is an important one to understand thoroughly. We 
tend to accept information and events that support our beliefs and 
interpret them favorably.
x
For example, consider interpreting a scienti¿ c study. If the 
conclusion of the study is something that we agree with, we accept 
it as a good, solid study. If the conclusion of the study is something 
we disagree with, we’re going to look much more carefully at 
potential À aws in the study to try to ¿ nd some way to dismiss the 
conclusion. Research shows that we expend a great deal of time and 
effort ¿ nding reasons to rationalize the data.
x
By noticing only the evidence that con¿ rms our beliefs, we are 
picking out bits of data from many potential data, and that’s why 
we systematically need to look through data in order to draw any 
meaningful or reliable conclusions about it.
x
An example of a con¿ rmation bias is called the toupee fallacy. 
Some people believe that they can always tell when a man is 
wearing a toupee because when they notice a man wearing a 
toupee, they take that as con¿ rmation of their ability. However, 
they’re not accounting for the fact that they don’t notice when they 
don’t recognize a toupee—that data is completely missing from 
their dataset. 
x
A congruence bias, a very subtle bias that can powerfully lead to 
con¿ rmation bias, is the tendency to test our own theories about 
things but not to also test alternative theories. In order to avoid this, 
we need to conduct observations that are designed to test multiple 
hypotheses—not just our own. This tendency leads people to ¿ rmly 
hold conclusions that may have no statistical basis in reality. 
x
The exposure effect is a form of familiarity bias in which we tend 
to rate things more favorably the more familiar we are with them. 
That’s why repetition is often used in marketing: The more you are 

74
Lecture 9: Heuristics and Cognitive Biases
exposed to a brand, the more familiar you are with it, and the more 
likely you are to purchase it.
x
A choice-supportive bias is a bias in which once we make a decision, 
we then assess that decision much more favorably. This is a way 
of relieving some of the anxiety or angst over whether we made 
the right decision. When we buy something, we therefore have a 
tendency to rate what we purchased much more favorably than we 
did prior to deciding that that’s what we were going to purchase. 
In essence, we’re trying to justify a decision that we already made.
x
Choice-supportive biases sometimes lead to an interesting effect: 
We may downgrade our assessment of the second item on our list. 
For example, when our decision comes down to our ¿ rst and second 
choices, and we ultimately decide to go with what became our ¿ rst 
choice, we will justify that decision by increasing our assessment 
of our ¿ rst choice and downgrading our assessment of the
second choice.
x
Experimentally, subjects are asked, “Number one is no longer 
available; what are you going to take as a replacement?” As a result, 
many skip over their second choice and take their prior third choice 
because they’ve already invested mental effort in downgrading 
their assessment of the second choice. 
x
Another very powerful bias is what psychologists call the 
fundamental attribution error, which is an actor-observer bias, 
or a tendency to explain the actions of others according to their 
personality traits while downplaying situational factors. However, 
we explain our own behavior with situational factors and downplay 
personality traits.
x
For example, if someone trips while walking down the sidewalk, 
we’re likely to conclude that he or she is a clumsy person. If we trip 
when walking down the sidewalk, however, we will blame it on an 
external factor, such as a crack in the sidewalk. 

75
x
Wishful thinking is another bias toward favorable ideas that are 
emotionally appealing to us regardless of the logic and evidence. 
This is also called an optimism bias. For example, this motivates 
people to seek highly implausible—even magical—treatments for 
their ailments over warnings that such treatments do not work. In 
this case, their desire for the treatment to work overwhelms their 
logic. The lottery industry is largely based on this bias. 
x
The Forer effect (also called the Barnum effect) reÀ ects a tendency 
to make judgments about vague or general descriptions and 
interpret them as being speci¿ cally tailored for us—as when reading 
astrological passages. Vague statements tend to be rated much 
more highly accurate than more speci¿ c statements. Essentially, 
con¿ rmation biases, familiarity heuristics, and availability 
heuristics are activated. We look for examples that support the 
statements that we are being told, and when we can ¿ nd them, we 
take that as con¿ rmation that they’re accurate. 
anchoring: The tendency to focus disproportionately on one feature or 
aspect of an item or phenomenon and base judgments on that one feature.
anecdote: An uncontrolled or poorly documented observation or experience. 
availability heuristic: The tendency to believe that a phenomenon is 
more likely or more important if we can readily think of examples of
the phenomenon. 
cognitive bias: A subconscious tendency to think in a certain way, or a bias 
toward certain decision-making pathways. 
congruence bias: The tendency to test our own theories but not alternative 
theories, which can lead to a false sense of con¿ rmation of our own beliefs. 
exemplar: A case that vividly represents a phenomenon, making it seem 
more likely, common, or signi¿ cant.
    Important Terms

76
Lecture 9: Heuristics and Cognitive Biases
exposure effect: The tendency to more favorably rate things or beliefs with 
which we are more familiar. 
Forer effect: The tendency to take vague or general statements and apply 
them speci¿ cally to ourselves, or to ¿ nd speci¿ c examples, making the 
statements seem more accurate and speci¿ c than they are. 
representativeness heuristic: The assumption or bias to believe that causes 
resemble effects. Therefore, for example, a large effect must have had an 
equally large cause. 
Evans, Bias in Human Reasoning.
Gilovich, How We Know What Isn’t So. 
Kida, Don’t Believe Everything You Think.
1. How do subconscious heuristics, or common patterns of thought, often 
lead us astray?
2. How can con¿ rmation bias make us feel very con¿ dent in a belief that 
is entirely false?
    Suggested Reading
    Questions to Consider

77
Poor at Probability—Our Innate Innumeracy
Lecture 10
K
nowledge of mathematics and probability are critical for making 
sense of the world, and while human brains excel at pattern 
recognition, understanding numbers and statistics can be very 
counterintuitive. This lecture will cover our inherent sense of numbers, the 
law of large numbers, and the nature of anecdote. Examples of the effect 
of innumeracy include the power of cold reading and retro¿ tting evidence. 
The solution to our innumeracy is metacognition—understanding the À aws 
in our natural cognitive tendencies—and substituting formal, mathematical 
analysis for our naive senses. 
Numbers and Coincidence
x
Humans are terrible at probability. Our brains are very good at 
certain tasks, such as pattern recognition, but we have a horrible 
innate sense of probability. We especially have dif¿ culty dealing 
with large numbers. We appear to have evolved an intuitive sense of 
small numbers but can only deal with large numbers in the abstract 
language of mathematics. 
x
This innumeracy, or failure to appreciate the statistical power of 
geometric progressions, leads to a number of probability-based 
cognitive biases. For example, we tend to notice coincidence, when 
two events seem to have a connection with each other, and we see 
coincidence as highly improbable.
x
This has led some people to speculate that there are no 
coincidences—that everything happens for a reason. This is part of 
another cognitive bias, the meaning bias, which is the need for the 
world to make sense or have some meaning. 
x
This naive assessment ignores all the many events that happen in 
our lives that do not line up. We experience, hear, see, and dream 

78
Lecture 10: Poor at Probability—Our Innate Innumeracy
thousands of things every day. By random chance alone, events 
should appear to line up occasionally.
x
For example, you dream of a friend you haven’t seen in 10 years, 
and they call the next day. In isolation, it seems amazing, but if 
there were never any such coincidences, that would be unusual and 
would demand some explanation. 
x
The belief in coincidences neglects the fact that there are many 
people in the world. For example, in a city with one million people, 
a one-million-to-one coincidence should happen to someone 
every day. Such stories are likely to propagate because they are 
compelling, so they spread. Therefore, you’re likely to hear stories 
of improbable coincidences. 
x
This is also an example of the lottery fallacy, in which we tend 
to ask the wrong questions. For example, what are the odds of 
John Smith winning the lottery? It’s hundreds of millions to one 
against him winning, so when John Smith wins, we might think that 
it couldn’t have happened by chance alone. The real question is: 
What are the odds 
of anyone winning 
the lottery? It turns 
out it’s pretty good. 
Someone 
wins 
a 
particular 
lottery 
every few weeks. 
It’s 
a 
statistical 
certainty 
that 
eventually someone 
will win. 
x
This 
also 
relates 
to the law of large 
numbers, which states that we are bad at intuitively understanding 
the probability of very large numbers. What are the odds of 
someone winning the lottery twice? This seems vanishingly small; 
The odds of anyone winning the lottery are 
actually pretty good; it’s a statistical certainty 
that eventually someone will win. 
© Ingram Publishing/Thinkstock.

79
it’s hundreds of millions to one against it, squared, but when we 
consider all people playing all lotteries, the probability is quite 
good, and this, in fact, happens on a regular basis. We underestimate 
the probability of events occurring at random.
x
Regardless of the mechanism that an alleged psychic uses—tarot 
cards, astrology, or palmistry—the underlying technique that 
they’re using is called cold reading, which counts heavily on 
con¿ rmation bias. The psychic throws out many statements, and 
they count on the subject remembering all the apparent hits and 
forgetting all of the misses; the subject will search for connections 
that seem to con¿ rm the psychic’s guesses, and then they will take 
that as con¿ rmation. 
x
Experimentally, after a reading, subjects do tend to remember the 
hits and vastly overestimate the accuracy of the reading, but when 
the misses and hits are recorded and counted, there are many more 
misses than hits. 
x
Furthermore, we underestimate how high probability some guesses 
are. This relates to the Forer effect, in which we tend to take vague 
statements and then apply them directly to something about our 
lives. For example, how many people know a woman whose ¿ rst 
name begins with the letter M? This may sound like a speci¿ c 
statement, but it’s a very high probability guess.
Con¿ rmation Biases and Research
x
Scientists believe that anecdotes are a very dubious form of evidence 
because the variables are not controlled and the observations 
are not systematic. The reason why that is a problem is because 
anecdotes are a way of subconscious data mining and are subject 
to con¿ rmation biases, memory effects, and other cognitive biases. 
x
As with coincidences, with data mining, we are not aware that we 
are mentally searching through large amounts of data from our 
everyday lives and looking for patterns. Therefore, seeing a pattern 
should not be surprising. We should expect, in fact, to see patterns 

80
Lecture 10: Poor at Probability—Our Innate Innumeracy
all the time. Anecdotes are a way of remembering hits and forgetting 
misses and seeing patterns in a vast, perhaps unappreciated, set
of data. 
x
Apparent patterns seen with data mining need to be con¿ rmed 
with independent data sets, so a doctor seeing a cluster of patients 
with the same ailments shouldn’t ignore that anecdotal evidence 
but should use it only as a method of generating a hypothesis. 
The hypothesis then needs to be tested in a more rigorous or
systematic way.
x
An example of searching for patterns through large sets of data 
is the fact that in Psalm 46 of the King James Bible, published in 
the 46th year of William Shakespeare’s life, the 46th word is shake 
and the 46th word from the end is spear. What are the odds of that 
occurring? Well, of course, it’s vanishingly small. However, what 
are the odds of some weird coincidence occurring? Because there 
are so many potential coincidences that could occur, the answer is 
that, over time, it is certain that a very low-probability coincidence 
like this will occur on a regular basis. 
x
Nostradamus is a classic example of retro¿ tting, or the process 
of looking—after time has passed—for some kind of pattern 
recognition as a way of mining an inadvertently large data set. 
Nostradamus was famous for his quatrains, most of which are 
vague poetic predictions. If you search throughout human history 
to look for a connection, you may be able to ¿ nd matches to some 
of his phrases.
x
The problem with Nostradamus’s predictions is that they predict 
too much. You could look at any segment of world history and ¿ nd 
some events that seem to match his so-called predictions. Whenever 
Nostradamus made his predictions speci¿ c by mentioning speci¿ c 
names, locations, or dates, they failed miserably. 
x
People can only ¿ nd matches after time has passed through 
retro¿ tting. In science in general, being able to explain things after 

81
time has passed is not a good predictor of a correct theory. You have 
to be able to predict things in the future, and that’s something that 
no one has been able to do with Nostradamus’s predictions. 
Real-Life Randomness
x
We have such a poor intuitive sense of randomness that we are 
incapable of even generating a mathematically random set of 
numbers. For example, we tend to alternate numbers more often 
than a random string of numbers would. We avoid lumping or 
clustering numbers together because clusters of numbers don’t look 
random to us.
x
For example, the stars in the night sky are fairly randomly 
distributed. An even distribution of stars would make it look like 
a grid pattern, and we probably would recognize that as not being 
truly random. The point is that the stars in the sky tend to cluster in a 
way that a random pattern does. Furthermore, we recognize patterns 
in those random clustering of stars; we call them constellations. 
This is what mathematicians call the clustering illusion: We have a 
poor, naive sense of the degree to which randomness clusters.
x
As another example, diseases tend to cluster. They’re not evenly 
distributed throughout society; they’re randomly distributed. The 
Center for Disease Control and other organizations whose job it is 
to track diseases ¿ nd that there are clusters that crop up from time 
to time. After investigation, they determine that most such reported 
clusters are statistical À ukes, but people who experience the 
cluster have a strong belief that the effects are real. However, only 
objective, thorough, rigorous, systematic analysis can determine 
whether something is a random cluster or a real effect. 
x
There are many statistically based biases and false assumptions that 
surround sports. In basketball, for example, we believe that when a 
basketball shooter is doing well, that he is on a streak and is more 
likely to make more baskets—called the hot hands effect. When 
statistically analyzed, there is no real effect. Instead, shooting 

82
Lecture 10: Poor at Probability—Our Innate Innumeracy
streaks tend to follow a random pattern. However, the belief in this 
effect among players and fans is pervasive.
x
Gambling is an exercise in probability, and casinos count on the fact 
that people are terrible at probability. If you À ipped a fair coin and 
it landed on heads 10 times in a row, what is the chance of À ipping 
heads on the next toss? Intuition often tells us that it’s probably less 
than 50/50: You might think that if you’ve just À ipped heads 10 
times in a row, then tails is due or that you’re on a streak of heads, 
so the chances increased. However, if it’s a fair coin, there’s a 50 
percent chance of heads and a 50 percent chance of tails on any À ip.
x
In fact, there is a fallacy called the gambler’s fallacy, which 
occurs in not recognizing the fact that each coin À ip is a completely 
independent event. What has happened previously, therefore, does 
not have any inÀ uence on the future events, and believing this 
is akin to magical thinking. Gamblers, just like sports fans and 
players, engage in superstitious, magical thinking to gain some 
sense of control over the randomness of events. 
x
Regression to the mean is the occurrence of what is called the 
drunken walk of randomness. As a drunkard will lurch at random to 
the left or right, a statistical effect may deviate from the statistically 
average effect but then tend to come back to an average effect—or 
regress to the mean—by random chance alone.
x
Two statistically unlikely events are unlikely to occur in a row. 
Therefore, players that perform statistically above their average are 
likely to regress to their average performance. Because statistics 
doesn’t seem like an answer to us, we invent magical thinking to 
explain this illusory effect of randomness.
x
For example, there is something called the Sports Illustrated curse. 
After a player appears on the cover of Sports Illustrated for having 
an above-average year, they will then have a bad year. Regression 
to the mean alone explains this occurrence. We don’t need to 

83
hypothesize that the player became overcon¿ dent or sloppy or 
perhaps was distracted by all the media attention, but we often do. 
clustering illusion: The tendency of people to underestimate the clumping 
of statistically random distributions, which gives the illusion of clustering.
cold reading: A mentalist technique by which the reader can seem to have 
speci¿ c knowledge of the target (the subject of the reading) using vague or 
high-probability guesses and feedback. 
gambler’s fallacy: The false belief that prior events dictate the probability 
of future events, even when they are random and independent, such as the 
results of random coin À ipping. 
lottery fallacy: The fallacy of using a speci¿ c post-event outcome to 
calculate the pre-event odds of any outcome. For example, the odds of a 
speci¿ c person winning the lottery may be very low, but that does not mean 
that the event is too unlikely to have occurred by chance alone because the 
probability of anyone winning was high. 
regression to the mean: A statistical phenomenon in which large deviations 
from average behavior are likely, by chance alone, to return to more
average behavior. 
retro¿ tting: Fitting predictions to known outcomes after they occur. 
Paulos, Innumeracy.
Randi, The Mask of Nostradamus.
Vos Savant, The Power of Logical Thinking.
    Important Terms
    Suggested Reading

84
Lecture 10: Poor at Probability—Our Innate Innumeracy
1. What errors in thinking result from the lack of an intuitive understanding 
of randomness?
2. How do mentalists use cold reading to appear to have speci¿ c knowledge 
about their targets?
    Questions to Consider

85
Toward Better Estimates of What’s Probable
Lecture 11
T
his lecture will cover the nature and perception of statistics and 
probability. Throughout this course, human cognitive strengths and 
weaknesses are examined, and statistics and probability are general 
areas of extreme weakness for most people. Of course, however, there is 
a lot of variation. Some people do have a better mathematical sense than 
others, but average people have a terrible sense of numbers. Fortunately, it’s 
easy to compensate for our innumeracy with learned skills—speci¿ cally in 
regards to mathematics. 
Statistically Based Cognitive Biases
x
We tend to overestimate the probability of something happening if 
we see it, which relates to the availability heuristic. We tend to latch 
onto examples that are available to us and that we are aware of.
x
In fact, we tend to worry about insigni¿ cant risks. The risks that 
are available to us—that are dramatically portrayed or perhaps are 
related to some deep-seated fear—may be evolved. However, we 
tend to ignore risks that are much more likely and much more real 
but that don’t capture our attention or are not dramatically portrayed 
in the media.
x
For example, the chance of dying from accidental poisoning in the 
United States occurs one in 19,400 times per year—many orders of 
magnitude greater than shark attacks or lightning strikes. 
x
There is a type of gambling fallacy called playing with house 
money in which we will tend to increase our bets in a casino over 
time as we win. This is because we have the sense that we’re not 
playing with our own money—it’s money that we won from the 
house. Therefore, our risk aversion decreases, and we increase the 
size of our bets. However, the odds have not changed. This fallacy 

86
Lecture 11: Toward Better Estimates of What’s Probable
stems from the tendency to link previous events with future events 
when they are statistically independent. 
x
Related to the house money effect is the break-even effect. After 
losing, we also tend to increase our bets in order to win back our 
losses. We would rather break even than end a night of gambling 
having lost. Interestingly, we are more averse to losing than to 
taking a further risk.
x
When gambling over a course of time, you will win sometimes 
and lose sometimes, which is random—as long as the house isn’t 
cheating. There is usually a small statistical advantage to the house, 
but if we ignore that, winning and losing will essentially occur in a 
random pattern. 
x
When a player is winning, the player is free to keep playing until 
he or she starts losing. However, when a player is losing, there 
is no point at which the house is forced to stop. In other words, 
when a player is losing, he or she is going to run out of money at
some point.
People tend to increase their bets in a casino over time as they win because 
they feel as if they’re not playing with their own money.
© Jupiterimages/Thinkstock.

87
x
In fact, even if there were no statistical advantage to the house, 
casinos would still rake in lots of money because there is what 
statisticians call an absorption wall at one end but not at the other. 
When a player loses all of his or her available money, that’s an 
absorption wall—at which point he or she has to stop gambling. 
However, when a player is winning, there is no point at which the 
casino goes broke and the player has to stop gambling. 
x
Another statistically based cognitive bias is what statisticians call 
the base-rate fallacy. For example, test X for disease A has a false 
positive rate of one percent and a false negative rate of one percent. 
Therefore, in the affected population, 99 percent of people will test 
positive, and in the unaffected population, 99 percent of people will 
test negative. 
x
John tests positive for disease A. What is the probability that he 
actually has disease A? Our naive sense might be that it’s 99 percent 
because that’s the accuracy of the test, but the answer actually 
depends on the base rate—or the broad likelihood of a particular 
event—of disease A. There is a tendency to fail to consider the base 
rate when thinking about probability or statistics. 
x
For example, if one person in 1,000 has disease A, then 10 people 
per 1,000 will test false positive with test X. A one percent false 
positive rate over 1,000 people means that there will be 10 false 
positives in 1,000 people, and there will be 0.99 people who will 
test true positive. In other words, there will be about 10 false 
positives for every true positive. 
x
Therefore, if John tests positive, there is a one in 11, or only about a 
0.09 percent, chance that he is true positive, but that’s not our naive 
sense. There’s a greater than 90 percent chance that John does not 
have a disease he tests positive for—even with a highly accurate 
test that only has a one percent false negative and a one percent 
false positive rate. 

88
Lecture 11: Toward Better Estimates of What’s Probable
x
This has huge implications for screening programs. In the general 
population, the false positive rate may vastly exceed the true 
positive rate, depending on the base rate of the disease. This can 
lead to follow-up tests that may be more invasive and more risky 
than the screening test that is being recommended. It may cause 
more harm from unnecessary tests and treatments than is bene¿ cial 
from early detection. 
x
This seems counterintuitive: How could a screening program that 
detects disease early actually hurt people? When the base rate is 
low enough, the false positive rate can vastly outnumber the true 
positive rate, leading to unintended consequences of complications 
and side effects from further testing and interventions that result 
from those false positive tests. In designing a screening program, 
you always have to consider the base rate.
Overestimation and Probability Puzzles
x
In estimating probability, we tend to be favorable to ourselves 
because of our need for self-esteem. We like to think positively 
about ourselves and to present ourselves positively to others. We, 
therefore, tend to overestimate our own ability, which is called 
overprecision. Additionally, we tend to overestimate our relative 
ability compared to others, which is called overplacement. 
x
Probability puzzles are a good way to demonstrate our inherent 
lack of intuition about probability. For example, how many people 
would you need to have in a room in order to have a greater than 
50 percent chance that two of them share a birthday? The answer 
is only 23, but most people naively guess that the number is much 
higher. Similarly, how many people would you need to have in a 
room for there to be a 97 percent chance that two of them share a 
birthday? The answer is only 50 people.
x
In the Monty Hall problem, you are playing Let’s Make a Deal with 
Monty Hall, and you have three doors in front of you. Behind one 
door is a new car; behind two doors are goats. You are asked to 
select a door, and of course, you are trying to select the door that 

89
has the new car behind it. It’s important to note that Monty Hall 
knows where the car is.
x
You make your guess, and then regardless of what door you guess, 
Monty Hall will open up one of the other doors to reveal a goat. 
He then asks you if you would like to change your choice to the 
remaining door or stick to your original choice. What should
you do? 
x
The intuitive answer that many people give is that it doesn’t matter 
whether you choose a new door or stick with your original guess; 
either way, your chance of winning is 50/50 because there are two 
doors left, and one has a goat while the other one has a car.
x
However, the truth is that if you stick with your original guess, your 
chance of winning is one-third. If you switch to the new door, your 
chance of winning increases to two-thirds.
x
To understand this problem, it’s key to remember that Monty Hall is 
giving you new information by opening up one door and revealing 
a goat because he knows where the goats and the car are.
x
Initially, when you pick one of the three doors, your chance is 
one-third, and it remains one-third if you stick with that choice. 
However, if you wish to switch to the other door, in essence, 
you are switching to the other two doors because no matter what 
you chose the ¿ rst time, Monty Hall will reveal a goat. Because 
you’re choosing two doors at the same time, your odds increase to
two-thirds.
Real-Life Innumeracy
x
Multilevel marketing schemes prey on our innumeracy. To 
maintain the pyramid structure of a multilevel marketing scheme, 
geometrically more people would have to be recruited.
x
For example, salespeople are encouraged to increase the number 
of people they recruit, who then sell for them. Sometimes they’re 

90
Lecture 11: Toward Better Estimates of What’s Probable
required to recruit a certain minimum number of people in order 
to make back their investment. Then, the recruited people have to 
recruit the same number of people to make back their investment. 
These progressions are inherently unsustainable, and we 
underestimate how quickly a community, city, region, or even the 
world can be saturated with any multilevel marketing program. 
x
It’s also very easy to use the public’s naive sense of statistics to 
create an impression that is misleading. A very common example 
that crops up with medical issues is the difference between relative 
risk and absolute risk. For example, one person in 10,000 gets 
disease A, and an environmental risk factor increases that to two 
in 10,000. The relative risk is 100 percent, or double. However, the 
absolute risk only increases to two out of 10,000, or 0.02 percent. 
Therefore, the relative risk can be used to exaggerate the effects of 
both risk factors and treatments, and it is misleading if it’s not made 
clear which type of risk you’re dealing with. 
x
Another common statistical fallacy is called the sharpshooter 
fallacy, whose name alludes to an analogy of shooting a gun at the 
side of a barn and then drawing a target around the hole, claiming 
that you shot a perfect bull’s eye. This might seem like an obvious 
fallacy, but it’s amazing how often that kind of behavior occurs but 
is hidden in more subtle ways. 
x
The sharpshooter fallacy relates to choosing criteria, or interpreting 
the signi¿ cance of those criteria, after you know what the outcome 
is, which is called post-hoc analysis. Furthermore, this relates to our 
developed ability to mine data for patterns and make connections.
x
For example, researchers have found many apparent connections 
between the assassinations of Abraham Lincoln and John F. 
Kennedy, such as the fact that the assassin who killed Lincoln 
shot him in a theater and then ran to a book depository whereas 
Kennedy’s assassin shot him from a book depository and then ran to 
a theater. After time has passed, these coincidences seem amazing, 
but it’s simply post-hoc analysis.

91
base-rate fallacy: Failure to consider how common a phenomenon is (the 
base rate) when estimating how likely it is, preferring other factors such as 
representativeness. 
multilevel marketing: A corporate structure in which salespeople must pay 
a percentage of their pro¿ t to sponsors above them and, in turn, can sponsor 
salespeople below them who have to pay them a percentage. 
overprecision: A psychological term that refers to the tendency for people to 
overestimate the accuracy of their own knowledge. 
sharpshooter fallacy: Choosing the criteria for success speci¿ cally to match 
the results that are already known. 
Dewdney, 200% of Nothing.
Mlodinow, The Drunkard’s Walk.
1. How is knowledge of statistics and probability critical to
medical diagnosis?
2. Why are most participants in multilevel marketing doomed to
lose money?
    Important Terms
    Suggested Reading
    Questions to Consider

92
Lecture 12: Culture and Mass Delusions
Culture and Mass Delusions
Lecture 12
I
n addition to our evolved tendencies and personal quirks, we are strongly 
inÀ uenced by those around us and our culture. We can even get caught 
up in group or mass delusions. This lecture will discuss the effects of 
delusions and culture on our beliefs. The fact is that people are social as 
well as emotional creatures. We respond to the beliefs of others, to our social 
group, and to the broader culture. It’s important to keep your critical thinking 
active and not to surrender to these group dynamics. 
Delusions of Individuals
x
A delusion is a ¿ xed belief that is not changed even in the face of 
overwhelming contradictory evidence. When delusions occur in an 
individual, the tendency to form delusions is considered a symptom 
of a mental illness.
x
The most common example of a mental illness that is described 
as a tendency to form delusions is schizophrenia, but in reality, 
delusions occur on a spectrum—with schizophrenia being only at 
the most severe end of that spectrum. 
x
To varying degrees, even healthy, normal individuals can have 
delusions. We all have this tendency, and as it becomes more severe, 
we may consider it a disorder at some point. When less severe, we 
may consider it a personality trait.
x
In essence, a delusion involves impaired reality testing and being 
compelled by the patterns we imagine we see in events around us.
x
There are various types of delusions. A paranoid delusion is one in 
which we think that people, organizations, or events are conspiring 
against us. We see a pattern of action that is against our interests. 

93
x
Grandiose delusions are ¿ xed beliefs about our own ability. As 
previously mentioned, all of us have a tendency to overestimate our 
own ability, but a grandiose delusion takes that further.
x
There are also personal empowerment delusions, such as the 
delusion that one might have the ability to À y, and there are 
delusions of hopelessness, which occur with severe depression. 
One fascinating manifestation of this type of delusion is that some 
people who are very depressed suffer from the delusion that they 
are dead; it is the only way they can make sense of the physical 
sensations they have. 
Delusions of Groups
x
Delusions can affect more than just an individual person at one 
time. A folie à deux is a delusion shared by two people. Typically, 
there is a dominant person, the one with the ¿ xed delusion, which is 
where the delusion originates. There is also a more passive partner, 
who is considered to be the recipient of the delusion.
x
A folie à deux delusion tends to occur in isolation with a very close 
intimate relationship. The delusion usually resolves in the induced 
or passive individual with separation, although it does not resolve 
in the dominant or originating person because they’re the ones with 
the disorder. 
x
When more than two people share a delusion, it’s called a group 
delusion, which occurs in a small group of people. This could be a 
family or perhaps even an isolated cult. Often, there is a charismatic 
or totalitarian leader to the group.
x
Group dynamics are the dominant factor in group delusions. 
Typically, individual members of the group surrender their will to 
the charismatic leader, who causes them to turn off their capacity 
for critical thinking.
x
Additionally, conformity with the social group is emphasized with 
group delusions. This is a natural tendency that humans share, but 

94
Lecture 12: Culture and Mass Delusions
group delusions emphasize conformity beyond normal amounts. 
This may also involve a theme of us versus them, of purity, or
of salvation. 
x
The Heaven’s Gate cult is an example of this type of group delusion. 
Marshall Applewhite was a charismatic cult leader who led a group 
of about 40 members to engage in a suicide pact and commit 
suicide. They believed that they needed to leave their physical 
bodies so that they could ascend to an alien-guided spacecraft, 
which was passing Earth behind Comet Hale-Bopp.
Mass Delusions
x
A mass delusion occurs when more than a small group is involved 
in a delusion. Psychological factors, illusions of perception, 
fallacies, fantasy creation, and social conformity are factors that are 
involved in the various types of mass delusions.
x
In a community threat, there is an exaggerated sense of danger that 
can persist for weeks or even months. It will tend to come and go 
in waves, so the level of panic is not always persistent or constant. 
It does not result in a panic or a À eeing from the community, so it 
is essentially a low to moderate level of continuous anxiety that is 
increasing and decreasing over time.
x
An example of this is the Seattle windshield-pitting epidemic of 
1954, in which there was widespread belief that pitting damage 
was being done to windshields. The damage was ¿ rst blamed on 
vandals, but once it became more widespread, there were fears of it 
being something dangerous, such as radioactive fallout. Reports of 
windshield pitting were reinforced by the media, but investigations 
revealed that people were just noticing pits that had been there all 
along. In essence, this windshield-pitting epidemic was created out 
of social means and contamination.
x
A community À ight panic is similar to this type of panic but is 
more short term and acute. It typically lasts for hours to days as 
opposed to weeks or months. Panic spreads very quickly, resulting 

95
in some people actually À eeing the local area. Because the fear 
is about something that is imminently about to happen, when it 
doesn’t happen after a few days, the fear will then subside.
x
Perhaps the most iconic example of the community À ight panic 
is the incident of Orson Welles telling a story by H. G. Wells. On 
Halloween eve in 1938, a radio broadcast of The War of the Worlds 
was dramatized by Orson Welles. However, he recon¿ gured the 
story so that it was given in the form of dramatic news À ashes, 
and the story was transposed so that the locations were in 
contemporary America as opposed to the original setting of the 
story in England. Many people tuned in late to the broadcast and 
missed the introduction to the story. In addition, radio was a new 
form of mass media, and people expected to hear news on the 
radio—not ¿ ction. 
x
As a social species, we are programmed to respond to the fears 
and the panic of other people in our community. Community À ight 
panic is simply that defensive or adaptive mechanism going out of 
control in response to a false or delusional panic. 
x
There are also larger delusions that go beyond just a single small 
community that are called symbolic community scares, which tend 
to wax and wane over years. These can involve entire countries and 
even continents. They often involve fears of a moral or existential 
threat that derive from some cultural fear that is held by many 
people in a broad culture or community.
x
For example, threats from immigration represent this type of 
community scare, in which there is the perception that out-of-
control immigration is allowing unsavory or perhaps criminal 
segments into a society to the extent that the purity, the safety, 
and the very makeup of a society is being threatened by this long-
term phenomenon. 
x
Witch hunts are perhaps the most iconic example of this type 
of mass delusion. In the United States, the Salem witch trials of 

96
Lecture 12: Culture and Mass Delusions
1692 led to the 
conviction 
and 
execution of 19 
people. However, 
the same thing 
happened 
in 
Europe between 
1470 and 1750, 
during which an 
estimated 40,000 
to 
100,000
witch executions 
took place. 
x
Collective 
wish 
ful¿ llment is a 
type of mass delusion that is similar to a symbolic community scare, 
but it involves something desirable rather than a deep-seated fear.
x
A classic example of collective wish ful¿ llment is the fairy sightings 
that were common in Europe in previous centuries. Typically, these 
types of sightings are culturally speci¿ c. In other cultures, this may 
manifest as Virgin Mary sightings or UFO sightings.
Mass Hysteria
x
Mass hysteria is similar to mass delusion, but it is different in that 
it speci¿ cally involves physical symptoms. The term hysteria has 
fallen out of favor recently because of its cultural misogynous roots; 
however, the term mass psychogenic illness means essentially the 
same thing and has largely taken its place. 
x
Mass psychogenic illness episodes are characterized by having a 
rapid onset and resolution of symptoms. They tend to spread by 
social information contact; in essence, the information is considered 
to be the contagion that spreads the psychogenic illness. Often, the 
alleged illness lacks biological plausibility, and the symptoms tend 
The witch trials of 1692 that took place in Salem, 
Massachusetts, serve as an iconic example of 
mass delusion. 
Library of Congress, Prints and Photographs Division, LC-
USZ62-94432.

97
to be vague and nonspeci¿ c—the kind of symptoms that even a 
healthy person can have from time to time.
x
A classic example of mass psychogenic illness is the Pokémon 
panic of 1997. Pokémon, a Japanese cartoon, involved battles in 
which an electrical attack was portrayed by À ashing lights, creating 
a strobe effect. It is true that some children have photosensitive 
epilepsy and could have had seizures as a result of seeing such 
images, but the few cases of legitimate seizures led to hundreds of 
children reporting episodes. In reviewing the reports, it seemed that 
the symptoms spread through information contagion, although the 
panic was triggered by what were probably some real seizures. 
x
Another common example is called the sick building syndrome. 
This involves buildings in which many people report symptoms, 
often over a period of years. This has even led to buildings being 
torn down and rebuilt to get rid of whatever it was in the building 
that was causing the illness. 
x
There may be genuine environmental cases of something in a 
building that is an irritant and can cause symptoms; however, upon 
careful investigation, many of the cases reveal no environmental 
contaminant or cause of the symptoms. The symptoms are often 
vague, nonspeci¿ c, common symptoms that occur frequently 
in daily life, including dizziness, headaches, blurry vision, and 
shortness of breath. 
x
There are also many product rumors. For example, the idea spread 
that silicone breast implants could cause autoimmune diseases, 
which led to the belief among many women who had silicone breast 
implants and autoimmune diseases that it was the implants that 
caused their illness. This led to multiple lawsuits, but after all the 
data was carefully analyzed, it turns out that there was no increased 
risk of getting any autoimmune disease in women who had
breast implants. 

98
Lecture 12: Culture and Mass Delusions
x
Often, such rumors are promoted by poor investigations and 
media reports—for example, poor questionnaires that ask leading 
questions or selective anecdotes. 
x
Another form of these types of rumors is urban legends, which 
represent cultural fears and anxiety. They spread by word of mouth, 
and now with the Internet, they spread even more rapidly.
x
One example of an urban legend is the claim that a fast-food 
restaurant accidently served a customer a fried rat—something that 
has never been documented to occur. This urban legend is probably 
based on our fear of being served food by others that we don’t 
completely control.
collective wish ful¿ llment: A form of mass delusion characterized by the 
ful¿ llment of a common desire or wish, such as making a fantastic discovery. 
community À ight panic: A form of mass delusion in which fear or an 
immediate threat spreads through a town or community, causing many to À ee 
from the alleged threat. 
folie à deux: A shared delusion between two individuals, in which one 
person tends to be dominant and the source of the delusion. 
group delusion: A delusion shared among a small group, such as a cult, 
typically led by one charismatic leader. 
mass delusion: A delusion or false belief shared among a large group of 
people—even an entire community or culture. 
mass hysteria: Similar to a mass delusion but speci¿ cally involving
physical symptoms.
sick building syndrome: A form of mass hysteria centering around a 
building that is believed to be the source of a mystery ailment. 
    Important Terms

99
symbolic community scare: A long-standing mass delusion that tends to 
wax and wane over years and is centered around a perceived existential 
threat to the community. 
urban legend: A false belief or story that represents a common fear or 
anxiety in society and spreads largely through rumor. 
witch hunt: The persecution of a person or group using accusations of 
heinous acts, or association with such acts, and using dubious investigating 
techniques designed to achieve the conclusion of guilt. 
Bartholomew, Little Green Men, Meowing Nuns, and Head-Hunting Panics.
———, Panic Attacks. 
Mul hearn, “The Psychology of the Witch-Hunt.”
1. What do documented historical cases of mass delusion teach us about 
the human capacity to form and hold false beliefs?
2. What role does the media play in spreading urban legends and mass 
delusions?
    Suggested Reading
    Questions to Consider

100
Lecture 13: Philosophy and Presuppositions of Science
Philosophy and Presuppositions of Science
Lecture 13
I
n this lecture, you will learn about the philosophical basis for how we 
know what we know, or epistemology. Speci¿ cally, the philosophical 
underpinnings of science—including methodological naturalism, 
scienti¿ c methods, and the limits of science—will be addressed. You will 
also learn about scienti¿ c reasoning, such as the application of Occam’s 
razor. In addition, the history of science reveals a great deal about science as 
a human endeavor and how ideas emerge and evolve within science, so this 
lecture will also address the views of philosopher Thomas Kuhn and post-
modernist challenges to science.
Science and Philosophy
x
Science can only exist within a speci¿ c philosophical framework. 
The philosophy of science de¿ nes the assumptions, methods, 
and limits of science. In this way, philosophy and science are 
complementary intellectual disciplines.
x
Science is the foundation of critical thinking; it involves the 
methods for testing our beliefs about the natural world. The 
strengths of science are that it is transparent, rigorous, systematic, 
and quantitative. In other words, science is a system of methods that 
seeks to compensate for the failings of human thinking, perception, 
and memory.
x
The discipline of philosophy that deals with knowledge is called 
epistemology, which deals with how we know what we know—the 
nature of human knowledge. It addresses questions about what can 
be known, even in theory. 
x
The speci¿ c philosophy under which the methods of science practice 
is called methodological naturalism, which states that material 
effects must have material causes. The material aspect refers to the 

101
stuff of the universe, which is not limited to macroscopic matter. 
Methodological naturalism also follows natural laws. 
x
Philosophical naturalism is similar to methodological naturalism; 
it is the philosophical stance that the material universe is all that 
there actually is.
x
Science, however, does not require such a belief, but it does require 
that we follow the methods that assume there is nothing beyond the 
natural world—or that we, in other words, do not rely upon any 
supernaturalism.
x
Science does make assumptions about the world. There is, for 
example, an objective reality. If we weren’t living in an objectively 
real universe, then it would not be possible to investigate how that 
universe works.
x
Science also assumes that the world is predictable and, therefore, 
ultimately knowable. Furthermore, science does not dictate that 
these things are true, but it requires the assumption that they
are true.
x
There is not a single scienti¿ c method; it’s a collection of methods. 
At the core of scienti¿ c methods is the notion of hypothesis testing, 
or formulating an idea in a manner that it can be theoretically and 
practically subjected to objective testing. This includes the notion 
that the idea must be able to be proven false.
x
Testing is not limited to laboratory experimentation; it could involve 
further observations of the world around us or inferences based on 
direct evidence. As long as evidence is gathered in a systematic way 
that can be for or against one or more theories, then the notion is 
testable and involves science. 
x
Historical science has often come up in this context. We cannot, 
for example, rerun the big bang in a laboratory. However, we 
can ask questions about the existence and nature of the big bang 

102
Lecture 13: Philosophy and Presuppositions of Science
and 
then 
make 
observations 
that 
test those theories. 
For 
example, 
we 
can 
make 
observations of the 
cosmic background 
radiation, 
the 
radioactive 
noise 
that was left over 
after the big bang.
x
We cannot go back 
in time to witness 
evolution happening, but we can make inferences from fossils, 
genetic evidence, and developmental biology. Furthermore, we can 
see components of evolution in laboratories. 
x
Science also needs to be understood as a human endeavor. 
Therefore, science is imperfect and messy, has many false steps, 
and is plagued with bias and error. Fortunately, science is also self-
correcting, which is perhaps its strongest feature.
x
There is also not one pathway of science. An observation leads to 
a hypothesis—which is a guess about what is responsible for the 
observation—that is then tested by an experiment. After the results 
of the experiment, the hypothesis is re¿ ned. Then, the experimental 
phase is repeated.
x
However, science doesn’t necessarily always follow in that same 
order. For example, we may make observations in light of existing 
theories, and those theories may bias the way in which we make 
those observations. We may ¿ nd inspiration from many sources that 
are chaotic, culturally dependent, and determined.
The evolution of various species cannot be re-
created in the laboratory, but inferences can be 
made from fossils.
© iStockphoto.

103
Philosophers and Physicists
x
Philosopher Thomas Kuhn developed the idea of paradigms, 
which are large, overarching ideas. Scienti¿ c progress, he argued, 
is divided into periods of normal science, in which there is slow 
progress and the re¿ nement of ideas within a paradigm, and is 
punctuated by periods of scienti¿ c crisis, in which problems 
cannot be solved by tweaking an existing paradigm. This leads to a 
paradigm shift, in which an explanatory model is entirely replaced 
by a new one. 
x
Kuhn’s ideas were used to further develop the post-modernist 
view that science is culturally dependent and not progressive. Post-
modernists took that view to the extreme by believing that the 
process of one paradigm becoming replaced by the next was not 
inherently progressive and that paradigms could only be assessed 
within their own framework. 
x
Kuhn rejected these ideas, arguing that there is copious evidence 
that progress does occur in science. Some ideas in science are better 
than others, and the long history of science is characterized by a 
slow, continuous grinding forward of producing increasingly better 
models of how the world works.
x
Kuhn’s model was criticized as a false dichotomy. In reality, there 
is a continuum of degrees of progress within science—from tiny 
steps to major shifts in our scienti¿ c thinking. 
x
Philosophers of science have largely moved beyond the post-
modernist view; they now understand that this view was extreme 
and not an accurate description. In fact, the speci¿ c criticism is 
that this view confused the context of discovery, which is chaotic 
and culturally dependent, with the context of later justi¿ cation. 
Regardless of how new ideas are generated in science, they are 
eventually subjected to systematic and rigorous observation, 
experimentation, and critical review. It is later justi¿ cation that 
gives science its progressive nature. 

104
Lecture 13: Philosophy and Presuppositions of Science
x
Another philosophical objection raised is the notion that science 
cannot prove itself. Philosophically speaking, this is true, but it’s 
irrelevant because the philosophy of science doesn’t claim that it 
has objective metaphysical certitude. Science works within the 
philosophical framework of science.
x
However, we can say that there has been a meta-experiment of 
science. If all of the assumptions of science are correct, then the 
endeavor of science should produce some objective, positive results 
over time. 
x
In classic physics, by the end of the 19th century, there was a 
Kuhnian crisis. The physics of the time could not explain several 
observed phenomena, including the orbit of Mercury, the ultraviolet 
catastrophe, and the absence of ether.
x
For example, physicists were trying to ¿ gure out what light 
moves through. At the time, the Maxwell equations told us that 
light travels at a constant speed—but with respect to what? They 
initially believed in the possibility of an ether through which light 
propagates, but research subsequently showed that the ether did
not exist. 
x
Albert Einstein was the ¿ rst to consider that space and time are 
relative and that the speed of light is constant with respect to 
everything. He completed his special theory of relativity in 1905. 
Perhaps, this was the most dramatic paradigm shift in scienti¿ c 
history—from a classical universe to a relativistic universe. 
x
However, despite the explanatory power of his theories, Einstein’s 
relativity was not accepted until it was con¿ rmed by later 
observations and experimentation; therefore, the real credit goes to 
the context of later justi¿ cation. 
Science at Its Best
x
Scienti¿ c methods are used to develop a model of how the world 
works. Hypotheses and theories are only useful to the extent that 

105
they can explain nature, but explanation is not enough. They must 
also make predictions about observations that have not yet been 
made and that can be tested.
x
There are competing theories in science. When there is more 
than one explanation that can account for data we already have, 
there must be a way to separate them experimentally. A theory 
is, therefore, only useful if it makes predictions that are different 
than other existing theories. Often, the most challenging aspect of 
science is ¿ guring out how to test something. 
x
If we have more than one theory that can explain all of the data that 
we have and there is no empirical way to separate those theories, 
then we can employ a philosophical rule of thumb called Occam’s 
razor, or the rule of parsimony, which states that the theory that 
introduces the fewest new assumptions is preferred. This doesn’t 
mean that the theory that has the fewest new assumptions is correct, 
but it at least should be preferred until eliminated. 
x
Otherwise, we could endlessly generate ad-hoc theories to explain 
any phenomenon. This is limited only by human creativity, which 
is fairly extreme. If you can eliminate an element from a theory 
and it makes no difference to the predictions or observations, then 
eliminate it. 
x
Science also needs to be understood as a provisional endeavor. All 
conclusions in science are subject to further evidence and new ways 
of interpreting the data. There’s no metaphysical certitude, which 
means that scientists and critical thinkers need to be comfortable 
with uncertainty. All data has error bars around it, but this does not 
mean that we do not know stuff or that we cannot use the methods 
of science to build reliable models about how the world works. 
How Science Progresses
x
The way in which science progresses depends on where along 
the progressive model a scienti¿ c theory is. In the beginning, the 
progress of science is similar to Kuhn’s paradigm shift model. 

106
Lecture 13: Philosophy and Presuppositions of Science
However, the more a scienti¿ c discipline develops, the less it 
resembles this dramatic paradigm shift and the more it resembles a 
process of re¿ nement.
x
The ancient Greeks knew that Earth was a sphere as a result of 
some basic observations. However, once explorers could sail 
around the world and measure it more accurately, they realized 
that it’s actually an oblique spheroid—it bulges more around the 
equator than around the poles. This method was later replaced by 
modern satellite measurements that were able to detect that it is 
also an asymmetrical oblique spheroid, meaning that the southern 
hemisphere is slightly larger than the northern. Therefore, the notion 
that Earth is a sphere is not wrong; the newer measurements were 
simply re¿ nements. We will not discover tomorrow, for example, 
that Earth is a cube.
x
Many sciences are established to such a high degree that while 
re¿ nements are always possible, fundamental knowledge will 
not be overturned. For example, although we are always learning 
more about the details of genetics and how DNA translates into 
developmental biology, the basic fact that DNA is the molecule of 
inheritance is not going to be overturned.
epistemology: The branch of philosophy that deals with knowledge and the 
methods of science. 
Occam’s razor: A rule of thumb, also known as the law of parsimony, 
that states that when two or more hypotheses are compatible with the 
available evidence, the one that introduces the fewest new assumptions is to
be preferred. 
paradigm: A term developed by Thomas Kuhn to refer to a set of scienti¿ c 
beliefs and assumptions that prevail at any particular time in history. 
    Important Terms

107
philosophical naturalism: The philosophical position that the natural world 
is all that exists—that there is nothing that is supernatural. 
post-modernism: A school of philosophical thought that treats all knowledge 
as equally valid social constructs or narratives. 
Asimov, Asimov’s New Guide to Science.
Klemke, Hollinger, and Rudge, In troductory Readings in the Philosophy of 
Science.
Novella, “The Context of Anecdotes and Anomalies.”
1. What is the core methodology that de¿ nes legitimate science?
2. What is the major critique of the post-modernist approach to science and 
scienti¿ c knowledge?
    Suggested Reading
    Questions to Consider

108
Lecture 14: Science and the Supernatural
Science and the Supernatural
Lecture 14
T
his lecture will explore the differences between philosophical 
and methodological naturalism and what is meant by the term 
supernatural. Are there such things as miracles, and can science ever 
answer such questions? What is or should be the relationship between science 
and religion? Progress over time is a more telling feature of science than the 
fact that at any given time there are anomalies we can’t explain. There are 
always going to be anomalies—until science has explained everything about 
the universe. 
Addressing Falsi¿ ability
x
Science is dependent upon methodological naturalism, which 
holds that material effects must have material causes. This is not 
an arbitrary choice, as some may claim. The methods of science 
simply do not work without this underlying philosophical basis.
x
This is because nonmaterial causes cannot be falsi¿ ed; therefore, 
they fail to meet a necessary criterion for science. The reason they 
can’t be falsi¿ ed is that they are not constrained in any way. There 
are no limits to what they can potentially do because they are not, 
by de¿ nition, following the laws of nature—of material cause and 
effect. Furthermore, constraint is necessary for falsi¿ ability.
x
For example, how are the characteristics of ghosts constrained? 
How could we falsify the hypothesis that ghosts are responsible for 
any particular observed phenomenon? If they are outside of what 
we understand as the law of nature, then they could theoretically 
do anything. If they could do anything—if they’re unconstrained—
then they are untestable by the methods of science. In other words, 
is there any observation or experiment that is not potentially 
compatible with the hypothesis that ghosts exist? If the answer is 
no, then ghosts are simply an unscienti¿ c notion. 

109
x
Other examples of claims that cannot be falsi¿ ed might include the 
notion that life is the result of creation. This is often defended by 
saying that the world was created recently but was made to look as 
if it is ancient. We could likewise argue that the world was created 
¿ ve minutes ago but was created to appear as if it is ancient. Such 
notions are not falsi¿ able in the way that they are constructed; 
therefore, they are not science. 
x
Another way in which creation beliefs are unfalsi¿ able is that the 
creator as an entity, by de¿ nition, is not constrained in any way 
scienti¿ cally. The creator could potentially, therefore, have created 
life to look like anything—even as if it had evolved. Therefore, any 
observation of the natural world is compatible with creation; as a 
result, creation is unfalsi¿ able and not a science.
x
Phillip E. Johnson is credited with founding the intelligent 
design movement. He argues for what he calls theistic realism. 
Essentially, he has been on a campaign since the early 1990s to 
allow supernatural explanations back into the realm of science. He 
and others argue that the rejection of supernatural explanations is 
nothing more than prejudice. 
x
This philosophical debate was fought in previous centuries. 
Philosophers already wrangled with this idea of the relationship 
between supernaturalism and scienti¿ c methodology, and it was 
realized by these philosophers that supernatural or miraculous 
causes cannot be introduced into scienti¿ c explanations because 
they’re not constrained and not falsi¿ able.
x
Philosopher Bertrand Russell came up with an analogy to explore 
the relationship between testable and untestable claims when 
making theories and hypotheses that is now known as Russell’s 
teapot: Imagine someone claiming to believe that there is an 
ordinary teapot in orbit around the Sun somewhere between the 
orbits of Earth and Mars. This could be a small, dark object—not 
visible by any instrument we have on Earth. We do not have any 
probes in outer space that could, theoretically, detect such a thing, 

110
Lecture 14: Science and the Supernatural
and even if we did, space is an awfully big place. The amount of 
territory between the orbits of Earth and Mars are so great that 
we can have dozens of probes looking for centuries and would 
still have an insigni¿ cant chance of detecting Russell’s teapot 
in a lonely orbit around the Sun. Therefore, the teapot cannot, in 
practical terms, be proven not to exist. However, does that mean 
that it’s reasonable to conclude that it, therefore, does exist and to 
promote a belief in this teapot?
x
Russell used this example to make the point that the burden of 
proof for any scienti¿ c claim lies with those making the claim. 
The inability to prove something false is not suf¿ cient justi¿ cation 
for the claim. Furthermore, there are endless potential claims that 
cannot be practically or theoretically falsi¿ ed.
x
This also raises the issue of proving a negative in science. While it’s 
not possible to absolutely prove a negative in terms of proving the 
nonexistence of something, you might be able to demonstrate that 
something is impossible, given what we know about the laws of 
science and nature.
x
The conclusion that something like Russell’s teapot does not exist 
can only be as good as the extent to which a thorough search for 
that thing has been conducted. Such searches are typically always 
incomplete and tentative. Still, this is not a justi¿ cation for a 
positive claim. 
x
What if all other explanations for a phenomenon have been ruled 
out? This is an argument from ignorance. Ruling out competing 
hypotheses means the phenomenon is simply unknown. There can 
always be other possibilities that we are currently unaware of, but 
this is related to another logical fallacy: confusing what’s currently 
unknown with what’s unknowable. 
The Epistemological Limits of Science
x
Science only deals with scienti¿ c claims, meaning that any claim 
can be theoretically rendered scienti¿ c by being stated in a way that 

111
is theoretically and practically testable and falsi¿ able. Interestingly, 
almost any claim can be rendered scienti¿ c if it is systematically 
insulated from falsi¿ cation. 
x
Paranormal researchers—those who promote beliefs in the 
supernatural—often try to have it both ways. They claim scienti¿ c 
evidence for the paranormal but then argue that the paranormal 
cannot be disproved. Any evidence against the paranormal 
phenomenon can be dismissed by saying that science cannot test 
it. Then, they employ special pleading in order to remove the claim 
from the arena of science. 
x
A claim must either be within or without the boundary of scienti¿ c 
investigation. Science can only be agnostic toward claims that 
cannot be tested, although philosophically it can be concluded that 
such hypotheses are unnecessary. For example, Occam’s razor can 
be invoked also to favor parsimony.
x
How can we operationally de¿ ne paranormal or supernatural? 
Anything within the natural world is, by de¿ nition, natural and 
not supernatural. For anything that has an effect on the world, that 
effect can be theoretically detected and measured. One could argue 
that any phenomenon that is unknown is simply currently unknown 
and is not necessarily paranormal. 
x
Could we de¿ ne paranormal as any phenomenon that is not just 
unknown but actually does not follow the laws of nature? This is 
not con¿ ned by the known universe and is unfalsi¿ able. The normal 
laws of nature are suspended or violated in the phenomenon that is 
therefore supernatural. 
x
Hypothetically, in a paranormal world, the best science could do is 
identify anomalies, which are things that cannot be explained by our 
current understanding of the laws of nature. Because, by de¿ nition, 
a paranormal phenomenon could not be explained by the regular 
laws of nature, such anomalies would remain forever unexplained 

112
Lecture 14: Science and the Supernatural
by science. This would still not result in positive evidence for the 
paranormal—just a persistent argument from ignorance.
x
However, if there were longstanding, persistent anomalies, that 
would at least be interesting, and it would hint that there was some 
aspect to the universe that science is missing. Could we then know 
that we lived in a supernatural or paranormal world? Additionally, 
what does it mean to be persistent? 
x
The scienti¿ c endeavor is only really a few centuries old and has 
only been institutionalized and robust for the last 100 to 200 years. 
Therefore, we don’t have a long enough experience with systematic 
scienti¿ c investigation of the natural world to have a sense of how 
long an anomaly might persist before we solve it. 
x
There have been questions in science that vexed scientists for 
decades, but then we ultimately did solve those problems. What if 
such anomalies persisted for not just decades, but for centuries or 
even millennia? Is there any point at which we would determine 
that a particular anomaly would never be solved by science and, 
therefore, would point to a supernatural or paranormal world? 
x
The history of science is one of fairly steady progress, and this 
speaks well for the philosophy of methodological naturalism. The 
anomalies of yesterday have largely been solved and have been 
replaced by new, deeper anomalies. As science progresses over 
time, our answers become more subtle and our knowledge becomes 
increasingly deeper. 
x
The scienti¿ c approach is to never give up, and concluding that a 
phenomenon is paranormal or supernatural is akin to giving up. 
Science assumes that anomalies are ultimately understandable and 
then proceeds along the empirical path of scienti¿ c investigation. 
At least so far, this approach has worked out to be very practical 
and useful.

113
x
Miracles are basically the same as paranormal phenomena; they are 
events or phenomena that appear to defy the laws of nature. In order 
to label a phenomenon as a miracle, it still requires an argument 
from current ignorance. 
Faith and Science
x
In the scienti¿ c sense, faith is belief without knowledge; it involves 
believing in something for reasons other than empirical evidence, 
logic, and philosophy. Whether faith and science are incompatible 
depends on how 
you 
de¿ ne 
and 
approach each.
x
People 
are 
free 
to 
believe 
what 
they 
will 
about 
u n a n s w e r a b l e 
questions—those 
that are outside the 
realm of science 
and 
to 
which 
science can only be 
agnostic. There are 
also certain kinds of questions that are inherently outside the realm 
of science: questions of value, aesthetics, and moral judgments. 
These cannot objectively be resolved by empirical evidence. Even 
when science can inform such questions, they are not ultimately 
within the epistemological limits of science.
x
Paleontologist Stephen Jay Gould coined the term nonoverlapping 
magisteria (NOMA) to describe his approach to this question, 
which was to separate faith and science into different magisteria, 
or different intellectual realms. He said that science deals with 
empirical knowledge of nature while faith deals with areas of 
morality and judgment—things that could not be ultimately decided 
with empirical evidence.
When considering whether faith and science 
are incompatible, de¿ ne what questions can 
and should be addressed by each.
© Jupiterimages.

114
Lecture 14: Science and the Supernatural
x
This would mean that faith should not be used to address questions 
that can be answered by science—nor should faith be used to limit 
what science can investigate. At the same time, scienti¿ c methods 
should not be used to address questions that cannot be investigated 
scienti¿ cally because they are not falsi¿ able. Science can only be 
agnostic toward untestable notions. The important thing is to de¿ ne 
what questions can and should be addressed by science and not to 
confuse the two.
x
It’s most important to keep scienti¿ c and unscienti¿ c questions 
separate. Therefore, we should not defend bad science by saying 
that it cannot be tested by science because, then, such propositions 
are outside the realm of science and become faith.
agnostic: The notion that unfalsi¿ able hypotheses, such as the existence of 
God, are not only unknown—but are also unknowable. 
falsi¿ ability: The key feature of any scienti¿ c hypothesis—that, at least 
theoretically, there must be some evidence to prove or imply that the 
hypothesis is not true. 
nonoverlapping magisteria (NOMA): The term coined by Stephen Jay 
Gould to describe his philosophy that science and faith are separate and 
nonoverlapping schools of thought. 
Russell’s teapot: A hypothetical teapot proposed by Bertrand Russell that 
is orbiting the Sun between Earth and Mars to make the point that not all 
claims that cannot be proven false should be accepted as true. 
Hines, Pseudoscience and the Paranormal.
Newberg and Waldman, Born to Believe.
Novella, “New Scientist on Miracles.”
    Important Terms
    Suggested Reading

115
———, “Science and Faith.”
Shermer, How We Believe. 
1. Can science ever prove a phenomenon that is alleged to be paranormal 
or miraculous?
2. What are the implications of anomalies that are currently unexplained 
by science?
    Questions to Consider

116
Lecture 15: Varieties and Quality of Scienti¿ c Evidence
Varieties and Quality of Scienti¿ c Evidence
Lecture 15
T
here are many different kinds of evidence in science: experimental 
versus observational, exploratory versus con¿ rmatory, and basic 
science versus applied science. Scienti¿ c evidence is also dependent 
on the statistical evaluation of data, which can take many approaches. 
This lecture will provide the tools for evaluating the nature and strength 
of scienti¿ c evidence. The complexity of designing scienti¿ c studies 
and evaluating the literature is a way of compensating for the À aws and 
weaknesses in our brains. It’s important to use the evidence to ¿ gure out 
what is true—not to defend what you wish to be true. 
Experimental versus Observational Studies
x
If you want your beliefs and conclusions to be based on solid 
evidence, then you have to know how to interpret evidence, but it is 
not always easy or obvious how to do so. There are different kinds 
of evidence that each have different strengths and weaknesses. 
x
On any complex topic, there’s going to be contradictory evidence, 
so you can’t look at any single piece of evidence and get a full 
picture as to what is going on. You need to know some methods of 
balancing and comparing the different kinds of evidence. 
x
In order to make sense of the scienti¿ c evidence, you need to know 
how to assess an individual study. Then, you need to be able to pull 
it all together, balancing all of the available evidence to compare 
different studies to each other.
x
The different kinds of scienti¿ c studies are roughly divided into two 
broad categories: experimental and observational. Experimental 
studies are designed to perform a speci¿ c intervention; they do 
something to affect nature or people and then measure some 
speci¿ c outcome.

117
x
The goal of a well-designed experimental study is to control for 
as many speci¿ c variables as possible. Ideally, one variable will 
be completely isolated so that the effects of that variable can
be determined. 
x
There are many examples of experimental evidence. For example, 
scientists can inject a drug versus a placebo, or fake drug, into lab 
rats and then measure some speci¿ c outcome.
x
The strengths of experimental studies include controlling and 
isolating variables. They also can be highly quantitative; you can 
measure some speci¿ c feature or outcome. You can also perform 
speci¿ c statistics on the numbers you obtain because there are 
comparison groups. 
x
There are also weaknesses to experimental studies. They may not 
represent, for example, real-world experiences. Often, they’re not 
practical. For example, we cannot withhold a known effective 
treatment or randomize people to be exposed to some toxin or
other risk. 
x
On the other hand, observational studies observe the world without 
doing any speci¿ c intervention—or with only minimal intervention. 
For example, you can correlate a risk factor to a disease. For 
example, how many people who smoke get lung cancer compared 
to those who don’t smoke?
x
The strength of observational studies is that they can gather large 
amounts of data just by looking at the data that already exists. 
You’re also able to compare different groups, and the minimal 
intervention reduces the risk of affecting the natural behavior of the 
system that you’re looking at.
x
There are also weaknesses to observational studies. Observational 
studies do not control many variables, although you can try to 
account for as many variables as you can think of. Therefore, 
observational studies are always subject to unknown variables—

118
Lecture 15: Varieties and Quality of Scienti¿ c Evidence
those that you haven’t thought of or that cannot be accounted for. 
Observational studies generally can only demonstrate correlation; 
they cannot establish de¿ nitively cause and effect. 
x
Experimental and observational evidence are complementary types 
of evidence. They work together to provide different kinds of 
information with different strengths and weaknesses.
Examining the Data
x
Studies with only a few subjects—whether it’s observational or 
experimental—are likely to be erroneous or quirky. Large studies 
are needed for random effects to average out.
x
The statistical signi¿ cance of results is often expressed as a P 
value, which is the probability that you would have gotten the 
results that you did, or greater, given the null hypothesis, which is 
the hypothesis that the phenomenon you’re studying does not exist 
as opposed to the evidence establishing that the phenomenon you’re 
looking at does exist. 
x
However, statistical signi¿ cance is not everything. A systematic 
À aw or bias in how a study is conducted, whether experimental 
or observational, can systematically bias the results in one 
direction. If you do enough trials, this can produce statistically 
signi¿ cant results; however, they will be erroneous because they’re 
representing some bias in the study.
x
Therefore, you also need to consider the effect size, or how large 
the effect is. The smaller the effect size, the greater the probability 
that some subtle bias was the result of that outcome. It becomes 
increasingly dif¿ cult to detect and weed out increasingly subtle 
biases; therefore, very tiny effect sizes are always tricky to
deal with. 
x
You also have to be suspicious of effect sizes that are right at the 
limit of our ability to detect them or that are similar in magnitude 
to the noise in the data—even if it’s highly statistically signi¿ cant.

119
x
You also need to consider whether the data collection was systematic 
and continuous. For example, if people are being surveyed, did 
everyone answer the survey? Were the responders self-selective? 
Was there some bias to the way people were chosen?
x
In a study of patients, was every patient, or were sequential patients, 
included? For example, did the patients who had a good response 
from a drug come back for follow-ups while patients who did not 
have a good response not come back?
x
What was the dropout rate of the study? This potentially introduces 
a further bias into the results. It’s possible that some subjects simply 
do not follow up with the study. A dropout rate of greater than 10 to 
20 percent seriously reduces the reliability of a study and calls the 
results into question. 
x
In other words, is all the data being counted? It’s easy to create 
false results if only a subset of the data is counted—for whatever 
reason. Skipping, dropping, or selecting data can systematically 
bias results, making the outcomes misleading. 
x
You can also ask whether a study is prospective or retrospective. A 
prospective study chooses subjects or objects to be observed and 
then observes their behavior and outcome. A retrospective study 
looks back at events and outcomes that have already occurred. 
Prospective studies are considered to be more rigorous because 
they are subject to fewer confounding factors. 
x
Another critical aspect of a study is whether it is blinded or double 
blinded. In any rigorous study, the scientists that are recording the 
outcome of the results should be blinded to whether what they’re 
looking for is in the intervention or in the control group. This can 
introduce signi¿ cant subconscious researcher bias into the results. 
x
This does not only hold for studies with human subjects. Even when 
measuring inanimate objects, the person doing the measuring needs 
to be blinded to the status of what they’re measuring.

120
Lecture 15: Varieties and Quality of Scienti¿ c Evidence
x
Experimental studies generally should be blinded in order to be 
reliable. Observational studies, on the other hand, can only be 
partially blinded. The bias of the researcher can inÀ uence the results; 
they will tend to get the results they expect. Often, erroneous results 
disappear when proper blinding is put into place. 
x
In a controlled study, you can also ask whether the controls are 
adequate. What is the subject of the study being compared to? Is the 
control treatment, for example, in a medical study truly inactive? 
If you’re using an active control, then that may obscure the 
comparison to the treatment, or the placebo may cause a negative 
outcome, making the experimental treatment seem arti¿ cially 
better. Perhaps the standard treatment to which a new treatment 
is being compared may be ineffective, making the new treatment 
seem more effective than it really is.
x
You can also ask whether the control group being studied is 
representative of the population that you’re interested in. 
Examining the Literature
x
Individual studies can be preliminary and À awed, or they can 
be rigorous and methodologically sound, but either way, it’s 
still a single study. Very few studies are so large, rigorous, 
and unambiguous in outcome that they can stand alone and 
can be considered de¿ nitive studies. You always have to put 
individual studies into the context of the overall research, or the
published literature. 
x
Therefore, the ¿ rst thing to consider when evaluating an individual 
study is whether the study has been replicated by independent labs 
and researchers. If so, is there a consistency to the results, or are 
the results mixed? In addition, did the results look at the same thing 
and control for the same variables? 
x
When looking at any research question—whether a toxin presents 
a risk factor for a disease or whether a treatment works and is safe, 

121
for example—you have to look at all of the literature and put it
into context.
x
One feature of the literature that needs to be taken into consideration 
is called publication bias, or the tendency for researchers to make 
more of an effort to publish their study results when the results are 
interesting, positive, and good for their career and reputation and for 
journal editors to have a bias toward publishing positive studies—
the kind that will get good press releases and draw attention to their 
journal—as opposed to negative results, which are less interesting. 
x
As a result, it may take many years for any speci¿ c research 
question to mature and advance to the point where we’re seeing 
reliable results. What we really expect to see in the literature are 
many small, preliminary studies with a large bias toward being 
falsely positive. Then, as more rigorous studies are conducted, they 
begin to show the real effect.
x
Effect sizes tend to shrink over time as better studies are conducted. 
Even when the effect is real, the effect size tends to be small in the 
more rigorous studies. For nonexistent phenomena, the effect size 
shrinks to zero.
x
One type of analysis that looks at many different studies that are 
addressing similar questions is called a meta-analysis, which 
combines the results of multiple studies into a new statistical 
analysis. This is a way of obtaining greater statistical power.
x
However, the meta-analysis introduces new possibilities for bias. 
If the preliminary studies were poorly designed and biased, the 
meta-analysis will still reÀ ect the bias of those preliminary studies. 
In fact, a meta-analysis is a poor predictor of the outcome of later 
large, de¿ nitive studies. 
x
Another type of review called systematic reviews look at all the 
evidence and consider the quality of each study, looking for patterns 

122
Lecture 15: Varieties and Quality of Scienti¿ c Evidence
in the literature, consistency, replication, and relation to effect size 
and study quality. 
x
While systematic reviews are a great way to look at the evidence 
in the literature, they are also subject to bias. For example, which 
studies do you include in your systematic review? What methods 
do you use to ¿ nd studies to include in the review? What were your 
inclusion criteria? All of these are choices made by researchers that 
can affect the outcome of the systematic review. 
blinding: In scienti¿ c studies, this refers to the process of hiding the status 
of a subject (whether they are in the intervention or the control group) from 
the subject (single blind) or also from the experimenter (double blind). 
control group: In an experimental study, the control group receives a 
sham or placebo intervention that is physiologically inert so that it can be 
compared to the treatment group. 
experimental study: Scienti¿ c studies that involve a speci¿ c intervention 
performed by the experimenters. 
meta-analysis: A mathematical process of combining the results of many 
studies into a single study for statistical analysis. 
observational study: Scienti¿ c studies in which the behavior of groups are 
observed in the real world—without experimenter intervention. 
prospective study: A study that involves selecting subjects and then 
following them to observe their future outcome. 
P value: A statistical term referring to the probability that the results of a 
study would be what they are or greater given the null hypothesis—that the 
proposed phenomenon being studied is not true. 
    Important Terms

123
retrospective study: A study in which subjects are selected, and then data is 
gathered about their history.
statistical signi¿ cance: A statistical term referring to the comparison 
of target and control groups in scienti¿ c studies; when the difference in 
outcome or a particular feature is greater than a predetermined threshold, 
then the results are considered to be statistically signi¿ cant.
systematic review: A review and analysis of all the relevant scienti¿ c studies 
published on a speci¿ c question.
J ohnson, The Ghost Map.
Novella, “Evidence in Medicine: Correlation and Causation.”
———, “Evidence in Medicine: Experimental Studies.”
Taper and Lele, The Nature of Scienti¿ c Evidence.
1. What are the strengths and weaknesses of experimental versus 
observational studies?
2. What factors are most important to consider when evaluating a scienti¿ c 
study—or the literature as a whole—on a speci¿ c question?
    Suggested Reading
    Questions to Consider

124
Lecture 16: Great Scienti¿ c Blunders
Great Scienti¿ c Blunders
Lecture 16
T
his lecture will cover some of the greatest mistakes and blunders in 
the history of science, but it will also show how useful it can be to 
be wrong. The scientists involved in each case made major cognitive 
blunders; they either were controlled by their own biases or failed to question 
themselves. These cases demonstrate clearly why initial skepticism is the 
best response to any new claims. You will also see, in the lessons we derive 
from these examples, many of the principles of critical thinking that have 
been presented throughout this course. 
René Blondlot: N-Rays
x
René Blondlot was a French physicist who in 1903, shortly after the 
discovery of X-rays by Wilhelm Conrad Röntgen, claimed to have 
discovered another form of radiation he called N-rays, which he 
believed were emitted by most materials.
x
N-rays, however, were invisible. They could only be detected by 
refracting the N-rays through an aluminum-coated prism, which 
would then cause a thread coated with calcium sul¿ de to glow 
slightly. Reports vary, but about 30 research teams replicated 
Blondlot’s experiments and saw the glowing in the thread. There 
were about 300 papers total published about N-rays that involved 
over 100 different researchers.
x
However, many prominent labs were unable to replicate the 
results. In addition, N-rays appeared to have seemingly impossible 
characteristics. The inability to see N-rays was often explained 
by believers as the scientists not having sensitive enough vision. 
Alleged photographic evidence was also plagued with sources of 
error, such as À uctuations in exposure time.
x
In 1903, Johns Hopkins physician Robert W. Wood was sent to 
do a ¿ rst-hand investigation of Blondlot’s labs and results. Wood 

125
had experience not only as a scientist, but as a skeptic. In the lab, 
he witnessed several experiments but could not see the effects of 
N-rays that were claimed.
x
In one experiment, he used his hand to block the path of the N-rays, 
but this could not be detected by the experimenters. In another, he 
removed the prism needed to refract the N-rays, and in yet another, 
he replaced the N-ray source with a similarly shaped inert piece of 
wood. These practices were unknown to the experimenters, and 
each time, the change was not detected. 
x
In retrospect, we know that N-rays do not exist and that the 
researchers were engaged in self-deception. They relied on 
subjected outcomes at the very edge of human detection and on 
very highly error-prone measurements to establish an entirely
new phenomenon.
x
N-ray researchers were insuf¿ ciently skeptical of their own results. 
They used special pleading to dismiss the negative results of other 
researchers, and they did not use proper blinding to eliminate the 
effects of their own biases. 
Jacques Benveniste: Homeopathic Dilution
x
In 1988, researcher Jacques Benveniste thought he had discovered 
proof that homeopathic dilutions—diluting a substance beyond the 
point where any original ingredient remains—could still retain the 
chemical properties of what was being diluted. 
x
Speci¿ cally, he was studying the triggering of activity in immune 
cells in response to an allergic trigger. Benveniste found that the 
cells would react even to homeopathic doses of the substance, 
meaning that there was absolutely nothing left—something that all 
of chemistry and biology says should be impossible. The results 
were dependent upon observations of a variable biological system, 
so they could not be replicated in many labs, although some labs 
did report replication. 

126
Lecture 16: Great Scienti¿ c Blunders
x
When Benveniste’s claims were met with skepticism, he claimed 
that he was the victim of closed mindedness—that he was being 
persecuted for challenging the scienti¿ c dogma of the day.
x
Benveniste also invented fanciful explanations for how the diluted 
solutions worked: The antibodies communicated their effects to the 
immune cells remotely through radio waves, and perhaps water 
acted as a template to remember these signals and give them off 
later. This even led him to believe that he could record these effects 
and transmit them over the phone or the Internet. 
x
However, he never tested or tried to demonstrate these assumptions. 
He could have designed fairly objective experiments to show 
if these radio waves existed, but he chose instead to stick with a 
much more tricky, variable biological system in order to support
his claim.
Stanley Pons and Martin Fleischmann: Cold Fusion
x
In 1989, Stanley Pons and Martin Fleischmann called a press 
conference in which they announced that they had experimentally 
produced cold fusion. If true, the announcement could mean a 
Nobel Prize for these researchers. It could also mean a revolution 
in energy production—an end to dependence on fossil fuels and a 
supply of endless cheap, clean energy. 
x
Existing nuclear power plants use the process of nuclear ¿ ssion 
to make heat, which they then use to turn a turbine and generate 
electricity. Fission is the process of splitting apart heavy radioactive 
elements, such as uranium. Fusion is the process of combining light 
elements into slightly heavier elements, such as fusing hydrogen 
into helium. However, tremendous temperatures and pressures are 
required to get atoms to fuse together. We can create nuclear fusion 
for hydrogen bombs, but we don’t have the technology to control 
fusion suf¿ ciently to use it for energy production.
x
Cold fusion, as the name implies, is fusion of light elements into 
heavier elements at low temperatures. Cold fusion would be ideal 

127
for energy production, but we currently do not know how to 
accomplish this. Many researchers have tried to make cold fusion 
happen in their labs, but no experimental design so far has been 
reproducible. Often, the claims are based upon measuring excess 
energy in an experimental setup, but there are potentially many 
sources of energy that is unaccounted for. 
x
It has been suggested, for example, that Pons and Fleischmann’s 
setup—using electrolysis to induce cold fusion in heavy water—
was contaminated with tritium and that this contamination could 
account for the spikes in energy production that they saw that they 
claimed were due to cold fusion. 
x
What followed from the press conference in March of 1989 has 
been referred to as the fusion confusion. After initial excitement, 
by the end of April of 1989, the press announced that cold fusion 
was dead and that Pons and Fleischmann’s experiments were an 
example of pathological science.
x
Pons and Fleischmann appear to have gotten carried away with the 
excitement of their potential ¿ ndings. It is far better to allow the 
wheels of peer review to grind away, to weed out the false hopes, 
before they become a disastrous embarrassment. 
Lord Kelvin: The Age of Earth
x
William Thomson, who would later be known as Lord Kelvin (of 
the Kelvin temperature scale fame), was the most famous physicist 
of his time. His specialty was thermodynamics, the study of heat.
x
Darwin and other contemporaries argued for a very ancient Earth, 
but Lord Kelvin, who was not impressed by the soft sciences of 
biology and geology, set out to calculate the age of Earth using 
the reliable, hard principles of physics. He ¿ gured that Earth 
was cooling, so if it started as an initially molten state, one 
could calculate how long it would take—using thermodynamic 
principles—to cool to its current temperature.

128
Lecture 16: Great Scienti¿ c Blunders
x
In 1862, he estimated the age of Earth as no greater than 100 
million years with a range of 20 million to 400 million years, given 
the uncertainty in the 
measurements at the 
time. He later re¿ ned 
his measurements and 
reduced his estimate to 
only 20 million to 40 
million years. This did 
not leave enough time 
for geological processes 
to create the Earth 
that we know today. 
In fact, Lord Kelvin 
was highly critical of 
Charles Darwin and the 
geologists of his day.
x
Lord 
Kelvin, 
while 
deservedly 
famous 
and 
respected, 
used 
his 
authority 
to 
arrogantly 
push 
his 
calculations for the age 
of Earth. He not only 
criticized geologists, but he casually dismissed much of 
contemporary geology, which was simply incompatible with 
his calculations. He was also hostile in the face of criticism of
his calculations. 
x
However, the deathblow to Lord Kelvin’s calculations came with 
the discovery of radioactivity. Radiometric dating allowed for 
a highly accurate method of dating Earth, with current results 
indicating that Earth is approximately 4.55 billion years old. 
William Thomson (1824–1907), who was 
known as Lord Kelvin after his death, was 
a Scottish engineer, mathematician,
and physicist.
© Photos.com.

129
x
There are several lessons to be learned from the story of Lord 
Kelvin. The ¿ rst is that scienti¿ c authority can never rest in one 
individual—no matter how famous or successful their career. 
x
Furthermore, Lord Kelvin was arrogant in dismissing the ¿ ndings 
of a scienti¿ c discipline of which he was not an expert. He thought 
he could use physics to casually wipe away all of the carefully 
accumulated arguments and evidence from geology. This doesn’t 
make him wrong, but it should have tempered his con¿ dence in
his conclusion. 
x
Finally, it would have been more reasonable to conclude from Lord 
Kelvin’s calculations that the inconsistency between the ¿ ndings 
of thermodynamics and geology—and also evolutionary biology—
meant that there was a piece to the puzzle that was missing, and that 
piece was radioactivity.
John Edward Mack: Alien Abductions
x
In the early 1990s, Harvard psychiatrist John Edward Mack began 
to investigate patients who reported that they were abducted by 
aliens. At ¿ rst, Mack thought they might be suffering from mental 
illness, but most of his patients did not meet the criteria for a mental 
illness diagnosis. He then, therefore, began to take their reports 
much more seriously. 
x
Mack seemed to have fallen victim to his patients’ own beliefs 
or delusions. He committed the false dichotomy that because 
his patients were not mentally ill, their reports were credible. 
He ignored the fact that perfectly healthy individuals can, 
through À awed thinking and other factors, arrive at ¿ rmly held 
but false beliefs. Additionally, he ignored the fact that there was 
no corroborating evidence for the stories of his patients, and he 
engaged in special pleading and the argument from ignorance to 
justify his conclusions. 

130
Lecture 16: Great Scienti¿ c Blunders
Albert Einstein: The Cosmological Constant
x
Albert Einstein fell prey to error in what he called the biggest 
blunder of his career: the incident of the cosmological constant.
x
Einstein published his general theory of relativity in 1916. His prior 
special theory of relativity dealt with the speed of light. The general 
theory, which was much more complex and dif¿ cult for Einstein to 
work out, also accounted for the effects of mass and gravity.
x
Einstein believed, as many others of the time did, in a static 
universe, but the predictions of general relativity led to the 
conclusion that the universe must be either expanding or collapsing. 
Einstein, however, couldn’t accept that, so he introduced a fudge 
factor: the cosmological constant, which is a repulsive force in the 
universe that would exactly balance the attraction of gravity and 
make the universe static.
x
However, Edwin Hubble later observed the redshift of galaxies, 
indicating that they are all moving away from us, and published 
his ¿ ndings in 1929. Essentially, the universe is expanding, and 
Einstein was wrong.
x
Einstein’s blunder was in introducing a fudge factor to rig his 
results and comply with his preconceptions, rather than seeing an 
anomaly pointing to a new fact about the universe.
x
Ironically, in the 1990s, the concept of a cosmological constant was 
resurrected to account for the observation that the universe is not 
only expanding but accelerating. However, this does not rescue 
Einstein from his blunder. 
cold fusion: A hypothetical technique that causes nuclear fusion to occur at 
relatively low temperatures, especially in a way that would be practical for 
energy production. 
    Important Terms

131
N-rays: A hypothetical type of radiation that was allegedly discovered by 
René Blondlot but that was eventually exposed as illusory. 
self-deception: The act of deceiving oneself due to a host of perceptual, 
cognitive, and memory biases and À aws. 
Ashmore, “The Theatre of the Blind.” 
Burch¿ eld, Lord Kelvin and the Age of the Earth.
Youngson, Scienti¿ c Blunders.
1. What failing of critical thinking is common to those scientists who 
commit epic blunders? 
2. What lessons do historical scienti¿ c blunders teach us about the limits 
of authority and the need for replication in science?
    Suggested Reading
    Questions to Consider

132
Lecture 17: Science versus Pseudoscience
Science versus Pseudoscience
Lecture 17
P
seudoscience is a term often applied to science that is so fatally À awed 
that it lacks suf¿ cient quality to even be considered legitimate science; 
however, there is often not a clean line of demarcation between 
legitimate science and pseudoscience. Many claims and beliefs label 
themselves as scienti¿ c, but are they? For example, is anthropogenic global 
warming a legitimate science or pseudoscience? What about cryptozoology, 
the study of unusual creatures unknown to current science, such as Bigfoot? 
This lecture will cover the features that characterize pseudoscience, which 
also serves as a list of behaviors that good science should endeavor to avoid. 
What Is Pseudoscience?
x
It is common to claim that one’s beliefs are based upon legitimate 
science. The mantle of good science is often used to market 
products and promote ideas while the counterclaim that a belief is 
pseudoscience is equally used to denigrate beliefs and practices that 
one does not hold. Science still carries a high degree of respect in 
our society.
x
The term pseudoscience refers to beliefs and practices that claim 
to be scienti¿ c but lack the true method and essence of science. 
They have the patina of legitimate science, but something has gone 
terribly wrong. Pseudoscience goes beyond just making a few 
errors or sloppy practices; the methods are so À awed that the entire 
endeavor is suspicious.
x
In reality, there is not a clean division between pristine science 
and rank pseudoscience. There is, rather, a continuum or 
spectrum between these two extremes. Many legitimate sciences 
may incorporate one or more features normally associated with 
pseudoscience, and some pseudosciences may occasionally make 
a valid point.

133
x
Between these two extremes of science and pseudoscience, there is 
a gray zone. This gray zone does not mean that these two extremes 
do not exist; we can still identify sciences that are mostly legitimate 
and practices that are hopelessly pseudoscienti¿ c. The key is 
to know how to recognize the features of pseudoscience and of 
legitimate science.
x
Examining extreme cases of pseudoscience is like a doctor studying 
an advanced form of a disease: The features will be much more 
obvious and extreme, but they will help the doctor to recognize the 
more subtle signs and milder forms of the pathology. 
x
In the same way, we can study these extreme pseudosciences 
to develop a picture of what pathological features they have in 
common. Then, we can recognize more subtle forms even in 
legitimate science. 
Features of Pseudoscience
x
The most prominent feature of pseudoscience is that it tends to 
work backward from desired results, rather than following logic 
and evidence wherever it leads. This is also referred to as motivated 
reasoning. If we know where we want to get cognitively, human 
beings are good at back¿ lling in justi¿ cations, making evidence ¿ t 
into preconceived notions. 
x
This is why it is so important to be one’s own most dedicated 
skeptic. Legitimate scientists endeavor to disprove their own 
theories and only give provisional assent once a theory has survived 
dedicated attempts at proving it wrong. They also consider alternate 
theories—not just their own theory. Otherwise, they would fall prey 
to the congruence bias, or only testing one’s own theory by looking 
for positive evidence but not testing alternatives that might also 
explain the evidence.
x
Pseudoscientists, in fact, endeavor to prove their theory correct. 
They will only look for con¿ rming evidence and will avoid 
discon¿ rming evidence. They may engage in special pleading and 

134
Lecture 17: Science versus Pseudoscience
try to shift the burden of proof onto others. However, proving their 
theory wrong—or at least attempting sincerely to do so—should be 
their job. 
x
Pseudoscientists also commonly fall prey to con¿ rmation bias, 
which is the process of looking for supportive evidence that leads 
to conclusions we wish to be true. This leads to choosing only 
the evidence that supports one’s own theory—favoring positive 
evidence, regardless of quality, to negative evidence, regardless
of quality.
x
There is also a tendency to rely upon anecdotal evidence and 
testimony. Anecdotes are uncontrolled, or ad-hoc observations. They 
are not systematic; they are, therefore, plagued with con¿ rmation 
bias and recall bias. Pseudoscientists will often heavily rely upon 
this type of evidence because, essentially, they could make it say 
whatever they want it to say. 
x
Favoring such low-grade evidence over more rigorous evidence 
because it gives the desired results can be used to support just about 
any belief—no matter how implausible it is. Emotional appeal is 
also a common tactic among pseudoscientists who are trying to 
defend positions that the numbers do not support. 
x
Additionally, core principles of a particular area of pseudoscience 
may be based upon a single case or observation rather than a large 
body of carefully collected data. They use preliminary evidence, or 
even a single anecdote, as a basis for an elaborate system of belief. 
They are essentially making the hasty generalization logical fallacy 
by basing far-reaching principles on a single piece of, perhaps, 
unreliable or À imsy evidence.
x
For example, D. D. Palmer based the principles of subluxation 
theory on a single case of an alleged curing of a person of deafness 
with neck manipulation. He then extrapolated from this chiropractic 
theory. Furthermore, the founder of iridology based the notion that 

135
the iris of the eyes reÀ ects health and disease of the whole body on 
the observation of a single owl and its broken wing.
x
Principles may not be based on just a single observation 
but on a philosophical idea, a philosophy that itself has not 
been empirically tested or 
developed 
as 
a 
scienti¿ c 
theory or discipline. 
x
The notion of life energy, for 
example, is a prescienti¿ c 
idea, 
but 
it 
forms 
the 
basis 
of 
many 
so-called 
alternative therapies, such as
therapeutic touch, acupuncture, 
straight 
chiropractic, 
and
even homeopathy.
x
Often, pseudoscience involves 
grandiose claims based on 
preliminary or À imsy evidence. 
This is sometimes called the 
Galileo 
syndrome 
for 
the 
frequent tendency to compare 
oneself to Galileo. In other 
words, 
far-reaching 
claims 
that overturn entire segments of well-established science are 
extrapolated from very little research or small bits of evidence.
x
This tends to occur with pseudoscienti¿ c endeavors because when 
theory conÀ icts with established science, rather than reassessing 
one’s own theory, the pseudoscientist will simply broaden the 
implications of their theory, claiming that mainstream science must 
be wrong because it conÀ icts with the theory.
x
In extreme cases, this leads to what proponents call alternative 
science, which results from this chain reaction of pseudoscienti¿ c 
Acupuncture is an alternative 
therapy that is formed on the 
basis of the notion of life energy, a 
prescienti¿ c idea.
© iStockphoto.

136
Lecture 17: Science versus Pseudoscience
claims. In the end, you have replaced all of science with an 
alternative version of reality. 
x
Another example is the comic book artist Neal Adams, who is a 
proponent of the idea of a hollow, growing Earth. This is the notion 
that Earth was much smaller in the historical past and has been 
slowly getting larger over time by the generation of new matter. He 
believes this is true because the continents of Earth ¿ t together like 
puzzle pieces.
x
When Adams’s ideas run up against laws of physics, he just 
rewrites them ad hoc. For example, he has replaced gravity with 
magnetism as the force that holds planets in place. He has added the 
spontaneous creation of matter from nothing. He has even rewritten 
all of modern geology—from tectonics to volcanism.
x
Pseudoscientists are also known for making very bold claims that 
are often absolute. The bottom line is that they go way beyond the 
evidence. It’s OK to make grandiose claims as long as you have 
the evidence to back them up, and even modest claims could be 
pseudoscienti¿ c if they extend too far beyond what the evidence 
can meaningfully support.
x
Good science, rather, is very careful and conservative. It tends not to 
make claims that exceed the evidence; it tends to be understated if 
anything. For example, in the process of peer review, when experts 
in a ¿ eld review a paper submitted by one of their colleagues in the 
hopes of being published, one of the speci¿ c things they have to 
decide is whether the conclusions of the researcher extend from the 
evidence. If the authors are making conclusions that go beyond the 
evidence, they will often be required to ¿ x that before the paper can 
be accepted for publication. 
x
Another aspect of pseudoscience is that simple answers are often 
offered to complex questions. While scienti¿ c progress often leads 
to simple or elegant solutions, pseudoscientists offer simplistic 
solutions even to very complex phenomena.

137
x
For example, we often call these a theory of everything. Scientists 
are legitimately looking for increasingly powerful and elegant 
theories that can explain more of the natural world, but when that 
process is taken to an extreme—where one small phenomenon is 
used to explain our entire understanding of the universe—then that 
becomes a theory that is exceeding the justi¿ cation.
x
Pseudoscientists also often demonstrate hostility toward scienti¿ c 
criticism. In the process of peer review, in which a community of 
scientists is essentially highly critical of any new claims that are 
made, scientists try to tear scienti¿ c theories apart. It’s the only way 
to separate those ideas that are useful and have potential from those 
that are a dead end. 
x
While nobody likes to be criticized, scientists have to develop 
a thick skin because criticism is part of the process of science. 
Pseudoscientists, however, generally cannot accept mainstream 
harsh criticism. As a result, they often do not engage with the 
scienti¿ c community. They claim that they are the victim of a 
conspiracy or a dedicated campaign against their ideas—perhaps 
because their ideas are simply too revolutionary.
x
Pseudoscientists may appeal to antielitist sentiments. As part of 
their antielitism, they may try to make a virtue of their own lack 
of education, training, or experience. They make claims that 
knowledge puts blinders on mainstream scientists and that only 
they have the vision to see things the way they truly are.
x
Pseudoscientists also use scienti¿ c-sounding terms to impress 
others but lack scienti¿ c precision. Scienti¿ c jargon exists for 
a reason: to have precisely and unambiguously de¿ ned ideas. 
However, pseudoscientists use words to obfuscate. They use terms 
that are often vaguely de¿ ned or have a shifting meaning.
x
For example, proponents of intelligent design often imprecisely 
use the term “information” by shifting meaning at different times 
without ever using a precise de¿ nition. They get tangled in their 

138
Lecture 17: Science versus Pseudoscience
own confusion. Another common example is the vague use of the 
term “energy.”
x
Pseudosciences are also marked by a failure to progress. Sciences 
that are legitimate and useful tend to progress over time whereas 
pseudosciences tend to be stagnant. Pseudosciences are chasing 
their tail or are endlessly trying to establish their basic principles 
or the very existence of the phenomenon that they are studying, but 
they do not build a body of evidence.
x
Research surrounding ESP is the best example, in which 
pseudoscientists are still searching for a reproducible research 
paradigm. As one paradigm fails, they just continue onto the next 
paradigm with nothing to show for it in the long term.
x
Anomaly hunting is another feature that is common to 
pseudosciences. Anomalies are very usual in science because 
they point to a shortage or hole in our current understanding, 
but looking for anomalies as a way of establishing a conclusion 
is called anomaly hunting, which does not seek to falsify or
explore alternatives.
Gardner, Science.
Novella, “Anomaly Hunting.”
———, “Anatomy of Pseudoscience.”
———, “Ghost Hunting Science versus Pseudoscience.”
Pigliucci, Nonsense on Stilts.
Randi, Flim- Flam!
Schick, Jr. and Vaughn, How to Think about Weird Things.
Shermer, The Borderlands of Science.
    Suggested Reading

139
1. What feature most characterizes pseudoscience? 
2. How does studying pseudoscience help the practice of legitimate 
science?
    Questions to Consider

140
Lecture 18: The Many Kinds of Pseudoscience
The Many Kinds of Pseudoscience
Lecture 18
I
n the previous lecture, you learned the difference between legitimate 
science and À awed pseudoscience. In this lecture, several speci¿ c 
examples of pseudosciences will be introduced and then deconstructed 
to illustrate some of the various features of pseudoscience. Believers in these 
pseudosciences think they are doing science, but they all have one common 
À aw: They are insuf¿ ciently skeptical of their own claims. They are dazzled 
by what they think they have discovered and set out to prove their ideas 
correct—instead of setting out to prove their ideas false. 
Iridology as Pseudoscience
x
Iridology is a practice that was invented in 1800 by Ignaz von 
Peczely. He based his entire belief of iridology on a single 
observation, in which he accidentally broke an owl’s leg and noticed 
a black stripe in the iris of its eye. When the owl’s leg later healed, 
the black speck went away. He concluded from this that the iris is a 
map of the entire body where the À ecks and color reÀ ect the state of 
health or illness of each different part of an organism’s body. 
x
This belief follows a more general belief known as a homunculus 
paradigm; many superstitious systems are based on the notion that 
the entire body is mapped in some way to a smaller subset of the 
body. There is, however, no scienti¿ c basis for this. 
x
What iridologists do is essentially what is called a medical cold 
reading, which involves making vague and common guesses about 
someone—in this case, their symptoms—to give the impression 
that they have detailed knowledge about their symptoms.
x
For example, if you ask your mom if she has any lung disease based 
on your reading of her iris, and she says that she doesn’t, you could 
say that, therefore, she has susceptibility for lung disease and that it 
might become a problem in the future.

141
x
A 2000 review by Edzard Ernst of iridology studies that were 
deemed to be of suf¿ cient quality concluded that they showed no 
diagnostic ability of iridology. In fact, iridologists perform no better 
than chance when they are properly blinded. 
x
However, the practice of iridology persists as an alternative 
practice. Current practitioners engage in special pleading; 
believers in iridology successfully isolate their claims from the
scienti¿ c evidence.
Ghost Hunting as Pseudoscience
x
The most common type of evidence that ghosts exist is ghost 
photographs. These often present what are called orbs, which are 
circles of light on photographic ¿ lm, or ghost globules, which are 
streaks or coils. At times, more interesting photos appear to have 
ghostly ¿ gures or people in the frame. 
x
Ghost hunting is a good example of anomaly hunting. Ghost hunters 
look for anything unusual and then conclude that it is evidence for 
ghosts. Therefore, they make the argument from ignorance. They 
may take the time to dismiss a token alternative or two, but they 
do not engage in hypothesis testing, which could distinguish a true 
anomaly from a photographic artifact like globules and streaks. 
Ghost hunters have no basis for what a ghost photograph should 
look like, so they can declare any anomaly a ghost. 
x
There are, however, explanations for the photographs that are 
presented by ghost hunters. For example, most photos with light 
effects use a À ash. Orbs are known photographic artifacts of lenses. 
Sometimes light can reÀ ect off of dust motes, which are things that 
you won’t see with the naked eye but that the camera will pick up 
when a À ash reÀ ects off of it. 
x
Streaks are often the result of the so-called camera cord effect, 
which could be due to anything that’s in front of the lens and very 
close—especially when a À ash is used. Even a black plastic camera 

142
Lecture 18: The Many Kinds of Pseudoscience
cord, when in front of the lens, will reÀ ect the light and cause a 
whitish streak across the ¿ lm.
x
Streaks also may be due to unfamiliar settings on digital cameras, 
such as the twilight mode, which uses a À ash but then keeps the 
shutter open so that camera movement can create streaks.
x
Other photographic effects might include an actual image of a 
ghostly person or ¿ gure in the ¿ lm or on the picture. This is not 
a streak or a globule, so this can’t be a simple artifact. Older 
cameras often created accidental double exposures. If the ¿ lm did 
not advance and a subsequent picture was taken, there could be a 
ghostly, faint image of a person from one exposure on the other.
x
Sometimes the ghostly image is just pareidolia, which may allow 
you to see what appears to be a distorted ghostly body or face in 
a foggy or blurry image. It’s also possible that a live person was 
simply in the ¿ eld of view but wasn’t noticed at the time the picture 
was taken. 
x
Ghost hunters use scienti¿ c instruments to take readings and then 
declare any apparent anomalous readings as further evidence 
for ghosts. For example, they are fond of using electromagnetic 
detectors to look for electromagnetic ¿ elds. Typically, they’re using 
off-the-shelf electromagnetic detectors. Most of the ones they use 
are not directional, and the electromagnetic range is arbitrary. 
x
In our modern society, electromagnetic ¿ elds are everywhere, 
so ¿ nding them in a house is not surprising. Furthermore, ghost 
hunters do no controlled studies that are designed to determine 
what the source of the electromagnetic ¿ elds is and to rule out
mundane explanations. 
x
Another phenomenon often presented by ghost hunters as evidence 
for ghost activity is ghost cold, which is a cold spot that is alleged 
to be the evidence of a ghostly presence. In fact, many houses have 

143
drafts and cold spots—especially older houses, the kind that ghost 
hunters are fond of investigating.
x
Despite the fact that nobody has been able to produce any reliable, 
compelling evidence that ghosts are a real phenomenon, people do 
have many ghostly or anomalous experiences. Of course, we are 
not dismissing that people have these experiences; the question is 
how best to interpret them.
x
There’s ample evidence that these experiences result from errors 
in perception followed by pattern recognition, hyperactive agency 
detection, and simple errors in memory. Often, ghost encounters 
occur under poor observing conditions. They may occur when 
people are sleep deprived or when anxiety or expectation levels are 
very high.
Free Energy as Pseudoscience
x
Of course, it would be great if we could ¿ nd a clean, renewable, 
cheap, abundant, and ef¿ cient source of energy. Our civilization 
would be greatly enhanced by this because our civilization runs on 
energy. Currently, much of that is fossil fuel because fossil fuels are 
cheap, abundant, and ef¿ cient, but they are not clean or renewable.
x
The strong need and desire for cheap energy leads to a lot of energy 
pseudoscience. The most extreme versions are often referred 
to as free energy because they promise not just abundant but
limitless energy. 
x
A common version of free energy is a motor that generates more 
power than it consumes. These motors are called overunity 
machines because they produce more energy than they consume. 
Usually, these are demonstrated on a small scale with a claim 
that they are able to scale up. However, at small scales, it’s easy 
to miss a small energy source and confuse that with anomalous
energy production.

144
Lecture 18: The Many Kinds of Pseudoscience
x
Ultimately, such devices never work. They don’t scale up because 
they can’t generate free energy. Often, however, there are deliberate 
scams; there are companies that claim to be on the verge of a 
breakthrough in free-energy machines to trick investors into sinking 
lots of money into their invention. 
x
Many of these devices use magnets and neglect the fact that the 
magnets themselves are providing a ¿ nite source of energy to the 
process. Believers of these machines do not try to prove their claims 
wrong; rather, they are looking for proof of their claims. 
x
The most signi¿ cant scienti¿ c problem with free-energy claims is 
that they violate the ¿ rst and second laws of thermodynamics—
that energy can change form but cannot be created out of nothing 
and that in any thermodynamic process, some energy will be
lost, respectively. 
Psi Phenomena as Pseudoscience
x
Psychic abilities, which involve the ability to read other people’s 
minds, are sometimes called extrasensory perception or anomalous 
cognition. Other forms include precognition, the ability to perceive 
things before they happen, or remote viewing, the ability to perceive 
things that are remote from your location.
x
These various phenomena have been actively researched over the 
last century; however, the plausibility is extremely low. Even if we 
put the plausibility issue aside, researchers have not been able to 
convincingly show that psi phenomena exist. 
x
In the past, psi researchers produced some classic examples of 
pseudoscienti¿ c methods. One such phenomenon is called psi 
missing, which is the notion that sometimes those with ESP may 
guess the incorrect answer more often than would be predicted
by chance. 
x
Another example is optional starting and stopping, which is the 
notion that some individuals with ESP may require a warm-up 

145
period before their abilities begin and that their abilities may fatigue 
and suddenly stop working.
x
Both of these are excellent examples of inadvertent mining of the 
data, or looking for and counting only those data points that are 
positive and eliminating the negative data points. 
x
Psi missing and optional starting and stopping come from a À awed 
sense of probability; they ignore the fact that in any random 
sequence 
there 
will be runs of hits
and 
misses 
that
will 
occur 
by
chance alone. 
x
Reproducibility 
is key in science 
because any single 
study 
can 
be 
a 
À uke. 
Publication 
bias—the tendency 
to publish positive 
studies, not negative 
ones—often occurs so that À uke positive studies can be chosen 
among many unpublished negative studies. If an effect is real, it 
should be consistently reproduced.
x
The key À aw in the psi literature is this failure of reproducibility. 
Initial impressive results tend to diminish over time as tighter 
controls are introduced and when protocols are replicated. This 
is generally seen in science and is known as the decline effect, in 
which effect sizes tend to diminish with replication. Real effects 
will persist while illusory effects will vanish.
x
Some ESP researchers have claims that psi effects truly decline over 
time, but this is just another form of special pleading, or making up 
an excuse because of lack of evidence or problems with a theory. 
People who have ESP, or extrasensory 
perception, claim that they have psychic 
abilities that involve reading other people’s 
minds.
© Thinkstock Images.

146
Lecture 18: The Many Kinds of Pseudoscience
Spontaneous Human Combustion as Pseudoscience
x
Spontaneous human combustion is the notion that people can 
spontaneously burst into À ames and be consumed by the resulting 
¿ re. This is based almost entirely on unusual cases of death by ¿ re. 
The idea of spontaneous human combustion combines anomaly 
hunting with argument from ignorance.
x
Most of the cases of spontaneous human combustion have an easily 
identi¿ able external source of À ame. Victims are often in¿ rmed, are 
on sedating medication, live alone, and are smokers.
x
Researchers have explained even the strangest cases by the wick 
effect, in which the body can act like a candle with the clothes 
acting as a wick. However, with so many ¿ res, we would expect 
the occasional unusual case. There are no plausible or known 
mechanisms for spontaneous human combustion.
free energy: The general term for any alleged process that produces energy 
without burning fuel or other limited resource. 
orb: The name given by ghost researchers to a sphere of light that is seen on 
¿ lm and believed to be evidence of ghosts but is more easily explained as a 
photographic artifact. 
overunity: A process that generates more energy than it consumes, in 
violation of the laws of thermodynamics. 
psi phenomena: Alleged phenomena dealing with extrasensory perception—
also called anomalous cognition—such as mind reading, remote viewing, 
and precognition. 
    Important Terms

147
Gardner, Fads and Fallacies in the Name of Science.
Nickell, “Ghost Hunters.”
Novella, “Bem’s Psi Research.”
———, “Spontaneous Human Combustion.”
———, “The Decline Effect.”
Polidoro, Secrets of the Psychics.
Stenger, “The Phantom of Free Energy.”
1. How do claims of free energy violate well-established laws of physics?
2. What features of psi research are cause for skepticism?
    Suggested Reading
    Questions to Consider

148
Lecture 19: The Trap of Grand Conspiracy Thinking
The Trap of Grand Conspiracy Thinking
Lecture 19
I
t seems that humans have an inherent tendency for conspiracy thinking—
although some have more of a tendency than others. However, we 
shouldn’t think that conspiracy thinking is restricted to a few crazy 
people. Keeping on the lookout for genuine conspiracies is important, which 
is partly why conspiracy thinking persists. By examining extreme conspiracy 
theories, we hope to identify those patterns of thought and cognitive traps 
that we all fall into to a lesser degree. Being familiar with grand conspiracies 
will also help us identify more subtle À aws in our day-to-day reasoning.
Conspiracy Thinking
x
To some extent, we all have a little conspiracy theorist inside each 
of us; the notion that dark and powerful forces are working against 
our interest grips our attention and can seem very compelling. 
x
This is, in fact, an adaptive trait, such as anxiety, for example. It 
is useful in moderate amounts but is counterproductive and even 
debilitating when extreme. While there are certainly conspiracies in 
the world, conspiracy thinking can be a cognitive trap that is easy to 
fall into but very dif¿ cult to get out of.
x
A conspiracy occurs when two or more people conspire together 
to engage in some secret or nefarious purpose. Grand conspiracies, 
however, are large conspiracies that involve many people across 
multiple organizations acting for long periods of time. 
x
Grand conspiracy theories tend to divide the world into three 
groups. The conspirators are a vastly powerful organization with 
incredible resources and evil intentions—up to and including taking 
over the world. They also, however, make simple mistakes or are 
incredibly careless because otherwise, nobody would know that 
they exist. 

149
x
The second group is the conspiracy theorists. They have the ability, 
in their own view, to see the conspiracy for what it is. They can 
read between the lines and recognize the hand of the conspirators at 
work. They often envision themselves as being part of an army of 
light whose goal is to save the world from the dark conspiracy. 
x
Everyone else in the world falls into the third group, which are the 
naive dupes—the people who are not engaged in the conspiracy and 
don’t see that it exists.
x
A classic example of a grand conspiracy theory is the belief in the 
Illuminati, which are alleged to be a shadow world government. 
Their goal is to bring about a new world order in which they rule 
and control the entire world. 
x
Conspiracy theorists see the Illuminati behind everything that 
happens in the world. According to conspiracy theorists, they 
reveal themselves through symbolism. For example, the seal on the 
U.S. dollar is the all-seeing eye that they claim is a symbol of the 
Freemasons, who are supposedly connected to the Illuminati.
x
Conspiracy thinking is a way to make sense of complex or 
mysterious events. More likely, conspiracy thinking occurs
when people feel 
that 
they 
lack 
control 
or 
are 
being victimized.
x
In fact, we often 
lack 
detailed 
i n f o r m a t i o n 
about 
important 
political 
events 
or other events, 
and this opens the 
door even further 
to 
conspiracy 
Innocent symbolism, such as the all-seeing eye 
on the back of the U.S. dollar bill, is evidence 
of the Illuminati to conspiracy theorists.
© Hemera.

150
Lecture 19: The Trap of Grand Conspiracy Thinking
thinking because we can ¿ ll the gaps in our knowledge with ideas 
or notions that are comforting in some way. Conspiracy thinking 
offers the illusion of certainty—much like superstitions provide an 
illusion of control.
Conspiracy Thinking and Pattern Recognition
x
Conspiracy theories are a form of pattern recognition. A conspiracy 
theory is a pattern imposed upon disconnected events. Conspiracy 
thinking is the cognitive form of pareidolia. Essentially, conspiracy 
theorists connect the imagined dots between disconnected events 
and see an invisible hand operating behind the scenes.
x
Pattern recognition is also more common in response to feelings 
of powerlessness. Conspiracy thinking potentially meets other 
psychological needs as well, such as self-esteem, which may be a 
reaction to an inability to attain one’s goals.
x
Being part of an army of light, for example, offers not only self-
esteem, but also a way to channel feelings of anger. It makes people 
feel as if they are part of a privileged, enlightened few—a notion 
that ¿ ts with the psychology of belief. 
x
There is a struggle happening in our brains between two inherent 
tendencies: reality testing and pattern recognition. Conspiracy 
thinking, a form of pattern recognition, is ¿ ltered through the 
reality-testing module in our brains. We see an apparent pattern, 
and then we ask ourselves whether the pattern makes sense and 
whether it conforms to our internal model of reality.
x
People vary in terms of the strength of their pattern recognition 
and reality testing. For example, the psychiatric disease known 
as schizophrenia is understood largely as a condition marked by 
diminished reality testing. 
Cognitive Traps of Conspiracy Thinking
x
There are various cognitive traps that conspiracy thinking falls into. 
One is con¿ rmation bias, which involves the tendency to see all 

151
evidence as con¿ rming the conspiracy. Any bit of evidence can be 
cast in a sinister light. Therefore, ambiguous and even negative 
evidence tends to reinforce the conspiracy theorists’ certainty and 
con¿ dence in their conspiracy after awhile. This is a self-reinforcing 
effect that makes conspiracy theorists incredibly resistant to change. 
x
Conspiracy theorists often commit to what is called the fundamental 
attribution error, which is the tendency to blame other people’s 
behavior on internal, rather than situational, factors. Conspiracy 
theorists tend to think that all actions and outcomes are deliberate 
and intended. They ignore or downplay the quirky nature of history 
and of individual action, refusing to believe that people may 
be innocently responding to a situation rather than deliberately 
orchestrating every detail. For these reasons and others, the 
conspiracy theory tends to become immune to refutation.
x
The biggest cognitive problem with conspiracy thinking is that it 
quickly becomes a closed belief system, which is a belief system 
that contains the mechanisms of its own insulation from external 
refutation from facts and evidence. All evidence against the 
conspiracy can be explained as being part of the conspiracy itself.
x
For example, any evidence that links Lee Harvey Oswald to the 
John F. Kennedy assassination could be seen as having been planted 
in order to frame Oswald, including the gun that he used, the bullet 
that was recovered, and the picture of Oswald training with the gun 
before the assassination. 
x
Furthermore, any evidence that is lacking for the conspiracy can be 
explained as having been removed or covered up by the conspirators. 
Therefore, conspiracy theorists can explain the existence of any 
discon¿ rming evidence and the lack of any evidence for their 
preferred conspiracy theory. They render themselves immune to 
any possible refutation. 
x
They also render themselves immune to any burden of proof. They 
frequently attempt to shift the burden of proof onto those who do 

152
Lecture 19: The Trap of Grand Conspiracy Thinking
not accept their theories. They challenge others to prove that their 
conspiracy is not true when they have the burden of providing 
evidence that it is.
x
They combine the shifting of the burden of proof with the moving 
goalpost. No matter how much evidence is provided in support of 
a nonconspiracy interpretation of events, it is never enough. For 
example, unless every quirky detail of the events of 9/11 can be 
explained to an arbitrary level of detail, conspiracy theorists will 
claim that there are still holes in the standard explanation.
x
Conspiracy theorists engage, like many pseudoscientists, in 
anomaly hunting. If you take any complex historical event—such 
as the John F. Kennedy assassination or 9/11—there will be many 
anomalous details, or events that cannot be fully explained. This is 
because of the law of large numbers—the fact that the number of 
variables is so high that quirky events and strange coincidences are 
bound to happen. Plus, we cannot know all the situational factors 
that may have contributed to how events occurred.
x
Conspiracy theorists often combine anomaly hunting with naive 
assumptions about how things should happen. For example, what 
should the debris ¿ eld look like after a large commercial jet crashes 
into a reinforced building, such as the Pentagon? 
x
It is naive to assume that we can know with any detail what would 
result; such an event is unprecedented, and the physics of high-
energy impacts are not always intuitive. However, 9/11 conspiracy 
theorists premise their claims on the notion that they can know 
what it should look like, and therefore, anything that does not ¿ t 
their assumptions is an anomaly and evidence that the standard 
explanation is À awed. 
x
To complete this chain of reasoning, the conspiracy theorist 
proposes a false dichotomy—that either the standard explanation 
of events is true or their conspiracy is true. Therefore, if they can 
knock holes in the standard story and call it into doubt, through 

153
naive anomaly hunting, then their conspiracy must be true. Of 
course, anomaly hunting is not persuasive. Additionally, there are 
other possibilities, so they are offering a false choice. 
x
Another cognitive trap that conspiracy theorists fall into is widening 
the conspiracy. This is another mechanism by which to render a 
conspiracy immune to contradictory evidence.
x
The greatest weakness of grand conspiracies is that they tend 
to grow, involving more people in order to answer problematic 
questions. Additionally, the power and resources attributed to
the conspirators grow until it seems there is almost nothing they 
cannot do.
x
Eventually, they simply collapse under their own weight. If the 
United States never really landed astronauts on the moon, then why 
didn’t the Soviet Union call us on it? They certainly could have 
tracked our rockets. Were they in on it? The number of people and 
organizations that would need to be involved becomes too great to 
plausibly maintain coordination and secrecy.
What’s the Harm of Conspiracy Thinking?
x
There is some debate as to whether the existence of conspiracy 
theories is a net positive or negative. On the positive side, 
conspiracy theories do serve the purpose of challenging the powers 
that be. They showcase de¿ ciencies in the sometimes-inadequate 
explanations the members of the government give for their actions. 
Conspiracy theories further promote the need for transparency and 
full disclosure.
x
On the other hand, conspiracy theories tend to erode con¿ dence in 
government and democracy. Some argue they may even hurt the 
cause of transparency; the existence of absurd and implausible 
conspiracy theories can be used to dismiss any questioning of 
conventional explanations.

154
Lecture 19: The Trap of Grand Conspiracy Thinking
x
Reasonable questions or attempts at keeping governments or 
corporations accountable can be lumped in with the lunatic fringe 
of crazy conspiracy theories. Some conspiracy theorists have even 
hypothesized that the government is responsible for some of the 
worst conspiracy theories in order to delegitimize any questioning 
of the of¿ cial version of events.
x
In addition, conspiracy theories often have a very simplistic or one-
dimensional approach to complex problems. They are sometimes 
often framed in very racist or bigoted terms. The conspirators 
may be, for example, an ethnic group. This both results from and 
reinforces the underlying racism.
x
Whether it’s harmful or not, conspiracy thinking is a type of 
pseudoscience that demonstrates many of the principles discussed 
in earlier lectures. They meet many psychological needs. They are 
built on cognitive biases and are maintained with logical fallacies. 
They are further fueled by errors in perception and memory. The 
entire process is fatally À awed because of circular reasoning that 
immunizes the conspiracy theory from any possibility of refutation.
grand conspiracy: A large, far-reaching conspiracy often alleged to span 
multiple organizations, people, and even generations. 
Goertzel, “The Conspiracy Meme.”
Klass, The Real Roswell Crashed Saucer Coverup. 
Novella, “Conspiracy Thinking.”
Posner, Case Closed.
    Important Term
    Suggested Reading

155
1. How do conspiracy theorists render their beliefs immune to refutation?
2. What role does anomaly hunting play in generating and supporting 
conspiracy theories?
    Questions to Consider

156
Lecture 20: Denialism—Rejecting Science and History
Denialism—Rejecting Science and History
Lecture 20
D
enialism is a subtype of pseudoscience that seeks mainly to deny 
an established science rather than promote a dubious claim. Deniers 
violate multiple principles of critical thinking, logic, and scienti¿ c 
methodology because of their a priori beliefs. By studying the tactics of 
deniers, we learn to recognize the more subtle versions that you engage 
in every day. Generally, we resist changing our beliefs, and a little bit of 
motivated denial can save us from a lot of cognitive dissonance—unless we 
deliberately engage our learned critical thinking skills.
Denialism as Pseudoscienti¿ c Thinking
x
Just as with grand conspiracy thinking, denialism is a category of 
pseudoscienti¿ c thinking worthy of separate consideration. This 
is not to say that the conventional wisdom on any topic is always 
correct or is beyond error. However, there is often a commonality 
among the various forms of denial; the same cognitive and logical 
errors are being committed.
x
Like any pseudoscienti¿ c thinking, denialism begins with a 
desired conclusion. Rather than supporting a controversial or 
rejected claim like many pseudosciences, denialists maintain that a 
generally accepted scienti¿ c or historical claim is not true—usually 
for ideological reasons. Denialists then engage in what is called 
motivated reasoning, or rationalizing why the undesired claim is 
not true or at least not proven.
x
They, therefore, are working backward from their desired 
conclusion, ¿ lling in justi¿ cations for what they believe rather than 
following logic and evidence wherever it leads.
x
Denialism has the same problem that we described previously with 
pseudoscience in general: the demarcation problem, or the line 
between denialism and healthy skepticism. There is a continuum 

157
between the two extremes with a gray zone in between, but also
like pseudoscience, there are a number of behaviors that
characterize denialism.
x
Of course, everyone thinks that they are true skeptics—skeptical to 
just the right degree. Anyone who believes something they do not 
must therefore be a true believer, and anyone who rejects a belief 
they accept must be a denier. It’s natural to use ourselves as the 
anchoring point for true scienti¿ c skepticism. Therefore, deniers 
often portray themselves as skeptics and even use the term. 
Strategies of Denial
x
The primary cognitive À aw that marks a belief system or behavior 
as denialism is moving the goalpost, or always demanding more 
evidence for a claim than is currently available. When that burden 
of evidence is met, then the goalpost is moved back further and 
even more evidence is demanded. As an example, those who 
engage in evolution denial, or creationism, often say that there are 
no transitional forms in the fossil record. They use vagueness in 
de¿ ning the term transitional fossil to move the goalpost.
x
At ¿ rst, it seems as if they mean there are no fossils that are 
intermediary in morphology between two extant species or between 
a descendent species and its ancestor. When such examples are 
provided, they then move the goalpost by changing the de¿ nition. 
They then demand evidence that the transitional fossil is not just 
morphologically intermediate, but that there is separate evidence 
that it is actually ancestral.
x
Related to this is the unreasonable demand for evidence. No 
science is established down to the tiniest possible detail; there 
are gaps in our understanding of even the most solid theory. The 
presence of incomplete knowledge, or gaps in our knowledge, does 
not necessarily call into question what we do know and what is
well established. 

158
Lecture 20: Denialism—Rejecting Science and History
x
HIV denial is an excellent example of the unreasonable demand 
for evidence. Deniers often demand to see a single study or paper 
that establishes that HIV causes AIDS. However, the role of HIV in 
AIDS is not established by 
any one piece of evidence 
but, rather, by a large body 
of research, each providing 
one piece to the puzzle. 
They further claim that 
HIV does not meet all the 
Koch criteria for proving 
that an infectious agent 
causes a speci¿ c disease
or syndrome.
x
However, Robert Koch’s 
criteria were formulated 
prior 
to 
the 
discovery 
of viruses, so they are 
better suited to the study 
of bacteria. Furthermore, we have many tools available today 
that were not available in the 19th century when Koch developed
his criteria. 
x
If you take a snapshot of any science at any time, there will be 
gaps in the available knowledge. It is more useful to ask how 
science is progressing over time. Are the gaps slowly being ¿ lled? 
Is our knowledge deepening? This is a better understanding of 
science in any case. How successful is a scienti¿ c theory in making 
predictions, leading to further and deeper questions, and ¿ tting 
together with other lines of evidence?
x
For example, we can look at the notion of denialism from the 
other side by comparing a legitimate science evolution to psi 
research. Over the last 150 years, evolutionary theory has been 
remarkably successful. It has tremendous explanatory power in 
terms of developmental biology, genetics, and paleontology. In 
The same cognitive and logical errors 
are committed among the various 
forms of denial.
© Photodisc/David De Lossy/Thinkstock.

159
fact, genetics provides powerful independent support for evolution, 
which in turn helps us make sense of genetics. Furthermore, our 
understanding of the patterns and mechanisms of evolution has 
deepened. It is one of the most successful scienti¿ c theories
ever developed. 
x
On the other hand, psi research—extrasensory perception—has 
been going around in circles and not progressing at all. It has yet 
to develop a single repeatable experimental demonstration of psi. It 
does not provide any predictive or explanatory power. In addition, 
it has not gained support from any other ¿ eld; we have discovered 
nothing in neuroscience that makes psi more plausible, and psi does 
not help us understand neuroscience.
x
Related to the strategy of pointing to gaps in our knowledge—rather 
than considering how useful and predictive a theory is—deniers 
will often point to disagreements within a discipline, often about 
small details, as if they call into question basic and well-established 
conclusions. There is always disagreement within a scienti¿ c 
discipline; it doesn’t mean the science is not solid.
x
For example, historians disagree about the precise number of Jewish 
people who were exterminated in the concentration camps. Deniers 
use this disagreement to argue that maybe the number is zero. 
x
Another strategy of denial is to arbitrarily narrow the kinds of 
research and evidence that counts as scienti¿ c. Deniers, in fact, will 
often cite the logical fallacy of confusing correlation with causation. 
While it is true that correlation does not necessarily equal causation, 
it is evidence that may be due to causation. Therefore, we cannot 
eliminate all correlation as a form of evidence; we simply need to 
use it properly. Otherwise, the entire ¿ eld of epidemiology, the 
study of the incidence and spread of disease, would vanish.
x
For decades, the tobacco industry denied the science that establishes 
that smoking is a signi¿ cant risk factor for certain kinds of cancer, 

160
Lecture 20: Denialism—Rejecting Science and History
especially types of lung cancer. Their primary argument was that 
the data showing a link was correlational only.
x
They acknowledged that smoking correlates with cancer but denied 
that this was enough to establish causation. Perhaps, they argued, 
cancer causes smoking, or perhaps some third factor causes both 
smoking and cancer. 
x
What they ignored is the fact that different causal hypotheses make 
different predictions and can be tested. If smoking causes cancer, 
then statistically, smokers will begin smoking before it is likely that 
the cancer developed. The duration of smoking will correlate with 
increased risk, and un¿ ltered cigarettes will have a higher risk than 
¿ ltered. Additionally, quitting smoking will reduce cancer risk. All 
of the predictions are true. In other words, multiple correlations can 
be used to triangulate to one speci¿ c cause and effect—in this case, 
that smoking causes cancer. 
x
Evolution deniers use a similar strategy: to deny that any historical 
science can be truly a science. They argue that because nobody was 
around when life developed, we cannot know what happened or 
that because we cannot reproduce evolution in the laboratory, it’s 
not a real science. This denies, however, all historical sciences—
including geology, astronomy, and forensics.
x
The core property that renders a science legitimate is that it 
engages in hypothesis testing. Hypotheses make predictions that 
can be falsi¿ ed. Those predictions do not have to be tested in a lab, 
however. They can be predictions about what we will ¿ nd when we 
look at nature.
x
For example, evolution makes predictions about what we will ¿ nd 
when we compare the genetic sequence for various proteins among 
species, so we can scienti¿ cally infer what happened in the past—
even if we cannot go there to see it for ourselves.

161
x
If you look hard enough, you can always ¿ nd facts that, taken out 
of context, can seem inconsistent with a scienti¿ c theory or claim. 
Pile enough of them together, and you can build a super¿ cial case 
against any theory.
x
For example, Holocaust deniers have pointed out that Zyklon-B, 
a form of cyanide gas, was used at Auschwitz and other 
camps for delousing. They also point out that the delousing 
chambers contained much more cyanide residue than the alleged 
gassing chambers and that some gas-chamber ruins contain no
cyanide residue.
x
Taken together, these facts may appear to cast doubt on the claim 
that gas chambers were used to exterminate humans at these camps. 
All of these claims are true, but they are out of context.
x
Zyklon-B was used for delousing objects, but it was also used to 
kill inmates. Also, humans are much more sensitive to cyanide 
than lice, and it takes a much smaller concentration to kill humans; 
therefore, the delousing chambers had a much higher concentration 
of cyanide. The cyanide residue washed off those walls that were 
exposed for years to the elements. Walls that were less exposed 
contain cyanide residue consistent with their use as gas chambers. 
Additionally, none of this addresses the other lines of evidence for 
the use of Zyklon-B in gas chambers.
A Campaign of Doubt and Uncertainty
x
The goal of denial is to sow doubt and uncertainty. This is done by 
pointing to apparent inconsistencies—current gaps in knowledge—
sometimes to points from legitimate dissent, and also to the 
messiness of science. Scientists make mistakes all the time by 
pursuing dead ends, using bad arguments or evidence, and even 
committing fraud.
x
It is, therefore, easy to ¿ nd ways to call any scienti¿ c claim into 
doubt—if that is your goal. Healthy skepticism, on the other 
hand, fairly considers all the evidence and puts it into the proper 

162
Lecture 20: Denialism—Rejecting Science and History
perspective: critical thinking. It acknowledges that all knowledge 
is incomplete and that scientists are imperfect people. We can still 
come to reliable conclusions with suf¿ cient evidence.
x
Deniers often combine their program of doubt with false dichotomy. 
If the accepted version of events is not true, then their alternate 
claim or version must be true. This, however, is also an argument 
from ignorance. What they rarely do is provide evidence for their 
alternate claim.
x
The ultimate fallback position for the denier is the conspiracy 
theory. When the evidence does not support their position, they 
will often just claim that the scienti¿ c evidence itself is all a fraud, 
a grand conspiracy, usually of their ideological opponents. This 
allows them to dismiss all the evidence—all the published studies 
that prove the science they do not want to acknowledge. 
demarcation problem: A philosophical term that refers to the dif¿ culty in 
clearly de¿ ning a separation between two ends of a continuum, such as the 
difference between science and pseudoscience. 
denialism: A form of pseudoscience that uses poor logic and distortion 
of science in order to deny the conclusions or legitimacy of an accepted 
scienti¿ c theory or fact.
epidemiology: The study of the incidence, spread, and associated factors
of disease. 
transitional fossil: A paleontological term to refer to a fossil specimen that 
represents a species that lies phylogenetically between two other species
or groups. 
Zyklon-B: The trade name for a form of cyanide gas that is used for 
delousing, but it is also infamously used by the Nazis during World War II to 
exterminate humans. 
    Important Terms

163
Grant, Denying Science.
Novella, “More on God of the Gaps.”
———, “Skepticism and Denial.”
———, “Scienti¿ c Proof and Evolution Denial.”
Novella and Smith, “HIV Denial in the Internet Era.”
Oreskes, “Merchants of Doubt.”
Specter, Denialism.
Tokuno, “Holocaust Denial.”
1. What are the major differences in method between healthy skepticism 
and denial?
2. How did the tobacco industry use a campaign of doubt to generate 
confusion about the correlation between tobacco use and lung cancer?
    Suggested Reading
    Questions to Consider

164
Lecture 21: Marketing, Scams, and Urban Legends
Marketing, Scams, and Urban Legends
Lecture 21
C
ritical thinking is not just for scientists; it can be applied to our 
everyday lives. For example, marketing often provides situations in 
which critical thinking should be exercised. Marketing messages are 
often deceptive and attempt to exploit human psychology. Scams are another 
area where critical thinking can provide an important layer of protection. 
Furthermore, in the age of the Internet, urban legends, chain letters, and 
rumors can spread virally, making it more important than ever to be able to 
determine which information is reliable and which is bogus. 
Motivated Misinformation
x
In one form or another—whether it’s spam, traditional mail, 
telemarketing, or mainstream ads—we are bombarded with 
motivated misinformation trying to manipulate us in some way. 
Persuasive speech, propaganda, and marketing have probably 
existed as long as human communication, but we are also living at 
the dawn of the Internet age—in which every point of view, bias, 
and agenda can vie for equal access on the web.
x
More than ever, therefore, we need basic critical thinking skills to 
survive in this age of misinformation. In other words, nobody is 
¿ ltering information for us, so we have to provide our own ¿ lters to 
assess which information out there is reliable and authoritative and 
which is biased and perhaps even malevolent.
x
There are different types of scams. Some are deliberate and 
malevolent. For example, you might have received something 
similar to the following e-mail: My associate has helped me to send 
your ¿ rst payment of $7,500 to you as instructed by the Malaysian 
Government and Mr. David Cameron, the United Kingdom prime 
minister. To claim your $820,000, all you need is a certi¿ cate, 
which you can get by sending me some personal information.

165
x
This type of scam is known as the Nigerian scam (also called the 
419 scam) because so many of them originate in Nigeria. This type 
of scam dates back to the 1920s; now, it’s a ¿ ve-billion-dollar-per-
year worldwide scam.
x
Obviously, like many scams, this type plays on the greed and 
gullibility of its victims. With e-mail, scammers can scan millions 
of potential targets, looking for nibbles. If you answer, then they 
will reel you in as far as they can. First, you will have to pay to get 
the certi¿ cate and then again to bribe a corrupt agent somewhere—
it will never end.
x
Some scams even try to get you to À y to an airport in another 
country to collect your cash, but the purpose is to kidnap you and 
ransom you back to your family. 
Urban Legends
x
Were 450 girls under 10 years of age forcibly married to men in 
their 20s in a mass wedding in Gaza in 2009? That’s the claim of 
countless e-mails and blogs.
x
This type of scam is more of an urban legend than crafted deception. 
Although it must have started from someone who knew they were 
making unwarranted assumptions or maybe even inventing details, 
it spreads mainly by those who believe the content. The evidence 
for the claim given is several photographs of the young alleged 
brides with their older grooms. These e-mails were used to stir up 
anti-Muslim sentiment. 
x
There was indeed a mass wedding put on by Hamas in Gaza in 
2009. However, the young girls were not the brides; they were 
young relatives of the grooms—nieces and cousins, as is the 
tradition. Western reporters present at the wedding con¿ rmed this. 
In fact, the youngest bride was 16 and most were over 18. 
x
There are many red À ags in these e-mails: The tone is highly 
political and emotional; no links or authoritative resources are 

166
Lecture 21: Marketing, Scams, and Urban Legends
given; the pictures do not provide any clear context, allowing for 
misinterpretation; and it seems unlikely that the mainstream media 
would have missed such a story. None of these factors prove the 
story false; they should just raise the level of skepticism. Only a 
few minutes of investigation uncovers the information to put the 
story into context.
x
Sometimes misinformation is not political or ideological but speaks 
to a common fear in our society. We fear exposure to toxins and 
chemicals from our highly industrialized society. We trust others to 
prepare our food. We fear invasion of our borders and the watering 
down of our values. We fear the intentions of others.
x
Those urban legends that spread are often ones that resonate with 
some fear or anxiety. The classic example of this is the kidney thief. 
A gentleman away on business gets picked up in the hotel bar by a 
beautiful woman only to wake up the next morning in a bathtub full 
of ice with a scar down his back and one of his kidneys missing.
x
Why wouldn’t the organ thief just take all the organs and leave 
behind no witness? In any case, such stories always happen to a 
friend of a friend. When investigated, they never lead back to the 
original source but are often spread by reporters in newspapers or 
online articles.
Food Myths
x
Have you heard that if you leave a fast-food hamburger out it will 
remain fresh for months? The implication is that the hamburger is 
so full of arti¿ cial ingredients and preservatives that it will not rot, 
as regular food should. The claim that the hamburger will not rot is 
sort of true, but the interpretation is suspicious.
x
The obvious experiment is to buy 100 percent beef from the 
supermarket, cook it, leave it out, and see what happens. A good 
experiment would also compare this to the fast-food version and 
control for variables such as thickness, thoroughness of cooking, 
the presence of a bun, and whether it is kept in a sealed container. 

167
When such comparisons are done, it turns out that any thoroughly 
cooked hamburger—especially if it is left uncovered—will not rot 
or mold. The reason is the 
lack of water; the dried-
out meat will not decay. In 
humid conditions, or when 
sealed to trap in moisture 
and prevent drying, the 
expecting 
molding 
and 
rotting occurs. 
x
You may have also heard 
that the arti¿ cial sweetener 
aspartame is responsible 
for many diseases and 
symptoms. 
Since 
1998, 
there has been a letter 
circulating claiming that 
aspartame is responsible 
for 
an 
epidemic 
of 
multiple sclerosis, lupus, 
Parkinson’s disease, and 
other diseases.
x
Some people further claim 
that at high temperatures, 
the wood alcohol in aspartame coverts to formaldehyde, ultimately 
resulting in methanol toxicity, which mimics multiple sclerosis
and lupus.
x
Aspartame has been approved for years by the FDA. Promoters 
of these aspartame fears also claim there is a conspiracy among 
manufacturers, the medical community, and the FDA to hide 
the truth. Apparently, the mainstream media is either in on the 
conspiracy or is simply incompetent. The websites also contain 
undocumented claims and anecdotal reports. There are many red 
À ags for skepticism.
There’s nothing arti¿ cial about fast-
food hamburgers that make them not 
rot if left out for months; it is merely 
the moisture content.
© iStockphoto.

168
Lecture 21: Marketing, Scams, and Urban Legends
x
There are numerous observational and experimental studies looking 
into aspartame toxicity. Epidemiological studies have found no 
association between aspartame use and any disease or symptom. 
Experimental studies have shown no toxicity even at doses 100 
times the average daily consumption by users.
x
The claim that aspartame is broken down into formaldehyde is true 
but misleading. The breakdown products of aspartame are mostly 
excreted from the body, but a small amount is converted ultimately 
to formaldehyde. However, formaldehyde is also produced by 
natural metabolic processes in the body. The small extra amount 
resulting from aspartame is negligible and has no measurable
health effect.
x
Claims like this are seemingly endless on the Internet and in popular 
culture. In order to assess such claims critically, consider the source 
and the plausibility of the claim, including all the implications. 
Then, try to ¿ nd multiple reliable sources of information to see if 
there is a consensus of opinion.
Common Marketing Strategies
x
Such phenomena as the psychology of belief, cognitive biases, and 
innumeracy can be exploited in order to manipulate you into buying 
something you otherwise would not buy. In fact, there are journals 
dedicated to marketing research—the science of psychological 
manipulation for making sales.
x
Why are so many products priced at $19.95? Are people really 
fooled into thinking that $19.95 is signi¿ cantly different from 
$20? The answer is yes: We have a leftmost-digit bias, in which 
the leftmost digit disproportionately affects our assessment of 
cost and value. We see this also with odometer reading and used
cars; retail prices for used cars drop signi¿ cantly at each 10,000-
mile increment. 
x
Exploiting our need for self-esteem—speci¿ cally, our need to 
appear consistent to others—a salesperson may ask whether 

169
you think a feature of a product is useful or if the product itself 
is handy after you have said no to buying it. Then, if you answer 
them in a positive way but still refuse to buy the product, you
seem inconsistent.
x
The ultimate manifestation of this involves getting down to the 
last objection. If the customer raises an objection, the salesperson 
can say, “So, if it were not for this one thing, you would buy the 
product?” Then, the salesperson will remove that ¿ nal objection, 
and if you still don’t buy the product, you essentially have to admit 
that you just lied.
x
Many purchase decisions are made subconsciously. Eighty 
percent of luxury-item purchases are impulse buys. This allows 
for psychological factors to play a large role. For example, the 
anchoring heuristic can make a price seem low by anchoring to a 
higher base price.
x
In addition to guarantees of satisfaction and making potential 
customers worry about nonexistent problems, guilt is another 
common form of psychological manipulation. Have you ever 
received a request for a donation with personalized address labels 
or a survey request with a small amount of money enclosed? The 
free gifts are meant to apply guilt to make you ¿ ll in the survey or 
send in a donation.
Psychological Manipulation
x
Such sales schemes as multilevel marketing are unsustainable, 
and each new level needs geometrically greater sales people to 
sustain the pyramid. In addition, multilevel marketing schemes turn 
customers into a sales force. It is easier to convince someone of the 
value of a product if they in turn are trying to convince others of 
those virtues.
x
Most multilevel marketing salespeople lose money. They are mostly 
just deluded customers, buying products for their own use or just to 

170
Lecture 21: Marketing, Scams, and Urban Legends
make a quota—even though they have no chance of selling what 
they are forced to buy. 
x
Whether in a multilevel marketing model or a more traditional 
model, dubious health products are increasingly common. They 
often make claims that are too good to be true—that appeal to 
wishful thinking, such as losing weight without effort. At times, 
sellers of these products incorporate conspiracy claims to explain 
skepticism. They often rely on testimonials because storytelling can 
be very compelling. Additionally, they frequently misrepresent or 
misuse the scienti¿ c evidence.
x
Cults are another phenomenon that uses psychological manipulation. 
Cults manipulate the environment to cause disorientation, sleep 
deprivation, and even nutritional deprivation. 
x
Members of cults inundate prospective members with love to make 
them feel unconditionally accepted. They then build a sense of in-
group acceptance and out-group rejection. They isolate members 
from their family and friends, creating dependency on the group. 
Additionally, the cult leader is a charismatic individual, who 
encourages the surrendering of critical thinking.
leftmost-digit bias: The tendency of people to focus disproportionately on 
the leftmost digit of a number when assessing overall value or amount. 
Nigerian scam: A type of scam in which the target is promised a large 
sum of money for helping a foreign national move money into the target’s 
country, but only after money is paid for bribes, customs, and other needs. 
spam: Unwanted e-mail messages usually used for advertising or to spread 
rumors or propaganda. 
    Important Terms

171
Brunvand, Fleming, and Boyd, The Big Book of Urban Legends .
Fitzpatrick and Reynolds, False Pro¿ ts.
Novella, “Aspartame Safety and Internet Urban Legends.”
———, “The Burger ‘Experiments.’”
———, “The Internet and Skepticism.”
Singer and Lalich, Cults in Our Midst.
1. How do marketers and salespersons exploit human psychology in order 
to manipulate their potential customers?
2. What methods can protect you from rumors and false information 
spread on the Internet and elsewhere?
    Questions to Consider
    Suggested Reading

172
Lecture 22: Science, Media, and Democracy
Science, Media, and Democracy
Lecture 22
W
e live in a world increasingly dominated by science; however, 
most people don’t have the basic critical thinking skills and 
scienti¿ c knowledge required to deal with the relevant science. 
In a free society, citizens have both a responsibility and a burden to not 
only stay well informed, but also to have the knowledge and skills to assess 
important issues of the day. This lecture covers the strengths and weaknesses 
of media reporting—especially of scienti¿ c and controversial topics—and 
the intersection of science with politics and social issues.
Finding Reliable Information
x
Before accepting any piece of information as probably true, make 
some attempt to verify it. Do not trust any one source as de¿ nitive. 
If possible, see what multiple, independent sources have to say. 
This is becoming more challenging on the Internet because, often, a 
single source is repeated endlessly on many websites. 
x
A very useful strategy is to speci¿ cally look for discon¿ rming 
information, or a contrary opinion. Searching for the topic of 
interest with key words such as scam, skeptics, skeptical, or fraud 
can help. See what all sides are saying about an issue before 
deciding who has the strongest case. It’s easy to make what seems 
like a convincing case if you are only presenting one side.
x
There have been many reviews of online information quality that 
have found that even the best sites have problems. In general, don’t 
trust sites that are trying to sell you something or have an apparent 
political or ideological agenda. 
x
The more controversial a topic is, the greater the chance that 
information will be skewed or biased. In such cases, it is especially 
important to survey a number of sources to get the broadest 

173
perspective possible. Some topics are genuinely controversial—
even among experts.
x
For example, are biofuels a useful strategy for achieving energy 
independence and limiting greenhouse gases? At present, for 
ethanol speci¿ cally, whether more energy comes out of the process 
than is put in depends on how the calculations are made, so
experts disagree. 
x
In addition, there are many so-called manufactroversies, or 
questions that are not controversial among scientists or experts but 
are made controversial by ideological groups.
x
For example, experts largely agree that vaccines are both safe and 
effective, but there is a vocal group of antivaccine activists that 
create a tremendous amount of information online that argues
the opposite.
Pitfalls of Science Journalism
x
Many people get the majority of their science news from the 
mainstream media—whether online or from more traditional 
venues like television and newspapers. Therefore, it is critical to 
have a working understanding of the strengths and weaknesses 
of how the media presents science. The quality of information is 
highly variable, as is the background knowledge and skills of 
journalists and editors. In addition, not all scientists know how 
to interface with the media, and they may unwittingly contribute
to misinformation.
x
An example of bad science news reporting involves the story of Lee 
Spievack, who reportedly cut the end of his ¿ nger off. However, his 
brother owned a company that was researching a tissue regeneration 
powder made from pig bladders, whimsically called pixie dust, 
which allegedly helped Spievack regrow his lost ¿ nger. 
x
This is what we call science by press release. With decreasing 
revenue going to traditional journalism, news sites have had to 

174
Lecture 22: Science, Media, and Democracy
cut back. Many have reduced or eliminated their science reporting 
divisions, and now generalist journalists and editors are covering 
the science beat. Distressingly, many outlets reprint, sometimes 
with little or no alteration or fact-checking, science press releases.
x
There are many problems with this. Even respectable universities 
have a press of¿ ce that may try to sensationalize a news story to 
get their institution in the news. Often, the researchers have little 
or no input into the copy of the press release. Furthermore, private 
companies can use the science press release essentially to advertise 
their products or drum up interest in their company, perhaps to 
attract investors.
x
In this case, the story was generated by Spievak’s brother, who 
was apparently trying to increase interest in his company and its 
new product. The story is almost entirely bogus: Spievek did 
indeed injure his ¿ nger, but not a single joint was missing; all that 
happened was typical wound healing. Sometimes the very ends of 
¿ ngers can regrow with such healing, but this is very different than 
the impression given in the press. Essentially, the press was duped 
by a self-serving marketing campaign—or perhaps many didn’t 
care, as long as they got their sensational headlines.
x
Another story involves Dr. Paolo Zamboni, an Italian vascular 
surgeon who claims he has found the cause, and cure, for multiple 
sclerosis. He claims that multiple sclerosis is due to a condition 
known as chronic cerebrospinal venous insuf¿ ciency (CCSVI), 
which is a partial blockage in the veins that drain blood from
the brain.
x
Furthermore, Zamboni claims that he can cure multiple sclerosis 
by opening up this blockage with venous angioplasty and stenting. 
This has excited the multiple sclerosis community, and the media 
around the world breathlessly reported how this lone maverick has 
challenged decades of multiple sclerosis dogma.

175
x
There are multiple problems with how this story has often been 
reported. Many reporters make the mistake of confusing the 
authority of an individual with the authority of the scienti¿ c 
community. No matter how many letters someone has after his 
or her name, the 
opinions of that 
person 
are 
still
the quirky opinions 
of one individual, 
but 
reporters
often 
defer 
to
such authority.
x
Good journalism, 
however, 
should 
put the opinions 
of one expert into 
the proper context. 
F u r t h e r m o r e , 
the press loves stories about a so-called lone maverick taking on 
traditional beliefs; it’s a narrative that resonates within our culture. 
Sometimes, those mavericks turn out to be correct, but far more 
often, their speculations do not hold up under further research.
x
This represents another common error: confusing speculative 
research with con¿ rmatory research. Most speculative research 
turns out to be wrong in the long run, but the public is often treated 
to an endless sequence of scienti¿ c research and then never hears 
that the preliminary ¿ ndings turned out to be wrong. Much of 
science news reporting is simply premature.
x
In this case, Zamboni’s claims have now been replicated many 
times, and most of the follow-up studies have not found what 
he found. Many patients with multiple sclerosis do not have 
venous blockage, and many patients without multiple sclerosis 
do. Furthermore, the evidence continues to pile up that multiple 
sclerosis is an inÀ ammatory disease with genetic predisposition.
It’s important to raise the overall quality of 
science news reporting because it’s where 
most people get most of their information.
© Digital Vision.

176
Lecture 22: Science, Media, and Democracy
x
Unfortunately, the promise of a potential cure for multiple sclerosis 
has caused some in the multiple sclerosis community to rally behind 
Zomboni, despite the negative evidence that comes in. Some are 
calling for clinical trials, which seem premature given the negative 
evidence. Some even claim that there is now a conspiracy against 
Zamboni—perhaps just to explain the evidence. 
x
This situation was greatly exacerbated by the premature press 
reporting. This is part of the double-edged sword of free access to 
information. Traditionally, these scienti¿ c controversies would be 
worked out in the scienti¿ c literature and at professional meetings—
long before the public was made aware of them. However, the 
Internet changes that.
x
Public accessibility to such controversies is good in the long 
run, but the public now has to be educated more about the messy 
process of science—and speci¿ cally about the nature of speculative 
research. In other words, it is essential to remain skeptical of new 
ideas until they have had time to go through the process of scienti¿ c 
research, peer review, and replication. 
x
Most new ideas in science fail, but if an idea turns out to be correct, 
it will stand the test of time. Therefore, the public needs to learn 
patience and healthy skepticism when it comes to new ideas in 
science, despite the frenetic pace and short attention span of the 
news cycle.
False Balance
x
Another chronic problem with many science news stories, especially 
those about controversial topics, is what we call false balance. 
Standard journalistic practice is to cover both—or all—sides of an 
issue equally. This makes sense when it comes to politics, or areas 
where opinion is dominant, but this approach does not work well 
with science news.
x
Science is about veri¿ able and testable factual claims. There is often 
a huge asymmetry in a science controversy or topic. For example, 

177
there is a small minority of people who believe that Earth is gaining 
matter and growing signi¿ cantly. Furthermore, they believe that 
the reason the continents seem to ¿ t together like a puzzle is not 
because of plate tectonics but because at one time, Earth was only 
as big as the continents, and they broke up when Earth grew.
x
When subjects like this get covered, however, reporters will feel 
obliged to give the issue balance, so they have one supposed expert 
present the growing Earth theory, and then a regular scientist 
explains what the other 99.9 percent of the scienti¿ c community 
think. Simply presenting the two views as equivalent gives a 
very false impression to the public, elevating the fringe idea to
equal status.
x
In recent science news, it was reported that researchers from the 
OPERA experiment (Oscillation Project with Emulsion-tRacking 
Apparatus) measured the speed of neutrinos, a type of primary 
particle, as faster than the speed of light. This violates Einstein’s 
theory of special relativity and is sensational science news—if true.
x
However, the results are almost certainly in error; even the 
researchers acknowledge this. The neutrinos were faster than 
the speed of light by only 60 nanoseconds, and there are several 
proposed causes for the error.
x
At this time, there is no de¿ nitive answer; it is a genuine scienti¿ c 
puzzle. However, it is likely that the media will not present the 
prosaic answer with as much enthusiasm or as high of a pro¿ le as 
the initial—and almost certainly incorrect—¿ ndings. 
x
Critical thinking is not just about science. In fact, the more political 
and emotional an issue is, the more critical thinking is necessary. 
This raises an important question: What is the relationship between 
science and politics, ethics, and values?
x
There is a range of opinions on this question, which is ultimately 
philosophical. The position that has the most merit is that science 

178
Lecture 22: Science, Media, and Democracy
can inform ethical thinking, but there will always be some 
subjective value judgments in the mix.
Science Education and Democracy
x
In democratic, industrialized nations, it is important for citizens to 
have a working knowledge of science, which involves the ability to 
have a basic understanding of important science-related issues of 
the day.
x
Research has shown that the vast majority of people living in 
industrialized nations do not have the minimal scienti¿ c knowledge 
to participate in important scienti¿ c issues of the day. Experts 
propose a number of possible causes, and ¿ xes, for this problem.
x
Obviously, the quality of science education needs to improve at 
all levels. Science textbooks also are frequently criticized for 
poor quality. One study shows that requiring science classes for 
nonscience majors at the college level has a signi¿ cant impact 
on scienti¿ c literacy. Improving the overall quality of science 
reporting, and science in mainstream media, is also important. 
false balance: The treatment of a topic in journalism as if it is controversial 
when it isn’t, or treating two sides of an issue as if they are scienti¿ cally 
equal when they are not. 
manufactroversies: A false controversy created in order to make a 
scienti¿ cally accepted idea seem uncertain or controversial.
Goldacre, Bad Science.
———, “Why Don’t Journalists Link to Primary Sources?”
Hazen and Tre¿ l, Science Matters.
    Important Terms
    Suggested Reading

179
Novella, “CCSVI—The Importance of Replication.”
———, “Science Education and Literacy in the U.S.” 
Spellman and Price-Bayer, In Defense of Science.
1. What common traps do journalists fall into when reporting science news 
stories?
2. What are the risks to a modern democracy if the population is largely 
scienti¿ cally illiterate?
    Questions to Consider

180
Lecture 23: Experts and Scienti¿ c Consensus
Experts and Scienti¿ c Consensus
Lecture 23
T
he goal of this course is to make you think about thinking itself, but 
no one person can be the de¿ nitive authority on a complex topic. 
Some individuals do possess genuine expertise, and their opinions 
should at least be taken more seriously than the average person. Expertise, 
however, is a complex topic because there are many levels of expertise. True 
authority, if such exists, rests with the consensus of opinion among relevant 
experts—not with any single individual.
The Nature of Scienti¿ c Consensus
x
We rely on experts all the time. Every time you visit a 
professional—a doctor, lawyer, accountant, or hairdresser—
to some extent, you are deferring to the perceived expertise of
that professional. Otherwise, we could not function in our
complex civilization. 
x
Nobody can be an expert in everything, so it’s reasonable to defer 
to expertise—to people who spend their lives mastering one small 
craft or area of knowledge—although it’s also a good idea to have 
enough of a basic understanding of important areas of life that you 
can judge if someone is a true expert or, perhaps, just faking it.
x
From one perspective, it takes an expert to truly know an 
expert. However, there are some things you can do to judge 
genuine expertise. Is the individual licensed in his or her trade or 
profession? Can the supposed expert document that he or she has 
had adequate training and has maintained a level of expertise? Are 
the individual’s views representative of the profession, or are they 
on the fringe? Finally, do the views of this person make sense? If 
not, get a second opinion. 
x
It is more dif¿ cult to judge an entire profession than an individual. 
Many professions become established, create the trappings of 

181
legitimacy, and even gain licensure without ever developing true 
scienti¿ c legitimacy. Psychics and astrologers, for example, often 
have their own organizations and institutions. In some states, they 
are even licensed.
x
Licensure is often granted as a means of controlling an industry, 
establishing professional standards, and collecting licensure 
fees. However, licensure is too often interpreted by the public as 
indicating that the body of knowledge on which the profession is 
based is scienti¿ cally legitimate.
x
Licensure is all about internal legitimacy—following the rules of 
the profession and ¿ lling out the paperwork, for example. External 
validity, however, means that the body of knowledge has been tested 
against reality and that there are mechanisms of self-correction and 
transparency. Without the methods of science to truly evaluate a 
system of knowledge, there is no way to establish external validity.
x
Some professions, such as the chiropractic profession, exist in the 
gray zone between internal legitimacy and external validity. Some 
of their practices are supported by evidence—such as lower back 
manipulation for uncomplicated acute back strain—but much of 
what they do is not validated by science and is, in fact, at odds 
with modern medical science. There is no system of external 
validity within the chiropractic community, however. There is no 
universal science-based standard, so dubious practices proliferate 
and continue. 
x
There is no guarantee of legitimacy. There are indicators, but 
nothing is de¿ nitive. Furthermore, there is no substitute for a 
critical analysis.
The Nature of Expertise
x
A common mistake that many people make is to consider someone 
who is an expert in one thing to be an expert in all things—as if they 
can have general expertise. Sometimes experts make this mistake 
themselves and stray from their true area of knowledge. 

182
Lecture 23: Experts and Scienti¿ c Consensus
x
Lord Kelvin was a legitimate expert in physics—speci¿ cally in 
thermodynamics—but that did not give him the expertise to make 
pronouncements about other areas of science, such as geology, nor 
to declare that attempts at an airplane would never be successful. 
Late in his career, Lord Kelvin was mistaken again when he 
declared the initial discovery of X-rays to be a hoax, but he soon 
changed his mind when he saw the evidence and even experienced 
an X-ray of his own hand. 
x
Sir Isaac Newton, the 17th-century mathematician and scientist, is 
another example of a famous scientist from history who maintained 
ideas that are now considered pseudoscience. While there is no 
question that Newton was a genius and that his advances in physics, 
speci¿ cally mechanics, transformed our understanding of science 
at the time, he was also 
very interested in alchemy. 
He spent much of his time 
engaged 
in 
alchemical 
research 
without 
any 
tangible results. To Newton, 
alchemy was central to 
his beliefs about how the 
world works—as much as 
mathematics and physics. 
x
Linus Pauling is the only 
scientist to win the solo 
Nobel 
Prize 
twice. 
In 
addition to being an iconic 
science award, the Nobel 
Prize carries with it the 
ultimate imprimatur of expertise. However, that did not stop Linus 
Pauling from adopting fringe ideas toward the end of his career.
x
Linus Pauling promoted the idea that megadoses of vitamin C could 
treat or prevent the common cold. He later expanded this claim to 
include the À u, a more serious illness. Furthermore, he claimed that 
Mathematician and scientist Sir Isaac 
Newton (1642–1727), a genius in 
physics, was also very interested in 
alchemy, a pseudoscience.
© iStockphoto.

183
megadoses of vitamin C could help cancer and that high doses of 
vitamins in general could promote health. His claims were never 
based on adequate scienti¿ c research, and subsequent research has 
shown his claims to be essentially false. However, because of his 
prestige, he had a tremendous inÀ uence on the public.
Experts and Overcon¿ dence
x
Experts are also notoriously fallible, especially when asked to 
make predictions. There tends to be an inverse relationship between 
individual con¿ dence and accuracy. When it comes to predicting 
the future behavior of complex systems, even experts are all but 
useless—but that does not prevent the popularity of asking experts 
to make such predictions.
x
Predicting the future in general is highly problematic. So-called 
futurism is very popular and may serve a purpose in terms of 
preparing for possible future technologies. However, the track 
record of futurists is appallingly bad. 
x
The overcon¿ dence of experts relates partly to the Dunning-
Kruger effect. In 1999, psychologists David Dunning and Justin 
Kruger described how those who are incompetent are generally 
unaware of their own incompetence; the same failings that make 
them incompetent also make them unable to see it. This leads to 
what is more casually referred to as the arrogance of ignorance. 
True expertise is comprised of an appreciation for the limits of 
knowledge—both individual and systemic. 
x
When it comes to scienti¿ c questions, there are two related but 
distinct types of expertise: the relevant science and critical thinking. 
Those with critical thinking skills but who lack speci¿ c scienti¿ c 
knowledge will still have dif¿ culty thinking about scienti¿ c 
questions. Speci¿ c in-depth knowledge is still important to 
developing opinions about scienti¿ c questions.
x
In addition, scienti¿ c knowledge itself may not be enough without 
critical thinking skills. Scientists who are insuf¿ ciently skeptical of 

184
Lecture 23: Experts and Scienti¿ c Consensus
their own claims tend to make critical thinking errors that result in 
serious errors. In order to avoid these kinds of errors, both science 
and critical thinking are needed.
x
Sometimes scientists believe that their scienti¿ c knowledge is 
enough, but this can make them easier to fool because they will 
be overcon¿ dent in their own ability not to be fooled. A famous 
example of this, called Project Alpha, involved researchers at 
Washington University who were given a grant to test alleged 
psychic abilities. Magician James Randi offered his consultation as 
an expert in deception to show the researchers how to avoid being 
tricked, but his help was refused.
x
To demonstrate the need for the speci¿ c expertise of deception 
in ESP research, Randi sent in two teenagers who were able to 
completely fool the researchers with simple sleight-of-hand tricks. 
The researchers were overcon¿ dent in their research skills but
were not prepared for deliberate deception; they were not 
experienced skeptics. 
x
Individuals are biased, quirky, À awed, have incomplete knowledge, 
and have a tendency to overestimate their own knowledge and 
expertise. However, some people have spent years mastering arcane 
knowledge. It’s no guarantee of legitimacy or correctness, but it’s at 
least worthy of being taken seriously.
x
The quirkiness of the individual is why a consensus of opinion 
is much more reliable. Individual biases will tend to average out. 
Gaps in knowledge and perspective will also tend to compensate for 
each other. Within science, differences of interpretation will tend, 
over time, to get resolved by seeking better evidence. Eventually, 
consensus is built upon solid ground.
x
This does not mean that the consensus of scienti¿ c opinion is 
always correct, but the probability of being correct is higher than 
the opinion of an individual. If you are going to disagree with the 
consensus of opinion, there should be a good reason.

185
Determining Consensus of Scienti¿ c Opinion
x
Sometimes it may be dif¿ cult to determine what the consensus of 
scienti¿ c opinion is on a topic. Different specialties may have a 
different opinion. For example, geologists are fairly convinced that 
a single asteroid wiped out the dinosaurs about 65.5 millions years 
ago. However, many paleontologists are not convinced. There was 
certainly an asteroid impact at that time, but some believe that the 
fossil evidence suggests a longer extinction of the dinosaurs.
x
The nature of consensus about global warming has been intensely 
debated. The International Panel on Climate Change (IPCC) 
represents a consensus of opinion among climate scientists, but 
some have criticized the process as political.
x
It seems that there is a robust consensus that Earth is warming, and 
anthropogenic causes are important, but there remains minority 
dissent. Political entities with strong opinions on this issue have 
challenged the notion that there is a consensus and have promoted 
the views of dissenters.
x
A 2011 study by Richard Muller—a physicist at the University of 
California, Berkeley—sought to replicate prior analysis by NASA 
and other temperature data about Earth. Muller was skeptical 
of prior analyses and the conclusion of warming, so he set out to 
disprove it. He and his team conducted their own analysis of all 
available data and concluded that Earth is warming by 0.9 degrees 
Celsius over the last 50 years. Furthermore, their temperature map 
closely matches that of NASA and others, con¿ rming the prior 
analyses of which they were doubtful.
x
In the end, Muller changed his mind and accepted the fact that the 
planet is warming. This is an excellent example of how science 
should work. Disagreement was resolved by further analysis, and 
the data held sway. As a result, the consensus on global warming 
was strengthened. However, it does not seem that this will end the 
political controversy over global warming.

186
Lecture 23: Experts and Scienti¿ c Consensus
x
The consensus of scienti¿ c opinion is an important factor to 
consider when evaluating any claim. A robust consensus should 
be built on evidence and hammered out in the research and at 
meetings. Consensus is more reliable than the opinions of any one 
scientist, but it is no guarantee.
x
While it is reasonable to respect a hard-won consensus, there 
is also a role for dissenting opinions. As with science in general, 
there is never any guarantee of being correct. The goal is simply to 
maximize the probability of being correct while remaining open to 
new evidence and analyses.
Dunning-Kruger effect: The phenomenon that one’s incompetence in a 
particular area also renders the individual incapable of detecting his or her 
own incompetence, resulting in a false sense of con¿ dence. 
Burton, On Being Certain. 
Freedman, Wrong .
Gardner, Future Babble.
Hallinan, Why We Make Mistakes.
Novella, “Beware the Nobel Laureate Argument from Authority.”
———, “The Nature of Consensus.”
Wilson, Consilience.
1. Why do experts frequently make serious mistakes?
2. What does it mean that there is a scienti¿ c consensus on any
particular question?
    Important Term
    Suggested Reading
    Questions to Consider

187
Critical Thinking and Science in Your Life
Lecture 24
T
his lecture will review the critical thinking skills outlined in this 
course. Taken together, they represent an outlook on science, claims, 
and belief that is referred to as scienti¿ c skepticism. Skepticism refers 
to an appreciation for the limits and foibles of the human brain combined 
with the power of the self-corrective approach of science; it puts in place 
a rational ¿ lter through which all claims to truth must pass. This lecture 
describes the skeptical approach, including how to deal with friends, family, 
and coworkers. It also covers scienti¿ c education and scienti¿ c literacy. 
Investing in Critical Thinking
x
The human brain, although powerful, has many À aws and 
weaknesses. We only perceive and attend to a small amount of the 
world around us, and that sensory information is highly ¿ ltered, 
processed, and ultimately constructed. Immediately after we 
experience that highly altered sensory information, our memories 
start to further alter it. Every time we remember something, we add, 
change, and fuse details; we update the memory. 
x
Humans are both rational and emotional creatures. We come with a 
suite of emotional needs and biases. Our default mode of operation 
is to make decisions for unconscious or only dimly seen reasons and 
then rationalize them with motivated reasoning. We then succumb 
to errors and biases in thinking, logical fallacies, insuf¿ cient 
information, and a poor grasp of math and statistics.
x
There are many cognitive biases that conspire together to lead us 
to take and ¿ rmly hold beliefs that potentially have little or no 
relationship to reality. What we believe is a narrative that is stitched 
together from À awed information and modi¿ ed by our cognitive 
biases and emotional needs and desires. This is an essential 
realization—one that separates critical thinkers from those who 
think less critically.

188
Lecture 24: Critical Thinking and Science in Your Life
x
Once we accept that we cannot trust what we think we remember, 
we become humble in the face of our experiences and knowledge. 
Then, we are open to the dire need for a systematic approach to 
knowledge—methods to compensate for all the many À aws of our 
brains’ function.
x
In a way, science and critical thinking are our ¿ x for all the À aws in 
human reasoning. That is why it has transformed our understanding 
of ourselves and of the universe in which we live. However, even 
for those who have a working knowledge of critical thinking, it’s 
dif¿ cult to fully appreciate the potential for self-deception. 
x
There is a tendency to grossly underestimate the human potential for 
self-deception, even among skeptics. We all want to view ourselves 
as rational beings, but that is the greatest self-deception of all. If 
you think you are not biased and cannot rationalize a completely 
erroneous belief, then that bias can undo all of your critical
thinking skills.
x
Often, scientists and great intellectuals may become more 
susceptible to self-deception because they lose their intellectual 
humility. The ultimate lesson of our understanding of neuroscience 
and psychology is humility in the face of our own knowledge.
x
Millions, or perhaps even billions, of people can be profoundly 
wrong about a belief for hundreds or even thousands of years. Entire 
cultures may endorse a belief system that is strongly reinforced by 
con¿ rmation bias. Eventually, the apparent examples of support 
seem overwhelming. There is also extreme social pressure to 
conform because our in-group identity is tied to the belief system.
x
Most of all, it is important to apply the rules of critical thinking to 
yourself—but there are barriers to this. Once you invest your ego in 
a conclusion, motivated reasoning will distort and bias your critical 
thinking into that direction. In the end, you will still be wrong; you 
will just be more con¿ dent in your error. 

189
x
If, on the other hand, you invest your ego in the process of critical 
thinking, and not in any particular conclusion, then you will be freer 
to follow the logic and evidence wherever it leads. You will, in fact, 
take pride in the ability to change your opinion as new information 
becomes available. 
Critical Thinking and Scienti¿ c Knowledge
x
We cannot change human nature. However, recent neuroscience 
is ¿ nding that the brain can actually change its wiring in response 
to use. This is called brain plasticity. Sometimes, this is simple 
learning; the brain can learn and remember. Habits of thought can 
also become ingrained. 
x
However, you cannot change your basic personality pro¿ le. 
Personality has proven to be a remarkable resistant to change. 
What you can change is how you deal with your basic emotional 
makeup. This begins with what some psychologists call emotional 
intelligence, which is an insight into your emotional makeup. This 
is why knowledge of psychological needs and cognitive biases is
so important.
Recent neuroscience has found that the brain is plastic; in other words, it can 
change its wiring in response to use.
© iStockphoto.

190
Lecture 24: Critical Thinking and Science in Your Life
x
We cannot change our basic evolved emotional needs and reactions, 
but we can change how we respond to those emotions. We can 
engage our frontal lobes, our executive function, and develop 
adaptive responses. These responses can be learned and ingrained 
through habit and effort.
x
The application of rational thought, scienti¿ c methods, and skeptical 
thinking frequently prompts some to ask whether this approach to 
life denies human emotions. However, the full emotional palette is 
part of the human condition, and it is healthy—even rational—to 
embrace it.
x
We live in a beautiful, subtle, elegant, and complex universe. 
Understanding something about how it works has a profound 
beauty of its own.
x
In terms of theories and facts, there is no absolute right and wrong; 
there are only degrees of con¿ dence. Furthermore, all conclusions 
are tentative because our information is always incomplete. 
Empirical knowledge is a journey—not a destination. If, however, 
you think you have arrived at absolute truth, then your journey of 
science and critical thinking is over.
x
To be a critical thinker is to be comfortable with uncertainty and 
with the limits of human knowledge and to be aware of all the many 
À aws and limitations of human intelligence—and, therefore, to be 
À exible in the face of new ideas or information but to not be afraid 
to acknowledge that some ideas are objectively better than others.
x
Reliable scienti¿ c knowledge is possible. Logic can be valid or not 
valid. Some data is more reliable than others. Being À exible does 
not mean being gullible in the face of any claim; it means critically 
analyzing all claims and judging them on their merits.
Applying Critical Thinking Skills
x
When someone states a belief or claim that you believe to be wrong 
or invalid, you ¿ rst have to recognize that nobody likes being told 

191
that he or she is wrong or having a valued belief taken from him or 
her. At the same time, you don’t want to appear to be accepting an 
illogical or false belief.
x
There may be important decisions at stake, such as whether to seek 
an unconventional treatment for a serious illness. One approach 
that is helpful is to simply ask questions; engage the other person’s 
natural tendency to be curious and ask questions.
x
In addition, recognize that it is extremely rare for someone to 
simply abandon a valued belief when confronted with discon¿ rming 
information. In fact, recent psychological research shows that 
when this happens, people tend to hold the erroneous belief even
more strongly.
x
Instead of challenging their belief, give analogies or examples that 
would encourage them to question aspects of their belief. Plant a 
seed of critical thinking, and then encourage it over time.
x
Critical thinking is the process of engaging your higher cortical 
function as a ¿ lter and control on the more primitive parts of 
the brain. Not only can we all strive to engage in more critical 
thinking ourselves, but we can also strive to make those around us
more critical.
x
Furthermore, if our goal is to engage society overall in more 
critical thinking, then we can also change our educational system. 
Additionally, our culture can change, especially the way in which 
science and information is presented in the media. 
x
Our society does a generally poor job of teaching critical thinking 
and scienti¿ c literacy. As a society, we can learn to approach science 
and education as a process rather than as a set of answers that 
never change. Science news items should be framed as part of an 
ongoing process of scienti¿ c discovery—not always as a de¿ nitive 
breakthrough that ¿ nally gives us the answer to a scienti¿ c mystery. 

192
Lecture 24: Critical Thinking and Science in Your Life
Education should focus on teaching children the process of 
science—not giving them authoritative answers to memorize.
x
Additionally, we need to think of education as more than what 
happens in the classroom. The evidence shows, in fact, that most 
learning takes place outside the classroom.
x
More scientists are engaging in blogs, podcasts, and other new 
media to make their scienti¿ c discipline accessible to the public. 
x
In statistical analysis, the method of constantly updating our 
knowledge as new information comes in is called a Bayesian 
analysis. We look at all the knowledge that currently exists to arrive 
at the best current answer to any question, and we acknowledge all 
the limitations and uncertainties in the data and in our conclusions. 
Then, we constantly update our tentative conclusions as new data 
comes in. 
x
Science, skepticism, and critical thinking—including formal 
logic—are rigorous processes that we use to handle the complexity 
of the world because our À awed brains are not up to the task
by themselves.
x
It is this process of updating that should be reÀ ected more in the 
classroom and in the media—not approaching science as an arcane 
ritual that is understood only by scientists within a given ¿ eld and 
that spits out authoritative answers.
Critical Thinking and Metacognition
x
The critical thinking approach to knowledge and life is very 
empowering, and its power comes from its acceptance of reality. 
x
Scienti¿ c skepticism is a mature view of the human condition and 
knowledge. It is not scandalized by the À aws in the human efforts 
of science—nor is it naive about the existence of those À aws and 
the limitations of the human brain. The critical thinking approach 

193
involves doing the best that we can with full knowledge and 
appreciation for those weaknesses and limitations.
x
The goal of this course is to understand all of the cognitive and 
other biases that rule our thinking. Without understanding of 
these biases, we are destined to simply be carried along by our
À awed inclinations.
x
With a thorough understanding of logic and cognitive biases, 
however, we have the opportunity to engage in metacognitive 
reasoning. We can consciously put into place a metacognitive self-
check on our own reasoning. We engage our frontal lobe executive 
function to ¿ lter and inhibit our more primitive cognitive impulses. 
We can try to transcend our biases by being truly aware of them.
x
Even with the understanding of metacognition, when logic and 
evidence leads us to uncomfortable conclusions, this will create 
cognitive dissonance. Recognizing that dissonance and how it 
will motivate you empowers you to engage in metacognition—to 
choose an adaptive, rational resolution rather than rationalizing a 
convenient answer. 
x
Metacognition is not easy, and it takes practice, knowledge, and 
dedication to stay in the metacognitive realm—to allow the facts 
and logic, rather than our biases and emotions, rule our thinking.
 Bayesian analysis: A method for calculating the odds of a theory or 
phenomenon being true by updating the prior probability as new facts and 
information become available. 
Ariely, Predictably Irrational, Revised and Expanded Edition.
Chabris and Simons, The Invisible Gorilla.
    Important Term
    Suggested Reading

194
Lecture 24: Critical Thinking and Science in Your Life
McRaney, You Are Not So Smart .
Novella, “How Gullible Are You?”
Tavris and Aronson, Mistakes Were Made (But Not by Me).
1. How can we apply critical thinking skills in our everyday lives?
2. How does an understanding of the details of our cognitive frailties 
empower us to transcend them?
    Questions to Consider

195
Glossary
ad hominem: A logical fallacy in which an assertion is said to be false or 
unreliable because of an alleged negative attribute of the person making the 
assertion; arguing against the person rather than the claim itself. 
agnostic: The notion that unfalsi¿ able hypotheses, such as the existence of 
God, are not only unknown—but are also unknowable. 
alien hand syndrome: A neurological syndrome in which a person’s limb, 
such as a hand, feels as if it is acting on its own—without conscious control. 
This results from damage to the brain pathways that compare the intention to 
move with actual movements. 
anchoring: The tendency to focus disproportionately on one feature or 
aspect of an item or phenomenon and base judgments on that one feature.
anecdote: An uncontrolled or poorly documented observation or experience. 
anomaly: A phenomenon that is incompatible with or cannot be explained 
by current scienti¿ c theories. 
argument: A statement that is used to support a conclusion or belief, often 
following deductive reasoning. 
availability heuristic: The tendency to believe that a phenomenon is 
more likely or more important if we can readily think of examples of the 
phenomenon. 
base-rate fallacy: Failure to consider how common a phenomenon is (the 
base rate) when estimating how likely it is, preferring other factors such as 
representativeness. 

196
Glossary
Bayesian analysis: A method for calculating the odds of a theory or 
phenomenon being true by updating the prior probability as new facts and 
information become available. 
blinding: In scienti¿ c studies, this refers to the process of hiding the status 
of a subject (whether they are in the intervention or the control group) from 
the subject (single blind) or also from the experimenter (double blind). 
change blindness: The experimentally veri¿ ed tendency of humans not to 
notice changes in their environment.
clustering illusion: The tendency of people to underestimate the clumping 
of statistically random distributions, which gives the illusion of clustering.
cognitive bias: A subconscious tendency to think in a certain way, or a bias 
toward certain decision-making pathways. 
cognitive dissonance: An unpleasant emotion generated by the simultaneous 
existence of mutually exclusive beliefs.
cold fusion: A hypothetical technique that causes nuclear fusion to occur at 
relatively low temperatures, especially in a way that would be practical for 
energy production. 
cold reading: A mentalist technique by which the reader can seem to have 
speci¿ c knowledge of the target (the subject of the reading) using vague or 
high-probability guesses and feedback. 
collective wish ful¿ llment: A form of mass delusion characterized by the 
ful¿ llment of a common desire or wish, such as making a fantastic discovery. 
community À ight panic: A form of mass delusion in which fear or an 
immediate threat spreads through a town or community, causing many to À ee 
from the alleged threat. 

197
confabulation: The ¿ lling in of details missing from either perception or 
memory. The brain invents the missing details to construct a consistent 
narrative. 
con¿ rmation bias: A cognitive bias to support beliefs we already hold, 
including the tendency to notice and accept con¿ rming information while 
ignoring or rationalizing discon¿ rming information. 
congruence bias: The tendency to test our own theories but not alternative 
theories, which can lead to a false sense of con¿ rmation of our own beliefs. 
constancy: The fact that our brains construct a constant and consistent model 
of what we perceive that generally matches reality.
control group: In an experimental study, the control group receives a 
sham or placebo intervention that is physiologically inert so that it can be 
compared to the treatment group. 
critical thinking: Applying systematic logic and doubt to any claim or 
belief; thinking carefully and rigorously.
data mining: The process of sifting through large sets of data and looking 
for apparent patterns. This is a legitimate way to generate hypotheses—but 
not of con¿ rming them—because this process lends itself to ¿ nding illusory 
patterns. 
deductive reasoning: Reasoning that begins with one or more general 
statements that are taken as premises and then concludes what must be true 
if the premises are true. 
default mode: A common behavior that results from evolved emotions and 
subconscious processes without metacognitive insight. 
delusion: A ¿ xed, false belief that is vigorously held even in the face of 
overwhelming contradictory evidence. 

198
Glossary
demarcation problem: A philosophical term that refers to the dif¿ culty in 
clearly de¿ ning a separation between two ends of a continuum, such as the 
difference between science and pseudoscience. 
denialism: A form of pseudoscience that uses poor logic and distortion 
of science in order to deny the conclusions or legitimacy of an accepted 
scienti¿ c theory or fact.
Dunning-Kruger effect: The phenomenon that one’s incompetence in a 
particular area also renders the individual incapable of detecting his or her 
own incompetence, resulting in a false sense of con¿ dence. 
electronic voice phenomenon (EVP): The phenomenon of apparent words 
or phrases being found in audio recordings of allegedly haunted locations. 
Believers ascribe EVP to ghost phenomena, but they are more easily 
explained as audio pareidolia. 
epidemiology: The study of the incidence, spread, and associated factors of 
disease. 
epistemology: The branch of philosophy that deals with knowledge and the 
methods of science. 
executive function: A function of the frontal lobes of the brain, speci¿ cally 
the ability to control and plan one’s behavior to meet long-term self-interest 
and social integration. 
exemplar: A case that vividly represents a phenomenon, making it seem 
more likely, common, or signi¿ cant.
experimental study: Scienti¿ c studies that involve a speci¿ c intervention 
performed by the experimenters. 
exposure effect: The tendency to more favorably rate things or beliefs with 
which we are more familiar. 

199
false balance: The treatment of a topic in journalism as if it is controversial 
when it isn’t, or treating two sides of an issue as if they are scienti¿ cally 
equal when they are not. 
false continuum: A logical fallacy in which the fact that a characteristic 
varies along a continuum is used to argue that the extreme ends of the 
continuum do not exist or cannot be meaningfully identi¿ ed. 
false dichotomy: A logical fallacy in which multiple choices are reduced 
arti¿ cially to only a binary choice, or where a continuum is reduced to its 
two extremes. 
false memory syndrome: The implantation of false memories that are 
thought to be real by the possessor of the memory, often resulting from 
strong suggestion, imagining, or hypnosis. 
falsi¿ ability: The key feature of any scienti¿ c hypothesis—that, at least 
theoretically, there must be some evidence to prove or imply that the 
hypothesis is not true. 
À ashbulb memory: A detailed memory or snapshot of a sudden, unexpected, 
and emotionally signi¿ cant event. 
folie à deux: A shared delusion between two individuals, in which one 
person tends to be dominant and the source of the delusion. 
Forer effect: The tendency to take vague or general statements and apply 
them speci¿ cally to ourselves, or to ¿ nd speci¿ c examples, making the 
statements seem more accurate and speci¿ c than they are. 
free energy: The general term for any alleged process that produces energy 
without burning fuel or other limited resource. 
free will: The ability of a sentient being to make voluntary choices and 
decisions. Philosophers argue about whether humans have true free will or 
just the illusion of free will. 

200
Glossary
functional magnetic resonance imaging (fMRI): Application of magnetic 
resonance imaging, a type of medical scanner, that can be used to image the 
degree to which different parts of the brain are active. 
fundamental attribution error: A psychological term that refers to the 
tendency to ascribe the actions of others to internal motives and attributes 
rather than external situational factors. 
gambler’s fallacy: The false belief that prior events dictate the probability 
of future events, even when they are random and independent, such as the 
results of random coin À ipping. 
global workspace: A controversial theory (disputed by recent research) that 
posits that a distributed network in the brain is the common pathway for all 
conscious experience. 
grand conspiracy: A large, far-reaching conspiracy often alleged to span 
multiple organizations, people, and even generations. 
group delusion: A delusion shared among a small group, such as a cult, 
typically led by one charismatic leader. 
heuristic: A cognitive rule of thumb or mental shortcut that we 
subconsciously make that may be true much of the time but is not logically 
valid. 
hierarchy of needs: The term coined by Abraham Maslow that describes the 
relationship among the basic and higher human needs—from physiological 
needs like food to emotional needs like self-actualization. 
hyperactive agency detection: The human tendency to detect a conscious 
agent behind natural or random behavior or events—for example, believing 
that random events are a conspiracy to punish us. 
hypnosis: Although not a trance, hypnosis is a state of mind characterized by 
alertness but also by high suggestibility. 

201
ideomotor effect: Subconscious muscle movements that conform to 
expectations. 
inattentional blindness: This phenomenon refers to the lack of attention 
to sensory information, especially while attending to other sensory input. 
Signi¿ cant information right before our eyes can be completely missed and 
is simply not processed.
inductive reasoning: Inductive reasoning begins with observations of the 
world and then derives general statements about what is probably true from 
those observations. 
innumeracy: A lack of working knowledge of mathematics, probability, and 
statistics. 
intelligent design: The term used to self-describe a new school of 
creationism that holds that life is too complex to have arisen from natural 
processes alone. 
intuition: Decision making or feelings, such as responses to social cues, that 
derive from subconscious brain processes.  
leftmost-digit bias: The tendency of people to focus disproportionately on 
the leftmost digit of a number when assessing overall value or amount. 
logic: A formal process or principle of reasoning.
logical fallacy: A logical operation that is not valid. 
long-term memory: Memories that have been consolidated into long-term 
storage. 
lottery fallacy: The fallacy of using a speci¿ c post-event outcome to 
calculate the pre-event odds of any outcome. For example, the odds of a 
speci¿ c person winning the lottery may be very low, but that does not mean 
that the event is too unlikely to have occurred by chance alone because the 
probability of anyone winning was high. 

202
Glossary
manufactroversies: A false controversy created in order to make a 
scienti¿ cally accepted idea seem uncertain or controversial.
mass delusion: A delusion or false belief shared among a large group of 
people—even an entire community or culture. 
mass hysteria: Similar to a mass delusion but speci¿ cally involving
physical symptoms.
McGurk effect: The phenomenon that the consonant sounds we hear are 
affected by the lip movements we see. 
meta-analysis: A mathematical process of combining the results of many 
studies into a single study for statistical analysis. 
metacognition: Thinking about thinking; examining the processes by which 
we think about and arrive at our own beliefs. 
methodological naturalism: The philosophical assumptions that underlie 
scienti¿ c methodology; speci¿ cally, the assumption that all effects have 
natural causes. 
multilevel marketing: A corporate structure in which salespeople must pay 
a percentage of their pro¿ t to sponsors above them and, in turn, can sponsor 
salespeople below them who have to pay them a percentage. 
multitasking: Dividing attention between two or more tasks or
sensory inputs. 
N-rays: A hypothetical type of radiation that was allegedly discovered by 
René Blondlot but that was eventually exposed as illusory. 
neocortex: The neocortex is the most recently evolved portion of the human 
brain—speci¿ cally, the frontal lobes, which provide executive function, 
among other things. 

203
Nigerian scam: A type of scam in which the target is promised a large 
sum of money for helping a foreign national move money into the target’s 
country, but only after money is paid for bribes, customs, and other needs. 
nonoverlapping magisteria (NOMA): The term coined by Stephen Jay 
Gould to describe his philosophy that science and faith are separate and 
nonoverlapping schools of thought.
non sequitur: A Latin term referring to an invalid argument in which the 
conclusion does not logically follow from the premises. 
observational study: Scienti¿ c studies in which the behavior of groups are 
observed in the real world—without experimenter intervention. 
Occam’s razor: A rule of thumb, also known as the law of parsimony, 
that states that when two or more hypotheses are compatible with the 
available evidence, the one that introduces the fewest new assumptions is to
be preferred. 
optical illusion: The common term for the failure of constancy, or a 
breakdown in the process of creating a constant and consistent view of 
reality. Illusions occur when what our brain constructs does not match reality 
or when there is an inherent contradiction or ambiguity in the way perceptual 
information is constructed.
orb: The name given by ghost researchers to a sphere of light that is seen on 
¿ lm and believed to be evidence of ghosts but is more easily explained as a 
photographic artifact. 
overprecision: A psychological term that refers to the tendency for people to 
overestimate the accuracy of their own knowledge. 
overunity: A process that generates more energy than it consumes, in 
violation of the laws of thermodynamics. 
ownership module: The part of the brain that creates the sensation that we 
own the various parts of our body. 

204
Glossary
paradigm: A term developed by Thomas Kuhn to refer to a set of scienti¿ c 
beliefs and assumptions that prevail at any particular time in history. 
paranormal: Any belief or phenomenon that allegedly is outside the 
naturalistic laws of science. 
pareidolia: The tendency to see patterns in random noise—for example, 
seeing a human face in the random features of a cloud. 
pattern recognition: The phenomenon of perceiving patterns—whether in 
visual information, other sensory information, or even events or behavior. 
Humans generally have a great ability to recognize patterns and a tendency 
to see patterns even when they are illusory. 
petitio principii: A Latin term for begging the question, or assuming one’s 
conclusion in the premise of an argument.
phantom limb: An illusory limb that does not exist but that the subject can 
feel and even have the sense that they can move. It is commonly, but not 
exclusively, the result of amputation. 
philosophical naturalism: The philosophical position that the natural world 
is all that exists—that there is nothing that is supernatural. 
post hoc ergo propter hoc: Literally meaning “after which hence by which,” 
a logical fallacy in which it is assumed that B is caused by A simply because 
B follows A. 
post-modernism: A school of philosophical thought that treats all knowledge 
as equally valid social constructs or narratives. 
premise: A fact that is assumed to be true, or treated as if it is true, as a 
starting point for an argument. 
prospective study: A study that involves selecting subjects and then 
following them to observe their future outcome. 

205
pseudoscience: A practice that super¿ cially resembles the process of science 
but distorts proper methodology to the point that it is fatally À awed and does 
not qualify as true science. 
psi phenomena: Alleged phenomena dealing with extrasensory perception—
also called anomalous cognition—such as mind reading, remote viewing, 
and precognition. 
psychosis: A psychiatric condition characterized by impaired reality testing. 
P value: A statistical term referring to the probability that the results of a 
study would be what they are or greater given the null hypothesis—that the 
proposed phenomenon being studied is not true. 
reality testing: A cognitive process by which the brain compares any new 
information to its internal model of reality to see if the new information 
makes sense.
recall: The act of bringing to the conscious mind memories stored in long-
term memory. 
reductio ad absurdum: A Latin term that refers to a legitimate logical 
argument in which a premise is taken to its logical, although absurd, 
conclusion. This can be a fallacious argument if the absurd conclusion is 
forced and does not follow inevitably from the premise. 
regression to the mean: A statistical phenomenon in which large deviations 
from average behavior are likely, by chance alone, to return to more
average behavior. 
representativeness heuristic: The assumption or bias to believe that causes 
resemble effects. Therefore, for example, a large effect must have had an 
equally large cause. 
retro¿ tting: Fitting predictions to known outcomes after they occur. 

206
Glossary
retrospective study: A study in which subjects are selected, and then data is 
gathered about their history.
Russell’s teapot: A hypothetical teapot proposed by Bertrand Russell that 
is orbiting the Sun between Earth and Mars to make the point that not all 
claims that cannot be proven false should be accepted as true. 
scienti¿ c skepticism: A comprehensive approach to knowledge that 
emphasizes critical thinking and science. Skepticism combines knowledge 
of philosophy of science, scienti¿ c methods, mechanisms of self-deception, 
and related ¿ elds to approach all claims to truth in a provisional and 
systematic way. 
self-deception: The act of deceiving oneself due to a host of perceptual, 
cognitive, and memory biases and À aws. 
sharpshooter fallacy: Choosing the criteria for success speci¿ cally to match 
the results that are already known. 
short-term memory: Memory for immediate sensory or internal information 
that lasts from seconds to a few minutes. 
sick building syndrome: A form of mass hysteria centering around a 
building that is believed to be the source of a mystery ailment. 
sound: In logic, this describes an argument that has both true premises and 
valid logic, and therefore, the conclusion must be true. 
source amnesia: The tendency to forget the source of information more 
easily than the information itself. 
spam: Unwanted e-mail messages usually used for advertising or to spread 
rumors or propaganda. 
split-brain experiment: An experiment on a subject who had the connection 
between their two brain hemispheres surgically cut that helped reveal the 
functions of the two hemispheres and how they work together. 

207
statistical signi¿ cance: A statistical term referring to the comparison 
of target and control groups in scienti¿ c studies; when the difference in 
outcome or a particular feature is greater than a predetermined threshold, 
then the results are considered to be statistically signi¿ cant.
subconscious: Brain processing that occurs without conscious awareness. 
supernumerary phantom limb: A phantom limb that is not simply a 
replacement for a missing limb but is experienced in addition to the four 
natural limbs. 
symbolic community scare: A long-standing mass delusion that tends to 
wax and wane over years and is centered around a perceived existential 
threat to the community. 
synesthesia: When more than one sensory modality is combined or when 
one sensory modality is interpreted as another, such as smelling colors. 
systematic review: A review and analysis of all the relevant scienti¿ c studies 
published on a speci¿ c question.
tautology: In logical terms, this is an argument in which the conclusion 
simply repeats the premise and is, therefore, not a true argument. 
theory of mind: A psychological term that refers to the ability to understand 
and think about the fact that other people have their own conscious existence 
with their own feelings and motivations. 
transcranial magnetic stimulation (TMS): Technology that uses magnetic 
¿ elds to either increase or decrease activity in speci¿ c regions of the brain. 
transitional fossil: A paleontological term to refer to a fossil specimen that 
represents a species that lies phylogenetically between two other species
or groups. 
urban legend: A false belief or story that represents a common fear or 
anxiety in society and spreads largely through rumor. 

208
Glossary
valid: An argument in which the logic is proper and not fallacious. 
witch hunt: The persecution of a person or group using accusations of 
heinous acts, or association with such acts, and using dubious investigating 
techniques designed to achieve the conclusion of guilt. 
working memory: A type of memory that is distinct from short-term 
memory because it consists of information that the brain can manipulate, 
such as performing mental calculations. 
Zyklon-B: The trade name for a form of cyanide gas that is used for 
delousing, but it is also infamously used by the Nazis during World War II to 
exterminate humans. 

209
Additional References by Lecture
Lecture 1
Chudler, Eric. “Brain Facts and Figures.” Washington University. http://
faculty.washington.edu/chudler/facts.html.
American Council on Exercise. “ACE Study Reveals Power Balance 
Bracelet to be Ineffective.” http://www.ace¿ tness.org/blog/1351/ace-study-
reveals-power-balance-bracelet-to-be.
Lecture 2 
Maslow, Abraham 
H. 
Motivation 
and 
Personality. 
New 
York:
HarperCollins, 1954.
Whitson J. A., and A. D. Galinsky. “Lacking Control Increases Illusory 
Pattern Perception.” Science 322, no. 5898 (2008): 115–117. doi: 10.1126/
science.1159845.
Trafton, Anne. MITNews. “Moral Judgments Can Be Altered ... by 
Magnets.” http://web.mit.edu/newsof¿ ce/2010/moral-control-0330.html.
Lecture 3
NTSB. “TWA À ight 800 NTSB report.” http://www.ntsb.gov/investigations/
summary/AAR0003.html.
Wynn, Charles. “Seen Any Red Pandas Lately?” http://www.nsta.org/
publications/news/story.aspx?id=53418.
Novella, Steven. “UFOs and Other Optical Illusions.” NeuroLogica Blog. 
http://theness.com/neurologicablog/index.php/ufos-and-other-optical-
illusions.
Sedgwick, H. A. “Space Perception.” In Handbook of Perception and 
Human Performance, edited by K. R. Boff, L. Kaufman, and J. P. Thomas, 
vol.1. New York: Wiley, 1986.

210
Additional References by Lecture
Skeptical Inquirer Editors. “The Stephenville Lights: What Actually 
Happened.” 
http://www.csicop.org/si/show/stephenville_lights_what_
actually_happened.
Morrot, G., F. Brochet, and D. Dubourdieu. “The Color of Odors.” 
Brain Lang 79, no. 2 (2001): 309–320. http://www.ncbi.nlm.nih.gov/
pubmed/11712849.
Musser, George. “Time on the Brain: How You Are Always Living in the 
Past and Other Quirks of Perception.” Scienti¿ c American. September
15, 2011.
Lecture 4 
Talarico, Jennifer M., and David C. Rubin. “Con¿ dence, Not Consistency, 
Characterizes Flashbulb Memories.” http://911memory.nyu.edu/abstracts/
talarico_rubin.pdf.
Skurnik, I., C. Yoon, D. C. Park, and N. Schwarz. “How Warnings 
about False Claims Become Recommendations.” Journal of Consumer 
Research 31 (2005): 713–724. http://research.chicagobooth.edu/cdr/docs/
FalseClaims_dpark.pdf.
Kensinger, Elizabeth A., Rachel J. Garoff-Eaton, and Daniel L. Schacter. 
“Effects of Emotion on Memory Speci¿ city: Memory Trade-Offs Elicited by 
Negative Visually Arousing Stimuli.” Journal of Memory and Language 56 
(2007): 575–591. http://www.wjh.harvard.edu/~dsweb/pdfs/07_05_EAK_
RGE_DLS.pdf.
McAdams, John. “Jean Hill, The Lady in Red.” http://mcadams.posc.
mu.edu/jhill.htm.
Loftus, E. F., D. G. Miller, and H. J. Burns. “Semantic Integration of Verbal 
Information into a Visual Memory.” Journal of Experimental Psychology: 
Human Learning and Memory 4, no. 1 (1978): 19–31. http://www.
psychology.uiowa.edu/Classes/31016/PDFs/Loftus_Miller_Burns.pdf.

211
Laws, Keith. “Reconstructive Memory: Confabulating the Past, Simulating 
the 
Future.” 
http://neurophilosophy.wordpress.com/2007/01/09/
reconstructive-memory-confabulating-the-past-simulating-the-future.
Rajagopal, Priyali, and Nicole Votolato Montgomery. “I Imagine, I 
Experience, I Like: The False Experience Effect.” January 13, 2011. http://
ssrn.com/abstract=1739953.
Lecture 5
Mikkelson, Barbara, and David Mikkelson. “Doll Talk.” Snopes.com. http://
graphics1.snopes.com/business/audio/mommycoo.mp3.
Novella, Steven. “Vision and How the Brain Organizes Itself.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/vision-and-how-the-
brain-organizes-itself.
Lecture 7
Jarcho, J. M., E. T. Berkman, and M. D. Lieberman. “The Neural Basis of 
Rationalization: Cognitive Dissonance Reduction during Decision Making.” 
Social Cognitive and Affective Neuroscience. http://uoregon.academia.edu/
Berkman/Papers/175223/The_neural_basis_of_rationalization_Cognitive_
dissonance_reduction_during_decision-making.
Lecture 9
Tversky, A., and D. Kahneman. “Judgment under Uncertainty: Heuristics 
and Biases.” Science 185 (1974): 1124–1130.
Tversky, A., and D. Kahneman. “Availability: A Heuristic for Judging 
Frequency and Probability.” Cognitive Psychology 5 (1973): 207–232.
Song, H., and N. Schwarz. “If It’s Dif¿ cult to Pronounce, It Must Be Risky.” 
Psychological Science 20, no. 2 (2009): 135–138. http://www.ncbi.nlm.nih.
gov/pubmed/19170941.
Lecture 10 
Carroll, Robert T. “Law of Truly Large Numbers.” The Skeptic’s Dictionary. 
http://www.skepdic.com/lawofnumbers.html.

212
Additional References by Lecture
Gilovich, Thomas. How We Know What Isn’t So: The Fallibility in Human 
Reasoning in Everyday Life. New York: Free Press, 1993.
Gilovich, Thomas, Robert Vallone, and Amos Tversky. “The Hot Hand 
in Basketball: On the Misperception of Random Sequences.” Cognitive 
Psychology. 17, no. 3 (1985): 295–314.
Lecture 11
Ichthyology at the Florida Museum of Natural History. “Annual Risk of 
Death during One’s Lifetime.” http://www.À mnh.uÀ .edu/¿ sh/sharks/attacks/
relarisklifetime.html.
Mulford, J. S., H. Oberli, and S. Tovosia. “Coconut Palm–Related Injuries in 
the Paci¿ c Islands.” ANZ Journal of Surgery 71, no. 1 (2001): 32–34. http://
www.ncbi.nlm.nih.gov/pubmed/11167595.
Kahneman, D., and A. Tversky. “On the Psychology of Prediction.” 
Psychological Review 80 (1973): 237–251.
Tversky, A., and D. Kahneman. “Extension versus Intuitive Reasoning: The 
Conjunction Fallacy in Probability Judgment.” Psychological Review 90, no. 
4 (1983): 293–315. doi:10.1037/0033-295X.90.4.293. http://content2.apa.
org/journals/rev/90/4/293.
Alpert, M., and H. Raiffa. “A Progress Report on the Training of Probability 
Assessors.” In Judgment under Uncertainty: Heuristics and Biases, edited by 
D. Kahneman, P. Slovic, and A. Tversky. Cambridge: Cambridge University 
Press, 1982.
Lecture 12 
Bartholomew, Robert E. “Two Mass Delusions in New England.” The Ness. 
http://www.theness.com/index.php/two-mass-delusions-in-new-england.
Ross, Rick. “Heaven’s Gate.” http://www.rickross.com/groups/heavensgate.
html.

213
Gosling, John. “War of the Worlds Radio Broadcast.” http://www.war-ofthe-
worlds.co.uk/war_worlds_orson_welles_mercury.htm.
Radford, Ben. “The Pokéman Panic of 1997.” http://www.csicop.org/si/
show/pokemon_panic_of_1997.
Lecture 14 
Gould, 
Stephen 
Jay. 
“Nonoverlapping 
Magisteria.” 
http://www.
stephenjaygould.org/library/gould_noma.html.
Russell, Bertrand. “Is There a God?” http://www.cfpf.org.uk/articles/
religion/br/br_god.htm.
Lecture 15 
Squire, Peverill. “Why the 1936 Literary Digest Poll Failed.” http://www.
scribd.com/doc/259298/Why-the-1936-Literary-Digest-Poll-Failed.
LeLorier, Jacques, Geneviève Grégoire, Abdeltif Benhaddad, Julie Lapierre, 
and François Derderian. “Discrepancies between Meta-Analyses and 
Subsequent Large Randomized, Controlled Trials.” New England Journal 
of Medicine 337 (1997): 536–542. http://www.nejm.org/doi/full/10.1056/
NEJM199708213370806.
Novella, Steven. “CCSVI Update.” Science-Based Medicine. http://www.
sciencebasedmedicine.org/index.php/ccsvi-update.
Lecture 16
Hines, Terrence. “Whatever Happened to N-Rays?” http://www.skeptic.com/
eskeptic/10-10-13/#feature.
Wood, Robert W. “The N-Rays.” Nature 70 (1904): 530–531.
Davenas, E., et. al. “Human Basophil Degranulation Triggered by Very 
Dilute Antiserum against IgE.” Nature 333 (1988): 816–818. http://www.
nature.com/nature/journal/v333/n6176/abs/333816a0.html.

214
Additional References by Lecture
Rognstad, Matthew. “Lord Kelvin’s Heat Loss Model as a Failed Scienti¿ c 
Clock.” http://orgs.usd.edu/esci/age/content/failed_scienti¿ c_clocks/kelvin_
cooling.html.
Harter, Richard. “Changing Views of the History of the Earth.” http://www.
talkorigins.org/faqs/geohist.html.
Lecture 17
Clarke, Hulda. The Cure for All Diseases. Chula Vista, CA: New Century 
Press, 1995. 
Pye, Lloyd. Everything You Think You Know Is Wrong. Madeira Beach, FL: 
Adamu Press, 1998. 
Novella, Steven. “Debate with Hollow-Earth Proponent—Neal Adams.” 
NeuroLogica Blog. http://theness.com/neurologicablog/index.php/debate-
with-hallow-earth-proponent-neal-adams.
Lecture 18
Barrett, Stephen. “Iridology Is Nonsense.” http://www.quackwatch.
com/01QuackeryRelatedTopics/iridology.html.
Ernst, E. “Iridology: Not Useful and Potentially Harmful.” Archives of 
Ophthalmology 118 (2000): 120–121. http://archopht.ama-assn.org/cgi/
content/full/118/1/120.
Wolfe, Frankie Avalon. “What Is Iridology?” http://www.healingfeats.com/
whatis.htm.
Hyman, Ray. “The Ganzfeld Experiment: A Critical Appraisal.” Journal of 
Parapsychology 49 (1985): 3–49.
Arnold, Larry. Ablaze. New York: M. Evans and Company, 1995.
Lecture 19
Hargrove, Thomas. “Third of Americans Suspect 9-11 Government 
Conspiracy.” http://www.scrippsnews.com/911poll.

215
Hofstadter, Richard J. “The Paranoid Style in American Politics.” Harper’s 
Magazine, November 1964. http://harpers.org/archive/1964/11/0014706. 
Swami, Viren, and Rebecca Coles. “The Truth Is Out There.” The 
Psychologist 23 no. 7 (2010): 560–563. http://www.thepsychologist.org.
uk/archive/archive_home.cfm?volumeID=23&editionID=190&Article
ID=1694.
Lecture 20 
Alfano, Sean. “Poll: Majority Reject Evolution.” http://www.cbsnews.com/
stories/2005/10/22/opinion/polls/main965223.shtml.
Livescience Staff. “Survey: 61 Percent Agree with Evolution.” http://www.
livescience.com/4787-survey-61-percent-agree-evolution.html.
National Center for Science Education. “New Survey Results on Evolution.” 
http://ncse.com/news/2010/05/new-survey-results-evolution-005542.
Berg, Friedrich Paul. “The German Delousing Chambers.” http://www.
historiography-project.org/jhrchives/v07/v07p-73_Berg.html.
Jewish Virtual Library. “Zyklon B.” http://www.jewishvirtuallibrary.org/
jsource/Holocaust/auschwitz_faq_06.html.
Sutera, Raymond. “The Origin of Whales and the Power of Independent 
Evidence.” http://www.talkorigins.org/features/whales.
Teeter, Tim. “HIV Causes AIDS: Proof Derived from Koch’s Postulates.” 
http://www.aegis.com/pubs/beta/2000/be000403.html.
White, C. “Research on Smoking and Lung Cancer: A Landmark in the 
History of Chronic Disease Epidemiology.” http://www.ncbi.nlm.nih.gov/
pmc/articles/PMC2589239.
Lecture 21
Nigeria—The 419 Coalition Website. “The Nigerian Scam De¿ ned.” http://
home.rmci.net/alphae/419coal.

216
Additional References by Lecture
Hoax-Slayer. “False Story Claims 450 Gaza Grooms Wed Girls under Ten 
in Mass Muslim Marriage.” http://www.hoax-slayer.com/mass-muslim-
marriage.shtml.
Kenji López-Alt, J. “The Burger Lab: Revisiting the Myth of the 12-Year 
Old McDonald’s Burger That Just Won’t Rot (Testing Results!).” http://aht.
seriouseats.com/archives/2010/11/the-burger-lab-revisiting-the-myth-of-the-
12-year-old-burger-testing-results.html.
Emory, David. “Aspartame Warning.” http://urbanlegends.about.com/
library/blasp.htm.
Magnuson, B. A., G. A. Burdock, J. Doull, R. M. Kroes, G. M. Marsh, M. 
W. Pariza, P. S. Spencer, W. J. Waddell, R. Walker, and G. M. Williams. 
“Aspartame: A Safety Evaluation Based on Current Use Levels, Regulations, 
and Toxicological and Epidemiological Studies.” Critical Reviews in 
Toxicology 37, no. 8 (2007): 629–727.
Manning, Kenneth C., and David  E. Sprott. “Price Endings, Left-Digit 
Effects, and Choice.” http://www.jstor.org/pss/10.1086/597215.
Lacetera, Nicola, Devin G. Pope, and Justin R. Sydno. “Heuristic Thinking 
and Limited Attention in the Car Market.” http://www.rotman.utoronto.ca/
facbios/¿ le/Lacetera_Pope_Sydnor_Final.pdf.
Lecture 22
Starman, James S., F. Keith Gettys, Jason A. Capo, James E. Fleischli, 
James H. Norton, and Madhay A. Karunakar. “Quality and Content of 
Internet-Based Information for Ten Common Orthopaedic Sports Medicine 
Diagnoses.” The Journal of Bone & Joint Surgery 92, no. 7 (2010): 1912–
1618.
Goldacre, Ben. “Pixie Dust Helps Man Grow New Finger.” http://www.
badscience.net/2008/05/¿ nger-bullshit.
Zamboni, P., R. Galeotti, E. Menegatti, A. M. Malagoni, G. Tacconi, S. 
Dall’Ara, I. Bartolomei, and F. Salvi. “Chronic Cerebrospinal Venous 

217
Insuf¿ ciency in Patients with Multiple Sclerosis.” Journal of Neurology, 
Neurosurgery, and Psychiatry 80, no. 4 (2009): 392–399.
Reich, Eugenie Samuel. “Faster-Than-Light Neutrinos Face Time Trial.” 
http://www.nature.com/news/2011/111005/full/news.2011.575.html.
Miller, Jon D. “The Measurement of Civic Scienti¿ c Literacy.” http://pus.
sagepub.com/content/7/3/203.short?rss=1&ssource=mfc.
Miller, Jon D. “The Development of Civic Scienti¿ c Literacy in the United 
States.” http://meetings.aps.org/Meeting/MAR10/Event/125280.
Harris, Sam. The Moral Landscape : How Sci ence Can Determine Human 
Values. New York: Free Press, 2010.
Lecture 23
Hauck, D. W. “Isaac Newton.” http://www.alchemylab.com/isaac_newton.
htm.
Barrett, Stephen. “The Dark Side of Linus Pauling’s Legacy” http://www.
quackwatch.com/01QuackeryRelatedTopics/pauling.html.
Pinker, Steven. “Steven Pinker Forecasts the Future.” http://www.
newscientist.com/article/mg19225780.088-steven-pinker-forecasts-the-
future.html.
Kruger, J., and D. Dunning. “Unskilled and Unaware of It: How Dif¿ culties in 
Recognizing One’s Own Incompetence Lead to InÀ ated Self-Assessments.” 
Journal of Personality and Social Psychology 77, no. 6 (1999): 1121–1134.
Lecture 24
Gawande, Atul. The Checklist Manifes to: How to Get Things Right. New 
York: Metropolitical Books, 2009.

Bibliography
218
Bibliography
 Ariely, Dan. Predictably Irrational, Revised and Expanded Edition: The 
Hidden Forces That Shape Our Decisions. New York: Harper Perennial, 
2010. Ariely analyzes the process of decision making and shows how people 
are fundamentally irrational—but in ways that are predictable—from the 
point of view of psychology and neuroscience. 
Ashmore, Malcolm. “The Theatre of the Blind: Starring a Promethean 
Prankster, a Phony Phenomenon, a Prism, a Pocket, and a Piece of Wood.” 
Social Studies of Science 23, no. 1 (1993): 67 –106. http://www.gesctm.
unal.edu.co/CMS/Docentes/Adjuntos/099037209_20080313_054242_
theatre%20of%20the%20blind.pdf. This article discusses the events 
that followed from the false discovery of N-rays by French physicist
René Blondlot.
Asimov, Isaac. Asimov’s New Guide to Science. London: Penguin Books, 
1993. Asimov explains the process of science using classic historical 
examples, offering the human dimension of how science works. 
Bartholomew, Robert E. Panic Attacks. Gloucestershire: Sutton Publishing, 
2004. A review of media-induced mass delusions, such as The War of the 
Worlds incident provoked by Orson Welles. 
———. Little Green Men, Meowing Nuns, and Head-Hunting Panics: A 
Study of Mass Psychogenic Illnesses and Social Delusion. Jefferson, NC: 
McFarland, 2001. Bartholomew reviews the various types of mass delusion 
with many fascinating and instructive historical examples.
Blackmore, Susan. Consciousness: A Very Short Introduction. New 
York: Oxford University Press, 2005. Blackmore explores the concept of 
consciousness and how it is constructed and altered by the brain.

219
Brunvand, Jan Harold, Robert Loren Fleming, and Robert F. Boyd. The Big 
Book of Urban Legends.  N ew York: Paradox Press, 1995. A description of 
200 bizarre stories that are all purported to be true.
Burch¿ eld, Joe D. Lord Kelvin and the Age of the Earth. Chicago: University 
of Chicago Press, 1990. A thorough exploration of Lord Kelvin’s foray into 
geology using thermodynamics and the controversy and folly that resulted. 
Burt on, Robert. On Being Certain: Believing You Are Right Even When 
You’re Not. New York: St. Martin’s Grif¿ n, 2009. Burton is a neuroscientist 
who explores the phenomenon of knowing something, or feeling certain, 
showing how it is just another À awed construction of the brain. 
Chabris, Christopher, and Daniel Simons. The Invisible Gorilla: How Our 
Intuitions Deceive Us. New York: Broadway Books, 2009. This book goes 
beyond inattentional blindness (the source of the title) to discuss a variety of 
cognitive illusions, how they affect our perceptions and beliefs, and how to 
mitigate such effects. 
Dewdney, A. K. 200% of Nothing: An Eye-Opening Tour Through the Twists 
and Turns of Math Abuse and Innumeracy. Hoboken, NJ: John Wiley & 
Sons, Inc., 1996. Dewdney gives an in-depth discussion of innumeracy with 
plenty of examples from politics, marketing, and everyday life. 
Evans, Jonathan. Bias in Human Reasoning: Causes and Consequences. 
London: Psychology Press, 1990. A thorough review of the psychological 
literature that establishes various biases in human reasoning. 
Fitzpatrick, Robert, and Joyce K. Reynolds. False Pro¿ ts: Financial and 
Spiritual Deliverance in Multi-Level Marketing and Pyramid Schemes. 
Charlotte: Herald Press, 1997. An in-depth look at the multilevel
marketing industry. 
Flew, Antony. How to Think Straight: An Introduction to Critical Reasoning. 
Amherst: Prometheus Books, 1998. Flew clearly reviews and explains the 
tools of logical thinking.

220
Bibliography
Freedman, David H. Wrong: Why Experts Keep Failing Us—And How to 
Know When Not to Trust Them. New York: Little, Brown and Company, 
2010. Freedman gives a harsh but realistic look at the advice of experts, 
showing how they are often wrong, but he also concludes with some rules 
regarding how to avoid being deceived. 
Gardner, Dan. Future Babble : Why Expert Predictions Fail—And Why We 
Believe Them Anyway. New York: Dutton Adult, 2011. Gardner explains the 
nature of expertise and the different approaches that experts take to make 
forecasts. He demonstrates that forecasting future trends is generally dismal, 
but some approaches work better than others. 
Gardner, Martin. Fads and Fallacies in the Name of Science. Mineola: Dover 
Publications, 1957. Gardner practically created a new genre of non¿ ction 
with this book by debunking popular pseudoscience. Many of the examples 
are still relevant today, and even the ones that are historical show how the 
features of pseudoscience never change. 
———. Science: Good, Bad, and Bogus. Amherst: Prometheus Books, 1989. 
A classic exploration of science and pseudoscience by one of the founders of 
modern skepticism. 
Gazzaniga, Michael S. The Mind’s Past. London: University of California 
Press, 1998. Gazzaniga is a cognitive neuroscientist who describes how the 
brain constructs a narrative of the past. 
Gilovich, Thomas. How We Know What Isn’t So. New York: The Free Press, 
1991. Gilovich illustrates many psychological tendencies that lead people 
astray with everyday examples that most readers will easily relate to. 
Goldacre, Ben. Bad Science. New York: Faber and Faber, 2010. Goldacre 
discusses many examples of media distortion of science news and gives a 
guide to evaluating science news items. 
Grant, John. Denying Science. Amherst: Prometheus Books, 2011. Grant is a 
popular science writer who takes on current issues of science that are denied 
as such because of ideological or political motivation. 

221
Hallinan, Jos eph T. Why We Make Mistakes: How We Look Without Seeing, 
Forget Things in Seconds, and Are All Pretty Sure We Are Way above 
Average. New York: Broadway Books, 2009. Hallinan explores the various 
factors that make people extremely error prone, with wide-ranging examples. 
Hazen, Robert M., and James Tre¿ l. Science Matters: Achieving Scienti¿ c 
Literacy. New York: Anchor, 1990. An excellent series of essays explaining 
scienti¿ c concepts that are likely to crop up in the news. 
Hines, Terence. Pseudoscience and the Paranormal. Amherst: Prometheus 
Books, 2002. Hines takes a no-nonsense scienti¿ c look at paranormal belief 
in popular culture.
Hood, Bruce. Supersense. New York: Harper Collins, 2008. In this book, 
Hood describes how our innate sense of essence and the supernatural leads 
us to accept magical beliefs.
Johnson, Stev en. The Ghost Map: The Story of London’s Most Terrifying 
Epidemic—And How It Changed Science, Cities, and the Modern World. 
New York: Riverhead Trade, 2007. A fascinating review of a historical 
epidemic and the emergence of scienti¿ c investigation that eventually solved 
the mystery. 
Kahneman, Daniel. Thinking, Fast and Slow. New York: Farrar, Straus and 
Giroux, 2011. Kahneman draws on his years of research and expertise to 
explore metacognition—the processes that affect our judgments, intuition, 
feelings, and decisions. 
Kida, Thomas E. Don’t Believe Everything You Think: The 6 Basic Mistakes 
We Make in Thinking. Amherst: Prometheus Books, 2006. Kida offers 
a very accessible discussion of the most common biases and errors in
human reasoning. 
Klass, Philip J. The Real Roswell Crashed-Saucer Coverup. Amherst: 
Prometheus Books, 1997. Klass, an expert on UFO mythology, explores the 
modern myth of Roswell and explains how a crashed weather balloon was 
turned into an alien icon. 

222
Bibliography
Klemke, E. D., Robert Hollinger, and David Wyss Rudge, eds. Introductory 
Read ings in the Philosophy of Science. Amherst: Prometheus, 1988. The 
standard introductory text to the philosophy of science; better suited for a 
classroom than casual reading. 
Macknik, Stephen L., Susana Martinez-Conde, and Sandra Blakeslee. 
Sleights of Mind: What the Neuroscience of Magic Reveals about Our 
Everyday Deceptions. New York: Henry Hold and Company, 2010. In this 
book, Macknik and Martinez-Conde collaborate with magicians to explore 
the neuroscience behind the deception techniques of stage magic. 
McRaney, David. Y ou Are Not So Smart: Why You Have Too Many Friends 
on Facebook, Why Your Memory Is Mostly Fiction, and 46 Other Ways 
You’re Deluding Yourself. New York: Gotham Books, 2011. McRaney 
gives a fairly extensive discussion of cognitive biases, logical fallacies, and 
heuristics and how they lead us astray. 
Mlodinow, Leonard. The Drunkard’s Walk: How Randomness Rules Our 
Lives. New York: Vintage, 2009. Mlodinow describes the counterintuitive 
behavior of random systems and shows how to think about randomness 
rather than falling for the illusions they generate. 
Mulhearn, Tony. “The Psychology of  the Witch-Hunt.” Socialism Today, 
October 2000. http://www.socialismtoday.org/51/crucible.html. An analysis 
of Arthur Miller’s The Crucible and the historic parallels that can be drawn.
Neimark, Jill. “It’s Magical, It’s Malleable, It’s ... Memory.” Psychology 
Today, January 1, 1995. http://www.psychologytoday.com/articles/199501/
its-magical-its-malleable-its-memory. A brief discussion of the malleability 
of memory. 
Newberg, Andrew, and Mark Robert Waldman. Born to Believe. New York: 
Free Press, 2007. Newberg and Waldman explain the biological basis for 
belief—what brain processes contribute to and inÀ uence what we believe. 

223
Paulos, John Allen. Innumeracy: Mathematical Illiteracy and Its 
Consequences. New York: Vintage, 1990. An entertaining treatment of how 
ignorance of math and statistics leads to erroneous beliefs. 
Pigliucci, Massimo. Nonsense on Stilts: How to Tell Science from Bunk. 
Chicago: University of C hicago Press, 2010. An exploration of science and 
pseudoscience that involves how to tell the difference between the two and 
why people often believe the latter over the former. 
Polidoro, Massimo. Secrets of the Psychics. Amherst: Prometheus Books, 
2003. Polidoro discusses his many investigations into psychics, mediums, 
and other paranormal claims. 
Posner, Gerald. Case Closed. New York: Anchor Books, 1994. The de¿ nitive 
book on the John F. Kennedy assassination.
Randi, James. Flim-Flam! Psychics, ESP , Unicorns, and Other Delusions. 
Amherst: Prometheus Books, 1988. A classic exploration of science and 
pseudoscience by a founder of modern skepticism. 
———. The Mask of Nostradamus. Amherst: Prometheus Books, 
1993. Randi gives the de¿ nitive skeptical look at the famous astrologer 
Nostradamus. 
Sagan, Carl. The Demon-Haunted World. New York: The Random 
House Publishing Company, 1996. In this book, Sagan gives an excellent 
introduction to critical thinking and the skeptical outlook and discusses why 
it is important to individuals and society. 
Schick, Jr., Theodore, and Lewis Vaughn. How to Think about Weird Things: 
Critical Thinking for a New Age. Boston: McGraw-Hill, 2010. This book 
serves as a quick guide to how to evaluate extraordinary claims. 
Seckel, Al. The Great Book of Optical Illusions. Ontario: FireÀ y Books, 
2005. As the title implies, this is a book full of hundreds of optical illusions 
of many types. 

224
Bibliography
Shermer, Michael. How We Believe. New York: Holt Paperbacks, 2003. 
Shermer explores religious belief as a sociological and neurological 
phenomenon in a way that is accessible and enlightening to both believers 
and nonbelievers. 
———. The Believing Brain: From Ghosts and Gods to Politics and 
Conspiracies—How We Construct Beliefs and Reinforce Them as Truths. 
New York: Times Books, 2011. Shermer reviews much of the literature from 
neuroscience and social science dealing with belief, including how irrational 
beliefs are formed and reinforced. 
———. The Borderlands of Science: Where Sense Meets Nonsense. New 
York: Oxford University Press, 2001. Shermer explores the gray zone 
between science and pseudoscience, illustrating those features that separate 
the two. 
———. Why People Believe Weird Things. New York: Henry Holt and 
Company, 1997. Shermer presents an exploration into the cognitive failings 
that lead many people to believe things that are not only likely not true but 
are strange beliefs. 
Shreve, Tara L. The Fallibility of Memory in Eyewitness Testimony: The 
Effects of Misinformation on Memory. Winter Park, FL: Rollins College, 
1998. This book presents the various effects on memory that cause people to 
remember events incorrectly, especially in regards to eyewitness testimony.
Singer, Margaret, and Janja Lalich. Cults in Our Midst: The Hidden Menace 
in Our Everyday Lives. San Francisco: Jossey-Bass, 1995. A classic book on 
cults, including the psychological phenomena behind their existence. 
Specter, Michael. Denialism: How Irrational Thinking Hinders Scienti¿ c 
Progress, Harms the Planet, and Threatens Our Lives. London: Penguin 
Press, 2009. Specter explores how America has turned away from science in 
regards to many important issues. 
Spellman, Frank R., and Joni Price-Ba yer. In Defense of Science: Why 
Scienti¿ c Literacy Matters. Lanham: Government Institutes, 2011. The 

225
authors review how science works and why it is important, including the 
need for scienti¿ c literacy in the public. They also review some basic 
scienti¿ c concepts. 
Taleb, Nassim Nicholas. Fooled by Randomness: The Hidden Role of Chance 
in Life and in the Markets. New York: W. W. Norton, 2001. A mathematician 
describes randomness in business and everyday life and how we mistake 
randomness for luck or skill.
Taper, Mark L., and Subhash R. Lele. The Nature of Scienti¿ c Evidence: 
Statistical, Philosophical, and Empirical Considerations. Chicago: 
University of Chicago Press, 2004. The authors explore the statistical 
relationship between data and theory, discussing the nature of evidence
in science. 
Tarski, Alfred. Introduction to Logic. New York: Oxford University Press, 
1994. A classic introduction to deductive logic.
Tavris, Carol, and Elliot Aronson. Mistakes Were Made (But Not by Me): 
Why We Justify Foolish Beliefs, Bad Decisions, and Hurtful Acts. Orlando: 
Harcourt Books, 2007. An excellent exploration of self-deception and why 
we rationalize our errors rather than admitting them. 
Vos Savant, Marilyn. The Power of Logical Thinking. New York: St. 
Martin’s Grif¿ n, 1997. Vos Savant explores the uses and misuses of statistics 
in politics and argument. 
Wilson, Edward O. Consilience: The Unity of Knowledge. New York: First 
Vintage Books, 1999. A classic popular science book. Wilson shows how 
the various disciplines of science work together to weave a cohesive picture
of nature. 
Winograd, Eugene, and Ulric Neisser, eds. Affect and Accuracy in Recall: 
Studies of “Flashbulb” Memories. New York: Cambridge University Press, 
1992. A detailed discussion of À ashbulb memory; an excellent review of the 
neuroscience of memory in general. 

226
Bibliography
Wiseman, Richard. 59 Seconds. New York: Knopf, 2009. Wiseman combed 
the psychological literature looking for practical techniques that are science 
based while countering the misinformation common in the self-help industry. 
———. Paranormality: Why We See What Isn’t There. London: Spin 
Solutions Ltd., 2011. Wiseman uses paranormal beliefs to illustrate the 
psychological and cognitive processes that lead people to believe the 
unbelievable. 
Youngson, Robert. Scienti¿ c Blunders. New York: Carroll & Graf Publishers, 
1998. This book contain a long list of historical episodes in which scientists 
were spectacularly wrong and includes a brief discussion at the end about 
pseudoscience. 
Internet Resources
“Argument.” Intern e t Encyclopedia of Philosophy. http://www.iep.utm.edu/
argument.
Carroll, Robert Todd. “A Short History of Psi Research.” The Skeptic’s 
Dictionary. http://www.skepdic.com/essays/psihistory.html.
“Con¿ rmation Bias.” The Skeptic’s Dictionary. http://www.skepdic.com/
con¿ rmbias.html.
DeRose, Keith. What Is Epistemology? A Brief Introduction to the Topic. 
http://pantheon.yale.edu/~kd47/What-Is-Epistemology.htm.
Ellis, Keith M. The Monty Hall Problem. http://montyhallproblem.com.
Fallacy Files. http://www.fallacy¿ les.org.
Fineman, Mark. “Sightings: UFOs, and Visual Perception.” The NESS. 
http://www.theness.com/index.php/sightings-ufos-and-visual-perception.
Goertzel, Ted. “The Conspiracy Meme.” CSI. http://www.csicop.org/si/
show/the_conspiracy_meme.

227
Goldacre, Ben. “Why Don’t Journalists Link to Primary Sources?” Bad 
Science. 
http://www.badscience.net/2011/03/why-dont-journalists-link-to-
primary-sources.
Nickell, Joe. “Ghost Hunters.” CSI. http://www.csicop.org/si/show/ghost_
hunters.
Novella, Steven. “A Neurological Approach to Skepticism.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/a-neurological-
approach-to-skepticism.
Novella, Steven. “Anatomy of Pseudoscience.” The NESS. http://www.
theness.com/index.php/anatomy-of-pseudoscience.
Novella, Steven. “Anomaly Hunting.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/anomaly-hunting.
Novella, Steven. “Aspartame Safety and Internet Urban Legends.” 
NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/
aspartame-safety-and-internet-urban-legends/.
Novella, Steven. “Bem’s Psi Research.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/bems-psi-research.
Novella, Steven. “Beware the Nobel Laureate Argument from Authority.” 
NeuroLogica Blog. http://theness.com/neurologicablog/index.php/beware-
the-nobel-laureate-argument-from-authority.
Novella, Steven. “Body Snatchers, Phantom Limbs, and Alien Hands.” 
NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/body-
snatchers-phantom-limbs-and-alien-hands.
Novella, Steven. “CCSVI—The Importance of Replication.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/ccsvi-the-importance-
of-replication.

228
Bibliography
Novella, Steven. “Conspiracy Thinking: Skepticism’s Evil Twin.” 
NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/
conspiracy-thinking-skepticisms-evil-twin.
Novella, Steven. “Data Mining—Adventure in Pattern Recognition.” 
NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/data-
mining-adventures-in-pattern-recognition.
Novella, Steven. “Evidence in Medicine: Correlation and Causation.” 
Science-Based Medicine. http://www.sciencebasedmedicine.org/index.php/
evidence-in-medicine-correlation-and-causation.
Novella, Steven. “Evidence in Medicine: Experimental Studies.” Science-
Based Medicine. http://www.sciencebasedmedicine.org/index.php/evidence-
in-medicine-experimental-studies/.
Novella, Steven. “Ghost Hunting Science versus Pseudoscience.”NeuroLogica 
Blog. http://theness.com/neurologicablog/index.php/ghost-hunting-science-
vs-pseudoscience.
Novella, Steven. “Holmesian Deduction.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/holmesian-deduction.
Novella, Steven. “How Gullible Are You?” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/how-gullible-are-you.
Novella, Steven. “How to Argue.” The NESS. http://www.theness.com/
index.php/how-to-argue.
Novella, Steven. “Hyperactive Agency Detection.” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/hyperactive-agency-detection.
Novella, Steven. “Memory.” NeuroLogica Blog. http://theness.com/
neurologicablog/index.php/memory.
Novella, Steven. “More on God of the Gaps.” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/more-on-god-of-the-gaps.

229
Novella, Steven. “New Scientist on Miracles.” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/new-scientist-on-miracles.
Novella, Steven. “Pareidolia in the Brain.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/pareidolia-in-the-brain.
Novella, Steven. “Science and Faith.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/science-and-faith.
Novella, Steven. “Science Education and Literacy in the U.S.” NeuroLogica 
Blog. http://theness.com/neurologicablog/index.php/science-education-and-
literacy-in-the-us.
Novella, Steven. “Scienti¿ c Proof and Evolution Denial.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/scienti¿ c-proof-and-
evolution-denial.
Novella, Steven. “Skepticism and Denial.” The NESS. http://www.theness.
com/index.php/skepticism-and-denial.
Novella, Steven. “Spontaneous Human Combustion.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/spontaneous-human-
combustion.
Novella, Steven. “The Burger ‘Experiments.’” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/the-burger-experiments.
Novella, Steven. “The Context of Anecdotes and Anomalies.” NeuroLogica 
Blog. 
http://theness.com/neurologicablog/index.php/the-context-of-
anecdotes-and-anomalies.
Novella, Steven. “The Decline Effect.” NeuroLogica Blog. http://theness.
com/neurologicablog/index.php/the-decline-effect.
Novella, Steven. “The Internet and Skepticism.” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/the-internet-and-skepticism.

230
Bibliography
Novella, Steven. “The Nature of Consensus.” NeuroLogica Blog. http://
theness.com/neurologicablog/index.php/the-nature-of-consensus.
Novella, Steven. “The Spinning Girl Illusion Revisited.” NeuroLogica Blog. 
http://theness.com/neurologicablog/index.php/the-spinning-girl-illusion-
revisited. 
Novella, Steven, and Tara Smith. “HIV Denial in the Internet Era.” PLoS 
Medicine. 
http://www.plosmedicine.org/article/info:doi/10.1371/journal.
pmed.0040256.
Oreskes, 
Naomi. 
“Merchants 
of 
Doubt.” 
Cosmos. 
http://www.
cosmosmagazine.com/node/4376/full.
Stenger, Vic. “The Phantom of Free Energy.” Skeptical Briefs. http://www.
colorado.edu/philosophy/vstenger/Briefs/phantom.html.
Sawicki, Mikolaj. “Innumeracy.” Bad Physics. http://www.jal.cc.il.
us/~mikolajsawicki/ex_innum.html.
Snopes.com. http://www.snopes.com/info/aboutus.asp.
Tokuno, Hajime. “Holocaust Denial.” The NESS. http://www.theness.com/
index.php/holocaust-denial.


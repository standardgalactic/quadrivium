Information	  and	  coding	  principles	  

Information	  and	  coding	  principles	  
•  De3ining	  entropy	  and	  information	  
	  
•  Computing	  information	  for	  neural	  spike	  trains	  
	  
•  What	  can	  information	  tell	  us	  about	  coding?	  

How	  good	  is	  my	  code?	  

How	  good	  is	  my	  code?	  

Surprise!	  
How	  good	  is	  my	  code?	  

Information	  
P(1)	  =	  p	  
P(0)	  =	  1	  -­‐	  p	  
informa6on(1)	  =	  -­‐	  log2	  p	  
informa6on(0)	  =	  -­‐	  log2	  (1-­‐p)	  

Information	  
Each	  bit	  of	  information	  
speci3ies	  location	  by	  an	  	  
additional	  factor	  of	  2.	  

What	  is	  entropy?	  
x	  
P(x)	  
Entropy	  
	  =	  average	  informa6on	  
	  
	  
	  
	  =	  -­‐	  Σ	  pi	  log2	  pi	  
=	  -­‐	  ∫ dx	  p(x)	  log2	  p(x)	  
Units	  are	  bits	  
x	  
P(x)	  
i	  

Maximizing	  the	  entropy	  
x	  
P(x)	  
Entropy	  =	  -­‐	  Σ	  pi	  log2	  pi	  

Maximizing	  the	  entropy	  
x	  
P(x)	  
Entropy	  =	  -­‐	  Σ	  pi	  log2	  pi	  

Generally	  P(x)	  is	  not	  
uniform…	  but	  it	  would	  
be	  best	  for	  you	  if	  it	  were!	  	
Maximum	  entropy	  squash	  

Back	  to	  our	  spike	  code:	  how	  about	  the	  stimulus?	  
0	  

How	  about	  the	  stimulus?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
+	  
-­‐	  
+	  
-­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	  
r	  
s	  

How	  about	  the	  stimulus?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
+	  
-­‐	  
+	  
-­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	   -­‐	  
-­‐	  
+	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	  
r	  
s	  

How	  about	  the	  stimulus?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
-­‐	  
+	  
+	  
-­‐	  
-­‐	  
-­‐	   +	   -­‐	   -­‐	   -­‐	   +	   -­‐	  
-­‐	   +	  
+	  
+	   -­‐	   -­‐	   -­‐	   +	   -­‐	   -­‐	  
-­‐	  
-­‐	   +	   -­‐	  
r	  
s	  

How	  much	  of	  the	  variability	  in	  r	  is	  encoding	  s?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
0	  
P(r+|+)	  =	  1-­‐q	  	  
P(r-­‐|+)	  =	  	  q	  
P(r-­‐|-­‐)	  =	  q	  	  
P(r+|-­‐)	  =	  1-­‐q	  	  
+	  
-­‐	  
+	  
-­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	   -­‐	  
+	  
+	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	  
r	  
s	  

How	  much	  of	  the	  variability	  in	  r	  is	  encoding	  s?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
0	  
P(r+|+)	  =	  1-­‐q	  	  
P(r-­‐|+)	  =	  	  q	  	  
P(r-­‐|-­‐)	  =	  q	  	  	  
P(r+|-­‐)	  =	  1-­‐q	  
H	  =	  -­‐	  Σ	  pi	  log2	  pi	  
i=1	  
2	  

How	  much	  of	  the	  variability	  in	  r	  is	  encoding	  s?	  
0	  
S	  =	  +	  
S	  =	  -­‐	  
0	  
P(r+|+)	  =	  1-­‐q	  	  
P(r-­‐|+)	  =	  q	  	  
P(r-­‐|-­‐)	  =	  q	  	  	  
P(r+|-­‐)	  =	  1-­‐q	  	  
Total	  entropy:	  	  H[R]	  =	  -­‐	  P(r+)	  log	  P(r+)	  -­‐	  P(r-­‐)	  log	  P(r-­‐)	  	  
Noise	  entropy:	  	  H[R|+]	  =	  -­‐	  q	  log	  q	  -­‐	  (1-­‐q)	  log	  (1-­‐q)	  

Mutual	  information	  
+	  
-­‐	  
+	  
-­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	   -­‐	  
+	  
+	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	  
r	  
s	  
The	  amount	  of	  entropy	  that	  is	  used	  in	  coding	  the	  s6mulus	  
MI(S,R)	  =	  Total	  entropy	  –	  average	  noise	  entropy	  
MI	  =	  -­‐	  Σ	  p(r)	  log2	  p(r)	  -­‐	  Σ	  p(s)	  [-­‐	  Σ p(r|s)	  log2	  p(r|s)]	
	  
r	  
s	  
r	  

Entropy	  
Information	  
Entropy	  and	  information	  
Error	  probability	  
Mutual	  information	  

Response	  is	  unrelated	  to	  stimulus
	  	  
•  What	  is	  p(r|s)?	  	  
•  What	  is	  the	  MI	  ?	  
	  
	  	  
	  
	  
	  
Response	  is	  perfectly	  predicted	  by	  stimulus	  
Mutual	  information	  you	  can	  calculate	  in	  your	  head	  
-­‐	  
+	  
+	  
-­‐	  
-­‐	  
-­‐	   +	   -­‐	   -­‐	   -­‐	   +	   -­‐	  
-­‐	   +	  
+	  
+	   -­‐	   -­‐	   -­‐	   +	   -­‐	   -­‐	  
-­‐	  
-­‐	   +	   -­‐	  
r	  
s	  
1.	  
2.	  
+	  
-­‐	  
+	  
-­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	   -­‐	  
-­‐	  
-­‐	   -­‐	   -­‐	  
r	  
s	  

Entropy	  and	  information	  in	  continuous	  variables	  
Fairhall,	  Shea-­‐Brown,	  Barreiro	  (2012)	  
r	  
r	  

Mutual	  information	  measures	  relationships	  
I(R,S)	  =	  -­‐	  Σ	  p(r)	  log2	  p(r)	  -­‐	  Σ	  p(s)	  [	  Σ p(r|s)	  log2	  p(r|s)]	  
	  
	  
	  =	  H[R]	  -­‐	  Σ	  p(s)	  H[R|s]	  	  	  
	  
r	  
s	  
r	  
Information	  quanti3ies	  how	  independent	  R	  and	  S	  are:	  
	  
	  
	  	  	  	  I(S;R)	  =	  DKL	  [P(R,S),	  P(R)P(S)]	  
s	  

	  	  	  	  	   	  I(S,R)	  =	  H[R]	  –	  Σs	  P(s)	  H[R|s]	  .	  
	  
	  I(S,R)	  =	  H[S]	  –	  Σr	  P(r)	  H[S|r]	  .	  
Mutual	  information	  
Information	  quanti3ies	  how	  independent	  R	  and	  S	  are:	  
	  
	  
	  	  	  	  	  	  	  	  I(S,R)	  =	  DKL	  [P(R,S),	  P(R)P(S)]	  

Grandma’s famous mutual information recipe	  
	  
Take one stimulus s and repeat many times to obtain 
P(R|s).	  	  
Compute variability due to noise: noise entropy	  H[R|s]	  
Repeat for all s and average: Σs	  P(s)	  H[R|s)].	  
Compute	  P(R)	  =	  Σs	  P(s)	  P(R|s)	  and the total entropy H[R]	  
Mutual	  information	  is	  the	  difference	  between	  	  
the	  total	  response	  entropy	  and	  	  the	  mean	  noise	  entropy:	  	  
	  
	  	  	  	   	  
	  	  	  	  	  	  	  	  	  	  	  I(S;R)	  =	  H[R]	  –	  Σs	  P(s)	  H[R|s)]	  .	  
	  
Calculating	  mutual	  information	  


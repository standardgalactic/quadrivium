
Software Architecture
visual lecture notes
Cesare Pautasso
This book is for sale at
https://leanpub.com/software-architecture
This version was published on June 1, 2021.
We plan to update the book from time to time, you may check on LeanPub if you still have
the latest version.
©2020–2021 Cesare Pautasso

Software
Architecture
visual lecture notes
Cesare Pautasso


Preface
This book collects the revised and illustrated notes of the Software Architecture lecture
of the Master in Software and Data Engineering held at the Software Institute at USI
Lugano, Switzerland during the Spring of 2020.
The book includes the script for the following lectures:
1. Introduction
2. Quality Attributes
3. Definitions
4. Modeling Software Architecture
5. Modularity and Components
6. Reusability and Interfaces
7. Composability and Connectors
8. Compatibility and Coupling
9. Deployability, Portability and Containers
10. Scalability
11. Availability and Services
12. Flexibility and Microservices
The lecture is designed for master students attending the second semester of the MSDE
master program with a background in data management, software engineering, software
design and modeling, domain specific languages and programming styles.
ii

Acknowledgements
I started to teach Software Architecture and Design back in the Spring of 2008, encour-
aged by Mehdi Jazayeri, the founding Dean of the USI Faculty of Informatics, who shared
with me a preview of Software Architecture: Foundations, Theory and Practice by Richard
N. Taylor, Nenad Medvidovic, Eric Dashofy, to whom I will always be in debt.
I would not have been successful in teaching Software Architecture without the in-
sightful experience and practical advice on architectural decision making shared by Olaf
Zimmermann, whom we had the pleasure of welcoming to Lugano as a guest speaker in
many editions of the lecture. It has been really great to harvest Microservice API Patterns
for the past four years together with Olaf as well as Mirko Stocker, Uwe Zdun and Daniel
Lübke.
During later revisions of the lecture I started to recommend to my students to read and
study Just Enough Software Architecture: A Risk-Driven Approach by George H. Fairbanks
(2010) as well as the more recent Design It!: From Programmer to Software Architect by
Michael Keeling (2017).
It is still a challenge to model something as complex as a software architecture. But
since Architectural Blueprints—The “4+1” View Model of Software Architecture were pro-
posed by Philippe Kruchten (1995), we have come a long way with, for example, Simon
Brown’s C4 model (context, containers, components and code) which as part of this lec-
ture has been augmented with connectors (C5).
The lecture also includes valuable concepts, good ideas and their powerful visualiza-
tions borrowed from: Joshua Bloch, Grady Booch, Jorge Luis Borges, Eric Brewer, Pe-
ter Cripps, David Farley, Martin Fowler, Luke Hohmann, Gregor Hohpe, Jez Humble,
Ralph Johnson, James Lewis, Rohan McAdam, M. Douglas McIlroy, Bertrand Meyer, David
Parnas, Jon Postel, Eberhardt Rechtin, John Reekie, Nick Rozanski, Mary Shaw, Ian Som-
merville, Michael T. Nygard, Will Tracz, Stewart Brands, Werner Vogels, Niklaus Wirth,
Eoin Woods.
The lecture has achieved his current shape also thanks to the feedback of many gener-
ations of students attending our Master of Software and Data Engineering and the gener-
ous support of my past, present and future teaching assistants (in chronological order):
Romain Robbes, Alessio Gambi and Danilo Ansaloni, Marcin Nowak, Daniele Bonetta,
Masiar Babazadeh, Andrea Gallidabino, Jevgenija Pantiuchina, and Souhaila Serbout. A
big thank you to everyone!
Since 2013, the entire lecture has been delivered through ASQ, a tool for Web-based
interactive presentations developed by Vasileios Triglianos, which now can also auto-
matically generate books like the one you are reading.
Cesare Pautasso
Lugano, Switzerland
iii

Contents
Preface
ii
Acknowledgements
iii
Introduction
1
De Architectura . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
The Art and Science of Building . . . . . . . . . . . . . . . . . . . . . . . . .
3
Foundation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
Closed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
Open . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
Forces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
When do you need an architect?
. . . . . . . . . . . . . . . . . . . . . . . . . . . 13
How Large? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
Software Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
Lecture Context
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
Why Software Architecture? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
Hiding Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
Abstraction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
Communication
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Quality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
Change
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
Quality Attributes
35
Quality
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
Functional, Extra-Functional Qualities . . . . . . . . . . . . . . . . . . . . . 37
Internal vs. External . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
Static vs. Dynamic . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
Meta-Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
Quality Attributes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Design Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Feasibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
Affordability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
Slack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
Time to Market . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47
Modularity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
Reusability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
Design Consistency
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
Simplicity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
Clarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
iv

Contents
Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
Composability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
Deployability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
Normal Operation Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
Performance
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
Scalability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
Capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
Usability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65
Ease of Support . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
Serviceability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
Visibility
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69
Dependability Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70
Reliability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
Availability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
Security Qualities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77
Defensibility, Survivability
. . . . . . . . . . . . . . . . . . . . . . . . . . . 78
Privacy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
Change Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
Flexibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82
Configurability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83
Customizability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85
Resilience, Adaptability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 86
Extensibility, Modifiability
. . . . . . . . . . . . . . . . . . . . . . . . . . . 87
Elasticity
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88
Compatibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
Portability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 93
Interoperability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
Ease of Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . 96
Long Term Qualities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
Durability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
Maintainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
Types of Maintenance . . . . . . . . . . . . . . . . . . . . . . . . . . 103
Sustainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
Definitions
106
Who is a software architect?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106
Functional Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
Cross-Functional Organization . . . . . . . . . . . . . . . . . . . . . . . . . 110
Facilitate Communication . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
Software Engineering Lead
. . . . . . . . . . . . . . . . . . . . . . . . . . . 113
Technology Expert . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114
Risk Management
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
Architect Tribes
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
Software Architecture and the Software Development Lifecycle . . . . . . . . . . 117
Bridge the Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
Think Outside the Box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 120
Evolutionary Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121
Agile Unified Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122
System Lifecycle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
Defining Software Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
Architecture vs. Code
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125
v

Contents
Architecture vs. Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . 126
Architecture vs. Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
Basic Definition
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
Design Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129
Design Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
Design Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Decision Making Phases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
Decision Trees
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138
Design Outcome . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
Modeling Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
Can this skeleton fly?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
Prescriptive vs. Descriptive Architecture . . . . . . . . . . . . . . . . . . . . 144
Green Field Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
Brown Field Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146
Architectural Degradation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
Causes of Architectural Drift
. . . . . . . . . . . . . . . . . . . . . . . . . . 149
From Drift to Erosion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
Entropy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151
Architecture or Code First? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152
Architecture Hoisting
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
Presumptive vs. Reference
. . . . . . . . . . . . . . . . . . . . . . . . . . . 155
Solution vs. Product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
M-Architecture vs. T-Architecture . . . . . . . . . . . . . . . . . . . . . . . 159
The $10000 boolean flag . . . . . . . . . . . . . . . . . . . . . . . . . 160
Art or Science? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162
Science or Art? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
Modeling
165
Capturing the Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
What is modeling? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 167
Abstraction and Interpretation . . . . . . . . . . . . . . . . . . . . . 168
Solving Problems with Models
. . . . . . . . . . . . . . . . . . . . . 170
Question first, Model second . . . . . . . . . . . . . . . . . . . . . . 172
Scope of the Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
What is a view? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
How many views?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 176
Multiple Views . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
View Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
Domain and Design Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
Modeling = Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
Domain Model
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 182
Example Domain Model . . . . . . . . . . . . . . . . . . . . . . . . . 184
Design Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184
Example Design Model (Interfaces) . . . . . . . . . . . . . . . . . . . 186
Example Design Model (Implementation) . . . . . . . . . . . . . . . 186
Some Modeling Notations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
Use Case Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
Example Music Player Scenarios . . . . . . . . . . . . . . . . . . . . 189
Feature Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189
Feature Model Example . . . . . . . . . . . . . . . . . . . . . . . . . 191
vi

Contents
Feature Model Constraints
. . . . . . . . . . . . . . . . . . . . . . . 193
Constrained Feature Model Example . . . . . . . . . . . . . . . . . . 195
Feature Configuration . . . . . . . . . . . . . . . . . . . . . . . . . . 196
From C4 to C5
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 197
System Context View
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
System Context View Example . . . . . . . . . . . . . . . . . . . . . 200
Containers View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Container View Example
. . . . . . . . . . . . . . . . . . . . . . . . 202
Example Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . 203
Components View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
Components View Example . . . . . . . . . . . . . . . . . . . . . . . 205
C4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 207
Classes View
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
C5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209
Connectors View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210
Connectors View Example . . . . . . . . . . . . . . . . . . . . . . . . 211
4+1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213
Logical View
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
Logical View Notation . . . . . . . . . . . . . . . . . . . . . . . . . . 215
Example Logical View . . . . . . . . . . . . . . . . . . . . . . . . . . 217
Process View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
Example Process View . . . . . . . . . . . . . . . . . . . . . . . . . . 220
Development View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
Example Development View
. . . . . . . . . . . . . . . . . . . . . . 223
Physical View . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
Example Deployment View . . . . . . . . . . . . . . . . . . . . . . . 225
Content is more important than representation . . . . . . . . . . . . . . . . . . . 228
Model Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
Accuracy vs. Precision . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
Model Quality - Advice
. . . . . . . . . . . . . . . . . . . . . . . . . 231
Model-Driven Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 233
Modularity and Components
234
What is a Software Component?
. . . . . . . . . . . . . . . . . . . . . . . . . . . 234
Hardware Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 236
Software Component . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
Examples: Application-specific Components . . . . . . . . . . . . . . . . . 240
Examples: Infrastructure Components . . . . . . . . . . . . . . . . . . . . . 241
Black Box . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 243
Recursive Components
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
Clustering Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 245
Design vs. Run-time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
Distributed Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
Component Lifecycle Decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
Externally Sourced Components
. . . . . . . . . . . . . . . . . . . . . . . . 249
Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
Selection
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
Test . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 253
Release . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
vii

Contents
Deploy
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
Stateful Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 256
Migration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
Backup and Recovery
. . . . . . . . . . . . . . . . . . . . . . . . . . 258
Properties of Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . 259
Component Roles
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 260
Stateless vs. Stateful . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 261
Stateless vs. Stateful Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
Stateless vs. Stateful Operations . . . . . . . . . . . . . . . . . . . . . . . . 264
Components vs. Objects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 266
Component Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
Component Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Component Frameworks Demo Videos . . . . . . . . . . . . . . . . . . . . . 271
Where to find components? . . . . . . . . . . . . . . . . . . . . . . . . . . . 271
Buy vs. Build . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
How much does it cost? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 275
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 277
Reusability and Interfaces
278
Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
Component Interface
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 281
Provided Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 282
Provided Interfacesand Component Roles . . . . . . . . . . . . . . . 283
Required Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
Explicit Interfaces Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . 287
Information Hiding
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 288
Effective Encapsulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 290
Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
Describing Interfaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 292
Principle of Least Surprise . . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
Easy to use? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 295
Interface Description Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . 297
Java/RMI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 300
C/RPC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 301
RAML . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
OpenAPI/Swagger
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 304
Working With IDL
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 306
API Documentation Demo Videos
. . . . . . . . . . . . . . . . . . . . . . . 307
What is an API? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 307
Is it really API? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
Many Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 311
Developers, Developers, Developers . . . . . . . . . . . . . . . . . . . . . . 312
Where to find APIs? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 313
Operating Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 314
Programming Languages
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
Hardware Access . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315
User Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316
Web Services . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317
API Design
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 318
Where is the API?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 319
viii

Contents
API Design: Where to start? . . . . . . . . . . . . . . . . . . . . . . . . . . . 320
Who to please? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 321
Reusable Interfaces
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 322
Usability vs. Reusability . . . . . . . . . . . . . . . . . . . . . . . . . 323
Easy to reuse?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 324
Performance vs. Reusability . . . . . . . . . . . . . . . . . . . . . . . 326
Small Interfaces Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 327
How many clients can these APIs satisfy? . . . . . . . . . . . . . . . 328
Uniform Access Principle
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 330
Few Interfaces Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 331
Clear Interfaces Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . 332
Let’s create a new Window
. . . . . . . . . . . . . . . . . . . . . . . 334
Expressive? No: Stringly Typed . . . . . . . . . . . . . . . . . . . . . 336
Consistent? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
Primitive Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . 340
Design Advice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 342
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 344
Composability and Connectors
345
Software Connectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 345
Connector: enabler of composition . . . . . . . . . . . . . . . . . . . . . . . 347
Software Connector
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 348
Components vs. Connectors
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350
Connectors are Abstractions
. . . . . . . . . . . . . . . . . . . . . . . . . . 352
Connector Roles and Runtime Qualities . . . . . . . . . . . . . . . . . . . . 357
Connectors and Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . 358
Connector Cardinality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
Connectors and Distribution
. . . . . . . . . . . . . . . . . . . . . . . . . . 361
Connectors and Availability . . . . . . . . . . . . . . . . . . . . . . . . . . . 362
Software Connector Examples
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 363
RPC: Remote Procedure Call
. . . . . . . . . . . . . . . . . . . . . . . . . . 365
File Transfer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 367
Shared Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372
Message Bus
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 375
Stream . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 378
Linkage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 381
Shared Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 383
Disruptor . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 387
Tuple Space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 390
Web . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 394
Blockchain
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 397
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 399
Compatibility and Coupling
400
Compatibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 400
Compatible Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 402
There’s an app adapter for that! . . . . . . . . . . . . . . . . . . . . . . . . . 403
Adapter . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 404
Wrapper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 405
Mismatch Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 406
Partial Wrappers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 407
ix

Contents
Types of Interface Mismatches . . . . . . . . . . . . . . . . . . . . . . . . . . . . 408
Synchronous vs. Asynchronous Interfaces . . . . . . . . . . . . . . . . . . . 409
Half-Sync/Half-Async
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 411
Sync to Async Adapter . . . . . . . . . . . . . . . . . . . . . . . . . . 413
Half-Async/Half-Sync
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 414
Async to Sync Adapter . . . . . . . . . . . . . . . . . . . . . . . . . . 415
How many Adapters?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 417
Scaling Adapters with N Interfaces . . . . . . . . . . . . . . . . . . . . . . . 419
Composing Adapters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420
One or Two Adapters? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421
Reusable Adapters and Performance . . . . . . . . . . . . . . . . . . . . . . 422
How Standards Proliferate . . . . . . . . . . . . . . . . . . . . . . . . . . . . 423
On Standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 424
Standard Software Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . 428
Representation Formats . . . . . . . . . . . . . . . . . . . . . . . . . 429
Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 430
Protocols . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 431
Addressing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 432
Interface Description
. . . . . . . . . . . . . . . . . . . . . . . . . . 433
Coupling
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 434
Understanding Coupling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 438
Coupling Facets
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 440
Session Coupling Examples . . . . . . . . . . . . . . . . . . . . . . . 442
Binding Times
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 444
Be liberal in what you accept, and conservative in what you send. . . . . . . . . . 446
Water or Gas Pipe? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 447
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 448
Deployability, Portability and Containers
449
The Age of Continuity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 449
Deployability Metrics
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 451
Release
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 454
Release Frequency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 457
Speed vs. Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 459
Software Production Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 461
High Quality at High Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . 466
Types of Testing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
Types of Release . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 470
Gradual Phase-In . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 473
Essential Continuous Engineering Practices . . . . . . . . . . . . . . 475
Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
Build Pipeline Demo Videos . . . . . . . . . . . . . . . . . . . . . . . . . . . 477
Container Orchestration Demo Videos . . . . . . . . . . . . . . . . . . . . . 478
Virtualization and Containers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
Virtualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 480
Lightweight Virtualization with Containers . . . . . . . . . . . . . . . . . . 484
VM vs. Container . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 485
Containers inside VMs . . . . . . . . . . . . . . . . . . . . . . . . . . 486
Images and Snapshots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 487
Virtual Machine Migration
. . . . . . . . . . . . . . . . . . . . . . . 488
Inverted Hypervisor
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 489
x

Contents
Virtual Hardware = Software
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 490
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 491
Scalability
492
Will it scale?
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 492
Scalability and Workload
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 494
Scalability and Workload: Centralized . . . . . . . . . . . . . . . . . . . . . 496
How to scale? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 497
Scalability and Resources: Decentralized
. . . . . . . . . . . . . . . . . . . 499
Scalability and Resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . 500
Centralized or Decentralized? . . . . . . . . . . . . . . . . . . . . . . 501
Scalability at Scale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 503
Scale Up or Scale Out? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 505
Scaling Dimensions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 506
Scalability Patterns
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 507
Directory
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 508
Dependency Injection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 514
Directory vs. Dependency Injection
. . . . . . . . . . . . . . . . . . 518
Scatter/Gather
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 519
Master/Worker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 524
Master Responsibilities . . . . . . . . . . . . . . . . . . . . . . . . . 527
Worker Responsibilities . . . . . . . . . . . . . . . . . . . . . . . . . 529
Load Balancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 530
Variants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 534
Sharding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 536
Computing the Shard Key . . . . . . . . . . . . . . . . . . . . . . . . 539
Looking up the Shard Key . . . . . . . . . . . . . . . . . . . . . . . . 540
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 543
Availability and Services
544
Components vs. Services
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 546
Business model: how to sell . . . . . . . . . . . . . . . . . . . . . . . 548
Design decisions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 551
Technology: how to use . . . . . . . . . . . . . . . . . . . . . . . . . 553
Availability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 556
Availability Questions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 557
Monitoring Availability
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 559
Which kind of monitor? . . . . . . . . . . . . . . . . . . . . . . . . . 562
Availability Incidents
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 564
Downtime Impact
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 565
Contain Failure Impact
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 567
Retry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 568
Circuit Breaker . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 572
Canary Call . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 577
Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 582
State Replication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 583
Which kind of replication?
. . . . . . . . . . . . . . . . . . . . . . . 585
CAP Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 587
CAP Theorem Proof . . . . . . . . . . . . . . . . . . . . . . . . . . . 588
Eventual Consistency
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 589
Event Sourcing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 591
xi

Contents
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 596
Flexibility and Microservices
597
API Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 599
Only one chance... . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 600
API Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 601
API Compatibility
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 603
Semantic Versioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 604
Changes and the Build Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . 606
Version Identifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 607
Two in Production . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 609
API Sunrise and Sunset
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 611
To break or not to break . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 613
Who should keep it compatible?
. . . . . . . . . . . . . . . . . . . . . . . . 615
Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 619
Layering Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . 622
Data on the Inside, Data on the Outside . . . . . . . . . . . . . . . . 624
Tolerant Reader . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 626
Which kind of reader? . . . . . . . . . . . . . . . . . . . . . . . . . . 627
Extensibility
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 628
Extensibility and Plugins
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 629
Microservices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 633
Monolith vs. Microservices . . . . . . . . . . . . . . . . . . . . . . . 635
Will this component always terminate? . . . . . . . . . . . . . . . . . . . . 636
Will this service run forever? . . . . . . . . . . . . . . . . . . . . . . . . . . 637
Will this microservice continuously change? . . . . . . . . . . . . . . . . . 638
DevOps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 639
DevOps Requirements . . . . . . . . . . . . . . . . . . . . . . . . . . 640
Feature Toggles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 642
How small is a Microservice?
. . . . . . . . . . . . . . . . . . . . . . . . . . 647
Continuous Evolution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 649
Hexagon Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 651
Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 653
Independent DevOps Lifecycle . . . . . . . . . . . . . . . . . . . . . 654
Isolated Microservices . . . . . . . . . . . . . . . . . . . . . . . . . . 655
Splitting the Monolith . . . . . . . . . . . . . . . . . . . . . . . . . . 657
Microservice Best Practices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 662
Bezos’s Mandate (2002) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 663
Evans’s Bounded Context (2004) . . . . . . . . . . . . . . . . . . . . . . . . 664
Bob Martin’s Single Responsibility Principle (2003) . . . . . . . . . . . . . . 665
UNIX Philosophy (1978) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 666
Separation of Concerns (1974)
. . . . . . . . . . . . . . . . . . . . . . . . . 667
Parnas’s Criteria (1971)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 668
Conway’s Law (1968) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 669
Vogels’s Lesson (2006) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 672
References
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 673
xii

Software Architecture
Introduction 1
Contents
• Architecture and other Metaphors
• Architectural Forces
• When do you need an architect?
• Why software architecture?
• Course Overview
1

De Architectura
• Durability the building should last for a long time without falling
down on the people inside
• Utility the building should be useful for the people living in it
• Beauty the building should look good and raise the spirits of its
inhabitants
Vitruvio, 23BC
Software architecture like software engineering is a metaphor that we borrow from
some other discipline and we bring into Informatics. There is something that we do when
we develop software at scale with large teams, when we need to make certain kinds of
important decisions, which reminds of what architects do. We call those decisions ar-
chitectural. How well does the architecture metaphor fit with software? There are some
similarities and there are some mismatches. So now before we talk about software ar-
chitecture want to spend a little bit of time to introduce what is it from architecture that
we can bring in and learn from. After all software architecture is a relatively young dis-
cipline. Architecture’s been around for longer.
For example, one of the basic ideas of Architecting Buildings is that you should know
how to design a building that can last for a long time. There are buildings from thousands
of years ago that are still standing today. They have been built using stones only. As with
every technology that you choose to adopt. You should know the limits and you should
know when it’s applicable when it’s not. You should be confident that the building will
not collapse while or after attempting to build it.
The second principle is that what you built should be something useful. A bridge which
connects two sides across a river, making it possible to cross it. A house for people to live
in. A university with classrooms for students.
The third one concerns the effect of the building on the people inside. Does it provide
a welcoming place for people to live inside? Do people enjoy working in it? Or do people
want to escape from it as soon as they can?
2

The Art and Science of Building
• Architects are not concerned with the creation of building
technologies and materials—making glass with better thermal
qualities or stronger concrete.
• Architects are concerned with how building materials can be put
together in desirable ways to achieve a building suited to its
purpose.
• The design process that assembles the parts into a useful,
pleasing (hopefully) and cost-effective (sometime) whole that is
called “architecture”.
Architects are not concerned about how to create new and improved building tech-
nologies. They are also not concerned with the actual construction of the building them-
selves.
They’re mostly interested in how to select and compose the right tools, introduce the
right elements, and connect them appropriately. How to establish the right kind of re-
lationships between the elements so that we have a good building that fits its purpose?
While doing this, the architect is making the decisions for the building’s plan, which will
be executed to construct the building by someone else. If you’d like to learn how to be
an architect, you need to learn how to plan. And you will learn how to do it before the
actual building exists.
The same holds for software architecture, full of ideas that you should be able to ex-
press in your design for the software before it’s being built so that someone else can go
and write the actual code for it. As experienced individual developers, you may wonder
how this works, where does the architect fit between the customer requirements and my
work writing the software? The architect is key to be able to lead the development effort
and scale it so that you can have hundreds of developers working on the same project.
And making sure these developers agree on how the architecture for this code should
look like.
Overall this is what we call architecture: a kind of design process, with selection and
composition activities, but also a process rather different to simply writing and testing
the code.
3

Eberhardt Rechtin, 1991
Architecting, the planning
and building of structures, is
as old as human societies –
and as modern as the
exploration of the solar
system
4

Architects follow a process going from a concept, represented with sketches, which are
refined into mockups and detailed plans, which can then be built. A successful result of
the whole process will embed and highlight the original concept.
Following such process helps to catch and correct mistakes early, explore different op-
tions, validate the concept also taking into accounts its relationship with its context (no
city building exists in isolation from the ones surrounding it). Likewise, it helps to obtain
buy-in from the customer, who can attempt to influence the outcome of the process.
You should learn how to sketch the architecture of a software system. You should learn
how to refine your sketch and make it into a plan so that somebody else can write its code.
5

Another concept that we borrow is the idea that when we build something, we need to
have a foundation for it. This is the point of contact with the ground, the surface that
supports the building.
Also when you write your software you are building it on top of a foundation made of
software and hardware. It is important to be aware of the kind of foundation you select
to deploy your software on.
6

Even if we don’t usually call it ”software foundation”, there are many different ”plat-
forms” on which you can deploy and you can write software for.
The latest metaphor to describe a foundation for software is not based on something
solid like the ground, but instead nowadays our software runs in the clouds. For some
reason this seems to be a good way to sell access to so-called ”platforms as a service”
(PaaS). You place your software in the Cloud. And this somehow gives you some stable
foundation you can rely on.
Which platforms have you built software for?
Is a programming language like Java a platform? The language can be part of a plat-
form. Still, while Java was marketed as platform independent, it actually is a special
platform to write software which then can run on multiple platforms. This is where the
foundation metaphor starts to show its limits.
7

Successful platforms last for a long time. Successful platforms attract developers, who
flock from platform to platform. Unsuccessful platforms are discontinued as it is not
economically viable to maintain and provide support for them. Platforms also evolve as
their underlying technology improves, gets faster and cheaper. Planned obsolescence on
the other hand is used to quicken platform upgrade cycles by force so that it becomes
possible to make platform adopters pay again and again, especially when there is only
one platform left in the market.
Eventually, you come to the realisation that there is nothing that is built on a solid
foundation. Everything is built on shaky ground. But we have to pretend that when we
decide to invest in building a piece of software, we can do so by reusing some kind of a
platform and trusting or hoping that it will remain there long enough.
It is no longer feasible for a single developer to design their hardware, define a proces-
sor instruction set, build their own computer, invent a programming language, write a
compiler, a standard library for it and use it to write the operating system for such com-
puter, and still have enough time left to write useful apps to run on it.
8

Other metaphors that we use when we talk about architecture is for example, the no-
tion of boundary. The first thing you should define is the system’s boundary, for example,
by drawing a box delimiting the inside of your system from the outside, the rest of the
world.
This boundary can be closed if your system is isolated and does not exchange any in-
formation with outside. You have complete knowledge about the state of the system and
there is no information that you have to go somewhere else to get.
Another way to look at the closed metaphor is to consider the functional requirements
that the closed system should satisfy. For example, let’s consider the game of chess or
the game of go. You can list the rules, you know exactly what they are, and the rules
are not changing. And these rules are limited and well known in advance. This makes it
possible to solve the game.
9

Your system is no longer closed when there is uncertainty about its boundary. Not only
requirements may be partially known, but they may change in the future in unexpected
ways.
Let’s say you build a application and you publish it on the open Web and it goes viral.
A few days after you launch, you get 2 million users that you did not anticipate. But what
if it never happens? Should every architecture be designed to scale to ”Web scale”? Or is
it more important to be able to grow and adapt your system when it becomes necessary?
Enter the Internet. Every thing becomes connected to everything else. Today every
system is an open system. Whether you want it or not, you cannot build a wall around
your system, it needs to be integrated with other systems to be useful. Your system be-
comes interdependent and interconnected to other systems over which you have no con-
trol. Service providers may disappear without notice or change their terms of service
(e.g., switching from a free to a paid service, or a stiff price increase), making it no longer
possible for your system to work with them.
10

When we have open systems which connect with other systems, the points where their
boundaries interact become very important. We call these interfaces and will dedicate
some time to dissect the issue of how to build a good API.
This biological picture represents a slice through some vegetables which has cells that
are quite large. Every cell has a boundary and an interface towards the other cells. Life
has taken a long time to find out how to evolve from single-celled organisms into multi-
cellular ones.
It is not a trivial problem for heterogeneous, autonomous and distributed systems to
communicate, interact and inter-operate so that you can use them as building blocks to
construct something larger, something whose value is greater than the sum of its parts.
This becomes particularly difficult when each system is built by people who do not
know one another and are unable to talk together and have a tendency to apply changes
without informing the others in advance.
11

Forces
Architecture
Desirability
Requirements
Viability
Time 
Resources 
Costs
Feasibility
Technology
Aesthetics
Simplicity 
Elegance
What are the challenges of designing an architecture that as an architect you have to
deal with?
Your architecture should be useful: how do you find out exactly what the requirements
are? What’s the problem that you are supposed to solve? It is usually not your problem,
but it belongs to someone else: how do you talk to your customer in the right way so that
you can understand what they want you to do? Since software can be applied across any
domain, this is particularly hard, unless you become an expert in the particular domain.
Assuming you understand what you need to solve, and assuming you can solve it with
software. Will you be able to do so within a budget? Can you build it within a reasonable
amount of time and by spending not too much money? Can you always speed up your
project by adding more developers to the team? Be careful not to fall into the mythical
man month trap.
Once you have a reasonable budget, you still need to pick a suitable technology plat-
form. Learn more than one technology, and be critical and aware that every technologies
that you choose will have some limits and these limits will affect where it is applica-
ble and where it should not be applied. And the choice of an expensive technology will
impact on your budget, or an immature or unknown technology will reduce your team
productivity. Although developers tend to be motivated to play with shiny new tools.
Finding a viable, feasible and desirable solution is also what engineers do. The dif-
ference with architects is their focus on making the solution as simple and elegant as
possible. Everything else being equal, never underestimate how challenging it is to find
an essential, beautiful and simple architecture. This is something worth striving for, even
if it may not so obvious what makes a piece of software beautiful. Even if you should have
enough experience to tell the difference.
For example, interfaces may be used to hide the ugly internal details. And developers
will enjoy or hate working with a certain language or library. You can help them to come
to your platform if you make it simple and easy for them to learn. However doing this is
hard.
12

It depends.
Is it a first-of-a-kind project? Or is it something your team has done many times be-
fore?
Also, it depends on how large and complex your software is going to be.
13

Small
One Person can build it
• Minimal planning
• Basic tools
• Simple process
Medium
An experienced team can build it
• Modeling plans
• Power tools
• Well-deѹned process
14

Large
Do not try this without an architect
Ultra-Large
Ultra-Large
Was this designed by an architect?
15

CodeCity, Richard Wettel
How Large?
Always choose the appropriate tools for the size of the project you
are working on
16

When you start a project ask yourself how large is going to be and how long is it going
to be alive for? And then choose the right team organization and choose the appropriate
tools to do it.
How Large?
• How much time to build it?
• How many people in the project?
• How much did it cost?
How to measure software size? 
, GB, $
Lines of code (LOC)
 
 
There are many different ways to measure the size of software. For example, you can
ask: How long does it take to build it? And this is actually quite ambiguous, you can
define it as the development wall time, starting from the very first Git init command
where no code exists yet and then checking how long does it take you to write all the code.
Or if you are using DevOps and continuous integration. How long does it take to build it
and release it? I push a change and then I go home and come back the next morning and
I have the fresh new build. Or I push a change and I can see the effect instantaneously
like with a live programming environment. It’s a totally different developer experience.
Small systems can be written by one developer, but most are written by teams. You can
have larger and larger teams, but how do you organize 5000 people working on the same
system?
17

What is the largest software project you have been working on?
1 month
Time
1 year
1 
Team
2 
10 
100 
Software Architecture
As the size and complexity of a
software system increase, the
design decisions and the global
structure of a system become
more important than the
selection of speciѹc algorithms
and data structures.
18

As the size and complexity of your software grow, there is something that becomes
gradually more and more important than just knowing what is the algorithm or picking
the data structure. Going back to the building metaphor, this would be like the choice of
the type of electrical wires, plumbing pipes or window glass.
Lecture Context
•  Software Engineering
•  Programming Languages
•  Algorithms and Complexity
•  Databases
How to build
components
from scratch
Semester 2011
•  Software Architecture
•  Component-based
Software Engineering
How to design large systems
out of reusable components
Most of the courses you have studied give you the impression that you will build ev-
erything every time from scratch. Design a new database schema. Write a new program.
The real world is not like this. You don’t often go into a software company to write
your application from scratch. You work on their existing code and the existing code can
have millions of lines. And what you build is usually an extension or an integration, a
new piece that connects with other pieces.
We assume that you know how to build individual parts, but once we have all these
components. How do we integrate them? How do we connect them so they can work
together? In this lecture, we talk about the big picture.
19

To motivate the need to learn how to work with software architecture, we need to place
ourselves in the right context, where there is one or more teams of developers working
on a large piece of software. How do we keep this alive and well but also under control?
20

Hiding Complexity
You should be able to work with powerful concepts that underneath have a lot of in-
formation attached. But you can talk about them because you learned how to use the
right technical terms that can help you to describe something that is complex in a very
compact way.
21

Abstraction
The difѹcult part is to know
which elements to leave out
and which to emphasize
In the geography domain, you have a satellite picture or you can have a map. And,
which one represents the territory, the real thing?
When we build a model we are trying to capture the information that is necessary for
us to solve certain problems for example, here we have a street map. And you want to
know, how do I go from A to B? Should I take the highway?
The difficult part when you do abstractions is to know what information do you keep in
the model in which information can you ignore. When you draw a picture of your software
that has 1,000,000 of lines of code. You have to decide which lines are important, and
which lines are not which ones should I visualize and which ones I shouldn’t.
In the example, we have 3 different representations, which when doing an architec-
tural model we call viewpoints or perspectives. They show different aspects of the same
architectural model. It’s important that all representations, while showing different as-
pects, are consistent. Otherwise, the developers that look at one map will do something
different than the ones looking at a different map, because they read an inconsistent
message about the system they are trying to build.
At the end of the day, the code is the ultimate source of truth about a software system.
What you write in the code embeds all decisions that were made to design it. However,
how can you gain an understanding of millions of lines of code? It is impossible to read
every single one. This is where using abstraction to manage such complexity helps. You
can see the software architecture as a map to navigate large amounts of code.
22

Communication
Architecture
Business
IT
A second very important aspect of architecture is about enabling the communication
between different team members. The goal is to have an agreement between the devel-
opers about what to do so they will build the the same system. They will write code that
can be integrated together.
But also there is a need for communication between the customers and the business
side of the world or the domain experts and the technical people. The architect is the
point of contact, who should be able to translate back and forth.
As an architect, you have to be an expert in the technology, you need to know how to
talk to a developer an be able to tell him how to build the software, but also you have to
be able to talk to the business side and explain why you need such amount of time to do a
project or why do we need to pay so much money to so many developers. So this may be
a challenge because if you just study software and data engineering, you are very strong
on one side but do keep in mind that as an architect you need also to be able to bridge
this gap.
23

Representation
Architecture visualization was born with those famous boxes and lines diagrams. Some-
times the discipline is criticized because it seems architects spend their time drawing and
looking at these diagrams. How is that connected with real software?
However, when you have very large software, the argument is that it’s important for
describing and understanding it, to be able to represent it. Not by using a programming
language, but using some kind of a diagram which follows some kind of a visual nota-
tion. There are many notations, UML is not the only one. This one is more like a data
flow diagram, showing how data is transformed while in transit between different storage
elements.
Whatever notation you choose, the visualization works if it tells you a story. If it ex-
plains how the software works and how it is built so let’s look at this example.
You should be able to tell me the name or at least the purpose of the system.
A search engine? Yes, Why?
It shows many individual functions (the bubbles) that you would expect to find in such
a system and also, if you look at the overall flow, there is a loop for crawling the Web
and everything leads down into to the searcher. Between some functions you have state
(the barrels) which functions read from and write into. Why are there so many barrels?
Because it is important to show at this architectural level that the index is huge, so prob-
ably they use sharding and also need to have fast access to it to improve the search query
performance.
Exactly which search engine is this diagram representing? This is actually an early pic-
ture of Google, you can tell by the ”Pagerank” bubble, which takes the Web link topology
into account when ranking the search results. This was their key innovation, sometimes
it is enough to add one new component to an existing architecture to make a difference.
24

Visualization
There is of course, the whole branch of software architecture visualization out of which
the layer cake is one of the most famous examples.
This is also the classical way to explain in the connection between operating systems
and applications, the different roles of user space and kernel mode and illustrate how
hardware is abstracted away through several layers of software.
This type of visualization also shows this concept of ”foundation“ on which your ap-
plication is built on. User programs use the services provided by the “underlying” infras-
tructure. What is also important in this picture is that every point of contact between the
layers becomes an interface.
Choosing a layered architecture, gives you a fundamental way not only for subdivid-
ing the system but also for deciding how the different parts are connected together (and
which parts cannot be connected) and what kind of interfaces do they should provide.
25

Visualization
Spring Semester 2011
Database (slow, persistent)
Load Balancer (assigns a web server)
Web Server (PHP assembles data)
Memcache (fast, simple)
Another layered architecture, a bit simpler than the previous one. This is the type of
picture that you would see if you join the company, to give you the overview when they
start explaining you the architecture of their system.
What system does it represent?
Something online, because there is a Web Server. Something that needs to scale to
handle lots of users, because there is a load balancer. Since it mentions PHP, it could be
Facebook.
These are all valid guesses, but independently of the actual name of the system, we can
look at the diagram and think about what are the most important concerns for the archi-
tects that designed it. How can this describe Facebook without mentioning the social
network graph of friends? Well, this picture tells a different story. It says we have bil-
lions of users and we need to scale. We need a load balancer, we use PHP (but we rewrote
the compiler to improve performance), we use a database, but we put a memory cache in
front of it to speed up read operations. If you are the architect in a company that went
from 0 to billions of users in a short time, your core concern is how to scale.
This scalability problem is one of the many possible qualities that you should worry
about as an architect.
26

Quality
Functionality
Usability
Ease of support
Cost
Resilience/Maintainability
Reusability
Security
Reliability/Availability
Dependability
Compatibility
Ease of Integration
Portability
Testability
Customizability
Scalability
Performance
Time to Market
Elasticity
You may have heard about the difference between functional and non-functional re-
quirements. If you satisfy the functional requirements you have a correct system. If you
satisfy the non-functional requirements, you have a non-functional system, a system
that doesn’t work.
In this lecture, we use the term ”extra-functional” requirements, to describe all quali-
ties of a system that go beyond its basic correctness. A functional system works, but how
well does it work?
If you want to satisfy your customer, delivering a correct solution is necessary, but is it
sufficient? What if you deliver a super scalable solution (like in the Facebook example)
but you forget to implement the ”invite a friend” feature?
How do you write a program that is correct? There are plenty of lectures where you can
learn that, so I assume that you know how to do that. What we’re going to do here is look
at everything else.
For example, once you deliver it to the user, your software should be usable. You can
study human-computer interaction (HCI) to learn how to and how do we design user
interface which takes advantage of what the hardware can do (e.g., keyboard and mouse,
touch, VR/AR, or voice) and choose the appropriate ones for the type of system you are
going to build.
What happens if something is not usable? People need to be trained to learn how to
use it. So it’s more expensive to start using the system because people cannot use it by
themselves, they need to go to a training course. However, when users get in trouble,
they ask for help; so how easy is it to support your user? Do you have a way to actually
see what they’re doing in the system so you can help them if they get stuck?
Another quality is how expensive is your software going to be. Are you going to sell it
for a certain price? If you give it away for free, then you don’t make any money with it.
How are people supposed to be supported while using it? Or In other words, if you have
a business model where the software is free, how are you supposed to pay the salaries of
the developers that build and maintain it? It’s also an important decision: What’s the
business model behind your software? Do you offer free software or people have to pay
do you have a one time license or do you pay every subscription every month? If you
don’t write the software and sell it to end users, but you write a component and that
component is sold to be used within other systems and then you can also sell it and get
27

royalties. You can also sell it as a service and people pay you and then depending on
how much traffic they send you then you charge them in your monthly bill. This is what
happens with service-oriented architectures and cloud computing.
Resilience determines how easy it is for you to keep maintaining the system. For ex-
ample, when a user tells you there is a bug. How much work does it take to fix it? Or they
have new feature requests, how easy it is to extend the system? What is the impact of
change within your architecture? Maybe you have designed and built the perfect system.
But you should never touch it again. Because it’s not possible to change it. Maybe you lost
the source code, maybe the system is still working fine, but nobody has the source code
anymore. So you need to call the reverse engineering consultants to do the retrofitting.
Another aspect that is different than usability is how reusable is your software. What
is easy to use for a certain application, may be difficult to reuse for another one. And
usually, when you make it reusable, it tends to be very general. You can transport it and
you apply it in many different contexts, but then it’s more complex. Because it has a
general interface, which is more difficult to learn and thus less usable.
Reusability also interacts with your business model. If you are in the business of selling
software as a component. Then you want these components to be highly reusable so you
can sell them to a lot of customers. If you build a component for one customer, a highly
customized component, specialized for one particular context, then you cannot reuse it
anywhere else. So the price will be higher.
What about security? It’s a very big topic, we’re not going to go very deep into it.
But it’s another aspect that is important to consider about your software? What kind of
information does it work with? Does it have any sensitive data? Is it important than only
some users can do something as opposed to others right? We need to put some kind of
access control into it.
How available and how reliable is the system that we built? Once people want to use
your software, is the software going to be there for them when they need it? Or the
people will have to live with the uncertainty that when they need to use this software
now, but maybe it’s not working and I have to come back later and hope for the best. As
an architect, you have to worry about whether your design is fault tolerant, then deploy it
the right kind of environment with enough redundancy. This is covered under the more
broad quality of dependability.
There are many other qualities. Sometimes you have a problem of interfacing a system
with another one, so how compatible is it? How easy it is to integrate? How portable?
How much does it cost to redeploy the system across different platforms? How easy it is
to test my system when I want to make sure that is correct? Do I have tests for it? How
expensive is to write tests with good coverage?
How easy it is to customize my system? Does it scale to many users? How good is
the performance? These are the usual examples when you think of requirements that go
beyond the correctness. Of course, we have a lecture on software performance.
You may have the most scalable and performant system, but it is no good unless we can
ship it today. What if you need to change something, fix a bug? How long does it take for
you to fix it and then to deliver the fixed version to the customer?
Last but not least, how elastic is your system? Assume we run it in the cloud, can it
scale just enough to handle the load and then shrink back to the small configuration so
that it’s cheaper to run?
So these are all qualities of a software system and you as an architect should see your-
self inside the circle with all these arrows pointing at you. And you need to decide which
ones are you going to consider which one are you going to be pushing back against. We
decide will make it really secure, but we have to sacrifice a little bit of usability because
people have to enter the password or scan their faces. We have to sacrifice a little bit of
28

performance because we need to encrypt the messages. Or we will make it portable so
that we can sell it too many customers but it will need to e virtualized and will be will run
a bit slower.
The message is that you cannot have the perfect system where every quality is maxi-
mized so you always have to balance and trade off one quality against another. One thing
that we will practice is how to do these trade offs between different qualities.
You can also look at this picture as a way to structure your path into the topic of soft-
ware architecture. We will spend some time going over the qualities and defining them
in detail, and then we will discuss how to design architectures which can deliver each
quality in a way that should be independent from the way that you program its code. So
that’s the main idea for this lecture.
29

Software is expected to change. Software is meant to change. The software architec-
ture includes all the design decisions about the software which are hard to change.
If you decide to write your code in a programming language, you may not be able to
afford to spend 6 months developing your system and then go back on your decision and
switch to another language.
Once you make an architectural decision, you decide to invest into a technology, you
decide to start moving in a certain direction, it becomes more and more difficult to switch
or steer your project to follow another path later on.
30

Evolution
Site
Structure
Structure
Skin
Skin
Services
Space
Plan
Stuff
Stuff
Architectural decisions:
hard to change later on
and that you wish you could
get right early in a project
Stewart Brands, Ralph Johnson
Architectural decisions are important because if they are difficult to change, then you
have to get them right and you cannot make mistakes the first time you make them.
You usually cannot change the location of a building after you started to build it. It’s
difficult to add more floors on top of an existing building unless the structure has been
originally designed for the building to grow. It is easy to repaint a facade to change its
color. It’s more difficult for example, to add a kitchen or add a bathroom where the
plumbing is not available. It is also possible to throw down non-load-bearing walls if
you need a bigger room, or you want to open or close a window. And some furniture is
meant to be rearranged on the fly.
When you think about your software, think about which elements are like the chairs
which are easy to change and which elements are like the columns which shouldn’t be
touched or everything will collapse.
31

Why Software Architecture?
˾. Manage complexity through abstraction
˿. Communicate, remember and share global design decisions
among the team
̀. Visualize and represent relevant aspects (structure, behavior,
deployment, …) of a software system
́. Understand, predict and control how the design impacts quality
attributes of a system
̂. Deѹne a Ѻexible foundation for the maintenance and future
evolution of the system
Course Overview
• Theory
• Modeling exercises
• Technology demos
• Design workshops
32

Modeling exercises
• Learn how to sketch, reѹne, communicate and defend your
architecture from multiple perspectives
• Describe an idea for your next software project, or represent an
existing one
• One model presented by each student
Technology Demos
• Learn about how software architecture meets the code
• Give a convincing demo of a component framework, a
connector, a continuous integration tool, a deployment tool
• One video for each student
33

Design Workshop
• Learn about how to make architectural decisions
• Present your alternative
• Compare it with another one, argue why your team should adopt
it
• One design discussion for two/three students
(one student per alternative)
References
• Michael Keeling, Design it! From Programmer to Software Architect, Pragmatic Bookshelf, 2017
• Michael T. Nygard, Release It! Design and Deploy Production-Ready Software, 2nd edition,
Pragmatic Bookshelf, 2017
• Richard N. Taylor, Nenad Medvidovic, Eric M. Dashofy, Software Architecture: Foundations,
Theory and Practice, John-Wiley, 2009
• Simon Brown, Software Architecture for Developers, LeanPub, 2015
• George Fairbanks, Just Enough Software Architecture: A Risk Driven Approach, M&B 2010
• Eberhardt Rechtin, Systems Architecting: Creating and Building Complex Systems, Prentice
Hall 1991
• Henry Petroski, Small Things Considered: Why There Is No Perfect Design, Vintage 2004
• Ian Gordon, Essential Software Architecture, Springer 2004
• Christopher Alexander, The Timeless Way of Building, Oxford University Press 1979
• Stewart Brand, How Buildings Learn, Penguin 1987
• Amy Brown and Greg Wilson (eds.) 
, 2012
The Architecture of Open Source Applications
 
 
34

Software Architecture
Quality Attributes2
Contents
• Internal vs. External Quality
• Meta-quality
• Quality attributes along the software lifecycle: design, operation,
failure, attack, change and long-term
35

Quality
Required
Defective
Desired
Ideal
Quality is the reason why we need the software architecture. If we want to get a soft-
ware that has a certain quality, we need to design it into the system. Later we will need
to make sure that we get the quality after we write the code. But before we start writing
the code, we need to think about: what is the goal? What do we want to get out of the
system? And depending on the context, depending on the customer, depending on the
users, the required quality will change.
When we talk about quality we talk about: something that we can observe, something
that we can measure and something for which we have a reference that tells us if we have
a good quality or a bad quality. Should you just go for the minimum acceptable level? Is
your software good enough? Or should you never ship it until it is perfect?
While perfection is typically impossible to achieve with finite time and resources, shoot-
ing for the stars could land you on the moon.
Anytime you try to design something, you have to have a reference, a target to be
achieved. And the customer should set it. What if the customer doesn’t have an idea
about what is the level of quality that is required? Then you have a bigger problem to
solve. It’s much harder to hit a moving target.
36

Types of Functionality
Functional
It works!
Non-Functional
It doesn't work
Dys-Functional
It doesn't work properly
Extra-Functional
It works well
Functional
• Correctness
• Completeness
• Compliance (e.g., Ethical Implications)
Extra-Functional
• Internal vs. External
• Static vs. Dynamic
Functional requirements need to be satisfied: your software should deliver what the
customer wants correctly. Your software should also comply with internal or external
regulations, legal constraints. When we talk about functional requirements, you should
also consider completeness: Is your software feature complete? Is the system already
doing everything it’s supposed to do? It is important to know when you are done. This
assumes that the customer does not come up with new requirements, as most successful
software in use tends to make customers ask for more features, so most projects are never
truly complete.
Functional requirements tell you whether the software works or not. What about de-
scribing how well does it work? Extra functional requirements describe all other quali-
ties that need to be delivered to make your stakeholders happy. Internal qualities focus
on your developers, external ones on the customer experience. Static qualities can be
checked before starting your software by analyzing its source code or the model for its
architecture; dynamic qualities (e.g. performance) will need to be observed while run-
ning the system.
What about non-functional requirements? Those are used to specify under which con-
ditions your software shouldn’t work. Like when an online supermarket flooded by cus-
tomers locked down at home sends you an apology email after mistakenly accepting your
order: the checkout page should have been non-functional at that time.
37

Internal vs. External
External qualities concern the ѹtness for purpose of the software
product, whether it satisѹes stakeholder concerns. They are
affected by the deployment environment.
Internal qualities describe the developer's perception of the state
of the software project and change during the design and
development process.
Note that whether you can deliver externally-visible qualities will also depend on what
kind of deployment environment you run your software in. Performance sometimes can
be improved without rewriting the software by upgrading from a slow Raspberry Pi to a
faster computer. As a software architect, to guarantee certain qualities, you also need to
control the target deployment environment, which can be made of hardware or, nowa-
days, mostly virtual hardware, e.g., sofware-defined hardware platforms in the Clouds.
Internal qualities determine the beauty of your code, or how badly written it is; whether
your team wrote it in a rush, or if it is fully covered by tests. Unless the code is open
source, nobody else will be able to appreciate these internal but fundamentally important
qualities, which will affect how rapidly your software can grow while still allowing you to
keep a good level of understanding and control over it.
38

Static vs. Dynamic
Static qualities concern structural properties of the system that
can be assessed before it is deployed in production
Dynamic qualities describe the system's behavior:
• during normal operation
• in the presence of failures
• under attack
• responding to change
• in the long term
As soon as you design your architecture and write the code you can already measure
and predict static qualities. But until you deploy, start, and run your software for the
first time you cannot measure dynamic qualities. Most qualities which matter to your
customers and users (e.g. performance, availability) will need to be monitored in pro-
duction. Still, if you can afford it, you can do some performance or reliability tests in a
dedicated environment so that if something goes wrong your users are not affected. How
representative will be the workload? Will you be able to reproduce typical failure sce-
narios? It may be hard to fully anticipate and reproduce the actual production traffic or
failure scenarios, or even malicious attacks to your software.
Different qualities cover different aspects of the software lifecycle and how your soft-
ware reacts to events which you may not fully control (a random cosmic ray hitting the
memory cell and flipping a bit). Will your software tolerate failures, survive attacks, and
recover back to normal operating conditions?
Another set of qualities describes how you can change, extend and evolve your soft-
ware. How flexible is your software? How many times can you go around the cycle? Does
software wear out? Do bits rot? For some reasons, in the long term, it may become harder
and harder to maintain your software: dependencies disappear, hardware regularly be-
comes obsolete, or it may become impossible to reboot your system for installing the
latest upgrades. What if your software is running an autonomous space probe and can
only be updated before the rocket launch? Or maybe you lost the original developers and
the new developers replacing them are not familiar with the system and have a hard time
maintaining it. Or maybe you thought you have built a throwaway prototype but it ends
up deployed in production and a business happily runs it for a few decades.
How do you balance short term thinking during rapid, incremental delivery cycles with
a long term perspective to make your software architecture sustainable?
39

Meta-Qualities
• Observability
• Measurability
• Repeatability (Jitter)
• Predictability
• Auditability
• Accountability
• Testability
Before we look at the actual qualities, I wanted to mention some qualities about qual-
ities. For every quality we will introduce later, you should ask yourself first: how can I
observe and measure this quality? Can I give a number for it? What kind of measure?
What kind of metric is it? Money for cost, time (in nanoseconds) for performance (re-
sponse time).
But some qualities are not so easy to measure: you look at the architecture and tell me
how complex it is. Measurability is important because if you cannot measure it, you can-
not control it and you cannot argue let’s improve it because you don’t know how to assess
and compare the impact of your architectural decisions on the quality of your software
system.
Another meta-quality concerns measure multiple samples over time: How stable is
the result? Are those values all over the place or do you get some nice distribution? If
you measure the system for a while can you predict where the quality is going? Can you
extrapolate and plan for when the target will be reached? Can you detect if the quality is
degrading?
Who wants to measure the quality? Who is interested to know about a certain quality?
You want to measure it yourself as an architect or as a developer. Maybe the customer
wants to see it as well. There can be some contractual agreements stating the expected
performance of the delivered system. And if the agreement is violated, there may be
some financial penalties involved. So you need to trust the measurement as the results
can have legal consequences. Sometimes you are supposed to keep an audit log that
shows when the system was up and running for a given period to track availability issues.
And often you need someone to blame or to be held accountable when a certain quality
drops below an acceptable level.
And then you have the problem of quality assurance: you have a quality to achieve,
you go through the development process, you release your system: How easy it is to
check that indeed what you deliver has the quality that you promise. As an architect,
you do your best to design the system in the right way. The developers take your design
and turn it into code. Did they do a good job? Did they violate your design and there-
fore the quality? How to make sure that it’s possible to test that a certain quality has
been achieved? While automated testing typically is done for functional correctness, but
can also be done for other qualities. Performance, for example, you run a benchmark and
40

measure the performance. If you’re concerned about reliability, you can follow chaos en-
gineering practices to inject failures and check whether the system survives and whether
your recovery strategy works. Different qualities have their own way to check whether
they have been achieved.
41

Quality Attributes
Reusability
Security
Availability
Compatibility
Portability
Scalability
Performance
Elasticity
Affordability
Time to Market
Feasibility
Functionality
Correctness
Compliance
Completeness
Ethics
Stability
Interoperability
Usability
Ease of support
Serviceability
Accessibility
Dependability
Customizability
Conѹgurability
Modiѹability
Recoverability
Survivability
Reliability
Durability
Safety
Conѹdentiality
Integrity
Privacy
Defensibility
Modularity
Consistency
Simplicity
Clarity
Operation
Design
Failure
Attack
Change
Composability
Resilience
Adaptability
Extensibility
Internal
Stakeholders
External
Visibility
Ease of Integration
Flexibility
Sustainability
Deployability
Long-term
Authentication
Authorization
Maintainability
Non-Repudiation
Evolvability
Manageability
Understandability
Explainability
Aesthetics
Disposability
Observability 
Measurability 
Repeatability 
Predictability 
Auditability 
Accountability 
Testability
Meta
Design Qualities
• Feasibility
• Consistency
• Simplicity
• Clarity
• Aesthetics
• Stability
• Modularity
• Reusability
• Composability
• Deployability
42

Feasibility
What's the likelihood of success for your new project?
• Affordability
• Time to Market
When you worry about feasibility you ask yourself what’s the likelihood of success of
your projects. Are we going to make it? The project can be a completely new software or
it can also be a modification: I have a system in this state. I have to extend it. I have to
change it. I have to fix it and are we going to successfully deliver the improvement?
What are the two main factors that affect this? One is whether you can afford it. And
the other one is time: how fast are we going to be, because maybe even if you’re going to
make it you’re going to be late.
43

Affordability
Are there enough resources to complete the project?
• Money
• Hardware
• People (Competent, Motivated)
• Time
• Slack
What does it mean to have enough resources for a project? What do you need to do
a software project? You need money. The money turns into people paid to develop the
software. Developers need also the right hardware and development tools, both to set
up a development environment to work on the system and also, later on, to test it and
operate it.
The other aspect is time. It turns out designing and writing code is not instantaneous.
It takes time.
You can try to trade off time with money under the assumption that if you pay more
you can go faster because you get more people. But there is a point in which you have too
many people on the same project and it starts taking longer. This is called the mythical
man-month.
And then you also need to have something called slack, not that one, the real slack.
44

Slack
Are there enough free resources (just in case)?
• Deal with unexpected events
• Breathing space to recharge
• Planning, Backlog Grooming
• Keep track of the big picture
• ReѺect and Refactor
• Learn and Experiment
If you look at your project plan, you have a schedule. And you know who will work on
what when. The plan is very detailed, you can predict what is expected to happen during
the next few months down to the minute. Does it have a little bit of space in between
the activities? More in general, do you have just barely enough of what is necessary to
complete the project? Or is there a little bit more? Do you have enough spare capacity?
Are you facing a tight delivery deadline? Or maybe you know that we will finish one week
before. Just in case, because you never know, and maybe at the end of the day, we can
use the extra week.
If you are a student, you should think about slack when you work on your assignments.
At the beginning of the semester, you have a lot of slack. But somehow unexplicably,
with the end of the semester slack disappears. And then you have a hard time to deal
with unexpected events. There is no breathing room left. You cannot keep track of the
big picture because you’re fighting fires one after the other; you’re trying to deliver all
these assignments. You can no longer, for example, think about how to prioritize the
work. No time is left to do what in the agile literature is called the backlog grooming.
Your backlog accumulates what should be done and you’re not supposed to just taking
on random activities to do, but you are actually trying to see what’s the best thing to do
next. But to do that, you need to afford the time to be able to think about it, to reflect, to
hold a retrospective: to check the quality of the outcome you have to check the quality of
the process that was used to do it and then you have to be able to learn something from
past mistakes so that you can improve the way you work.
Another thing that is typically lost when there is a lack of slack is the ability to do
refactorings and care about the internal quality of your software. You write the code in a
rush so you can ship it and immediately start working on the next iteration. You build on
it, and every time you add stuff but you never have time to clean up the code refactoring
it.
Technology evolves, when do you keep up with it? You have to learn how new tech-
nologies works, what’s new and check how changes affect your system. This also takes
time, either you plan for keeping up, or you do it in your spare time.
45

When you budget time and resources for a project always add a little bit of breathing
room in the schedule but also maybe you want to have more money just in case you need
more people. Then you can have the time to hire them without stress, and it will be less
likely you need to deal with burn outs.
And you can afford to waste time chatting on the other slack as well.
46

Time to Market
How soon can we start learning from our users?
Slow
Fast
Build from scratch
Reuse and Assemble
Perfect Product
Minimum Viable Product
Design by Committee
Dedicated Designer
How do you know you have allocated or acquired enough resources for your project to
be feasible? You cannot know unless you have a target, a deadline to ship. It is feasible
because we will be ready to ship tomorrow.
If you are writing gaming software, you have to ship before people start looking for
Christmas presents. If you ship in January, chances are your competitors have won all
the market for the season.
When you get your software not only running but you deliver it in the hands of your
customer is fundamental. And it’s getting more and more necessary to shorten this time
to market. Ask yourself: Can I give it in the hands of my customer right away and or how
long do I need to wait before somebody can actually use it?
Why is it important that they start using it? because you can start learning from them.
Either because you ask them for feedback, or they come asking you for new features, or
you just sit back and observe them.
What if shortening your time to market means you do not write any code at all? What if
you just rapidly prototype, combine and compose products out of existing reusable com-
ponents? What if you ship user interface mockups with fake buttons, and only implement
their behavior if you detect users who try to click on them? If nobody ever clicks, then
why should you spend time writing code that will never run?
When you start a new project, your goal should be to ship an MVP: minimum viable
product, or a walking skeleton. Definitely something not perfect. If you are perfectionist,
you should look for a different line of work. Perfect software tends to have an infinite time
to market, because you can always tweak it or polish it just a little bit more before giving
it to your users.
Another way to be fast is to improve your architectural decision making processes.
And the fastest way to make decisions is to make them by yourself, so if you need to have
a group of people that need to meet and they have to come to a democratic consensus
and they have to have an agreement and everybody’s voice has to be heard as everybody
voices their suggestions which should be somehow included or crammed into the design.
Not only this takes time, but the complexity of what you’re designing will explode and
the time to get there will also increase.
To keep your time to market under control, it’s better if you can get the right person to
make the right decisions by themselves at the earliest or latest possible right time.
47

Modularity
Is there a structural decomposition of the architecture?
Prerequisite for: Code Reuse, Separate Compilation, Incremental Build, Distributed
Deployment, Separation of Concerns, Dependency Management, Parallel Development in Larger
Teams
Programming Languages with modules:
Ada, Algol, COBOL, Dart, Erlang, Fortran, Go, Haskell, Java, Modula, Oberon, Objective-C, Perl,
Python, Ruby
Programming languages without modules:
C, C++, JavaScript
Modularisation is a design issue, not a language issue (David Parnas)
This is the first example of internal quality, helping you to observe the structure of
your architecture.
If your system has a single box, there is only one element, only one module, then you
have a monolithic system, also called the big ball of mud. You will have no chances to,
for example, reuse existing code: there is only one element, either you reuse the whole
thing or you rewrite the whole thing.
It takes time to compile a large program, so splitting it into modules helps to separately
compile them and incrementally rebuild only the parts that have changed. If you touch
one line of the monolith, you need to rebuild the whole thing.
If you have one module and you want to run it, you have to run it in one place.
There is no possibility, for example, to run the user interface on a mobile phone and
to run the back end in the cloud. It’s all in one piece: so either you download it all on
the phone or you upload it all into the cloud. Modularity is a prerequisite for distributed
deployment.
If you have one module and you have one developer, everything works fine. If you
have one component but you have 100 developers working on the same component, then
they will probably spend most of their time dealing with merge conflicts. To manage the
parallel construction of your software you need a modular architecture.
Modularity is so important that many programming languages have embedded it into
structural constructs, such as abstract data types, modules, packages, or namespaces.
Still, very few languages are aware of the architectural concept of software components.
The goal is to represent inter-dependent partitions of your code which can be developed
independently. At some point, to build the whole integrated system, separate compo-
nents will need to be composed and connected together.
Despite its importance, it is surprising that there are some famous languages that don’t
have explicit modularity constructs. But they’re still successful and widely used because
you can still do modular design, give a shape to the structure of your code, even if the
programming language doesn’t support it and has no way, for example, to manage mod-
ule dependencies or document module interfaces to make it easier to discover and reuse
them.
48

"Well used" software
Which car would you buy?
• Used by 1 user
• Used by 100'000 users
Once we have modules your goal is to be able to assemble them into a larger system,
but also to be able to amortize your investment into them by reuse.
Which software would you rather reuse? One that has never been used before? Or
another which has lots of existing users? When it comes to software reuse there is an
avalanche effect, the more it gets reused the more likely to be reused it becomes. But
the avalanche does not start easy for software that has never been used before. It’s very
difficult to start or bootstrap such virtuous cycle.
49

Reusability
Can we use this software many times for different purposes?
• Reuse Mechanism: Fork (duplication) vs. Reference
(dependencies)
• Origin: Internal vs. External (Not Invented Here Syndrome)
• Scope: General-purpose vs. Domain-speciѹc
• Pre-requisites for reuse: trusted "quality" components,
standardized and documented interfaces, marketplaces
It is often easier to write an incorrect program than to understand how to reuse a
correct one (Will Tracz, 1987)
When you want to reuse a piece of software you have two fundamentally different ways
to do it.
The first is copy’n’paste. Every developer does it. Another way to call this is to “fork it
on GitHub”.
You have a project in which you have initially developed a component. And then you
clone it and carry it over to the next project where you reuse it. What’s the disadvantage
of doing reuse by copy’n’paste? What’s the problem? You make a replica: you have been
duplicating code. You have been duplicating bugs. And if you fix them in one place, they
still need to be fixed in the other copies.
If you do the fork on GitHub then you can have pull requests or you can try to synchro-
nize so it’s not so bad, but there is this problem due to the redundant clones.
How to do software reuse without this problem? We don’t duplicate. We call. We refer
to. We define shared dependencies so there is only one copy to be maintained well and
then you refer to it by importing it in many other places.
What can possibly go wrong? Different parts change at different speeds.
Both strategies have benefits and disadvantages. So the advantage here is that if we
need to do an improvement we can fix everything at once, and the same is also a dis-
advantage. If we make an improvement which is not compatible. Then we break all the
dependencies. This doesn’t happen when you fork because you decide whether you want
to pull the changes or not. If you don’t pull the changes you don’t get the fixes. But you
can avoid pulling breaking changes.
Fork or dependencies: this is a fundamental decision that you need to make if you want
to bring reusability into your architecture.
When you decide to reuse software, another big question concerns where does it come
from? Do you intend to reuse your own code? Every developer over time develops a
golden chest of treasures, code very nicely done brought along from project to project.
And this is your own code. You wrote it yourself. Maybe in a previous project. You know
it. You know it exists. You know because you wrote it, maybe you documented for yourself
and you don’t hesitate before you take it and reuse it.
50

Scale this concept to the whole organization. My team did it. We already have a lot
of experience with it. We don’t have a problem with using it. The difficulty is when you
are moving from your own closed world to an open world where sources of code meant
for reuse can be external vendors: are those components as good as they claim to be?
Do we trust the external service provider? Can we integrate them without any adverse
consequences?
What if we picked a very popular package. The maintainer doesn’t have any time any-
more to keep updating it. Somebody else takes over and then they take advantage of
the fact that these are popular packages everybody is using them. And they inject some
improvement which is actually doing something malicious and then the next time you
upgrade you get a virus, you get a back door, you get a sniffer that is stealing credit card
numbers.
This is code that is externally sourced: who knows what’s in there? And this is even
more so if you don’t have access to the source code. If you do reuse, but not the source
code level, but you do it at the binary level, you should be even more careful.
If you think about the purpose of the software, we can reuse general-purpose software,
for example, a database. Databases help us to store information persistently. We need
it in most systems. Let’s reuse a database. Of course, you need to choose what kind of
database out of many different ones. If you are somebody that builds databases, you can
sell them to customers from different application domains: finance, health, transporta-
tion, doesn’t matter. The database can store any kind of data. But also it doesn’t know
how to store by itself anything. You have to define the scheme. You have to write the
queries. You have to take the general database and specialize it to your domain.
The alternative we can consider is reusing within the same domain. We say we have
a very successful library, like a game engine. This is still very general, but its scope of
applicability is limited within a particular domain.
So this is a choice that you make: the more specific your software, the less potential
users. But these users will be pleased to reuse your solution because it fits with their
domain, it does exactly what they want.
The moment that you want to establish an industrial approach to software develop-
ment, you need to have reusable components and you have to assess and trust the qual-
ity of such components. You cannot afford to keep following the “not invented here“
syndrome.
The points in which your software comes into contact with external, reusable software
are fundamental. The points of contact between the external components and your sys-
tem are called the interface. Interfaces of reusable components have to be documented.
They have to be standardized so that it’s easy to connect them together. And we also
need to have a way to discover these components. Where do you find them? So there has
to be a marketplace, there has to be a repository and this could be a way for people to
sell components and people to buy components. This used to be one of the Holy Grails
of software engineering and when the field was born in 1968, D. McIlroy mentioned soft-
ware would become a serious engineering discipline and software would grow from craft
to industry unless there will be a market of reusable components where people can go
make money by not selling entire application to their end users but by selling parts that
can be assembled into larger systems. It’s been a while since back then and I am happy
to say such reusable software components do exist now. They are everywhere. Making a
living by writing and selling reusable software components is possible.
51

Design Consistency
What's the design's conceptual integrity and coherence?
Understanding a part helps to understand the whole
Avoid unexpected surprises:
• Pick a naming convention
• Follow the architectural style constraints
Know the rules (and when to break them)
It is better to have a system reѺect one set of design ideas, than to have one that
contains many good but independent and uncoordinated ideas (Fred Brooks, 1995)
After looking at the structure, whether it’s modular and if the modules can be reused,
we can observe the overall consistency of the design.
Why is having a consistent design important? Because if you have a large but consis-
tently designed architecture, if you understand how a part of the system is designed you
can take that knowledge and apply it everywhere else.
One example can be naming conventions, once you learn the convention you can ex-
pect to read code following it everywhere. If this is not the case, then every module that
you read, every file that you open, will be different, and it will be much more expensive
and time consuming to understand it.
When we talk about architecture we will talk about design decisions. We can also talk
about constraints over these design decisions as we talk about architectural styles. But if
you claim that your architecture follows a style, then you expect a certain level of consis-
tency because the style is guiding you by constraining the way you make the decisions.
There can be some exceptions. But you need to be aware of them and before you break
the rules, you need to know what the rules are. Otherwise, you get a random and totally
inconsistent design.
You can infer the experience of the designer just by looking at the consistency of their
designs. If the design doesn’t have any consistency probably there’s not a lot of experi-
ence just as if the design is totally consistent. But when in a few places, the right places,
there are some variations, there are good reasons for introducing those: that’s the mark
of an experienced designer.
And there are so many good architectural styles. There are so many good ideas, pat-
terns, and design concepts. We want to adopt them all. Maybe this is your experience as
students of design patterns. You learn a lot of patterns and then the next time you do a
project you want to use all of them. And then you don’t get a consistent design because
you just put too much stuff into it. And so it’s better to have the system use a few good
patterns that make sense as opposed to just trying to reuse everything you know. It’s
very easy, in other words, to over-engineer your design and lose consistency. Learn to be
selective. Learn how to recognize which tools are necessary. Learn to pick which tools
are sufficient.
52

Simplicity
What's the complexity of the design?
• A simple solution for a complex problem
• One general solution vs. many speciѹc solutions:
• Lack of duplication
• Minimal variability
• Conciseness
• Resist changes that compromise simplicity
• Refactor to simplify
As simple as possible, but not simpler (Albert Einstein)
How simple is your solution? It’s great to strive to have simple solutions for complex
problems. Or is complexity unavoidable? Among two alternative ways, pick the simplest
one. Sometimes you will be surprised how simple the solution would be, but how hard it
is to find it. So if you are pressed for time, chances are that your design will not be the
simplest possible one.
How can you simplify? Look for the essence. Abstract and make your solution gen-
eral. Or make it super specific, knowing that it will work for only a simple case. But
that’s what you need anyway. Sometimes you can get simplicity because you do not re-
peat yourself, DRY solutions are simpler because they lack redundant parts. The code is
smaller, concise, compressed one may say. The code takes less time and effort to write af-
ter you invested in simplifying its design. Avoiding duplication helps to keep your design
consistent when you evolve it.
There is a point in the evolution of the system in which you achieve a simple version
1.0. You won your fight against complexity and you have released a nice clean solution.
Congratulations. Don’t let your guard down. Entropy and complexity will start to grow
again as change creeps in.
Refactoring (e.g., factoring out commonality, building layers of powerful abstractions)
is what can help you simplify your code.
There is also the problem of over-simplification. The opposite of over-engineering. So
don’t make it too simple. That’s easy to say, but also difficult to achieve.
53

Complexity
What is the primary source of complexity?
• The number of components of the architecture
• The amount of connections between the components
Say we have a modular architecture. Where does its complexity come from? Does it
get more complex because you have a larger and larger number of different modules? Or
does it depend on the number of possible connections between them?
Complication (or size) grows linearly, while complexity grows quadratically.
Imagine you have a developer for each component. If every component is interdepen-
dent with every other component, all developers need to talk to one another all the time
to understand how each component works and to coordinate how they can be changed.
How large can your architecture grow? The larger the coordination meeting, the longer
it takes and less time remains available for coding.
Your job as an architect not only to draw the boxes and arrows, but to cut the unwanted
or unnecessary edges to keep complexity under control. If you prevent these connections
from happening, you will keep modules independent, loosen their coupling and keep
developers from attending too many meetings.
54

Clarity
Is the design easy to understand?
A clear architecture distills the most essential aspects into simple
primitive elements that can be combined to solve the important
problems of the system
Freedom from ambiguity and irrelevant details
Deѹnitive, precise, explicit and undisputed decisions
Opposite: Clutter, Confusion, Obscurity
Architecture represents the system so that different people can talk about it. How easy
is it to understand what people are talking about? How clear is their view over the details?
Do they easily reach a shared understanding?
If you hand an unclear architecture to developers, they will be confused. They will not
know what code to write, how the solution is supposed to work, or even worse, which
problem they are supposed to solve.
Clarity can be enhanced by simplicity, but even complexity needs to be clearly de-
scribed.
As an architect, you make a decision. It has to be clear cut, you say: black or white;
yes or no, you cannot say maybe. Unclear decisions breed ambiguity, which leads to con-
flicting interpretations. Some of the developers will do it in one way, the others in the
opposite way. So you get an inconsistent implementation riddled with conflicting deci-
sions.
Learn how to make clear decisions; explain decisions clearly and don’t put irrelevant
details or pick from ambiguous options. This is something that we will emphasize when
you work on modeling. We will look at your architectural models and we will try to assess
their clarity.
Notations help a lot to avoid fuzzy sketches. Learn how to use them well. If you use a
standard notation to represent your architecture, you clearly express the meaning behind
what you draw and people should be able to understand it more clearly.
55

Stability
How likely to change is your design?
Unstable
Stable
Prototype
Product
Implementation
Interface
Likely to break clients
Platform to build upon
Experimental spike, throw-
away code
Worthy of further investment: building, testing,
documenting
The design process is something that progresses over time. You incept the original
concept, refine it, modify it, extend it until you reach a point in which you stop. Your
architecture has reached a stable state. Then you can move on to the implementation
phase.
There are different types of software artifacts to distinguish whether they are stable or
not. For example, if you hear the word “prototype” you know that this code may work
today but might not continue to work tomorrow and it definitely shouldn’t be used in
production. It is important to set the expectation about stability with users or developers
who may depend on the particular artifact.
Also, which elements of an architecture are supposed to be stable? The point of con-
tact at the interfaces between components. Interfaces should be very stable while the
internals of the components can be a bit more unstable. You can easily change the im-
plementation as long as you don’t touch the interface because the change stays confined
within the component and the other components – as they only depend on the stable
interfaces – are not affected.
Platforms are meant to give a stable foundation on which developers build applica-
tions. What if you write your application and it will stop working if you upgrade the
compiler for the language you used to write it?
If you write code that is unstable – a spike, or a quick and dirty prototype – you want
to learn something from it and then throw it away.
If you have a product that is stable, then it’s worth to: freeze its interfaces, write the
documentation, train users, write automated tests for it.
Unstable user interfaces have the same issues. Every improvement will impact users
who may get confused, may need to be retrained to relearn how to do things they already
spent time figuring out how to do with a previous version.
56

Composability
How easy is it to assemble the architecture from its constituent parts?
• Assuming all components are ready, putting them together is
fast, cheap and easy
• Cost(Composition) < Cost(Components)
• Components can be easily recomposed in different ways
How easy it is to take all of your modules and assemble as components of a larger sys-
tem? We talk about a composable architecture when the effort that it takes it to assemble
it is less than the cost for actually building all of its parts.
How much effort does it take to replace an existing component? Can you just plug
the new one in and everything will still run? Or does it take hours of configuration?
Or a month-long high-risk integration project, where you end up developing your own
middleware connectors?
Even if individual components may still be expensive to build, the consequence of a
composable architecture is that their composition is supposed to be cheap. And some-
times not only components are meant to be reusable, but you can also start reusing the
glue in between them.
57

Deployability
How difѹcult is it to deploy the system in production?
Hard
Easy
Manual Release
Automated Release
Scheduled Updates
Continuous Updates
Unplanned Downtime
Planned or No Downtime
Wait for Dependencies
No synchronization
Changes cannot be undone
Rollback Possible
We are now making the transition from design to run time. We have a design, we have
the code. We are making a release so that we can deploy it and people can start to use the
newly released version. This deployment process can be manual or automatic. You can
have a continuous integration and delivery pipeline: I push into the repository, there is
something that checks out the code, compiles it, builds it, tests it. If everything is green,
even automatically pushes it out to the production environment. Maybe somebody has
to check and sign off before the golden master can be shipped and take responsibility for
it.
If deployment is automatic, it will be a repeatable, deterministic process: it will always
have the same output given the same input. If it is manual, then it will depend on who
does it, what they had for breakfast and maybe sometimes it works sometimes it doesn’t,
and the release unexpectedly fails.
How often do you ship a new release? Once a year or 356 times per day? Are you making
very small, incremental but continuous changes? Or you ship a whole new system every
once in a long while. Earlier on, it was a big deal to make a release and it would happen
unfrequently. Look at the history of most operating systems. Nowadays, releases are
getting deployed much more frequently. If you sell mobile apps, the bottleneck is how
long does it take for the app store to check and publish your updates. If you provide a
Web service and you have control over your data center infrastructure, you can open a
stream of continuous fixes and updates.
Why is it better to deploy continuously? because you write and put in the hands of your
customers every improvement as soon as possible. It takes time to debug and write the
fix. It should not take time to ship the new version so that it lands on your users’devices.
There are also risks when you deploy changes. Can you depend on a system which is
continuously changing under your feet? Is it really always getting better? Do all users
always need or like all changes? And what if you’re driving a car and in the middle of the
highway they make a new release of the auto-pilot software? How safe is to patch a live
system?
When you deploy, are the users going to notice? What if you have to do a payment,
you open to the e-banking website and you read: ’Sorry we are down while deploying an
upgrade. It will be marvellous when it starts working. Come back tomorrow when we are
back up and running again.’ Deployment is not only a transition from design time to run
time, but also a transition from old (and broken) to new (and better). It’s introducing a
58

change of the system that affects the users both while the change is being put into place
but also afterwards. If downtime is planned, your users can expect it. For example, don’t
assume certain services work at night or during weekends. But if it’s unplanned and the
very moment you need the system it’s not available because it’s being upgraded, this is
not really good in terms of deployability. You can annoy your users by simply politely
asking them if they would like to reboot their computer while they are watching a movie,
or giving a live presentation.
Now, when you make a change and you want to ship the new release and you have
multiple components. Can you change one without asking the others for permission?
Or do you need to wait until everybody is ready to ship and then you do a big release at
the same time? If you need to coordinate the release is much more complicated because
everybody has to be ready to give you the green light and this usually doesn’t happen
at the same time. If you can ship everything independently, you can do releases much
faster.
What happens if the deployment goes wrong? What happens if you ship a bad release,
you install it, and then you start getting all kinds of problems. Can you click on undo?
Can you go back to the previous version? If you can’t. Then your system is not as (un-
)deployable as one that can be rolled back with ease.
When you install software, you should treat it like a database transaction that can be
aborted or committed. And can also be rolled back. Installation of software packages is
for some reason not always like that. Sometimes you install it and you will never be able
to disentangle it from the rest of its environment unless you throw away and reinstall the
whole system from scratch. The ultimate failed deployment leaves you with a brick or an
expensive doorstop.
59

Normal Operation
• Performance
• Scalability
• Capacity
• Usability
• Ease of Support
• Serviceability
• Visibility
After deployment, during operation we start to use the system. The system is running.
Let’s look at the most important quality attributes during normal operation.
60

Performance
How timely are the external interactions of the system?
• Latency
• Communication/Computation Delay
• User-Perceived: First Response vs. Completion Time
• Throughput
• Computation: Number of Requests/Time
• Communication: Limited by Bandwidth (Data/Time)
Everybody has some experience and can give some examples of software that has a
good performance or poor performance. It either meets or exceeds their expectations for
how long an action should take, or it is so slow that they wonder if it will ever get it done.
From the user’s perspective, performance can be observed in terms of response time: the
delay between the last input and the first corresponding output. I send a request. I click
a button and wait until I see the output. I have received a response with a result and it
took time to compute it. So the latency of the response can be short and then we have a
good performance. If the latency is extremely long, at some point we will start to suspect
that maybe the system is never going to answer, then we need a way to entertain the
user, show a spinner, flash a colorful progress bar, preview partial approximate results,
and check that the job is still alive. Ideally, we can give an estimate of when the output
will arrive. It’s all about expectation management.
The latency is both affected by the processing time and by the communication time.
With a distributed deployment, network latency has an impact. Systems deployed in
different data centers in the cloud will behave differently depending on where the users
are with their mobile phones. You might choose a data center closer to them to improve
the performance. This has nothing to do with the software itself; it concerns the choice
on where you deploy it in relationship with the users location.
From the service provider perspective, we can also observe performance in terms of
how many customers a system can handle per unit of time. This is different than the time
it takes for individual interactions. Maybe those are slow, individually, but it is possible
to execute many of them concurrently. And this is what we call the throughput, so we
can measure it for computational tasks. We can measure how many clients and how many
requests can your system process over time? If we have a multicore computer, we can run
computations in parallel threads, and get better throughput. Regarding communication,
this is usually measured in terms of bandwidth. How many kilobits, megabits or gigabits
can you send or receive in one second?
These are all different types of indicators that can help you to define, measure, predict
and compare the performance of your architecture.
61

Scalability
Is the performance guaranteed with an increasing workload?
• Architecture designed for growth:
• client requests (throughput)
• number of users (concurrency)
• amount of data (input/output)
• number of nodes (network size)
• number of software components (system size)
by taking advantage of additional resources
• Scalability is limited by the maximum capacity of the system
• Software systems are expected to handle workload variations of 3-10 orders of magnitude
over short/large periods
Related with performance, scalability is a function of how performance is affected by
changes in workload or in the resources allocated to run your architecture. You may have
an incredible performance as long as you use the system only by yourself. The moment
additional users show up, the performance degrades noticeably.
Scalability by itself is a meaningless term. Since scalability is a function, you need to
specify which variable the performance depends on. Scalability in terms of the workload
may depend on the number of concurrent requests, the number of concurrent clients or
users, the amount of data sent as part of each request. Scalability in terms of resources
may depend on the number of available CPU cores, the amount of memory, the amount of
storage. Will your computation scale to fill up a supercomputer? Will your architecture
store and process big amounts of data?
Regarding its internal structure, an architecture may need to scale to use a large num-
ber of components. Your development process may need to scale to involve thousands
of developers.
In general, you have a system that scales because when the workload grows, you can
proportionally increase the resources, but the performance stays the same. The system
doesn’t scale if when you increase the workload or you increase the resources, the per-
formance degrades.
What if you had infinite resources? Do you get instantaneous results? Maybe if you’re
willing to pay for them? There is a limit in the amount of resources that you can have
and this limit is what we call the capacity of your system. We can see a scalable system
as a system that can fill up the available capacity. It’s impossible to scale beyond it, if the
workload grows the system is first saturated and then it overloads.
If you look at the workloads that software usually is expected to work with, they can
62

change between 3 to 10 orders of magnitude. Billions of users. Petabytes of storage. So
it’s important to be aware of what kind of variations to expect during the lifetime of your
architecture.
If you expect a small variation you will design it in a certain way; if you expect a huge
variation the design will need to be different. The advice is not to over-engineer your
system for scalability when you don’t need it. Maybe some system needs to grow and
scale beyond the initial workloads, but not all systems will go viral overnight and die of
their own success unless they can figure out how to scale.
63

Capacity
How much work can the system perform?
• Capacity: Maximum achievable throughput without violating
latency requirements
• Utilization: Percentage of time a system is busy
• Saturation: Full utilization, no spare capacity
• Overload: Beyond saturation, performance degradation,
instability
• Ensure that there is always some spare capacity
Capacity gives us an idea of how much work we can get out of the system. This is the
maximum throughput achievable without violating the latency expectations of individ-
ual requests.
There are only so many requests/second we can perform without having the queue of
incoming traffic get longer and longer. If to service the workload you are running at 100%
utilization of your processing or storage resources, you are running at the limit and enter-
ing saturation where there is no slack left. There is no spare capacity. If your workload
goes beyond saturation, then you overload the system. This is when performance will
suffer, clients will randomly disconnect after the network protocols time out or worse,
they will start hitting their refresh buttons, thus further increasing the workload.
The concept of slack we have seen for project management is also applicable to devops.
We can adopt this good idea during operations: avoid running with your workload match-
ing the exact capacity that you have. If, for some reason, one more user than expected
shows up, you and everyone else will notice.
64

Measuring Normal Operation Qualities
Results are displayed
1 second
after users submit their input
The system can process messages sent by
1M concurrent clients
After
1 hour
of initial training, users are already productive
Last Friday the workload reached
1000 requests/second
Usability
Is the user interface intuitive and convenient to use?
• Learnability (ѹrst time users)
• Memorability (returning users)
• Efѹciency (expert users)
• Satisfaction (all users)
• Accessibility
• Internationalization
Beyond performance, scalability and capacity, another externally visible quality con-
cerns the interaction between users and the system’s user interface. To assess this qual-
ity, you need to think about who are the target users of your system. Which expectations
do we have to meet? This can be unpacked looking at the ability of users to learn how
65

to use the system. Make the user interface intuitive, so that beginners can be produc-
tive without training. At the same time, the user interface should not get in the way of
experts. How do you give a path to discover the features of your system? Welcome new
users so that they have a good experience and come back. Make them do the journey from
first time user to expert quickly. If they come back, will they remember or need to be re-
minded with subtle cues on how to pick up your user interface again? Expert users by
definition don’t have problem learning or remembering, but they have a problem being
efficient. And typically need shortcuts, a fast track to get the job done.
In general, are users satisfied after using your system? There is a standard system
usability scale which can help to measure that, which requires however explicitly asking
users. This means users need an incentive to provide you with feedback beyond their
sheer happiness or hate for the system.
There are also users with special needs, and Accessibility is the specific usability quality
to check if your user interface can support them. Internationalization is also important
for supporting people speaking different languages. Do you need to rebuild your app
to switch its display language? These qualities might be ignored with early releases but
need to be considered once the user population grows worldwide.
66

Ease of Support
Can users be effectively helped in case of problems?
Hard
Easy
Cryptic Error Messages
Self-Correcting Errors
Heisen-bugs
Reproducible Bugs
Unknown Conѹguration
Remotely Visible Conѹguration
No Error Logs
Stack Traces in Debug Logs
User in the Loop
Remote Screen
Intuitiveness and convenience help until users attempting to use your application get
into trouble. Something goes wrong and all users get is a cryptic error message: Error 25.
How are they supposed to understand what they did wrong? How can they correct their
mistake by themselves? How can they get help?
Ideally, when something goes wrong your system should be designed in a way to even
self-correct the problem. Users should not notice, that’s the idea of a robust, fault toler-
ant, or self-healing design.
You try to open a file to edit it. The file doesn’t exist: do you get an error? or do you
create the file? What’s the best design? Better to empower people to solve problems by
themselves.
A big challenge for supporting users is due to hard-to-reproduce issues: Heisen-bugs,
the ones which disappear when the user calls support for help, and reappear right after
hanging up the phone.
To reproduce a bug, you need to know not just the input, but also the configuration of
the container or specific runtime environment. Where was the system deployed? How is
it configured? Does an ordinary user know that? And how can support personnel access
the information? Without it, there is no context for troubleshooting. Is there just a
cryptic error message popup? or is there an error log with user accounts, stack traces,
IP addresses and timestamps? Leaking stack traces or other details can also be a security
issue, since it reveals internal details which are necessary for supporting users in trouble
but can also open up an attack vector.
Is it easier to land a pilot-less plane by directing some random passenger over the radio
or by remotely taking over the controls? Having users in the loop means that the user
is helping you to debug. This can be great for the heisenbugs problems but it can be
a problem because the user is also misunderstanding what you tell them and you can
just indirectly observe the screen through their eyes. If you have remote access into the
system, you can even type yourself commands to fix the problem or to reproduce the
bug. This may be not always possible, as it happened to that consultant who ended up in
a security-conscious company, and could only tell employees what to type, but was not
legally allowed direct access to the terminal’s keyboard. This is a particularly expensive
way to support your users.
67

Serviceability
How convenient is the ordinary maintenance of the system?
Hard
Easy
Complete Operational Stop
Service Running System
Reboot to upgrade
Transparent upgrade
Install Wizard
Unattended Installation Script
Restart to apply conѹguration change
Hot/live conѹguration
Manual Bug Reports
Automatic Crash Report
If we switch to the internal perspective, during normal operation we need to maintain
the system: do whatever it takes to keep it running. For example, update or upgrade a
component or its execution environment. Sometimes your Mac shows you a pop up where
you read: “to update please click to reboot”. How many of you click it right away, and how
many of you postpone it until tomorrow, every time? It would be much better (both to
keep a system up to date and for reducing user distruption) if system improvements are
delivered without the user noticing and applied on the fly while the system is running.
To prepare a new machine for service, sometimes you have to install software on it.
How many of you are using “brew”? Why are you doing that? Because it is so much
straightforward to get software installed from the command line as opposed to drag and
drop or filling out wizard forms and watching progress bars. if you have to install some-
thing once on your personal machine, you might be willing to go through the effort, but
if you have to prepare all the computers for the whole University, are you going to pay a
person to sit and click through point and click wizards for days?
What if you are tuning a system’s performance by changing some configuration pa-
rameters. Is it better if you have to reboot it after every change? or if you just change a
few parameters and the system picks the new values up without any restart?
When something goes wrong, do you need to have a trusted person to give you a good
bug report, featuring screenshots and a step by step guide on reproducing the issue? or do
you just have an automatic crash detect and report bot, which sends to the developers all
the necessary stack and heap memory dumps and input/output logs so they have enough
information to be able to fix it?
While planned outages and service downtimes were acceptable with local users tak-
ing weekend offs, since the maintenance work could be scheduled during weekends or
at night, with global users expecting 24/7 availability, it gets more challenging to get
serviceability.
68

Visibility
Is it possible to monitor runtime events and interactions?
To which extent the system behavior and internal state can be
observed during operation?
Are there logs to debug, detect errors or audit the system in
production?
Is the system self-aware?
Process Visibility: can the progress of the project be measured
and tracked?
We see in order to move; we move in order to see. (William Gibson)
When it comes to being able to observe what the system is doing while it’s running, we
need to be able to go beyond its external input/output interface. To get visibility into the
system we need to open up the black box and look inside.
Can developers observe the internal state of the system, or trace what the system is
doing while it’s running and processing our requests? Are there some normal operating
logs? What about error logs? Can users monitor and track their usage, e.g., especially if
they get billed on a pay-per-use basis?
Basic visibility is one-way information flow from the inside to interested parties out-
side. What if this flow could be fed back to the system itself? Visibility is the pre-requisite
for self-awareness: if you know you are running out of battery, you can kill non-essential
tasks. If you detect a workload peak that brings you into overload, you can automatically
provision additional resources to add capacity for absorbing the peak and free them once
they are no longer needed. Doing this requires to be able to observe the consumption of
resources and manage their internal allocation.
We also talk about visibility in the context of project management, when tracking the
progress of the project. You can observe commit logs, count issues in the backlog, spot
trends concerning failing tests, and even try out preview releases. Aggregating these
statistics helps to track the speed of a project or predict whether it will be late.
Good visibility also helps when something goes wrong and you need to detect faults or
deal with the consequences of failures.
69

Dependability Qualities
• Availability
• Reliability
• Recoverability
• Safety
• Security
What could possibly go wrong? Will your software still work if you flip one random
bit of its code? what if the data gets corrupted or lost? Faults can originate from within,
but also be due to external problems (e.g., failed dependencies, network communication
outages) affecting our system. Was the lost packet carrying an alarm about someone’s
wearable heart monitoring sensor? or just an unimportant frame of a social media video?
How much we can depend on our system? Can a life depend on your software? If so,
when something goes wrong, someone may die. Or maybe it’s just a little bit of digital
money that gets locked forever inside a smart contract. Maybe all you have to lose is a
little bit of your precious time, just a nuisance.
Dependability is a very broad quality and we can decompose it into availability and
reliability; in how easy it is to recover normal operation when things go wrong (recover-
ability); safety and robustness, as well as security, are considered part of dependability.
70

Reliability
How long can the system keep running?
• MTBF - Mean Time Between Failures
• MTTF - Mean Time To Failure
Recoverability
How long does it take to repair the system?
• MTTR - Mean Time to Recovery
• MTTR - Mean Time to Repair
• MTTR - Mean Time to Respond
If the system is running ok now, for how long is it still going to run? We need to use
the system from now until the future for as long as possible. So we rely on the system to
work, to be there for us.
We distinguish 2 different time metrics to quantify reliability: the time until the next
failure and the mean time between failures.
To understand reliability, think about these two states: up and down. Up means every-
thing is fine: the system is working. Down happens when a failure occurs. That’s where
the system is no longer working.
How much time do we spend between these 2 failure events? If these are far apart, a
system is more reliable than if it would crash every hour. What if you don’t know when
the last failure manifested itself? Then you can look into the future and worry about how
long will it take until the next failure. What happens when you unbox a new phone or a
new laptop. What are your expectations about its reliability? Will it break as soon as the
warranty expires?
We distinguish MTTF from MTBF because it is important to consider not just the transi-
tion from green to red, up to down, working ok to failure, but also hopefully the opposite
direction: coming back up after recovery. Only biological systems fail once and then
they’re dead and unfortunately there is no way to bring them back. Artificial systems fail
and then hopefully there is a possibility to recover them. Once something fails, first, you
need to be able to observe and detect the fact that it has failed. Maybe the root-cause and
the fault triggering a complex chain of events happened earlier, but there is a moment in
which its ultimate consequence visibly breaks the system, which has failed.
This is when we can start our attempt to recover. What is the classical and universal
recovery strategy across computer science? Reboot it. How long does it take? Depending
on the operating system rebooting may not be instantaneous.
What if recovery requires something more complex than a simple reboot?
71

Before you can recover something you have to fix the actual cause of the problem, you
have to repair it and before you can perform the actual repair, you need to know who
is the right person to call. So until systems get fixed by themselves, we need to get the
right person at the right place so they can start typing the right commands on the right
keyboard.
Before you recover you have to repair it and before repairing it, you have to respond.
Sometimes you have support organizations that will guarantee that when something goes
wrong they will respond within a contractually specified number of minutes. From the
time they are called and notified of the problem, there will be a person physically present
to start the repair process. But this just a really first response which doesn’t guarantee
that the problem is solved right away. It just states that somebody will be working on it.
And before you can respond you need to know who to call. And what if this person is on
vacation, quarantined or simply unavailable? Your “down time” just got longer.
Depending on what went wrong it may not be trivial to fix it. If a backup plan or re-
dundant part has been foreseen in advance, it may be a good time to fail over while the
original component is being worked on. If there is no slack, then your recoverability track
record just got worse.
72

Availability
How likely is it that the system is functioning correctly?
Availability and Reliability
• Availability = MTTF / (MTTF + MTTR)
Availability and Downtime
• Availability = (Ttotal - Tdown) / Ttotal
Availability
Downtime (1 Year)
99%
3.65 days
99.9%
8.76 hours
99.99%
53 minutes
99.999%
5.26 minutes
99.9999%
31.5 seconds
What’s the difference between reliability and availability?
The system is on, but how can you be sure that it’s working properly? How do you
measure something that is uncertain? If you have a reference point for how long it takes
for the system to respond to a specific probe, we could try probing it, sending a request.
After asking a system to do something, how long will it take to complete the operation?
How do you know that it will actually finish eventually? What if it never responds? Maybe
it doesn’t matter if an answer will actually materialize. All it matters is how long you are
willing to wait for it.
When you reach a point in which you say: “I’m not willing to wait anymore”. That’s
when the system is not available.
How do you deal with such uncertainty? What’s the probability for the system to give
an answer within 1 second? 100%. Sure. Well, that depends on the input, it depends on
whether it is about to fail. Maybe you don’t get an answer after all.
We call availability, the probability for the system to successfully process and provide
a timely response to your input at the time you actually need it.
How can we measure how likely or what’s the probability that the system is functioning
correctly: it gives me a correct answer within the time that I’m willing to wait for it.
We can use the relationship between availability and reliability. If the system is failing,
when it is down, it’s not available. So we can divide the mean time to failure (MTTF) by
the total time, which also includes the time to recover (TTR).
If it takes 0 seconds to recover, you have 100% availability because the moment that
you fail you immediately bounce back and you’re recovered again and then you don’t even
notice that it was unavailable. If it takes a non-zero time to recover you have a smaller
availability.
We can fudge the metric if we do not include as down time the planned outages, which
are expected by users, and only measure unplanned outages as actual down time.
73

What if you don’t consider this and just say we want to always be able to use the system
for one year, If something goes wrong, for how long is the system going to be unavailable
during that time?
Sometimes you see availability defined by counting how many nines. They say we have
5 nine availability: 99.999%. This means approx. 5 minute downtime over one year. It’s
actually quite challenging and expensive to add 1 nine. It takes redundancy, automation,
processes and still making sure that people don’t go on vacation at the wrong time. If
you have to call somebody at the beach to fix something, you already lost the critical 5
minutes.
All of these metrics: availability, time to respond, repair and recover are part of service
level agreement contracts and can be tied to financial penalties, not to mention the loss
of reputation and customer trust.
74

Measuring Availability and Reliability
The service level agreement states up to
1 hour
downtime per
1 month
, an availability of
99.861%
After we call support, they need to be there within
30 minutes
Rebooting the server takes
5 seconds
The uptime of our oldest server has reached
4 years
Safe
Robust
Secure
Is damage prevented during
erroneous use outside the
operating range?
Is damage prevented during use
within the operating range?
Is damage prevented during
intentional/hostile use outside the
operating range?
B. Meyer
75

Before we go into the security discussion, we should mention this important definition
about the safety of a system. And to understand it, you need to consider the type the in-
put sent to the system. Whether you are sending valid input to a system that is running in
normal conditions, then the system shouldn’t fail. It’s safe if no humans come to harm
under these conditions. What if we are sending wrong, invalid, malformed input to a
system that is failing or misconfigured? Then it’s not about safety, but about robustness,
which means that the system can tolerate being used in the wrong way. Unintended er-
roneous use happens all the time. That’s why dangerous editing operations come with an
undo option. That’s why you shouldn’t be able to put your car gear in reverse when driv-
ing down the highway. That’s why a cat walking on the keyboard of the control panel of
a nuclear plant should not cause a meltdown. If the system is robust, there is no damage
when misusing the system unintentionally.
The difference between a robust system and a secure one is due to the intention of the
user. Is it just an honest mistake due to incompetence? or is the incorrect input sent as
a part of an attack? Like when someone is trying to trick your server to remotely execute
the content of a message whose size overflows the buffer allocated to receive it.
When you design your system, it has to be safe - minimize the risk that users trained
to work with it come to no harm. Then you can go to the next level and make it robust:
no damage even under random input. Security is the most challenging to achieve as
attackers are intentionally misusing your system and targeting its weaknesses.
76

Security
• Authentication
How to conѹrm the user's identity?
• Authorization
How to selectively restrict access to the system?
• Conѹdentiality
How to avoid unauthorized information disclosure?
• Integrity
How to protect data from tampering?
• Availability
How to withstand denial of service attacks?
We can decompose security into different aspects: we should know the user. We should
be able to authenticate the identity of the user. After we know who the user is, we need to
know whether the user is supposed to be doing what it’s doing right: Are they authorized?
To do so, you need to put a filter on top of the architecture, which tracks the user’s input
and checks whether the user is allowed or not.
Once the user is sharing information with the system, is this information going to leak
everywhere else? or does the system preserves the confidentiality of the input and as
well as the output? Is the output given only to the corresponding users on a need to
know basis?
How do we avoid that external attackers tamper with the information inside the sys-
tem? This is about guaranteeing the integrity of the storage, but also the communication:
when we transmit data, nobody should intercept it and modify it without us being able
to detect it.
Security is also connected with availability. A system can be overloaded because it is
very successful and everyone wants to use it. Or because it is undergoing a denial of
service attack, where malicious users flood it with requests so that ordinary users get
stuck in the queue and are no longer provided a timely service.
77

Defensibility
Is the system protected from attacks?
Survivability
Does the system survive the mission?
If you look into military applications, you need to worry about how defensible is your
system. If you design a system that is 99% secure and you connect it to the open Internet,
how long does it take for someone to hack into it? Sooner or later, it’s pretty much certain
that it will be hacked because all it takes is one point of weakness and the Internet is ready
to exploit it.
You need protection, a firewall. Maybe you build an air-gap defense where you don’t
have any network connectivity into your system. But it is still possible to monitor elec-
tromagnetic radiation and read what you display on your screen. It’s difficult to defend
electronic systems. If the blockchain cryptographic protocols are considered secure, the
focus of the attackers will shift to the exchanges and your wallet where you store your
private keys. How strong is your password? Do you remember to change it every 24
hours?
Survivability is another aspect. If you use your system as intended, how much sur-
vives the mission and can be used again later? If you launch a rocket: will it deliver the
payload and return to base? Or will the whole system get destroyed after a successful
mission? Going back to civilian applications: How long does your phone keep working?
For how many years can you keep installing apps on it? When does the manufacturer
stop supporting it with operating system updates? The choice of embedding planned ob-
solescence into a business model will also impact how your system survives its normal
usage. Usage which even if supposed to be as intended, will slowly cause your software
to rot.
78

Privacy
How to keep personal information secret?
Privacy
Good
Poor
Default
Opt-in
Opt-out
Purpose
Speciѹc, explicit
Generic, unknown
Tracking
None
Third-party Fingerprinting
Personal identiѹcation
Data anonymization
Data re-identiѹcation
Retention
Delete after use
Forever
Breach
Prompt Notiѹcation
Silent
Security should not be confused with privacy. Privacy is about personal information.
So you need to distinguish information that is made public because you want everybody
to know about it, from information that you should keep for yourself, or just shared with
a controlled set of people. Please note that a shared secret is no longer a secret: you can
allow someone to get a copy of your bits, but once the copy is out there, digital rights
management tools are somehow supposed to prevent those bits from further spreading
the information, but information wants to be free...
A quick example illustrating the difference between privacy and security: if you con-
sider cloud providers, they claim to be highly secure. Nobody else can get into your ac-
count and they have the massive size required to invest in hiring top talent for keeping
their data centers secure. To make use of their services, however, you send them a copy
of all your data. So your data is now in the cloud, stored right next to the one of your
competitors. All the systems processing your data run the cloud. They are in a secure
environment. But there is no privacy, because you have just released a copy of your per-
sonal or corporate information into an external environment. You gave away a copy to
the Cloud provider, who guarantees that nobody else can see it, but they can! So implic-
itly or explicitly you have to trust them not to violate your privacy.
This trust is represented by regulations such as GDPR, or privacy policies, or those
friendly cookie popups which are meant to provide informed consent for people to agree
with. They describe what is going to happen to the information once you put it into the
system: Is it going to stay private or not? Typically it is not, but you still have to ask the
people to give up their privacy, even if you just do it by default: if you come here I take
everything for free unless you opt out.
When you take information from people do you use it for one specific purpose only or
do you keep a copy around just in case? You share a picture of a fleeting moment with a
friend: a copy will be kept safely stored and digitally preserved in case the police needs it
a few years down the road. Or if you ride your e-scooter through a pedestrian zone, or set
your e-car’s cruise control above the speed limit, you will get an automatically generated
fine.
79

What kind of tracking do you do on the users: do you receive their input, you give them
the output and you forget the interaction ever took place? Or do you preserve detailed
logs of every transaction forever? Is this tracking outsourced to a third party? Are such
logs kept with personal information or are you going to strip them of the associated user
identity? There is a whole industry dedicated to harvesting supposedly anonymous usage
logs and reconstructing de-anonymized user profiles from them.
Finally, there is the retention issue. Once you put data into the system, does it stay
there forever? Or is it going to get deleted after a certain time? And when something
leaks, everything leaks eventually, do you tell the people that their emails or their credit
card numbers have been stolen or do you just stay silent and pretend it didn’t happen by
not sending out any breach notifications.
80

Change Qualities
What changes are expected in the future?
No Change: put it in Hardware
Software is expected to Change
Versioning
Before we discuss how easy or how difficult it is to change our software system, or to
evolve our software architecture, let’s discuss why change matters. Where does change
come from? Why software is supposed to change? On the one hand, functionality will
change, so the correctness is a moving target. Concerning completeness, if the system
is only partially correct, we have to complete it. Users come up with new ideas for more
and more features so we need to keep the user engaged with the system. We need to give
you more. The expectations for what software can do today have changed compared to
what software should have been able to achieve decades ago.
Another possible source of change: we have to fix bugs. A dangerous activity, because
fixing a bug may introduce more bugs. This gives a limit on whether such improvements
can be applied forever. Every change that we introduce is risky: because we go from a
known state, which might be flawed, but its flaws are known. As we make the change,
we jump into the unknown. Maybe it’s a better state, who knows?
Changes can be driven by users, but also be imposed by the technology platform on
which we depend on. How stable is this platform? Information Technology quickly evolves
following technology cycles, in which we make progress, and sometimes we run just to
stay in the same place. Hardware provides more performance so that software can waste
it spectacularly. There are new and improved ways in which we can interact with the
user. You used to type commands into physical terminals, then you can do the same in
a virtual terminal window, or type commands into a chatbot channel, or use your voice
to type the same commands. Keeping up with technology evolution, either requires to
fundamentally rewrite your software every decade or so, or to build layer upon emulation
layer to keep the semblance of a backwards compatible environment.
Ultimately, if you never expect the software to change then what’s the point of writing
software? Just burn it into the chip and do it in hardware. The hardware is expected not
to change as quickly as the software. But the main reason for doing it in software is that
you expect it to change, to grow or to adapt: in general to be flexible.
The main tool to control change is versioning. You make a version of the software.
Then you change it and therefore make a new version that includes the change. There is
a whole technology stack just be able to do so reliably and efficiently with the software
code. Never write any code without keeping track of it with a version control system.
There are many versioning schemes, some linear (with a counter), other non-linear (with
multiple counters: v1.0.1) to distinguish internal from externally visible changes. While
the original purpose of versioning is technical, it has also been exploited for marketing
purposes to establish a vintage (the 1995 edition) to put pressure on users to keep buying
the latest shiny software, again under the assumption that the newer the better. Version-
ing is also used for determining compatibility, whether changes break dependencies.
81

Flexibility
• Conѹgurability
• Customizability
• Modiѹability
• Extensibility
• Resilience
• Adaptability
• Elasticity
Flexibility is the most general quality.
Configurability, customizability, modifiability, and extensibiliy deal with how we can
satisfy changes of functional requirements, how we can keep keeping our users happy.
Changes can come and go, like the seasons. Resilience deals with temporary changes,
while we need to adapt to changes with a permanent nature: either we make the transi-
tion or we’re not going to be able to adapt and survive the change.
Elasticity is about changing the resources allocated to run the system to deal with work-
load changes.
In general, try to avoid one-way changes. It’s always good to be able to change without
the pressure of making irreversible changes. Rule number one of flexible software is: I
can always go back.
82

Conﬁgurability
Can architectural decisions be delayed until after deployment?
• Component Activation, Instantiation, Placement, Binding
• Resource Allocation
• Feature Toggle
Poor
Good
Better
Undocumented
conѹguration options
Documented
conѹguration options
Sensible defaults
provided
Hard-coded parameters
(rebuild to change)
Startup parameters
(restart to change)
Live parameters
(instant change)
The first thing you can do when you design your software and you want to make it easy
to change is to avoid changing it altogether. In other words, write code that is a bit more
general and it can be specialized by configuring it. As an architect, you make a decision
not to decide, so that as a developer, you have to write enough code to handle multiple
configurations, which will be actually chosen by whoever ends up with the pleasure of
configuring the system: the operator, the installer, the consultant, or even the brave end
user.
Configuring can be as simple as activating or deactivating which components get in-
stalled, or which features get switched on or off. We don’t make this selection in advance.
We leave it open and then somebody that knows how to activate the feature can go there
and do it if they actually need it.
The basic configuration may be accessible for free, but advanced configurations may
be expensive. It’s the same software. We just switch different parts on or off depending
on how much money you’re willing to pay.
Another important dimension that is usually very flexible concerns the idea of deploy-
ing the software in a certain execution environment. As long as the environment is com-
patible, the moment you deploy you have a choice: how powerful you need the execution
environment to be if the user needs to have a certain level of performance? The software
is the same, you just dramatically changed its performance by paying for faster hardware.
Every deployment can be done with the option to activate or deactivate features. You
can start by making a dark launch, where you have the feature implemented in the code,
but you don’t activate it. You check whether the system is stable enough. When you’re
confident the change that you introduced with the new feature doesn’t affect the stability
of the system, you can activate the toggle. Does the new feature work? If the newly
activated feature is problematic, you can switch off the toggle and revert back to the
stable configuration.
Let’s see some more concrete examples. Even if my system is very configurable, I just
don’t tell you how to do it. There are these hidden folders where you can find config-
uration files. Maybe you’re not supposed to know it. Just developers can understand
83

them. But if you know where to touch it, you can try to reconfigure it at your own risk.
Unsupported configuration changes may violate the warranty.
Sometimes configuration options are well documented so that the users are told in
which way that it is expected to be configured. Giving default values for every option is a
good idea to ensure it still works even if they forgot to configure it. You’d be surprised by
how many systems that come out of the box cannot start without proper configuration.
They are so flexible that they will not stand straight on their own.
How expensive is it to change the configuration? Do you need a full rebuild? Are you
going to set the configuration early in the pipeline? Many options are hard-coded into the
software in the source code and if you want to change something, you need to recompile
it. Or is this more dynamic so you can change the configuration until before startup and
the system will read and apply the configuration settings when it boots. Or you can make
it even more configurable while the system is running. I don’t have to stop and restart it
in order to configure it.
So there are two configuration philosophies: the one in which after setting the option
you have to click on apply, then click on ok, then reboot and it will eventually work; or
the other one which you just set the option live. Guess which one is easier to recover
from when you end up in a dark corner of the configuration space?
The assumption behind configurability is that the software is the same and we can
change its behaviour by changing its configuration. There is one system, with one code-
base, written with infinite variability. This has been the foundation of business empires,
where the software is so flexible that you have armies of highly-paid consultants who go
out to just configure it. And yes, there is a thin line separating configuration languages
from programming languages.
84

Customizability
Can the architecture be specialized to address the needs of individual customers?
• One size Fits All
• Product Line
• White Labeling
• UI Theming, Skin
• Conѹgurability, Composability
How do you call a piece of software which has been written or configured to satisfy
the exact requirements of one customer? Very few customers can afford custom-made
software, written entirely from scratch. But there are tricks one can play with the archi-
tecture to make custom solutions, or custom configurations, or custom extensions.
The architecture is so general that you can derive a whole family of specialized products
from it.
The customization process can follow multiple steps along the software vendor food-
chain. As you write the software, you do not give it a name. You do not brand it with
a logo. Someone else will, as they resell your software under different labels. From the
user interface, it looks completely different, but under the hood, it’s the same software.
This is similar to skinning the user interface. The scope of the change is limited to
the visible surface. This is a way to limit the impact of the change while maximizing
the visibility of the change. Users don’t usually notice code refactorings, but they will
notice if you change colors or the visual language of the toolbar icons. And they will be
delighted if they can buy a new theme to slightly change the layout of their website.
How to customize a piece of software? Change the configuration to compose its com-
ponents in a customer-specific way.
85

Change Duration
• Temporary: Resilience
Can the architecture return to the original design after the change is
reverted?
• Permanent: Adaptability
Can the architecture evolve to adapt to the changed requirements?
We should also consider whether the change is temporary or permanent. Can the need
to deal with the change be ignored? Eventually, it will go away, but there may be conse-
quences if your architecture is not resilient. Resilience is about the ability to change but
also to undo the change. Resilience may also be achieved only for small changes, but if
the impact of the change goes beyond a limit, then it may not be possible to absorb it.
Adaptation uses an ecological metaphor. How well does your software fit within its
niche? Which evolutionary forces make your software adapt? What if a competitor ap-
pears? What if the environment changes? Adapt or face extinction.
86

Adapt to Changing Requirements
• New Feature: Extensibility
Can functionality be added to the system?
• Existing Feature: Modiﬁability
Can already implemented functionality be changed?
Can functionality be removed from the system?
Extensibility is the ability to grow and support new and unexpected features. Success-
ful software is never complete: you can always add more features to it. It is possible to
design extensibility into the architecture.
It is more difficult to do the opposite and change a system by removing features. The
effort may not be much but the risk will be much bigger than when the feature was added.
It’s safer just to disable the corresponding API and hide the feature from the user inter-
face.
In general, changing existing features without breaking everything else requires dis-
cipline, focus, and clarity of thought. Always think about the impact of changes before
applying them. While extensions can be independently developed and bolted on an ex-
isting plugin interface, invasive changes require to develop an understanding and build
enough confidence to touch already written code. It doesn’t matter if the code was writ-
ten by someone else, or by a younger version of yourself.
87

Elasticity
Can workload changes be absorbed by dynamically re-allocating resources?
• Assumption: Scalability + Pay as you go
• Cost(SLA Violation) >> Cost(Extra Resource)
• Example: Cloud Computing
How can we be flexible in our capacity so that we can deal with a dynamic and unpre-
dictable workload? Allocated capacity costs money, but if we’re not using it, can we avoid
paying for it? What if the workload increases with a peak beyond the capacity, how can
we increase the capacity to deal with it? And what to do after the wave washes away?
Elasticity is the ability of a system to dynamically adjust its capacity by allocating or
deallocating resources. So you can make this part bigger or smaller, you can make its
processor more or less powerful. You can add or remove storage. You can add or remove
cores, memory and network connectivity and everything else that you need to run the
system. The goal is to avoid overload situations. Avoid the penalty of making your cus-
tomers wait in line. If they wait for too long, they are not happy, and switch to a faster
competitor. The assumption is that you can afford it, because when you increasing ca-
pacity, then it will cost more.
An elastic architecture is sustainable if the extra cost to other capacity is less than what
you would need to pay to the customers that are not getting serviced as promised.
This is the main quality that defines Cloud computing as a specific type of execution
environment. One of the original unique selling points of the Cloud was that on it you
can have elastic scalability.
88

Elasticity
Can workload changes be absorbed by dynamically re-allocating resources?
Workload
Static Resource Allocation
Static Resource Allocation
Workload
Static Resource Allocation
Static Resource Allocation
Overload
Overload
Wasted Capacity
Wasted Capacity
Elasticity
Can workload changes be absorbed by dynamically re-allocating resources?
Workload
Ideal Elastic Resource Allocation
Ideal Elastic Resource Allocation
Workload
Real Elastic Resource Allocation
Real Elastic Resource Allocation
89

To visualize elasticity you can have the first situation, in which you have the perfect
capacity to deal with the highest workload peak, but all the red part is wasted because it
is unnecessary to process the rest of the workload. If you statically allocate less capacity,
there will not be enough to deal with the peak. That’s when your customers will expe-
rience outages: availability goes down. In a non-elastic architecture, the capacity line
can only move up or down. But it cannot change over time. This fits with how physical
hardware behaves. Once your server is built and installed, it can only process so many
requests. Especially if it’s designed by some manufacturers, you cannot easily replace
its CPU or increase the amount of memory or storage. Ideally, we would like to have a
system in which if you don’t use anything, You don’t pay anything. The moment that
you have some work to do, then you dynamically add the necessary resources, which can
be freed up when the work goes away. This is sort of the most efficient type of resource
allocation. What can one do to achieve this type of curve?
You can monitor the workload and the performance of your system and if you notice
it lags, you can provide more resources. Can these resources become available instanta-
neously? What is the delay involved in ordering a new piece of hardware, delivering it,
and installing it? What if it’s enough to spin up a new virtual machine in the Cloud? If
you rent a virtual machine: What’s the minimum period you will have to pay for it?
90

Compatibility
Does X work together with Y?
• Interfaces
• Protocols and Data Formats (Interoperability)
• Platforms (Portability)
• Source vs. Binary
•
 (Backwards and Forwards Compatibility)
Semantic Versioning
 
 
Sometimes users get so happy: you start from one user, you get their family, then
you get all the friends, all the neighbors and eventually you have the whole city. Then
everybody from all over the world comes to try your awesome cookies.
And you have to grow. Your architecture has to work at scale. You shouldn’t touch the
recipe of your cookies. But you have to hire more cooks.
So you introduce a new clone of the architecture. You adapt it to fit with local envi-
ronment. The moment that you have a modular architecture (it’s enough to split it into
two parts) it means you have multiple components. And those multiple parts are going
to have to work together.
Why is this connected with flexibility and change? Usually, you start from a state in
which everything is compatible. Everything works together, then something changes on
one side, and you break the compatibility with the other. You update your dependencies
and we can no longer recompile our system. When assessing compatibility, you focus
on the point in which the two systems are touching. We call it the interface. This is the
point in which you can determine whether the two elements are compatible or not. If the
interfaces fit and the two sides can work together.
There are two types of compatibility. One is along the vertical direction, which is about
compatibility between your system and the platform on top of which it runs. That’s also
called Portability: we want to be able to take our system and transfer it between different
platforms.
Interoperability is horizontal. It describes the ability of different components across
two independent systems to work together.
There is also source and binary compatibility. After changing something, you just have
to recompile it and it will work again. Or you can make a change and still work with the
old binaries. You don’t have to recompile it. Which is better?
How do you know whether two elements are compatible? You can do a fully-fledged
compatibility test, go through a detailed checklist to make sure every interface fits. Or
you can compress such information into the version identifier. Take a look at the link on
semantic versioning if you want to know more. Recently, the idea semantic versioning
itself went to version 2.0, so you can version even versioning schemes. Very briefly, you
can introduce a convention over multi dimensional version numbers to express whether
there is forwards or backwards compatibility.
91

If a platform is backwards compatible, it means you can update the operating system
and still run old apps on it. You can update your compiler and use the new version to
compile the old code. Forwards compatibility is the opposite: new applications can run
on an old operating system, today’s fresh new web browser can still display a website
from the early 1990s.
92

Portability
Can the software run on multiple execution platforms without
modiѹcation?
• Write Once, Compile/Run/Test Anywhere
• Cost(porting) << Cost(rewriting)
• Platform-Independent vs. Native Code
• Deployment: Universal Binaries
• Runtime: Virtual Machine Layer, Hardware Abstraction Layer
Portability is a specific type of compatibility between a system or application and its
platform. How expensive is it to switch between different platforms? This is the original
platform the system was developed for and then, for some reason, our users want to use
this other platform instead.
There are two options to satisfy these requirements. We can do a rewrite. We know
what the software should do. The one running on the new platform should do exactly
the same, which is already a non trivial statement. We also know how to write the code
for the original platform. So can we just rewrite it for the new platform? That will take
some time and effort, which can be less than the original because we are familiar with
the requirements and we have the original code, but we also need to learn how to use
the new platform. What if we could directly recycle the existing code? Let’s find out how
to make it also compatible with the other platform. If you do so, we can say that your
system is portable because the cost of switching is less than the cost of rewriting it from
scratch.
So how do we make a system portable? We can abstract the platform through a layer of
protection - code which makes the rest of the system independent of the platform. Then
we can just switch the mapping for the new environment. The rest of the code shouldn’t
be affected.
Sometimes you don’t know which platform you’re going to use until the deployment,
until the user is going to install it. You can provide them with a package which contains all
possible executables or possible binaries and then you just activate the one that fits with
the platform. Sometimes you can just give them bytecode that will work everywhere.
Sometimes you can achieve portability from the bottom up. Instead of mapping your
assumptions into what is provided by the environment, you build a complete replica of
the previous platform on top of the new one. You virtualize the hardware so that the
system doesn’t know and doesn’t need to know that is being switched. You introduce a
virtual machine layer
93

If your goal is to design a portable architecture, you have three layers: 1. your platform
independent code, which should cover the whole functionality of the application. 2. the
target runtime platform, which can change. 3. the portability layer, which isolates one
from the other. This layer can be written together with your application code, or it can be
considered as part of the platform. You can design a portable architecture because you
will implement the isolation layer for many platforms by yourself, or because you will
reuse ready-made virtual machines that can give you a general solution to decouple the
execution platform you depend on from the actual one that will be chosen a long time
after you have finished writing your code.
94

Interoperability
Can two systems exchange information to successfully interact?
• Abstraction Levels:
• Payload Syntax
• Message Semantics
• Protocols/Conversations
• Content Type Negotiation
• Standardization
• Mediation
Interoperability is about achieving successful interactions, mutually understood com-
munication, correct information exchange. To do so, both parties need to share a com-
mon set of assumptions, regarding how information is represented and encoded into a
bit stream, how to discover which data elements are present in messages, and in gen-
eral, how to give meaning to the effects of each interaction. The goal is to achieve an
agreed-upon interpretation semantics.
That’s the same problem with two pieces of software: they have to agree on the se-
mantics of the messages. They have to agree on the protocol to deliver them. They have
to agree on how to transfer the data across the wire. The good news is that there are
standards for that. The benefit of interoperability standards is that they make it possi-
ble to have interoperable systems across the whole Internet. HTTP comes to mind as an
example, but we have many different standard protocols to choose from if you want to
increase the interoperability of your system.
If – on the other hand – every piece of software that you publish on the Internet comes
with its own custom communication protocol, its custom data representation as well as
its own interaction semantics, then it would be super expensive to integrate your com-
ponents, both among each other but also with any other component out there.
In case you have a problem with interoperability, your components cannot directly talk
together. You may hope (but it’s not a guarantee) to solve this if you can introduce – just
like with portability – an intermediary dedicated to achieving interoperability. This me-
diator is going to translate between heterogeneous protocols or representation formats.
So that the other side can understand. It’s a bit less efficient, but it’s the only possible
solution in some cases where each side is not willing to change its original mismatching
interface.
95

Ease of Integration
How expensive is it to integrate our system with others?
Expensive
Easy
Hub and Spoke (2 systems)
Point to Point (2 systems)
Point to Point (N systems)
Hub and Spoke (N systems)
No API
Standard Interface
Custom Binary Data
Standard Text, XML, JSON Data
Air gap
No Firewall
Batched, periodic
Continuous, real-time
Related to the notions of compatibility and composability, one may wonder how ex-
pensive it is to build a large system out of the composition of existing ones.
Do components have an API? How do we exchange data with a component if it is not
designed for another piece of software to interact with it? If there is no API, can you still
reuse it and integrate it with other components? What if there is an API, but it is not
documented? Can you look at the source code to figure out what commands to send and
what data to extract? What if there is no source code, all you have is a black box with some
sort of user interface? Another possibility would be to scrape the user interface. This is
where the user is interacting with the system and the user is sending information inside
and getting the output back. For example, you connect to a website with the browser.
When you get the output, you can always parse it and try to extract the data you are
looking for.
A concrete example: you want to retrieve the price of a book. If you have an API, you
have an operation called: ”get the price of the book“ and then you send the ISBN and you
receive the price along with its currency. If you don’t, you can still get to the information
via the website: pretend to be a user that is using its browser to search for the book
and after you get back a results page and you look inside the page for that green colored
floating point number right next to a string which should look like a currency code. You
can be relatively sure you are reading the price of some book.
You can always pretend to be a user when you interact with the system. But doing
integration this way will be expensive and it will be brittle. If the color used for the price
changes, your scraper will break and will be unable to retrieve it.
To make it easier to integrate your architecture, to open it up towards other systems,
use a standard interface or API. This is a component explicitly designed to make it easy
for your system to be integrated with others.
If we have an interface but it contains only binary data, it means that every time you
want to parse the data, you have to figure out how it is structured and write your own
parser. It’s much cheaper to represent the data as text and use for example XML, JSON,
YAML or some format for which parsers have already been written.
It’s easy to integrate, if you have network connectivity into the system. You can actu-
ally communicate with it. In some cases, there is no connection: the Firewall is closed,
and there is no way to get there unless you hack your way in with a tunnel.
96

When you have two sides communicating, do they do it in real time? What is the la-
tency with which the effects of their interaction become visible? Do they perform a live
data exchange or just synchronize every night?
If you need 2 systems to talk together, you can set up what is called a “point-to-point
integration”. They get connected directly, it’s a one-time job, problem solved.
The moment that you want to generalize this to N systems, which all have to talk to-
gether, then it’s better to avoid direct connections and go through a central hub. This is
going to translate everything in a common representation, which in turn can be trans-
lated back to what every other system can understand. This makes it cheaper to integrate
additional systems, but is more expensive in the short term, as you typically would start
by integrating only two systems.
This is just a quick reflection about the need to be able to build large systems out of
the integration of many small ones, which is ultimately what a modular architecture does.
That’s one of the main concerns of architectural thinking. You have to reuse many parts
which may or may not have been designed to be composable, and you have to figure out
how to efficiently assemble them together.
97

Long Term Qualities
• Durability
• Maintainability
• Sustainability
In the long term, nobody knows what will happen. Consider a time frame of decades,
stretch it to half a century. You would be surprised how much software is still running
today that was written more than 5 decades ago. If a company is successful, if a company
runs their software and they are satisfied with it, there’s no reason they should stop using
it.
Some would blame the hardware manufacturers that want to sell new hardware or the
software platform vendors they want to sell software upgrades. But the existing software,
if it’s well used and understood, should keep working. There’s no reason why the software
should stop working.
What really affects it, is that everything else around it may change. The question in
the long term is: How to isolate a system so that it can survive despite all of the changes
happening in the world surrounding it? We talk about 3 different aspects: one of those
concerns the data that you manage through your software. Is the data for you more or less
important than the software that goes with it? Sometimes the data is internal, sometimes
actually the data is next to the system, or in between different systems.
Then we need to maintain the system itself, and then I would like you to think about
the term sustainability. What does it mean to grow your user base in a sustainable way?
What does it mean to sustain the survival of a software architecture over decades?
98

Durability
How permanent is the data?
• Persistence Layer (DB, Container, OS)
• Checkpoint and Restore
• Backup and Disaster Recovery
• Long-term Digital Preservation
When focusing on the data, the first thing that you should do when you design the
software architecture is decide which parts of the system should contain information
that needs to be persistent. Which data is going to live longer than the software itself?
Put it outside and manage it carefully.
What are you going to use to store it? You can use the file system. You can use a
database. You can replicate it across different Cloud data centers. There’s lots of tech-
nology helping you to solve that specific problem.
Depending on the type of data, you will find many different types of databases where
you can store tuples, documents, objects, graphs, you name it. Your job as an architect is
to choose the right one for the particular data and the specific access patterns found in
your system.
While the system is running, what if its internal state is lost. How can you avoid data
loss? How can you make it durable? You use redundancy over time and space: make a
backup. You take a snapshot, you make a checkpoint so you can recover from it. How
often do you do it? You can do it every hour or every night. You could do it every week. It
depends on how quickly your state is changing. How many distinct copies of the backup
should you keep? What if you keep the backup snapshot on the same physical disk as the
original data? What if the backup contains all the emails of the University department?
To determine how aggressively you want to replicate your backup, you should consider
how valuable are the bits you are trying to preserve. If they are priceless, paying for
additional copies and keeping those in a safe location, offsite, should be a no brainer.
Still, if the information is supposed to be private, the more replicas you have floating
around, the more chances someone else can take a peek at them. So in this case you also
have a security and confidentiality issue, which you can solve with encryption. Now you
have another failure scenario: what if you forget to back up your encryption keys?
In general, every software we design stands on a technology platform and this is true
also for the data storage part. If you go down a few layers, you have your database, the
database stores data files. The files are stored into some kind of file system, which is
stored on some kind of a block device, which is taking advantage of some kind of storage
technology, which could be magnetic, optical, or solid state electronics.
Every layer and every technology has their own expiry date. I had a beautiful music
CD from my young age but now it doesn’t play anymore. Is the encoding format for
the sound waves no longer compatible with the player software? Did the digital rights
management server – whose address is hard coded in the media – just go offline forever?
Or simply, being physically just a piece of plastified aluminum, chemical corrosion or
99

physical damage will eventually win. The result is that your memories are gone, you
cannot access the information anymore.
So if you really want to preserve your data and make it durable in the long term, you
should not do so passively. You just store it somewhere in a vault under the pyramids
and forget about it. You have to keep renewing. You have to keep refreshing. You have
to keep migrating across technology waves. From tapes to CDs, from DVDs to Blue Rays,
or from external disks to USB memory sticks.
Shifting up the stack, from hardware to software, if you are a Mac person that has all
the data on the Apple file system and if you would want to jump the chasm and access
your data from a Windows PC, your new computer will happily prompt you if you would
like to format the unrecognized disk.
In general, this is the question of digital preservation originally faced by cultural in-
stitutions, like a library or museum, whose goal is to store all digital artifacts of our day
and age. On a smaller, personal scale, this is what everyone may want to try to achieve.
Unless your digital memory span will only last as long as the average lifespan of your
phone.
To preserve the data, you need to preserve the software that can read that data? Be-
cause formats change, consider video codecs, they get better all the time. And it’s expen-
sive to keep supporting legacy video formats. Can you still open a word document from
20 years ago? What if the company which wrote the software used to edit your precious
files is no longer around? What if they were taken over by their competitors? If you are
trying to preserve your data for posterity, look the simplest possible open format you can
trust to stick around.
These are all implications of taking a long term perspective about the durability of
your data. Typically you completely ignore them under the time to market pressure, or
the need to save costs and limit redundancy. But if your software is successful, its data
will face the challenge of surviving technology cycles, and these problems will come back
to haunt you.
100

Maintainability
How to deal with software entropy?
• Change is inevitable
• Keep the quality level over time
• Adaptive, perfective, corrective, preventive maintenance
• Re-engineer, reverse engineer or retire legacy systems
If you never kill anything, you will live among zombies (Gregor Hohpe, 2015)
Let’s look at the software again. How can we maintain it? How do we fight against
software entropy? Like the physical entropy in the real world, does the total software
entropy only increase? You can try try to keep your system design clean and beautiful,
but to do so all the ugly and messy parts will be swept under the rug or somewhere else
inside another box.
How can we ensure in general that under the impact of change the quality of the system
remains stable or at least under control? This is the general definition of maintainability.
We cannot avoid to change the system. Can we do so without degrading it?
We distinguish between corrective maintenance that we do because there are bugs to
be fixed, because we need to correct errors spotted by users. The goal is to decrease the
number of bugs over time.
External changes also require maintenance. You discover new version of your com-
piler, operating system, a new release of a library you depend on. Should you install it
right away to see what happens? Should you make a back up of the previous versions be-
fore? It seems to be a good practice to install the latest security patches. So after you do
the upgrade, you try to run your system and it crashes. You just found out the new ver-
sion is not compatible and something breaks and then you have to start a new adaptive
maintenance project. Your system needs to be brought up to level with the new change
affecting its dependencies. You have to adapt your system to external changes.
What about performance improvements? What about the need to scale? How can you
avoid death by success? How can you make it sustainable to have more and more users?
This is what perfective maintenance targets. The idea is to get the minimum viable prod-
uct released and then maintain it all the way to perfection. Performance and scalability
are one example, but in general, this can target any externally visible quality attribute.
Quality has a minimum, acceptable level. Perfective maintenance means going beyond
that, and trying to go towards perfection. While reaching the ideal level of quality, if you
can afford it, you can try to make some progress towards it.
Preventive maintenance is focused on internal quality attributes. The system is OK
from the outside, but inside is a mess. And you know that if you just look how ugly the
code is, very difficult to understand, even by yourself, because you wrote this 6 months
ago and you forgot, then you look at it and say I want to change this. I need to refactor
101

it. There is a new feature request, but it’s impossible to do it because if I touch one line
of code, I cannot control the effects of this change. This is why preventive maintenance
tries to increase the internal quality of the system so that you can do all the other three
types of maintenance.
There are two rythms during the maintenance cycle. One concerns the normal devel-
opment lifecycle. You have a backlog, you take story, you implement it, you push it, it
goes through the build pipeline, tests are green. you deploy the new release and start
again. You repeat this process with the original developers team and reach milestone af-
ter milestone and finally ship. Then maintenance starts. The cycle is now more irregular,
everything is quiet for long periods of time, then the pace picks up again frantically. The
system has been running for a while, deployed happily in a certain environment. Users
suggest opportunities for improvement. Your favourite hardware manufacturer shifts
the ground underneath you. And you lost touch with the original development team.
You also lost access to the source code. Maybe you never had access to it, because you
bought the software as a binary package from someone else.
What do you do? Reverse engineering helps. Before you can improve the system you
have to bring it into a state which makes it possible to work on it. So basically decompile
the binary code, bring it into the right tool environments, set up the tests again. Typically
when you ship a release, you don’t ship the tests right, so maybe you have to rewrite them
again. Eventually you can put together enough confidence to evolve the system.
Sometimes the level of entropy has gotten to a point where it’s not feasible to evolve
the system any longer. So what you do is throw it away and rewrite it. Maybe what you
wrote is only good for 100 users and if you want to go to 10000 you need a new architecture
with a more scalable concept. From the original system you can keep the tests, you reuse
all the knowledge about its features, but the design has to be rethought and reengineered.
This is really drastic, but it does happen, also if the new developers teams refuses to or
is unable to cope with the code left by the previous team.
Sometimes you have ancient systems that have been working for a long time. You
manage to write a potential replacement. How easy is it to finally get rid of the old one?
It’s not enough to just wish people use the new one. Chances are that 80% of the features
are reimplemented successfully but 20% will not yet work. There may be an expensive
phase in which you need to operate and maintain the old system as well as the new partial
replacement. So as Gregor Hohpe says: “if you never kill anything you will live among
zombies”. And that’s not a nice place to be in.
102

Maintainability
Adaptive
Comply with New
Law
Year 2038 Time
OverѺow
Upgrade
Dependencies
Perfective
New Feature
Optimize
Performance
Corrective
Fail Over to
Backup Data
Center
Bug Fix
Preventive
Write
Documentation
Refactor
Types of Maintenance
Adaptive
Deal with external "evolutionary" pressure
(avoid quality gets worse over time)
Perfective
Improve external qualities
Corrective
Remove defects (ensure acceptable, good
enough quality)
Preventive
Improve internal qualities
103

Sustainability
• Technical
How to avoid your software becomes obsolete in the long term?
• Economic
How to ensure your software development organization does not go bankrupt in
the long term?
• Growth
How to bootstrap the growth of your startup?
We can look at sustainability from 3 different perspectives, the first strongly connected
to adaptive maintenance. Technically, how to avoid your software becoming obsolete in
the long term? For how long can you hope they keep your platform backwards compati-
ble? In the long run, things will eventually break. So we have to keep running (rewriting
our code) to stay in the same place.
Economic sustainability involves the paradox of infinite growth on a finite planet, or
optimizing away redundancy and investing into lean stockpiles so that if everyone has to
stay locked down at home, the whole system crashes after two weeks. From the software
business perspective, the goal is to stay afloat. After you graduate, you want to do a
startup, your software company. You need not only to write and ship the software, but
make money with it. Monetization: you have a free service, a beautiful website. People
can go there and use it, but nobody pays anything. So how can you keep the lights on?
How can you keep the system working in the long term? If you want to do maintenance,
you have to afford it. You need to pay developers to maintain the system. If you don’t
make any money then how can you maintain the system? This is a big problem with open
source. There’s lots of volunteers, but there is no long term perspective and it’s difficult
to predict when bugs will be fixed or new features added. Sometimes people give up and
they stop working on their open source libraries. But there may be many other projects
which depend on them. Either they find an investor, or the dependencies are willing to
pool the money to pay to keep maintaining it. Maybe they have to switch to an alternative
library or they have to run with the risk to rely on an obsolete unmaintained dependency.
You have shipped your beautiful software. You give it away for free, then it goes viral
and at the end of the month you get the bill from your favourite cloud provider. Congrat-
ulations, your traffic went through the roof and now you have to pay for it. The following
month, next to the reminder to pay the previous bill you get a letter: ”Congratulations,
you have just been acquired. Signed: your Cloud provider. PS: Your debt is forgiven.“
It’s great if you have been growing really fast. But if you grow too fast, you can also
die of success. Your architecture was ready to scale, but it was not ready to scale in a
sustainable way. Getting more and more users becomes more and more expensive. You
have to keep a balance if you want to grow sustainably between the free users who drive
interest to your system to the users who economically support it with real money. How
many paid users do you need to support how many free users? Make sure you keep this
ratio in check for a sustainable growth.
104

References
• George Fairbanks, Just Enough Software Architecture: A Risk Driven Approach, M&B 2010
• Douglas McIlroy, Mass produced software components, NATO Software Engineering
Conference, Garmisch, Germany, October 1968
• Will Tracz, Confessions of a Used Program Salesman, Addison-Wesley, 1995
• Frederick Brooks, The mythical man-month: essays on software engineering, Addison-Wesley,
1975
• Tom De Marco, Slack, Getting Past Burnout, Busywork, and the Myth of Total Efѹciency,
Broadway Books, 2002
• Diomidis Spinellis, Georgios Gousios, Beautiful Architecture: Leading Thinkers Reveal the
Hidden Beauty in Software Design, O'Reilly, 2009
• Algirdas Avizienis, Jean-Claude Laprie, Brian Randell, Carl E. Landwehr, Basic Concepts and
Taxonomy of Dependable and Secure Computing. IEEE Trans. Dependable Sec. Comput. 1(1):
11-33 (2004)
• Ken Thompson, 
, Comm. ACM, 27(8): 761-763, August 1984
Reﬂections on trusting trust
 
 
105

Software Architecture
Deﬁnitions
3
Contents
• Who is a software architect?
• Architecture and the software lifecycle
• Deѹning Software Architecture
• Architectural Decision Making Process
• Prescriptive vs. Descriptive Architecture
• Architecture-focused design and hoisting
• Presumptive Architecture vs Reference Architecture
• Solution vs. Product Architecture
• Technical vs. Marketing Architecture
106

Who is a software
Who is a software
architect?
architect?
Before we define what software architecture is, let’s talk about who is a software archi-
tect. After all, software architecture is what software architects do.
As a job position or title, being a software architect comes with great power and re-
sponsibility. For example, Bill Gates after stepping down as Microsoft CEO became its
chief software architect in 2000.
The job also requires significant experience, so it is unlikely that after taking this class
you will be able to directly apply for such jobs. But hopefully your path to get there, if
you wish to do so, should be shorter.
107

Functional Organization
Developers
System
Administrators
Database
Administrator
Quality
Assurance
Helpdesk
Architects
Project 
Managers
Lead Architect
Test Manager
Support
Manager
Operations Manager
Product Manager
When we look at the structure of a software development organization, we can de-
scribe it in terms of the different roles involved in the development process. Software
development is a team sport with different types of players.
We split the organization across the two different phases of the development lifecy-
cle. On one side, here we have the development deparment. We have the developers
that write the code. They organize themselves to into different projects. After you write
the code, you should do some testing so you have the Quality Assurance Department.
While it can be discussed whether developers write their own tests, or whether testers
also can write tests. For sure, testers run the tests and then give the feedback about the
failures they encountered back to the developers, who have to fix their code. And on top,
overseeing it all, you have the ivory tower where the architects live.
Half of the company is doing the development; what is the other half doing? They’re
actually running the software for the end-users since developers just run it on their own
systems. Developers make a release. The release is tested. If everything is green and the
quality is good enough, the software gets deployed in production. And then somebody
else takes care of making it available to the users.
What is the responsibility of the operations department? Operators must keep the
systems running. They give you the dynamic qualities that we discussed. For example,
performance, they make sure the system is configured and tuned correctly. They allocate
enough capacity so that it can scale to service its workload.
They make sure it’s available. Like for example what just happened now. The WiFi went
down again. To whom do you complain? The help desk right, the first line of defense
against the users complaining about their problems. After telling the users to reboot
their clients doesn’t solve the issue, maybe you have to reboot the servers. Maybe you
have to check and reboot the WiFi router. if everything is online, maybe your account got
locked out. So someone should call the security helpdesk and get the password reset.
These are just a few examples of what system administrators do. But the problem is
still there. It’s actually a bug. We discover a genuine functional issue: the system as it
has been released and installed is actually incorrect.
108

First we try to gather the information that is necessary to reproduce the bug, and then
we file a bug report so that someone else can take care of it. For example, a tester can
write a test so that if the test is green it implies the bug has been fixed. Then a developer
can be tasked to fix the broken build by correcting the code. After re-running the tests,
if everything is green, you make a new release. As a developer, your job is complete.
Operators now take over and get the software redeployed. Installing a new version can
be a relatively big effort and the users hopefully will soon find their bug fixed. If they can
indeed verify it, the bug report can be closed.
How does a functional organization enable following this cycle to continuously im-
prove your software based on the feedback of your users?
If each activity is done by a different department, each group focuses on doing their
job really well. Developers write lots of code. They burn through the backlog. They ship
releases.
The helpdesk worry about how many complaints they get from the users and how fast
they are in resolving those?
If there is a different management structure on each side, how easy is the communi-
cation flow between the different groups? How smooth is the transition of a released
shipped to be installed in production by someone else?
What is the delay between the time a bug report is opened and a developer starts work-
ing on fixing it? What if there is a problem during an upgrade? Here is where the blame
game starts. Since each reports to a separate management structure, administrators can
always say that developers have produced a buggy software and the developers can say
the administrators have failed to properly install it. So in this type of organization a
lot of time is wasted finger pointing as opposed to quickly finding a solution for user’s
problems.
109

Cross-Functional Organization
Developers
System
Administrator
Database
Administrator
Testers
System
Administrator
Database
Administrator
Manager
Architect
System
Administrator
Database
Administrator
Developers
Testers
Manager
Architect
Developers
Testers
Manager
Architects
A cross-functional organization also features multiple teams, but within each team we
have representatives of all of the previous functions.
In one team we find a manager, an architect, developers. And in the same team we
have testers, but also the system administrator who can directly access the production
environment.
If the user has a problem, it can happen that it gets to report it directly to a developer
who is answering the call for help. This is the team when developers are incentivized not
to make mistakes because they will be woken up in the middle of the night and told that
something went wrong.
This is how you can also iterate a bit faster because it’s the same group of people: they
virtually sit together. They talk to each other. They’re not like in different buildings or
countries.
Depending on the budget, a team contains a person that has the explicit job of the
architect. It’s also possible that the role of the architect is be played by a developer.
What does it mean to play the role of the architect? Every once in a while you make
decisions for the team. Whenever I have to make an important decision about my code.
I will wear the hat of the architect. And maybe this is done with a group of people you
know. If it’s not a single person that has this job position, who is only doing architecture,
there is a risk decisions will be made without touching the code.
An architect is a role that you need to have in your team. If you don’t have it as an
explicit person, you still need to have somebody that worries about making those impor-
tant design decisions that affect the quality of your project and ultimately will impact
your users experience.
110

Facilitate Communication
Customer
Developer
Data Scientist
Product Manager
Architect
SysAdmin
Marketing
Vendor
UI Designer
Tester
Technical Writer
User
We saw the architect as the connection point between the business world and the IT.
The bridge over the IT-business gap.
In more detail, as an architect you are going to have to talk to all of these other roles
involved in a software project.
We have technical roles working close with the architect: the developer who writes
the code and the testers already mentioned before. Usually developers prefer to write in
a programming language, but they don’t like as much to write in English. So we have
dedicated technical writers. They write the documentation. We already mentioned the
operation side to make the system run. There we find the system administrators, the
operators, the database administrators. Another figure very close to the user is the user
interface designer. Their skills focus on the surface of the system and their goal is to
design a usable interface. Sometime they have developers skills, or they prefer to work
with photoshop and do mockups or sketches of the UI and then the developers implement
it alongside. The whole project is managed by dedicated manager.
Very often nowadays we also have a data scientist on the team who is going to help the
whole team learn how to improve the project. Data scientists can come up with experi-
ments. Hypothesize on possible changes and check if the user like them using metrics.
They observe the users where they get stuck, where they make mistakes. Or can also
determine whether there is an actual need for new features. They suggest to introduce
buttons in the user interface that are not implemented, and then if the user clicks you
get a signal that says the user wants to do that and only then it should be implemented.
Developers typically know how to implement the button and its behavior, but they don’t
know how to reason about the value and the need for proposing the button to the user
population. So this is the role of the data scientists: running experiments to motivate or
justify the developer’s work.
111

Users will need to use your system. They may or not be the customer, who is going to
pay for it. Sometimes we don’t have a customer yet. Instead we have marketing people
that want to do marketing campaigns to generate interest about the product. So the
marketing is a proxy for the customer. They should know the market, they should know
what the customer wants and they can tell you if you implement this feature people will
buy the software.
Who is the vendor? If the marketing group wants to help you generate interest for your
product. The vendor is actually trying to sell it. If the vendor is part of your organization,
then they will be the sales people who go out and help you sell the software. Architects
also interact with vendors who works for other software companies. They want to sell you
their software so you don’t have to build it yourself. You must buy this library, it will save
you so much time. You’ve got to adopt this framework. You cannot work without these
fancy new tools. The architect should be able to evaluate if those are really necessary for
your project, your team and what is their impact on the quality of your software. It is
very important for the architect to be able to interact with the sales people on both sides.
If you build a completely self contained architecture where you don’t have any external
dependencies, then you don’t have this problem. You develop everything yourself, no
need to deal with vendors.
We can see the architect is in a critical position in the middle of everything and every-
one, continuously communicating and – most important – translating between different
languages and levels of detail.
112

Software Engineering Lead
• Excellent Software Engineering Skills
• Promote good development practices
• Solve the hard problems
• Lead technical development team by example
Understand impact of decisions
Defend architectural design decisions
Plan and manage software releases
What are the main characteristics of a good software architect? You can see architects
as a highly experienced software engineer. This helps them within the team to promote
good practices for developing software. They dedicate their attention to the difficult
parts of the code.
Leading by example means that you can come up with the idea for a solution. You
should prove it’s a good one and explain to developers how to do the right thing in the
right way.
As an architect, you make technical decisions about the design of your system. You
have to understand their impact. You have to defend your decisions. You need to be able
to convince the developers to adopt your approach. Developers have a tendency to know
it all by themselves. They are very opinionated sometimes. And as an architect you may
need to change their mind, or be ready to understand someone else’s views.
You are also involved with every release: You’re responsible for deciding in a software
release which features are going to go out and what features are not yet ready to ship.
The architect is the one that signs off the release.
113

Technology Expert
• Know and understand relevant technology
• Evaluate and inѺuence the choice of 3rd party frameworks,
components and platforms
• Track technology evolution
• Know what you do not know
Architects also make decisions about technology choices. To do so you have to un-
derstand the technology. You typically have graduated into being an architect from a
development career, so you should have this expertise and know how to keep yourself up
to date.
This is particular important for deciding the platform. This choice affects everything
else, and the choice of the platform is an important architectural decision. The platform
technology evolves, so you have to keep in touch, follow the latest trends with a critical
eye.
You also need to reflect about your knowledge: know what you don’t know and and
be able to to know your limits. It is easy to be confident about decisions about what you
know. If you are not sure, do not be afraid to hire people who can complement your gaps.
114

Risk Management
• Estimate and evaluate risks associated with design options and
choices
• Document and manage risks, making the whole team aware
• Make good decisions with insufѹcient information
• Prevent disasters from happening
The ability to deal with uncertainty is also another very important characteristic. When
you make a decision, there is some risk. What if you make the wrong decision? If you
make a mistake with the architecture, the resulting software system will not have the
qualities that it should have.
Sometimes you want to share this concern with the team. Make them aware we are
discussing a very important design issue and there is some risk involved. Maybe we have
to learn a little bit more about the different options and their implications so that we
can take the best decision. But usually you have to be also a little bit lucky as often you
have to make good decisions with with limited information. This is simply because the
more information you learn, the more the more you study, make experiments, the longer
it takes. And time is limited.
If you get stuck into into this, you hesitate too much before it’s too risky. “I’m not
going to make a decision yet because I have to wait and see”. Then your project will get
stuck into analysis paralysis without making further progress towards shipping the code.
Clearly, if you make a mistake as an architect there can be serious consequences, the
software quality can degrade. Ultimately, the customers may leave as you no longer meet
their expectations. They might stop paying for the license renewals or subscription fees.
They stop hoping for the next release which will fix everything. They go away and even-
tually the software company goes bankrupt.
115

Architect Tribes
• Enterprise Architect
• Business Architect
• Software Architect
• Data Architect
• Information Architect
• Solution Architect
• Product Architect
• Security Architect
Architects have spread out throughout the industry and have started to specialize and
focus on different aspects.
Large enterprises need architects with a very broad focus beyond individual systems.
They look at the whole landscape of all the applications within a company.
Business architects design the business model and the internal organization and pro-
cesses of a company.
There are not only software architects, but also data and information architects. In-
deed data needs to be modeled and design thinking can be applied to solve the problem
of capturing information and turn it into knowledge.
You can architect solutions or you can architect products. A solution is a whatever it
takes to solve the problems of one customer. Its architecture can be the integration and
composition of different products. A product instead requires a larger investment which
is amortized by selling it to many customers.
There are also architects who just worry about the security quality, which in some ap-
plication domains can become so important that you need a dedicated architect just for
that.
116

Waterfall
Where is the architecture?
˾. Requirement Analysis
˿. Design
̀. Implementation
́. Unit Testing
̂. Integration
̃. Acceptance Testing
̄. Deployment
̅. Maintenance
Let’s now shift our focus from who is the architect to when we do architecture in a
project. We can connect to what you already know about which process that we follow in
software engineering to develop software. This process is very ancient. Very traditional:
it’s called the waterfall process. Where do we position software architecture within the
waterfall, and then we can define what software architecture is.
A waterfall is a metaphor for a strictly a sequential process where you first analyze the
requirements. And once we have a complete specification of all the requirements, we can
go and do the design and continue with the project. After we design ’something’, we’re
going to write the code using ’something’ we can call architecture. Once we have the code
we can compile it, it compiles, that’s already success. Next step is to test it with the unit
tests. The majority says after we test we do the integration so we have all the pieces that
are tested separately. After combining them together, we can do the accepting testing.
Then we can deploy it so that now the user can start to use it. After we deploy it and then
we can go to maintenance, which is like the pool at the bottom of the waterfall. After the
thrill of the jump, you just want to keep floating there forever. It turns out most of the
life time of a software system is spent in maintenance.
The waterfall does not have to follow a single linear path. Sometimes, Before you make
a big deployment, if you have a staging environment, you can check new release candi-
dates, give a preview to some users. And only after the pioneers accept it, you can make
the full deployment everywhere.
What’s the goal of the acceptance testing? Why is it different than unit testing? Why
is important to test the parts before you put them together? Because if you, if you test
the whole thing only after you put it together, how do you know where is the problem?
So first we test each of the pieces by themselves. The fact that they work individually
does not mean they will work after we assemble them. So we do the integration and then
we can do the end to end testing. If the user is involved that we call it acceptance testing
becauses the user makes the decision to accept or reject that what you released.
117

Before you can have a user involved, you need to have some kind of deployment: trans-
fer the packaged release into an execution environment accessible to testers or end users.
I don’t know if you have the same experience, but when I write the code it always works
on my computer. And then to give it to someone else, I need to deploy it. And then the
problems begin. So that’s the way to find out, you give it to your user and if they accepted
it means that it works for them after the deployment.
We do have one test driven person in the class, you write the tests before the code.
That’s called test driven development. In this case, the unit testing is actually split into
two parts. You write the test, you implement the code and you run the tests to check the
code.
If I ask you: given this waterfall, where would you find the architecture? Which of
these activities produces an architecture which can influence and be consumed by the
downstream activities?
Architecture is something you do relatively early into your development waterfall. You
do it even before you implement, before you write the code, you need to have the design
and this is somehow related with the architecture.
118

Bridge the Gap
Problem
Deѹnition
Functional
Requirements
Architecture
Data Models
Extra-Functional
Requirements
Code
Acceptance Tests
Problem Space
Solution Space
We distinguish between the problem space and solution space. The problem is how to
satisfy the requirements: input from your customer that defines what is it the software
should do. The solution is your plan for how is the system going to do it.
The architecture is the first thing that you do when you go from the problem space to
the solution space. As soon as you have an understanding of some of the requirements
(necessaryly not all of them – We’re not assuming strict waterfall anymore), you have an
idea of what is it that you want to achieve with your software, you can then start to sketch
or model the architecture.
Once you have a good idea of how are you going to do it, then it makes sense that you
invest into actually doing it, that is writing the code. If you just prefer implementing
stuff, you think maybe you can skip this step. You can be faster at the beginning, but
overtime the people that invested into their architecture, they will catch up. In other
words: “one hour of Architecting and save you one month of coding”.
Also, since you do this at the beginning, your interpretation of the requirements and
your design decisions will have a critical impact on everything else that you do. Make
sure that you’re making good decisions.
In the same way that people have to spend their life inside this building designed by
an architect who has just taken a picture with it once standing outside the entrance, also
developers will have to spend their their working hours writing code within a system that
you designed. So you are responsible if as a result of your decisions you make their work
easy or their life miserable.
119

Peter Cripps
Think Outside the Box
Requirements
Requirements
Architecture
Architecture
Challenge Requirements 
Avoid making assumptions 
Someone will have to live 
with your architecture: 
Make sure it is good
We are in this position at the moment. We have the requirements. We are in the white
box which translates them into the architecture. So you can see this information flowing
from one side to the other.
This is not how it works in reality. When somebody talks to you and tries to describe
requirements sometimes you hear something cryptic full of ambiguous details. You will
try to make it more structured, clear and precise.
Sometimes if you just don’t understand what the requirement states. We need it to be
very fast. What? The response time. How fast? is one millisecond fast enough? How do
you know what’s the right level? What’s the target?
So you have to all the time challenge the requirements. You have to clarify them as
much as possible. You have to get the customer to agree that your interpretation is the
one that they also have, because sometimes you get the sentence you read it and then
you understand something completely different than what the customer wanted to say
and this is what we call also making assumptions.
Sometimes the customer doesn’t say everything and the gaps are filled by your assump-
tions, and if you’re lucky the assumptions match. But typically your assumption is not
what the customer wanted, and then you end up with everything else that depends on
those assumptions being affected. And once the customer finds out, it will reject your
assumption and everything that goes with it.
To make sure this happens as quickly as possible, there is a very important cycle that
should be set up. You elict requirements, analyze them and then you come back and you
clarify. You make a joint decision on how to interpret them.
Once they are clear, what do you do next? You prioritize. What’s the most important
requirement? What’s the least important requirement? Because sometimes you cannot
satisfy all of them at once. Doing all will take too long before we can ship the code. But
you can attempt to do so gradually.
120

John Reekie, Rohan McAdam
Evolutionary Model
Preliminary 
requirements 
analysis 
Design of 
architecture and 
system core 
Develop 
a version 
Deliver a
version 
Elicit 
customer 
feedback 
Incorporate 
customer 
feedback 
Software 
concept
Architecture
Architectural decisions 
are made early in 
the development lifecycle
Here is a bit more modern picture of a software development cycle where such feedback
loops are actually visualized; it’s not strictly top down, but it shows we are iterating. The
activities are similar to what we were discussing before. Starting from the concept, you
have the preliminary requirements which are used to design the core.
The core: this will be the where the architecture is established. This is what gives you
the structure, the road map which you can follow as you start to write code one iteration
after the next.
An incremental process helps to shape, grow and improve a system version by version.
As long as you have real users, chances are they will have ideas on how to improve it. And
if you care to listen for them, you can go through the cycle one more time and actually
make it better.
To do so, how can your architecture help? Even before you start, you design a system
that is flexible and extensible. It will make it possible to grow through this cycle. If you
design a system that has no extensibility, you design a system that has no way to change.
Then chances are that you do the cycle once, but as you start it for the second time you
get stuck and have to do a full redesign before you can continue.
121

Agile Uniﬁed Process
Avoid Big Upfront Design, just enough Architecture
This is the classical agile process picture, showing how much effort is dedicated to the
various activities over time. Can you spot the term architecture somewhere?
Agile developers don’t really like to use the term architecture, but if you look at the
list, the first activity is called ”model“. Modeling is a synonym for planning, represent-
ing reality and reasoning about how to solve problems. You also notice that this is the
most important task in the elaboration phase. This is when you make decisions, which
will constrain and guide the code implementation, testing and deployment, which will
happen later on.
122

John Reekie, Rohan McAdam
System Lifecycle
Architectural decisions 
affect the whole lifecycle 
of a system
Inception 
Development 
Deployment 
Maintenance 
Alteration 
Legacy 
operation 
Vision
Death
Operation
Architecture
No matter where you are in the life cycle; no matter whether you are still developing,
you are already running, or maintaining your software; even if your sofware has become
legacy: no longer worth it of maintenance – the architecture is something that affects all
of these phases of the lifecycle.
The decisions that you make will impact the quality of the system and they make it
more or less easy to change, more or less easy to operate, more or less easy to develop,
deploy and eventually retire.
This concludes this overview about the connection between architecture and the soft-
ware lifecycle.
123

Before we try to give a definition, let’s remember that it has not been an easy path to
get there. In the past history of software architecture research, there have been many
attempts – many iterations – to come up with a definition.
At the beginning the focus was on capturing an abstraction of the static structure and
the behavior of a software system, seen as a set of interacting elements. This artifact-
centric view was more recently taken over by a more process-centric view, positioning
architecture in a close relationship with design decisions for which it is very important
to build a shared understanding and capture their rationale before it evaporates.
124

Grady Booch
Architecture vs. Code
• Every system has an
architecture
• Some architectures are
manifest and visible, many
others are not
• A system's architecture
ultimately resides in its
executable code
• A system's architecture may be
visualized and represented
using models that are
somehow related to the code
Architecture
Code
We should remember the ultimate ground truth about software lies in its programmable
code as it is written by developers, assembled by a compiler, or generated by a machine.
That’s the reality. Still, our axiom is that ”every system has an architecture“.
In some cases you can see the architecture explicitly documented, in other cases you
just have the code embedding the architecture.
Architecture is an abstraction over the code. The purpose for the abstraction is to make
the code understandable for you as a developer. So architecting involves significant mod-
eling effort. We want to be able to describe the code at the different level of abstraction
so that we can present only the details which matter to us: only what is important to
reason about our software. We also want to be able to do this before the first line of code
has been written.
125

Grady Booch
Architecture vs. Technology
• A given technology only serves to implement some
dimension of an architecture:
• The network is part of the architecture
• The database is part of the architecture
• The X framework is part of the architecture
• Architecture is more than just a list of products
that are used in a project
• Technology shapes an architecture, but an
architecture should not only be bound to the
technologies that form it (technology changes too
fast)
Architecture is not just the technology shopping list that someone decided to adopt to
build a project.
Sometimes when you join a new company, on the first day, they introduce you to the
team and show you one slide with “the architecture“.
The slides enumerates which database they use to store the data; Which Web frame-
work for the front end; and which programming language they picked for the back end.
All beautifully illustrated with their colorful logos.
All of these things are part of the architecture, but architecture goes well beyond list of
technology products, the list of the framework, or the list of the tools that you use in your
project. Behind each of them there is a decision that you have to make as an architect,
which should first consider the fundamental problems that you have to solve, and only
later pick a suitable technological solution. What if one of those goes out of fashion?
What if it becomes obsolete? You run a big risk by binding too strongly your architecture
to the technology.
Once the technology disappears (possibly to be replaced by some other shiny new tool)
– then you will have nothing left.
126

Peter Cripps
Can you use the word design as a synonym of architecture? What’s the difference?
To design is to decide how you are going to solve it. You come up with an original
solution. It’s a nice design for a chair: lightweight, elegant, but also comfortable and
strong. Your code uses design patterns to solve challenging problems all the time.
The question is: what’s the relationship between your design and the goal that you
have? Architecture is somewhere in the middle, as it establishes the connection between
the problem and the solution.
Also architects have their reasons for doing what they do. They connect the solution
with the problem and provide the reason for that connection. It’s not just about artistic
design, a form of self-expression.
So when we worry about architecture we need to know what is problem that you’re
solving, to consider not just how we’re going to solve the problem, but also to understand
why you are solving it in that way. And that’s probably the most difficult thing to embed
in your code.
You can write a lot of code that perfectly solves a problem, but when somebody else
reads it, they will think really hard and wonder: why did they write the code in this way?
What was the reason for adopting this particular solution?
If you can, document the reasons why you actually decide for certain choices. That’s
where you will start to shift your attention from the design to the architecture.
All architecture is design. There is some design that is not architecture. For example,
the one which ignores extra functional quality attributes. You can make a working design,
which is correct from a functional perspective. But what about performance? What about
scalability? What about security?
That’s where the architecture comes into play. Depending on your target quality and
the reasons behind your trade off, you are going to design a completely different archi-
tecture.
127

N. Taylor et al.
Basic Deﬁnition
• A software system’s architecture is the set of 
principal design decisions made about the system.
Architecture = {Principal Design Decisions}
• It is the blueprint for a software system’s shared understanding,
necessary for its construction and evolution
Finally our state-of-the-art definition: software architecture is a set of design deci-
sions. Not all design decisions are the same, so we pick the principal ones: the most
important ones, which have an impact on the resulting quality of the software.
And once you made enough architectural decisions, you have a clear plan for building
the system. You have a clear understanding about the reason why the system is designed
in a certain way. And this understanding can be shared and the plan agreed upon: now
we know how we’re going to do it and why we’re going to do it this way.
The definition is rather abstract: what happened to the components and connectors?
They are still there, you simply justify their existance with a decision. You decide them
into existence.
We’re going to split the architecture into two components. We’re going to connect
them. We’re going to deploy them.
128

Design Decisions
on any aspects of the software
system:
• Structure
• Behavior
• Interaction
• Deployment
• User Interface
• Implementation
Principal Design Decisions:
the "important ones" depending
on the goals of the stakeholders
Which are the decisions about architecturally important aspects of a software system?
Of course: having a modular structure is fundamental. If there are two components, part
of the system can be reused, while the other part can be built from scratch.
But decisions about the static structure are not enough. Software is something that
runs, so we need to design and control its behavior. When we decide to establish a con-
nection between those two components, this means we have different parts which are
supposed to interact. So we should decide, what kind of interactions we allow.
The deployment decision will be critical for reliability. If we have two components
deployed on different containers, each can fail independently. And if this one fails, how
is this one going to be affected? What kind of coupling do we introduce with our decision
to connect them together? What if you deploy one component in the Cloud and one in a
mobile phone? This is a very common decision, you can collect those into architectural
patterns.
What kind of programming language are you going to write your code in? What kind
of version control system? Which framework are you going to pick? This is also one of
those decisions which are constrained (in this case by what language your developers
know) and also are very difficult to change later on.
These are just a few examples of decisions you can take and how you can try to reason
about their consequences. To be able to support your decision making you make a model
of the architecture. Given that there are so many different decisions, we need to prioritize
our decision making and learn how we can reuse already made decisions, for example by
referring to patterns and architectural styles.
129

What makes a design decision a principal one?
Let’s go back for a second to the real world building architecture. What are the de-
cisions that those architects make? You mean besides the structure of their building;
besides the materials that will be used; besides how the facade looks like; besides the
layout of the rooms on each floor inside.
But if you ever walk down the street in the center of Lugano and you look up, you also
see that the architect may decide to plant a tree right at the corner of the roof.
This tree did not naturally sprout from the roof. Also, it is unlikely that the construc-
tion workers put it there, unless they were following the plan. And who wrote in the plan
the decision: “I want a tree to be there”?
The legend goes that there was a tree growing at the street corner and as usual when-
ever construction starts, first all the trees must be cut off. But there was no space to plant
a replacement tree. So the architect lifted it up a little bit higher and put it on the roof.
So as a software architect, when you are about to make a decision you should not doubt:
is it architectural or not? If you say so, as a certified architect in charge of the project, it
will be.
This has also been proposed at some point as a self-referential definition of software
architecture: ”Architecture is what architects do“.
130

Design Process
Let’s now take a look at the process that we follow to design the architecture. The
initial set of principal design decisions is empty. We don’t know exactly where we are
going and how to get there. But at some point we make a decision. This decision will
constrain the following ones and slowly but surely the decisions add up. We start to see
the architecture. Eventually we say we have made enough decisions so we are confident
we can start to build it, to write some code. This doesn’t mean that the process is finished.
So to design is a process that starts from a concept, a sketch, some initial ideas and
then eventually it produces an output, which is also called the design. I design the de-
sign. When you follow this process, you produce a set of decisions that define your ar-
chitecture.
131

Let’s look more in detail. What happens while we go through this process? Every step
along the way we make a decision.
Why do we need to make so many decisions? Because we have a problem to solve;
because we want to achieve a certain quality.
In any case, we cannot always make decisions in isolation. For example, we choose a
certain programming language. Then probably we can look for a suitable framework for
that language. If we change the language, We cannot keep using the same framework.
Design decisions are not independent. They are related. Making them generates con-
straints for future decisions. Be careful you do not paint yourself into a corner: while it
is hard to design without constraints, too many constraints may be impossible to solve.
Who is making decisions? The lonely architect? Or the team? Usually most decisions
are made by the team, so there is a group of people that have to agree on what is the best
way to do something.
The democratic architect can lead the discussion, make proposals, try to convince, but
everybody or at least the majority has to agree. So you need to reach a consensus, unless
you switch to dictatorship mode: whatever the architect says goes.
Especially if multiple decision makers are involved, the design process takes time. It
is impossible to make all the decisions together instantaneously in one clean sweep.
What kind of strategy can you follow then? Where to start from? At the beginning
of the design process, you can still change your mind with little consequences. There
are only few decisions affected by your indecision. Sometimes you have to undo certain
decisions and change which alternative you picked. You can still modify decisions even
later, although it will be costly to evolve the architecture of a system which has already
been built.
132

Another definition of which are the principal design decisions making up the architec-
ture is based on when they are taken during the design process. Architectural decisions
are the ones made early. Since every further decision depends on them, they are impor-
tant because they will be get more and more entangled within your design, making them
difficult to change. So leave the minute details, the final touches for the end. But get the
decisions which have the most far reaching implications out of the way early on.
By the way, why do we call it design decision and not design choice? If you make a
choice, you can easily pick another option later. For example, you make it configurable
so users can choose for themselves. If you make a decision, you firmly pick the option.
For example, you will write the code in this programming language. This will affect ev-
erything else: the tools that you use, the compilers, the development environments, the
libraries, the frameworks. Unless you are doing polyglot programming (which is also a
decision) you will not easily switch to a different language after the code has been writ-
ten.
133

When we worry about planning for the architecture of a system, we need to make a lot
of these decisions and each of them concerns a specific design issue.
One issue, for example, is the choice of the programming language. Another concerns
the structural decomposition strategy. As part of this issue, we need to determine how
many components should be split our system into. Another separate issue is about the
type of deployment that we’re going to do.
These are all different decisions that are made by selecting one alternative associated
with the issue. Issues can be formulated as a question.
If you focus on the programming language issue: ”which language are we going to write
our code in¿‘ You decide by first making a list of languages. How many languages do you
know? Then you can prune it, looking, for example, the intersection of the languages
known by all developers on the team. Then you drop the ones you are less confident or
experienced about. Finally you pick one. That’s your decision.
In general, we have design issues which need to be decided, for each issue, we need to
consider a set of possible alternatives. This is important: to make a good decision you
should never make it without considering at least three alternatives. And what if you only
have one alternative? You can have cars in any colors you want, as long as it’s black. This
is not really a decision left open for customers, but someone on the car manufacturers
design team had already decided for them.
134

Here is another example. This is an ancient design for mobile phones. This shows
which kind of decisions were taken until January 9 2007.
They were called smart phones. They came with physical keyboards. The more keys,
the smarter the phone.
Here is the design issue: how many keys should telephone keyboard have? We can have
12 keys to dial numbers and two special hash and asterisk characters. These are enough,
until people started to send short messages - so the keyboard got expanded with more
and more keys.
135

What happened on January 9, 2007? Somebody proposed a new alternative.
We’re going to make a phone that has only one physical key.
This decision would not work by itself. This decision affected everything else, starting
from the display, which became a touch screen. This opened up a new issue within the
operating system design: how to make a virtual keyboard using software? One positive
consequence would be that switching the language of a software keyboard would cost
nothing. Still, there was lots of uncertainty associated with the alternative: will users
learn how to type by touching the screen?
Now everyone knows how to use touch on their mobile phones, starting from 2 year
olds, all the way to 100+ years old. But at the beginning, people would hesitate when
they saw a touch screen.
Some other designers were not bold enough to go all the way down to only one key, so
we have phones with 3 or even 5 keys.
This was an example of a design issue that has big consequences on everything else.
Some alternatives are well known. Others are more risky, experimental or completely
novel. Do you go with the flow? If people design it like this, there is no reason why it
should be any different. It takes courage to make a big bet and totally change the design.
136

When you make decisions you have a question to answer. You know that you need
to worry about this particular issue. So there is a point in which you wonder about the
answer, you diverge. You collect suggestions.
Your entire team starts brainstorming. Everybody in the team please write down one
option that we could use to solve this problem. Everybody come up with your own solu-
tion. Then you can aggregate them. You can compare them. How many distinct alterna-
tives did you find? You don’t want to have too many or to fee, between three and seven
should be enough. If you go beyond, then the next phase becomes a major project. Do
you have the stamina it takes to cut down from 300 options to one? But you have to have
a reasonable variety. And then you start to converge. For example, you can eliminate
alternatives which are not allowed because of the decisions you made earlier.
You can also design by committee, democratically: you can vote. The easiest thing is
to ask everybody in the team to vote, and you could get a majority right away. Or maybe
you have to go through multiple rounds, dropping the least popular option every time.
You can also rank alternatives according to their impact on a critical quality attribute.
You sort them by performance. Or you sort them by reliability. Or you sort them politi-
cally, starting from the ones preferred by top management or the customer. You also can
do a multi-objective optimization, look for the Pareto frontier and see if there are some
alternatives which are dominated by others. So you can drop those. Eventually you have
to pick one by making a trade off between different qualities.
I mention all this because I want to make you realize that to design requires to actually
work. It actually takes effort to think about the impact on the architecture while making
these design decisions. And the goal of this lecture is to give you lots of examples of this
type of decisions. This is very valuable architectural knowledge, which will allow you to
know what you can expect will happen if you pick an alternative. If you choose this, it
will have this impact on the quality.
137

Sometimes these decisions recur and they are structured as patterns. We have an issue:
how to solve a problem. We choose to introduce a pattern to solve it. The pattern comes
with some ready made decisions, but as a consequence, we need to make other decisions.
Patterns do not give you a complete solution. They just give you a structure that then
you can fill out in more detail by making 2nd order decisions.
Once you’re done deciding this issue, you continue and you go to the next. How do you
know where to go next? Design space are not only a random set of issues that you need
to consider when you work on the architecture of a certain domain, or in a certain type of
system. While they do not give you necessarily a single sequence of issues that you can
follow in order (like whe you scan a checklist or read a book from cover to cover), they do
contain a non linear structure connecting all the known design issues.
We can see the design space as a graph, where nodes are design issues and alternatives,
and edges are relationships between them. Sometimes you can cut the graph into a tree.
This is useful because trees have roots, and you can start your decision making process
from there.
Following the tree, every decision will tell us where to go next. Sometimes you’re lucky
and you can make a decision that will cut a lot of these branches so that you can reduce
the need for making so many decisions later.
That can also be an argument to make certain decisions first. You choose them because,
depending on the alternatives you pick, the rest of the structure can become irrelevant,
and thus simplify your design process. If you pick some other alternative, there may be
significantly more further decisions left open for you to worry about before you can reach
the end.
138

To make a decision, for every issue you have to pick one alternative. Once you go
through all of the relevant ones, you have filled up the set of principal decisions about
your system which we call its architecture. This is the outcome of the design process.
Two architectures may share the same design space, but they can be completely dif-
ferent because you picked different alternatives for each of them, although the potential
issues to be considered were the same.
This last part on meta-design theory was very abstract. If you keep going we will be-
come more and more concrete as we start making lots and lots of architectural decisions.
139

Grady Booch
Modeling Architecture
• Static
• Structure
• Decomposition
• Interfaces
• Components
• Connectors
• Dependencies
• Dynamic
• Behavior
• Deployment
• Styles and Patterns
• Design Process
• Constraints
• Rationale
• Quality Assurance
• Team Organization
• Target Audience
• Technical Developers
• Marketing/Customers
• Management
Instead of talking about abstract design issues, let’s see concretely what kind of design
issues we need to worry about when we talk about software.
So when we talk about the software architecture, we can decompose its design space
into static or dynamic issues. You can decide about the structure of the architecture. How
do we decompose the system? The simplest system that you can design is the monolith:
everything is one piece. There are two metaphors: one block of stone, or a big ball of
mud.
They have different connotations: the first is more like the state in which every design
starts. From is this single component, one block, you can carve out or split the monolith
into multiple elements. Eventually, you have so many pieces that you can no longer tell
them apart, there is no structure left. The big ball of mud: the state in which every system
ends up.
What do you do when you make a decision to decompose it? Now we we know that this
has a certain structure. There must be a reason for having separate elements. For exam-
ple, they play different roles: the user interface, the logic, the state. So we have three
different parts. And then we can select and adopt appropriate technologies to manage
the state, we can have the logic written using some programming language (which one?)
and then we have the UI. Here the decisions are different. For example, we can make
mobile apps or we can make websites. So this gives you a way to control the complexity
of this system because you decided we will cut it into three parts.
The moment that you cut it into different parts, these are separate by interfaces: the
point of contacts between different layers, between different components. These inter-
faces become critical because they hide what is behind them.
For example, once we agree on the higher level protocols do I have to reimplement the
client if I change the way it accesses the API from 5G over to WiFi? This will only affect
a lower layer. You can change it without affecting everything else.
140

This is a fundamental idea: having standard interfaces makes it possible to isolate
some parts of the system when you change the others.
Another way to look at this is to say we have these different components and they
are connected, they talk together. That’s also decision. You can make a fully connected
graph and then you end up into the spaghetti monster where everything is connected to
everything else.
As an architect, your most important decision making tool are the scissors, to cut the
connections, decouple your components.
Once you start to decompose your system, you need to decide for each component
whether we have to write it ourselves, or this component is like a library: we can find
it on the Internet we call an API. Do we want to introduce external dependencies in our
system or do we want to avoid external dependencies? This is another important decision
if you depend on something else you can build it faster, but if you depend on something
else you don’t control how it evolves.
So far we have been discussing design issues – things that you need to worry about
and come up with a decision – which are relevant even before you switch on your system.
Even before it runs, you have to know the structure you have to know how different parts
are connected together. Then you can plan the dynamics of the system: we need to be
able to represent and understand its behavior. How do the various parts interact? Where
are they going to be deployed?
Those are at least another two different kinds of decisions. Monoliths just need one
place to run. More complex are distributed systems, where every element is running in a
different hosting environment and in between you have a network (no shared memory).
Now we will go back to this example. You run something on the phone and you run
something in the cloud. Since our system is split into two, we will have all kinds of prob-
lems concerning the network connectivity or the cloud being not available. How is the
other part going to be affected? These issues are what worry about when you design the
dynamics.
There’s more. What if you have a certain style, a style can help you to make decisions
because it gives you some constraints. Alternatives that should be avoided. For example,
here we follow a layered architectural style. What does it mean? That you put one layer
on top of the other so that you prevent arbitrary connections from happening. The style’s
constrains states only neighbors can connect to each other. But if they are far away, they
don’t. And you should be able to enforce it so that your layered design respects it.
Does the style prescribe exactly how many layers to use? No, this would go beyond
what a style is for. If you decide to introduce a pattern, instead, then you will find a ready
made solution in which a few issues of the pattern design tree are stil left open.
So these are tools that we learn as architects to help and guide our decision making.
We can have a style. We followed the style constraints. If you’re a designer and you have
to design something and you have a white sheet of paper. You are facing an empty white
board: it’s like in a nightmare due to the huge complexity of the design space that you
have in front of you. Is is just too much. It’s overwhelming.
Instead, I tell you: design something but please follow this rule. Then you, as a de-
signer, are super happy because I give you this is a starting point, I suggest where not to
go. And now you can start exploring. But if it’s empty, it’s like you’re on the edge of the
cliff and there’s just too much empty space ahead and below you.
So this is about the software itself. How do we structure it? How is it going to be
deployed? How is the behavior going to look like?
Then we also worry about how we’re going to build it, which software development
process to follow, who will need to do the testing, we need to organize the team and how
to collaborate over version control. Those are also decision that affect the success of a
141

project.
When we make these decisions, we need to remember communicate the decisions: we
need to represent the architecture. For decisions about the static structure we can use
visual metaphors to model them. Other types of decisions are more like written down
into into some document that people have to read, and they have to enforce.
When you model the architecture you will communicate the decisions to different types
of people: developers, managers, or customers. Depending on who you talk to, different
decisions may be more important than others and they might need to be communicated
in a different way. So if you sell the architecture to customers, the main tool that you
use is likely to be PowerPoint. If you talk to developers, you may want to use some more
formal modeling languages. Some more precise way to represent your decisions. We will
see later various options.
142

Can this skeleton ﬂy?
At the end of the day, don’t forget that there has to be a connection between what you
are trying to design, the solution that you propose and the original requirements.
And not all solutions are feasible, not all solutions can be successful. Sometimes it’s
already enough to look at the structure of your software to be able to tell if this software
can scale, or if the software can be reliable or not.
So you should learn how to predict whether your design is going in the right direction
before we actually build the bridge; drive a truck over the bridge and then relieved an-
nounce we are successful. Civil engineers should be able to make predictions: under this
load, it will not collapse.
143

N. Taylor et al.
Prescriptive vs. Descriptive Architecture
Prescriptive 
Architecture 
(TO BE)
System 
Artifacts
Intent
Descriptive 
Architecture 
(AS IS)
Realization
Recovery
There are different ways to do architecture, depending on at which point in the life
cycle of the system we are. At the very beginning, there is no system yet, no code has
been written, all design decision are still to be made.
This is when we want to come up with a plan for the software that we will build. This
is what we call the prescriptive architecture. This is the architecture that captures our
intention. It’s all looking towards the future, since there is no real system yet. This is
what you would do when you draw up the plans for a building before it gets built. You
make a plan so you can give it to the builders. They will build following it and then you
can check whether the results complies with your original plan.
Prescriptive architecture tells you where you want to be. But there is nothing yet. After
you build it. Then you have actually been writing the software. The software actually
runs, it works. At this point we don’t talk about the prescriptive architecture anymore,
but we talk about the descriptive architecture, which is just a description of the reality of
the system which now exists.
First we make a plan: a model for something doesn’t exist yet. We are prescribing to
the developers, what they’re supposed to do.
If the system exists already, we can describe it after the fact, after it has been built.
This is also the point in which we can compare before and after. Does the description of
the system after it has been built complies or fits with the original plan? or maybe along
the way we had to make some modifications. We made a change during the construction
phase of the project. The original plan is no longer entirely true.
It is possible to describe the actual system through some metrics or some visualization
tools, extracting a model from the code to recover its descriptive architecture. The pre-
scriptive architecture, once lost, cannot be recovered by observing the result. It is only
possible to ask the original architect to remember it.
While it is always possible to describe a system, it is much more difficult to recover its
goals, the intention of the designer and reconstruct the rationale for its design decisions.
You can be sure about which problem was solved how, but you will never be certain about
why that solution was picked.
144

Green Field Development
Before they are built from scratch, systems only have a
prescriptive architecture, deѹning the intention of the architect
Prescriptive 
Architecture 
(TO BE)
System 
Artifacts
Intent
Descriptive 
Architecture 
(AS IS)
Sometimes we do a software project from scratch, this is a typical academic scenario,
doesn’t happen often in industry. This is called greenfield development: this is where
the prescriptive architecture comes into play. Since we do it from scratch, there is no
pre-existing software to describe.
There’s nothing, so we need to try to design a plan before we start to build it.
145

Brown Field Development
Brown Field Development
If you go to the real world, you do what is called brownfield development: an exten-
sion or modification of an existing software system. This can be described with a picture
featuring lots of boxes and lots of connections between them.
Somewhere in there you have to pick a spot where we can put a new piece of software,
or try to determine if we touch this part, what else is going to be affected.
146

Brown Field Development
Systems developed reusing existing components already have a
descriptive architecture
Prescriptive 
Architecture 
(TO BE)
System 
Artifacts
Descriptive 
Architecture 
(AS IS)
Realization
Recovery
If you are in this scenario, then you can just reconstruct and describe what you observe.
You cannot make a plan about how to introduce a change unless you understand what
already exists. After we understand the impact of the change, we can go and make the
actual modification.
That’s the main difference between green and brown field development. Do you model
about where you want to go (prescriptive), or do you model where you are (descriptive)?
We describe something that exists and prescribe something that we want to achieve.
147

N. Taylor et al.
Architectural Degradation
• Ideal Case (P = D)
• D always a perfect 
realization of P
PD
Ideal Case
• Not all P decisions can 
be implemented
• Over time, P and D 
change independently 
and drift apart
P
D
Realistic Case
Ideally you always build following the plan to the letter. After you finish writing the
software, you realize that developers have followed your plan exactly, so the descriptive
architecture is the same as the prescriptive one. All of the decisions that you made when
you were planning were followed and enforced when you actually did it, so in the end the
description matches the prescription, they are the same thing.
This never happens. Only in the ideal case. In reality, we have some mismatches.
There are some decisions which are the same and other decisions which turn out to be
not exactly the same.
This is also what happens over time, while maintaining and evolving your system. At
the beginning, maybe you are close to the ideal case: the first release is a perfect imple-
mentation of your plan. But then you start to make changes and slowly the description
of the new version begins to drift apart from the original plan.
Originally we wanted to use this color, but it’s no longer available, we had to find an
alternative to paint the new wing. New unexpected requirements came up: we had to
make some changes and now we are drifting.
148

N. Taylor et al.
Causes of Architectural Drift
Prescriptive 
Architecture 
(TO BE)
System 
Artifacts
Descriptive 
Architecture 
(AS IS)
Ad-hoc Changes 
Quick Fixes
Why the reality turns out to be different from the plan? Because we directly touch
the code. We can always make a modification to the actual system while forgetting to
synchronize the corresponding architectural model (or its documentation). It is cheaper
to fix the bug in the code directly, it takes additional effort to keep the model consistent
with the fixed code.
This also happens with model driven engineering where you generate the code from
the model, so they are perfectly aligned the first time. But then as you make changes to
the code, the model is no longer in sync. A big challenge for this type of modeling tools
is to keep generating code from the model, but also regenerating the model from the
code. This way, the cycle becomes: we can change the model, update the code, change
the code, update the model. And we can avoid the situation where the two are drifting
apart.
149

N. Taylor et al.
From Drift to Erosion
D
P
P
D
Drift
Drift
new D decisions 
do not violate P 
decisions
P
D
Erosion
Erosion
D, P inconsistent
Over time, decisions are added to D
We use the term drift to indicate the presence of non-conflicting differences. For ex-
ample, we may make minor modifications on a fine-granular level. We still have the same
number of classes, but we added a couple of methods. The structure is the same, but we
had to write the code in a slightly different way.
We go from drift to erosion when the description becomes inconsistent or conflicting
with the prescription. We decided to write 3 classes in Java but we ended up implementing
5 in Python. The result has very little or has nothing to do with the original plan.
Erosion is a metaphor visualizing how the system has degraded. During its evolution
time has been unkind to the system’s architecture.
150

Entropy
Complexity
Initial 
Release 
Time
Ad-hoc 
Changes
This is typically what happens with most software systems.
You start from a un-differentiated state where all things are possible and then you
make these design decisions following a process. Each decision you makes helps you to
pinpoint precisely where you want to be and this helps you to control the complexity,
which will reach a minimum level as you ship your first release.
Then over time entropy creeps back in as the architecture degrades. You start to make
changes that are violating some of some of the general principles or stylistic constraints
that you carefully followed in the original design.
For example, at the beginning we had a nice layered architecture. Every layer was
on top of the other ones and then they were clearly separated and delimited with crisp
interfaces. As the system evolved, we had to make some exceptions and layers started
to spill over, the code was making direct calls bypassing entire layers. Here we are now,
without any resemblance of layering left. We just had to do it. We had to quickly release
a work around to keep our customers happy, but to do so we sacrificed the integrity of
the architecture.
151

George Fairbanks
Architecture or Code First?
Architecture-indifferent design
Write the code 
ѹrst (architecture 
may emerge from 
the code)
Design the 
architecture ѹrst 
(the code may 
drift from it)
Architecture hoisting
hoisting
Design the architecture ѹrst to 
guarantee a given quality 
(independent from the code)
Architecture-focused design
There are two general approaches to manage the relationship between the code and
the architecture.
One option is to just start coding. We are going to be agile. We will not do any design
up front. Whatever happens, happens. There will still be a descriptive architecture but
it will be organically grown based on whatever code you write. The architecture which
emerges can still be a good one. If you’re lucky, although you didn’t make a plan for it,
you need to just live with whatever results came out of it. If you make the right choice of
tools, frameworks and so on, maybe it’s also a decent architecture.
Alternatively, we can turn around the relationship. Our project will actually focus on
the architecture: we first design it and then we write the code following the design deci-
sions we made. Although typically you will start coding a little bit later, your coding will
be done in the context of a well defined structure. In general, you will have a plan. You
will know what what you want to do and how to do it as opposed to just trying to write
some code with the best intentions and high hopes.
If you skip the big upfront design and start coding right away, there is an expectation
that it will become more and more difficult to add more code. If you make a plan first,
then you have to wait a little bit before the plan is ready, but then the coding will be more
efficient, focused and directed.
So that’s idea we will follow in this lecture. If you don’t take the lecture then by defi-
nition you just spend your career writing code and getting surprised about what kind of
architecture emerge.
152

George Fairbanks
Architecture Hoisting
• Design the architecture with the intent to guarantee a certain
quality of the system.
• Developers can focus on coding the functionality while the
architecture design constraints achieve the extra-functional
requirements.
• Examples:
• Security: place sensitive data behind the ѹrewall
• Scalability: make critical components stateless
• Persistence: use a database
• Extensibility: add a plug-in framework
We are now at the point in which we ask ourselves when we start a new project: Do we
start coding first? or do we start with the design first?
We said that architects are the technical leaders of our development team. They’re sup-
posed to make the difficult decisions with limited information about their impact, and
they try to design a system which can deliver the quality that is required by the stakehold-
ers. We’ve seen these quality attributes scenarios which help you to understand what the
customer wants. Then you can try to come up with a solution, with an architecture, with
a design that will deliver those qualities.
Some agile teams they prefer to just write the code and not think about the design. Still
they will still implicitly make some general assumptions which will be deeply embedded
into their code. Maybe they choose a framework that will guide us anyway to have a
certain structure for the code. Maybe they will follow a presumptive architecture for the
particular application domain. No matter what, by definition every software system has
an architecture. Some of those describe the code you wrote. Others you can see them as
a plan for the code you are about to write. They will help you to avoid second guessing
the reason for the developers to write their code in in this particular way.
In this lecture we take a model driven approach where we first plan and then we can
build.
The idea is that if you think about what you’re going to do, you think about it first, then
you can be more efficient in the way that you do it later as opposed to just doing stuff with
little understanding. During your bachelor, you manage to acquire the skill of copy and
pasting bits of code from the Internet, putting them together, and getting it somehow to
work. That looks like you become a magician that can get a lot of stuff working, but then
you don’t really know why this is working on you don’t have a deep understanding, you
just put it together and it works.
153

So thinking about what you’re going to do first is thinking about the architecture. What
is interesting is that when you design a system with a certain architecture you have a
target quality as a goal.
Hoisting is when, thanks to your architecture, you get the quality you want no matter
what the developers are going to do. This sort of the most prominent claim or the main
challenge for working with software architecture. Can we think about our solution: the
way that we’re going to structure it, the way we’re going to deploy it, which elements we
are going to introduce so that, for example, we can design a scalable system. Then the
developers can write all the code they want to implement the functionality. But even if
they make mistakes, the system will still scale.
This is sort of the Holy Grail for architects: if you can learn how to design a good ar-
chitecture in a good way, then you do architecture hoisting, which basically means that
your design would produce the quality that you want, no matter what the code is going
to be like.
I can now give you some examples of the kind of principal design decisions that you
can make to do so.
A firewall helps to control access into a system to filter the traffic, so it’s easy to see
that introducing one in the right position would help with security.
Extensibility means to be able to grow the system with new features. Maybe you want
to extend it even after the system has been deployed. A plugin framework sounds like a
good idea so that you can plug in additional components even after releasing the system.
Using a database helps to preserve the data so that it becomes persistent. You protect it
from inconsistency, from failures during transactions, and if you make a backup regularly
then you can make it durable so your data can survive along time. You just have to figure
out how to connect your code to the database possibly using an object-relational mapper.
If you have a component that is stateless, it’s easier to scale. Why is that? If it is
stateless, its behavior only depends on the input and if it takes a long time to process it
then you can make a replica you can distribute it and process in parallel the work. If the
replicas don’t share information, they don’t have to use locks to synchronize their access
to it, and therefore you can grow the throughput of your system.
So this is a decision that you make and you say this stateless component should not
keep state between independent requests, and it shouldn’t talk to a database. All the
information that we need comes from the input and produce an output, so we can forget
about each request after processing it. Then we can scale.
If you don’t respect this constraint, then you have a scalability problem right there.
You need a database, the database becomes a bottleneck. You need to add caching for
read-only requests. You need a load balancer which can manage sessions.
Hoisting. To summarize the definition of this concept: we have a goal, we have a qual-
ity to achieve, our design of the architecture is focused on achieving this quality. This is
our extra functional requirement, for example, scalability. Within our design, developers
can just focus on making sure that the computation that you have to perform is correct,
they focus on the functionality. But the scaling comes from the architecture, no matter
which code they wrote.
154

George Fairbanks
Presumptive vs. Reference
• A Presumptive Architecture is
the default architecture for a
given application domain.
• Architects need to justify
variations from it
• Example: 3-Tier architecture
for Web applications
• A Reference Architecture is
predeѹned for a given
application domain with
explicitly deѹned variation
points.
• Architects can choose among
many reference architectures
and need to justify their
choice
The architecture gives structure to your solution for a certain problem in some domain.
For example, we will build a financial application. This is not a new domain. Software
has been written in this domain for a long time, you can learn from the past and you
can say the software for this domain usually has this structure. This is what we call the
presumptive architecture, the architecture that comes to mind to domain experts.
It is the obvious architecture for that particular type of problems recurring in a given
domain. If you know the domain, if you have experience with doing architectures in this
domain, then you can just reuse this experience and you don’t have to come up with
original solutions that you are unsure about. So this is about lowering the risk and un-
certainty about your solution. You know the domain, you know the expected solution,
you already know most of the design decisions and how to make them correctly. If you
need to change some of those decisions with some unusual alternatives, then you have
to have a reason for that. But you don’t have to justify going with the default. But if you
make a variation then you should justify it.
Another example: say you want to build a Web application, you need to do the browser
front end; you need to have the server and you need to have a database. Everybody knows
that. But if the web application is mostly static, maybe you don’t need a database. Instead
you can use Jenkins to generate the site from a database. The database is used at compile
time, but doesn’t need to be there when you run the application. Just when you build it.
So there will be a variation from the presumptive Web architecture.
What’s the difference with the reference architecture? Given a certain domain, we have
a solution which can be chosen among other possible reference architectures. You have
your domain, then you have your architecture. If there is only one, we can say this is a
default. If there is more than one possible architecture, and they’re all good, well known
and established, they become reference architectures.
And of course we need to make a decision which of the reference architectures are we
going to use?
So you can see that if there is a default reference architecture: this is the presumptive
architecture. If we depart from the default, we can change it to another architecture. We
need to justify it.
If there is no default, we need to justify the choice of which reference architecture we
adopt.
155

These definitions hold for the whole architecture. The whole set of decisions that you
make.
But you can see they are also applicable every time when you make a choice. When
you make a decision in your design process, at a certain point you have many options.
Maybe one of the options is the default. Yeah, so you can always say for this particular
issue we need to choose a programming language. What’s the default? Then you can go
and see the most popular programming languages and you can say the one at the top is
the default. But maybe there’s more than one suitable alternative, and then you have to
justify why did you choose that one vs. all the others.
156

Olaf Zimmermann
Solution vs. Product
Products
Customers
1
Many
1
Many
Product
Solution
A solution architect solves the
problems of one customer by
designing a solution
architecture made of multiple
products that are integrated
and reused as components
A product architect designs one
product architecture that can
solve the problems and satisfy
all the requirements of multiple
customers
This other definition has to do with how general is your architecture. If you have found
one customer – customers by definition have very specific requirements – your need to
customize your solution for them. Everybody has different needs, and your architecture
should solve exactly your customer’s problem. It will be extremely expensive to write
software and only sell it to one customer. This customer has to afford it.
So how can you solve the problems of one particular customer, while keeping your
solution affordable? You will do a solution: which is an architecture that integrates,
combines, customizes, configures, composes a lot of different pieces.
A solution is built by reusing many products. You put them together in a unique way
and here is your solution. You have another customer that has a slightly different prob-
lem. You select another set of products, you integrate them in a slightly different way.
There is a whole sub-industry delivering custom software solutions that does that. They
don’t necessarily develop software from scratch, but they assemble it for the specific cus-
tomer. This is something that is economically viable because you don’t have to invest
into building all the products, all the components. But you can just reuse them and re-
combine them in many different ways.
You’re a solution architect if you understand the problems on one customer at the time
and for every customer you come up with a different assembly that will solve that prob-
lem.
The other approach to building general purpose software is about writing products:
one component, one software which can actually satisfy the requirements of many, many
customers. This is for example why when you open Microsoft Word you have an endless
set of options in the menus. How many of you will need to use all of those commands
as they edit one document using this product? Everybody will use a different subset, but
the product as itself can satisfy all of you together.
So this is a product: one architecture which can help a lot of customers. And this is a is
economically viable because there’s a lot of people that buy it: everybody needs their own
feature and everybody sustains the investment into implementing their feature. Over
time features accumulate, the product grows and gets more and more complex.
157

The 1 product, 1 customer quadrant is just too expensive.
The other quadrant: general solutions integrating many products, sold to many cus-
tomers, can happen if you discover how to generalize your integration beyond the needs
of individual customers.
158

Luke Hohmann
M-Architecture vs. T-Architecture
Marketing Architecture
• Describe how to market and sell the software system to
customers (business model, licensing model, terms of use)
Technical Architecture
• Prescribe how to build deploy and conѹgure the system (styles,
patterns, components, connectors)
The architecture helps to communicate how you design a solution to different people,
who do not have the same background, the same level of understanding and who may
need to know different aspects.
Developers need technical details and expect precision. You should give them a de-
scription of a plan that can help developers to actually build the system as you want it.
Sometimes although developers have been writing code for one system, there are mul-
tiple deployment options. You can run it in a centralized environment. You can run it in
the cloud, you can melt it on the edge across multiple independent devices. The system
is the same one, but then you have different deployment plans.
Thse are all technical aspect, with decisions focusing on how to build, deploy and op-
erate your software.
But architecture is actually broader than that. And sometimes you’re interested to
design a solution for the problem: how are we going to sell the software to different
customers? Maybe we can make the people that want to a lot of features pay more than
the people that want to just to use a little bit.
159

The $10000 boolean ﬂag
M-Architecture
• Different products with
different features are sold at a
different price
• Customers who do not pay
subscription fees loose access
to some features
T-Architecture
• For multiple “products” there
can be a single technical
architecture and a single
codebase.
• Simple conﬁguration ﬂags can
be used to switch between the
different M-architectures
perceived by customers
If you think about Windows, but this holds for most major commercial software prod-
ucts, there is really more than one version available for sale. You can get the family
edition, the professional edition, as well as enterprise edition.
This also true with some software as a service offerings in the cloud: you can get a very
simple access for free and then if you want to use more you have to pay. You need to
design for each price category what features you want to bundle. It’s very difficult to get
the selection right so that people feel that they pay a fair price for what they’re using. If
you don’t overprice it, you lose customers to cheaper competitors.
What is interesting is the connection between the two sides: technical and marketing.
Say we build Windows family edition, the cheap one. And then we have the expensive
Windows Enterprise. Do you think that behind the scenes there are two separate code
repositories, written by two separate development teams? One is the family team and
the other one is the enterprise team? Or do you think it really is just one architecture,
one system and one codebase?
How can the same thing inside look different from the outside? If you try to sell it,
it looks like different products with different prices, but inside typically the difference is
just a configuration file: If you want, is what we call the $10,000 boolean flag. Somewhere
along the build process, there is a configuration switch that says: now we are going to
make a really lightweight package release which is the family edition. You have to select
the right features to deactivate, which are the icons to include, and as a result you have
a an image that users can install for that price.
At the same point in the build process you have some other configuration option that
describes how to make the expensive version. Make sure to change the logos. You will
include more packages and then here it is.
This is actually an idea going back to the old days of hardware, right is also was true
for the hardware.
Back in the days it was too expensive to buy computers with a lot of memory. But it was
also too expensive to replace the computers when you wanted to upgrade the memory.
So if you wanted to get more memory for your computer, you call the hardware company
160

and then the somebody will show up at your building. They go to the computer room.
They open up their briefcase. They take out the key. They open the computer with it.
Then they take another key. They turn the key and here is your memory upgrade. Please
pay now.
The memory is already there: it’s too expensive to ship computers that have different
memory size configurations so they will ship all the same. But people pay for only the
amount that is actually switched on. This also happens with e-car batteries, whose range
can be increased or decreased via software configuration changes.
So you can also think about this from the point of view of your software. You design a
beautiful software system. It can do a lot of stuff and then you cripple it. Then you cut
off access from some features. You hold them for ransom, unless people pay for them.
You need to worry about both sides: technical and economic if you are interested to
to achieve economic sustainability of your software. If you just give it away for free, you
may make it easier for people to try it, but then it will be hard to maintain it over long
periods of time.
161

Grady Booch
Art or Science?
• The “artistic” part of software architecture is minimal
• Creativity, Originality and Aesthetics are less valued than
Feasibility, Viability and Fitness to a purpose.
• Even the best architects copy solutions, styles and patterns
that have proven themselves in practice, adapt them to the
current context, improve upon their weaknesses, and then
assemble them in novel ways with incremental improvements.
• An architectural process can be established with intentional
artifacts, clear activities, and well-deѹned project management
milestones
Going back to thinking about these metaphors that we use: software engineering, soft-
ware architecture. They come from these long established disciplines in which it is pos-
sible to claim that it is possible to know and predict the impact about decision making.
We can make a model of a bridge so that we know whether the bridge will collapse after
we build it. This reduces the artistic part of what you do as engineer or architect. There is
a need to satisfy, a problem to solve, some physical constraints which cannot be broken.
In the end you want to know the right methods, apply them, and then you get results you
can trust.
Stateless components will be easier to scale than stateful ones. This is an example
prediction that you can make about the quality of a software system.
Therefore, if you know that there is a good pattern, there is a good style, then you can
copy it. You’re encouraged to copy that. You shouldn’t have to learn by yourself the hard
way. You are no longer among the pioneers. You shouldn’t make easy to avoid mistakes
with your own team. That was wrong. We made a mistake. It crashed. We had the outage
and our users couldn’t access our application for a while. We lost a lot of money, which
you can hope to earn back once you fixed everything. We lost our reputation, which is
harder to win back.
You want to prevent those mistakes by learning from the people that already did them.
Within the architecture practice, there is a lot of knowledge sharing: experts have a lot
of experience and they want to transfer it to you, the next generation.
This is done usually in form of patterns. So you will learn here about architectural
styles and patterns, similar to design patterns you have already heard about.
Those are all things that can help you to benefit from past experience. Still, the chal-
lenges that we have to face today to write software are much more difficult than what
people used to do 50 years ago. People now have incredible expectations about what
the software can do, so it gets all the time more difficult, the bar gets raised higher and
higher. So in some projects, there is also a little bit of research involved, especially if no-
body has done this type of system before. That’s when you should innovate, that’s when
you have to be original.
162

Grady Booch
Science or Art?
• There exists only a modest body of knowledge about software
architecture
• Scientiѹc and analytical methods are still lacking - those that do
exist are hard to apply in practice
• There is no perfect design: architecture is a wicked problem
involving subjective tradeoffs and the management of extreme
ambiguity and contradiction
• Experience counts: the best architects are grown, not born
Compared to the real architecture, software architecture is young discipline. I cannot
teach you the perfect method that you can use throughout the rest of your career.
I will give you a lot of useful ideas, point out hidden problems so you are ready to
spot them. My goal is to give you a vocabulary that will help you speak the language of
software architects. This will help you when you join a team and you can think beyond the
code itself. You can try to make your career evolve a little bit faster. Based on what you
learn here it will be easier for you to listen to the right conversations and pay attention
to the right people.
And boost your decision making skills with inherited guidance and experience, which
you will of course to specialize and refine as you learn on the job.
When it comes to making these decisions, there are so many alternatives, so many
issues that you have to worry about, that the complexity of the design space explodes.
So much tacit information and too many combinations that you have to consider. This is
what we call a wicked problem, which is difficult to solve, because there is no formula, no
crankshaft method that gives you the optimal solution. You have to explore. You have to
use heuristics. There is no single solution, but multiple solutions are acceptable.
When we going to do modeling of software architecture, you will see this as everyone
of you can do a different model for the system. This is perfectly fine. Actually, that is sort
of what we expect. So we will be suspicious of models which are too similar.
When you make these choices, which decision do I make? You have to learn how to
make compromises. When you pick your solutions, you’re making trade offs between
qualities which may have different priority. Sometimes you do it because you talk to dif-
ferent people and they have different importance. It’s not always objective: Remember
the tree on the roof? That is your decision as an architect. For the next architecture you
are free to choose completely different ideas.
As you make decisions, which need to be clear cut, you have to also learn something
that is very difficult for students. You have to learn how to work with ambiguity.
The information that you get as input, the model of the domain is often unclear and
then you have to uncover which are the tacit assumptions, work to sharpen the ambigu-
ities. How do you do so? guess what? You’re also making decisions: you make decision
on how do you interpret what the customer is or is not telling you.
163

References
• George Fairbanks, Just Enough Software Architecture: A Risk Driven Approach, M&B 2010
• Richard N. Taylor, Nenad Medvidovic, Eric M. Dashofy, Software Architecture: Foundations,
Theory and Practice, John-Wiley, January 2009
• Luke Hohmann, Beyond Software Architecture: Creating and Sustaining Winning Solutions,
Addison-Wesley, February 2003
• Ian Gorton, Essential Software Architecture, Springer, April 2006
• Luke Hohmann, Beyond Software Architecture: Creating and Sustaining Winning Solutions,
Addison-Wesley, 2003
• Peter Cripps, 
, 2010
Architecture vs. Design
 
 
164

Software Architecture
Modeling
4
Contents
• Modeling: Why, What, How
• Views
• Canonical Models: Domain, Design and Code Models
• Domain Modeling: Use Case Scenarios, Feature Models
• C5: Context, Containers, Components, Connectors, Classes
• 4+1: Logical, Process, Development, Physical
• Quality of Models: Accuracy vs. Precision
• Model Driven Architecture
165

Grady Booch
Capturing the Architecture
• Every system has an architecture
• Some architectures are manifest and visible, many others are
not
• A system's descriptive architecture ultimately resides in its
executable code
• Before a system is built, its prescriptive architecture should be
made explicit
• A system's architecture may be visualized and represented
using models that are somehow related to the code (existing or
yet to be written)
Architecture is about making the principal design decisions about your system. And
when you make all these decisions, you collect them into a model. The model can be a
description of the actual code that defines the actual existing system, so we can make a
model describing the code. Or, before anything exists, we make a plan, make a prescrip-
tion of what to do in order to solve the problem, satisfy the requirements.
Even if we are not going to start from scratch, an architecture represents your plan for
the future, based on where you are now. So you see, right now I’m here and I’m going to
go there. You describe where you are. You have to understand where you are and then
you decide about making a change. We introduce a change and that brings us where we
want to be. If you make a change without knowing where you are. You have no guarantee
that you will end up where you want to go.
And that’s why it’s important to be able to model and to know: when to do the model?
which kind of model? for which purpose? Are you describing something that exists or
are you coming up with a prescription of your intention, determining where you want to
go?
The motivation for modeling lies in the difficulty of scaling human understanding of
large amounts of code.
Developers live in a world of millions of lines of code. They work at this low abstraction
level, they commit, push and pull changes, run tests. Sometimes they get lost and need
a map. Here comes our architectural model, let’s go higher up in the abstraction level,
looking for the essence of the problems, the rationale of the solutions.
166

Richard Taylor
What is modeling?
• An architectural model is an artifact that captures some or all of
the design decisions that comprise a system’s architecture.
• Architectural modeling is the reiѹcation and documentation of
those design decisions.
Modelling is making models which capture design decisions that build up the software
architecture.
While modelling, we go through a process to actually write that model, to make those
decisions, to document them and make them explicitly.
There is a difference between waking up in the morning having dreamed a great idea
and then going out and convincing everybody else in the team that indeed it is a great
idea and making sure that it actually gets implemented into the code.
There is some inevitable friction between your ideas and your will as a designer, and
the rest of the world. How do you convince others that their code should follow those
ideas?
167

Abstraction and Interpretation
Model
Abstraction
some information 
is intentionally 
left out
“Real”
Interpretation
solve ambiguities 
add missing 
decisions
System
• The architecture models only some interesting aspects of a
software system.
How do we do abstraction? We simplify. We leave out details.
We need to know: What is it that we’re trying to describe? What is interesting about
it?
We will learn that depending on, for example, the type of quality that you worry about
– for example, security – you will need to understand particular aspects about your sys-
tem that have nothing to do with the ones that you need to grasp when you want to worry
about, for example, performance, for which you would benefit from a completely differ-
ent type of models.
After you leave out information, the moment that you have an abstraction, if you want
to go back, unfortunately there are gaps: missing details which we need to deal with as
we interpret the model. Given a model, there is somebody looking at it, and providing
their interpretation by filling out the missing details.
This is what every builder does when they look at the master plan. They have it on a
piece of paper and they look at the wall; the plan shows a hole, there is supposed to be a
window there. We look at the wall. There is no window. Let’s start digging.
A model is separate from the reality it represents. You’re trying to check all the time
whether the architectural model fits with the code. If the code doesn’t exist yet, then
it’s even more difficult because you have to follow the plan and then check that your
interpretation embedded in the result matches the original intention.
This is how the process of dealing with ambiguities works: deciding how to fill in these
gaps, decide how to specialize abstract decisions.
The model explicitly contains some of the decisions but others are left open. It’s up to
the developers to close them based on their knowledge, experience and personal inter-
pretation.
As an architect, you may know who the developers are. You know what they can do,
then you know which decision you can leave open for them to worry about. If instead
you design an architecture and you don’t know exactly who is going to build it, then you
168

have to be more careful and precise.
The process can also run in the opposite direction when you try to recover decisions
that were embedded into the code: you are trying to come up with a reason why previous
developers wrote it in a certain way. They chose a linked list. They didn’t use an array.
Why did they do that?
169

Mary Shaw
Solving Problems with Models
Problem
Solution
Solve directly
Abstract
Problem
Abstraction
Interpretation
Abstract
Solution
Solve model
• Abstract models help to ѹnd solutions to difѹcult engineering
problems.
Models are not just something that helps to abstract, to describe, and to make a plan.
Models are also useful as a technique that helps us to solve some real-world problems
which are more difficult to solve directly in reality as opposed to solve them in the context
of a model of reality.
This is where you can take the complicated example of building the bridge that won’t
collapse. Or to design a wing which will lift the plane up. You can just do it, build it and
see what happens as vehicles of increasing weight are self-driving over the bridge.
Alternatively, you make a detailed plan, which is amenable to simulation. We know
exactly how heavy is the airframe, how many passengers and items of luggage we are
going to load. And don’t forget the fuel tanks. We feed this into a simulation, control for
different weather conditions, and check if the design meets our expectations.
The problem is that the boundary conditions for which the bridge design is guaranteed
to work are valid in a given historical context. Half a century later, the traffic doubled,
trucks are twice as heavy, and the climate is getting warmer. And there seems to be a
problem with the bridge, which after all has the same architecture of a similar one which
just collapsed.
As opposed to a direct solution, solving problems with models requires to go up and
down. We did both abstraction and interpretation. In the middle, however we added
something else.
First we make the abstraction of the problem domain. Then we make a transformation,
on the model level, going from abstract problem to abstract solution. Finally, we go down
and come back into the real world, and check if our solution fits. This is how we can use
models as a problem solving technique.
Example: if I ask for two volunteers to help me with solving this problem. If one of you
will drive north from Lugano. And the other will drive south from Zurich. Where are you
going to meet somewhere in the middle? Will it happen exactly on the Gotthard pass?
170

There is only one way to find out: who has a driver’s licence?
While they go get their cars, does anyone know how to make a prediction about when
and where their paths are going to cross?
We can make some elementary abstraction: we can start to model the car as a physical
point, that has a given mass, position and speed. We can assume each car has a con-
stant speed and it drives in a straight line. We can check on a map how many kilometers
are there between Lugano and Zurich. And if there is no traffic, we can sort of make a
prediction based on the average speed of the meeting point location and time.
Everybody who has been admitted to this study program should be able to solve that
problem with paper and pencil, or maybe a high performance calculator.
That’s what you learn in elementary school: how to solve simple arithmetic problems.
You have some formulas, some rules, enough knowledge that allows you to transform
the input, the description and turn it into a shape which fits with your formulas. You
have algebraic methods which help you to solve for the unknown variable. And finally,
problem solved: here is your result.
A result which of course is approximately wrong when you go back to reality. There’s
lots of uncertainty. Straight roads are unlikely to be found, especially crossing the Alps.
Cars don’t have instantaneous acceleration and deceleration (you will need to stop the
car before you can meet in the middle). And, especially on the Lugano side, traffic speed
will significantly depend on the time of the day.
Those things are all challenging to abstract. You can also try to include those in the
model to feed them into your prediction, or you can just open Google Maps.
They offer a model-driven problem solving service, thanks to crowdsourced traffic in-
formation, and their detailed, annotated maps, which include metadata layers such as
speed limits. They will freely give you a more faithful estimate, which can also be dy-
namically updated as they track you along the trip, thus further improving their traffic
models.
Indeed, we can always try to solve problems with ”black box“ models, but would you
trust such oracle with your life? Or do more like those brave astronauts who wouldn’t
step into the capsule on top of the rocket, unless a human would have double checked
the orbital numbers obtained from the machines in time for the launch.
171

George Fairbanks
Question ﬁrst, Model second
• Different models have different purposes
• Know what questions you want the model to answer before you
build it
All models are wrong; some models are useful.
This is what we’re going to do: We will make two kinds of models.
The domain model represents the world, the problem that we’re trying to solve, and
then we make design model which is the solution. How do we go from one to the other?
What kind of languages, notations or meta-models can we use to represent that infor-
mation as precisely as possible.
As we work with models, we can spend time and effort to decide whether to capture
more or less details.
But you should only invest into such effort to make a model if there is a reason for
doing so. If the model has a purpose. If you just do modeling for modeling sake, you
will capture a lot of information, which could be useless or potentially useful. It will take
a lot of work. You can even make some beautiful diagrams, but then you look at them
with wonder and ask yourself: what’s the point? Does this model help me to make some
architectural decisions in a better way?
Before you invest into doing a model – as opposed to writing code, for example – you
should be aware about why you’re doing it. What is the question that you’re trying to
answer with the model?
Keep in mind that the moment that you go from the reality to the model, you leave out
information. You simplify or even oversimplify things. Your model never perfectly fits
with the reality it should represent. Considering the previous example, we cannot say
that the road is always straight. We cannot say that the load at take off is necessarily the
same as when we land the plane.
But still, even if your model can only be an approximation, make sure it is a useful one.
172

🔎 Scope of the Model
˾. Parts of the software (e.g., as it is deployed on each device)
˿. Speciѹc use cases (e.g., which API is invoked by some user)
̀. The whole architecture of the entire software (covering all use
cases)
́. The style of the software (constrain all possible architectures
without describing any of them)
Overview ѹrst, zoom and ѹlter, then details-on-demand.
When we work with the code, the code can be very large. Real software systems are
huge. Do we need to model the whole thing all the time?
When we do a model, how are we going to focus its scope? We can try to represent
the entire architecture covering all use cases. This is the panopticon, where you can see
everything, the model is complete: we know about everything and we made all possible
decisions. None is left to interpretation or chance. If one could only capture the position
and speed of every particle in the Universe.
Let’s be pragmatic: how much time do you think it will take you to finish such complete
model? If I ask you to do an assignment like that: pick an open source software system,
then model everything in it. All the use cases should be covered, no matter whether most
users perform them every day, or just few users may need to go through them once in their
lifetime. Would your modeling effort be justified? What if you are following a waterfall
process, will you ever be able to start writing that first line of code?
Sometimes it’s much better to model the architecture planning how to solve only se-
lected use cases. Look at the scenarios, prioritize them, and identify the critical ones.
If we don’t do these well, we will fail. And then you can start modeling those and focus
your problem solving power on thinking about how to solve those first.
The resulting model will not describe everything, but it will fit on a slide you can
present to the team in a finite amount of time.
Concerning the solution part, we may know that our architecture will be structured
with five components. Do we need to describe the inner workings of all of them? What if
we decide to reuse some components? What if we buy another one? We can concentrate
on modeling the interfaces, the dependencies and how the components will be wired
together. Maybe it’s important to worry about a particular deployment configuration as
opposed to planning for all the possible ones.
Sometimes the system has grown to be so large and so complicated, which we know it’s
173

impossible even to attempt to ever describe it completely. For example, the system is so
dynamic and so large by the moment that you finish making a list of its parts, something
has changed.
Have you tried to describe the architecture of the World Wide Web? Start making a list
of Web servers, and a list of Web browsers running on the corresponding fixed or mobile
devices. Each list is likely to contain billions of entries. Then you can enumerate – at
one particular point in time – which browser is connected to which server. While such
enumeration will be immediately obsolete, will you be even able to take a consistent
snapshot of the Web’s deployment configuration?
This is an example of something so large, so dynamic that doesn’t make sense to talk
about its current configuration because it’s obsolete by the time that you finish describing
it.
So how can you understand how it works? How can you reason about it? You don’t
look at the instance, but you model the underlying design principles. Ask: what is the
style that this software is following? The answer is a model that constraints the decisions
that you make when you look at the individual system, but you don’t describe the actual
system because the system size is overwhelming.
That’s another level of abstraction that you can use to still work with software archi-
tecture. In this case the models will be about clearly defining design constraints, spelling
out the general principles that you want to guarantee and enforce across all and no matter
how many concrete instances will be deployed.
Another dimension that you need to understand when you do a model concerns the
reason why you do the model. What are the concerns that you are trying to address?
Are we trying to make a quick sketch? Are we going to make a complete and detailed
representation of the entire system? Are we going to capture decisions at the meta level
about the style of multiple systems that we want others to design?
When you do your assignments, we will start with one use case and one component.
This is a model with a narrow scope, but gives you a core which you can grow. You can add
more use cases. You can add more components. At some point you have enough com-
ponents and you notice that you can satisfy additional use cases by simply recomposing
the existing components as opposed to adding new ones. And you don’t have to cover
the whole thing, you can stop anytime, write some code and then go back to modeling if
you start encountering unknown territory.
A useful trick to remember when you’re describing something, especially if you don’t
do it on paper, but if you do it on a computer, take advantage of the fact that you can
zoom in and out. And you can filter and interact with the model.
You should be able to explain quickly the big picture. This is what we call the nap-
kin model or the one powerpoint slide model. It should always be possible to explain
something in one minute to a person that never heard about it.
Imagine you have new developers joining the team. They have to get on board their
first day. How are you going to explain them about your software without spending years
in training or having them go through the entire commit history log? There should be a
model, providing a general overview and then as you get questions you can zoom in and
show as many details as necessary.
174

What is a view?
• No single modeling approach
can capture the entire
complexity of a software
architecture
•  Various parts of the
architecture (or views) may
have to be modeled with a
different:
•  Notation
•  Level of detail
•  Target Audience
• A view is a set of design
decisions related by common
concerns (the viewpoint)
View
System
Viewpoint
In this part we start to present many different types of models of a software system.
While doing so we will also show some notations that we can use to represent them.
The idea of views comes from the fact that software is a complex, multi-faceted digital
artifact. Both considering its code, but also beyond code, there’s just so much complex
information that you cannot capture it all in one place, with a single model.
If you want to organize the model of a software system, you want to decompose the
model according to different viewpoints or perspectives. Each of those is concerned with
different aspects of the software.
This is similar to what you do when your plan a building’s architecture. You make
a plan for the structure, another plan for the electrical system, a separate one for the
plumbing. These are all models that are given to different types of builders. Different
types of developers that will then of course work on the same building.
Splitting the model into different views requires that the views are kept consistent.
They should share the same building, be applicable to the same system. Don’t make the
rookie mistake to forget to include the same shared elements across all of the views. It
would be like forgetting to wire the last floor of the building.
Each view can be characterized by a different level of detail, by a different notation, by
a different way to represent it. Also each view can be used to talk to different people: a
view that is used by marketing people to sell your software (e.g., a colorful PowerPoint
slide), or the big picture view used for onboarding. Another view can be so detailed that
can be used to automatically generate the code from it by developers.
175

Rozanski & Woods
How many views?
• System Context
• Functional
• Logical
• Physical
• Deployment
• Development
• Information
• Process
• Concurrency
• Operational
• Security
• Performance and Scalability
• Availability and Reliability
• Evolution
• Teachability
("Welcome to the team")
• Regulatory
• Marketing
• Business Impact
We consider our model as a set of design decisions for which we picked one alterna-
tive for the corresponding design issue. Since there are so many of them, we can try to
group them together. For example, all decisions that have to do with the structure of the
system in one view. All decisions that have to do with the interaction between different
parts. Decision on how we deploy the system. These are all decisions that can be grouped
together in the same view.
There is one model, one set of decisions, which we project or subdivide into multiple
views. We make multiple views over the same architectural model. You can represent the
views with many different diagrams types.
What’s the difference, for example, between a class diagram and a sequence diagram?
One is about structure, the other is about the dynamic behavior. So those are two differ-
ent viewpoints.
You can slice your model and just look at the surface. Just look at the part that has to do
with the user and then you can focus on that and decide what kind of user interface you
are going to use. This depends on: how long does it take to perform certain operations?
Is our system fast in responding or slow? If it is slow, how do we let the user track the
progress? or cancel some operations? These are concerns that come up from the outside
and they need to be addressed as you go back modeling the inside.
What is the most appropriate notation to design a user interface? What’s the most
appropriate notation to worry about security?
How do we understand what the system is supposed to do? We model all the use cases,
we collect the stories. We put them together in a view concerned with describing the
problem domain.
When I say we need to make a decision whether to run the system in the cloud or not.
What kind of viewpoint would that be?
We have three components. This is our structural decomposition. Then we need to
decide which ones we are going to run in the cloud. Which kind of decision is that? The
Cloud is related to the runtime environment that we want to target: the physical view.
However, the moment when you connect the structure with the physical view, you are
making a deployment decision. Structurally speaking, we have three components. And
176

then we have the cloud over here and the mobile phone over there. So we have two
physical deployment targets for three logical components: we need to decide which goes
where. Given this logical view and these physical views, how do we relate them? How
do we decide where to put each of the components? This is described in the deployment
view: a possible mapping between logical and physical views.
Given the same structural decomposition, we can have multiple deployments and dif-
ferent runtime environments. We can make decisions about the structure of the system,
which are independent of where each component is going to run.
As we start to design the architecture, we need to sketch an overview and answer: What
is the context of my system? If you have zero knowledge about the system, please start
from modeling its context. This is very simple to do. You just have your system in the
middle and then you have the rest of the world outside, surrounding it. So try to be as
abstract as possible in describing the rest of the world. Typically this would include the
other systems that are in contact with our system and the type of users that interact with
the system. If you design a self-contained system meant to be also isolated, with no
relationships with other systems, then there is no need to put it into context.
Another viewpoint concerns decisions on the development of the system. For exam-
ple, the choice of programming language. The choice of which development process and
version control tools to adopt.
Other decisions impact the structure of the information we work with. Is it structured
data? or semi-structured data? How do we turn raw data into information so that it can
be refined into knowledge? You have three other lectures about that.
The static structure is complemented by models of the system’s dynamic behavior.
What happens over time? What is the workflow? What kind of concurrency are you going
to put in the system? is it going to be single threaded or do you have multiple threads?
Is it going to be distributed? How?
Further decisions come into play after deployment: How do we operate the system?
How do we start it? How do we monitor it? How do we check that everything is running
fine?
We also have all the viewpoints that have to do with different qualities like perfor-
mance, reliability or security. We introduce a security view, if we want to emphasize the
weaknesses: all of the points that an attacker could use that we need to defend, but we
can ignore the rest. So if you make a plan about how to defend your system: where are
you going to do the access control? or where are you going to authenticate the users?
Where are you going to do the encryption? It’s important to map the communication
paths and highlight the ones expected to travel across the local network or the open In-
ternet, so we know where to place the firewall. Those kinds of decisions we can see them
under a security viewpoint.
Other views can represent the evolution of the architecture over time. For example,
watch with an animation visualizing how the structure grows over different releases. Or
charts to track the user traffic growth and the corresponding conversions, monetization
and marketing costs, to observe whether it is sustainable.
There is the ”welcome to the team“ viewpoint: how do we get the people onboard so
that they can start to contribute their effort to developing the system? How can you join
the team and have the right knowledge about where the project is going?
You might even have views that just focus on: how do we make money with this system?
How do we sell it? And how do we comply with legal rules and regulations?
Your goal should be to dive into each of these viewpoints. You should learn: What kind
of decisions need to be captured? How do you properly describe them? Which notations
are there?
177

Multiple Views
• There is too much information to model: we need multiple views
Logical
View
Real
System
Deployment
View
Physical
View
View Consistency
• Views are not always orthogonal and should not become
inconsistent
Real
System
Physical
View
Logical
View
Deployment
View
Views are projections of your architectural model into multiple viewpoints. You can
have as many as necessary.
178

However, the moment that you put them back together, you need to ensure the deci-
sions found within each view are globally consistent. You are going to build one system,
and therefore all of its views need to be aligned, point in the same direction.
For example, a model’s structural view has three logical components. But when you
look at the deployment view you count them and you see four components. Where did
the extra one come from? Is there a missing logical component, or did one of the logi-
cal components get instantiated twice during deployment? These are the questions you
should not raise with your models.
Inconsistencies make it difficult for the developers to know which decision to follow:
in one view, you make one decision in the other one you contradict yourself (or the rest of
the architecture committee). This is beyond ambiguity, which you can expect developers
to interpret away, inconsistency requires them to take sides.
179

Domain and Design
Models
Fairbanks Domain
Model
Design Model
Boundary
Internals
Bosch
System context
Component Design
D Souza Business
Concept
Blackbox
Whitebox
Jackson Domain
Domain+Machine
Machine
RUP Business
Modeling
Requirements
Analysis & Design
Brown
Context
Containers 
Components
Domain 
Model
Having introduced the fundamental idea of multiple views, we can now move on and
split the model into the two most important perspectives.
One of them collects decisions which are outside of your control as an architect. The
other instead collects your main design decisions and is fully under your responsibility.
These two canonical models have been mentioned in the software architecture litera-
ture under many different names. One of the first to use this term was Michael Jackson in
his book about requirements engineering, distinguishing between the domain, the ma-
chine, and the domain plus machine: the point in which both sides come into contact.
We can understand domain modeling as a form of business modeling, where we capture
the architecturally-relevant requirements.
There’s no point in having an architecture if you don’t know what the requirements
are. We can apply our decision centric view also to requirement engineering.
You can put decisions in the domain model. I decided this is how the world looks like
and I have agreement from the customer that this is true and this is indeed so. My as-
sumptions and interpretation about the requirements are explicitly listed in the domain
model.
If I make a mistake here then I will be building the wrong system with the wrong quality.
The design model represents your solution. Here we have lots of different terms that
we can use: the machine, containers, components, or simply white and black boxes.
This is important, as you can describe your system just from the surface, from its
boundary. Or you can look inside and have a more detailed model of how it works. In
general, the design will focus on how do we plan to solve the problem.
180

Modeling = Learning
Design
Domain
General
Reѹned
Level
of
Detail
Speciѹcation
Path
The modeling process is non-linear
Analysis and Synthesis cannot wait for each other
Architecturally relevant requirements are discovered from the
domain while making progress with the design
Before we dive deeper into each of the domain and the design models, let’s clarify their
relationship.
Let’s once and for all stop the waterfall notion that we have collected all the require-
ments at one point and then and only then we can go into the design.
This is never the case. Instead, the more progress with your solution, the more you are
learning about what the domain is about. It’s not necessary to have complete knowledge
of the domain before you can start to solve its problems. Only when you need more infor-
mation during the design, you go back to the customer and get the missing information.
At the beginning of the process, you know nothing about the domain. Also there is
nothing in your design, they are both empty. The set of architectural decision is empty.
While you grow it, you accumulate both design decisions and the domain knowledge re-
quired to make them.
While modeling, you follow a non-linear approach in which you don’t just analyze
the requirements to synthesize your solution, but you actually do both and continuously
switch back and forth between different sides.
When we teach you, we do it in two discrete steps. First, let’s practice domain modeling
and then we are going to do the design. But in reality you actually keep moving between
these two all the time.
Still, it’s important that you’re aware where you are working. Are you working on the
domain? or are you working on the design? Because the design you decide, but you are
only an observer of the domain.
The idea of jumping back and forth was illustrated with a metaphor called the ”Twin
Peaks model of requirements and architecture“. You do not climb one peak before the
other, you climb both at the same time.
181

George Fairbanks
Domain Model
• Refutable truths about the
real-world
• Outside your control
• Your system will be evaluated
against it
• Architecturally signiѹcant
requirements
• Problem domain description:
•  Information (invariants,
navigation, snapshots)
•  Functionality (use-case
scenarios, feature models)
•  Deѹne shared vocabulary and
understanding towards your
customer, domain expert
•  Avoid analysis paralysis: stop modeling the domain when all
your questions about the problem have been answered by the
domain experts
Let’s now look at how to model the domain. The domain is a collection of decisions
that are outside of your control. Modeling the domain is about learning about how the
problem looks like, how the real world looks like. Typically this is not something that
you imagine, but it’s something that you discover.
After you produce a solution by writing some software, the domain model can be used
to put it through the acceptance testing. Users and customers will evaluate it and see if
it’s good enough for them as it fits with their requirements.
The domain knowledge should be like a contract stating all the goals that we need
to achieve. This is a list of all the architecturally significant requirements. If they’re
not architecturally significant, they are second order requirements. You can give them
directly to the developers, as they won’t affect the architecture.
So here is already one filter: does the requirement matter? Is it architecturally signif-
icant?
If yes, you collect it in the domain model and then you take it into account when you
design the rest of the system.
You cannot go against the domain model contract, but you can still influence whenever
you make a clarification question, when you interpret and formalize the input from the
customer. In general, you have to let somebody else decide ultimately what is in the
domain model.
For example, you begin with a goal of delivering your software with medium-high avail-
ability: what exactly does this mean? Is 99% enough? You don’t know, but you can pro-
pose the figure and get it signed off by the customer who originally brought it up.
What is a refutable truth? A statement which you can evaluate whether it’s true or
false. A statement about the real world in which your system will have to fit in.
No matter what notation you use to capture this, you can start by collecting a list of
statements about the environment in which your system will work.
182

So how do we capture how to describe the domain? Usage examples, invariants (no
matter what, this has to hold), configuration snapshots, concrete data samples. It’s eas-
ier for customers to come up with multiple concrete cases, which you can attempt to
generalize and extrapolate, while still validating the result. If we want to describe func-
tional aspects, try to use stories, scenarios, but also feature models.
The goal is to be able to communicate with, listen to and get feedback from so called
domain experts. The domain expert is the person that gives you the information for the
domain model. The domain expert can be the customer, or someone who comes along
with the customer.
If you would architect an application for a bank, for example, you would need to inter-
act with the foreign exchange desk of the finance department. With your IT background,
you probably do not grasp most of the concepts on how currency markets work. Your goal
is to discover what is relevant for your domain model. Collect working definitions of data
items, clarify the lifecycle of the main objects, know what information is supposed to be
captured so that you have a clear picture of what you are supposed to do with it as you
design the application.
In the same way you do not complete the design process instantaneously, you cannot
discover and learn everything about the domain model in one shot. It’s a gradual process,
which – as we discussed before – goes hand in hand with the design.
183

Example Domain Model
• Music songs are organized in albums
• The same song can be authored by many artists
• Listening to each song costs 0.99 CHF, but short samples can be
heard for free
• Songs can be downloaded and also live streamed
• Songs are stored in ѹles of standard MP3 format
Design Model
• Refutable truths about your
system
• Within your control
• Prescriptive: Your system will
be built based on it
• Descriptive: Your system is
represented by it
• Interfaces (externally visible
behavior, data interchange)
• Quality Attributes (how to
achieve them)
• Structural decomposition,
component assembly
• Deѹne shared vocabulary and
understanding within the
development team
As opposed to the domain surrounding our system, we look now at our world inside the
system that we need to create. This is where you can invent your own world. You write
it, the software becomes alive running on your computer and you are totally in control
of what is going to happen. Sometimes you write code but then something unexpected
happens. Still, it is your design decision, for which you take responsibility.
184

The design model can prescribe a plan for the system that will be built, or describe an
existing system which has already been built.
What kind of information do we collect in the design model. Starting from the outside,
for example: what kind of data interchange, the input, the output. How does it look like?
What’s the information structure? What kind of commands will kind of operations we
can perform from the user point of view?
Then we need to decide which are the important qualities: How are we going to deliver
them? You will also have an idea of: how to split the problem? How to decompose your
solution? How do you assemble back together the system from its constituent compo-
nents?
The domain helps you to talk with a customer and reach a shared understanding about
what does it mean to talk about the main domain concepts – like a song, an artist, or
streaming versus downloading. These are all concepts which should be clarified towards
the outside.
The design model helps you to have a shared understanding within the team. When
you mention components – like the music player – everybody knows what you are talking
about and you don’t have to give all the details again.
The reason why we try to come up with models, with representations is that with little
information we talk about a lot. We refer to the name of a thing, everybody knows what
it is. We don’t have to explain again what we’re talking about. Naming things, com-
ing up with the shared vocabulary, is really difficult. This is the most important thing
that you do: assign good names for the right components, which will be adopted from
now until the rest of the history of the system. You will use this name. If you give a
good name, people will understand, and if you come up with the wrong name, will be
an obscure component that everybody stays away from because they don’t know what it
means. Or worse, they will assume different semantics, detect their misunderstanding
later and waste energy arguing on how to solve the renaming problem as opposed to the
real problem.
185

Example Design Model (Interfaces)
• Streamed songs should begin to play after a max delay of 5
seconds
• Payment messages should be transferred with an encrypted
standard protocol
• Songs are stored in ѹles of standard MP3 format
• Playlists cannot be modiѹed while they are being played
Example Design Model (Implementation)
• Playlists are programmed as a double-linked list
• The playback thread should have a high priority
• Sound decoder component built with a MP3 library
• Payment messages should be validated before processing them
186

Some Modeling Notations
Domain
• Use Case
Scenarios
• Feature Models
Design
• C5: Context, Containers,
Components, Connectors,
Classes
• 4+1: Logical, Process,
Development, Physical
(and Use Cases)
How to distinguish design from domain? If you hear about linked lists, chances are
that this is an implementation design decision, but it’s not always easy to distinguish
because it really depends on who said what: you, the architect, or your customer, the
domain expert.
How can we represent each type of decisions? For the domain, I believe you already
know about use cases. We will also work with feature models.
For the design we have, for example, multiple viewpoints: context, containers, com-
ponents, connectors, all the way down to classes. Starting from the outside, the design
gets more and more specific.
The context shows the overall big picture of your system. Then you decompose it into
containers, which are related to the runtime environment needed to operate it. Every
container will have some components deployed inside, which will also need to be con-
nected together. Finally, inside the components we have the classes. This zoomable
representation follows the progressive refinement construction and is a relatively new
idea. When you start from scratch, you first describe the big picture and then as you
refine it, the notation helps you to gradually get closer to the code.
We also present one of the first proposals that was made to take advantage of multiple
views for modeling software architecture. This is called the four plus one: logical, pro-
cess, development, and physical are four different viewpoints. The plus one is supposed
to be the use cases from the domain model. For each use case, you make a logical, pro-
cess, development and physical view. These are all merged together, since you need to
create the design for one architecture which can satisfy all use case scenarios.
187

Use Case Scenarios
• The model of the architecture can be broken down in scenarios,
each illustrated using the other views
• Scenarios help to ensure that the architectural model is
complete with respect to requirements
• Scenarios can be prioritized to help driving the development of
the system (most critical for success, most expensive to build,
most useful, most risky) according to different stakeholders
expectations
Philippe Kruchten
Use cases can help us a lot to deal with the question that we asked earlier: why are we
doing the model in the first place?
One answer can be: we do it because we need to describe how are we gonna support a
specific use case. Depending on the use case, we use some other views to represent how
the system works in each case.
The more scenarios you model, the more functionally complete your architecture is
going to be.
Since there are typically multiple scenarios, you should learn how to prioritize them:
Which scenarios are important? and which scenarios are critical? Who decides whether
something is important, critical or relevant? It depends.
In some cases you can talk to the user. You can talk to the customer and ask: if we
deliver half of the scenarios, which one should come first?
Sometimes you talk to developers concerning how difficult each scenario is: I can do
it in 5 minutes, we will need two weeks of hard work before we can actually build it. This
one we don’t know how to estimate, it’s too complex and risky.
Should you start building the most difficult case? Or start from the easiest use case?
You can prioritize based on risk: low risk ship first, postpone high risk until we actually
learn how to do it.
In general, when you build a system to support all the scenario, the sequence that you
follow is important. You don’t get the same result if you change the order of the scenarios
as you work incrementally, so it’s important to think about what’s the right sequence of
steps that we follow to build a system. One scenario at a time. If you try to do them all at
once. It will not work. It is too much, so you have to start from somewhere. All complex
systems which work have started as a smaller, simpler system which worked. So make
sure that once you add a new scenario, you still leave the door open for adding the rest
of the scenarios.
188

Example Music Player Scenarios
˾. Browse for new songs
˿. Search for interesting songs
̀. Play the song sample
́. Pay to hear the entire song
̂. Download the purchased song on the device
̃. Play the song
̄. Play multiple songs on a predeѹned playlist
̅. Play multiple songs in random order
̆. Share songs with friends
˾˽. Make a backup of the device's content
˾˾. Suggest related songs
˾˿. Generate a tasteful playlist
˾̀. Display album cover image
˾́. Show the device's battery status
˾̂. Record sounds with a microphone
Feature Models
• Decompose complex functional or extra-functional
requirements into features (required vs optional)
• Represent valid feature combinations for product lines
• Constrain the set of all possible products and control feature
interactions
• Determine which feature conﬁguration is found in an actual
product
• Compare competing products (commodity vs differentiating
features)
Feature modeling helps to take complex set of use cases and requirements which can
be both functional or extra functional and give it some structure.
Features can be decomposed into a tree, and we can classify them, for example by dis-
tinguishing the ones that are required from the ones that are optional. You can also
189

determine whether some features combinations make more or less sense.
While it’s not a problem to manage a large collection of features, the main challenge
lies in capturing and dealing with expected or unwanted feature interactions. A feature
model helps you to explain to the customer that if they introduce one feature, they cannot
have another one. So you can also present mutually exclusive features.
Feature models go beyond enumerating the features for one particular system, but they
give you all valid combinations of all possible features that you can have.
Compared to design spaces, with issues and alternatives, feature models use a slightly
different terminology. Features can be broken down into sub-features, with multiple
valid options. A feature configuration captures your decision on which set of feature
options do you select for this particular product.
If you don’t make the decision at design time, what you end up is offering to the end
user a family of products which can be configurable based on the feature model.
This is like when you buy a car, you spend a lot of time going down the list, cherry
picking interior finishings, outer colors, how many doors, how many gears, which type of
fuel, while keeping an eye on the price and the delivery date. Then you end up ordering
at a premium the already configured and actually built car sitting in the dealers parking
lot, which you can drive away today.
Also when you install software, you can configure the features that you want to have
in your concrete instance of the system you are about to deploy. A user friendly wizard
will ask you: Do you want to enable or disable all of these features? That’s also a feature
selection decision that you make as the user as you pick a different software-as-a-service
subscription plan.
Feature models are also used to make a comparison when you have alternative prod-
ucts. They share the same domain, they give you similar set of features with some differ-
ent options. Using these models you can spot which are the commodity features, the ones
which all products support, and which are the special features which make your product
unique.
If you can differentiate your software’s features, then you can position it in the market
and know how you can sell it better. For example: we are the same like this famous com-
petitor, but we run on this newly emerging platform. If you want these N features, you
need N separate products, we are the only one integrating them all together. Everyone
can do this, but we have (or are working on) an extra super cool feature.
190

Feature Model Example
Screen
Media
Network
Input
Storage
Video
Audio
Keyboard
Wheel
Touch
SD-slot
Memory
1 GB
2GB
4GB
8GB
Media Player
SIM
Radio
WiFi
BT
3G
4G
Headphones 
Jack
Color
B/W
Color
Let’s go back to our running example and see how we can model the features of a media
player software. It’s a device which can play media: what does it mean? To do that you
need to have some storage to store the music. You need to have some input, for example
to control whether to play or stop. Maybe you want to browse the songs, select the ones
you want to play. Then you should also determine what kind of media are you going to
play. Indeed media is very general: video, audio. Which type of media format? Does the
device feature any kind of a screen? Not necessarily, if you wear it on your armband, you
just want to listen to the music while you go running. So this sounds like an optional
feature.
Where does the music come from? What kind of connectivity does the device have?
There is a generic network type feature, which is still very abstract.
Overall, the feature model tells us we need to worry about storage, input, media, screen
and network, but it doesn’t really tell us how are we going to do it.
If you want to make the media concept more concrete, we can describe a media player
for audio, another for video, and one for both.
There are also many options concerning the input controls. The original iPod had a
click wheel, excellent for scrolling but painful for typing. Or you could mix the input and
the output with a touch screen. If you choose the touch screen feature, you need a screen.
If you want to store the media files, you need to have some capacity. That feature can
be part of the business model: the bigger it is, the more music it can store, the more
expensive. We also have the option whether we can have an optional slot to add more
storage with preloaded music.
What kind of network connectivity do we need to support? Wireless or wired? Which
kind of antenna do we need? Bluetooth or 3G, 4G, 5G. This is a broad set of options and
probably not every device will support all of them, and some of those will need a SIM
card, right? If you want to do 3G over the cellular network then you have to have a SIM
card, which itself comes with different shapes and sizes.
Finally, the most controversial feature of all: If we have an audio player, should we be
able to connect it with some headphones? Should we do so through a standard cable? Or
if there is no headphone jack, we need to pair them via Bluetooth?
191

If you have courage, sometimes you remove features. Once they become optional in
the feature model, they are no longer expected to be present in all devices.
So this is a quick example and we could keep going and refining the tree, making a
deeper, making it broader. The screen feature can have many different sub-features. For
example, the resolution comes to mind, but the simplest thing that you can decide is
whether it’s a black and white or colorful screen. If you want to watch video, you probably
need color.
192

Feature Model Constraints
Required vs. Optional
Features
Alternative
Features
(Pick one)
Feature Combinations
(Pick a non-empty
subset)
Dependencies and Feature Interactions:
F1
F2
Require
Exclude
F1
F2
If we want to be more precise and give you more details about whether, for example, a
feature is required or optional, then we can annotate the tree using some simple visual
convention.
If we draw black dots, then we say that this feature is required. Every single instance
of this particular system must have it. Or, like the headphone jack, maybe it becomes
optional (white dot).
If you make it optional, somebody has to choose whether they want it or not. If it’s
required, you already made the decision. It has to be there.
What kind of choice should you make regarding the network connectivity? Here mul-
tiple combinations are possible, or should you just pick one out of the possible ones?
Alternative options (listed under a white triangle) exclude each other: the screen is ei-
ther black and white or color, although color screens can also just display monochrome
images.
If it is possible to pick an arbitrary nonempty subset of the options, then you group
them under a black triangle. You can pick at least one option.
You can also leave the branch unmarked and then it will be possible not to pick any
option.
There is a couple of more relationships that we can express, for example dependencies
between features: if you want to this feature, you need to have the other feature. If you
want to have an input based on touch then you have to have a screen that you can touch,
otherwise it doesn’t work.
Or you can have mutually exclusive features. This is similar to the white triangle, but
the white triangle only works from one level to the next. For example, again, the color
of the screen can only have one of the two options. If you draw the double headed arrow
you can do express this ’both cannot exist at the same time’ relationship between any
arbitrary pair of nodes of the tree.
By adding these constraints, expressed using this notation, then your model becomes
more powerful because it can help you not just to describe what are all the possible feature
combinations that you can have in a system, but it can also help you to figure out which
193

features make sense in combination. Distinguish which features you want to interact
synergetically from which unwanted feature interaction you want to prevent.
Given the feature decomposition and its constraints, you can check if you are going to
build a system that actually supports an invalid configuration of features.
194

Constrained Feature Model Example
Screen
Media
Network
Input
Storage
Video
Audio
Keyboard
Wheel
Touch
Color
B/W
Color
SD-slot
Memory
1 GB
2GB
4GB
8GB
Media Player
SIM
Radio
WiFi
BT
3G
4G
Headphones 
Jack
There is not necessarily a single correct answer: whether something is optional or not
depends on your customer. Depends on who is making the decision about this require-
ment. Your task is to explicitly and clearly record the decision, and now you have the
language to do so.
Does it make sense to have a media player for which the storage is optional? Maybe the
external card slot can be, but you need to have at least a minimum of internal storage.
What if you are designing a streaming client device? Then you just need a temporary
buffer for the bits you are downloading from the network and then throw them away
after they get played.
If you make a media player, at least the audio should be required.
So either storage is required and then you can work disconnected or you just do it
streaming device with network and then hope like in this case maybe you skip because
you drop packets and the quality is not so good. Speaking of radio connectivity: should
it allow an arbitrary combination of protocols? The advantage would be to offer a future
proof device which can also provide backwards compatibility.
It also looks like that for this generation of students, the headphone jack has become
an optional feature.
We use a tree so that we can be efficient in our decision making: once we decide we do
not want to include an optional sub-feature, we can skip all of its corresponding sub-tree.
When you refine your feature model, you can start from the top or from the bottom.
You can mark individual features as required or optional, or you can reason about this in
relationship to all other options under the same feature.
When you consider a group of features, like we have for the radio, then the typical
thing to do is to check if they are exclusive alternatives or if they can be found in any
combination.
195

Feature Conﬁguration
Screen
Media
Network
Input
Storage
Video
Audio
Keyboard
Wheel
Touch
Color
B/W
Color
SD-slot
Memory
1 GB
2GB
4GB
8GB
Media Player
SIM
Radio
WiFi
BT
3G
4G
Headphones 
Jack
Given this feature model which describes a family of products: all possible feature
combinations found in a set of products, we can instantiate it for a specific product.
One example: we pick Bluetooth connectivity, no headphone jack, and only audio play-
back, no card slots, only 4GB internal storage and also no display. These decisions de-
scribe a specific product, you should name it. You have now a target for your further
development: this is the system that we’re going to build. It has exactly this feature,
nothing else, nothing more, nothing less.
You can also use this colored feature model to compare it with other products. We
are here and the competition is there. They have a screen, we don’t. How much more
expensive would it be if we add support for the feature that is missing?
This is another way to start building feature models, start with concrete products and
then merge their concrete feature model into a more general one covering all products
in a given market niche. Maybe your device has some USB, wired connectivity to get the
media files into the device storage. So we discovered a missing option for our feature
model, which we can always add to make sure we not only support the media playback
use case, but also allow users to load the music onto their device before they can start to
enjoy it.
As you instantiate the feature model for a specific product, sometimes you don’t need
to always make the final decision yourself: you can delay the choice until later. For ex-
ample, users can configure their devices before they are built to order. They can pick the
size of the storage when they buy the concrete product, or even dynamically activate the
necessary video codecs just before starting to play their movie.
196

System Context View
Scope of the
Design Model
System Interface
Actor
Existing
System
Existing
System
• Distinguish what needs to be built from what already exists and
deѹne the dependencies and the integration points
The first thing you should do when you design a system is to draw a box and say this is
the boundary of my system. Inside the box I am in control. I will need to build something
in there. The rest of the world is outside my box, usually outside of my control. Still, I
have to build something that can interact with it.
In reality there are no separate systems: there is only one continuous world. Drawing
the boundary can help get the discussion started about where your system starts and the
rest of the world ends.
The minimal entity that is outside of the system but needs to interact with is the user.
They will exchange information, send commands, observe their effect and get some re-
sults. This is the minimal context that you can have: one user and the system itself.
The software system that you build will not be an isolated system. It will not exist by
itself. There will be other software that already exists that you have to integrate with.
This is why the context gets more complicated.
We have two existing systems, and if you look carefully, their connection with your
system is not exactly the same. We can establish a different type of relationship, or de-
pendency to be more precise. Our system depends on this existing external system. If this
system is not present, our system will not work because we depend on it. Or we can rep-
resent the opposite direction: this external system depends on our system. If our system
is not working correctly, if our system is not available, then it will be misunderstanding
by the other one, which in turn may also fail.
197

System Context View
• User roles, personas - who do you expect will use the system?
Are the users all the same? How many users can share the
system at the same time?
• Dependencies - which external systems need to be integrated
with the system? are there some open API that let other
(unknown or known) systems interact with the system?
Successful software has millions of users, so you do not want to list all of them in your
model, but somehow abstract the types of targeted user (or persona): These are the skills
that I imagine for this user, the needs that the user has, their requirements to be satisfied.
The users in the context are directly connected to the use case scenarios that you already
have elicited in the domain model.
Indeed, the user outside your system represents the tip of the iceberg of your domain
model, the part which comes into close contact with your system to perform certain sce-
narios.
Can your system be used only by one user at a time? Or do you expect to have multiple
users concurrently working with your system? This is a completely different scenario
with different scalability implications.
How do we visualize this dependency? We use this notation, where the simplest rep-
resentation can be two boxes whose sides touch. We can also separate them and draw a
line connecting them.
How to you show which is the direction of the dependency? For example, we want to
show one system providing something that the other system is going to use? We use the
lollipop notation to offer access to some interface to some functionality so that we can
require it from the other side. What kind of mechanism are you going to use to interact
with the other systems? What if they are all implemented using the same programming
language? Should this influence the choice of the language to write your new system in?
If we choose to reuse something that already exists: by reusing it you establish a de-
pendency, of which you should be aware of. And if this system will change, maybe you
will break, because you depend on some feature which has been removed or changed.
Showing the direction of the dependency helps you to predict the likely impact of
changes. Which side will be affected? Which side should worry about introducing po-
tential breaking changes?
In some cases, you don’t care if they change. But in other cases, if they change, you
have to be ready to deal with their consequences.
198

To summarize: this is the context for our system. This is boundary separating what we
need to build from what already exists around it and what kinds of users we have. In the
simplest case, we just have one type of user. In general, we might have multiple types of
users (e.g., anonymous, authenticated, admins).
When we work on the design model, we focus on the system within the boundary. What
is around it? What is the context? When we describe the context, we answer these ques-
tions, the model will help us to define our expectations, our assumptions about the types
of users that interact with our system.
The dependencies make you aware of what other existing external systems are there
and what kind of interfaces or integration points do they have.
199

System Context View Example
Music Player 
System
Customers
listen 
with
Artists
provide 
content for
Payment 
System
charge customers
Let’s look at the context of the music player example.
We have customers who listen to the music, but we also have the artists that provide
the content. By itself, the player is useless unless there is a reliable supply of music.
What is the incentive for artists to create it unless consumers can reward them with their
attention and their wallets? Listeners buy music through the system. Now if you want
artists to make money with it, and you as a middleman to take a reasonable cut, you need
to rely on some kind of existing payment system.
Typically you would not usually come up with your own cyber currency for a music
player application. Unless you want to keep the system self contained. If so, you will
end up implementing many more use cases related to securing financial transactions,
tracking exchange rates, filing tax returns, and safeguarding against double spending
than originally planned.
In terms of use case scenario prioritization, this context diagram indicates that there
will be competing interests. The artist want to make money and they have to upload the
content and the customer will then download the content, listen to it and pay for it (not
necessarily in this order).
200

Containers View
• What are the main logical execution environments in which the
system can run?
• Containers can be deployed separately and indepedently
evolved
Once we delimit the context, then we can zoom in and add more detail as we switch to
the containers view.
Here we focus inside the box and we start to decompose the system from the runtime
environment point of view. Before we can talk about the actual software we need to
talk about the hardware on which it will run. Before we can think of what programming
language are we going to use, we need to decide: How many containers are we going to
need? To answer it, we need to compose the runtime environment out of different types
of containers in which we can pour our software and our data.
The term container has a technical connotation, but we define it as an architectural
concept, going beyond Docker. Containers are separate units of deployment that can be
independently evolved. How do you know if you have separate containers? You can start
or stop them independently. When one is running, the other may not necessarily have to
be.
201

Container View Example
Music Player System
Customers
listen 
with
Artists
provide 
content for
Payment 
System
charge customers
App
Songs 
Repository
Customer 
Database
download 
music
If you think about the player example, we have the container which will run our soft-
ware application to play the songs, the one which will store them and the cloud where
we’re going to manage all the payments and also keep the huge collection of the music
available for sale that hasn’t been bought yet. We cannot just let the user take all the
music with them (even if we will soon carry in our pockets multi-terabyte USB sticks), so
we need to split the music storage container so that users just take what they pay for with
them.
From the outside, this looks like exactly the same diagram as before, we just made
some room inside our system for the containers which stay well inside the boundary of
the system. They are: the mobile device where we run the application itself, the customer
database tracking which digital artifacts users have paid for. This information we cannot
store in the app because we don’t trust the user that has the device in their hands. They
might tamper with it, so we simply store it somewhere else. The user can read that but
they cannot modify it, unless the corresponding payment transactions commit.
This also holds for the music collection, a container representing storage space suf-
ficiently large to store all the music ever recorded by every artist in the world. This is
where listeners will download from and artist upload their own music into.
These are the three containers which make up the environment in which we run our
system. They are not the same type of container: two have to do with storage: databases
or big file server. one runs the application, the software that we need to design.
202

Example Containers
• Server-side Web application
• Client-side Web application
• Client-side desktop application
• Mobile app
• Server-side console application
• Shell script
• Microservice
• Data store (Database, ѹle system, Key-value, Blob, content
delivery network)
When modeling the container view, you don’t have to just make a decision in terms of
how many containers do I need: 1, 2, 3, or a lot.
You need to also choose the containers which will fit on the type of devices that you will
use: a virtual machine in the cloud? a local physical device? You also should distinguish
the role and responsibility of the container: storage or computation? Will it run software
to process data flowing through it or store information at rest?
The first decision is about decomposing in multiple containers, then you need to clas-
sify them. Here are more examples that you can choose from. If you design a Web ap-
plication, you need to pick among two different types of containers: the browser and the
server.
You can also pick a classical desktop application, which needs to be locally installed on
some operating system. A native application, which could also run on a mobile device. It
doesn’t need to have a graphical user interface, but can also be just a console command
line application, which may run either on the server or also on the client.
Another container is the shell, which can run scripts. As opposed to writing code for a
standalone program, you write a script to glue together many programs.
Another type of container is a microservice, we will define it later, but this can defi-
nitely give you the boundary for a runtime environment which has an autonomous life-
cycle and can independently evolve.
Other containers do not run custom software applications, they run specialized data
management software. The container doesn’t process any information, just stores the
information using a database (SQL or noSQL), using the file system, a key-value stor-
age, a blob, a bucket in the Cloud, or a content delivery network, abstracting a complex
hierarchy of caches and mirrors spread all over the world.
You decision depends on whether the information is expected to change; what kind of
access patterns you expect; how many concurrent clients are going to read or write it;
does it have a regular structure; is it textual or binary data. It is a whole tree of decisions
that you have to make to pick the correct data container.
203

Components View
• What is the structural decomposition of the software with
related functionality encapsulated behind a well-deѹned
interface?
• What are the dependencies between components?
• Are there shared components that will be deployed in multiple
containers?
• What is the technology used to build the components?
(programming languages and framework decisions)
The next level of detail is about the components deployed within each software con-
tainer. The software architecture has a structural decomposition, which uses more than
one piece as every component encapsulates different functionality, different state, and
makes it accessible through a well defined interface.
Components are also related to each other. They need each other. They establish com-
plex relationships. One component will call the other one. Another pair will share and
exchange data.
Do you put multiple components in the same container? or do you have a super simple
solution where within each container you deploy exactly one component?
For performance reasons it may make sense to place multiple components in the same
container. Crossing container boundaries may have a cost, data may need to be copied,
the latency goes up. Some tightly connected components may not tolerate being de-
ployed far away across different containers.
Another aspect concerns failure scenarios: when a container crashes, all the compo-
nents inside are lost. If you put multiple components in the same container, if the con-
tainer crashes, you just lost all of them. Which can be a good thing, as opposed to partial
failures of some container where the rest of the system which survived the crash has to
figure out how to deal with the disappeared container.
Once you have a structural decomposition of logical components, you can start to de-
cide which language are we going to use to program them. Should all of the components
be written in the same language? or each component in a different language? or polyglot
components, with multiple languages within the same component?
The language decision depends on if you have access to developers that can write code
in different languages, or all developers that you have only speak the same language, so
there is no alternative to consider.
Once you pick the language for a component, then you can pick the corresponding
framework, libraries, compilers, and development tools.
Or maybe you don’t have to write the component at all. Instead of coding it from
scratch, you can recycle existing code or just buy an existing reusable component.
204

Components View Example
App
Customers
listen 
with
Songs 
Repository
Customer 
Database
download music
User 
Interface
Music 
Player
User 
Account
Songs 
Cache
So if we go back to the media player architecture, we focus on the application container,
since for the customer database and the repository and storage containers it’s enough to
come up with a schema and a reasonable file system folder structure. That’s relatively
simple compared to the type of structure that we find in a software container. The one
that runs the application.
In this case, for example, we can think of decomposing the architecture into four soft-
ware components. One of them is the user interface, making accessible the system to
the user. Then we have the core of the player that knows how to play the music, how to
transform bits into sound. We have the user accounts which takes care of making sure
that only people that pay for the music can hear it. And then we have a component that is
actually about storage: We download the music from the song repository, and that music
is going to be copied into the local cache. This is a local replica of the remote music and
with this component we achieve the scenario of being able to make the system work when
there is no connectivity. We are able to give a good quality playback experience becauses
thanks to this buffer we are not so sensitive to the network connectivity issues, latency
jitter, dropped packets.
Overall, inside the application container we will deploy and run four components. The
components are also connected to each other. For example: you can see a thin edge
between the user interface and the account, between the user interface in the player,
between the player in the cache. But not all components are connected, between the
cache and the account there is no edge. So here we don’t have any connection. We can
say these two components are decoupled, they don’t need to talk to each other.
How do we know whether different components are talking to each other? It depends
on their dependencies, which in turn occur due to their dynamic behavior and their in-
teractions. To find them, we need to think of how we are going to implement the use case
scenarios, which components will be involved in each of the scenarios.
205

For example, if we want to listen to the music, you will need to click the play button on
the user interface to send a command to the player and probably also some information
about which song do you want to listen to, so that when the player can go and take the
music from the cache and play it. Before you can do that, you should remember to check
with the user account if the user has the rights to play that particular song. When the user
account component needs to know about whether the user has paid or not it will check
with the customer database in the other container. When you start to play the music the
cache may be empty. Then you have to download it from the repository container. So
connections also span across container boundaries.
That’s one example to show you why these four elements are connected in this way,
because in this use case, they need to all work together to achieve a common goal.
206

Simon Brown
C4
Context
Containers
Components
Classes
What I’ve shown you so far for the design model is called C4. It is a meta model for
representing the structure of a software architecture. It uses a hierarchical representa-
tion where you start from the outside (representing the context) and then you refine it
into the runtime environment with the containers, inside which you deploy components.
What’s the next level of granularity? What do you find inside components? Classes. This
is where we start to program the code, whose fine-grained structure – for some object-
oriented programming languages – can be represented using class diagrams.
207

Classes View
• What is the structure of the code inside each component?
• How is a component implemented?
• Map the architecture down to the code as a blueprint for
software developers
• Representation: UML Class Diagrams
We assume you are familiar with UML class diagrams and you have experience with
object-oriented programming.
What matters here is that also at this level of detail, you need to make decision about
the local structure of the code within each component. You have to help developers im-
plement those components: What classes are they going to use? What kind of methods?
What data structures need to be stored, processed and transformed?
This is where the model of the architecture gets so concrete and so detailed that in
some cases after you precisely model a class diagram, you can click a button to generate
the code. You can generate code from the model.
Viceversa, is if you have the code, you can always reconstruct the class diagram out
of it. What about the next level of abstraction? It’s more difficult to go from the class
diagram to the components. How to gradually cluster the classes into packages, modules
and larger and larger entities such as components?
In the following we will focus on the other higher level abstractions with the knowledge
that eventually the component is a piece of software that has a class structure, if you
make a decision to use an object oriented programming language to implement it. Class
structures may be irrelevant if you use a scripting language, or a functional language.
Still, no matter which language, there will be a fine-grained structure that you can use
to plan the internal implementation and reveal at the externally visible interface.
208

C5
Context
Containers
Components
Connectors
Classes
If you are interested to make a more complete set of decisions about the architecture
of your system, you cannot ignore this additional concept, which is on a higher level
than the classes, on the same level as components. It’s not enough to hierarchically
decompose the structure into components nested into containers, but you also need to
describe how such components are connected together. That’s what connectors which
link together components are for.
In the plain connector view, we have already been drawing some edges between the
components. These edges are called connectors and there are many principal design de-
cisions you should make about them.
Connectors determine: How are you going to interconnect components that depend on
each other? How are these components going to talk to each other? What’s the coupling
you are going to introduce between your components?
To represent these decisions we introduce a dedicated view: the connector view and
we extend C4 into C5: context, containers, components, classes, and connectors.
209

Connectors View
• How are component interfaces interconnected?
• What kind of connector(s) are chosen?
• What is the amount of coupling between components?
These decisions may depend on the deployment conѹguration
There are many different kinds of connectors, and the choice of the connector has a
strong impact on the performance, the reliability, the security of your system, and also
on how easy it is to change the various components in your system.
If components are disconnected, you represent this decision by not drawing any edge
between them: for some reason they never talk to each other; they are decoupled. They
are independent. Such information about the lack of connectors is important because if
you change one component, the other one is not affected. If you switch off one compo-
nent, the other one is not affected. If one component fails, the other one survives because
they’re disconnected. There is no coupling between them.
When you make a different decision and draw a line between the components, you
show these components are connected together. Chances are that if you change this
component, the other one will also need to change. If one component fails, maybe the
failure will cascade across the line and the other component will also fail. And if some-
body wants to steal information, they can intercept it right on the line. So you have to
protect the exposed line of communication between the components, especially if the
connector goes across two containers.
The connector view helps us to represent a model of how to compose components so
we can choose which type of connectivity are we going to introduce between components
both deployed in the same container or across different containers.
210

Connectors View Example
App
Songs 
Repository
Customer 
Database
User 
Interface
Music 
Player
User 
Account
Songs 
Cache
call
remote call
stream
ѹle transfer
message 
queue
Let me show you an example of different types of connectors and start to discuss their
implications. There will be a full lecture about them later. Here are some examples for
a decision that you make every time you choose how you are going to make those two
components interact and talk together.
When you select the song and you hit play, there will be the need to verify that for
this particular song you have bought the permission to hear it. To do so, the user inter-
face makes a call to the other component which will return the answer: yes or no. You
have to make a call because until you know the result of the call, you cannot continue. So
you’re blocking during the sequential interaction between the two components: The user
interface will not continue until the call is completed. This is one particular type of rela-
tionship: a synchronous dependency. If the user account doesn’t have the information
locally, then they have to make a call to a component that is on a different container:
a remote call across the network, which could be implemented with Java RMI: remote
method invocation. This is a particular type of connector. You have two objects, running
in separate virtual machines, which can invoke one another’s methods although they are
separated across the network.
Another example is the data stream connector. This is useful where you are playing
the music. Maybe you do not want to use a call to load the data for the music because
of memory size limits. If you call the cache and load the whole music data for the song.
This may work if the song is very short. What if you want to listen to the recording of
this lecture? Do you really want to load the entire recording before starting to play it?
Is it going to fit in the player memory? Another solution is to be able to just stream the
data: load the data progressively, incrementally. As the data comes through, it is fed to
the player and then discarded. This is a very different type of integration that you make
between two components. There is no request followed by a response. It’s a continuous
stream of data that carries the the music as you need it, and then you have to buffer it.
211

You have to do flow control: tell the sender to keep sending or stop sending because the
buffer is almost full, or start sending again because there is now space available in the
buffer and we are about to run out of bits to play. You have all these issues to deal with
which you would not have with a simple call.
When you are modeling the “buy and download a song” scenario, you can use a file
transfer connector. This is not a stream where you read the data incrementally but you
are transferring a whole MP3 file at a time.
To connect the player with the user interface, we also don’t use a call. Instead, we
use a message queue. Messages are sent to control the state of the player by sending
commands such as stop, pause, unpause or start playing. When the player is ready to
change its state, it can take them out of the queue and process them. Message queues
or buses are typically introduced when, for example, you have independent threads that
are working in parallel and use messages to communicate asynchronously. If you would
use calls, then you would call the stop method and this call would block and return only
after the player has stopped. If the player is busy playing however, will it have a chance
to process you call? If you want to stop playing in the middle, how are you going to
implement this interrupt? You need to be able to interact with the component even if
the component is busy, so you want to have some kind of asynchronous, message-based
interaction between the two components.
Other examples of connectors are: shared memory or share database where multiple
components can use it as a blackboard to collectively share some information. In the
connector lecture, we will also distinguish between local ones, which work well within
the same container, from remote connectors, that help you to distribute your software
across multiple containers
So this is C5, C4 plus connectors.
212

Philippe Kruchten
4+1
Logical
Development
Physical
Process
Use Case 
Scenarios
One of the first proposals to represent models from multiple viewpoints has been 4+1.
Here we also separate the logical view from the physical one, and thus can represent
a mapping between the two with the deployment view. We also have the process view
representing the dynamics of the system and the development view.
Logical is similar to the component view as it shares the same structural concern. Phys-
ical is also similar to the containers, although slightly more hardware oriented, because
when it was invented, virtualization or containers were not yet so widespread. Repre-
senting dynamic behavior using a static diagram is challenging, although different pro-
cess modeling notations have been proposed, including sequence diagrams, activity di-
agrams, or BPMN.
The development view is the most detailed, although it is not meant to only capture
the code structure like the class view. It is more general describing how do we plan to
develop each of the components that we specify in the logical view.
These views belong to the design model, however there is the “+1”, which is about the
domain model, in particular, use case scenarios, which should be supported by the solu-
tion modeled in the other four views. To help you managing the complexity of modeling
according to multiple viewpoints, for every use case you may specify the corresponding
logical, physical, development, or process views. This will result in a scenario-specific
projection of the architectural model.
213

Logical View
• Decompose the system structure into software components and
connectors
• Map functionality/requirements/use cases onto the
components
• Concern: Functionality
• Target Audience: Developers and Users
The logical view is similar to the components view of C5. They share the same goal: to
represent the structural decomposition of the architecture in components and determine
their dependency relationships. The difference is that we are not concerned yet about
where the components are going to be deployed, which is represented in a separate view.
Every component should have a purpose. You should know the reason for introducing a
component: reason can be as simple as making the component responsible for delivering
the functionality for some use case scenario. Or this component may help another com-
ponent to do so. So you need to introduce the component because another one depends
on it to deliver the functionality that you need.
When you are about to draw a new box, ask yourself this question: what is the purpose
of the component? to which use case, to which functionality does it correspond? And
then you should keep a mapping between the requirements and components, where the
domain drives your design decisions.
The main concern for the logical view is to make sure that you have a correct and com-
plete system when it comes to fulfilling the requirements.
The main audience for this type of view are developers who should understand the
structure of the system, but also the users they might want to make a choice of which
component they want to actually deploy. The more components they buy, the more ex-
pensive the price.
214

Logical View Notation
Disconnected
Components
Connected
Components
Abstract
Connector
No Coupling
Coupling
Component
1
Component
1
Component
0
Component
2
Component
Provides
Requires
Dependent
Dependency
Interdependent
Components
Independent
Components
How are we going to represent the logical view? We have a logical view with four com-
ponents. Usually components have a name and the name should mean something in
relationship to their purpose.
The components on the left are disconnected because there is no relationship between.
Pairs of components can also be connected together. The simplest type of connection
is just shown by drawing a line between them. So in this case the components on the on
the left are disconnected and then the components on the right are connected.
What’s the difference between having introduced a dependency in the logical view and
having a model in which there is no dependency? with this simple line that you draw,
you are actually representing something really important. You are saying that there is
coupling, a dependency, a relationship that binds together the two components.
Depending on the type of connector, you can expect at runtime one component is going
to call the other. Or when you build the system, you need to get the dependencies and
link them together with the component it needs them. Two components connected need
each other to properly function.
Components disconnected or independent are without any coupling: For example, if
you change component zero, since it is disconnected, the other components will not be
affected.
The others, because of the relationship between them, if one component changes, the
other will probably notice, and viceversa. Can we make this relation more precise? Yes,
if we establish some kind of asymmetric direction between the two components. One
component offers access to his services to another component that needs them, that is
dependent on them. As you can see the edge, the edge that we have on the right is drawn
using these shapes. So we say that a component provides an interface for the rest of the
world to reuse and to integrate with, and you have a component that is dependent on this
interface that requires it to work correctly.
This logical view diagram uses a simple graph, where components are the nodes and
215

the directed edges represent dependencies. Independent components do not have any
direct (or indirect) path linking them, while interdependent components do.
216

Example Logical View
Songs
Repository
Payment
Service
Music
Player
User
Interface
Customer
Database
So let’s go back to our example. We have the song repository where we store all the
songs. We have the player itself, responsible for transforming the bits into the music
that you listen to. There is a connection between them because of the use case scenario
”play the song”: the player has to fetch the MP3 data from the song repository.
We also have the user interface which will provide access to the user to control what
the player is doing. Also the UI needs to find the metadata about the songs so the user
can for example search or browse them and select which one they want to play. This is
why we have these two connections. So this will be sufficient for satisfying the use case
where the user can manage his own music and listen to it.
However, there is another type of user – the artists – who want to make money with
this system and therefore the song cannot start playing unless the user paid for them.
In this case we also introduce a customer database component which keeps track of the
access rights: which songs can be played for which user. In case the user wants to buy
a new song, it will transfer the payment credit card information to a payment service to
charge the user for downloading a certain song.
While looking at these boxes, I tell you a story of how these boxes interact to explain
why we need to include them.
This architecture, this structure is really simple: with five components we already have
an idea of the main pieces that we need to build to run the system and then we start
to have a discussion on: Which components are we going to build ourselves? Which
components are we going to buy, and reuse from somewhere else? Which components
are mostly related by storage? Again, the song repository just looks like the candidate to
just have a big file system where you store all the MP3 files. Another components will be
external: we need to interact with the payment service from a bank, so it’s important to
model a dedicated component for that.
217

Example Logical View
Customer Database
Music Player
User Interface
Payment Service
Songs Repository
Refresh
@startuml 
title Example Logical View 
 
interface " " as MPI 
interface " " as SRI 
interface " " as CDI 
interface " " as PSI 
 
[Customer Database] as CDB 
[Music Player] as MP 
[User Interface] as UI  
[Payment Service] as PS 
[Songs Repository] as SR 
 
MP - MPI 
CDI - CDB 
SRI -- SR 
PSI -- PS 
 
MPI )- UI 
UI --( SRI 
UI -( CDI 
MP --( SRI 
CDB --( PSI 
 
@enduml
Process View
•  Model the dynamic aspects of the architecture:
• Which are the active components?
• Are there concurrent threads of control?
• Are there multiple distributed processes in the system?
• What is the behavior of (parts of) the system?
• Describe how processes/threads communicate
(e.g., RPC, Messaging connectors)
• Concern: Functionality, Performance
• Target Audience: Developers
Philippe Kruchten
We can now switch perspective from the structural decomposition, which is something
static, to represent the dynamic behavior of the system.
218

When you design the process view you make decisions on, for example, which are
the active components in your architecture. By active components, I mean components
within which at least one thread of execution is started.
Whenever you write a piece of software, you need to have at least a main function
to start the execution from. That function will be located within the active component
which runs the main system execution thread. Sometimes you start more parallel threads
and these threads are located in different active components.
Go back to the previous diagram and determine which components are the active ones.
Where do you need to start a thread that executes something? For example, when you’re
playing the music there is a thread that starts fetching the bits from the repository and
feed them into the sound device driver and the music comes out. You want this to be an
independent read because you don’t want the user interaction with the system to inter-
fere with the quality of the playback.
Another example is when you have a service: independent component which will be
waiting for requests from other components. To do this, services usually run their own
threads that are activated when you get request and you want to process them. So this
is one decision to separate active and passive components and then the other important
part is to describe the behavior of the system in terms of how the various components
communicate with each other.
219

Example Process View
Use Cases: Browse, Pay and Play For Songs
Songs
Repository
Payment
Service
Customer
Database
Music
Player
User
Interface
Browse Songs
Buy Song
Charge VISA
Play Song
Get Music
We typically describe the behavior using a sequence diagram. You will have one se-
quence diagram for each use case. In this case we have a overarching use case about the
user who is going to browse, pay and play the music. So first the user interface will fetch
the list of songs from the repository. It will display the list to the user, who can select
the song and through the customer database he can buy the song. This will trigger the
request to charge the credit card. If the payment is successful and we can start playing
the music. So we tell the player: now it’s your turn. Please start playing. To do so, the
player gets the music from the repository.
This is only one possible solution to show the simplest, the minimum amount of in-
teractions that we need to introduce between the various components.
Every time you have one of these direct communication edges between components,
this should correspond to some kind of a connector into the logical view. This is where
we start to see that the various views are about the same model of the architecture and
indeed they should be kept consistent.
If the user interface sends a command to the music player, in the logical view the
user interface should be connected with the music player so that this command can go
through.
I assume that you’ve seen sequence diagrams at some point in your career, so I don’t
have to explain the notation in detail. The main idea is that we only include the com-
ponents that are involved in the particular use case. Vertically we have the time, which
naturally gives us a sequence which orders the various interactions and this is how we can
capture an example of the behavior of the system in terms of the interactions between
its logical components.
220

Example Process View
User Interface
User Interface
Music Player
Music Player
Songs Repository
Songs Repository
Customer Database
Customer Database
Payment Service
Payment Service
Browse Songs
Buy Song
Charge Customer
Play Song
Get Music
Refresh
@startuml 
title Example Process View 
 
participant "User Interface" as UI 
participant "Music Player" as MP 
participant "Songs Repository" as SR 
participant "Customer Database" as CDB 
participant "Payment Service" as PS 
 
UI -> SR: Browse Songs 
UI -> CDB: Buy Song 
CDB -> PS: Charge Customer 
UI -> MP: Play Song 
MP -> SR: Get Music 
 
@enduml
Example Process View
User Interface
User Interface
Music Player
Music Player
Songs Repository
Songs Repository
Customer Database
Customer Database
Payment Service
Payment Service
Browse Songs
List of Songs
Buy Song
Charge Customer
alt
[payment success]
ok
Play Song
Get Music
[payment fail]
refused
show payment failed error
Refresh
@startuml 
title Example Process View 
 
participant "User Interface" as UI 
participant "Music Player" as MP 
participant "Songs Repository" as SR 
participant "Customer Database" as CDB 
participant "Payment Service" as PS 
 
UI -> SR: Browse Songs 
SR -> UI: List of Songs 
UI -> CDB: Buy Song 
CDB -> PS: Charge Customer 
 
alt payment success 
 
PS -> CDB: ok 
UI -> MP: Play Song 
MP -> SR: Get Music 
 
else payment fail 
 
PS -> CDB: refused 
CDB -> UI: show payment failed error 
 
end 
 
@enduml
221

Development View
• Static organization of the software code artifacts (packages,
modules, binaries…)
• A mapping between the logical view and the code is also
required
• Concern: Reuse, Portability, Build
• Target Audience: Developers
The development view is where you capture all of the decisions about your plan to
organize the development of the software. This view mainly concerns the people that
have to develop and build the system.
For example you can decide: which kind of software code artifacts are you going to
have? which kind of programming languages will you write the component in? in which
repository you store the code? will you create a dedicated IDE project for each compo-
nent?
These are all decisions that impact how you’re going to build the software and where
you can run it. The more languages, the more repositories will make it more complex the
build process. Different languages will make it more or less easy to deploy the result of
the build in different execution environments.
Before planning how to develop and build each component, you should decide whether
you want to build it in the first place. Maybe you can reuse a component that already
exists. Some components you don’t have to build at all, you just have to buy them.
222

Example Development View
Songs
Repository
Payment
Service
Customer
Database
Music
Player
User
Interface
Language: Java
Repository: SVN
MySQL
MySQL +
FileSystem
Buy a licence
Get an SLA with
a provider
There is no standard notation to capture the decisions that you make in the develop-
ment view. In this example we just use sticky notes to associate the decisions with the
various components. You can also just write these notes down in a spreadsheet.
These are always the same five components but now we have to make decisions on how
are we going to develop them. For example, here we decide that the user interface will
be written in Java and we’re going to use these type of repository to store the code.
Regarding the storage components, we will use this type of database for the customer
database and for the song repository, we use a mix between a database for the metadata
and file system for the MP3 files. After this decision, we need to design a schema, de-
fine the component data model to know how the data is going to be structure. We don’t
have to write any programming language code, we could generate SQL from an entity
relationship diagram.
The player is just too much effort to write from scratch. Decoding MP3 files, streaming
the audio bitstream to the sound drivers, there are beautiful libraries to help you with
that. So we decide to reuse the component. We need to buy a license of the software
component that we can integrate with our system.
The impact of this decision will be seen in your project budget, also because it will
make it faster to ship the whole system once we figure out how to reuse the interface of
the player component we just bought.
We’re also not going to invent our virtual digital coins. We need to talk with a bank and
get some kind of service level agreements to access their payment services. This way we
can guarantee that whenever the customer wants to buy the music, the payment service
will be there. It will be available with 99.99% availability. At this point we can choose
which credit card brands we are going to support and how much each transaction fee is
going to cost.
223

Physical View
• Deѹne the hardware environment (hosts, networks, storage,
etc.) where the software will be deployed
• Different hardware conѹgurations may be used for providing
different qualities
• Deployment View: Mapping between logical and physical
entities
• Concern: Performance, Scalability, Availability, Reliability
• Target Audience: Operations
The physical view defines the kind of hardware environment that you will use to deploy
your system.
This is where you choose the computers, lay out the networks connecting them, select
storage devices, configure mobile phones, shop around for the cheapest cloud offering.
It is possible that you actually have multiple physical views to consider. You want your
software to be flexible when it comes to the target that you have in mind when you want
to run it. You typically never write software for just one particular piece of hardware:
hardware changes very often and you want to avoid that your software will become obso-
lete too quickl. Your goal as an architect is to try to keep the software independent from
the hardware used to run it.
But you are still making assumptions about the minimal hardware requirements, and
the physical view is where you collect them. The decisions that you make in the physical
view are very important regarding how much redundancy do you introduce in the system?
How reliable it is going to be, how scalable. And the operators in the need to install and
run your system will need to know which kind of configuration your architecture expects
to have, so they have to provision the hardware so that later they can install the software
and run it with a certain level of performance and capacity to absorb the workload you
expect.
224

Example Deployment View
Cloud
Payment
Service
Customer
Database
Music
Player
User
Interface
Songs
Repository
Secure
WS-*
HTTPS
Songs
Repository
HTTP/FTP
The deployment view is the connection between the logical view, listing the compo-
nents that you have and where do you want to place them in the given physical view.
The classical example for a physical view is very simple: a mobile phone and the cloud.
The deployment view is more interesting: there you have to decide where to deploy and
run each of your logical components.
Do you make this decision statically – so that you know in advance where each compo-
nent is expected to run? Or can some components float around and – depending on how
powerful your phone is, or how much battery charge is left – dynamically switch their
deployment target between the mobile client and the cloud server?
If you have multiple target runtime environments (multiple physical views) then you
need to have multiple deployment views as well.
Let’s look at the example. First we use a visual and non-standard representation: the
payment services runs in the cloud, somewhere far away ourside our control. The cus-
tomer database and the song repositories are storage components, so they get their own
databases and storage devices. This is similar to the container view: we have a container
for each component, so these are database containers.
Then we have the mobile phone which is going to run the player and the user inter-
face. We need to put these two together as close as possible to the user to guarantee a
fast reaction time. Also when we play the music we don’t want to be dependent on the
network so the player will run locally.
However, if the data that we need to play is far away, we have a problem, because there
might be some lag or there might be some gaps in the in the playback. So to improve that
aspect we make a decision to keep a local copy of the song repository. So in this case we
need to duplicate a component which is a single one from a logical point of view, but it
gets instantiated twice when we deploy it. We decide to make a copy of two instances
of this component. One will store a cache of the music and the other one will store the
whole dataset. So we introduce partial replication of the data.
225

In this view we also represent how, from a networking point of view, what kind of pro-
tocols are we going to use to make the various components talk to each other when they
are distributed across the network. You should recognize some of those protocols.
226

Example Deployment View
Mobile Phone
Database
File Server
Cloud
User Interface
Music Player
Songs Repository
(Cache)
Customer Database
Master
Songs Repository
Payment Service
SWIFT
FTP
HTTPS
Refresh
@startuml 
title Example Deployment View 
 
node "Mobile Phone" { 
 [User Interface] as UI 
 [Music Player] as MP 
 [Songs Repository\n(Cache)] as SRC 
} 
database "Database" { 
 [Customer Database] as CDB 
} 
folder "File Server" { 
 [Master\nSongs Repository] as MSR 
} 
cloud "Cloud" { 
 [Payment Service] as PS 
} 
 
CDB -- PS: SWIFT 
UI -- MP 
SRC - MSR: FTP 
UI - SRC 
MP - SRC 
UI - CDB: HTTPS 
 
@enduml 
227

Content is more
important than
representation
No matter which notation and how many viewpoints you use, what is really important
are the decisions that you make, whether they are correct and it doesn’t matter as much
the way that you represent them.
You should learn how to think about your software architecture, how to design it, how
to reason about the implications of your decisions. Pick the representation which doesn’t
get in your way while doing so. Pick the most suitable representation so you can clearly
communicate your decisions with it.
Make the right decisions, but do not worry if the notation that you use to capture them
is not standard. If the notation is understandable by the people that you want to com-
municate your decisions with, then this is enough.
228

Model Quality
• Ambiguity
• A model is ambiguous if it leads to more than one
interpretation
• Incomplete models can be ambiguous: different people will
ѹll in the gaps in different ways.
• Accuracy
• A model is accurate if it is correct, conforms to fact, or
deviates from correctness within acceptable limits.
• Precision
• A model is precise if it is sharply exact or clearly delimited.
Assume you have just completed a model: your model can have different qualities. We
can evaluate it and because it is a model there will always be some degree of ambiguity.
The model is ambiguous because it’s abstract: we cannot avoid that.
If you give an abstract ambiguous model to different people, they will interpret it in
different ways. Depending on their assumptions, you want to minimize the ambiguity,
but depending on who you show the model to, this ambiguity will always be there.
We can also check whether it is accurate. Accurate just means that the model is correct:
you have made the right decisions in your design.
And we can also look at the model and discuss if the model is precise. If it is clearly
described. Or is the model is a bit fuzzy, sketchy. It is not exactly clear what information
is contained in there.
Ambiguity is unavoidable due to the abstraction; precision is due to your proper use of
a language. You can have a model that is represented, for example with natural language.
And then you’re using English as precisely as possible. But if you use a formal notation
with a clear semantic, then your model would be more precise. Whether the model is
accurate depends on whether it correctly conforms to reality, whether it leads you to a
correct solution of the problem you are trying to solve. Incorrect models do not solve the
problem, or worst: attempt to solve the wrong problem.
229

Accuracy vs. Precision
Inaccurate,
Imprecise
Accurate,
Imprecise
Inaccurate,
Precise
Accurate, Precise
To see if you understand the difference, look at this example. Are you consistently hit-
ting the center? Are you consistently hitting the wrong target? Or are the hits scattered
randomly?
If you are consistently hitting the wrong spot, congratulations: you are super precise
in your aim, but you’re just aiming in the wrong place because your goal is to hit the
center of the target. And it doesn’t matter how many time you throw, how much effort
you invest in gold plating, if all your throws always hit the same wrong spot you will be
precisely inaccurate.
If we average all your throws, you are hitting the center. You are correct, but imprecise
because there is a lot of variation. You are on the right track, now you can invest in
polishing.
Which one should we prioritize: precision or accuracy?
230

Model Quality - Advice
• Make sure your architecture is accurate
(a wrong, inconsistent or conѺicting architectural decision is a
recipe for disaster)
• Sometimes you can even make it complete
(but it will be more expensive, so only do it for critical aspects
of the system)
• Precision helps, but avoid over-specifying and over-designing
the architecture, especially if the architecture is inaccurate,
adding details will not ѹx it. (developers may be trusted to add
missing details)
You have to make sure that your architecture is accurate: always make the right correct
decisions. If you make the wrong decision, it doesn’t matter how much time you spend
documenting them, carefully drawing out your diagrams. They are wrong: you made a
mistake and you cannot hide it by adding more details.
Sometimes your model can be complete, so you try to represent the whole system and
add as many details as possible. This will help you to minimize the ambiguity. But it’s also
very expensive to do because the more time you spend modeling the less time you spend
coding and at a certain point you want to have a running system. If you keep modeling
all the time, you don’t get there.
Precision is also important, but is less than accuracy. As long as other people under-
stand what you’re trying to communicate to them, as long as they get the correct idea
and they follow it consistently in their code realizing your architectural decisions, then
adding details can be sometimes unnecessary. If you know the developers that are on
your team and you know their skills, then you can trust them to follow through with the
right work even if you do not give them a super precise plan.
231

Wrong
Accuracy
Correct
Low
Precision
High
+
+
sketch
skeleton
If we look both at the accuracy and precision, we have four cases. The worst possible
one is when you make a wrong decision and you have a high precision, you invest a lot in
making a beautiful UML diagram, but its content is wrong.
We can also make a wrong decision and just to have a low fidelity representation, but
it’s still wrong, but at least it’s not trying to look good.
What we want to have is either a sketch: a model that is accurate, but it has low pre-
cision. Or we can refine the sketch with more and more details to make it more precise
until we make it a skeleton. Why skeleton? Because with model-driven architecture it
can be used to generate code so that the skeleton can start to walk.
You goal is to correct mistakes before starting to increase the precision. Avoid investing
in refining a wrong sketch into an incorrect skeleton. If your model is wrong, stop and
fix it before further detailing it.
232

Model-Driven Architecture
• MDA promotes modeling as the main software design and
development activity
• The design is organized around a set of models and model
transformations within and between different abstraction layers
Code
Model
Model
The code is the model!
Code Visualization
Code
Model
Model
Code and model co-exist
Roundtrip Engineering
Code
Model
Model
The model is the code!
Code Generation
References
• Michael Jackson, Problem Frames: Analyzing and structuring software development problems,
Addison-Wesley, 2001
• Richard N. Taylor, Nenad Medvidovic, Eric M. Dashofy, Software Architecture: Foundations,
Theory and Practice, John-Wiley, January 2009
• Philippe Kruchten, Architectural Blueprints—The “4+1” View Model of Software Architecture,
IEEE Software 12 (6). November 1995, pp. 42-50
• Scott W. Ambler, 
• I. Asimov, The Relativity of Wrong, The Skeptical Inquirer, Fall 1989, Vol. 14, No. 1, Pp. 35-44
• J. Cleland-Huang, R. S. Hanmer, S. Supakkul and M. Mirakhorli, 
 IEEE Software, vol. 30, no. 2, pp. 24-29, March-April 2013.
• Simon Brown, 
, Leanpub
• Nick Rozanski, Eoin Woods, Software Systems Architecture: Working with Stakeholders using
Viewpoints and Perspectives, Addison-Wesley, 2012
• plantUML: 
, 
, 
Agile Modeling
The Twin Peaks of
Requirements and Architecture
The Art of Visualizing Software Architecture
DSL Speciﬁcation Online Server Download
 
 
 
 
 
 
 
 
 
 
 
 
233

Software Architecture
Modularity and
Components 5
Contents
• Deѹning Software Components
• Application-speciѹc vs. Infrastructure Components
• Nested Components
• Component Lifecycle Decisions
• Component Types vs. Instances
• Distributed Components
• Component Roles: User Interface, Active/Passive,
Stateful/Stateless
• Granularity: Components vs. Objects
• Component Technology Frameworks
• Decisions: Buy vs. Build
234

Software components are used to model the logical and structural decomposition of
a software architecture. In this lecture we are going to go more in depth and discuss
what are software components, what is their lifecycle, main properties and roles. We will
also compare components as an architectural abstraction against objects, a code-level
abstraction. We will close by looking at concrete technology examples of component
frameworks and discuss the buy vs. build decision, which you should consider for every
component that you introduce in your architecture.
We use the Lego building block metaphor as a mechanism to represent the fact that
we can assemble larger constructions, larger systems out of building blocks: software
components are meant to be such a building block.
They are meant to be both composable and reusable, and there is a whole industry that
is selling components that others can include in their application and resell them again
to the end users.
However, the beautiful visual Lego block metaphor breaks down because software com-
ponents from a technology perspective are very heterogenous and very different from
each other, so assembling them is not usually as easy as just clicking together 2 pieces of
plastic. There is actually some cost and some effort involved into composing the whole
architecture by assembling or mashing up components together.
235

Hardware Component
• Reusable Unit of Composition 
Made of smaller
components
Can be composed
into larger systems
Let’s switch to a different example, which is about a different type of component: a
hardware component. The original vision for the software industry was to become like
the hardware industry, which is fundamentally a component-based industry.
When software was still in the early days, there was already the technology to build
and assemble together hardware systems out of components.
Hardware components became more and more powerful, integrated and the software
could just watch how that industry developed, grew and matured. They would wish that
it would actually be possible to do the same for the software.
If you look at this picture: can you suggest what the component is this one?
A network card. How do you know that? How can you tell the purpose of the compo-
nent? By looking at its the interface. Which kind of connectors does it support? Which
kind of ports?
You can see the ancient coax antenna cable plug and the RJ45 standard cable socket.
But there is also another interface: What kind of interfaces is that? The PCI interface for
installing this network card within a PC.
You can also see that this card has a few electronic chips on it. Where does the power
come from for the network card to actually work? Does it come from the antenna cable
or does it come from the PCI interface? Or is there an internal battery?
So power is something the component requires and it needs to be provided from an ex-
ternal source. So you should not ignore the internal facing interface, this is the interface
that the component requires to get the power from but also so that we can integrate it
within the larger system which is the PC or the computer in which this card is meant to
be embedded.
236

At the same time, this component is made of smaller components and this is a compo-
nent that is meant to be composed into a larger system. It is not really useful by itself. It
doesn’t work by itself. This is just a part that we insert into a computer so the computer
can connect to the Internet, can connect to other computers. We’ll see that a component
which enables communication (data exchange) and coordination (control flow) can be
considered a connector.
A composable element, which can be reused, which can be assembled within larger
computers as long as they are compatible, as long as they have such PCI interface, as
long as they have such carefully placed screws so that the component can physically fit
within the larger system. So that’s an example of a hardware component.
237

Software Component
• Locus of computation and state in a system
Processing
State
Provided 
Interface 
Environment
Required 
Interface
I cannot show you a powerful image as with the hardware or the Lego blocks for soft-
ware. I just show you a box. The box isolates the inside of the component and separates
it from the outside. The box identifies the boundary of the component distinguishing it
from the rest of the system. The box encapsulates what’s inside from the external envi-
ronment, from the other components, from the rest of the architecture.
We know that most components are used for computing or data processing. Inside the
component we find the function, implementing the input/output transformation. Inside
we find algorithms. That’s where we’re going to place how some particular data gets
processed.
Some components are also stateful: they remember. Architectures have to store in-
formation somewhere and there will be one or more component responsible for storing
such information, and this will be a stateful component. We will discuss the different
roles played by stateless and stateful components.
To use components, we need to satisfy their dependencies. To reuse components, we
need to understand their interfaces. The boundary of components is critical to determine
how to access the processing functionality and information stored within components.
Design decision: what kind of interface your component provides to the rest of the world?
Will it be compatible with some standard connector? We also need to know not only what
the component provides, but also we need to know what the component requires: what
are the dependencies of this component?
A component without any dependencies is a self-contained component: a component
that offers functionality to the others, but it doesn’t rely on any other component to do
so.
We call components without any provided interface the useless component, because we
cannot access what the component does. All components must have provided interface
otherwise we cannot use them. Some components also have dependencies that we need
to satisfy in order for the component to function so that it can deliver the functionality
as promised through its provided interface.
238

As we identify and introduce components, we also need to consider: components do
not run in isolation. They will need to be deployed in a certain container, in a certain
execution environment, which will also satisfy some of its dependencies. We will talk
more about interfaces later on. In this lecture we just focus on the notion of component.
239

Component Examples
MP3 
Codec
Payment 
Service
Play 
List
Customer 
Data
Song 
Classiѹer
• Application-speciﬁc
• Directly involved in implementing the functionality
• Require speciѹc domain knowledge to build correctly
• May be difѹcult to reuse across other application domains
• Reusable, but may provide much more than what is necessary
in one speciѹc application
Here are some example of software components, which belong within the same domain
of our media player application. All are application or domain specific components. They
are necessary to implement the functionality of a particular application. To program
them correctly, you need to have some knowledge about that domain.
You have to understand what the user wants, what the requirements are, how the MP3
standard format works. If you take an MP3 library to decode the music, it might be diffi-
cult to apply this in a different domain, for example, to predict the next weather forecast.
There will be a mismatch between the domain knowledge embedded within the compo-
nent and what the component can do and what you need to do in that other particular
application domain.
Domain-specific components are reusable as long as you stay within the same domain.
There can be many different applications in which you can benefit from the ability to
encode or decode MP3 sound or music files.
240

Component Examples
Media 
Player
Math 
Library
GUI 
Toolkit
Web 
Server
Database
• Infrastructure
• Independent of a speciѹc application domain, address the
needs of multiple classes of applications
• Highly reusable
• May require some customization, conѹguration or additional
programming to satisfy the funcional requirements of the
application
• Deliver support for extra-functional requirements (e.g.,
interoperability, scalability, availability, durability)
There are other types of components that we call ”Infrastructure components“, which
are domain independent. They help you to solve problems which occur across multiple
application domains. They are highly reusable because they make very few assumptions
of what the application does. Instead they can help you to achieve some of the extra
functional requirements for your application.
For example, if you need to make some data persistent and durable, you know you
should store it in a database. Usually you do not have to implement the database by
yourself. You take it off the shelf, and you add it to your architecture. Before you can
actually start using it, you have to configure it, customize it. You have to specify the
schema that you will use to store the information, write the queries. By itself the database
is empty; doesn’t know how to store anything useful. You have to tailor it to your domain
so that it can store your domain specific data model.
This holds for many other types of infrastructure components. For example, when you
build a graphical user interface, there is a whole ecosystem of frameworks that help you
to do that. They are very powerful, but they’re also completely useless, unless you build
a concrete user interface that you need for your particular application. Nothing prevents
you from writing the user interface from scratch, but this problem had to be solved so
many times that over time the reusable part of the solution has solidified and separated
out of the application specific software.
Thus it is usually faster to specialize a general, ready-made solution than to solve a
specific problem from scratch.
Another typical example is the math library: you can implement the square root func-
tion by yourself, the standard deviation statistical function by yourself. Or you can just
call the library and trust the mathematicians to provide you with a correct implementa-
tion which you can just reuse. There’s no point in writing this code by yourself.
Infrastructure components are highly reusable components which you find everywhere,
241

you don’t even think that you are depending on them – maybe they come bundled with
the programming language – but you should be aware it’s also an architectural decision to
pick the right type of database, the suitable type of Web server, the correct user interface
toolkit.
242

Black Box
˾. Encapsulation
• Clearly separate the content from the rest of the system
˿. Abstraction
• Hide implementation details behind the interface contract
̀. Modularity
• Reusable unit of assembly/deployment
́. Composability
• Components are meant to be interconnected
• Components are made out of components
Here are some fundamental properties of a software component.
Encapsulation: we draw box which separates what’s inside the box from what is out-
side.
Abstraction: components have an interface, which hides their internal implementa-
tion. And we only need to understand the interface if you want to use the component
and we don’t need to care about its implementation details.
Modularity: components provide the building blocks making your architecture modu-
lar. You can still choose to design a monolithic architecture with only one component.
But then you will not be able, for example, to distribute the system and run it across
different machines. You will not be able to change only individual independent parts,
instead every time you touch something everything will be affected.
Composability: if we can use encapsulation, abstraction and we have multiple mod-
ules, we can build systems out of reusable parts that can be assembled, deployed and
evolved independently. Once we have multiple components, by themselves, they are not
enough to make the architecture work. We also have to assemble them back together
with the appropriate connectors so that the value of the result is greater than the sum of
the value of its parts.
In this lecture we focus on the modularity, the next will discuss how to design interfaces
so we will look at the abstraction and later on we will talk about connectors which help
us with the composability.
243

Recursive Components
• Components are meant to be
composed into larger ones
• Components (inside the black
box) are made of components
• Software Architecture is Fractal
(up to a certain magniѹcation
point)
• Design Decision: When to split a
component? When to stop the
recursion?
Components themselves may as well have other components inside, recursively.
As you sketch your first components, you may notice that some components are bigger
than others. They do not always have the same level of granularity. It can happen that
some of these large or heavyweight components will actually contain some of the other
components if you open them up and look inside.
When we decompose the system into multiple components, we start at the context
boundary for the whole architecture. Inside we have multiple components. But then you
can repeat exactly the same decomposition for every component. You can go inside and
discover that every component actually contains other subcomponents.
It’s important to realize you have to stop at a certain point. There is a risk that your
components become so small that probably you are already writing the code, you are just
doing visual programming.
That’s also why in the C5 model we had the context, the containers, the components,
and then immediately went into the classes. That was the only one level of refinement.
Sometimes for very large systems it makes sense to do this for a certain number of itera-
tions (with components, sub-components, sub-sub-components).
In general when you model a component you have made the decision to introduce this
component in your architecture. Another decision which you can always make is to split
it if the component becomes too large or recompose them if the components are too
small.
Let’s decompose it: when you do so, you can split it and make smaller components
inside. Or you can split it and have multiple components on the same level. They will
be just a little bit smaller. The smaller your components become, the more components
you need to deal with, and at some point you should stop as you run out of pixels on your
diagram screen.
244

Clustering Components
Music Player - Logical Component View
User Interface
Cover Image
Display
Player Controls
Business Logic
Song
Randomizer
Music Player
MP3 Decoder
Data
Song Repository
Playback History
Playlist
Customer Data
Don't Know
245

Design vs. Run-time
Component
• At design-time, a
ComponentType deѹnes 
the properties shared by 
a class of components
• Convention: CamelCase
component
• At run-time, we have one 
(or more) component
instances
instances of a given type
• Convention: lowercase
When we draw a box, we put the name inside, we introduce a component in your model.
But what exactly are we modeling?
If you want to be more precise, what we’re really describing in the logical view is the
type of the component that later will be instantiated at deployment time within some
physical execution environment, some container.
If you want to be precise, you can distinguish the type of the component from the
instance of the component type. This is the same convention as when modeling classes
and objects (instances of classes) in UML.
On a first level of approximation we talk about components, on a more refined level we
can distinguish component types and their instances. If you look at a logical view, you
can expect by default to see component types, while the deployment view will by default
show component instances.
Distributed Components
• Components can be deployed
on the same physical host
• Components can be
distributed over multiple
physical hosts
Concerning deployment, a very important consequence of modularity and components
246

is that they help you to reason about how to distribute your system across multiple exe-
cution hosts.
Here on the left we have a deployment view which uses only one physical host, we call
it a centralized deployment. All the components run on the same machine, or the same
container. There are multiple logical components, but only one container, so they are all
in the same place.
But if you have multiple logical components, every component, in principle, can be
running in a different container. It is possible, with a modular architecture, to target a
runtime environment with multiple containers or multiple physical execution hosts. We
can take the same logical architecture (the components are already there) and we can just
deploy it by distributing each component on different devices.
This is another benefit of components: if we do not have components, we have a mono-
lithic architecture, then we can only deploy everything to run in only one place, because
we don’t have the ability to split the system.
If we nevertheless still want to use a distributed environment, we can make multiple
replicas of the same logical component. It just takes a for each loop in your deployment
scripts to clone your big ball of mud across an entire Cloud data center.
247

Component Lifecycle Decisions
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Internally built components
What are the decisions that we need to make about components as we go along the
software development and operations lifecycle?
The classical development activities are listed in the blue part while the yellow part is
operation.
We have different cycles depending on whether the software components are going to
be built in house or not.
If we build it ourselves we need to write the code, test it, package into a release, deploy
it, then we can start to run the component. Or after we release it, somebody else is going
to install it and run it.
Afterwards we receive feedback and we plan for the new changes to apply, we imple-
ment them, we check the results of the tests to spot any regressions and go once more
through the cycle.
248

Component Lifecycle Decisions
Discover
Discover
Select
Select
Integrate
Integrate
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Externally sourced components
There is an alternative possibility that these components are externally sourced, they
come from somewhere else.
This means we are not going to write the code ourselves. We’re not going to build them,
but we’re going to have to integrate them: to make sure that they work together with the
rest of our system.
Maybe half of the components we build and the other half we buy. If we buy them, we
have to first discover where they are. It’s not obvious where to find a component. Maybe
it exists and we find it somewhere. Or we need to find multiple providers, alternative
vendors who are competing to sell it to us. We need to select the winner.
And after we make the selection, we download the component and we need to integrate
it and test that everything works.
It is not true that if you buy a component from the outside you do not have to do any
testing. On the contrary, you have to test if the components actually work as expected.
You cannot just trust somebody to give you a piece of software and blindly deploy it in
production and expect it will work flawlessly in front of your users. You still need to do
a few checks yourself first.
249

Component Lifecycle Decisions
Discover
Discover
Select
Select
Integrate
Integrate
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Does a suitable component exist? Where to ѹnd it?
While you don’t have to build it, you still need to discover the existence of the compo-
nent. Making the selection and bring it into your system should be faster and hopefully
cheaper than having to build it yourself.
So this is an important decision that you have to make every time you draw a compo-
nent into logical model of the architecture. You say there is component X. The next step
is to decide: Do we build it ourselves? Or are we going to go and buy it from somewhere?
If we buy it (or reuse an existing one for free), we need to find it. Does such component
exist at all? How we discover it?
If I ask you right now to go and look for a component: can you give me some exam-
ples where you would expect to find some piece of software that you could reuse in your
application? Would you know where to go to find software components?
NPM, Google and Apple App Stores, github for open source. You will search and find
lots of results, so you need to assess which would be the right component for you.
250

Component Lifecycle Decisions
Discover
Discover
Select
Select
Integrate
Integrate
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Which component should be chosen?
How to trust the quality of the component?
You can choose based on the price – in this case commodity open source free compo-
nents have the advantage.
In other cases, you don’t want to get a free open source component, you want to get
a commercial component, because then you get support and you get what you pay for,
right? This way you get better documentation, and the overall quality may be better.
But the moment that you take software from the outside and you bring it into your sys-
tem, you’re trusting that you can rely on these external components: these components
are dependable and trustworthy.
This is something that happens quite often. Actually, if you consider the NPM example.
There are developers that publish very successful and useful components. Everybody else
wants to use them. Thanks to “npm install”, they get it into their application very easily.
After a few years these developers have something better to do and they stop maintaining
their successful package. And somebody else kindly steps forward and proposes to take
over the job. In the spirit of open source the original developers hand them the keys of
the repository, so now someone else starts to maintain these very popular components.
And then in the next release they inject some easter egg, some malicious code, some
keystroke logger, some crypto currency miner. And everyone who does “npm install”
gets it into their system.
At the beginning you trusted the original developer because they gave exactly the soft-
ware you needed. But later on, somebody else took advantage of your trust to deliver into
your system something that you definitely do not want to have.
So it’s not so easy to find reputable sources of reusable software components that you
can integrate, although it has become so easy to ship and integrate reusable software.
251

Not only students but professional developers who should know better are taking ad-
vantage of the convenience of package managers which gladly resolve the transitive clo-
sure of their component dependencies by pulling in software from all over the Internet.
You don’t even think what exactly is it that you are downloading when you install your
components, until it is too late.
As part of the component selection process, you cannot just reuse it as a black box. You
need to inspect the component, to check it does not contain any surprises. This may be
easier to do with open source components, where you can track the change history log.
Component Lifecycle Decisions
Discover
Discover
Select
Select
Integrate
Integrate
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Will the component ѹt within the rest of the system?
How to connect the component?
During the next step we need to integrate the components so that it will fit with the
rest of your system. How are you going to integrate it? and how are we going to connect
it?
The answers have a lot to do with the type of interface that the component has and
whether the component shares many assumptions with the rest of the components within
your architecture. Is it written in the same programming language? Does it run on the
same operating system? Will it save its state in the same database? While it is definitely
possible to integrate components written in different programming languages, their con-
nection will be more challenging, expensive and risky.
252

Component Lifecycle Decisions
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Is the quality of the component good enough?
How to benchmark the performance of the component?
No matter whether we develop the component in house or we discover, select and inte-
grate it from outside, we have a quality assurance issue both from a functional and extra
functional perspective. We need to do the testing to make sure everything works. We can
first test components individually and then do the end to end testing of the integrated
system.
We also need to benchmark the performance, determine how components react to
faults, allocate enough resources to ensure they have enough capacity. Each of these per-
formance, fault tolerance, scalability tests can be done over the individual components
or with the whole system. How do you determine which component is your bottleneck?
How do you identify the weakest component within your system, which may bring down
the whole architecture if something happens to it? It’s better to answer these questions
before the software is put in production.
To do so, you need to clearly specify the type of workloads, for example, the amount
of traffic that your architecture needs to process. What is the typical input going to look
like? Then you need to define metrics. How do you know what measuring the perfor-
mance means? For example, response time of critical operations. Then you have to
specify the type of environment in which you run your software.
As you subject your system to the benchmark workload, and observe the metrics, then
you need a target: what is the reference that you have to achieve? Compared to this
benchmark target, your system is better or worse.
Also when you make any release, every time you change your system, you run the
benchmark again so you can detect if your system is getting faster or slower. To un-
derstand the impact of changes, you have to consider a lot of factors that can affect the
performance of your system, and there is a lecture called software performance where
you can learn more about that.
253

Component Lifecycle Decisions
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
How to package the component for release?
Where to publish the component?
After the tests are green, we make a release. What does it mean to release a compo-
nent? We have to turn software into data by packaging it, taking all the software artifacts,
the documentation, the compiled code, the metadata, and format it so that it can be pub-
lished somewhere (typically on a website).
The goal of making a release is to ship your software so that it becomes accessible for
others to download and install it, or to simply remotely invoke it across the Internet.
254

Component Lifecycle Decisions
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
How to download and install the component?
Once you make a release, you or someone else will take your freshly released software
and install it within their execution environment, which needs to be compatible with the
dependencies and assumptions made by your component.
Depending on the type of component technology, the installation process might be
more or less straightforward – for components delivered as a service there is no need to
install them, maybe only a thin, lightweight client is necessary.
After the installation is complete, we are ready to launch. The component can start,
initialize itself and run. We have completed the transition from development to opera-
tion.
255

Component Lifecycle Decisions
Migrate
Migrate
Backup
Backup
Recover
Recover
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
Stateful components
Not so fast: stateful components have additional steps that cannot be ignored.
256

Component Lifecycle Decisions
Migrate
Migrate
Backup
Backup
Recover
Recover
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
How to update a stateful component?
Will it be able to initialize from its old state?
If your component is stateful, as we go around the lifecycle with one more iteration,
we also need to be able to migrate its persistent state from the old version to the new
version.
This is sometimes ignored or disregarded, because if you make a fresh installation there
is no previous state that needs to be upgraded.
But whenever you attempt to push all the users to use the latest version of your systems
and the existing users will push back and ask: what about our data? Is the new version of
the component compatible with the old data? The old data is the one stored by the former
version of the component. Can I start my upgraded system and it will be automatically
able to read the old data and migrate it into a form which the new version can read? Or
is it going to crash because it is not compatible with the old format, or the old schema of
the database? What if I skip one version during my upgrades: will the latest release be
able to understand data from ancient releases?
Updating stateful components adds one more challenge, as opposed to stateless com-
ponents, which do not leave any messages for their future selves buried in a time capsule
database.
257

Component Lifecycle Decisions
Migrate
Migrate
Backup
Backup
Recover
Recover
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
How to backup a stateful component?
How to recover it after disaster strikes?
Stateful components that rely on stored information also require setting up suitable
disaster recovery processes, which usually entail taking periodic snapshots so that they
can be recovered from backups in case the physical storage media fails.
258

Properties of Components
Encapsulation
Separate the inside
from the outside
Abstraction
Hide implementation
details behind the
interface
Modularity
Reusable unit of
assembly/deployment
Let’s recap our basic definitions to reflect about the basic properties of software com-
ponents: modularity, abstraction and encapsulation. What’s the difference between en-
capsulation, abstraction and modularity? How can you define the three main properties
of a component?
When we talk about abstraction, we try to raise the level of detail of the representation
with the goal of making things appear simpler than they are. And that’s what the purpose
of having interfaces for our components. You just have to understand the interface if
you want to know how to use the component. And that’s the power of abstraction: you
can disregard, ignore and avoid to consider all the implementation details as long as the
dependencies of the components are satisfied and you share the same assumptions as
they are exposed in the interface. You can just use it from the interface and you don’t
have to care about what’s really going on inside.
Encapsulation is also a similar idea: We’re going to delimit the boundary. Whenever
we build something, we need a clear separation between what’s going to be built (inside)
and what already exists (outside). The boundary separates, but at the same time is also
the point of contact.
We also did a similar thing from the point of view of the whole system, the whole ar-
chitecture, where we modeled the context diagram. This helped to delimit the boundary
of our system, to encapsulate it from the rest of the world. Encapsulation is something
that you also do for every component that you’re going to add to the structure of your
architecture.
How do you access something that has been encapsulated? That’s why you need to have
an interface, which opens up what’s hidden inside so that you can still see just enough of
it.
Modularity is a very basic, fundamental engineering concept that allows us to build
something complex out of the assembly of smaller parts. This has also been called divide
and conquer problem solving approach. And each module contributes different func-
tions or a different solution parts that can be assembled and composed together. Such
composition can be static (before deployment) or dynamic (at runtime).
259

John Reekie, Rohan McAdam
Component Roles
User Interface
Active
(Passive)
Stateful
(Stateless)
Now that we have introduced a component into our architecture, we can classify it and
we can give it different roles. Refining the model with such roles, will allow us to specify
certain properties about its behavior and identify whether the component is likely to
constrain some of the quality attributes of the architecture.
We can identify user interface components. UI components make it possible for the
user to interact with the system. Why is this an interesting property to highlight into
the model of the architecture? There should be at least one component with this role.
Otherwise your system will not be accessible to users, will not be usable. There can be
a whole layer of UI components. The whole surface of the architecture can be designed
with multiple UI components, each dedicated to a different type of user, or a different
workflow, or – just another example – editing different types of documents.
Another distinction is the one that separates the active components from the passive
ones. Active components contain an execution thread. More precisely, they encapsulate
the code which will initialize a thread of execution. By default components are passive,
they don’t do anything unless their interface gets invoked from an active component.
Again, for your system to correctly start after you deploy it you need at least one ac-
tive component: this is where the main function that starts the whole application will
be found. Of course, the thread in the active component can also call passive compo-
nents, but the passive components by themselves will not do anything until the active
components interact with them.
Decision: How many threads should you have in your architecture? At least one, or it
will not even start. But you can have as many as necessary to parallelize the execution,
improve the performance for large workloads and scale to take advantage of multicore
processors. With this notation you can decide where they are born, annotate which com-
ponent they’re actually activated from.
Another important classification helps us to separate components that are stateful
from the stateless ones (by default).
260

Stateless vs. Stateful
Stateful
Stateless
y = f(x)
f
y
x
<y, s'> = f(x, s)
f
y
x
s'
s
A stateless component has an interface which performs a computation over the input.
The result of this computation only depends on the input.
The results of stateful components not only depend on the current input, but also de-
pend on the current state of the component.
But what does the state of the component depends on? The previous inputs. So another
definition of a stateful component, is one whose output depends on its current input but
also on all the previous inputs that were ever given to it during the entire history of the
components. You initialize the component with an initial state and then every time you
interact with it you will modify its state so that further results will depend on the history
of all the interactions.
If you have a stateless component, you can assume that every time another component
interacts with it, the result will be independent from the previous interactions of any
other component.
How does a stateful component work? Here is a picture of a stateful component, we
can split it into two parts. One of those is the stateless part, which is just computing
something. However, the input of this computation is not only the input from the out-
side. But is also the current value of the state. After you perform the computation, this
component produces not only the final result that you see, but also it will compute how
to update its state. So the internal state of the component gets updated every time you
ask it to process something.
Stateless components are easier to work with, since offer a predictable and direct rela-
tionship between your input and their output. Stateful components are more complex to
understand, but could have a simpler interface, since they can remember about previous
interactions. Even if in the notation the stateless component is the default, you should
assume that most components in your architecture will be stateful.
Will it be always possible to access directly the state of a stateful component? For ex-
261

ample, if you store it using a database, you can always query the database (bypassing the
component interface) and see what the current state is. You could even reset or replace
the component’s state directly, again bypassing the component interface.
But in most cases, since it is a component, also its state is encapsulated and abstracted.
If the designer of the interface made it possible, you could read or write its internal state.
But in other cases – and these are the most difficult to manage – the state could be com-
pletely hidden inside the component. From the outside you can just interact with its
interface, but you will be surprised to discover that given the same input, the output may
change across different interactions.
Again, it is your decision: how much of the internal state should be exposed through
the interface of a stateful component? Different types of clients should get more or less
visibility and access. If you are trying to debug your component, you better be able to
observe and control its internal state. If you are trying to increase the available account
balance, you should do so by sending a payment transaction and not by simply setting
the desired monetary value.
262

Stateless vs. Stateful Code
Stateless
sort(a)
i+1
Stateful
a.sort()
i++
We start with a very simple code: can you spot the code that is stateful or stateless?
Will these expressions just return a value or have some side effect?
Do you sort this array? or you return a sorted array, without affecting the original one?
Do you increment the variable or just compute the next integer value?
One way to tell the difference is to see what happens if you execute the same code
twice, will the result change? If we look at the sort example, the result won’t change, but
depending on the sorting algorithm, the performance may change if you try to re-sort an
already sorted array.
263

Stateless vs. Stateful Operations
Stateless
getNextID(old_id)
add(x,y)
Stateful
GET /42
DELETE /42
getID()
getDateTime()
random()
PUT /42
Let’s now look at different component interfaces. Which operations you would expect
to find in a stateless component? Which ones assume a stateful component?
You just pass two values. You add them, you return the result. The addition by itself
doesn’t have a history. Doesn’t remember anything, the result only depends on the two
input values. So this is a stateless as it gets.
What if you have a component that gives you a unique identifier every time you call it?
This is a corner case, since the operation does not accept any input, but it is expected to
return a different output.
Can you have a stateless implementation? Or do you need to remember the previous
results so that you do not ever repeat any of them? If you need to keep track of the history
of the results, it means that somewhere you have to remember (you become stateful!)
that you have already produced a certain ID. What’s a simple possible implementation
to produce a sequence of unique values? You can use the current date and time stamp.
If your clock is precise enough, we can assume every time you get a call, you return a
different time stamp. Still, how does the clock knows what’s the current time? Just as you
could once easily demonstrate by pulling the plug on a VCR device, after reconnecting it,
you would see a blinking clock set to midnight. Those were the days before the internet
of things, when devices were not smart enough to query a time server over the network
to initialize (and keep synchronized) the state of their clocks.
What if you use a simple counter? Every call increments it so that the next time you
give the next value. But you need to store the value of the counter somewhere.
Another very nice interpretation of this interface is to return the component identifier,
so it will always be the same result, but it should be unique within the whole architecture
so we can identify distinct components and know that when we read the same value of
the identifier, the component is the same. But how does the component know which
ID value to return? Should this information be stored somewhere? So this will also be
264

part of the component state, especially if you can have different instances of the same
component, you cannot just rely on the name of the component (type) to identify it.
If you want to implement a factory for the objects and you want to create new objects
and assign them the unique ID, then you will need to use those strategies: a timestamp,
a counter. Or maybe you can use a random number. If you are sure the generator is
random enough, you don’t even have to keep track of the previous values, just throw
the dice every time. But unless you are sampling the cosmic background radiation, you
probably can reproduce the sequence of random numbers. It means that you can control
the initial state of the pseudo-random generator, or the seed. So also in this case, we
have some state hidden inside the random number generator.
What if we add one parameter to the ID function? What if we pass the old identifier,
for example, so that the function can produce the new one? It could be as simple as
incrementing it, but could also do a more complex transformation (still guaranteed to
produce unique values). This would look like a stateless implementation, as long as you
can guarantee that the call is made with the last ever returned identifier. Otherwise the
values would no longer be unique.
So you can see now how splitting the stateful part with the stateless part of a compo-
nent can help. The stateful part stores the last result (or in the worst case, all previous
results) and the stateless part can use this information to produce a new value guaranteed
to be unique.
The other HTTP based operations are stateful: you initialize the state associated with a
resource URI with PUT, you read the current state with GET, and at the end of the lifecycle
you destroy it with DELETE. Note that GET /42 depends on the input (the resource URI
/42) but also depends on the history of the previous interactions with the component.
Initially the call will fail (404 not found), after a PUT has been performed the result will
match whatever was stored and the call will fail again after a DELETE. Even if multiple
execution of GET do not change the result, since it is a read-only operation, the result
will be affected by state changes performed through different operations.
Most component interfaces which give you direct access to the component state will
use some variation of the Create, Read, Update, Delete primitives. Databases are the
classical example of stateful components. Through their interface, you can send a query
to read information: the result depends on whatever value is actually stored at the mo-
ment in the database. If you drop the table and you do the select query again, you get
nothing. Because you just deleted all the content of the database table, and the state has
changed.
Whenever you have write operations, you can modify the state. If you read it, you
can observe the changes, and the result depends entirely on the state (no input). For
example, you open a file, you save the file and then you read the file. These are all stateful
interactions because they depend on the content of the file and the content is the state
we are referring to.
Later when we will discuss the scalability of a system or how do we recover a system
from failures, the strategies that you can use are completely different whether the com-
ponent that you’re working with is stateful or stateless.
265

Components vs. Objects
Component
x = require("x")
import x
Object
x = {x: 42}
X x = new X()
What’s the difference between components and objects? Let’s look at some code ex-
amples.
Objects and classes are programming language constructs. It is still rare to see com-
ponents as such part of programming languages, as they are a more coarse grained and
found when you abstract the code while making architectural models.
In most programming languages, there is a notion of encapsulating a large piece of
code so you can reuse it. Sometimes it also called a library, a package or a module.
Everybody knows how to create an object from a class or like in JavaScript – where
classes were only recently added to make the transition of Java programmers easier –
where objects are created from prototypes.
We can spot components (or libraries) as they get imported. What gets imported? other
objects or functions that are published as part of the library’s interface. This code takes
the name of a component and then does whatever that is necessary to load the corre-
sponding component and link it with the caller so that its interfaces becomes available,
it becomes callable. Sometimes it even compiles it on the fly and then eventually you get
access to the component.
When we program most of the time we write this type of code to instantiate objects
from their class types. But whenever the program grows above a certain size, we also need
to know how to package the code into larger, reusable parts which we call components.
266

Components
Objects
Abstraction
Architecture
Code
Encapsulation
State and
Functionality
State and Functionality
Granularity
Coarse-grained
Fine-grained
Modularity
Unit of Composition
and Deployment
Identiѹable Unit of
Instantiation
Interface
Well-deѹned,
documented
Optional
Reusability
Explicit
dependencies (can
be self-contained)
Entangled with other
objects (hard to reuse
by itself)
Let’s look at the similarities and differences in more in detail.
A component is an architectural concept. An object is a construct of an object-oriented
programming language, so it fits within the code abstraction level.
Objects can be plain objects, or they may have a prototype. Sometimes they belong to
the same class, which may have one or more types.
Also components can have types and get instantiated as they are deployed in different
containers.
What is the purpose of a component or an object? They share the same goal of encap-
sulation. The goal of an object is to bundle together some methods operating on the state
of the object. That’s exactly the same that you do with a component. The difference is
that you do it at larger scale: objects are small, components are big. But their purpose:
encapsulation, is the same.
You want to be able to encapsulate a part of the architecture which may have some
state and should have a well defined functionality.
We saw before that we can have stateless and stateful components. In the case of ob-
jects, we can have static class methods, but what’s the point of writing an object with
only methods and no private fields?
In terms of the modularity of your architecture, components are units of composition
and deployment. They define the logical structure of the system, made of multiple com-
ponents assembled together. When you prepare the system to be executed in a certain
environment you can take each component and deploy it separately in a different con-
tainer in a different execution environment. They are very important so that you can
cleanly break them apart so that for example part of the system runs on a mobile phone
and part of the system runs in the cloud. To accomplish that you need to have at least
two separate components.
When it comes to objects, they also get instantiated at run time. However, objects
live at a much finer grained scale: you can create millions of instances of objects with a
267

minimal footprint, and all of these objects can refer to one another, efficiently exchange
messages and keep their own local private separate state.
Regarding abstraction, components have an interface that is supposed to be well de-
fined, documented to help developers learn how to use the component, select whether
the component is the suitable one, and access the functionality that it provides.
Component interfaces – as we are going to see in the next lecture – also cover not just
what the component provides but also what it requires. When you have a component,
you need to know what are its dependencies that should be satisfied.
In some cases, objects or, more precisely, their classes can implement a certain inter-
face. But not always. There can be objects without any kind of formally specified inter-
face. In some languages, you have objects that have private and public and protected and
you have these access control features of the language that allow you to enforce some ba-
sic information hiding principle. Distinguish what is visible from the outside and keep
it separate from what is meant to stay hidden inside? In some languages this is not pos-
sible. Like in JavaScript, where every feature of an object is public, and you have to rely
on naming conventions to achieve some kind of separation between the interface and
implementation.
The final very important difference concerns reusability.
Object oriented programming was invented to solve the problem of software reuse.
Thanks to objects, the selling argument went, you would write program by reusing ob-
jects. This would make your coding so much more productive. Write objects once and
then take the same code along and bring it across different projects. It is so easy to take
an object and reuse it again and again.
Let’s do an experiment: open your last project that you wrote using an object oriented
programming language. Stay in the same IDE, just create a new project. And then copy
a random class that you have written in the old project and paste into the new project by
itself. Copy and paste is the most common software reuse technique: is this new class in
the new empty project going to work? Is it even going to compile? What is the probability
that this will work for a random class from your average project? What’s the reason why
it doesn’t compile? missing includes? broken dependencies? When you write your code
for this class, you are going to depend on or refer to other classes, other types that are
defined somewhere else.
Your objects are entangled with one another. You try to take one out of your spaghetti
dish and you end up with the whole plate hanging from your fork. What I am trying to
visualize is the fact that even if you start from one class, it may be directly related to
a few other ones, but these others will depend on more, and eventually you realize you
should have just copied and pasted the whole project. Which is the whole point of reusing
components.
Want to solve the problem of reusing your code? Objects are too small and they are in
relationship with so many other objects within the same project that it’s difficult to reuse
them by themselves. The unit of granularity is too small.
And if we want to reuse software, we need something that is larger but also easier
to separate it from the rest. So that’s the fundamental difference between objects and
components. Components are supposed to have a clear boundary, an explicit interface,
which includes explicit dependencies, so you know exactly what does this component
need and how you go about satisfying those dependencies.
If you want to make a component highly reusable, your goal is to remove as many
dependencies as possible and make it self contained.
One of the main challenges addressed by component technology is how to describe,
manage and maintain the dependencies of software components. How do we make it
easy for people to install the component and also automatically satisfy its dependencies?
268

Components vs. Objects
• Component types are structured by composing many classes
• Component instances are started by creating many objects
• Objects can move between connected components at runtime
As we go from architecture down to code, we discover that components are imple-
mented using classes and objects. What do you find when you design the internal struc-
ture of your components? We already saw the zoomable representation concept: starting
from the context, then the containers, in which you deploy the components and inside
you have the classes.
If you write the code you structure it with the classes, but still these need to be packaged
and encapsulated into a reusable and composable unit of independent deployment: your
component.
This component is supposed to be easy to reuse, thanks to its interface. Although
inside you have many classes tangled together, when you run the component you create
an instance of its classes. Inside a component you will have a lot of objects that get
created and deleted following the overall component lifecycle.
The objects that are found within one component can actually flow to another compo-
nent as the two components need to interact, transmit and transfer objects to exchange
information.
Objects can move between connected components at runtime.
The opposite sentence doesn’t make any sense: you cannot say components can move
between connected objects at runtime.
Can entire software components dynamically migrate between different containers or
virtual machines? Yes, it is possible, although in most typical cases, components will be
statically deployed within a container, and the container (or the virtual machine) is the
one which moves.
269

Component technology delivers everything that you need to support the component
abstraction and to package and release your software into reusable components, which
can be shipped and deployed. With it you can take a large (or small) piece of code, docu-
ment its interface, define its dependencies and publish one of these components some-
where so that others can find it, discover it, and decide to integrate it in their own archi-
tecture.
Every software component depends on the framework needed to turn its software into
a component. The problem is that there are too many of these frameworks, and, by acci-
dent or by design, they are not always compatible.
270

Component Frameworks
• Java
• Maven Central
• SpringSource
• Enterprise Java Beans
• Eclipse OSGi
• JavaScript
• Web Components (Bower)
• Node Packages (NPM, Yarn)
• Web Services
• RESTful API
• WS-*
• UNIX (apt-get, yum, brew)
• .NET
• Ruby Gems
• PERL CPAN
• Cocoa pods
• R CRAN
• PHP Pear
• Python PyPi
Component Frameworks Demo Videos
˾. Discovery: where/how to ѹnd a component?
˿. Documentation: how are components described?
̀. Reuse: how is a component delivered and installed?
́. Integration: how are components embedded into a larger
software system?
̂. Publishing: how can you package a component for reuse and
advertised so that it can be discovered?
̃. Update: how are components versioned? and how to deliver
updates?
̄. Removal: can components be uninstalled? how to remove them
from the repository?
271

Where to ﬁnd components?
•
•
•
•
•
•
•
Component Source
Eclipse Marketplace
The Comprehensive R Archive Network
Debian Packages
Cocoa Controls
Wordpress Plugins
React Parts
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Here are some links in case you are not yet convinced that the software component
economy exists. There are marketplaces where you can find software components of
all kinds for free or also for payment. If you are considering a career making money by
writing software and making it reusable so that others can buy it, these are the places to
go to sell your own products.
You will not be selling products to the users like in an App Store. You will be selling
software to the people that build such retail applications, and you will work a little further
up the food chain.
You will be surprised by how large but also how highly specialized some of those niches
are.
272

Buy vs. Build
Do we 
know how
to build it?
Does the 
component
already 
exist?
By when
 do we
need it?
How 
much 
does it 
cost?
Do we 
know what
we need?
Does it
do exactly
what we
need?
yes
no
no
yes
yes
no
yes
no
Customize
it
soon
Buy
it
Does the 
component
already 
exist?
yes
no
Build
it
later
Since we mentioned software component marketplaces, let’s have a design discussion
about making one particular design decision: whether to buy or to build a software com-
ponent.
After you have decomposed your software architecture into multiple components, for
every component listed in your logical view, you should reach a decision of whether the
component should be built in house (write its code by yourself) or for some reason (it’s
easier, cheaper, faster, you get better quality) if you buy it from somewhere else.
How do we make this decision? Let’s start from the beginning: do we know what we
need to buy? This checks if we have a clear vision or a clear idea for the component
that we’re looking for. If you want to try to buy it, but also if you decide to build it, the
prerequisite is that you actually know what software component you are looking for.
Can you really make the decision if you don’t know exactly which component you need?
Do you even know its name? Did you write down the interface it should provide you
with? What kind of dependencies are you ready to satisfy? Maybe you can take the agile
solution: by now you’ve already started writing the software and hope that while you
build it, you will gain a more a clear picture of what it is supposed to do along the way.
Let’s assume that we know the component that we need, so we have made a clear de-
composition in our logical view and at least a sketch of the interface of the component.
The next question is whether we actually know how to build it.
In some cases you discover a need for certain software, but then you realize that you
don’t have the capability of building it. Then you have no choice but to buy it. If you
cannot buy it because it’s not available for sale anywhere, you can hire a development
team or outsource the development. This will be expensive.
273

So the pre-requisite for building it, is that we know how to develop software. The
assumption behind buying it, is that it exists somewhere, we can find it, and we can afford
it.
How fast do we need it? How much money can we afford to pay for it? (a budget is
always needed, either to pay the price or to fund the development effort) What’s the
acceptable level of quality that we require at the end of the process?
If we found something, does the component do exactly what we need? If it does exactly
what we need, might as well buy it. But what about the price?
If the component we found does not do exactly what we need: there is a missing fea-
ture, or it does too much. Then we may still buy it, but we will still need to do some
construction work around the edges. We need to do some adaptation, some customiza-
tion, some extension, some configuration. And that requires also some development
capability, but probably less effort than when we build it from scratch.
The other important criteria is: how soon do we need it? Do we need the component
right now? Or can we wait for some time before the component is ready? If it exists and
we need it right now, why wait? Just buy it. So this is an important decision driver: Are
we in a hurry? Can we afford to wait?
From the seller perspective, you have a potential customer for which you can deliver
exactly the component that they need, and you can give it to them right now. This sounds
like you do not need to offer any discount.
However, if you can afford to wait a little bit, then you have time to look around, find
some alternatives or even start building it yourself. If you can rapidly build it in a rea-
sonable amount of time: why not?
If we try to make a flow chart to help making the decision that we can try to track the
option along the tree.
The preselection is: are you really sure you want to get into this decision? Do we
have a clear idea of the component that we need? If not, you just stop and then try to
improve your design so that you have a better picture of what is the requirement that the
component supposed to satisfy.
If you know what you need, you can assess whether the capability to build it is there at
all or not.
If not: we are not capable of building that particular component. Which means that if
the component does not exist, then you’re also in a bad situation, unless like someone
was suggesting you outsource its development at that point.
But if the component exists and you find it, you need to check: if the component does
not do exactly what you need, then you will customize it: you can still buy it, but then
you have to do some extra work. And if it does exactly what you need, and then you might
as well just buy it: there’s no point in considering the other option. Especially if you have
an urgent need for it.
If we backtrack a little bit and we assume we actually know how to write software, and
the component does not exist, then we have to build it.
274

How much does it cost?
Buy
Licensing fee
Support contract
Subscription fee
Customization
Both
Integration
Licensing fee for
dependencies
Testing
End user training
Deployment
Build
Project Management
Hardware and software
development tools
Technical Writing
Development Team
To close the buy vs build decision, let’s see how to set the right expectations concerning
the budget. How much are we going to pay for it?
This can help you to estimate which option would be cheaper, assuming you can wait
for the construction time and take the risk of project failure.
I would also like you to consider what are the elements that contribute to the cost,
depending on which decision you take. There are some common aspects, but also others
which can be ignored.
For example, licensing fees: you can see that as the price of the software that you are
about to buy. But there is a licensing fee for the software for the component and there
is also this one licensing fee for dependencies. In some cases you pay the whole price
to buy the whole package. In other cases, you buy a package, but then you discover: if
you want to really run this, you have to buy more software because the software you just
bought depends on it.
For example, you have to buy a database license, operating system licences (why do
you think open source is so popular in the Cloud?) Sometimes the cost of licencing de-
pendencies can be bigger than the cost of the product itself.
If you build the software and you have dependencies, you will also need to pay for them.
When you buy the software, you would expect to pay once for the licensing fee and
then you acquired the right to use ”your“ software forever. Nowadays, more and more
the cost becomes a recurring subscription: you have to pay the licensing fee every year.
Once you buy a piece of software, you have to sign a separate support contract so that
you get the help you need in case there are some problems with it. It’s not enough to pay
for installing it, and then you have to pay to get help starting to use it.
If you build it, you have or you need a development team: You have to pay the developer
salaries. Developers need the proper tools: hardware, repository servers, build servers for
275

the continuous integration pipeline and running all the testing. So that requires some
infrastructure which also costs money.
And if you do a large development project, you also need to hire a project manager,
and – don’t forget – an architect to lead the developers.
While you develop your software, you need to test it. However, also if you buy a piece
of software, you need to test it. When they sell you software they overpromise and then
they underdeliver. Testing is what helps you to clarify whether the original expectations
are met. With airplanes it’s easy: because the machine has to fly from the assembly
plant to the airport where the airline who bought it will embark the first passengers.
With software you always have the temptation to drop the package you bought directly
in production and make your users test it on Monday morning.
When you buy it, you have to install it so you have to deploy it. This is true also if you
build it, unless the developers are the actual end users. This is also something that you
have to invest from both sides. Maybe you’re going to buy software as a service then in
that case deployment is much cheaper because everybody can access the system remotely
through a universal client, like a Web browser.
Let’s imagine that when you buy it, the component does not do exactly what you need.
What you need to do a little bit of customization. So the customization could be a project,
so we can also consider project management as a common cost. Another argument would
be that the deployment of the product that you buy is so complex that you would need a
whole project to be able to acquire the software.
That is also additional aspects that we can consider. What is a cost only for building
and not for both? We would need to do some technical documentation writing, some user
guides, training material.
In both scenarios the users will need training (unless you buy a commodity product
which you can expect everyone to know how to use)
If the component is just a component into a larger architecture, you need to integrate
it with the rest, and this can be more or less expensive depending on whether you buy it
or you build it. If you buy it, you can expect that it will not fit exactly, if you build it you
better take the opportunity to make it fit precisely.
276

References
• Doug Mc Ilroy, 
, NATO Conference on Software
Engineering, Garmisch, Germany, 7-11. October 1968
• Clemens Szyperski, Component Software : Beyond Object- Oriented Programming, 2nd
Edition, Addison-Wesley, 2002 (See Chapter 11 for more deѹnitions on components)
• George T. Heineman, William T. Councill Component-Based Software Engineering: Putting the
Pieces Together, ACM Press, 2001
• David Parnas, 
, CACM 15(12):
1053-1058 (1972)
MASS PRODUCED SOFTWARE COMPONENTS
On the Criteria to Be Used in Decomposing Systems into Modules
 
 
 
 
277

Software Architecture
Reusability and
Interfaces
6
Contents
• Required and Provided Interfaces
• Information Hiding and Encapsulation
• Interface Description Languages
• Tradeoffs: Usability vs. Reusability vs. Performance
• APIs: Examples and Design Advice
• API Design Principles: Explicit Interfaces, Least Surprise, API
First, Small Interfaces, Uniform Access, Few Interfaces, Clear
Interfaces
278

After we have seen components and how they make the architecture of our system
modular, today we are going to discuss more in detail, how do the interfaces of compo-
nents look like and how they make it possible to build the architecture out of reusable
elements.
The main element that makes it possible to reuse a component is its interface.
I would like to begin with this picture showing you a different type of interface, not the
interface or piece of software, but the interface a biological system.
How is it possible to compose a larger organism out of individual cells? Cells are sep-
arate entities, but they can live together in multi-cellular organisms, thanks to the in-
terface, which separates them but also allows them to communicate and exchange stuff.
The interface or the cellular membrane also separates living space inside from the out-
side world. It acts as an extremely thin filter, which for example, is also very important
to keep viruses out.
279

Component
• Locus of computation and state in a system
Processing
State
• Reusable unit of composition
The last lecture was about modularity and components. Components are the result of
the structural decomposition of your architecture. They are responsible for processing
and storing data. Thanks to their interfaces, components are also meant to be composed
as a building block and assembled into larger software architectures.
280

Component Interface
• How to use a component?
Processing
State
Provided 
Interface 
Required 
Interface
• How to reuse a component?
If we focus on the interfaces, the main concern becomes: How do we use the com-
ponents which make up our architecture? How do we make their data processing logic
and their state accessible to other components so that developers can build and integrate
them into larger systems?
We use components through their interface which gives us access to the functionality
they provide.
To make it possible to reuse a component, we also need to satisfy its dependencies. A
component that is missing some dependencies will not work. It may refuse to start. Or if
for some reason it did start, you try to call it and the component will crash.
How to use the component? How to reuse the component? These are not the same.
From the perspective of a component vendor, the goal is not only to make it usable
for one developer within their specific context, but to make it reusable so that a lot of
customers want to buy this component.
If you want to make a component usable, you need to invest into making the provided
interface as simple and as easy to learn as possible. A general and widely applicable
interface will instead make your component more reusable.
If you want to use a component then you also need to focus on making sure that its
dependencies are satisfied. If you want to make a component reusable, then it should
not have any dependencies. Your customers can take your component and start using it
immediately without bothering about installing any dependencies.
Both use and reuse are interdependent. Start by making the component usable in one
system, for one customer, for one solution. And then try to generalize its interface and
make it applicable to multiple domains. Turn it into a generic, highly reusable, infras-
tructure component which everybody cannot do without for building any kind of system.
281

Provided Interface 
• Specify and document how to use the externally visible features
offered by the component:
˾. Data Types
• Deѹne the component data model
˿. Operations
• Call functionality, modify state
̀. Properties
• Read/Write visible state attributes
́. Events
• “Call-backs” – Notify about state changes
Let’s look at each side starting from the provided interface. This is the interface through
which we get access to the functionality of the component.
This is the surface on which we need to make internal features visible so that devel-
opers can use them. To do that, you need to have some kind of a specification, some
documentation that explains how to use this component.
What kind of information do we need to include in the provided interface? We’re going
to define the data model for the component: what kind of data types are going to be used
to represent the information that is exchanged as input or output via this interface.
This is typically done using the type system of some programming language, using it
to define a data model. It’s also possible to use some programming language agnostic
type system, for example XML or JSON or Unicode character strings.
The goal of the interface data model design is to define what exactly is the representa-
tion of the information that I can exchange with the component. Depending on the data
formats that you select, you can be more or less compatible, more or less interoperable.
The second aspect comes into play for components which are not just repositories for
storing information, they are also active elements that can process and transform the
data. So the interface should enumerate which operation the component offers. How to
call the functionality implemented by the component? Is the call going to have an effect
on the state of the component? or is this a purely functional operation?
For stateful components, you also want to access the state of the component and this is
what we do through the interface. The content of the storage within the component can
be directly accessible: you will be able to read and write properties, or state attributes
that have a certain type according to the data model.
282

The interfaces of active components will also feature events. They notify the outside of
the component that something interesting has just happened inside. The goal of events
is to make it possible to observe through the interface internal state changes of the com-
ponent.
Data Model, Operations, Properties, and Events: these are the four basic features that
you may find within the provided interface of a component.
Stateful
(Stateless) 
(Passive)
op
y
x
Operations
w
r
Properties
p
y
x
Operations
Active
Stateful 
Active
e
Events
y
x
Operations
e
w
r
Properties
p
y
x
Operations
e
Events
e
Provided Interfaces
and Component Roles
Which type of components provides each of these features? We can classify compo-
nents as being stateless or stateful as well as active or passive. These are orthogonal, so
we can have: passive and stateless, passive and stateful active and stateless, and active
and stateful.
A stateful component remembers the entire history of its inputs. The output of inter-
action with the stateful component depends not only on the input that you provide, but
also on the previous inputs.
Active component start running a thread of control, and therefore it can do things on
its own. Passive component doesn’t do anything unless another component calls it.
Starting with the simplest type of component, the one that is stateless and passive,
its interface will only feature basic operations which take some input, process it and give
you back the output. This will happen only when you call the operation from the outside.
Since there is no state, the component will not remember that you did that, so every time
you call it with the same input again, you can expect the result will be the same. The
passive component will not execute the operation unless you call it from the outside.
In case the component is stateful, then we have to also consider the fact that inside
the component the effect of the operations will be stored. Not all stateful components
have properties, since their internal state can be indirectly manipulated via operations.
283

But it is also possible to access directly the internal state. If you decide to expose
the internal state of the component at the interface level, that’s what properties are for.
When you set the value of a property you are going to modify the corresponding internal
state. When you read the property you will have the chance to directly check what is the
current state of the components. That’s what properties are for: to offer direct access to
the state.
Sometimes properties are not present in the interface even if the component is stateful,
because when you call the operation, the operation can modify the state but also give you
access to indirectly read its current value.
If you don’t have a stateful component, then properties at the interface level are useless
because even if you set the value of a property, the component doesn’t know where to
store the value.
If we look at the case in which we have an active component with some background
thread running inside, then it becomes interesting to make the effects of such thread
visible at the interface level through events.
When you call an operation, you typically give the input and then you wait for the
output. And the thread that calls the operation jumps into the component, executes the
code and then returns control back to you.
If you have a thread inside the component, then you can just work with events where,
for example, you submit the input into a queue. And then you can continue doing some-
thing else when the internal thread of the component becomes available, takes the input
from the queue, processes it, and as it gives back the output, it notifies you with an event
that there is a result and then you can pick it up at your earliest convenience.
Events make sense to be present in the interface if you need some asynchronous inter-
action with an active thread of control inside the component, behind the interface. You
can find events in the interface of active but stateless components: whenever you sub-
mit some input, it will take it and start a thread to process it in the background and give
you back a notification when it is done. There is no event queue, since the component is
stateless. Also there is no memory about the past, so every time it will start processing
the input from scratch, without the possibility to cache previous results.
If we combine these two, we have a stateful and active component. This is the most
complicated one, with the richest possible interface, featuring: operation (blocking),
events (asynchronous interactions), as well as properties (direct access to the internal
state).
I hope that this example has explained even more the difference between operations,
events, and properties.
For all three, you will need to also define what is the corresponding data model. If you
have an event: what’s the data associated with it? is it a pure signal (just an identifier to
distinguish it from other events) or does it carry a message (with a complex payload)?
For properties even more so, how do you expose a projection over the internal state?
what kind of data structure do you see from the outside through the interface?
It is your decision as an interface designer to use these elements. You can also have
a stateful active component which only features operations in its interface. It will not
be possible to observe or alter directly its internal state, since properties are missing.
All interactions through operations will block the caller, since event notifications are
missing.
284

 Required Interface
• A component can be reused only if its dependencies are
satisѹed
• The platform is compatible:
• Runtime Libraries/Framework
• Operating System/Device Drivers
• Hardware
• The environment is setup correctly:
• Databases
• Conѹguration Files
• File System Directories
The required interface describes what a component needs to work properly. It answers
the question: what are the dependencies which we should satisfy?
We find the information that tells us what the component needs in its required inter-
face.
The required interface can be satisfied by introducing another component that pro-
vides exactly the interface required by our component. If we can connect the two in-
terfaces together we have successfully satisfied the required interface. This needs to be
solved recursively, so now let’s look at how to satisfy the required interface of the newly
introduced component.
The required interface can have broader requirements. A component may only work in
a certain runtime environment. They need to be deployed on a certain platform. They re-
quire a certain version of the operating system. They might need some particular device
drivers. Maybe the components are directly dependent on the presence of some particu-
lar hardware, some sensor, some type of processor, or GPU.
These are also all dependencies, which may get overlooked. Nowadays we are used to
write portable software components, which do not make assumptions about which exact
version of the operating system they run on. Sometimes they don’t even care about which
exact operating system they will run on, as long as there is one.
These environmental dependencies can be significant deal breakers. You have a com-
ponent that you want to use, and then you discover that it doesn’t work without a certain
device driver of the operating system. Either you upgrade (or downgrade) the operating
system or you find another component.
You really wanted to install the official national covid tracking app, but it turns out
it needs access to the patched bluetooth API which has been only included in a recent
release of your mobile OS, which unfortunately doesn’t seem to run on your device.
Sometimes these hard to satisfy dependencies do not come from the component itself.
285

The component could perfectly use an old version of the operating system as it is not us-
ing any of the recently added APIs. It turns out that the compiler that you use to compile
the code of your component only supports a certain version of the operating system as
target to run the code that it produces.
You will be surprised how subtle can dependencies be and how mostly harmless archi-
tectural decisions (such as the choice of your favourite programming language) will limit
where your software can run.
Another source of unexpected dependencies come from the dependencies of your de-
pendencies. Those are also difficult to control. You can control the immediate depen-
dencies from a component, but if this component needs other components, they also
have their own dependencies and your system will work only if the transitive closure of
all the dependencies is satisfied.
More dependencies: Not only we have to pick the right platform, but we also need
to set up the system with the right configuration. For example, if you have a stateful
component, it will need to connect to a database to manage the storage of its state. The
component needs to know: where is the database? How do I connect to it? What kind
of database account am I going to use? And if you if you start the component without
this information, as soon as the component will attempt to store some data, it will fail to
locate an appropriate database and will crash or continue to work in a zombie-like state
with the unsatisfied assumption that its state has been stored persistently.
Such configuration needs to be written in some configuration files. Storing the con-
figuration in the database itself opens up a recursive meta-configuration problem, which
is better to avoid. A very frequent reason for initialization crashes or startup failures of
components is that the configuration files are missing, incomplete, corrupt or incorrect.
You hope that if you leave some configuration default and but also the default configu-
ration is not there.
Failure to satisfy required interfaces impacts the ability to build but also deploy and
start operating your component. You need to make sure that you deploy it in a compatible
platform and you also configure its runtime environment correctly.
As an architect first you try to maximize the component compatibility with as many
platforms as possible. Try to avoid restricting it unnecessarily. As a developer, for ex-
ample, you should write your code in a defensive way, where you always give defaults to
configuration options and you do not crash if the directory that you expect is not present.
Instead, you can just create it on the fly so that everything works smoothly and you can
switch on the system and start it the very first time without any problems.
Dependencies: we tend to forget that these exist, but the moment that you take a com-
ponent that you didn’t develop yourself and you want to buy it, for example, knowing
detailed information about its required interface is critical because otherwise you risk
that the software you just bought will never work for you. It did work for the original
developer though.
286

Bertrand Meyer
Explicit Interfaces Principle
• Components always communicate through interfaces
• No Shared State
• No Out-Of-Band Communication
When we talk about interfaces, we make a fundamental assumption: in our design
and in our implementation the only communication channel between components is the
interface.
It is tempting when you write the code to ignore that assumption. What if you want to
call an operation hidden inside a component and not visible in its interface. Because you
are using a certain language that doesn’t properly encapsulate private functions, you can
still – if you know where to look – find the operation and make the call.
For some reason, they forgot to make it accessible through the interface, but your quick
solution solved the problem: you got to what you need.
Another temptation is to just store the partial result in a global variable so that all your
components can now happily share the information they need without having to expose
it through properties or getter operations in their interfaces.
Why do you think it’s a bad idea to do this? What could be a consequence when you in-
troduce a global variable? You’re breaking the encapsulation that the component bound-
ary gives you. You break the independence of the components you use. Their interface
is no longer truthfully stating which are the available operations, which are the points in
which the component can read input or write its output results.
Now the components sharing a global variable are strongly coupled because of the
shared global variable. If one component changes it, every other component (it’s a global
variable after all) will see the effect. Is this what you really want? If you access data pro-
tected by an interface, you could assume that if there is concurrent access, the interface
takes care of the serialization to keep the state of the component consistent. But if you
just bypass that what do you expect will happen in case of concurrent access to a global
variable shared by multiple threads?
At the end, if you violate the explicit interface principle, you no longer have separate
components. If you bypass their interface, you introduce direct coupling between the
outside and the inside. If someone manages to patch through a call to a private function,
this function is no longer private. You are no longer free to evolve it independently of
the rest of the architecture. Everyone could be calling this function. How do you know?
287

If learning how to use a component through its official interface is hard enough, how
hard do you think it is to learn how to use it by going through its entire implementation?
What if someone else implemented the component?
The interface concept results from lessons learned in many decades of software ar-
chitecture. We are still arguing on how to design them properly, but we all agree they
are important, powerful and useful. Bypass interfaces at your own risk, you have been
warned. Your architecture will never go beyond a big ball of mud or spaghetti monster,
where everything is connected to everything else.
David Parnas
Information Hiding
• Always keep the implementation details secret (invisible from
the outside) and only access them through the public interface
• Makes it possible to replace components (as long as they share
the same interface)
• Ensures component decoupling
The second, very important, aspect of interfaces is about their purpose to abstract the
internal implementation of components. You just work with the interface, but you don’t
care about what happens inside the coffee machine. As long as you put in the money,
select the amount of sugar, then you get the result of the computation. No need to worry
about where is the source of the electrons boiling the water, or which part of the world
the coffee was harvested from.
The goal of establishing an interface is to raise the abstraction level so much that for
example you can no longer tell if the task you submit is implemented by a machine or
by a human intelligence. Was this lecture recording transcribed by a human listener or
automatically generated? Does it matter if you can just read it in this book? This was at
some point called the Mechanical Turk, which literally worked thanks to a person hidden
within the box, moving the automaton.
If you do not have an interface then everything will be visible, you will need to consider
how is the functionality implemented if you want to learn how to use it.
Thanks to the interface, we can ignore the implementation details and only focus on
how do we use the component without having to worry about how the component im-
plements what we’re going to use. We use the term “black box”, as a metaphor for a box
whose inside is not visible. We are actually not interested in looking inside the box.
288

What is information hiding about? The goal is that since you have an interface, you
keep the implementation secret and hide it from the outside. And why this is important?
As long as the interface doesn’t change, you can replace the implementation. You can
switch the implementation as long as the interface is not affected.
You establish a dependency to the interface in order to avoid establishing a dependency
to the implementation.
Information hiding ensures that who is using your components remains decoupled
from your implementation. They are, of course, coupled to the interface, but the point
of the interface is to separate them from what is behind the interface, which is the im-
plementation, which remains a secret thanks to the interface.
This separation works both ways. So who is using your component will not depend
on the implementation. But your implementation should also not depend on who’s us-
ing your components. The only thing that is in common is the interface, and both are
dependent on it.
If we change the implementation so much, it is no longer possible to contain the effect
and the change will leak at the interface level. If this happens, every other component
depending on the interface will be affected.
289

George Fairbanks and David Parnas
Effective Encapsulation
• Reduce the cognitive burden for using a component
• Well designed interfaces are simple to understand
• Every interface should hide one secret (avoid leakage: do not
reveal unnecessary details)
• Changes to the implementation hidden behind an interface do
not affect clients
• Difѹcult to hide the impact of all changes
• How to choose what should be hidden?
When you encapsulate a component and abstract it through the interface, you make it
easier to use the component because you just have to understand its interface and you
can ignore the implementation details.
We will discuss what are the qualities of well designed, easy to understand interfaces.
The core of what you do as a interface designer (one of the various tasks an archi-
tect should work on) is to avoid leaking implementation details at the interface level.
You don’t want to reveal details that are unnecessary if you want to use the component.
Make life easier for the developers using your component, it was already hard enough to
successfully implement it. You don’t have to worry about all of these things, look, I just
hide them from you.
The advantage is that – again – if you change the implementation and the implemen-
tation change did not affect the interface, then you know you have managed to contain
the impact of your change within the boundaries of the component.
In general, it is impossible to hide the impact of any arbitrary change that you make
to an implementation. What if your change requires to produce or consume more data.
What if you are now changing the nature or the semantic of an existing operation and
make it compute something different, something new, something unexpected?
I modified the implementation of an existing operation, but now I need more param-
eter. The operation is still the same, but callers have to send along one more parameter.
How can you mitigate this interface change? What if you can give a default value for the
new parameter? If they don’t give it to me, it still works thanks to the default. And if they
want to take advantage of the new version, which is more powerful, they need to change
their code and they have to pass the new parameter.
And your task as an interface designer is to make these decisions. Do you start with an
existing component, which is already implemented and then try to abstract its interface?
What are you going to hide? What are you going to reveal?
This are difficult design decisions to make, because they mainly depend on who is on
the other side: who wants to use your component. They will need certain things and will
expect to find them in the interface. Nothing else.
290

Example
• Pre-condition: unsorted content
• Post-conditon: sorted content
• Hidden details:
• What sorting algorithm is used?
• Is the sorting algorithm stable?
• What if the content is already
sorted?
• What data structure stores the
content?
• Can I provide my own comparator
strategy?
sort()
sort(cmp)
In this example we have a very simple interface with an operation which transforms an
array whose elements are in some random order into a sorted array.
That’s all the information that you have to describe what’s in the interface.
What do you think this abstraction is hiding about the implementation?
We are hiding: the sorting algorithm. The complexity of the sorting algorithm. From
the outside, when we call the sort operation, we don’t really know about these details.
What’s the performance we observe, can we send different array sizes and try to recon-
struct the algorithm based on the time it takes to process the input?
It is also not specified the sort order: ascending or descending?
What about: how the values are stored? Are we working with an array with a linked
list, or with a double linked list? Is there a limit on the size of the input array? This would
belong into the data model of the interface. Maybe the implementation code is indepen-
dent from how the input data is structured. But definitely writing the sort algorithm code
for an array is not the same as implementing it for a list.
Let’s change the interface: now it exposes one more parameter. This is a comparator
function, which helps to sort arrays where the elements store arbitrary content. As long
as you know how to compare a pair of values, you can sort the whole array.
This is another assumption: does the implementation know how to compare and eval-
uate the order of two arbitrary elements. Making this part of the interface, helps to shift
this knowledge originally buried inside and make it accessible to the outside.
Another assumption: What if the input array is already sorted? Can this be detected?
If so, can the actual sort be skipped?
One last assumption concerns the stability of the algorithm. Do equivalent or identical
elements retain the same position? or will they be shuffled by the sort even if they have
the same order?
291

What is also not fully clear is whether this is the interface of a stateful component,
where the array to be sorted is stored within the component itself, or it is an operation
of a stateless component, which would be applied to an array given as input and forget
about the result afterwards.
After abstracting away the implementation to satisfy the needs of the developers trying
to reuse our component, what is left?
Whatever is left, we need to describe it.
Let me give you an example to show to which extent or how precisely you can give such
a description and why it’s important to do so.
If we just list the operation but we do not give its name, we know that it’s possible to
invoke the operation ’f()’, but we don’t know exactly what the operation does, we just
know it exists.
The lack of name is ambiguous and imprecise. If we give it actually a name, through the
name, we already share a lot of information and create the expectation that this operation
will compute something: it will perform an addition. This is a step forward, but it’s still
not enough. Do we really know how to use it? Do you know exactly what is going to
add? Is it a stateful component which will do an addition over some values already stored
inside the component? But if the component is stateless, then we need to know more:
We need to know how do we give the input data so that we can process it and perform the
computation.
So now we know that actually this operation is a function that can add two parameters.
Is this enough or do we need to further refine our description?
Out of all the possible pairs of parameters that we can pass, we should specify also
their type. So this refers to describing the data model: is it going to be an addition over
floating point or integer numbers? What if you pass strings? will it try to concatenate
them?
292

It depends, we have data types for the input parameters, but we should also specify
them for the output result.
Here we have now reached the full operation signature with the types and names. If you
are going to implement the component with a statically typed programming language,
you have now enough information to write the code.
The previous examples were enough to use the component and call it from a more
loosely typed programming language.
Having a detailed specification of a statically typed interface helps to check statically
that the interface that you have selected is compatible with your code.
The description makes it possible to verify the corresponding expectations at compile
time or at run time, just before making the call: the function with the given name exists,
the parameter names (and types) match. The less assumptions are explicitly described,
the more difficult is to do any validation and the incompatibilities will be discovered only
at runtime.
The ultimate truth about what this interface does lies actually in the implementation
itself: the only place where you truly know what is the actual relationship between input
and the output.
In this example, do you see something unexpected? There is a little bit of a surprise.
There is a small inconsistency between the multiplication expression that we have in
the implementation and the name of the function that it says that we’re going to do an
addition.
293

Principle of Least Surprise
Interfaces should behave consistently with the
client's expectations
• Obvious
• Natural
• Consistent
• Predictable
• Easy to guess
• Easy to learn
The name of a function should reѺect what it does
The name of a parameter should indicate what it means
The principle of least surprise should be respected when abstracting and describing
interfaces.
Especially when you pick the names that you use inside the interface, make sure that
they create the right expectations in the developers reading your description.
This expectation needs to match what you find in the implementation. If you call the
function called ”addition“ but you multiply the numbers, you violate this principle.
The principle of least surprise is important not only for consistency, but also for making
things predictable, easy to learn, easy to guess.
If you give a name, the name should help to explain what the corresponding interface
feature does. Giving names to interface elements is most critical: names (and the names
of types) should compress the intended semantics in one CamelCase word.
You can use additional documentation, such as natural language descriptions to mit-
igate the effect of incorrect names, clarify ambiguous ones, and shed more light on the
semantics and usage scenarios of your interface elements. Still, even without such addi-
tional metadata, the names you pick should speak for themselves.
Not to add any more pressure, but: once you come up with a name, give it to some
interface element, and release the interface, it will be difficult to change the name once
others start using in. A wrong or ugly name will stain your interface for a long time.
294

Easy to use?
Usable
b = new Button("ok");
b.setDefault();
b.disable();
boolean equals(Object other);
Array getData()
{
return []; //if empty
}
Unusable
Array getData()
{
return null; //if empty
}
boolean equals(Object obj);
new Button("ok", true, false)
About making the interface easy to learn and easy to use, I wanted you to reflect about
these various examples: find a pair that is more usable than the other one. Some are
specifications of interfaces, others are examples of usage for the interface, others also
include the implementation.
We can start from the new button interface. There are three parameters: the first could
be easy to guess: the text that you put inside the button. What about the others: true,
false. How can you tell what they mean?
Now if we look at the alternative, we’re using properties to configure the button after
we construct it with its text: it’s the default button and it’s disabled.
So by transferring what we know from here, we can guess that the 2nd parameter indi-
cates whether the button is disabled or not; we disable it because we set it to false.
If you just read the code, reading these boolean flags, especially if you find a lot of
them, makes it hard to understand what the code means unless you remember the posi-
tion of each flag. That’s not a nice practice when you design your interface, unless you
rely on people having a very good memory about the order of the bits they should pass
to configure a button.
In the alternative the interface is more user friendly because we use the names to give
meaning to those boolean flags.
The second example is about minor difference on how to name one parameter: obj
vs. other. This can impact the interpretation of the same ”equal” method: is it equal to
other, or is it equal to object.
Sometimes we give names to parameters that are just a placeholder for their type. The
language gives us two independent elements, the name and the type, but we only use
the type: you already know it is an object from its type the name ’obj’ does not convey
additional information. If we name it ’other’, we give more meaning: the comparison
equal checks the parameter against the state of the current object. So that is slightly
more clear.
295

The third example is a piece of code that implements the ’getData’ which returns an
array. Think about how you would write the caller of this code. When you retrieve an
array you typically want to iterate over it. How do you deal with the empty array case?
Is it easier to write a client which can just iterate over an empty array, i.e., do not
process any element if none are there? or do you prefer to write code which tests for
a special case, the null result, and thus explicitly skip executing the loop if the data is
empty?
While there is a different between the semantics of missing data (null) from empty
data ([]), you should be aware of this difference and do not use one for the other. While
it is better to return empty arrays so clients can process them as array without having to
detect and deal with null values, in general every null or nil value that an interface can
return generates a potential crash in the client code, which somewhere (not necessarily
right after getting the null result) may forget to protect against dereferencing null values.
Sometimes determining whether your interface design is usable or not depends on who
you expect to read and program against your interface. Sometimes it is a matter of style:
what you’re used to work with as opposed to what’s a common convention while writing
code in a given programming language.
Read interfaces, look for patterns, assess whether it’s easy for you to understand how
to use them, so that you can acquire a certain taste of what makes it an interface easier
to use and to learn for developers that have never seen it before.
296

Interface Description Languages
CORBA 
IDL
RPC 
IDL
WSDL
Java Interfaces 
DCOM 
MIDL
1990 
1980
2000 
WADL
2
Interface Description Languages
BA 
L
WSDL
es 
2000 
WADL
2010 
RAML
OpenAPI
Swagger
2020 
How do we describe such interfaces? Describing interfaces is so important that there
297

are specific languages designed just for that.
Sometimes these languages are a subset of an existing programming language, and
sometimes they are completely separate. The only thing you can do with an IDL is to
model the content of an interface: describe its data model, enumerate the operations,
the events. and the properties.
If you want to implement the component behind the interface, you have to use a sep-
arate programming language, although it is possible to generate code from the IDL de-
scription (and viceversa).
Let’s see from a historical point of view how IDL have evolved. The first IDL language
was introduced to help with describing remote procedure calls. When you had a remote
server separated from a client, then you had to have a mechanism to help the client know
what the server was providing and be able to invoke it remotely, and so there was a spe-
cific language to describe all the remote procedures. You could already even generate
both client and server code from it. This code would support the distribution of your sys-
tem; when you move from procedures to objects, then you have also distributed objects
environments like CORBA. Each of these middleware tools came with their own interface
description languages. And also the Java language, which is an object oriented program-
ming language, has a subset which is used to describe interfaces when you work with
remote method invocation (RMI). This was very popular in the 90s.
Based on their names, you can see who is behind these technologies: what does the M
in front of the interface description language stands for? Do you know which company
implemented this technology?
You might have heard about it even if it’s really ancient history: the M stands for Mi-
crosoft. Yes, good guess. So this was a Microsoft technology. Java was meant as an
open alternative: an attempt to make ”platform independent“ software portable beyond
Microsoft platforms. Then we had also this other group of middleware vendors which
standardize around the CORBA OMG standards. While CORBA and Java did become in-
teroperable at some point, there was never any interest from Microsoft to make their
distributed object technology compatible with the one of the rest of the world.
So there was always this gap, this fragmentation in the market for software compo-
nents, with interfaces that are either of these kind or that kind but there is no way that
we can make them talk together.
Then Web services came out: the HTTP protocol and the XML representation syntax
were used to solve the interoperability problem within distributed systems across the
Internet. It took more than one decade and then finally here, you actually had widespread
industry support for the WS-* standards, and you had both Microsoft implementations,
and non Microsoft implementations, so that was the big step forward.
So we had the web service description language (WSDL) used to describe how to com-
pose XML messages that could be exchanged between different remote APIs. A very in-
teresting feature of this language, thanks to the fact that it was XML based, was that this
language was independent of the programming language used to implement the service
component on the server side and also the client program that would invoke it.
That was the time when XML was the most popular, so hyped that it took many years
before people realized that XML was actually rather inefficient to use it as a serialization
format to transfer data between different network hosts and mostly performance will
be spent parsing complex XML documents, which tend to also have a low information
density leading to larger message payloads.
Slowly people started to realize that it was possible to actually solve the same problem
by using other – still text-based – formats, like JSON, or other binary representations like
protocol buffers.
The languages for describing interfaces changed accordingly, from the web service de-
298

scription language (WSDL) to the web application description language (WADL), which
was not so widely adopted.
While the underlying HTTP foundation never shifted, there were other attempts and
iterations (RAML, Swagger) to describe HTTP-based or sometime also RESTful APIs lead-
ing to the current OpenAPI standard. We are now in the 2020s, but we still depend on
HTTP port 80 being open by default through firewalls to transfer data and remotely access
our software components delivered as a service.
299

Java/RMI
import java.rmi.*;  
public interface Adder extends Remote
{ 
    public int sum(int x, int y) throws RemoteException;
}
Let’s look at some examples.
Here is a description for a Java remote method invocation (RMI) interface. This is ac-
tually a piece of Java code. You know that this is an interface description because there is
a specific language keyword: interface. You know that this is a special kind of interface
that is remotely accessible because you extend this marker interface called Remote and
you import the RMI package, which will give you access to the Remote and the Remote
exception.
Why is it important to include this boilerplate code? While if you call such a method
on a local object, you would expect that such call will never fail, the moment that you
try to do this across the network, you have a lot of problems that can happen, and so
you want this information to be visible in the code. Warning: During the call, there is
network communication going on and things may go wrong, so you have this exception
which may occur.
If you read the content of the method signature, this is a classical interface descrip-
tion for a typed language: You need to specify names and types for all operations and
parameters.
300

C/RPC
Parameter direction: [in]  Input, [out]  Output, 
[in, out]  Input and Output
interface adder {  
  const long ARRAY_SIZE = 10;  
  typedef long array_long[ARRAY_SIZE];  
  long sum([in] long a, [in] long b);  
  void array_sum([in] array_long a, 
                 [in] array_long b,  
                 [out] array_long c);
}
Let’s look at something more ancient, more exotic. This is the original RPC IDL which
was used for describing calls to remote procedures, mostly implemented in C.
The syntax it is also similar to C, with a few extensions, starting from the keyword
interface, found in prominent position.
Interfaces have names, so you can tell which server is offering which set of calls. Inside
the interface we have type definitions. So that’s about describing the data that we can
work with, and then we also have operations, which look like function signatures, but
there is no implementation like what you would find in a header file.
One function adds two long parameters. The second can process arrays; because of
the intricacies of the C language where arrays are passed with pointers, when you trans-
fer data across the network you cannot just send the pointer. This interface definition
language was trying to abstract how the data will be copied around and where it will be
stored.
In addition to names and types, you can use annotations for describing the direction
of the transfer of the data between the remote function that implements the operation
and the client. So in the first case this is a normal function. The parameters behave
as expected. We pass the input and the result comes out from the return value of the
function.
In the second case, since we work with arrays, the only way to get the output was to pass
two array parameters as input and use the third parameter to receive the output array.
This is kind of unusual or unexpected to call a function, but only the first 2 parameters
need to be set while the third one is going to contain the result after the function returns.
So this a simple example of the origin of a whole family of interfaces description lan-
guages: we can already recognize the need for describing data types, and in this case we
enumerate different operations. We have also this concept to describe in which direction
we exchange information.
301

RAML
/adder: 
    get: 
        description: Add two values 
        queryParameters: 
           a:  
              type: number 
              required: true 
              example: 40 
           b: 
              type: number 
              required: true 
              example: 2 
        responses: 
           200: 
             body: 
               application/json: 
                example: | 
                   { 
                     "result": 42 
                     "success": true, 
                     "status": 200 
                   }
Let’s fast forward to this century. Here we can see how we use HTTP to access the same
interface.
This is an example of a language not only used to give us machine processable infor-
mation (for a compiler to read), but also some human readable description to express
goal of interface. These interface description languages were born from the need to gen-
erate documentation for the interface: describing its features for developers attempting
to first understand what it does and second how to use it. As opposed to just reading the
Java-like, or C-like code, like with previous older attempts, here developers read color-
ful, interactive HTML documentation, which is generated from the model of the interface
represented in RAML, Swagger or OpenAPI.
We can see in the example how URLs are used as an identifier for the different op-
erations of the interface, so we can distinguish them because we have different links,
different Web addresses pointing to each of them.
Since we communicate over HTTP underneath, we need to describe how to send a re-
quest with the GET method. Since we want to add 2 values, we need to pass these param-
eters, which have names and types. Here we also note that they are required. It’s also
possible to have optional parameters, for which you should provide sensible defaults.
Another useful technique to help developers understanding how to use the interface is
to provide a concrete usage example: try to call it with these values and see what happens.
This could also be considered as a simple test case, to make sure that given this input you
get the expected output.
We also have information about the response, which will carry status code 200. And
it will contain a certain body payload representing the result using JSON as a data for-
mat. And here is a concrete example of the object, which contains ther result, but can
also be used to represent error codes in case something went wrong during the remote
interaction.
So this is an example where the networking world comes into contact with the software
interface concept. Network protocols are used to transfer information back and forth. We
302

reuse this ability to encode and transfer information so that we can process it with our
code and not just use HTTP to download the pages of a website for people to read, but in
this case we want to use HTTP to be able to compute a function remotely on the server
and – for example – add 2 numbers. It is a bit overkill to compute an addition remotely
on a remote server in the first place, no matter whether using HTTP or some other more
efficient protocol.
But this is a general problem of distributed component technology, it has become so
easy to split your architecture into multiple components and distribute them around the
Internet. Sometimes we get carried away and decide that it is actually a good idea to
make a remote call to add two numbers.
303

OpenAPI/Swagger
/
cost/
drivers/
eta/
profile/
rides/
ridetypes/
{id}/
cancel/
destination/
rating/
receipt/
swagger.yaml
 
 
Let’s look at a more complex and meaningful interface example. The OpenAPI code is
available at the link, and you will find that it uses the latest fashion, the YAML syntax.
This is a more complex interface which we simplify and visualize on the slide. The
tree visualizes the resource addresses, and their corresponding HTTP methods. If you
want to get a ride from this particular provider, you can get the cost for the ride. Why
do you need to use get? How much does it cost to go from here to there? These are
geographic locations, modeled with planetary coordinates: latitude and longitude. Given
these parameters, the result of the get method will return an estimate for the cost. The
actual content of the response payload is defined in the data model for the interface and
only referred to from the operation definition.
Other operations return the list of nearby drivers together with an estimation of – given
where you are – how quickly somebody can come and pick you up.
If you’re interested, you can scroll through the YAML code and you can get an idea of
how to work with this particular API. This is just a summary as a visualization that shows
you the structure of the paths that you can access, or the various features of the interface,
which in this case are not a flat list of operations, but they’re structured, nested along a
tree.
Regarding drivers, the estimation for how long it takes them to arrive, the information
about our profile is all read-only information. The only method that we could apply was
GET. This part of the interface give you access to read information, but you cannot change
it. It makes sense, because based on where you are you want to know how long it takes
for somebody to pick you up: this is not going to change the state of the system behind
the interface.
However, if you book a ride, that’s where it gets interesting, because you have both the
possibility to access the list of the rides that you’ve done in the past, but there is also
a POST method. This is something that allows you to place an order for a trip: given
304

this origin and destination locations, I want to book the trip. That’s when you create and
modify the state of the component, which is represented as a collection of rides; inside
the rise you have their IDs. This gives you access to each ride individually, so that you can
get detailed information, You can also get a receipt (that’s also a get method - a read only
operation). But we can also give a rating for the drivers. So this would be a modification
that we do on the content. Like if we want to update the destination or cancel the ride
altogether. For some reason they didn’t use a DELETE, they use a POST method again.
Given the model of an interface specified using OpenAPI, it is possible to analyze it
and summarize it using visualizations like this one, which can be very useful if there are
hundreds of resources like in some of the largest interfaces that you can find if you search
for more examples of OpenAPI descriptions.
305

Working With IDL
Interface 
definition 
(IDL)
IDL Compiler
Client code
Server code
client runtime environment
client stub
generated
generated 
code
code
generated
generated 
code
code
server stub
local 
interface
local 
interface
development tools
server runtime environment
server 
connector
client 
connector
Remote Protocol
Client
Server
network
Once you have such detailed description of an interface, what can you do with it? On
the one hand, you can generate documentation for people that need to understand how
they can use your component. Having documentation, examples and getting started tu-
torials is key for them to learn how to use your component.
On the other hand, describing the interface using such a precise language makes it
possible to actually generate code from such a specification with a compiler that can
translate the interface definition in something that helps you to develop the implemen-
tation of the component on the server side. But it can also help you to write code which
can call the interface on the client side.
The blue part is actually a component that is automatically generated by what you
can call the IDL compiler or a model to code transformation. It takes the model of the
interface as input and produces the code that will help you deal with the remote com-
munication, code which abstracts away the fact that the two components are deployed
across different containers so that the code you write in the White Boxes can pretend to
call a local interface.
The actual distribution is managed by the middleware infrastructure so you don’t have
to solve the problems of: How to transfer arbitrary pieces of data from between two com-
puters? How to process multiple incoming concurrent requests? How to guarantee that
data is not lost in transit? When you deploy components in a distributed setting, this is
all infrastructure that will do this for you.
As long as you can describe the interface precisely enough, these issues are taken care
of. When your client code makes a local invocation, the generated code is going to take
the input, transfer it across the network. The server will deserialize it. Pass it on to your
implementation, and then when the server-side implementation has a response ready,
the same will happen on the way back.
We have seen a very important application of the concept of modeling interfaces, which
306

is of great practical help as we deploy components in a distributed execution environ-
ment.
API Documentation Demo Videos
• How is the API documentation structured?
˾. Show an example of an existing API and go over its main elements.
˿. Which kind of data model or schema information is included with the interface
description?
̀. Which kind of extra-functional requirements can be described?
́. Is there some versioning metadata?
̂. Is there some API repository where such descriptions can be found?
• Given an API speciѹcation:
˾. How to generate a developer website?
˿. How to generate client code to invoke the API?
̀. How to generate tests?
́. How to generate an implementation stub for the API?
̂. How to compare different versions of the API?
307

What is an API?
API
Application
Component
After introducing interfaces in general, now we focus on one special kind of interface
which are so called application programming interfaces (APIs).
When you design your architecture, you have many components connected together:
precisely the interface encapsulating and abstracting the components are connected.
The interface provided by one component can satisfy what the required interface of the
other component needs.
For one architecture there will be one particular kind of interface, which allows to con-
nect the system with the rest of the world, across the overall boundary of the context
view.
While every component has an interface, every architecture has a special interface
called the API, which opens up the architecture and makes it possible to connect it with
other systems that somebody else has developed at a different time and place.
Also, when you have a platform, when you have an operating system, or some pro-
grammable infrastructure software which can run different applications developed by
someone else on top, the interface between the platform and the applications is also
called the application programming interface (API).
What is an API? It’s an interface, yes, but is an interface that has a very precise purpose
which is to allow external developers to build applications on top of the interface.
You can see the API as a slice through your architecture which separates the general
part from the specific part. The API separates the part that you have to implement it
behind the API from the part that somebody else can implement for you.
If you are going to launch a Web-based service, you typically are responsible for imple-
menting the server side. But if you choose to open up your service with an API into your
system, then you can let others build the clients for you, while keeping full control of the
interface you provide. So you can wait and see which clients become more successful,
which ones have the best user interface, which ones gain more traction with a growing
308

user community. And then you can buy them. And through the API, of course you have
one point into your architecture, which will make it possible to talk to you system, but
you also make it possible for you to control who is talking to your system, check whether
they are allowed to do so, and extract rent from your position.
309

Rule of Threes (Will Tracz)
Is it really API?
1
2
3
• You know you have designed a reusable API only after at least
three applications have been built on top of it
• A component interface with only one known client does not
qualify as a reusable API
Does any interface of any component in your architecture share these properties? Prob-
ably not. So there is this rule, called the rule of threes, which is very simple to understand.
It just says that you can claim to have designed a reusable API, if there are at least three
applications which have been independently built on top of it.
You can always have a component which provides an interface that is used by one other
component within your architecture. That’s already enough to call it an interface because
it abstracts the implementation of the component and makes it possible for the other
component to access it.
But if there is only one known client, this doesn’t make it a reusable API. If we want to
have a claim that this is a reusable and you have to have at least three known uses, which
represent applications built by someone else.
After all it’s much easier to define an interface if you are the user and the implementer
at the same time. It’s much more difficult to do so while opening up your system so that
others can use an interface that you provide and build their own applications using it.
310

Many Applications
Application
API
Component
APIs thus are not found in all architectures, but only in those designed to be open and
stable platforms supporting externally developed components and applications.
The API is key to make this possible, as it opens up the underlying platform compo-
nents to the rest of the world, while making the developers that build application more
productive, since they can reuse what the platform provides them with.
Because they don’t have to solve the hard problems that you solve for them inside the
component, they just use the API and can benefit from your efforts.
311

Steve Ballmer
Developers, Developers, Developers
More
applications
More
users
More
developers
Platform
API
More $
So we have many applications built on top of an API: but where do the application
come from?
As Steve Ballmer once said: Developers, developers, developers, developers, develop-
ers. He was in a conference room crowded with developers, they are all developers for
his API: their job is to build applications for it.
If you want to be successful with your platform and you have defined a ”good API“ for
it, you will be interested to start this positive feedback loop.
The more applications, the more users will use them, the more developers will be in-
terested to write more applications, for more users.
What is the incentive? Somewhere in this loop, the more people you have using the
applications on top of certain technological platform, the more money you can make with
it.
Developers will be able to sell applications to users and at some point along the way
the platform will take a cut – like the 30% Apple tax.
Users will pay to install the platform, will pay to buy a license for the apps. Again, the
more apps you have, the more user will come, the more money you will make, both as a
platform provider and maintainer of the API, as well as a humble application developer,
both enabled and supported but also limited by what the API can do.
312

Where to ﬁnd APIs?
• Operating Systems
• Programming Language Runtimes
• Hardware Access
• User Interfaces
• Databases
• Web and Cloud Services
There are many examples of APIs and many examples of different kinds of APIs, which
depend on the type of platform or underlying system they provide access for.
We will see APIs of many different categories. For example, we have local operating
system APIs, or standard programming language libraries, as well as distributed and re-
mote APIs to interact with databases or Web service providers.
313

Operating Systems
• Win16, 
, Win64
• POSIX
• Cocoa (OS/X)
• android.os
Win32
 
 
Why is it important to standardize the API of an operating system? What property do
you get for your applications if the operating system has a standard API?
You can write multi-platform applications which are portable both across different op-
erating systems, but most important across different version of the same OS.
Consider the historical backwards compatibility of Windows. Within the 64bit API,
somewhere you can still find a copy of the Win32 and within Win32 there is a corner
where Win16 is still there. This is a possible way to provide backwards compatibility so
that old applications still work in the newer version of the operating system.
Standard APIs also give consistency because different implementations will work in
the same way. If this is not the case, every OS upgrade becomes risky. Your applications
may stop working or need to be reinstalled or recompiled if the API of the OS changes
and the old apps are no longer compatible.
314

Programming Languages
Standard libraries of any language runtime
• C #include <stdlib.h>
• Java Platform API (SE, EE)
• libstdc++
• Python Standard Library
• .NET Framework class library
Hardware Access
Abstractions to program any kind of hardware device
• Graphics (
, WebGL, OpenCL, CUDA)
• Network (NDIS)
• Printers (CUPS)
• Device Drivers (WDF, I/O Kit)
OpenGL
 
 
The goal of these hardware APIs is to give developers an abstraction so they can pro-
gram applications for a certain hardware device category without worrying about the low
level details.
It would be too expensive to write applications embedding knowledge and assumptions
about every possible concrete hardware device.
APIs make it possible to decouple applications from devices. Device drivers will map
the API primitives to the specific device implementation.
315

User Interfaces
Widgets, Gadgets, Controls
• Tcl/Tk, Qt, GTK+
• Java Swing, AWT, SWT
• Windows MFC, WPF
• JavaScript HTML5 APIs
Databases
Standardized database access
• JDBC
• ODBC
• PHP PDO
Why it’s important also here to access database through standard APIs? To keep state-
ful components independent of the specific database implementation which will store
their state.
As long as it’s a SQL database, you can use these API to send the queries in, retrieve
back their results and in general access the content of the database, no matter which
one it is. The example shows that while relational databases have standardized their
structured query language (SQL) since a long time, this was not enough to decouple them
from the applications, until APIs such as JDBC or ODBC focusing on the mechanism used
to perform the actual interaction with the database were introduced.
Without these standards APIs, every database that you select will be tightly coupled
into your component into your system and it will become impossible to switch the database
from one type of product to another, because every database will have their own specific
interface. This is currently the case for noSQL databases.
316

Web Services
Remote access through standardized protocols:
• SOAP/WSDL Services
• REST/Hypermedia Services
• JSON-RPC/HTTP Services
Examples: Google, Amazon WS, Facebook Graph, Twitter Firehose,
Salesforce
Web service APIs are used to access remote components across the Internet (or the
Web).
They make it possible to access these cloud based services like Google, Twitter, Face-
book, Flickr, Amazon. (And many, many others).
They all use standard protocols so that it is possible for them to interact with client ap-
plications without having to re-implement the client for every possible communication
stack and for every different provider that you want to interact with.
It turns out that the HTTP protocol is the default choice, given it is easy to use program-
matically and on top of it, people have been building, for example, the ability to perform
remote procedure calls (RPC) using JSON payloads, or to query remote data source using
GraphQL.
Since the good old XML days, which saw the rise of the WS-* standards, now they are
no longer very popular but still work in case you need to describe and invoke ”legacy”
Web services.
And yes, somewhere there are also those truly RESTful, hypermedia APIs, which most
claim to be, but still very few understand how to correctly design or even what their pur-
pose actually is.
317

I would like to spend the rest of this lecture to cover the topic of how do we design a
good API. What kind of design principles can help us to make the right decisions? What
are the options that we should consider when we need to design an API?
This is applicable as well to any kind of interface that you design, but since we said that
the API is the integration point between your system and the rest of the world, it is also
a critical part of the architecture that you have to get right.
The goal is to make it easy for as many clients as possible to interact with your system.
318

Where is the API?
API
App
App
App
Platform
Before we start, let me just show you a logical view with some anonymous components.
Try to point out which of these is the one that plays the role of the API.
The API seems to be the one in the middle, the one through which all the other inter-
action have to go.
If we refine the design and for example, on top we have many components and at the
bottom we have a single component. And if we open up the single component, the API
is the one which exposes its internal implementation components towards the external
ones.
Whenever you have this type of graph topology of your logical dependencies between
components, the API should be easy to spot, even if it may not be yet explicitly called
API.
This topology is also the natural result of applying the Strangler pattern, where you
want to take a system in which there are a lot of dependencies, with components con-
nected to many other ones and you gradually try to limit and control the connectivity by
making all dependencies go through a dedicated separation point. For example, that’s
the place in which you can check, for example, whether the external applications are
authorized to talk to your component.
This is also the point in which you abstract the complexity of the internal implemen-
tation to make your platform easy to access and easy to use for the applications, while
keeping them isolated from its internal implementation components.
319

API Design: Where to start?
Application
API
Component
• The API should be designed
from the perspective of the
application developers using it
• Do not leak into the design of
the API details from the existing
component implementation
Here’s your API: the main interface between your architecture and the rest of the world,
the platform on top of which they built (or will hopefully build) all the applications.
So the other question I want to ask you is: if you are starting to design your API, and
you are in this situation, cutting the system into two parts, where should you start from?
Which side: you have your component, you decide what do you expose in the API. Or:
you have your application that needs to access certain functionality and then you can use
its required interface as the requirements to design your API.
Everyone agrees you should not start from the implementation. You do not want to
leak implementation details and expose them to the interface.
Where you want to start from is the outside. You start from the application or in general
you start from what the application would need for the API to provide.
You check your API design and evaluate it against the requirement of the applications
which will need to use it. Once the AI design is sufficiently complete, we build applica-
tions with it. And then we can go and implement the underlying components, or map
the API onto an existing implementation.
Outside in: never just take a component, click a button and make its private imple-
mentation accessible as its public interface.
The reason why this is not a good idea is that you want, after all, your interface to be an
abstraction: you want the interface to hide the implementation of the component. You
do not want the interface to be directly connected with the implementation.
If you take the application developers perspective, your API will be easier for them to
use.
Who cares about how it’s implemented? You can even switch the implementation and
if the API is good enough, it should not change and – as an added benefit – the application
shouldn’t change.
That’s how you can also keep the backwards compatibility when you evolve your plat-
form – for example, you change the operating system version, refactor its implementa-
tion – never touch the API and your apps will keep working unaffected.
320

90
9
0.9
Who to please?
• 90% of the clients should immediately be able to use the API
(which call?)
• 9% of the clients can use the API with some effort (which
example?)
• 0.9% of the clients manage to mis-use the API to do something
unexpected (which hack?)
Note: Cannot always satisfy all clients
We need to start from the needs of the application, from the perspective of your cus-
tomers. They want to build something on top of your platform. You start getting a lot of
requirements. You have a lot of potential users that need to build many different kinds
of applications. Can you really make everybody happy?
Well, the rule is that 90% of the clients should be able to use the API immediately.
What does it mean? They find the right operation, the exact events, the precise property,
which they can use to do what they need.
Maybe they need to look for it, but once they find, it is there and there they are fully
satisfied.
A smaller group will need a little bit more effort to figure out how to use the API. They
need to find a more complicated example or figure out how to combine different opera-
tions so they can achieve their goal.
And then an even smaller subset, will need to hack your API to satisfy their needs. They
will to do something that from your perspective as a platform provider, is completely
unexpected.
Overall the API shouldn’t be too strict. Always leave some flexibility so that people
can still solve their problems with it, even though you didn’t anticipate those use case
scenarios, but this is typically only true for a very small set of clients.
If they need to hack a solution on top of the API, maybe the API is not exactly designed
for that so they’re not probably so satisfied with what you offer, since they need to misuse
the API to get results and will always feel that their hack may suddenly stop working.
The message is that if you have so many clients and everybody has different require-
ments, it would be impossible for you to satisfy everybody. So you need to prioritize.
This is the same thing that we did when we had different use case scenarios in the do-
321

main model. We had many use case scenarios and we had to prioritize them, starting
from the ones that are more common, so that by supporting them with your API, you can
make the highest number of clients happy.
Reusable Interfaces
Self-contained 
components 
with minimal 
dependencies 
are the most 
reusable
Dependencies
Dependencies
Reusability
Reusability
Let’s look at the required interface. Which is the component that is most reusable? Do
you pick the component that is on the right, or on the left?
The component on the left has a lot of Dependencies, while the other on the right
doesn’t have so many dependencies.
Increasing reusability is about minimizing those dependencies making the component
as self contained as possible.
This is a also true if you build an API, if you have a platform, if you have a program-
ming language, people should be able to build applications on top of it right away without
having to install additional components and worry about satisfying too many extra de-
pendencies.
When you are trying out a new platform, a new language or a new tool, what is the
time it takes to go from zero to hello world? how many times do you need to fetch a
missing package? or look up some cryptic error message to find a workaround? or does
everything works smoothly the very first time you try it?
322

Ian Sommerville
Usability vs. Reusability
Usability
Usability
Reusability
Reusability
• A general interface helps to reuse a component in many
architectures
• A general interface is more complex and difѹcult to use in a
single architecture
Back from the provided interface perspective, there is another important tradeoff to
realize between usability and reusability depending on how general is your API design.
Since we need to satisfy the needs of many clients: how usable is your API for the
specific client that needs to use it in a certain context?
So to visualize this with a concrete example, you can look at these power sockets. When
you travel around the world you typically have this problem of carrying adapters because
different places will have different power sockets. Unless you go into one of these hotels
or conference venues which offer you this type of plugs which can accommodate many
different types of connectors. So this is a general ” power charging API“ in which you can
plug in any kind of cable and it will give you the power that you need.
These other sockets are much more restricted: they only work within a certain type
of plug. But if the plug exactly matches, for that specific use case, they will actually be
easier to use. There is no doubt whether it will work or not or how to connect the two
elements.
While a more general API supports many more clients, the problem is that for each
specific client it could be a bit more difficult to use than a specific API designed and
individually tailor made just for that one client.
Be careful how general your API becomes. The more general, the more difficult it will
be to use for everybody. It’s a tradeoff: you need to find a sweet spot between how many
clients you need to survive and as a consequence how general your API becomes without
making life impossible for too many clients, or they will start looking for a better fitting
alternative.
323

Easy to reuse?
Usable
append(item)
f.isDir()
div.style.color = "black";
Reusable
insert(position,item)
f.getType() == FileType.Directory
div.css("color: black");
Let’s try to see how the trade off applies with these simple code examples. Which of
the code examples is more usable for a specific scenario? Which of the code examples is
more general?
The question is: how likely we can actually reuse the same interface without changes,
when we find it a different use case?
Here we have two different ways to work with a certain data structure. We will append
an item at the end, for example of a list. Or we can insert the items in any position,
including of course at the end.
Everybody found the insertion the most general. Because after all, if you change the
position, you can put the item where you want while appending is more specific and
limited. However, if you want to put the item at the end of the array. Then you just call
’append’ and that you don’t have to worry about remembering: How do I identify the last
position? Do I need to subtract 1 from the length or not? So ’append’ is less reusable but
more usable for the specific use case.
The second pair of snippets is about working with the file system. We have an object
’f’. We ask the object whether it is true of false that it is a directory. Or we can have a
more general interface, where we ask the same object for its type and we compare the
result with the expected value of the type, which in this case would be the directory.
For the third pair, we have the ’css’ method that takes a string. The syntax of this string
follows the rules of an entire programming language, which is – in this example – CSS.
The actual CSS string is used to specify that the color of the element should be black. The
alternative example does exactly the same, but uses a simpler, object-oriented approach,
where given the page element ’div’, you select its ’style’ property, and use it to change
the ’color’ and directly set it to a certain value.
The boolean is as simple as it gets, either it is a directory, or not. This gives exactly the
answer to our question, no more no less. However, if you have a different type of files. If
324

you have a symbolic link, a named pipe, or whatever other type of file your file system API
may support, then you will need to come up with additional boolean methods for each
kind. So you should consider the other, more general solution. Give me the type of the
file and then you compare it with certain value and if you want to create a different type of
files we just have to extend the enumeration of the types, but the method to retrieve the
type does not change. The price developers using this general API have to pay consists of
writing the condition to check whether the type matches the expected one by themselves,
as opposed to embedding this condition within the API itself and simply returning the
boolean result.
It’s a trade off. And is it better to extend the set of file types that we support? Or is it
better to add one more boolean ’is’ method for every type we will ever want to support in
the API?
This is a very specific example about the file system and working with directories, but
you can see this in many different contexts. Do you want to have a boolean flag that
checks the identity or the existance of a property? Or do you want to have a more general
way too check what is the type out an extensible set of many possible values?
There is no absolute better or worse answer, you need to see from the client perspec-
tive. You need to consider how many possible values are there. Will the client developer
remember where to find the set of possible values? Is this documented somewhere? Are
you going to use an enumeration so that the compiler can catch wrong conditions? or
will any integer do, so we have plenty of space for future extensions?
The one that is most controversial seems to be the one about the CSS. So in which way
is this one more reusable than the other?
If we can pass a string (a string that of course has to be properly constructed, its syntax
has to fit with a certain language, but from the point of view of the main language, this
is just a string), then behind the API you will need to check that it has the valid syntax,
that the property that you’re trying to set is correct.
If you use the original language, it’s more direct, just set properties following along a
complex object tree structure.
However, that’s the only thing you can do: You can set one property at a time. Here in
the string version, if you want, you can pass a whole CSS file with lots and lots of property
definitions, so this is definitely much more general and powerful, much more open for
reuse but also even misuse.
Another place where we see this all the time is for example with database access, where
you can send a SQL strings through the database driver. The database driver API happily
takes any string and sends it on to the database. This is a different level of generality as
opposed to working with something like where you have an object structure that you can
access and then behind the scenes you make it persistent.
String APIs are so general that they can become security concerns: If you can sneak
through arbitrary strings to be processed elsewhere, then your API is not able anymore
to check whether these strings are allowed.
See for example SQL injection attacks, where after pretending to check user credentials
you append an extra query to drop the content of the database. All the API does is take
the string and pass it onto a separate component, which will execute it as long as it is
syntactically correct.
Like when an entire schema declaration was bolted on the database connection string
of a configuration file. The connection string is just a string, you will be surprised what
desperate clients will write in it.
325

Ian Sommerville
Performance vs. Reusability
Performance
Performance
Reusability
Reusability
• A general, reusable component may be less optimized and
provide worse performance than a speciѹc, non-reusable one
• For the same (reusable) interface, there can be multiple
implementation components optimized for different hardware
platforms/execution environments
Other tradeoffs that we have to consider when we work with reusable interfaces and
APIs concern the opportunities for improving the performance of our components.
If you make a general component, like a platform independent component, for exam-
ple, to make it reusable you may have to sacrifice its performance.
If you make a highly specific component, you can take advantage of its local envi-
ronment, by requiring or depending on specific features which improve its performance,
something that you may not be able to do if you need to keep it portable across multiple
platforms.
However, if we provide a reusable interface, then we can switch its implementation so
the clients are not affected as we improve the performance.
Having an abstract interface, does not necessarily make it problematic to get good per-
formance. It gives a constraint (the interface should not change), but behind it you can
freely rewrite and optimize the implementation. For example, each implementation can
use different processor instruction sets. You can have GPU-specific implementation, or
an implementation which can take advantage of solid state disks.
There are after all Web servers running inside light bulbs, and some of the side channels
exploiting processor speculation were written in JavaScript.
Sometimes if you worry too much about performance, you think you should not make
your API too abstract because you will lose the ability for your clients to access important
features of the underlying platform. There may be more layers of indirection and there-
fore performance will get worse. But the good thing about having an interface is that you
have a clear constraint, so you can focus your effort on optimize the performance of the
implementation behind the interface. And the clients will see a benefit without having
to change because the interface stays the same.
326

Bertrand Meyer
Small Interfaces Principle
• If two components communicate, they exchange as little
information as possible
As part of these API design discussion, I also want to introduce various principles that
you should keep into account as you consider different alternatives for the design of your
API.
The small interface principles recommends that every thing else being equal, pick the
smallest interface.
If two components communicate through one interface, they should exchange as little
information as possible. So here we see two components connected. Here we have the
provided interface. Here we have the required interface. They are connected together.
Here we have the same situation, they’re connected, but this provided interface is big-
ger than it should be. Here we have a component that offers us a lot. It’s interface is
very, very expressive, very powerful, but we just use a small subset of that. The rest are
a smorgasbord of features that are provided to us, but we don’t really need them.
In this situation, the interface isn’t as small as it should be.
A variation for this principle holds for programming language design as well: your
language should be as expressive as necessary, but no more. Not all use case scenarios
or application domains for a language require Turing completeness.
327

How many clients can these APIs satisfy?
Few
getTotalAmountofFirstActiveOrderofCustomer(id)
Many
getCustomer(id).getOrders().filter('active')
[0].getTotalAmount()
getOrdersforCustomer(id,'active',{totalAmount})[0]
To reflect about the small interfaces principle, I wanted to show you these examples.
These are interfaces shown from the point of view of the client using them. I don’t have
room in the slide to give you a complete specification of the interface.
The use case is the following: there is a client that needs to retrieve the total amount
of the first active order of certain customers.
While this is just one use case, as far as you know, and that’s the only use case that you
have regarding how you expect your clients to use the API.
When you design the API to make it possible for the client to get this information out
of your system, you have these three options.
The three designs are not the same when it comes to satisfying potential use cases of
other clients. Maybe there are other clients, who will also need to access the orders of a
customer, but maybe they don’t want to see the active orders they want to see the orders
that are already paid. Maybe they’re not interested in the 1st order, they want to read the
2nd order.
Some of these designs will make it possible to access the other information. Others are
very narrowly focused on the needs of the original client.
In particular the ’getOrdersforCustomer’ is meant to mimic something called graphQL:
as part of your request to the API, you describe the shape of the object that should be
returned.
This other design uses a single shot function, which has a very long name: its result
gives exactly the total amounts of the first active order of the given customer. One perfect
call doing really a lot of stuff for you, so that you get back exactly what you want. But
that’s probably the only thing that it does. If you need to get some other property of the
order, there’s no way to read them, this only gives you the total amount. If you want to
get the 2nd order, sorry this returns only the first.
Now if we go to the other side, this is a more traditional design: we start from the
328

customer ID, we use it to retrieve the customer for that particular ID. Then this object
will give us access to a collection of orders which we can filter. Because we need to just
get the active ones. Then we take the first and finally we can retrieve the total amount.
With this solution we leave the door open for retrieving other information about the
customers: we could get their address, we can fetch a collection of orders that can be
filtered by many other criteria. And once we have an order object, we can get the total
amount, but also we can get a list of the items that have been ordered. We can get a lot
of different information about the order.
What is a drawback of using this solution for the API design? How many calls do you
need to make to the API if you want to get to your specific result?
What’s the minimum: one call. What’s the smallest result: one value, the total. What’s
the maximum: four calls. We start from the customer’s ID, then we get the orders. Then
we do a filter (this could be a local operation), but still more work to do and then we get
the order’s total amount. So there’s four steps. And every time you do one of these calls,
there may be some remote interaction, some data to be downloaded, possibly encrypted,
overall it can be rather inefficient. Think about the filtering: why should we retrieve the
entire order collection, if we need to pop the first item only?
This also an important trade off when you design an API. Do you want your clients to
access the whole data collection and then they are responsible for filtering it. Or, since
the data collection is huge. And they just want to filter out a very small subset, like – for
example – 1are active and the rest is all the history. So why should you have to download
the whole history of the orders of this customer that has been with us for the past 20 years,
when you just want to see the last active order. Maybe this is also relatively inefficient as
a solution, and you want to perform the selection and projection of the data behind the
API.
Another strategy would be to make the API less general, so in this case we want to get
the orders for a customer in one step. This would take the customer ID as parameter and a
filter condition for identifying which subset of the orders to retrieve. And also we want to
have the total amount which is basically something that will project the result and limit
its content. According to the API data model, an order can contain a lot of information,
but we just want to read this part.
So here we have the best of both worlds, We have one call featuring server-side filtering
both in terms of selection (the active orders) and projection (the total amount). Still,
the API design is flexible, as we can change the order attributes as well as the selection
criteria.
This filtering of the first element is something that we still do on the client side. We
could also have a paginated result, where we start from the first result and incrementally
retrieve the rest of the elements one by one or page by page. This can help to avoid down-
loading upfront lots of unnecessary data. That could be a possible design compromise if
the collection is too large.
I hope this example was interesting to reflect about the different options that we have
when we design an API, we can make it super specific and make one particular kind of
customer very happy because they find exactly what they need. But maybe this is too
specific, so no other client will be able to use the interface.
If we need to come up with these type of operations for every possible combination of
queries that we want, we want to try to generalize the API and make it more complicated
to use for a specific case because you need to know which parameters you have to pass,
which are the valid filters and how to further process the result. But we can still keep the
maximum number of round trip interactions and the amount of data to be transferred
under control.
329

Bertrand Meyer
Uniform Access Principle
• Facilities managed by a component are accessible to its clients
in the same way whether implemented by computation or by
storage
• Helps to introduce caching or memoization without affecting
clients
Another API design principle is about the abstraction of the interface with respect
the implementation of the component. This is the uniform access principle: no mat-
ter whether functionality is implemented by computation or storage, it should be made
accessible in the same way to its clients.
Why would you want to be able to do so? Well, the clients do not have to know if the
result of their requests is computed on the fly or if it is actually stored somewhere, and
you just retrieve the information that corresponds to their input, and you give it back.
If clients are unaware, then you have the opportunity – behind the interface – to do
interesting time vs. space trade-offs. For example, let’s make your component faster by
caching, or doing memoization of the results, as opposed to wasting time, CPU cycles
and melting the polar ice caps by recomputing the same results every time.
The result given to the client will not change, the way used to interact with the interface
will be the same, although the performance may improve, so the client may be able to tell
if the first time a request is sent the result is slow, and the next time the same result is
much faster.
From the implementation point of view, you have the opportunity to decide whether,
for example, some requests arrive so frequently that it makes sense to save CPU by stor-
ing the results and returning the same precomputed result to multiple clients. Is there
enough storage to do that? Maybe you don’t have enough storage so you have to recom-
pute certain results which are not so frequently requested.
If the interface remains the same, you have the opportunity to optimize the imple-
mentation performance. One way to do so is by recycling previously computed results (if
enough storage space is available).
If you violate this principle and only later you discover that it may have been a good
idea to add a cache somewhere between your growing clients and your limited capacity
server, then you will need to change and refactor your clients before the architecture can
scale to absorb more traffic.
330

Bertrand Meyer
Few Interfaces Principle
• Every component communicates with as few others as possible
Another principle has to do with the amount of interconnections found between dif-
ferent interfaces.
While interface isolate the impact of changes applied to their underlying implementa-
tion, sometimes interfaces themselves need to change.
Imagine a nice layered architecture where every component only talks to its neighbors.
So if we look at one component, this component is connected with the one in front of
it, but also with the one behind it. And this component here does not see the effect
of changes done to any component behind it. All of these components can be changed
without affecting this component.
If you don’t follow this principle, like in the figure, if you allow all components to freely
communicate with whoever they want as much as they want, then you will not be able
to control and limit the impact of changes. Every interface you evolve will potentially
impact every other component, which may depend on it: every change will propagate
and ripple across the entire architecture.
331

Bertrand Meyer
Clear Interfaces Principle
• Do not publish interfaces with useless or unused functionality
• Design interfaces that are needed and actually used by other
components
The clear interface principle recommends to avoid publishing useless or unused func-
tionality in interfaces.
This follows what we discussed before: if a client needs it, it has to be there. But try
not to over-generalize your design to support hypothetical clients which may never be
there, or publish features within interfaces ”just in case”. Maybe in the future somebody
will need it. Now, however nobody does. And congratulations, your API just got more
complicated with no immediate benefit.
It has to be needed right now by some actual clients: then you can publish it in the
interface.
You can see in the figure some of these provided interfaces are used by another compo-
nent. However, some interfaces are there, but there is a problem: there is no component
for them.
Why should you spend the effort to design, document, publish, operate and maintain
an API, if nobody is using it?
They just make the life of who is trying to connect to you more complicated: to find
the point in which they can connect, the feature they need to use, they have to consider
all of these other interfaces which nobody else is using.
While we can pay the price of supporting multiple actual clients and increase the size
and complexity of an API as a consequence of its success, there is no reason to make
the API more complex if this cost does not get offset against clients benefiting from the
additional features, which may clutter the API design.
Other formulations of the clear interfaces principle include:
• If in doubt, leave it out.
• YAGNI: You ain’t gonna need it.
• DRY: do not repeat yourself.
The first means that if you’re not sure whether something should be published as part
of an API. Don’t worry, you can always add it later, but if you add it you have to be sure
about it, because it will be difficult to remove it later.
Should your car dashboard look like an airplane cockpit? That would be so cool, how-
ever: how much extra training do you need to become a pilot as opposed to getting a
drivers licence? How much time do you need to spend in a simulator to learn how to
use this interface? So when you design your own interfaces for your own software, be
332

careful not to end up in this situation where the developers that need to learn how to use
your API would have to go through three years of training to understand all the options
on how to use all of your operations. Chances are that they are not going to need all of
them. And if you are doubting that, whether you will need it or not, leave it out.
The DRY principle is about removing redundancy in your API, make sure that your
interface stays as simple as possible, drop repetitions, only give one easy way to get the
same thing done.
333

Let's create a new Window
HWND CreateWindowExA( 
  DWORD     dwExStyle, 
  LPCSTR    lpClassName, 
  LPCSTR    lpWindowName, 
  DWORD     dwStyle, 
  int       X, 
  int       Y, 
  int       nWidth, 
  int       nHeight, 
  HWND      hWndParent, 
  HMENU     hMenu, 
  HINSTANCE hInstance, 
  LPVOID    lpParam 
);
Microsoft Win32
NSWindow init(contentRect: NSRect,  
styleMask style: NSWindow.StyleMask,  
backing backingStoreType: NSWindow.BackingStoreType,  
defer flag: Bool)
Apple Appkit
JFrame(String title, GraphicsConfiguration gc)
Java Swing
window.open(url, windowName, [windowFeatures]);
Web Platform API
QWindow(QScreen *targetScreen = nullptr)
QT
public Form ();
Microsoft .NET Forms
 
 
 
 
 
 
 
 
 
 
 
 
I want to show you a historical perspective on API design. Let’s look at this example
from the user interface domain: we need to be able to create windows to display infor-
mation on the screen.
Let’s go back in time when people were actually writing this in C. You call the ’Cre-
ateWindowExA’ operation: What do you get as a result? a handle to the window, like a
pointer to the object representing the window that later you can of course manipulate.
For example, you open the window and then you can close it.
To open a window, what information do you need to pass? What is the difference be-
tween the style and the extended style? Then we have the class name of the window. We
have the name of the window itself. These are strings. A window is a visible object on
your screen with a geometrical shape: so we have some coordinates. Where do we put it?
how big it is?
Windows are also related to each other, so every application has many windows open
so we should identify the main window. Windows have a menu, they belong to a given
process. And there are also additional generic parameters that you may want to pass just
in case. So this is for future extensibility, so we do not have to change all the existing
applications in case we want to add parameters in the future.
It is not so uncommon to have so many parameters in this type of APIs. We can track
over time how this number evolves. This particular API is also influenced by the choice of
the programming language, and you can also see a very strong impact of a certain naming
convention that was used at the time.
Let’s look at the competition from Apple. This had been more recent as an API, but
it does the same thing. You can recognize the style, but here we have a dedicated type
that contains the style mask. This benefits from the higher level of abstraction of the
programming language. With the previous example, in C, we need to work with bits, and
334

bitwise operators to compose these values. Here we just have a data type that is called
style mask.
These four parameters X, Y, width and height have been compacted inside a rectangle,
so that’s also an attempt to raise the abstraction, to make it simpler. And everything else
has disappeared: We just have these these two parameters about how the windows can
be drawn and the option to defer the actual creation of the window on the screen until
later.
Let’s go multiplatform with Java. Definitely we have a much more abstract solution.
We just pass the title for the window. This will be similar to the name. And there is an op-
tional object which contains some options for configuring the window, which resembles
the style that you use in the other two APIs.
Have ever opened a pop up browser window in JavaScript? This is also very simple.
You just write ’window.open’, pass the link to the page that you want to open. This will
be used to control what’s inside the window. And then you give it a name and then you
can pass an open-ended set of other parameters, for example, to control the positioning
or other features of the window.
This is probably the only case in which you just make one call and can obtain a fully
functional, fully populated Window, since you pass a link to retrieve its entire content. In
the other APIs, you have to create the object and then you have to write code to actually
construct all the display widgets, all the content inside the window.
You can see from the names that prefixes or letters are use to brand the API identifiers
and explicitly connect them to the corresponding platform.
This convention was not followed by this other graphical user interface toolkit, which
is also the most abstract. They have come a long way since the original Windows API,
here we just need to create the Form object, no need to pass any parameter, unless you
need to override their default values.
This was just an example to show you how the same concept has been designed into
APIs overtime. You can see many attempts to come up with the simplest and the most
abstract way to solve this problem across different interfaces, influenced by the program-
ming language, by previous attempts, by the conventions of the platform.
You can estimate how complicated or how easy it will be to write a program that uses
each API, with different levels of abstraction, to create Windows and display graphical
user interface objects on the screen.
Click on the links to get more technical details about each operation and the meaning
of their parameters, and also compare how the way APIs are documented has changed
over time.
335

Expressive? No: Stringly Typed
Usage examples:
Do you really want your clients to parse the result out of a String
representation?
Use the programming language type system (e.g., enums) so that
the compiler can detect incorrect API usage
Resort to strings only for inter-language compatibility or tunneling
String do(String something);
do("1 plus 1") // returns "result = 2"
do("1 divided by 0") // returns "error: division by 0"
do("nothing") // returns "ok"
What’s the most flexible and expressive API design? Here is the simplest and most
general API you can design: it takes a string as input and returns a string as output.
If you want to use it, you have to, of course, pass the correct strings to it.
How expressive is this type of interface? It is rather difficult to answer if you just look
at this piece of information. To know exactly how to use it we need to describe the input
language and the output language and how they relate. The compiler will not help be-
cause as long as you exchange strings, there is no data representation problem. A string
can represent any data structure, as long as it can be converted into a string.
These interfaces have been called ”stringly typed“, in opposition to strongly typed,
which are interfaces which feature parameters and the result having a very well defined
type. It is not that strings are not well defined, they are strings after all, but they can be
super flexible to tunnel any serializable data type.
Strings can represent natural language text, numbers, data items, base64 binary data,
markdown, error codes, exceptions, complex data structures serialized using JSON or
YAML, or even XML.
But when you look at the interface definition, you don’t know exactly what the inter-
face expects as input or returns as output.
A chatbot could use this interface, this could be the API for a smart home assistant,
which would transform speech into text and feed it to this interface. What can you tell
to your assistant? Anything goes.
Strings can also contain executable code, for example SQL statements that you feed
into a database, or JavaScript code evaluated by your trusted Web browser.
If you think about processing the results, do you really want your clients to use such
an interface and then have to parse the actual result out of a string representation?
All you can do when you program your client against this interface, is to assign the
result and store into a variable of type string.
336

If you need to process the result beyond printing it out, you need to feed the string to
a parser, which can extract its content, based on some assumptions you make about the
language defining the syntax of your string.
That’s why type systems in programming languages have been invented. You don’t
have to do the parsing yourself, but the compiler helps you to structure the data and give
it a representation which can be manipulated from your code.
If you have a string in your interface, does it have any impact on its compatibility?
Strings are compatible with strings, so if all interfaces exchange strings, you solved all
compatibility problems in your architecture, and everything can be connected with ev-
erything else.
What if you have a more constrained type system defining your interface data model,
then you can take advantage of type checking for detecting whether this interface is mis-
used or used correctly, whether it is connected with other interfaces whose types match
or don’t match. This check you can perform statically, based on descriptions, no need to
actually call the APIs and observe whether they accept or reject your strings.
It’s not possible to detect all possible incompatibility situation because sometimes
even if the types actually match, there are deeper semantic issues. But they already help
you to catch a quite a few cases in which you make a mistake.
Another important case in which strings are useful is if you want your API to be pro-
gramming language independent. For example, Web service API exchange messages
which are basically a string representation (e.g. in XML, or JSON). Somewhere you need
to convert your data into strings and back, but this makes it possible to exchange data
between different programming languages.
To summarize: How general can we go with the API design? One of the most general
solutions, concerning data representation, is the one in which everything is tunneled
through strings. Advantages are that you can make this interoperable across different
languages. The big disadvantage is that until you receive the actual string, you cannot
tell whether it is compatible or not. There is no static checking that you can do about
compatibility or whether the right parameter is in the right slot because you don’t know
anything about the types that describe the data model of the interface.
337

Consistent?
The strcpy() function copies the string pointed to by src, including the terminating null byte ('\0'), to
the buffer pointed to by dest. The strings may not overlap, and the destination string dest must be
large enough to receive the copy. Beware of buffer overruns!
The memcpy() function copies n bytes from memory area s2 to s1. It returns s1. If copying takes place
between objects that overlap, the behavior is undeѹned.
The bcopy() function copies n bytes from src to dest. The result is correct, even when both areas
overlap.
char * strcpy (char * dest, char * src);
void * memcpy (void * s1, const void * s2, size_t n);
void   bcopy  (void * src, void * dst, int n);
Here are a few more examples to show you that when you design your API, it is also
important to be consistent.
These three functions, they do exactly the same. They are used to copy data from
one location of memory or another. In some languages you are doing manual memory
management and there are some cases in which you have a problem of copying data.
They do not belong to the same APIs; can you spot the inconsistency? Look at the way
parameters are positioned. Or at the parameter names.
For example, you need to copy a string into another one. You have to use this type of
functions. As we observe these signatures we can see that the types are different: void
pointers or character pointers, which make sense for copying character strings. We pass
pointers to the source string and to the destination. If we look at the last one, the order
of the parameters is actually the opposite.
One function asumes we can start from the source location and continue copying until
we reach the end because the strings are terminated by the null (zero) character. Another
function instead takes the number of characters to copy, so that’s a slightly different
convention. The length of the string is embedded in the string encoding. The length is
given as an explicit parameter.
One function just takes two pointers: s1, s2. So based on reading this and your knowl-
edge of C, do you know which one is the source and which one is the destination? Or
maybe you can look up the natural language documentation for the function and that
can help you. s2 is a constant pointer. This subtle detail tells us something important.
That this is a read only pointer, because we cannot modify it, which is consistent with
the parameter order of the first example.
Other differences concern not only the signatures, but the way that the implementa-
tions deal with overlapping memory buffers.
But what can be the impact of such inconsistencies? What could go wrong if one func-
tion assumes the source follows the destination, and the other function uses the opposite
parameter order?
Have you ever done something that is called copy’n’paste programming? You write
your code using the first function. Then you do copy paste. In the cloned code, you
switch to the third function.
338

This happens to me quite often, I copy and paste, I change the name of the function,
assuming that the parameters are fine, they do not have to be adapted.
If the parameter order needs to change and we forget, what could possibly go wrong?
We switch the pointers for the source and the destination. Can the compiler help us
The code that used to work, now it is actually copying the destination over the source,
which can definitely lead to unexpected behavior, as it corrupts the memory heap of our
program.
Consistency is important. Following conventions is important. Don’t surprise your
API developers by changing the order of the parameters. If you do, warn them, and use
explicit parameter names: source and destination are much more clear than s1, s2.
339

Primitive Operations
Which is the primitive operation?
■playSong(mp3)
■playSongs(mp3[])
Primitive operations cannot be implemented by combining any
other operations
Derived operations can be implemented as a combination of
primitives and do not always have to be included in the interface
To help you choose what we need to include in an interface to keep the interface as
clear and as simple as possible, we can use the notion of primitive operations.
Primitive (or atomic) operations cannot be implemented by combining any other op-
eration you find in an API. Derived operations can be implemented by composing other
existing API operations.
If you remove a primitive operation, there is no longer a way for the client to perform
it. The operation is gone and the client has no alternative, no work around is possible.
Removing a derived operation, even if you remove from the API, will not break the
client, as it is still possible for the client to access the same functionality by composing by
itself the primitive operations corresponding to the derived one which was just removed.
The question here is between the play song and play songs (one takes a single song and
the other one takes a list): Which of the two is primitive?
It should be possible to implement the derived operation based on the primitive one.
Play song: primitive. Play songs: derived, because it can be implemented using play
song.
How would you do it? Just write a little loop: for every element, play song.
If the API can play one song for the client, it’s possible to use it to play multiple songs.
What is your assumption when you write that loop? How can you claim that they are
equivalent? What if the play songs will play the songs of the playlist one after the other?
So you can write using a loop, if you assume that play song will start playing the song,
wait until the song is finished, and then return so you can call it again with the next song.
So we assume ”play song“ is actually a blocking call, which will play the whole song
until the end, and then return. We assume in other words, that it is not just a call to start
playing the song.
If this is true, then this is primitive and this is one just a derived operation, which we
can drop from the API or choose to include if we want to provide support for playlsits.
If the assumption is false, then it is not possible to use play song to implement play
songs, and both are considered primitives, both should stay in the API.
340

Simplify the API
Primitive
Derived
playSong(mp3,from,until)
loadSong(mp3)
seek(t)
play()
stop()
pause()
resume()
togglePlayPause()
isPlaying()
getDuration()
getPosition()
setPosition(t)
onSongLoaded
onPositionChange
onSongFinish
Here is a more complicated example where it is possible to find different subsets of
primitive operations.
To detect derived features, try to see if you can compose other features and obtain the
equivalent behavior.
If – like the case of ’playSong’ – the resulting composition spans many primitive oper-
ations and events, and if it turns out to be a popular operation, it will be definitely useful
for clients to find it directly in the API and not having to figure out how to implement it
themselves combining the basic primitives.
If we look at ’setPosition’ and ’seek’, these two look redundant, one would be enough,
one can be implemented using the other (and viceversa). In this case which one do you
keep? Consistency with ’getPosition’ would recommend to keep ’setPosition’, but ’seek’
is also a commonly used term in the media player domain model.
Sometimes events notifications of state changes can be redundant regarding proper-
ties. Is it enough to be able to read the current state (’getPosition’) or to get notified
when it changes (’onPositionChanged’), or do we need both? This would give the most
efficient usage of the API: read the state initially and then keep track of it as soon as it
changes via the event? (no need to keep refreshing the position property).
341

Joshua Bloch
Design Advice
• Keep it simple
• Do One Thing and do it well
• Do not surprise clients
• Keep it as small as possible but not smaller
• When in doubt leave it out
• You can always add more later
• Maximize information hiding
• API First
• Avoid leakage: implementation should not impact interface
Joshua Bloch
Design Advice
• Names Matter
• Avoid cryptic acronyms
• Use names consistently
• Internally Consistent
• Naming Conventions
• Argument Ordering
• Return values
• Error Handling
• Externally Consistent
• Imitate similar APIs
• Follow the conventions of the underlying platform
342

Joshua Bloch
Design Advice
• Document Everything
• Classes, Methods, Parameters
• Include Correct Usage Examples
• Quality of Documentation critical for success
• Make it easy to learn and easy to use
• without having to read too much documentation
• by copying examples
• Make it hard to misuse
We close with a summary of what we know about API design.
Simplify. Maximize information hiding. Outside-in. Abstract.
Naming conventions are fundamental; consistency of names and order of parameters
is important. Also keep the API consistent with its environment and programming lan-
guage conventions.
Everything has to be documented. Everything else being equal, if you have a high qual-
ity documentation, this makes developers more inclined to learn how to use your API and
eventually adopt it.
Documentation should include examples, not just describe and explain features, but
examples make it easy for developers to reuse them by copy’n’paste. However, developers
should be able to be productive without having to read too much documentation: if you
need to read 500 pages before you can write a Hello World, there is some problem.
With interfaces, we learned how to design reusable components. In the next lecture,
we are going to connect them together.
343

References
• Joshua Bloch, How to Design a Good API and Why it Matters, Google Tech Talk 
 
• Michi Henning, 
, ACM Queue, Vol 5, No 4, May/June 2007
• Will Tracz, Confessions of a Used Program Salesman, Addison-Wesley, 1995
• Jaroslav Tulach, Practical API Design: Confessions of a Java Framework Architect, APress, 2008,
ISBN 1-4302-0973-9
• Jasmin Blanchette, 
, Trolltech, 2008
Slides Video
API: Design Matters
The Little Manual of API Design
 
 
 
 
 
 
 
 
344

Software Architecture
Composability
and Connectors 7
Contents
• Software Connectors
• Components vs. Connectors
• Connector Roles and Qualities, Cardinality, Binding Times
• Connectors and Distribution
• Connector Examples: RPC, File Transfer, Shared Database,
Message Bus, Stream, Linkage, Shared Memory, Disruptor, Tuple
Space, Web, Blockchain
345

The topic of today’s lecture is about software connectors and how do we use them to
compose multiple components and connect their interfaces together so that we can build
a large software architecture.
There are many different kinds of connectors. And they have a different impact on the
coupling they introduce between the components they connect.
You may have already programmed software connector when you wrote something as
simple as a function call. You had the client side calling a function and you had the
server side implementing it. The function was running locally or maybe the function was
running remotely on a server. This is just one of the simplest forms that we know of
how to connect together two different pieces of code. We will see there are many more
examples.
Software connectors are the elements in the design of your software system that al-
lows you to interconnect the interfaces of different components. They enable to build a
large, distributed software architecture based on the elements that we have studied so
far: components and their interfaces.
Here you can see a picture of hardware connector: the cabling that you find underneath
a supercomputer. Depending on the type of cable, you will get different performance:
communication bandwith or latency. Exchanging data between different components is
just one of the purposes of connectors. Another is to coordinate the work of the compo-
nents.
346

Transfer
data, control
signals
between ports
Plug into matching
component ports
• Enable architects to assemble heterogeneous functionality,
developed at different times, in different locations, by different
organizations.
Connector: enabler of composition
Hardware connectors allow you to plug components together: they make it possible to
compose your system.
As you can see from this picture, connectors have two sides. One is the cable, responsi-
ble for enabling communication and coordination, the transfer data and control between
different ports. The other are the plugs, which can be the same or different on each end,
and which have to match the interfaces of the components.
That’s why we are studying connectors after we have learned how to describe and de-
sign software interfaces. The next step is to literally plug interfaces together with a cable;
wiring them together using a connector.
Connectors are fundamental because they make it possible to assemble components
together, even if components are very different from each other. Components can be
developed at different times by different people and the connectors make it possible to
reuse them by composing them in different and unexpected ways.
As you design the structure of a software architecture, you decompose it into compo-
nents so that you can implement them independently. You can reuse some of those if
they already exist. You can build or buy others. Once you have split the architecture into
components, the connectors make it possible to do the opposite, to bring the compo-
nents back together. Depending on the choices that you make on how you will reconnect
your architecture together, you will affect the reliability, the performance, the security
and also how easy it is to evolve your system.
Some connectors introduce strong coupling: they make components highly depen-
dent on each other. Other connectors are designed so they can actually minimize the
coupling between the components. If you change something about one of the connected
component, thanks to the connector you choose, the other component connected to it
may notice the change or may not be affected at all.
347

Software Connector
• Perform and regulate interactions between components
• A connector couples two or more components to:
• perform transfers of control and data exchanges at run time
• represent logical dependencies at design time
• adapt mismatching interfaces to ѹt
How do we represent connectors in a model of a software architecture? We already
know that the components look like boxes. Whenever we draw a line between two boxes,
we are introducing a connector between them.
The elements of the architecture which performs and controls the way the components
interact is called connector.
Connectors have three different but fundamental purposes. One purpose is to per-
form control and data transfer: so that you can desig how components communicate and
coordinate their work. By transfer of data, we mean sending a message with some infor-
mation from A to B. By transfer of control, we mean that, for example, one component
will send a command to the other. When the command is received, the other component
will start execution to process the command just received. These type of interactions go
beyond data communication, they are actually telling the other component what to do
and telling the other component to do something on behalf of the sender.
Connector give a visible representation to component interactions which happen at
runtime. They help you to describe the behavior of the architecture and how the different
parts interact.
Connectors also help you at design time because if you draw the boxes and you put a
line between them, you know that there is some kind of a logical dependency between
the components. If you don’t draw it, if you keep them separate or disconnected, this
gives already very valuable information at design time.
Connectors capture the decision about whether to keep two components independent
or to make them interdependent.
This important decision impacts how you will build and operate your system. Inde-
pendent components can be assigned for development to two independent teams. The
teams do not ever have to talk to each other because the two components are completely
detached.
When you draw the line between them, you are not only connecting the components
348

together, but you’re also changing their development life cycle. Now the teams that are
going to develop the components have to be aware of one another and the decisions that
you make concerning the interface of one component will need to be seen from the other
side.
The third purpose of connector is also to help resolve interface mismatches. In the
simplest scenario, when you draw this line between the two interfaces, they have to per-
fectly match: what one component provides, the other component requires exactly.
It’s also possible to have mismatching interfaces. One component needs something
and the other one almost provides what the other one needs, but it’s not exactly com-
patible. Connectors can help you to deal with these incompatibilities: in some cases you
can have a connector which plays the role of an adapter to solve some types of interface
mismatches.
In this lecture we focus on the control and data transfer aspects so we can see how to
use the connectors to design our system. We are now looking at the connector viewpoint.
349

Components vs. Connectors
• Components can be mapped to speciѹc code artifacts
(compilation units, deployment packages or images)
• Connectors are not usually directly visible in the code
(linkage between modules, connections across the network,
conѹgurations of server addresses, shared references to
memory addresses, database tuples or Web resources)
• Components can be both application-speciѹc or application-
independent (infrastructure)
• Connectors are mostly application-independent
For many years there has been a religious war within the early software architecture
community: are connectors just a specialized type of component, or should they be con-
sidered as something fundamentally different? Just like graphs have nodes and edges,
the structure of software architectures has boxes and lines: components and connectors.
Let’s see what are other similarities and differences.
Components and connectors are dual concepts. If you build and implement a connec-
tor, you need components. A connector will abstract inside the line a very complex piece
of software with lots of components.
When you try to discover where are these connectors found in your code, it’s a bit more
difficult to identify them as opposed to when you look for ordinary components.
How do we structure a large piece of software code? you have multiple compilation
units, you have multiple packages. When you build your system, you’re going to bake
into an image, which you can deploy and install somewhere.
These are all physical software artifacts, they have are usually stored as files on disk,
and files have names and are organized into folders. Depending on the convention you
follow, you can sort of recognize some of these files and folders as independently build-
able or deployable components.
However, once you have built or installed these packages and you want to connect
them, it is more difficult to actually see this connection materialize somewhere as a file
or folder in your repository of artifacts.
Sometimes a connector is just some configuration entry that points to the address of
the database. If you change the address of the database, magically your component now
becomes connected with a different server and can suddenly see different data shared
with other components.
At a certain point during the build process, multiple object files are linked together,
the result is a single artifact which groups together multiple libraries all glued together
and ready to call each other: they are connected.
350

Sometimes it’s enough to to connect to a certain URL address of a Web API. You need
to know where to find the end point of the API, before you can start exchanging messages
with it. If you switch the address, your component will be connected to another set of
components sharing a different Web resource.
The connector may be as simple as sharing a pointer to a location in memory so that two
different components can exchange information by copying it into this shared memory
buffer.
We will see there are many different kinds of connectors, some are locally embedded
into components, others have a life of their own and depend on external components.
For example, a message queue needs to be up and running somewhere, so that you can
actually send messages through it.
While there are both application domain specific and general infrastructure compo-
nents, connectors are typically part of the infrastructure and they are application inde-
pendent.
351

Connectors are Abstractions
• Connectors model interactions between components
• Connectors are built with (very complex) components
• Design Decision: when to hide away components inside a
connector?
Connectors are an element that we introduced in the model of the software architecture
to represent which and how components interact with each other.
To do so we just draw a line between the boxes. This line is an abstraction because –
depending on the connector – to implement it, you will have actually lots of components
hidden away inside the line.
How do we raise the level of abstraction of our model? Do we want to show the details
of how the connector works or do we want to hide all of this complexity?
Do we want to just show two components which are directly calling each other, or do
we want to describe the internal architecture of some remote procedure call (RPC) mid-
dleware, which is going to intercept the call, serialize its parameters, transfer all data
across the network, receive it, deserialize it, execute the call on the server side and do
exactly the same on the way back with the result.
You would need to model the whole networking stack on both sides. Are you sure it is
so important to describe how the Internet works? Connectors have a lot of moving parts,
that’s why by mentioning their name is enough to describe the connector view of your
architecture. We draw a line, decide which connector it represents, and that’s enough to
explain how we solved the problem of composing the two components.
352

C lient-side
Server-side
WebSocket
Server
HTTP
Server
DB
Video
Prof les
Stream
C ontroller
Upload
C ontroller
Registration
Login
DB
C onnector
Server
UI
C lient
HTTP
Client
WebSocket
Client
Encryption
Handler
Login /
Registration
Player
Webcam
View
Uploader
Video
C hunks
Connectors, like interfaces, are powerful abstractions that we can use to simplify our
architectural model.
This is an example of a client/server architecture with also a database in the back end:
which are the components that could actually be hidden away within some connector?
One candidate could be the one called DB Connector. Also look for infrastructure com-
ponents. Components whose responsability is to communicate and transfer some video
files across the client and server containers.
Also the HTTP related components are clearly a candidate: we have a Web server com-
ponent on one side and the HTTP client on the other. The protocol is standard, the roles
and responsabilities of those two components should be well understood. So they could
be just collapsed within a connector labeled with the name of the HTTP protocol.
Should the security features be embedded into the connector? To make the communi-
cation secure we need to have some encryption but also – if you notice it is missing – on
the server side there should be some decryption.
In general, when you have a distributed deployment you will find these situations in
which you have a client-side and server-side of the interaction. So you need to model a
component which can speak the protocol for each side. Or you can just abstract it away
and show a connector instead.
The components which deal with the video stream could also be abstracted away, but
we keep them because they represent after all what the application domain is about.
353

C lient-side
Server-side
DB
Video
Prof les
Stream
C ontroller
Upload
C ontroller
Registration
Login
Server
UI
C lient
Login /
Registration
Player
Webcam
View
Uploader
Video
C hunks
HTTP
WebSockets
JDBC
Here we can see that the resulting architectural view is much simpler. In the connector
view, we focus on the most important components and the connectors between them.
The other ’connector components’ have been abstracted into the edges.
It is enough – at this level of detail – to decide: we’re going to use HTTP to bridge the
divide between the client and the server. We will use Websockets for the live stream. And
we choose the standard JDBC API for connecting to the database.
354

Which components could be hidden inside a connector?
Database
Cache
API
Cache
Client
Sink
Pipe
Filter
Pipe
Source
Web
App
REST
Framework
HTTP
Server
HTTP
Client
Browser
Here is another example of simplifying a logical component view into a connector view.
Select which components you would like to abstract.
This is just to practice how to make abstraction decision. In your model you are trying
to decide which are the components that shouldn’t really be represented as components.
Instead you prefer to hide them and turn them into connectors.
You cannot click on the outermost components: those should stay there. What we’re
interested in is to see if we can simplify the intermediate components.
First example: A client invokes an API, which queries the database. In between we find
a cache to speed up the calls. The cache both on the client side and on the server side
seems to be a good candidate. We can use a connector which features caching to help
speed up the performance.
Second example: In a pipeline that starts from a stream data source we need to carry
the stream along through the filter and make it reach its destination. The pipe – by
definition – is the connector between the filters, so it can disappear. The filter itself is
actually part of what the application is trying to do. You take a data stream, you need
transform it: we need to show that there is a filter that is processing the data in some
way. So between pipe and filter, I would say the pipe is the edge (the connector) while
the filter is the node (the compoennt). The filter represents what you want to do to the
data being streamed through the pipes.
Third example: Looking at Web technology, the browser needs to talk to the Web ap-
plication deployed on the server side. To do so there are various layers of the Web stack.
We have an HTTP client going through the network to interact with an HTTP server, with
all the necessary infrastructure that transforms the protocol originally used to publish
documents and hypermedia applications and turn it into a channel for delivering some
kind of application software development platform; I agree with your choice, you could
just abstract everything in this in these layers and just show that the browser uses HTTP
to talk to the web application.
355

Which components could be hidden inside a connector?
Database
API
Client
Sink
Filter
Source
Web
App
Browser
This is the result.
Why do we have three arrow heads here? The three arrows are just a visualization of
the fact that we have a stream. The stream is a continuous flow of data between the two
sides. As opposed to the single headed arrow where we just have one message followed by
one response. With the stream, we want to show that there is an infinite flow of messages
going through. So three arrow heads are enough to represent infinity.
This is the first example of connector view where we try to visually distinguish which
kind of connectors we have introduced. Different arrow or edge shapes: in the first case,
we have calls; the second uses a data stream; the third again request-responses messages
of the HTTP protocol.
We will see later what these shapes mean in more detail: there are many more con-
nectors that we can introduce. Here we just notice that they help to distinguish different
information flow directions and help give structure to the communication happening
between different components.
356

Connector Roles and Runtime Qualities
Communication
(Data Flow)
Coordination
(Control Flow)
Adaptation
Performance
Security
Reliability
Maintainability
What is the purpose of the connector abstraction? Why should one decide to use a
connector within an architectural model?
Connectors are the elements responsible for the communication, the data flow, as well
as the coordination, the transfer of control (or control flow), between components. We
will see that they are also used for adaptation.
While connectors are dedicated to enable the composability of your architecture, the
choice that you make to introduce a specific kind of connector affects many additional
quality attributes of the architecture, starting from the coupling between the compo-
nents.
Also it may impact the performance: depending on the implementation of the connec-
tor, you might have a low latency or high latency communication.
Connectors are also relevant concerning security: if you want to eavesdrop on our lec-
ture, you can just intercept all the bits that are streaming through the network. So con-
nectors show not only which component talks to which other component, but also can be
used to indicate where the communication should be protected. The connector is both a
weakness in your architecture, but also the point that you can decide to protect.
Connectors definitely affect the reliability of your architecture. Depending on the con-
nector that you have, if you try to transfer control to something that is not available:
What happens to the other side? Established a connection implies a dependency between
two components. These components could be subject to something called “cascading
failures”. Which means that if a component fails, the other one – because it is connected
– will fail as well. Maybe not right away, but eventually it will fail. By modeling such
dependencies, you can observe and predict along which path the failure will propagate
through your system. By choosing the right type of connector, you can control and stop
the dominoes from falling.
What about maintainability? Connectors also represent constraints on how compo-
nents can evolve. We make a change to one component, this change may impact the
other components connected to it. The fact that some connected component will be af-
fected may limit the ability to freely change an interface.
357

Connectors and Transparency
Direct
Components are directly connected and aware of
the other component
Indirect
Components are connected to the others via the
connector and remain unaware
Server
Client
Client
f(x)
Queue
Server
emit
on
f(x)
queue.emit(m);
queue.on(m=>{});
Different connectors will also affect to which degree the connected components are
aware of each other.
Let’s see if I can give you an example. When you make a function call, you need to
know the name (the identity) of the function that you’re calling. So the client expects
that the function named F, and which takes one parameter, actually exists on the other
side. The client knows that is calling this particular function. If you rename the function
on the other side, the component that depends on this function F will be surprised: you
just change the name of the function and you will not be able to complete the call unless
you also change the name of the function known by the client.
We will see there are many forms of coupling, but this is one is rather important: Do
you need to be aware about the existence or the actual identity of the component that
you depend on?
If you have this type of connection, the client directly connects to the server: it directly
depends on the existence and on the identity of the specific component which has been
connected with (the function F).
It’s also possible to have different types of connections, for example a message queue,
which isolate components and keep them unaware.
If you connect components indirectly through a message queue, the nice property of
messaging is that components only need to know and agree on the identity of the queue
Q you have to deliver a message into, or you are listening for messages from. The same if
you have a message bus: I subscribe to a topic. When a message about this topic arrives,
call me back. You want to publish a message about this topic, please forward it to all
interested subscribers.
Both sides are directly connected to the queue, but they are indirectly connected to
each other. When you publish a message into the queue, you have absolutely no idea
whether the message would be actually received by anybody else. You copy it in the
358

queue: only if someone is interested they will pick it up. Maybe they don’t pick it up
today. They pick it up later. Or maybe it’s nobody is subscribing to this, so the messages
just ends up in the dead letter queue. But you as a sender will have no awareness of who
actually received the message. The communication will work, the data will flow, but the
recipients will not need to know about the identity of the original sender.
This is a separate form of connection which preserve a higher degree of independence
between two components because they remain unaware about the existence or the iden-
tity of the others.
Still, there has to be some shared assumption: If you change the name of the queue, of
course both components will be affected. Both components have to agree on the name
of the queue, because they are directly conneced to it. The queue itself acts as a layer of
indirection between the two components,
359

Connector Cardinality
Point to Point (1-1)
Multicast (N-M)
The other classification we can make is about whether connectors connect only a pair
of components (they are shown visually as the classical edge between the nodes in the
graph) or some more complex connectors can be used to connect more than two compo-
nents together (the connector view becomes a hypergraph).
It’s easy to transform one into the other, since a connector with cardinality N>2 can
be split into N connectors of cardinality n=2 which connected each of the components
to a new node, representing the original connector. Like we have seen before, this node
would represent the queue through which all other components can send and receive
messages.
At some abstraction level, it is simpler to visualize a multi-pronged connector as op-
posed to many point to point ones.
360

Connectors and Distribution
• At design-time connectors deѹne the points where the
distribution boundaries are crossed so that components can be
deployed over multiple physical hosts at run-time
Connectors are also fundamental to enable a distributed deployment of your architec-
ture across multiple containers.
At design time they help you to pinpoint the spots in which you will need to cross the
boundaries between different runtime environments.
So do all connectors give you support for a distributed deployment? Not all connec-
tors support remote interactions. Some of those are actually local. For example, a shared
memory buffer helps to transfer some data between processes running on the same op-
erating system, but it is difficult to use shared memory in a distributed environment.
You can pick a local or a remote connector and your choice will constrain whether
components need to be co-located in the same host or can be freely deployed anywhere
across the Internet.
Or conversely, you can place your components in the corresponding containers, and
then pick suitable local or remote connectors to make sure they can stay within reach.
Warning: make sure that the decisions you represent in the deployment view are con-
sistent with the decisions you make in the connector view. Take care to ensure that all
your components can communicate and coordinate with each other, depending on where
they are.
361

Connectors and Availability
Synchronous
Both Components need to be available at the
same time
Asynchronous
The connector makes the communication
possible between components even if they are
not available at the same time
The last characteristic that I wanted to point out concerns the difference between a
synchronous vs. asynchronous connectors.
This has a big impact on whether the pair or set of components that are connected can
be available independently from one another.
In the case of synchronous connectors, when we draw a line between the boxes, we
assume that this will work only if both components are available at the same time. This
means that when a component needs to interact with the other one, the other component
has to be there. If it’s not, the component may fail, and in case your design features a
beautiful chain of synchronous connectors, you will have a cascading failure, potentially
affecting the entire architecture.
If, on the other hand, we have asynchronous connectors, it means that the commu-
nication or the interaction would be successful even if some of the components are not
available at the same time.
How is that possible? Let’s go back to the message queue example again. If you put
a message into the queue. Whoever receives the message doesn’t need to be available
and at that exact time. You just assume that eventually they can receive it and read the
message from the queue. When they are available, the message is delivered. For the
delivery to work, the original sender does not have to be available at that time.
You just have to make sure that there is a moment in which both the sender and the
queue are available at the same time so they can exchange the message being published.
Later on the queue and the receiver have to be available at the same time, so that the mes-
sage can be delivered to its destination. But the original sender and the ultimate receiver
of the message do not have to be available at the same time, thanks to the asynchronous
connector between them.
Another way to put it: if there is a chance in the state of availability of one component,
this change will not affect the other side only if there is an asynchronous connector be-
tween them.
362

Procedure Call
Remote Procedure Call
Remote Method Invocation
Stream
Message Bus
Linkage
File Transfer
Tuple Space
Web
Blockchain
Disruptor
Shared Database
Shared Memory
Software Connector Examples
In the second part of the lecture, I wanted to make the topic of software connectors
more concrete by giving you a number of options that you can choose when you model
the connector view of your architecture.
The goal is to transform the graph linking the components that are logically dependent
on each other and refine it by classifying each of the edges: which connector are you going
to use?
And, as a consequence, which properties can you expect about the overall behavior of
the system?
A bridge is a very important type of connector in the real world: it enables people to
safely cross a river; it allows the people on one side to meet the ones on the opposite side.
Over time, the bridge becomes not only a way for solving the problem of getting across
the river without boats hanging from a rope, or having to swim against the current. Some
bridges become meeting places, where people started to work and live on the bridge. You
have a marketplace, with tourist shops and customs checkpoints.
The function of a bridge gets actually quite overloaded with many more features in
addition to the original use case. A lot of software connectors have experienced the same
type of complexity growth: you start trying to solve a simple problem, and then you add
more and more responsibility on the connector.
Let’s go over the software connector examples that we are about to compare in detail.
You have probably already seen some, you may have already used some, like if you shared
the same database between multiple components.
The most common mechanism used to share information between different systems is
the file transfer. Files and databases are concerned with data exchange. A more advanced
form of data exchange is the stream: a continuous exchange of infinite amounts data.
As opposed to file transfer, where files are exchanged one at a time, bits are flowing
through a stream continuously. We also can promote our local shared database to make it
363

accessible from the whole World Wide Web: so we can do data sharing at a global scale. Or
we can have a more powerful form of database, which not only supports communication,
but also coordination among a set of components: tuple spaces.
If you’re interested about coordination: there is the procedure call or remote procedure
call. With it, components can transfer control with a very simple and easy to program
type of interaction. One makes a call, the other does the work to process the data and
then give back a result when it’s done. And while this happens the caller waits. The call
blocks and can continue only after the result arrives. Calls both handle communication
but also coordination. If you want to avoid the synchronous/blocking limitations of a call,
you can switch your decision to use the asynchronous message bus as a connector. The
call introduces a direct connection between the two components. The bus is an indirect
connection because all interactions between publishers and subscribers happen via the
message queue in the bus.
Another very simple connector is used to link together different components. What is
interesting about linkage is that it can be not only static but also dynamic. So if you want
to build an extensible, open, plugin-based architecture you will need to have a linkage
connector somewhere.
Shared data can be slowly persisted over long periods of time, or volatile, with local,
shared memory-based connectors. What if you have a local (co-located) set of processes,
which need to share some data? What if you need a low latency, lock-free solution? Take
a look at the disruptor.
One of the most recent additions is the blockchain: which helps to share an immutable,
append-only, transaction logical maintained over a decentralized network of untrusted
peer.
As we go over each of these examples, we will see how each connector works more in
detail. We can also classify them according to whether they are synchronous or asyn-
chronous, whether they establish a direct or indirect connection, or whether their pur-
pose is to perform data or control transfer, or both.
364

RPC: Remote Procedure Call
Call
Client
Client
Server
Server
request
response
• Procedure/Function Calls are the easiest
to program with.
• They take a basic programming language
construct and make it available across
the network (Remote Procedure Call) to
connect distributed components.
• Calls are synchronous and blocking
interactions
The first connector is one of the simplest ones: the call, friend of every programmer.
We have seen components as a basic construct helping to design modular systems, with
the ability to delimit and encapsulate reusable parts of your code into functions (or pro-
cedures). When functions separate and structure your code, what is the corresponding
construct to assemble functions together? How do you decide that you need to make use
of this function at the right place? You call the function.
What does it mean to call a function? First make a request passing some input data.
The other side is going to receive the request and process it. And, as soon as possible, a
response will deliver the result back the caller.
In the sequence diagram, you notice that there is a gap in the control flow. First the
client is active, then as the call starts the control is transferred to the server, which was
not active before the call started. Since the client is waiting for the result, we say that the
client is blocked until the response comes back. Once the server completes processing
the request, the response is delivered and the client becomes active again.
I focus on this detail to show you that with the call connector we have both communi-
cation and coordination. We have a request message carrying the input parameters that
is sent followed by result data that is sent back with the response. We also have coordi-
nation because the client will block as it waits for the processing on the server to finish.
This interaction is also about transferring control, making the server (a passive entity)
do something and then resuming processing once the server sends back the response to
the client.
This combination makes calls very popular, as most developers know how to write code
that uses method calls, function calls, different types of calls and we can use the same
abstraction remotely. It’s much more complicated to implement: a local call is just one
CPU instruction to jump and execute code at a different. It takes a few nano seconds. If
you try to make a call across the Internet with the server running in a different continent,
then you have a significant latency due to the finite speed of light. Also, while it is also
365

possible to locally jump into a bad address, as you make a call across the network, the
other side may not necessarily be available when you try to call them. Also the network
itself could be problematic, dropping the response message although the request went
through ok.
If you decide to introduce a call because you want to make it easy to implement, as a
consequence, you pay the price of these drawbacks in terms of performance and reliability
as you rely on a synchronous connector.
Since the call will also block the client, in a way, the interaction is inefficient because
the client will remain idle while they call is happening.
RPC: Remote Procedure Call
Call
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
×
✓
✓
×
✓
×
✓
×
Let’s summarize the main properties of this connector. When you make a call you
jump to the other side. If the call has parameters or returns a result, then you will be also
sending some data back and forth: for sure calls imply a control transfer, but also data
exchange.
You can have both local and remote calls. Whenever we have remote interactions, in
most cases, they can also happen locally.
A call is a direct connection between exactly two components: the caller and the callee.
The component being called must be available to answer the call: calls are by definition
a synchronous type of interaction.
We will enumerate these properties for all connectors, so you have a frame to compare
them against.
366

File Transfer
Write Copy
Watch Read
• A component writes a ѹle, which is then copied on a different host, and
fed as input into a different component.
• The transfers can be batched with a certain frequency
• Transferring ѹles does not require to modify existing components which
can already read/write ѹles
• Source and destination ѹle formats need to match
If you’re interested about communication and transferring data, the file transfer is ac-
tually one of the most popular connectors. You will be surprised how many architectures
still use the file transfer protocol (FTP) to transfer information between different sys-
tems. I was once visiting a company and they were very excited about having found this
anonymous FTP server somewhere on the Internet and they discovered that they could
actually use it to copy data between the systems. You know, let their users from their
desktop computers upload some files on the FTP server so that we can download it from
the other side and we can share the information this way.
You should know FTP is an ancient Internet Protocol that is totally unsecure; if you
want to use the file transfer connector today, pick an encrypted protocol.
What makes it a popular connector it the only assumption that you need to make about
the components that can be connected by file transfer: their ability to read and write files.
You can take any existing code written since many years, and the simplest way for it to
interact with the external world is by reading an input file and writing to an output file.
If you want to connect and integrate this program with another one, just copy the file
and give it as input to the next step.
When we transfer the files we need to decide when we actually transfer the files. If
you make a decision to use a file transfer you have to know: which data gets transferred?
What’s the right frequency? Once per day? Every night? Or do you transfer it after the
file reaches a certain size?
Another important assumption beyond reading and writing files, is that if you write a
file and export it from a component, the component that is going to import the file has
to be compatible. Many integration projects have failed because components could not
even import by themselves the files that they were exporting. Both components share
assumptions about the format and the content of the file being transferred, which needs
to be understood by both sides.
If you write a file in a certain format and the other the other side doesn’t understand
it, what can you do? You never had this problem? What if your friend sends you some
files: can you always open them? How can you solve the mismatching file problem?
367

File Transfer
Write Copy Watch Read
Source
Source
Destination
Destination
File
Write
File
File
File
File
Copy
File
Read
Let’s see more in detail how file transfer works: here we have the two components:
what are the primitives asociated with the connector? How do we use them to describe
what happens when a file gets transferred?
There is a lot of machinery under the hood for file transfers to work successfully.,The
origin component has to write the file. How often does it happen? There will be a point
in which the file is ready. The transfer can start. The connector makes a copy. This al-
ready has a performance impact. You have to double the storage cost: each side needs
to have storage space allocated to keep a copy of the file, especially if there is no shared
file system among the containers in which each component is deployed. If you are in a
distributed runtime environment, you use FTP, SCP, or some other secure file copy pro-
tocol to execute the transfer. After the actual transfer happened, the file has arrived on
the other side, then the component can read it. This completes the basic file transfer
interaction.
368

File Transfer
Write Copy Watch Read
Source
Source
Hot Folder
Hot Folder
Destination
Destination
Watch
File
File
Write
File
File
Copy
File
File
Notify
Read
While copying a file is a form of data transfer, how does the destination component get
to know that the file has been copied successfully and it is ready to be processed?
We can look at the size of the file: if the file is not empty or its size has stopped in-
creasing for a while, then you know that it can be read. One can test if the file exists. You
can check if its latest modification date has changed. This may mean that it has been
refreshed, so there’s probably something new in there. These are all heuristics, which go
under the ’watch’ primitive offered by the connector.
Watch is what should be done on the destination side to become aware of the change
in the modification date on the change of the size of the file. Or just to detect the ap-
pearance of a new file. Watch produces a notification, which triggers the processing of
the file.
If we want to actually coordinate the execution between the two sides and not only
solve the data transfer problem, but also add a little bit of coordination on top,the inter-
action becomes slightly more complicated. We use the concept of a hot folder to repre-
sent the fact that destination expects files to appear in a certain location. The hot folder
is being watched by the destination component. There are file system APIs where a com-
ponent can subscribe to be notified if something happens inside this folder.
For the coordination to work, the files need to be copied in the right location. This is
another shared assumption: sender and recipient agree on the content, the format, as
well as the location of the hot folder in which the file should be placed. Each recipient
component can have their own hot folder so that files being transferred are routed ap-
propriately. Once the file appears in the watched location, the component is notified and
it can start reading it.
It should be avoided that the destination reads the file while it is still being copied.
There has to be a mechanism that detects when the copy operation is finished. Only
369

after the file being copied is closed, the other side gets the notification: the file can now
be opened for reading it.
So we also have a clear mechanism to transfer the control. You can see that the sender
is not blocked and can continue writing more files, while the recipient is processing the
ones which have just been copied across.
In general, different connectors provide different primitives to manage the data and
control transfer. In the simplest case, we have seen there is only a call (which performs
both control and data flow). The file transfer connector has actually four different prim-
itives (write, copy, watch, read) that you need to know how to combine to make the in-
teraction work.
370

File Transfer
Write Copy
Watch Read
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
1-M
N-M
✓
✓
✓
✓
×
✓
×
✓
×
✓
×
File transfer is about copying the data across. Sometimes you FTP the data and then
you make a phone call and you tell the other side: the data has arrived; run your code to
process it. So the control flow is achieved via out of band communication.
The transfer can both work locally or remotely.
Is this connector direct or indirect? Are the two components directly aware of each
other? What is the assumption that they make in order for the transfer to work? Re-
cipients know about the file but not about its source, the sender. The most important
assumption concerns where the file is going to appear (the hot folder); maybe they need
to agree on the name of the file. But they don’t care where the file is coming from. The
interaction is mediated through the file which doesn’t necessarily carry any knowledge
about its origin.
Is this connector synchronous or asynchronous? Does the availability of the destina-
tion impacts the ability of the source component to write the file? There is a point in time,
in which both parties need to be present: when you make the copy. For transferring the
file across two different systems, both of them need to be present. But when writing the
file, this is a local operation. And when reading it, the file has already arrived, so that’s
also a local operation and the other side doesn’t need to be present. So there is only one
step, when you’re actually doing the transfer, during which synchronous availability is
needed. From the perspective of the components, since they are indirectly connected via
the file, then they can interact asynchronously. For example, during the day you collect
all the changes and write them into a file. At the end of the day, you make the copy so
the other side can process it at night. The file transfer will be delayed until both sides
wake up to be able to exchange the data.
Regarding the topology: in the simplest case is of course one to one: one writer and
one reader. It is possible to generalize this to multiple readers. Once you have a file, you
can transfer it and send it to multiple destinations.
To summarize: this is the most popular connector as it doesn’t require any change
to the code. You can use it as long as your existing software components know how to
read and write files. Don’t forget: the content and the format of the files need to be
compatible.
371

Shared Database
Create Read
Update Delete
• Sharing a common database does not require to modify pre-existing
components, if they all can support the same schema
• Components can communicate by creating, updating and reading
entries in the database, which can safely handle the concurrency
Here is another approach that helps to solve the problem of having multiple writers to
the same shared file. Instead of copying the file around, just connect all components to
the same database.
A database is already present in most systems that have stateful components to store
persistently their state. The shared database connector simply proposes to reuse or mis-
use this already existing database and configure multiple components to use the same
database to store their states. If all the components share the same database, they can
actually communicate via the database. One component, for example, writes informa-
tion into it and another component can read it.
Components already know how to query a database. We just have to trick them into
connecting to the same one. It is enough to reconfigure all of the connection strings of
all of your components and point them to the same database. Assuming they support
the same schema, the data that they store is compatible. If they understand each other’s
data, the shared database is a big improvement over the file transfer.
One problem of the file transfer is that it is a batched operation that happens with
limited frequency (e.g. once per day). The advantage of switching to a shared database is
that right after one component writes into the database and the transaction is committed,
the information becomes visible to all other components.
This can have a significant performance impact. Many companies work with multiple
systems that are customer facing. It can happen that if they use file transfer to synchro-
nize the data, the customers perform one operation – for example in person in the store
– when they go home and check the result through the Internet, they may find that the
result of the operation is not yet visible. Because the updates performed via the front
office system in the store would be sent to the Web backend database using file transfer,
and the changes would be propagated only after 24 hours to the rest of the systems. So
there is a very inconvenient and annoying delay from the customer experience point of
372

view, due to file transfer.
If you put a shared database instead, the propagation delay disappears. As soon as the
transaction commits, the data is already available so that the customer can even get a
notification about their purchase from the mobile phone. The advantage is obtained by
centralizing all the information in one place.
Another advantage is that the database is designed so that you can have multiple con-
current writers. Components run different atomic and isolated transactions and the
database serializes them to ensure that the shared data remains consistent.
Another feature provided by databases is access control, so we can use it to check
whether components should be allowed to write or only read certain shared data items.
What could be a disadvantage? If you worry about scalability, that could be an issue,
with lots of clients is a database going to scale to handle queries from all of them? Perfor-
mance depends on your expectations. Compared to the file transfer connector, latency
is better. What about throughput? What is the most efficient way to transfer terabytes
of data? Availability: if the database is not available, nothing works, because everything
depends on it, so that’s a serious issue. The shared database is a single point of failure.
What about the schema? what could possibly go wrong? Yes, in the same way that
with the file transfer, we assume that the format of the file is compatible, here we assume
that the schema of the databases is and remains compatible. All components have to be
compatible with the shared database schema. What happens if you make a change to the
schema? If one component needs to modify the structure of the data, if you use a shared
database, you need to ensure the compatibility of the schema modifications with all of
the other components. You cannot freely evolve your components anymore in terms of
the way that they represent their data. Since it needs to be kept compatible, you can no
longer independently evolve your components concerning the way they represent and
store their state. Every component is affected since their state is no longer private, but
potentially shared with all other components. Changes to its representation (not to its
content) become much slower, since all components need to agree with the changes and
be brought up to date in lock step.
373

Shared Database
Create Read
Update Delete
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
×
×
✓
×
✓
×
✓
×
✓
So far we have discussed data transfer, what about coordination? We share the data
using these primitives: read, create, update. We can also delete it. But, are the other
components able to react to those state changes? Probably not, unless they poll (re-
peatedly read) the database very frequently to see if some changes happen. If something
changes, they can do something about it. This is a very inefficient way to transfer control.
Is there a primitive provided by the database, which would enable components to be
notified about changes? With plain CRUD (create, read, update and delete) primitives
there is no way that you can efficiently transfer control.
This connector is also remote and indirect: we have database servers that we can con-
nect to from anywhere, which act as intermediaries between components, which do not
need to be aware of one another, as long as they share the same database connection
string. They connect to the database and then through the database they can exchange
information.
Do the components need to be available at the same time? Well, this can also be a
consequence of the indirect and remote properties. If you have an indirect, remote con-
nection, then it will be most probably asynchronous. As long as one component can talk
to the database, it doesn’t matter if other components are available at the same time to
read the information being updated. This may happen any time later.
Databases support multiple connected clients, so the sharede database connector car-
dinality is many to many.
374

Message Bus
Publish
Subscribe Notify
• A message bus connects a variable number of components, which are
decoupled from one another.
• Components act as message sources by publishing messages into the
bus; Components act as message sinks by subscribing to message types
(or properties based on the actual content)
• The bus can route, queue, buffer, transform and deliver messages to
one or more recipients
The fourth most popular connector after file transfer, remote procedure call and shared
database is the message bus.
A message bus routes and delivers messages between different queues. A message bus
is used to connect two or more components while keeping them unaware of each other.
Message buses are very useful to decouple the components connected to them. The
only assumption components need to make concerns the addressing scheme that they
use to identify the destination or source of the messages. It’s possible to publish a mes-
sage into the bus without any knowledge of where or when exactly the message will be
delivered to, or if it will be delivered at all. As long as you receive the message from the
bus, you do not need to know who sent it (unless you need to reply exactly to the sender).
Subscriptions can be based on meta-data associated with the message, based on their
content, or simply based on the name or address of the queue. To successfully exchange
messages, senders and recipients need to agree on the queue name. Instead, with content
based routing, subscriptions are defined based on the content of the messages. In this
case you just look inside each of the messages and based on the information you find in
the actual data, you can decide who is interested to receive it.
375

Message Bus
Publish Subscribe Notify
Source
Source
Bus
Bus
Destination
Destination
Subscribe
?
Publish
Notify
The messaging primitives are: subscribe, publish and notify. First the subscription
tells the bus about the topic of interest, about the queue, or the content about which
the destination is interested. Only after the subscription is active, messages that are
published which match the subscription are going to be delivered to the subscriber.
There is a clear sequence: If you subscribe after a message has already been published,
you’re not going to be notified about it. First, you need to subscribe so the bus knows
where it should deliver future messages.
Notification combines data and control transfer: once the message arrives, it gets de-
livered and the notification wakes up the recipient so that they know there is a new mes-
sage to process. This is also a way to transfer control not only data, the content of the
message, but to react to the ’you have got mail’ event.
As you notice by looking at the sender: after the sender publishes a message into the
bus, the sender is not blocked. So you send a message and then you can continue working
on your side because you’re not interested to hear any answer as opposed to the call,
which would block after sending the request until receiving the corresponding response.
Here we just send a message and we can continue while the message is delivered and
processed elsewhere.
Sometimes the primitive ’publish’ is also called ’emit’. The notification is an event, so
we use ’on’. When you write ’on’, you are doing a subscription, and you pass the listener
that would be called back with the notification every time a message arrives.
Message buses always work with this three basic interactions: First you say ”I’m inter-
ested“. Then somebody says something that is interesting and then you get the notifica-
tion and you work with it.
Within the message bus, many things can happen. The bus routes, buffers, transform
and deliver the messages. Routing means figuring out based on the message where it
376

should be delivered to. Based on the current subscriptions, the destination can actually
be multiple recipients. Not all recipients may be available to pick up their copy of the
message at the same time, so the bus may need to store messages in transit and forward
them along when the recipients are ready for them.
We use term queue to indicate that there may be multiple messages going through the
bus and there has to be some ordering between the messages. Different systems give you
different guarantees. In some cases, you can ensure that the order in which the messages
are sent is the same order in which they are received on the other side. But especially in
case of failures, this is not always true, so in some cases you send the messages in order
and then you receive them in exactly the opposite order. Why can this happen? Because
inside the message bus there are buffers where you store the messages, and if something
fails during their transmission the bus will retry sending the message. These repeated
attempts can cause out of order but also duplicate message delivery.
Very powerful ”enterprise“ message buses can also perform message transformations:
they can convert messages between different formats. This is typically not not available
in a file transfer connector where you copy the bits of the file as they are. A message
bus can be configured with message transformation rules to apply some translation and
modifications to the content in transit.
Message Bus
Publish
Subscribe Notify
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
×
✓
×
✓
×
✓
×
✓
Let’s summarize the properties of the message bus connector: both control and data
transfer, remote (but also local), indirect, as well as asynchronous (by definition) with
one to many cardinality (thanks to multicast or broadcast, where multiple recipients are
involved), but also many to many (if different components can send from the same ad-
dress).
377

Stream
Send Receive
• Data streams let a pair of components exchange an inѹnite sequence of
messages (discrete streams) or data (continuous streams, like
video/audio)
• A streaming connector may buffer data in transit and provide certain
reliability guarantees with Ѻow control
• Streams are mostly used to set up data Ѻow pipelines
Similar to the file transfer connector, the stream also enables to communicate two
different components, one sending the data, the other one receiving it. We talk about
stream data source and the stream data sink.
As opposed to the batched file transfer, or the discrete message bus, the stream is con-
tinuous and is meant to deliver an infinite sequence of messages. The file transfer hap-
pens once, while the stream flows all the time. The advantage is that the information
that we need to communicate appears on the other side as soon as possible, as opposed
to the file transfer, which will require to copy the entire file before the other side can
start processing it.
We can see the difference between file transfer and streaming with another example.
When you watch a video online: if you stream it, you can start watching it as soon as the
enough data has been received. If you download it it means you’re doing file transfer,
so you need to wait for the whole transfer to complete before you can actually open the
video file and watch it. Also once you stop watching the video stream, it’s gone, while if
you downloaded the video, you have a local copy which you can read as many times you
want.
The streaming connector purpose is to make the data flow continuously: to do so the
data may be buffered as it is in transit between the two sides because the rate of sending
and receiving may vary. A stream connector may also employ flow control protocols to
slow down or speed up the sender if the receiving end is not able to keep up or is waiting
for more data to arrive.
A stream connects two components, but multiple components can be connected along
a streaming pipeline. A linear topology of components is found in data flow pipeline ar-
chitectures. In that case, we have one source of data which will be streamed an processed
by multiple filters, which will eventually reach the final sink. The role of the sink is usu-
ally to store the information that has been processed through the pipeline or to display
and visualize it to the user, or both.
378

Stream
Send Receive
Source
Source
Pipe
Pipe
Filter
Filter
Pipe
Pipe
Sink
Sink
Send
Receive
Send
Receive
Send
Receive
Send
Receive
Let’s see more in detail how the various components interact along the pipeline. The
source sends stream data elements into the pipe. The pipe will wait for the filter to receive
it and deliver it. Once the filter is done processing, it will send the result along to the next
pipe which will buffer it and then wait for the sink to receive it. This happens once for
the first stream element to go through the whole pipeline.
When the source sends the next stream data element, the whole pipeline will process
it again in the same way. As opposed to the message queue, which delivers individual
messages to one or more subscribers, here we have the pipe which is responsible for de-
livering an infinite sequence of messages from one sender to one receiver.
379

Stream
Send Receive
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
×
×
✓
✓
×
✓
×
✓
×
What is the data stream used for? Mainly data transfer, since all components are ac-
tively producing and consuming data into and from the streams they are connected to.
The stream can both be a local (in memory structure), or it can also be implemented
across the network using some streaming protocol.
The stream establishes a direct connection between a pair of components, which need
to agree on which end of the pipe they connect with.
The stream is a synchronous form of communication, in which both components needs
to be available and connected at the same time. While the pipe can provide buffering,
usually the stream is processed live. Once the information flows through the stream, it
is not persistend. If you are not available at the time it was streamed, sorry, you missed
it.
And the topology is one to one, which results in linear data flow pipelines. Sometimes
also known as pipe and filter architectures. Some components can split or demultiplex a
stream into multiple outgoing streams, or merge or multiplex multiple incoming streams
into one.
380

Linkage
Load Unload
• Statically Linking components enables them to call each other, but also
to share data in the same local address space
• Dynamic linking also enables the components to be loaded and
unloaded without stopping the whole system
The next connector that we’re going to discuss is the linkage. Linkage takes two pieces
of software and links them together so that they can call each other and they can access
each others state.
Linkage can happen statically, while the system is being built and as a result, out of
two input components, we link them and we have one resulting component, which can
be deployed as a single unit. This is an operation that happens after you compile the code
and before you package it so that you can release it.
Linkage can also be done dynamically. So this means that the two components are
already deployed. We can load dynamically one component and link it with another one.
It is also possible for the opposite to happen, where we detach a linked component and
separate it from another component before unloading it. With dynamic linking, in the
ideal case, we can do so without having to stop the whole system.
381

Linkage
Load Unload
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
✓
×
✓
×
✓
×
✓
×
Linkage is a special connector that is used to enable data sharing and calls between
the components. By itself it doesn’t implement directly any of those. As it is more a tool
used to construct and the component out of smaller parts, it is a local operation. The
two components are directly linked with each other. And by definition the components
have to be present and available at the time they are linked together. If you can link two
components, then you can link any set of components, one pair at a time.
382

Shared Memory
Read Write Lock
Unlock
• Shared memory buffers support low-latency data exchange between
threads/processes
• Concurrent access must be coordinated via read-write locks or
semaphores
Like a shared database for remote components, the shared memory connector works
with local (co-located) components. The purpose is to coordinate access to a shared data
structure, which can be read or written by the different components.
As opposed to a shared database, this connector does not provide persistent storage
but provides high performance, thanks to the low-latency, zero-copy access to shared
data.
Exactly the same memory is mapped on the different components which they can trans-
fer data with a very low latency, just by sharing a pointer to it. You just copy a pointer
and then dereferncing it, you find the data that you want. So this is the most efficient
way to communicate.
The problem is that since you are sharing a pointer to a shared data structure, you need
to coordinate concurrent read and write access to it. One solution involves using some
locks or semaphores.
383

Shared Memory
Read Write Lock Unlock
Source
Source
Buffer
Buffer
Destination
Destination
Write
Read
This is the simplest scenario. We have a component that is going to write into the
buffer. Somehow the destination knows the reference to the same buffer. The source
component can write information into the shared memory buffer before the destination
component reads that particular value.
This example shows you how we can use the read and write primitives of this connector.
384

Shared Memory
Read Write Lock Unlock
Thread 1
Thread 1
Buffer
Thread 2
Thread 2
Lock
Write
Unlock
Lock
Read
Write
Unlock
Lock
Read
Unlock
Buffer
Here we show how we can synchronize access to the shared memory buffer. For exam-
ple, one thread before interacting with it needs to lock it. And then it can write some data
into the buffer. After this update is complete then it can unlock it. This makes it possible
for other components to acquire the lock only when the information is in a consistent
state. So we can see the second component is blocked until the first component releases
the lock on the buffer. While this second component has the lock, it can read the infor-
mation and modify it and then release again the lock on the buffer. The first component
is trying to access the data once again, but it has to wait until the second component has
unlocked the buffer.
This interaction is more complicated, but thanks to the locks we can make sure that
we only read the data after somebody’s writing has completed.
385

Shared Memory
Read Write Lock
Unlock
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
✓
×
×
✓
✓
×
×
✓
Let’s summarize the main properties for the shared memory connector. We can use it
for efficient data transfer and communication thanks for the read and write primitives,
but we can also use it for coordination because of the locking primitives.
Shared memory is by definition a local connector. It’s indirect because the components
only know the address of the shared buffer, but remain unaware of each other.
The interaction is synchronous because since this is a local deployment, either all the
components are present or none is.
Like the shared database, this also is a connector that enables two or more components
to share access to the same local memory buffer.
386

Disruptor
Next Publish
WaitFor Get
• Lock Free: Single Writer, Multiple Consumers
• Cache-friendly Memory Ring-buffer
• High Throughput (Batched Reads) and Low Latency
The next connector is an optimization of the shared memory buffer, which works with-
out the need to have these expensive locks which can block concurrent access.
There are many data structures that are lock free. The disruptor is one example, which
is particularly interesting if you have a large number of concurrent threads which need
to process a large amount of shared information.
For example, here we can see one producer and two consumers. They share a memory
buffer of a fairly large size. The buffer uses a circular data structure, which we call a ring
buffer. It offers very good performance in terms of throughput and latency because of
the lack of synchronization.
It’s interesting also because it allows consumers to read the information bit by bit, but
also to perform a batch read.
387

Disruptor
Next Publish
WaitFor Get
Producer
Producer
Disruptor
Disruptor
Next
1
Publish 1
1
Next
2
Publish 2
2
Next
3
Publish 3
3
Consumer A
Consumer A
Get 1
Get 2
Get 3
Consumer B
Consumer B
Get 1,2,3
WaitFor
WaitFor
Next
4
Publish 4
4
Ring
Buffer
Get 4
Get 4
Let’s see how the disruptor works. In this configuration we have one producer entering
data elements into the buffer.
Before we can write some information into the buffer, we have to ask the buffer where
to put it, and this is done using the ’next’ primitive. The next primitive will give us the
address of the next available location for writing.
We can repeatedly publish the new data into the next available position in the buffer.
The buffer is being populated by the producer which is writing into it. The first consumer
wakes up and asks ring buffer to get the first element. So far we have a managed to
transfer one piece of data between two components like before, but there is more data so
we can keep getting all of it.
The other consumer is independent from the first one, but it gets the whole data present
in the buffer with one single sweep. This is an efficient way to catch up.
Since the buffer is empty, the two consumers can register with the buffer and ask to
be notified when more information is added. This happens right after the producer pub-
lishes the next element. In this case the two consumers are unblocked and can get it.
As opposed to the previous solution where we had locks at every step, here we can read
and write information without locks because the consumers will always read one element
behind where the producer is writing.
Once the producer runs out of space for writing the next element and the consumer
runs out of new elements to read, then this is the only point in which we have the syn-
chronization.
388

Disruptor
Next Publish
WaitFor Get
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
✓
×
×
✓
✓
×
×
✓
The disruptor connector was originally implemented for high throughput, low latency
multi-core processing scenarios which occur, for example, with high frequency trading
applications. It can also be found within the pipes of big data processing pipelines, as
long as all producers and consumers are deployed in the same container with lots of mem-
ory available. The assumption is that if the ring buffer is big enough it never gets full as
the consumers will never catch up with the producer, but after the slots have been read
they will be recycled for the next round of writing.
389

Tuple Space
In Out Rd
• A tuple space acts like a shared database, but offers a simpler set of
primitives and persistence guarantees
• Components can write tuples into the space (Out) or read matching
tuples from it (Rd).
• Components that read tuples can also atomically take them out of the
space (In)
• The connector also support different kinds of synchronization (blocking
or non-blocking reads)
The next connector is also a data sharing connector, which improves upon the shared
database because it adds a few interesting coordination primitives.
A tuple space organizes the shared data as a set of tuples. Tuples are a set of typed
values. For example, you can have a tuple which contains the customer name, telephone
number. Another with the account number and the balance. These tuples are all mixed
together into a space. It is not necessary to structure them into tables or predefined
relations. You just throw all the tuples inside the space.
Components can also read them by looking for tuples matching a given structure. For
example to retrieve the phone number of a given customer, they can query the space with
a tuple containing the given name and a typed placeholder, which will be filled with the
number, if a tuple maching the name is present and found.
It is easier to understand the primitives that you can use with the tuple space from the
point of view of the components invoking them. If we have a component that is writing
into the tuple space, it will use the ’out’ primitive. The opposite will be the ’in’ or the
’read’ primitive. What is the difference between them? You simply read whatever tuple is
present that matches the expected structure. Read does not affect the content present in
the tuple space. It is a non destructive operation. However, there is also the possibility
to read tuples and atomically take them out of the space in one shot. This is what the
’in’ operation does: take information from the tuple space and bring it in the component
reading and removing it.
What happens if you try to read something, but the information that you’re looking
for is not present yet? If you query a normal database, when you send a query to read
some information, the database will simply answer: sorry I don’t have any information
that matches this query at this time. Please come back later. If you do the same with the
tuple space, it is possible to actually block the reply until a tuple that matches what you
390

try to read appears into the space.
Thanks to this very simple concept you can actually implement interesting coordina-
tion scenarios between multiple independent components that not only share informa-
tion in the tuple space but they can also use it to coordinate their work.
391

Tuple Space
In Out Rd
Component
Component
Tuple Space
Tuple Space
Component
Component
Component
Component
Out
<X>
Rd <?>
<X>
Rd <?>
<X>
In <?, ?>
In <?, ?>
Out
<"Y",2>
<"Y",2>
Out
<"Z",3>
<"Z",3>
Let’s take a look at a concrete example of how these various primitives can be used. We
have the components connected to the same tuple space in the middle. One component
is going to perform an ’out’ operation to write a certain tuple into the space. The other
component wakes up and it looks for the matching type tuple. Since this is a read oper-
ation the tuple is not removed from the space, but only a copy is sent to the component
which can process it. The same tuple can also be sent to another component, because
the read operation is non destructive.
After both component finish processing the tuple they have read, they perform an ’in’
operation. In this case, there is nothing in the tuple space yet. So they block and wait.
When the other component which plays the role the producer is going to ’out’ a matching
tuple, it will be the responsibility of the tuple space to deliver this tuple to one of the
components that was waiting for it.
At this point the tuple space detects that there are actually two components waiting to
read and will need to decide which one wins the race to extract the tuple that is coming
in. It can use a first come first served policy or some other load balancing algorithm to
deliver it to the first component, which will receive it and unblock.
The other component doesn’t unblock: since the only matching tuple was delivered
to the other one, it is still waiting. However, when the producer performs the next ’out’
with another matching tuple, it will only have one component waiting to receive it.
The blocking reads and in primitives are similar to message bus subscriptions. How-
ever, they’re subscription that disappear the moment the matching tuple is delivered.
This example shows you how we can use a combination of simple primitives to share
and coordinate access over information in a distributed environment. The tuple space
runs like a database server, accepting remote connections from the other components.
392

Tuple Space
In Out Rd
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
✓
×
✓
×
✓
×
✓
×
✓
To summarize, the properties of the tuple space, like a shared database helps to transfer
data. But thanks to these blocking read primitives, and the possibility to atomically read
and delete matching tuples by a set of competing consumers, the tuple space helps to
coordinate them as well.
The tuple space runs its own server for remote components, which interact indirectly
– they just need to agree on the structure and the content of the tuples they are reading
and writing. Otherwise they are completely unaware of the presence and location of each
other.
The tuple space connector is also asynchronous, the various components can come and
go, and as long as the tuple space is available they can interact through it asynchronously.
Its cardinality is many to many, with multiple producers and multiple consumers con-
nected to the same tuple space in the middle routing everything.
393

Web
Get Put Delete
Post
• Components reliably transfer state among themselves using shared
resources accessed with the GET, PUT, DELETE primitives. POST is used
for unsafe interactions.
• Components refer to their shared state with global addresses (URI)
which can be embedded into resource representations (Hypermedia).
If you use the web as a software connector, you can build a shared data structure of
global proportions. And connect to it an arbitrary number of components.
What is the purpose of the web as a software connector? It enables a set of components
to reliably transfer data among themselves using a shared data structure that is stored in
various distributed locations, which can be addressed globally.
Components deployed anywhere can access the Web and can become part of it. It is
enough for them to have a link: the reference to a global address of a shared Web resource.
They will follow it and dereference it to get information from the corresponding Web
resource. Reading is what happens 99% of the times as information gets downloaded
from a website. But it’s also possible to put or post new content associated with a Web
address or even delete it altogether.
The interesting thing about using these global Web addresses is that Web resources are
discovered dynamically by asking a resource where to find related resources. This is the
core idea of hypermedia: decentralized discovery by referral.
394

Web
Get Put Delete Post
Component
Component
Web Resource
(URL)
Web Resource
(URL)
Component
Component
Component
Component
PUT
GET
GET
PUT
GET
GET
DELETE
Let’s see how we can use the Web as a connector to perform these reliable state trans-
fers between different components. Let’s say that there is a component that wants to
inform the others about some data: this component can publish it by associating the
data with a certain Web address. After this is done, the information is persistently stored
at this particular location. This data can be then transferred onto the components in-
terested to process it. When each component is ready to receive it, it will perform a get
request that will retrieve a copy of the current state asociated with the particular ad-
dress. The get operation is non destructive so you can read the current state of resource
as many times as you want and also multiple components can read it at the same time by
retrieving and downloading their own copy to process locally.
Another example: We take information from a component A. We publish it on the Web,
so that two other components can retrieve it.
In these examples the Web acts as a form of shared memory at the global scale, because
each memory address is not located within the local environment in which components
are deployed, but every component connected to the Web can be running anywhere. Also
the address of each Web resource can be found anywhere on the Web (as opposed to be
matched against the tuples stored within a particular space).
The interaction can be more complex. We can have some component decide to update
the data. And this data can be then be read by the other components. It’s also possible
once a resource has been initialized with the first ’put’ to have another component clean
it up and delete it.
This example concludes the whole life cycle of these shared Web resource. This is sim-
ilar to what you would do with a shared database where components can insert data into
tables which then eventually are dropped.
As opposed to the tuple space, there is only one type of read operation which is called
’get’, but there is no way to atomically read and delete a resource. Delete is actually a
separate primitive.
395

The web enables the reliable transfer of state because each of these primitives (get,
put, delete) are idempotent: if there is a failure, you can retry the interactions and even-
tually the data will make it across. If this component is unable to connect to the resource
when attempting to write into it, the component can keep retrying the ’put’ request, and
eventually the data will make it across. The same is even more so for the read operations:
if the ’get’ fails, just retry it as many times as necessary.
Web
Get Put Delete
Post
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
×
×
✓
×
✓
×
✓
×
✓
Let’s summarize the properties of the Web as a software connector. The Web is used
primarily for globally publishing shared information among a very large number of com-
ponents that are interested to mostly read it. However, there is no control transfer prim-
itive associated with the Web HTTP protocol.
The connector is by definition as remote as it can get; all the components are indirectly
interacting through the shared Web resources. Each of the components can be active at
a completely different time. We have observed on the Web that bits published 25 years
ago are still available today.
Also the Web can scale to support a very large number of components sharing the same
resource.
396

Blockchain
AddTx Read
• The blockchain is a trusted shared transaction log built on top of an
untrusted and decentralized network of peers.
• Components may read the transaction history and add transactions to
extend the blockchain
• All information shared on the blockchain is replicated on each peer
• The content of the blockchain can only be changed if the majority of
peers agrees to do so
The last connector of this collection is also the most recent addition. The blockchain
comes into play when you have a large number of components which need to agree on
the value of shared data but they do not have a centralized place in which they can store
a copy of the information that they have to collectively agree about.
What is the blockchain? A fully replicated data structure, whose copies are stored with
every component that needs to access it. By using cryptography, it is possible to check
that the existing data remains unchanged and additions are only allowed at one end of
the chain.
If you look at this particular chain, we can see that this is the initial block, a.k.a., the
origin block. This is followed by other blocks attached to it over time. The most recent
block is being formed with the new information.
From the past you can only read, while if you want to write you can only append trans-
actions from the top.
Is it true that the chain is immutable? Well, it is immutable as long as the majority
of the components does not agree that it’s OK to change it. Since the blockchain is also
meant to scale to a very large number of peers, it will be too expensive to gain control
over the majority of the replicas and therefore there will not be a chance to modify the
past.
Technically, the blockchain is a trusted shared transaction log: a linear data structure
which goes from old entries to newer entries. And you build trust by sharing such trans-
action log among an untrusted and decentralized network of replicas.
The components that used a blockchain as a software connector can use it to read the
past transaction history, which is similar to having a shared, but immutable database.
They can only modify the content found at the head of the log. So the blockchain can
always grow, can never shrink. It is not possible to update existing blocks or to remove
blocks from it. Everything that has been logged in the past will always be remembered.
397

There are also costs involved with all of this because we have full replication of all the
information shared on the Blockchain. The larger the system grows, the more copies you
need to have, so the more storage you consume and also you have to continuously scan
the chain to make sure that nobody has been trying to temper it with it, and this is also
very wasteful in terms of CPU power and energy consumption.
Blockchain
AddTx Read
Data
Control
Local
Remote
Direct
Indirect
Synchronous
Asynchronous
1-1
N-M
✓
×
×
✓
×
✓
×
✓
×
✓
So the blockchain is about data transfer, in particular from components which append
transactions to the blocks to other components which can read these transactions forever.
Blockchain is a highly decentralized distributed system, so it supports remote, indi-
rect and asynchronous interactions between multiple components reading and append-
ing transactions onto it.
398

Software Connector Demo Videos
• Cardinality: 1-1 (Point to Point)
˾. How to exchange data from A to B (and back)?
˿. How to transfer control from A to B (and back)?
̀. How to discover the existence and location of A and B?
́. How to perform a rendez vous/barrier synchronization (A waits for B or B waits for A)
• Cardinality: 1-N (Multicast)
˾. How to exchange data from A to B*?
˿. How to coordinate multiple connected components?
̀. How to safely concurrently modify data shared between multiple components?
• Reliability: What happens if A or B are not available?
• Maintainability: What happens to A if the interface of B is changed?
• Adaptation: How to connect heterogeneous but compatible interfaces? (if possible)
• Richard N. Taylor, Nenad Medvidovic, Eric M. Dashofy, Software Architecture: Foundations,
Theory and Practice, John-Wiley, January 2009, ISBN 978047016774
• Nikunj R Mehta, Nenad Medvidovic, Sandeep Phadke, Towards a taxonomy of software
connectors, Proc. of the 22nd international conference on Software engineering (ICSE 2000),
Pages 178-187.
• Andrew D. Birrell and Bruce Jay Nelson. Implementing remote procedure calls. ACM Trans.
Comput. Syst. Volume 2, Number 1, Pages 39-59, February 1984.
• Martin Thompson, Dave Farley, Michael Barker, Patricia Gee, Andrew Stewart, 
, May
2011
• Gelernter, David. "Generative communication in Linda". ACM Transactions on Programming
Languages and Systems, Volume 7, Number 1, Pages 80-112, January 1985
• Cesare Pautasso, Erik Wilde, 
, pp. 911-920, Proc. of the 18th International World Wide Web Conference (WWW
2009), ACM Press, Madrid, Spain, April 2009.
• Xiwei Xu, Cesare Pautasso, Liming Zhu, Vincent Gramoli, Alexander Ponomarev, An Binh Tran,
and Shiping Chen, 
, 13th Working IEEE/IFIP
Conference on Software Architecture (WICSA 2016), Venice, Italy, April, 2016.
• Cesare Pautasso, Olaf Zimmermann, 
, IEEE Software, 35(1):93-98, January/February 2018
References
Disruptor
Why is the Web Loosely Coupled? A Multi-Faceted Metric for
Service Design
The Blockchain as a Software Connector
The Web as a Software Connector: Integration Resting
on Linked Resources
 
 
 
 
 
 
 
 
399

Software Architecture
Compatibility
and Coupling 8
Contents
• Adapters and Wrappers
• Interface Mismatches
• Adapters and Interoperability
• Adapting between Synchronous and Asynchronous Interfaces
• Scaling Adapters to N Interfaces
• Compatibility and Interface Standards
• Coupling Facets
• Binding Times
400

How is it possible for the train to roll on the tracks? The wheels are the point in which
the train touches the tracks. So that’s the interface element between the rest of the train
and the tracks. The assumption behind their compatibility is that the distance between
the wheels needs to match the distance between the rails.
There is not only one standard setting for this value: depending on the history and ge-
ographic location, the type of train network, there are many different possibilities. The
chosen distance is actually arbitrary, an historical accident. It may be due to political de-
cisions to make the trains compatible or incompatible between different train networks.
The distance has also an impact on the size and the speed of the trains. Spanish trains
have the most comfortable riding experience because they are so much larger. If you are
riding a train across the Swiss Alps, there are lots of sharp turns: it pays off to shrink the
rails distance so that you can make trains climb to the top of the mountains.
What is the connector in this scenario between the train and tracks? Gravity. It’s good
to have such force on your side if you want to keep components together.
401

Compatible Interfaces
• To be connected, component interfaces need to match perfectly
The main assumption that we make when we connect together two components is that
they are compatible. In other words, the interfaces connected by the connector need to
match, perfectly.
But what is it that keeps the two software components together? The coupling between
matching and connected interfaces.
402

There's an app adapter for that!
You can build a whole industry just by solving interface compatibility issues. Provide
people with a solution to connect incompatible interfaces: there is an adapter for that.
You may rightfully wonder if the design of incompatible interfaces is actually inten-
tional. You would not be able to sell any adapters if all interfaces were compatible in the
first place.
403

Adapter
• Adapters help to connect mismatching interfaces
• Adapters deal with transforming the interaction provided by one
to the interaction required by the other interface
• Warning: if data or functionality are missing the adaptation may
be impossible
When we work with software we assume that the component interfaces have to match
perfectly so they can be connected. If you have two components but their interfaces do
not match and they are not compatible, then you have a problem.
Sometimes you can solve the problem by building an adapter. The first part of the
lecture is going to be about: how do we work with software adapters? and what kind of
incompatibilities can be solved with adapters and which ones are impossible?
What is an adapter? It’s a specific kind of software connector which is dedicated to
solving interface mismatches.
To deal with such mismatches, some kind of data transformation might be needed. As
each interface comes with their own data model defining the data types that the interface
understands, and can exchange with other interfaces sharing the same data types and
structures.
The first function of the adapter is to convert between different data representations,
between different formats within the same semantic domain.
But there is more than just incompatible data representations. There are also different
types of interactions. For example, you could have the blue component working with an
API based on synchronous calls, and the red component assuming that it depends on a
message-based asynchronous interface. Adapters may also need to transform between
asynchronous and synchronous interactions.
Is adaptation always possible? Not always: if the data that you depend on, or if the
functionality that you require is missing. The adapter cannot find it anywhere within
the interface it is trying to adapt. In this case, the adaptation would fail, unless you
would overload the adapter with a re-implementation of missing functionality or make
it stateful by storing the data which is not found in the original interface.
This goes beyond what an adapter should be dedicated for: it is after all just a con-
nector between two components, rewiring the pins. But if you go back to the hardware
example, you will be surprised by the complexity of some of those adapters. Not only are
the interface incompatible, but missing implementations have been outsourced into the
adapters themselves.
Here is the adapter: with it we are able to connect two components even though their
original interfaces are not compatible. Remember: adaptation is not always possible.
404

Wrapper
• The mismatching end of the adapter is hidden inside a wrapper
component
• The adaptation logic is encapsulated within the wrapper and
made reusable
What is a wrapper? A wrapper makes the adapter reusable by hiding it inside a com-
patible interface.
The Blue Interface is the original one, which may be incompatible. The Red Interface
is the interface of the wrapper, which allows to directly connect to other components
without these components realizing that the component is actually incompatible with
what they require.
The red interface is compatible because it wraps the incompatible (blue) one. From the
outside, you do not see the old interface, but you can use the adapter directly.
Wrapping is a combination between adaptation and abstraction; adaptation with en-
capsulation gives you a wrapper.
405

Mismatch Example
id upload(user, image); 
image download(id); 
setTitle(id, title); 
title getTitle(id); 
ids[] list(user);
A
id upload(user, image, title);
{user, title, time} getImageMetaData(id); 
image getImageData(id); 
ids[] list();
B
Are Interfaces A and B equivalent?
Yes (A can replace B and vice-versa)
A can be replaced by B
B can be replaced by A
Not completely
×
×
×
✓
This examples shows you the type of problem you have to deal with when attempting
to connect two mismatching interfaces.
Equivalence means that it is possible to write an adapter which transforms one into
the other and vice versa. Is one interface replaceable by the other and vice versa? If this
is true for both, then they are equivalent.
In general, it’s unlikely that a pair of arbitrary interfaces can be considered as equiva-
lent, because there is always going to be some feature that is present in one interface but
missing from the other.
It is more likely that it is possible to do the replacement in one direction only. It may
be possible to have an adapter from one to the other interface, but not the other way
around.
Also if it is not possible to build an adapter for the complete interface, you may be lucky
that your component does not require the whole interface but only a subset for which the
adaptation is possible.
Consider the example and try to answer the question before we try to write the two
adapters.
406

id upload(user, image); 
image download(id); 
setTitle(id, title); 
title getTitle(id); 
ids[] list(user);
A
id upload(user, image, title);
{user, title, time} getImageMetaData(id); 
image getImageData(id); 
ids[] list();
B
Partial Wrappers
A
B
A➙B
id upload(user, image) { 
    return b.upload(user, image, "");
} 
image download(id) { 
    return b.getImageData(id);
} 
setTitle(id, title) { 
    ?
} 
title getTitle(id) { 
    return b.getImageMetaData(id).title;
} 
ids[] list(user) { 
    return b.list().filter(user);
}
A
B
B➙A
id upload(user, image, title) { 
    id = a.upload(user, image) 
    a.setTitle(id, title) 
    return id;
}
{user, title, time} getImageMetaData(id) { 
    return {?, a.getTitle(id), ?}
}; 
image getImageData(id) { 
    return a.download(id);
}; 
ids[] list() { 
    return a.list("*");
};
In the example, it’s not possible to completley replace one with the other.
Let’s start from A to B: we have a component that depends on A and we try to write an
adapter that implements A’s interface using B’s interface.
There is one property offered by A, which has the ’getTitle’ and ’setTitle’. While we
can map the property read to the ’getImageMetaData’ getter, we cannot find the corre-
sponding feature in B. A workaround would be to re-upload the image every time the title
changes, but this would invalidate the image identifier and fill up the B implementation
with duplicate images. The adapter would need to keep track of the identifiers associated
with duplicate images, thus becoming stateful.
If you switch perspective, and try to go from B to A, then we spot one case in which we
have a very broad operation: the ’getImageMetaData’ which returns all the metadata for
a certain ID. If you look at what is available to do the same in interface A, we only have
the title, so you will be able to return the title, but you will not be able to return the user
or the time unless you keep track of those inside the adapter. To do so, you will have to
make a stateful adapter that remembers when the image was uploaded as well as which
user is associated with it.
Adding state to an adapter introduces redundancy (overlapping information about same
item is stored in different places) which leads to potential inconsistency.
Stateless adapters simply map or transform interfaces. Stateful adapters do so but also
need to remember the entire history of the interactions with the interface and their life-
cycle becomes tightly coupled with the one of the component they are adapted.
If you really need to add state to an adapter, then make it a wrapper, so that you can
hope that all interactions with the mismatching component go through the adapter. By-
passing the adapter and using the original interface may render the adapter state incon-
sistent.
407

Types of Interface Mismatches
• Cannot be solved with adapter:
• Missing required feature
• Can be partially solved with adapter:
• Different data types
• Can be solved with adapter:
• Same feature with different name/order
• Operation granularity (one vs. many)
• Data granularity (one structured parameter vs. many simple
parameters; scalar vs. vector)
• Interaction (e.g., synchronous vs. asynchronous)
In general, if we consider what types of interfaces mismatches are there, we can see that
if a required feature is missing, we cannot solve this with an adapter. Required features
missing from the provided interface are a big problem, because there is no adapter that
will actually solve it unless you’re willing to re-implement the missing feature inside the
adapter. But that’s not the job of the adapter.
There is another case in which we can solve it partially with an adapter if, for example,
we have different, but partially overlapping data types. If you have to transform integers
into floating point numbers. All integers are floating points, but if you go the other way
then you have a rounding problem.
There are other cases in which it’s easy to do the adaptation. For example, if you have
a different name for the same thing, you just have to rename it. If you have the wrong
granularity, it’s possible to decompose a large operation by composing the small ones
offered by an interface. It’s more difficult to do the opposite with a stateless adapter.
Mapping many fine-grained operations on top of a coarse-grained interface may require
to accumulate all the small calls and then make the big call in one shot. But how do you
know when you have enough data to pass on to the coarse grained interface? And what
if the last piece of information never comes? The big call will never happen.
Mismatches can be found at the level of operations, events and properties but they can
also be at the level of the data models. There can be APIs that work with individual data
items and others which work in batch mode with collections of items. It can be more
efficient to transfer a whole batch of objects that have the same structure as opposed to
having to make calls for every separate object.
Another case that is a very challenging concerns mismatches in the type of interaction.
The data is compatible. The operation is exactly the same, but there is still a difference:
one interface is synchronous while the other is asynchronous.
408

Synchronous vs. Asynchronous
Interfaces
Synchronous
y = f(x)
Block the caller
Require polling/busy waiting to detect
events
Easier to program with
Asynchronous
f(x,(y)=>{})
Use callbacks for event notiѹcation
emit(x) on(y=>{})
Closer to the hardware
Let’s compare synchronous and asynchronous interfaces. You can recognize the dif-
ference in the code snippets and also if you can see what properties, challenges or con-
straints each comes with.
In some scenario, it is actually useful to have a synchronous interface, and in other
scenarios it is better to have an asynchronous one. So it is not always clear which one to
pick.
And as a result, it’s unavoidable to have both in the same architecture, and as a conse-
quence you have to be able to bridge this mismatch.
Synchronous interfaces involve calls: you make a call, wait until you get the answer
and, as a consequence, you – the caller – are blocked. You will need to wait until the
result comes back. Since there is nothing simpler to program than calling a function,
they are the easiest to write your code against.
Synchronous interfaces have a little drawback. What if the purpose of the call is to
monitor state changes inside the component being called? In other words, you want to
detect if an event has happened, then synchronous interfaces they require busy waiting.
You have to keep calling to retrieve the latest state until you detect something happened.
Or you call once and remained blocked until something happens. Both are extremely
inefficient ways to detect events: they keep the caller busy. And they also consume re-
sources on the component being monitored because it’s being called all the time.
The interface would work much better if you can just have a callback to notify about the
event. And this is the main reason why we have asynchronous interfaces. You can make
a call and pass the callback to receive the result. Or you can just use different primitives.
You can have event-based or message-based interfaces to receive asynchronous event
notifications. Whenever our interfaces encapsulate hardware components, within the
components the software will be triggered by interrupts, signaling low-level events. For
example, if you have an operating system API that gives you access to the mouse. You will
be able to observe the mouse moving by receiving an event that tells you its new position.
You can also check what is the current position with a call, but you shouldn’t keep asking
for the current state all the time if you want to efficiently track the movement.
409

Synchronous and Asynchronous
How to mix?
Part of our architecture works synchronously (with call-based interfaces) and the other
part works asynchronously (with messages or event-based interfaces).
The challenge is: how do we mix them? How do we come up with an adapter? How
to connect synchronous and asynchronous interfaces? How can we transform between
calls and messages? We are going to use an adapter, which will be different depending
on whether we transform one synchronous call into the exchange of two asynchronous
messages (the request followed by the response) or vice versa.
410

Half-Sync/Half-Async
Use an adapter hiding asynchronous interactions
behind a synchronous interface (or viceversa)
Sync2Async adapter: buffer synchronous calls using queues and map
them to low-level asynchronous events.
Async2Sync adapter: invoke synchronous interfaces when receiving the
corresponding message and emit another message with the call results.
How to connect synchronous (call)
and asynchronous (message queue) interfaces?
Good news: the mismatch can be solved in both directions.
If we map from synchronous to asynchronous, we have a synchronous call coming in.
We have to buffer it using message queues. The incoming call produces an outgoing re-
quest message. The adapter listens for the event representing the arrival of the incoming
response message. When this happens, the adapter is able to answer the call and unblock
the caller.
We can also have an asynchronous clients that needs to be interfaced and connected
with synchronous interfaces. Also in this case, the adapter will be listening for inbound
messages from the asynchronous client. When the message arrives, we can extract the
input parameters needed to make the call. When the call returns, we emit the outbound
message with the results back to the original client.
411

Half-Sync/Half-Async
Adapter
Synch
Client
The adapter converts from synchronous calls to asynchronous messages
Asynch
Interface
1. Call
Request
2. Send
3. Receive
4. Send
5. Receive
Response
We start from the case in which we have a synchronous client which needs to interact
with an asynchronous interface, which expects to be connected to a message bus.
The adapter bridges between two different connectors. On the left side we have a syn-
chronous call. On the right side, the asynchronous message bus. You cannot directly
connect them together. By now we have seen that is possible to solve this mismatch with
an adapter.
How does the adapter actually work? The interaction begins with the call, intercepted
by the adapter. Then the call gets transformed into a request message. The message
goes out on the bus. The message is sent on the bus with the call input parameters by
the adapter. The asynchronous interface is going to receive it directly from the bus. The
asynchronous interface will do whatever it has to do when such messages arrive. Even-
tually it will produce a response message, which will go out on the bus. The adapter will
be listening for it, and will receive it. Once the response has been received, it can be
delivered to the synchronous client.
From the perspective of this client it looks exactly as if it was interacting with a syn-
chronous interface. Behind the adapter we have something completely different: a bus
with asynchronous interactions.
412

Sync to Async Adapter
Client
Client
Sync Interface
Sync Interface
Adapter
Adapter
Bus
Bus
Async Interface
Async Interface
f(x)
f(x)
wait
receive
send
receive
notify
y
send f,x
f,x
y
y
f(x) { 
    var result; 
 
    bus.onreceive = (y) => { 
        result = y; 
        notify; 
    } 
 
    bus.send(f,x); 
 
    wait; 
 
    return result;
}
413

Half-Async/Half-Sync
Adapter
Synch
Interface
The adapter converts from asynchronous messages to synchronous calls
Asynch
Client
Response
Request
1. Send
2. Receive
3. Call
4. Send
5. Receive
The structure remains the same. However, the order of the interactions is inverted.
The asynchronous client is connected with the bus, which delivers the request message
to the adapter. When the message arrives, the adapter converts it into the call. The call is
synchronous and blocking the adapter until the result is returned. The adapter packages
the call result into the response message, which is sent back to the client.
414

Async to Sync Adapter
Client
Client
Bus
Bus
Async Interface
Async Interface
Adapter
Adapter
Sync Interface
Sync Interface
send
receive
onreceive
f(x)
y
send
receive
f,x
f,x
y
y
//Async Client 
f(x,c) { 
 
    bus.onreceive = (y) => { 
        c(y); 
    } 
 
    bus.send(f,x);
}
//Async to Sync Adapter 
 
    bus.onreceive = (f,x) => { 
 
        var y = f(x); 
 
        bus.send(y); 
    }
Half-Sync/Half-Async
• Beneѹts of adaptation:
• Simplify access to low-level asynchronous interfaces (no need
to deal with buffers, interrupts, and concurrency)
• Visibility of interactions (all communication happens through
the message bus)
• Disadvantages:
• Potential performance loss due to layer crossing (data copy,
context switching, synchronization)
• Mapping requires each call to match one pair of messages
To summarize, we cannot build systems only with one type of connector. It’s too sim-
plistic to assume we will just make calls everywhere.
415

You have to be able to mix asynchronous messaging with calls. You have to mix data
streams with shared databases and all different kinds of connectors we have seen.
Interfaces are affected by the expectations that they make on the type of connectors
that you can use with them.
For example, if you have an interface that is close to some low-level stateful hardware
device, you will need to be able to query its current state but also efficiently monitor state
changes by listening for events. This can be complicated, so in some scenarios having a
simple blocking call that returns when the event is detected can be easier on the client.
Calls are also more difficult to monitor, unless you can inspect the content of the stack
of the current thread. Message buses can easily log and trace messages in transit. This
makes it possible to observe at runtime the actual interactions and the communication
flow between various components.
Every adaptation has a penalty: You cannot just use an interface directly, but you have
to convert. You have to copy data. Maybe you have these locks which you need to ’noti-
fy/wait’. A local call is very efficient, while transforming it into four different calls to send
and receive two messages that go back and forth in the bus can add significant overhead.
If you look closely, you will be surprised how many times this sync/async adaptation
happens along the whole software stack. One asynchronous layer is hidden under a syn-
chronous one, but this one again is turned into an interface which is asynchronous and
so forth.
416

How many Adapters?
Two Adapters
via intermediate 
standard interface
N
N
2
One
Point to Point
Adapter
So far we have discussed why we need adapters to connect incompatible interfaces and
how we can use encapsulation together with adapters to build wrappers of components
so they present an interface that is more compatible than their original one. We have also
discussed different types of interface mismatches. Some can be resolved and others are
impossible to resolve with adapters. And then we looked at how can we build adapters
that help us to connect pairs of incompatible interfaces and in particular focused on the
problem of mapping between synchronous and asynchronous interfaces.
In this part we are going to work on: how do we scale the notion of adaptation to work
with more than two interfaces? And we will see how standards play an important role
in that. We will conclude by looking at the other effect of connecting together two com-
ponents: they become coupled together. We will discuss different facets of the coupling
concept, and in particular we will highlight when coupling is established.
How can we scale adaptation to multiple incompatible components? If you look at this
picture, every square has a different color, meaning that has a different interface. Each
line connecting the boxes means that it’s possible to do the adaptation: it’s possible to
convert from yellow to orange using a specific adapter. If you have some information
inside the yellow component and you would like to transfer it over to the orange one,
you can do so because the adapter can transform it from one format to the other.
You can limit the number of adapters but go through multiple adaptations to transfer
information along the outer rim, or you can transform directly between the source and
destination format. It’s always possible to build an interoperable architecture by adding
more and more adapters and eventually you reach the format of the level of compatibility
that you need. Sometimes this is inefficient because there are many transformations that
need to be executed every time you go through one of these edges, there is a cost to pay.
417

And also there is a risk that the transformation could lose information, which is fine for
a direct adapter but could make it challenging to compose multiple adapters.
What if you draw an adapter directly between every pair of components that you want
to connect? If you take this to the ultimate consequences with a fully connected graph,
you end up with a lot of adapters. Every time you add one more component to your
architecture and you want to integrate it with the rest of the system, then you will need
to make sure that this component can interact with every other component into your
system. Every time you do so, you have to add a larger and larger number of adapters. It
doesn’t scale as it gets increasingly more expensive with quadratic complexity.
To control the complexity we can introduce a standard: an intermediate interface to
which all of the component can be converted to and from. In this case, with only two
transformations, we can go from one side to the other. Instead of going directly from
yellow to orange, we go from yellow to black and then from black to orange. We can go
from this one to the intermediate, and then from the intermediate we can go everywhere
else.
This is an important concept that helps you to reduce the complexity of the scalable
adaptation problem. You do not have a direct transformation anymore. You have a com-
bination of two transformations, but this is a compromise compared to the earlier sce-
nario in which you had to go through a transformation all around the outer rim of the cir-
cle and use up to N/2 combined adapters. So here we are in the middle. This is sometimes
called a hub and spoke architecture. We go through the center so that we can convert to
the standard, and then we have access to everything else. If we want to add one more
component to the system like before, then it’s much less expensive: all we have to do is
write an adapter that converts between the new component interface and the standard
and vice versa, as opposed to building a mapping with every other existing interface.
418

Scaling Adapters with N Interfaces
Use a standardized intermediate representation and
communication protocol
Before they can be connected, incompatible components need to be
wrapped with adapters. These adapters enable interoperability as they
implement a mapping to common means of interaction.
How to enable communication between many different
heterogeneous interfaces?
If we take a heterogenous system with many different interfaces and we want all of
these to be compatible and, more precisely, interoperable, the solution is to first develop
a common standard; then build adapters which translate every local interface to the com-
mon standard and the interoperability problem is solved.
How does such standard emerge? How is it possible to find such a common represen-
tation? It sounds deceptively easy, but it’s difficult to achieve in practice: Do you seek
the minimum common denominator? Does the standard cover the intersection (to make
sure all mappings are feasible) or the union so that every interface will be fully covered
and fully represented, but some adapters may disregard or need to provide defaults for
some data elements which are missing from the local interface.
419

Composing Adapters
Component
(Platform A)
Component
(PlatformB)
Adapter
(Platform A)
Adapter
(Platform B)
Components of different platforms can interoperate through
adapters mapping the internal message format to a common
representation and interaction style
If we look at this pattern from a process viewpoint, the interaction between mismatch-
ing interfaces happens through a two step transformation. First, from the local platform,
from the local language, from the local representation, we go through an adapter that
converts it to the standard (shown in green). Then we do the reverse.
Such transformations have to work in both directions: from local to global, and from
global to the other local. This is the minimum requirement for mono directional inter-
actions. If we need a reply to go back in the opposite direction, then we need adapters
able to perform the inverse transformations.
420

One or Two Adapters?
• Why two adapters?
• For Point to Point solutions one adapter is enough (direct
mapping between two interfaces, message translator)
• In general, when integrating among N different
heterogeneous interfaces, the total number of adapters is
reduced by using an intermediate representation (or
canonical data model)
In case you have to do a quick integration between two components, you don’t have
to come up with a standard. This is called a point to point integration, where you just
design a direct mapping with one adapter.
In general, if you foresee the need to scale the system to a larger number of heteroge-
neous interface, it pays off to invest into a common standard so that you can later keep
the cost of further growing the system under control.
Sometimes this standard is called intermediate representation, or canonical data model.
You can find these standards in any kind of integration architecture spanning thousands
of different applications, which work together because they agree on a common canonical
data model.
You find it as well inside the architecture of a compiler pipeline. First you parse the
input language, the result is an abstract syntax tree (AST), which works like an interme-
diate representation, as different stages of the pipeline operate on it performing different
transformations, optimizations, and code generation for specific backends. Having such
intermediate representation, separates (or de-couples) the parser of the input language
from the back-end emitting the target language. And it even makes it possible to gener-
ate code for different processor instruction sets without having to change the parser.
421

Reusable Adapters and Performance
• Adapters do not have to be re-implemented for each
component, but can be generalized and shared among all
components of a given interface type
• This pattern helps to isolate the complexity of having to deal
with interface differences when building each component. The
adapters make the heterogeneity transparent.
• Warning: performance may suffer due to the overhead of
applying potentially complex transformations at each adapter
(better to design compatible interfaces if possible)
If you target a standard, adapters can become reusable. You can generalize these type
of transformations between common formats so that as long as you can map your local
interface to the standard, then there can be many different reusable transformations that
you can use to connect with other standards.
Be aware: by resorting to one or more combined adapters, there is a performance
penalty because the more layers that you have to traverse, the more transformations or
adaptation layers you have to inject, the longer it will take for the interaction.
As a design goal, it is much better to try to be compatible in the first place. After all,
adaptation is always a backup strategy to recover compatibility, but if you can design
something to be compatible in the first place, by all means do it.
422

It’s always possible to attempt to come up with a universal standard that fully covers
all the previous use cases partially embedded in existing standards.
As a consequence, here is yet one more standard.
And that’s how standard proliferate.
Typically the young developer, or the inexperienced architect will attempt to do that:
create a new standard. You – as a student – haven’t seen this happen before in our indus-
try, but after you get a bit more experience you will see that this standardization cycle
happens all the time.
People try to come up with better standards. This is progress. But for every new stan-
dard, we seem never to be able to get rid of the legacy.
This is why I would like now to open a little parenthesis on where standards come
from. As you may have noticed, standards are particularly important when we assess the
compatibility of component interfaces.
423

On Standards
Standard
Implementations
de facto
de jure
• Standard compliant component interfaces are compatible and
their implementation replaceable
• Component vendors need to comply with standards to get to
the customers depending on them
• Claims of standard compliance should be tested and
independently certiѹed
If an interfaces is standard compliant, it means that clients that depend on the inter-
face being compatible with the standard will be able to connect to the interface. Standard
compliance makes life easier for the clients which needs to interact with your interface.
And at the same time you also make life easier for clients to replace the implementation.
The interface remains the same, it’s standard. The implementation can change without
affecting the clients.
We can observe a fundamental tension between vendors competing to give you the best
implementation for the standard, and customers having a low cost in switching between
different implementation which are standard compliant and thus easy to replace.
For vendors, there is always the temptation to go beyond the standard to offer com-
ponents that yes fulfilled the standard interface, but then offer you extra proprietary
interfaces. These may fill the gaps of the standard (with useful non-standard features)
or are just designed to be more convenient to use than the basic standard.
Once the clients are hooked, they start to depend on the standard as well as on the
non-standard part of the interface. This is the vendor lock-in problem. We originally
choose this component due to its standard-compliant interface, but then we started to
use and depend on the other features so much that we are no longer able to replace it.
This ensures the future of the vendor of the specific implementation, who has now a pool
of captive clients.
Whenever you see a label on a component stating compliance with a certain interface
standard, you shouldn’t believe this claim. It can be just a marketing ploy. As you review
a checklist of features, it is cheap to add a mark under standard compliance, but more
expensive to provide a test suite or have an indipendent certification of such label. If
you don’t test it, later on you might encounter some surprises, for example, discover
that the standard was only partially supported and that the exact feature you need has
not yet been implemented.
424

On Standards
Standard
Implementations
de facto
de jure
Which is ѹrst, the standard interface or its compliant
implementation?
• De facto standards promote existing proven and successful
implementations (winner takes all, ѹrst mover advantage)
• De jure standards may still need to be implemented (design by
committee)
Where do standards come from? This is like a ”chicken and egg“ problem. Do you start
from the standard and then you implement it? Or would you rather find some existing
proven implementation that becomes standard?
Here we distinguish: de facto vs. de jure standards. If we start from a standard spec-
ification and build one or more implementations from it, we have a de jure standard.
Sometimes there is only one implementation that is so fast to the market and so power-
ful, so successful, that becomes – even if there’s no formal standard – a de facto stan-
dard. Everyone knows that in practice this is the only commonly accepted way to solve
that problem.
Being the original developer of a de facto standard gives you first mover advantage and
control over who is allowed to implement it after you and who is allowed to actually use
it.
What if we implement a standard only after it has been specified? This is a bit risky:
how can one standardize something without knowing yet how to make it work? And how
can you come up with a design for your interface that is worthy of standardization if no
implementation already exists? There is a risk that if you have too many people around
the table you get what is called design by committee. The result often is sub-standard in
terms of quality (e.g., lack of simplicity) so there is uncertainty on whether it will actually
be feasible to implement in a timely manner.
After the initial version, standards typically iterate by cleaning up the specification
by incorporating feedback from attempts at its implementation. Committees will stan-
dardize, developers will implement, feedback to clarify and improve the standard will be
provided. Not necessarily in this order.
425

On Standards
Standard
Implementations
de facto
de jure
Who owns the standard?
• Open Standards: everyone can participate and implement them
• Closed Standards: the owner controls who can implement them,
competitors usually not allowed.
Standards may have an open nature. For example, if there is an open source, refer-
ence implementation and everybody can fork it, improve it and integrate it into their
products. With an open standard, everybody shares its ”intellectual property“ and can
participate in a standardization process based on “rough consensus and working code”.
When the standard is about to be released, there is the possibility for anybody to com-
ment and provide feedback, and vote to finalize it. Also, there is no limit to who can
actually implement it.
The Alternative is to have a closed standard where there is a clear ownership of the
technology and the interface design, as well as strict control over who is allowed to im-
plement it. If you want to re-implement a certain API, you have to get permission first.
And to get it you may have to pay some licensing fees. And if you are a direct competitor,
maybe you’re not going to be allowed.
There was once a big controversy going on between Oracle and Google concerning
whether it’s legal for Google to give an alternative implementation of interface stan-
dard called: ’Java API’. Google has reimplemented it for their Android operating system,
but Oracle has bought it from Sun. While Sun was promoting Java, they took an open
standardization strategy. But after Oracle bought it, the strategy changed, and this gave
rise to a legal battle about whether it’s possible to actually have multiple implementa-
tions for an interface unless the owner of the interface allows it. And this can have a big
impact on the future of the software industry, because if you can control a software API
and you can use the law to limit who is allowed to implement it, this gives you significant
monopoly power. The U.S. Supreme Court eventually declared that Google’s usage fell
well within their rights to do so. Now if only someone would try to provide alternative
implementations of popular Web APIs such as the one of Twitter and Facebook.
426

On Standards
Standard
Implementations
de facto
de jure
Who writes standards?
• IETF, W3C, OASIS, IEEE, OMG
• ISO
• SWIFT (Financial Messaging)
Where do standards come from? They come from standard organizations, like ISO:
the International Organization for Standardization. Within the information technology
sector, we have standards that are related to networking, hardware, modeling notations,
programming languages, as well as software interfaces.
I also added the SWIFT standard to the list in case you’re interested about banking and
financial messaging. That’s just an example of a domain specific standard for ensuring
the compatibility of a certain class of applications.
This should give you an idea of how compatibility is achieved. Ultimately compatibility
comes from the designer providing interfaces agreeing with the developers who will be
consuming them.
One way to reach and encode the agreement is to vote on a standard spelling it out
explicitly. If we focus on interfaces, there are several aspects that can be standardized.
427

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
When you would like to successfully connect your component with the interface of a
separate component, you need to achieve interoperability between them with or without
an adapter. You have to first make sure that the two parties can communicate. Whatever
connector you choose, you will need to make sure that it is possible to transfer some data
from A to B. How can the communication work?
First, there needs to be an agreement on the type of transport protocol. If you’re de-
ploying A and B in a distributed environment, the communication happens through the
network. If they are co-located, then we know that we can use other kinds of connectors
like for example shared memory. In this case the transport is a bit simpler, but no matter
what type of transport you use, you are going to exchange some data that has certain
representation format, using a certain syntax.
There has to be an agreement between the two sides about which syntax they intend
to use. When A writes out the message as a sequence of bits or bytes across the transport
the bits appear on the other side and then you reconstruct a message carrying the same
content.
Once you have exchanged the bits, then you have to give them a structure so that their
semantics can be understood.
For example, the recipient has to understand that a messages about a payment just
arrived: this is the amount of money, and this is the currency. So you have to assume that
both sides agree not only on how do they represent the amount of money (using integers
or floating or some other way to encode financial quantities) as well as the currency.
Maybe it’s just a string, but you don’t have to come up with your own possible expected
currency values, as there are standards listing all possible currencies that you can pick
and choose.
If both parties cannot agree on which standard works best, then you would need an
adapter to convert between different formats and mismatching data models.
You want to connect components to invoke operations so the components can call each
other. And the set of operations that you can choose from is also a good candidate for
standardization. This way there would be an agreement on what is the set of operations
that the interface provides, as well as what each operation actually means. The same can
be extended to all the interface features: properties, events and the data model.
You also can have a standard way of locating or addressing the interface that you want
to interact with. So we need to be able to identify it. We need to know where it is, and we
need to know how to reach it.
These are all various aspects, which standardization efforts have been devoted to. As
a result, there is a whole technology landscape related to how to construct interopera-
ble and compatible software interfaces. Let’s use this as a map to position, classify and
compare different tools.
428

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
Representation Format (Meta-Model)
Data Model
Plain Text
XML
RSS, HTML, SVG, SOAP
JSON
JSON-LD, GeoJSON
Binary
Protobuf
Regarding the representation of the data being exchanged across, the first decision that
you have to make is whether the data will be sent over the network as text or as binary.
Sending text messages, which can be read by a text editor, doesn’t necessarily make them
human readable, but at least they can be copy and pasted into an email, for example. Or
if you choose binary, you can use all the bits (increase the information density of the
message) while making it more difficult to view and debug without custom tools.
If you prefer to stay with a readable format, a bit less dense and less efficient in terms
of bandwidth, and then you can choose for example between JavaScript object notation
(JSON) or the extensible markup language.
The valuable property of these formats is that they prescribe a syntax but they don’t
assume anything about the actual structure of the messages that you are going to send
following their syntax.
That’s the reason why XML has an X because its syntax is used to define other markup
languages by fixing the set of tags and their semantics. So from XML we can have HTML
(for Web pages), SVG (for vector graphic images), SOAP (for messages), WSDL (for service
interface descriptions), RSS feeds (for event logs).These are all specific formats that once
you choose to go with XML, then you still need to make another decision which will set
the exact type of XML document to be exchanged.
The same is true for JSON, which gives a more lightweight representation for the struc-
ture of objects and you can add more assumptions about the field names, or the conven-
tion that used to name the fields. This way you can obtain, for example, the GeoJSON
format to exchange your geo-located datasets. Or JSON-LD, to represent linked data,
with documents that carry references to other documents.
This little decision tree should help you to get started with the interface design deci-
sion: How am I going to pick a representation syntax? Before you try to define your own,
be aware that there are many standards to choose from. Picking one of them, would make
it much easier for others to send messages to your interface, or to read information your
interface provides them.
429

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
• Operations
• HTTP Methods (GET, HEAD, PUT, DELETE, POST, PATCH,
OPTIONS)
Just one brief example about the operations, showing the most widely successful ex-
ample of standardizing the set of operations that you can perform on a certain interface.
The methods that are part of the hypertext transfer protocol (HTTP) offer a limited, but
clearly distinct, set of operations that any Web resource can perform. Anywhere on the
Web you will find billions of different resources, which let you read their whole state
(GET), or just a subset (HEAD). Some also support updates (PUT) and can be deleted. You
can call the resource and perform arbitrary computations (POST). You can also incremen-
tally modify the current state (PATCH).
Additionally, you can perform reflection: ask the interface what is it that you can do
with it. OPTIONS is a meta method, which tells you which subset of the previously men-
tioned methods this particular resource can perform. So you don’t have to assume in
advance which operations are actually there, but you can discover them dynamically.
430

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
• Protocols
• HTTP (Hyper Text Transfer Protocol)
• SMTP (Simple Mail Transfer Protocol)
• MQTT (Message Queue Telemetry Transport)
• AMQP (Advanced Message Queue Protocol)
• XMPP (Extensible Messaging and Presence Protocol)
The chosen transport protocol helps to exchange data from A to B. Are you going to
work with sockets? or can you choose a higher level protocol? So here are a few exam-
ple protocols that on the networking stack are above TCP or UDP. If you are interacting
with a remote Web API, you will probably use the HTTP protocol. There was a time in
which people were trying to also use the email protocol (SMTP) to do the same. Then,
other, more appropriate, messaging protocols emerged like MQTT or AMQP. This is also
a space where you find many proprietary or de-facto standards for transporting messages
between two components. Usually it takes decades before the dust settles down and peo-
ple have found an agreement on which is the winner protocol.
Your choice may depend on what kind of device you plan to deploy your software on.
Will the device be part of the Internet of Things? Or the Web of Things? Will it be a cloud-
based service? Each of these comes with different presumptive transport protocols, the
kind of protocol you would be expected to choose by default.
431

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
• Addressing
• URI (Uniform Resource Identiѹer)
• UUID (Universally Unique Identiѹer)
• DNS (Domain Name System)
• IPv4, IPv6
Another important aspect is about: where do we find the interface? how do we address
elements found within the component interface?
To solve these problems, we can use again networking standards. For example, IP ad-
dresses: they have run out for IP version 4, but we can have a bigger address space with
IP version 6. We can then use a standard registry for transforming symbolic addresses
into the numeric IP addresses: the Domain Name System (DNS).
We can choose to take advantage of Web technology. For example, URI and URI schemes
help to invent or reuse powerful mechanisms to structure address identifiers.
Another problem related with addressing is: who is responsible for producing new ad-
dresses? Who is managing the addresses? And who is giving names to things? And if you
work with the UUID scheme then it is possible to decentralized that operation. All the
components in your system can come up with unique identifiers without having to agree
beforehand. This independence is more difficult to achieve, for example, with the DNS,
which is more hierarchical, but still depending on top level domains, where centralized
control over how addresses are handed out is provided.
If you think about Java and Java packages, how are you naming them? Basically you’re
following a convention to use the reverse DNS symbolic address. If you own this domain,
you can name your components after your domain. Noone else should be able to do the
same.
432

Standard Software Interfaces
Transport
Representation
Syntax
Content
Semantics
A
B
Operation
Addressing
• Interface Description (Metadata)
• Schema Deѹnition Languages (XML Schema, JSON Schema)
• Interface Description Languages (WSDL, Open API, Async API)
• Data Semantics (schema.org)
There is another level which is about making sure that we can agree on how to describe
the interface. It’s not about the content of the interface itself, but it’s about the languages
and the tools that we use to model and represent what is inside the interface.
To do so, we can benefit from standard data management technologies. Schema lan-
guages describe a data model: What information does this software interface consume
or produce? Just specify the schema. Even though you’re not necessarily going to write
queries for that schema or store data according to the schema, but you have a schema
that tells you: given a certain message, does the content of the message fits within the
expected schema? If the message doesn’t comply within this schema, it can be filtered,
bounced back, or simply discarded and ignored.
In the same way that you work with the type system to compile your code and statically
check the validity of object structures based on their classes. Here we can check the
messages as they arrive and validate them whether they fit within the constraints of the
corresponding schema types.
The schema is only the data modeling part of your interface. The interface, in addition
to the data model, also has the operations, properties and events. Those are the focus of
interface description languages (IDLs). For example, OpenAPI for Web based APIs or the
new AsyncAPI, focusing on message-based interfaces.
You can further constrain messages by specifying not only their syntax, structure but
also semantics of the content. This way who is trying to use your interface would know
what the data actually means. And the purpose of each operation. In addition to the
plethora of Semantic Web languages, it is worth looking at the conventions followed by
schema.org.
If you’re interested about this, you can follow these pointers to find examples of what
it means to describe interfaces, including their semantics.
433

What is the effect of connecting together two components? We couple them together.
There is a benefit because connected components can interact, they can exchange infor-
mation and communicate, they can transfer control and coordinate their work. They can
call each other.
But there’s also some other less positive effects. For example, they depend on each
other: what could possibly go wrong? If you need a component and the component is
not there, maybe there is some consequences for you: you could have cascading failures,
propagating along the fault lines of your architecture, induced by coupling.
What if the other side is going to change? You make a change on one component (either
the implementation or its interface). Since you are connected and coupled to it, is this
change going to affect you? It depends. If you simply touched the implementation, you
have a chance to control the impact of the change. If you modified or removed existing
features of an interface, then you are looking for trouble.
434

Highlight the code making these assumptions
Representation
Format
Location
Address
Content
Semantics
 
   
 
   
 
   
 
 
 
   
 
  
                                
 
 
 
   
 
 
 
 
   
 
 
IPHostEntry hostInfo = Dns.GetHostByName("asq.click");
IPAddress address = hostInfo.AddressList[0];
IPEndPoint endpoint = new IPEndPoint(address, 8888);
Socket socket = new Socket(address.AddressFamily,
SocketType.Stream, ProtocolType.Tcp);
socket.Connect(endpoint);
Solution answer = new Solution("asq.click", 1);
byte[] message = answer.getBytes();
socket.Send(message);
socket.Close();
I would like to ask you if you can apply what you understood about coupling to high-
light in these concrete examples the assumptions that we make about the representation
format, the address of the component, as well as the semantics of the information that
we exchange.
Here is a piece of code that embeds all of these assumptions. The program is able to
connect to a server,revealing the intricacies of opening a socket and sending information
through the socket so that the other side can get the data that you’re trying to exchange.
The code embeds assumptions that will affect the coupling between your client and the
server. These assumptions could change or become invalid: being aware of them and
being aware of the coupling that you introduce into your architecture when you write
this type of code is what we want to discuss today.
The location address encodes the knowledge about where is the server that we want to
connect this client socket to. The part of the code that knows about the location uses the
symbolic address of the server. This is the address that we are looking up with the DNS:
if you want to switch the address you have to change this string. Two lines later there is a
port number (8888) as well, which is part of the address. Everything else is just boilerplate
code that you need to write if you want to open a TCP socket using the address as an input,
but you don’t have to change it unless you want to switch to a different protocol.
The content semantics is basically embedded in the code that is responsible for know-
ing the meaning of the data that you are exchanging. The ’Solution’ is a class that en-
codes a certain ’String’ with a certain color following the interface data model defined
elsewhere.
When you transform the ’answer’ object into a set of bytes, this is where you decided
which representation format to use. The byte array is basically the format for the data
that you use to send a message with. The data is a solution that contains a string and
a color, and this byte array is the type of the message in which the data should be sent
with.
435

We can recognize the need to perform a transformation between the internal represen-
tation of the data (stored in memory within the structure given by the ’Solution’ class)
and the external representation of the data (also stored in memory as a byte array so that
it is ready to be copied along the protocol stack). This ’getBytes()’ is the adapter between
the inside and the outside, this is where all the assumptions on how the data gets serial-
ized are found. These assumptions need to match the ones of the message parser on the
server side.
Highlight the code making these assumptions
Representation
Format
Location
Address
Content
Semantics
 
    
                
 
                
 
 
            
    
 
 
 
 
    
 
            
 
 
 
    
fetch( "https://asq.click/api", {
headers: {
'Accept': 'application/json' }
} )
.then( response => response.json() )
.then( sol=>{
highlight( sol.text, sol.color );
})
It’s very rare that you actually would directly program sockets these days, so let’s take
a look at a higher level example. This is no longer Java, this is JavaScript. You should still
be able to pick the same three aspects.
Also, we can see the code of a client that will contact a Web API using HTTP via the
’fetch’ asynchronous JavaScript API. The code will process the response by parsing the
JSON string which gets transformed into an object. Two specific fields of the object get
passed to the ’highlight’ function.
The location address is probably more clear to spot, since it is found in only one place,
written as a URI string encoding the absolute address of the API of the Web resource. By
default fetch uses the GET method, so we do not have to explicitly configure it.
How does the code control which representation format will be used? There are two
places where this assumption is encoded. The first configures the ’Accept’ header to ask
the server to use a particular media type as representation for the resource being fetched.
The second is where the response is transformed into an object. The body of the response
is actually received as a string. We parse the string and transform it into the object,
assuming the string uses the JSON syntax.
Then we work with the object and we start making assumptions about its content se-
mantics, by extracting the text and the color fields. This code is actually independent of
where the content of the object comes from and how it got there.
You can interpret this code also in terms of connectors and adapters. The first fetch call
represents the low-level HTTP protocol used to connect this client code with any remote
Web resource (here you need to make sure you select the right address). Its result needs
to be fed into the specific, local interface ’highlight’. There is a mismatch between the
extremely general result (a string) downloaded as the payload of an HTTP response and
what the local function requires as input (two specific parameters with a domain-specific
436

semantics: text and color).
How to solve this mismatch? Use an adapter which can translate the original string into
an object, from which the specific fields can be extracted. Does the local call to highlight
the solution depend on where its input data parameters came from? No. Does the actual
content of the object depend on the server sending the expected content with the correct
semantics in the response? Yes.
What if you change the structure of the object to represent a true/false answer? Which
part of the code will need to be changed? Can you reuse the fetch and the JSON parse steps
without changing them? They are completely orthogonal from the content semantics.
Likewise, you could reuse the same highlight function and pass it an object hydrated
from a SQL query sent by a object-relational mapper, as long as the schema would match
the content that should be in the object.
Addresses, Protocols, Representations and Semantics are always present when we con-
nect together two different pieces of software. They can be controlled, configured and
designed independently: we can take the same content with the same semantics and rep-
resent it in completely different ways. And if for some reason we need to move the server
to a different location, most of this code will not be affected. The question is whether it’s
a good idea to actually hard code these addresses into your code, or should they be actu-
ally read from some separate configuration settings, to keep the code location agnostic.
437

Understanding Coupling
• Different connectors introduce different degrees of coupling
between components.
• Only disconnected components are fully de-coupled.
• What type of assumptions are implied by a connector?
• component exists
• component matches interface
• component deployed at a given location
• component running and available to be invoked
• component can freely evolve
Depending on the connector that you use, you will have a different degree of coupling.
Different types of assumptions you make will affect the coupling between the compo-
nents.
As long as components are connected, they are coupled. Sometimes you read about
different degrees of coupling: loose coupling, tight coupling. Not all the coupling is the
same, but only components that are completely disconnected are fully decoupled.
Coupling is about the impact of what happens on one side to the other side. This im-
pact directly depends on the assumptions that you make when you select and configure
a certain type of connector.
The most fundamental assumption that you already do at design time, is the fact that
there is a component on the other side that exists and it matches the interface that you
have on your side. This is implied just by the fact that you are drawing the line between
the boxes.
The edges connecting the nodes of the structural dependency graph of the logical view
of your architecture represent an important assumption: each connected node assumes
the other side exists and it matches the interface it requires or provides.
When you switch between the design of your system and its deployment, then you also
need to know where you will find the component that you depend on. So we assume the
component exists and that the component has been deployed in some container and we
know where to find it.
When do we discover the existence of a component? When do we know the location?
And do we hard code these answers like in the previous examples? This is a string with
the IP address that we write into the code and if we want to change the location we have
to change the code and recompile it. That’s sounds like a strong coupling: a change of
location requires a rebuild. Instead, we should be able to separate the information and
the knowledge about the location of our dependencies from the code that actually uses
them.
438

After the deployment we start the system; its components start running. That’s when
components start to interact with each other. Components that need to interact with
each other, depending on the connector that you choose, may assume that the other
side will be available to answer their invocations. Or maybe with different connectors,
a component will take advantage of the opportunity to actually interact with another
component that is not available at the same time. Depending on the connector, the state
of the availability of one component may affect the success of the interaction with the
other component.
While you operate your system something will fail. How does a failure of one com-
ponent affect the other side? Will the failure of a random component which you didn’t
know existed bring down your component? Again, this depends on the type of connector,
which can either propagate the failure or isolate it.
As you evolve the system, you make a new version, you ship a new release and deploy
it. Components you depend on have a life of their own, they change. Components that
used to work, stop working after upgrading them. Interfaces which used to match, are no
longer perfectly matching. Which changes on one side are going to be visible and affect
the other side?
When you connect together two components, you establish a dependency. You couple
them together. And that means they are no longer independent from each other, they
cannot change independently. Changes of location, state, availability state, implemen-
tation as well as interface will propagate in different ways along coupling relationship
introduced by different connectors.
439

Interface/API
Interface/API
Client
Runtime Platform
Implementation
Implementation
Timing
Timing
must the client and the component
be available at the same time
to exchange a message?
Discovery
Discovery
how does the client
ѹnd the component
location?
Interaction
Interaction
must the client
directly connect to
the component?
Binding
Binding
when does the client
select the component? 
Session
Session
do clients and component 
share session state? or is each
message independent?
Platform
Platform
is the client affected if
the component is ported
to another platform?
Interface
Interface
to what extent the 
API can change 
without breaking clients?
Coupling Facets
Let’s see more precisely these different coupling facets. Let’s decompose the definition
of what is the coupling between two different components into multiple facets.
Here we can see that there is a client that is connected to a certain interface. The
interface has an implementation, which runs in a certain container on a certain runtime
environment.
The first question that you should ask yourself when you’re about to choose the type
of connector to introduce is: should both the client, and the component it depends on,
be available at the same time in order to successfully communicate?
The answer can help you to constrain your choice of connector. The timing facet of
coupling depends on wether the connector is a synchronous or asynchronous one. If the
connector is synchronous, for example: remote procedure call, we assume that when we
make the call, the other side is available to answer it within a certain time window. If it
takes too long, the client will not receive an answer and typically the call will fail due to
a timeout.
The other aspect of coupling is related to the location: How does the client discover
the address of the interface? How does the client learn about the location of the other
side? The client needs to know where to find the component that it depends on. This
can be resolved when you write the code of the client, you hard code the IP address of
the server into the client. You write it in a configuration file. For example, when you
start the client, the client reads the configuration file and it discovers the address of the
server. This could also be something that the user can give to the client. If you open
a web browser, the first thing you have to write is the address of the Web server. The
browser doesn’t know it; it’s the user who knows. Here are different approaches to keep
the client as independent as possible from the knowledge about the location of the server.
The more the client knows, the larger impact a server migration will have.
440

Another difference separates direct or indirect connections between the two compo-
nents. For example, the Message bus is indirect because the client is going to send a
message into the queue. There is a direct connection between the client and the queue,
but the other side does not talk to the client, it just talks to the queue to receive the mes-
sage once it’s ready to be delivered and the server is available to process it. Also in this
case your choice of connector will change the interaction facet of coupling.
The binding facet is related to an earlier decision that you make related to the selection
of which component you are going to depend on. Once you picked the component you
can discover its location. But first you need to know which component you are going to
use in the first place. Given a certain interface, we can have multiple implementations.
So when do we select the implementation? A binding decision is a decision that you make
when you choose the component that you want to connect with. Discovery is different
because once you have chosen the component once, you know who you want to talk to,
then you have to know where to find it. The same component can be located in differ-
ent places. You may want to search for the nearest location to minimize latency among
a globally replicated deployment. The binding is done based on the functional require-
ments, the discovery helps to optimize the extra-functional requirement of performance
and availability.
What if you work with this component and all of a sudden they tell you that from now
on you have to start paying 10 times as much if you want keep calling the component.
Here it doesn’t matter where the component is located. What you want is to switch to a
cheaper provider and change your binding (assuming you can find a replacement). As a
consequence you will also have to change the location. We can see the location as a more
technical facet related to having a distributed runtime and components need to commu-
nicate over the network. The binding is more of a business decision which establishes a
relationship with a given provider.
Platform independence is a facet which determines whether changes in the runtime
environment of the container in which one component is deployed will affect the other
side. For example, consider if you switch the programming language used to implement
a component: is the client going to notice? This is usually noticeable concerning the
data model of an interface and how data is represented in different languages. If you ex-
pose a binary representation which directly maps the memory layout of your objects, you
may notice even if you switch to a different compiler version of the same programming
language. If you use programming language independent standards, such as JSON, XML,
then there are already enough standard adapters between most programming languages
and these representations so that your interface becomes platform independent. This
way, changes to the dependencies of your dependencies are abstracted away and will not
impact whoever depends on you.
Another critical coupling facet concerns the assumptions the client makes about the
content of the interface that it depends on. We can pick the right connector to mitigate
changes to the location, representation format, timing. We can even rewrite the imple-
mentation using a different programming language. And still keep the client compatible,
thanks to information hiding. But what if you deprecate or eventually remove some in-
terface feature? The client would be totally destroyed, as it’s trying to use something that
is not there anymore. We will study in the lecture about flexibility what kind of changes
we can do to an API that do not affect clients and what kind of changes you should never
do to an API unless you want to break all of your clients.
The session facet depends on type of interaction between the two sides. If the client
performs an operation on the interface, will the result of the operation depend on the
previous interactions of this client? This is similar to stateful interfaces, for which the
results depend on the history of the previous interactions of all clients. Establishing a
441

session between the client and the interface means that interactions are no longer inde-
pendent from previous ones. The interface will remember and the client will assume the
interface remembers. What could possibly go wrong? Let’s see an example.
Session Coupling Examples
Stateful session
>cd tmp
>rm -rf *
>Hi! What's your name?
<Olaf 
>Hi Olaf, this is today's
menu. What would you like to
order?
<Menu 1
No session
>POST /order
customer: Olaf
choice: Menu 1
Let’s start from the shell command example: How many of you would want to run the
second command independently of the result of the previous command? The first shell
command sets the current directory, which will definitely determine the outcome of the
second command. That’s why sometimes it is called a shell session: every command will
change the state of the shell and you enter the next command assuming the shell is in
a certain state. When your assumption fails to match the state of the shell, that’s when
you could end up deleting your entire file system. Don’t try this at home. Always ’pwd’
and check the result before doing a ’rm -rf *’.
The chatbot example shows a conversational session. Each party will exchange multi-
ple messages in a certain order. And the big question is whether they expect to remem-
ber previous messages or whether each message is treated independently of the previous
ones. Will a previous message affect the behavior of how further messages are processed?
Is it even possible to send a message without having sent another message beforehand?
The advantage of chatbots is that they are supposed to be more human friendly, more
natural. You feed them information step by step. On the other side, the information
gets accumulated until you get to the point in which you want to place the order and
commit the transaction. While it makes perfect sense to have this conversation within a
few seconds, but how long is the waiterbot willing to wait to have the second part of the
conversation. Human short term memory tends to expire after a brief time, this is not the
case of bots, which would gladly keep waiting for you to complete the order forever. But
if you resume the conversation months later: Do you assume that the state on the other
side of the session is still there? How likely is that? Have you ever experienced a long
hiathus in a personal relationship but then resumed the conversation decades later as if
no time had passed? In this case, the protocol should include mechanisms for checking
whether the state of the session is still in synch between the two sides. There is nothing
worst than entering data into a complex Web form and getting a session timeout error.
442

Speaking of Web forms, the third example is about making an HTTP request to place
an order. The request here contains both the selection for the menu but also the name of
the customer. With one command we get the job done. This is an example of an interface
designed to work without establishing a session.
If you have interfaces which require multiple rounds of interactions and every inter-
action builds on the previous ones, the longer the session takes, the higher the chances
that something may go wrong and there may be a partial failure: a failure in which one
of the two sides of the interaction looses the state of the interaction, while the other still
remembers it. This makes the session state that you have established across the connec-
tion inconsistent.
The risk of partial failure and the complexity of recovering from it makes session-ful
connectors more coupled. Session-less connectors can simply retry failed interactions
since both sides are always in sync as their session state by definition gets reset after
every interaction.
443

Binding Times
Discover
Discover
Select
Select
Integrate
Integrate
Migrate
Migrate
Backup
Backup
Recover
Recover
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Operate
Operate
early
binding
early
binding
static
binding
late
binding
dynamic
binding
very late
binding
Binding Times
• Depending on the connector, the topology of the graph of
connected components may be deѹned, constrained, reѹned or
modiѹed at different points during the system lifecycle
• design-time (as components are selected)
• build-time (as components are packaged together into
deployable units)
• deployment-time (as components are unpacked and
installed)
• or even modiѹed at run-time (to optimize the behavior of the
system, e.g., switching between service providers).
• during recovery, when something fails and needs to be
replaced
444

When do you make the selection of the components that you want to reuse and will
need to depend on? This can happen throughout the whole lifecycle of your architecture.
Early binding indicates a choice made before running the system. A choice made based
on the documentation of the interface we have discovered. We select it, but we still
haven’t been putting it in action. An early decision is made before you even compile
the code.
Static binding also happens before starting up the system. This could be done both for
components that you built yourself, or components that you have selected to be inte-
grated from somewhere else.
After you built your system, then you can run tests with it, this is where binding deci-
sions become more dynamic.
Binding after you start to run the system is also known as late binding. For example,
after you ship a release, during its installation one may choose which component gets
deployed. Sometimes deployment is atomic, all the binding decisions have been made
statically and you can only drop the entire package or none. Sometimes you can cus-
tomize which components get plugged in or out of the system at deployment time. You
have already built the system, it’s already tested and released, and now during deploy-
ment is the time to do the customization.
Dynamic binding happens while the system is running. It requires support for dynamic
loading (and unloading) of components without the need for restarting the system. This
can be useful to procrastinate the decision about which component to load until the very
last moment right before you actually need to call it. Until you’re about to make the call,
you can still switch the implementation targeted by the call.
If you call something and it fails, it means that your binding decision on what to call
was misguided. Maybe it used to work, but for this particular call it didn’t because your
dependency crashed. If, as part of your recovery, you decide to change your selection of
the component that you’re going to use, you will re-evaluate the binding doing what we
call very late binding.
The difference between late and very late binding is that late happens during deploy-
ment or startup; very late happens after a failure during recovery, which may involve
another startup cycle. As part of recovery, to stabilize your system, you decide not to
depend anymore on a flaky provider that is unreliable. You found a better provider or
you had a back up provider and you can change the binding.
The more dynamic your binding, the later you wait, the more flexible your system is
going to be.
You make early binding decision because you prescribe a connection between the two
components based on their documentation. As you build your system, you can statically
bind components while packaging a release artifact. Static linking results in a single ar-
tifact, which can be atomically deployed. Dynamic linking supports swapping or adding
more components at runtime, while the system is operating, making decisions based on
information only available at runtime (e.g., performance/workload/resource utilization/-
cost).
The earlier they are bound together, the stronger the coupling between components
and the more difficult it will be to replace some of them without having to go again
through the entire lifecycle (e.g., rebuild or reboot) for your system.
445

Jon Postel
Be liberal in what you
accept, and conservative
in what you send.
Strict
A
B
Flexible
We can conclude this lecture on compatibility and coupling with two messages: some
food for thought.
The first concerns the compatibility of two connected components. For example, they’re
supposed to comply with a standard interface. How do you check or enforce this de-
cision? You have two options: to be as strict as possible in how each side follows the
standard, e.g., how it represents the data being exchanged. The other option is to be as
flexible and forgiving as possible while exchanging information, which may deviate from
the standard, but still work.
One side sends a message to the other. Which side should be strict and which side
should be lenient? Having both strict sides is not practical. It would imply having only
a single interpretation (or implementation) of the standard. Flexibility on both sides is a
recipe for chaos, since there would be too many possible variations from the agreed upon
interface.
You can have components producing noisy colorful messages, and strict components
rejecting most of them. This also does not make it easy to increase the size of the system
while keeping most components compatible.
The only viable alternative for large-scale interoperability is to be forgiving as you
accept messages from unknown senders, but being strict while sending messages to the
rest of the world. This principle helped to support the growth of the Internet protocol, as
it recognizes that while there can be many interpretations of a standard, it is important
to provide incentives for compliance.
446

Water or Gas Pipe?
Sometimes incompatibility is the goal
The second message is that in some cases compatibility can be a problem.
Like in this example, where we have two identical pipes but we want to reuse them to
carry different content: water and gas. This makes it more efficient to source the pipes
while constructing the building. However, how can you avoid connecting them together?
After all, you want to prevent starting a fire when opening your bathroom faucet.
The solution is simple: make the interfaces incompatible so that they cannot be con-
nected together by mistake. You just need a standard or to pick a convention where gas
and water pipes are threaded in the opposite direction to avoid they fit together for safety
reasons. Safety can justify incompatibility, and you should not try to work around it with
some adapter.
Incompatibility can also be a business strategy to create vendor lock in, keep customers
captive within your walled garden as it becomes too expensive to switch to alternative
platforms. Incompatibility can fuel the growth of an industry dedicated to producing
adapters.
447

References
• Richard N. Taylor, Nenad Medvidovic, Eric M. Dashofy, Software Architecture: Foundations,
Theory and Practice, John-Wiley, January 2009, ISBN 978047016774
• Gregor Hohpe and Bobby Woolf, 
, Addison-Wesley, October
2003, ISBN 0321200683
• Douglas C. Schmidt, 
, 1999
• Cesare Pautasso, Erik Wilde, 
, pp. 911-920, Proc. of the 18th International World Wide Web Conference (WWW
2009), ACM, Madrid, Spain, April 2009.
• Jon Postel (ed.), 
, RFC 761
• Cesare Pautasso, Gustavo Alonso, 
, Proc. of the 4th Workshop on Software Composition (SC 2005), Edinburg, Scotland,
April 2005
• XKCD 927: 
• Klint Finley, 
, Wired, 23 May 2016
Enterprise Integration Patterns
Half Sync/Half Async
Why is the Web Loosely Coupled? A Multi-Faceted Metric for
Service Design
TRANSMISSION CONTROL PROTOCOL
Flexible Binding for Reusable Composition of Web
Services
Standards
The Oracle-Google Case Will Decide the Future of Software
 
 
 
 
 
 
 
 
 
 
 
 
 
 
448

Software Architecture
Deployability, Portability
and Containers 9
Contents
• Deployability Metrics
• To Change or not to Change
• Continuous Integration, Delivery and Deployment
• Platform Independence: Virtualization and Containers
449

The Age of Continuity
1 release every 2-3 years
1985
1987
1990
1992
1995
1998
2000
1 release every second
2014
After we have discussed the qualities of a software architecture at design time, today
we’re ready to make the big transition: we’re going to deploy our system in production.
This is the lecture in which we focus on how easy it is to deploy the software system in
production that we are designing and what kind of architectural decisions we can make
that affect this deployability. Before we can deploy, we have to build it and make sure
that it can be executed in a container in a runtime environment. So today we will also
discuss virtualization and containers in the second part with an eye towards portability
as well. From the next lecture on, we will focus on runtime qualities such as scalability,
availability, flexibility.
Why is deployability important? The way that we built our software systems has been
undergoing a dramatic change as we have been switching away from an age in which
software was built on a slow moving cycle with a major release of a software system ev-
ery couple of years. It’s important to worry about this quality and to design the proper
system to build our system because we have been changing the frequency with which we
ship releases and deploy software. In the past, making a release was a major undertaking
that happened relatively unfrequently, so here you can see for example the first decade
of Windows releases happening every two or three years. Somehow between that time
and today we have dramatically increased the frequency with which software is released.
Some people actually speak about the end of the software release: from the user perspec-
tive there is no more the concept of what is a version of software system, because every
time you access a system, it actually has changed. Today deployability is about making
releases continuously, so that we can improve continuously the qualities of our software:
add more features, fix more bugs, and do so with confidence at high speed.
Some major cloud providers have announced, for example, that in 2011 they would
make new releases every 10 seconds. Then in 2014 they actually increased the speed ten-
fold: now they are doing new releases every second. How can achieve this high speed of
release without sacrificing the other qualities? That is the major challenge that has been
addressed in the industry and which we will discuss in this lecture.
450

Deployability Metrics
• Latency:
• What's the delay between developers writing code and users
experiencing the improvement?
• What's the delay between users giving feedback and
developers releasing a ѹx?
• Throughput:
• How many releases are shipped to production every
hour/day/month?
Deployability is a quality attribute that can be measured. And there are many ways to
measure the deployability of a system.
One concerns the latency, which we can define in terms of what is the delay between
the code the developer writes and the user observing the effect of that code after it gets
deployed in production. If the developer is writing a new feature: how long does it take
before this new feature gets delivered to the user? And if the developer is fixing a bug:
how long does it take before the user that reported the bug observes that the bug is no
longer there?
These are examples going forward, but you can also observe and measure deployabil-
ity in the backward direction. For example, a user reports a bug, gives some feedback
about something that should be addressed. Of course, you have to route the feedback to
the right developer and make a decision whether you will actually implement it or not.
Afterwards you can measure again how long it takes after the developer has written the
code and the user has seen and accepted the improvement.
So how long does it take you to do that? Is it just like a click over the ’deploy’ button
and then the user automatically gets an update and no additional effort is required to
benefit from the improvement? Or is this like a major project over several months to
successfully go through this cycle once.
Considering the throughput, we can measure deployability for the whole system in
terms of how many releases do you ship to production over time? You do a release every
two years, or you do multiple releases every second? That would be the expected range
for deployability throughput.
451

Deployability Metrics
• Time:
• For how long will the system not be available during
upgrades?
• Is the release late with respect to the marketing target
deadline?
• Cost:
• How many people are involved in a release?
• How many Virtual Machines are needed to host the software
production pipeline?
Let’s also consider how expensive it is to do a deployment. Is this something that
one developer can do with a click to launch the corresponding script in their IDE? Or
does it entail a major undertaking with multiple teams involved, where developers have
to coordinate with operations, to schedule a suitable upgrade window? Or do multiple
development teams have to synchronize? For example, one team is making a release for
the server side and all the affected client endpoints need to be redeployed as well.
When you’re doing an upgrade, is the system availability going to be affected? Are the
users going to see a little message apologizing for the ongoing maintenance: you cannot
use the system while it’s been upgraded, take a long coffee break. Your architecture has a
better deployability if, during the change, as a new version of the system is deployed, the
users do not notice. It is very important not to be disruptive of the productivity of your
users. If you have to stop what they’re doing to wait for an upgrade, consider upgrading
over the weekend. Disruptive, unplanned upgrades in the middle of the work day can be
a very expensive proposition.
Often one hears marketing announcements about the availability of new software prod-
ucts. For example, a new game will be released on the Black Friday or the Christmas sea-
sons. It is critical to hit this particular launch window. If you are late and cannot ship
it in December, probably this is a big missed opportunity for revenue. We can also mea-
sure deployability in terms of whether the release is shipped on time so we can meet our
customers expectations generated by the marketing campaign.
Another aspect for measuring the cost of deployability can be observed by assessing
how expensive is the infrastructure to do the build and the testing along the software
production pipeline. Can you build your system on your developers laptop? or do you
need a whole cluster of virtual machines to process the compilation of the code and the
parallel execution of all the automated quality assurance tests?
There are some systems with millions of lines of code which require several hours to
compile and the complete testing will run for a couple of days. In this situation, making
a build is very expensive computationally, and if you make a small change and then you
452

have to wait for the whole night before you see the release of the result of the nightly
build, your productivity becomes limited by the time it takes to get feedback due to the
expensive deployability of large architectures.
Deployability Metrics
Traditional
Continuous
Latency
High
Small
Throughput
(Releases/Time)
1/Years
Many/Day
Downtime
Noticeable
0
Deadline
Death March
Not Applicable
People
Dedicated Team of
Release Engineers
0 (Full
Automation)
Infrastructure
?
Investment
Required
Traditional deployments – as an extreme example – have very low throughput of yearly
releases; when making a deployment, users will notice. If you need to catch a released
deadline, you will attempt to do so through so-called death marches, which will lead to
burnout and high developer turnover. Since every release is risky, requires heroic efforts,
you do not want to go through crunch times too often. Making releases requires a big in-
vestment into a team of release engineers who take the code, build it, test it and package
it so that it can be released by copying it on some website so that it can be downloaded
and installed. In the good old days this also included people or robots burning the soft-
ware on CDs and placing them into shrink wrapped boxes. There was no viable concept
of automated builds and continuous integration delivery pipelines. Deployment was an
ad-hoc, manual, error prone, very slow and expensive.
Nowadays, if you want to make a release, you can do it quickly. You can do it often. Your
users do not notice, as you can apply techniques that help you to minimize the disruption
involved into making a change to a production system. Since the release is continuous,
you don’t have a target window anymore, you just do it all the time. There is no longer a
single opportunity, you can continuously improve the system. This has become possible
because the process is fully automated: you just have to invest into setting up the pipeline
with the proper tooling and the proper infrastructure to build your software. Then the
pipeline reliably runs unattended: shipping a release has matured from a craft involving
black arts into a well oiled industrial workflow.
453

Michael T. Nygard
Release
Opportunity or Risk?
• Deliver new features
• Fulѹll requirements
• Bug ѹxes
• Performance improvements
• Generate revenue
• Retain and grow customers
• Match or exceed competition
• New failure modes
• New bugs
• Higher resource capacity
required
• Peak of support calls
• Deployment may fail
• Deployment failure may be
unrecoverable
Every time you introduce a change, you need to make a new release. When you make an
improvement of your software, it’s a big opportunity because you are going to deliver new
features, fix bugs and in general attempt to make your users happier because you better
fulfill their requirements, so this is a positive opportunity to address users constructive
feedback. New releases can be expected to improve not only the correctness but also extra
functional qualities, for example: better performance. Another consequence of shipping
a new release is the opportunity to make your customers pay for the upgrade. This is one
of the business models for software components: every time you make a new release,
your users have to pay to install the upgrade. Some customers have a tendency to flock
to freshly released software. Others, more conservative, wait for the release to age before
daring to install it. From a business point of view, making a release is critical to retain
existing customers and get new ones, since your software now delivers more features
than the competition, or it has been finally improved to catch up with the competition.
These are all reasons why you should not hesitate before making a new release of your
software.
However, there is also the flip side in which when you make a new release, all of a
sudden the system behaves in a new way - sometimes in an unexpected way which may
lead to new failure modes. You already knew in which configuration and for which input
your previous release was unstable. You make a change and now you need to discover
even more cases in which things can go wrong.
For every bug that you fix, you introduce a couple of new ones; you have to be careful
not to catch those bugs before you put them in. The performance might be improved, but
maybe also your capacity requirements have increased, so now we need more resources
than before, so it has just become more expensive to run the system.
Your new features interfere with the normal operations of the users. They confuse
them and lead to an increased cost for training and support, helping users get used to the
change: Why did this change? Where did my favorite feature go? Why did you move the
menu somewhere that I cannot find it anymore?
454

While you’re in the middle of the deployment, things can go wrong. The deployment it-
self is a dangerous operation which changes the state of running system into an unknown
one. If the new state works, congratulations for your success! But if the deployment fails,
what happened to the previous release? Avoid ending up in a situation in which the new
release is broken and the old release is gone. Your users will definitely notice. That’s
why weekends have two days: Saturday to deploy the new release, Sunday just in case,
to clean up or revert back to the previous one. Always have a time buffer and a strat-
egy to recover from a broken upgrade. If you’re not careful in the way that you do the
deployment failures could even be unrecoverable.
Thanks to virtualization this should never happen anymore, but when people used to
work with physical computers. You have a physical object in which you installed an op-
erating system. And then you have a disk with the software and the data in a certain
version. Since it’s expensive to have a new physical server on which to place the new re-
lease, some brave system administrators may just decide to install the new version over
the existing one. If you do so and something goes wrong, the old one is gone - because
you have just formatted the disk and forgot that you were supposed to migrate the old
data. While recovering the old software may be possible with enough effort, losing data
during deployments is unforgivable.
455

No Change = No Risk
The only software which does not need to change is dead
software which nobody uses
Minimize risk due to change
instead of
minimizing change
Release many small improvements often
Every release should be reversible
For these reasons, experienced operators grow into being conservative. When they
hear about your new release, their reflex is to ask: Are you really sure you want to install
the update? Because after all there is a risk that it may not work as it used to. It is
reasonable to hesitate before going from a known state to an unknown one.
In the extreme case: simply don’t make changes, ever. Stay in known territory. If
you don’t change, there is no risk: problem solved. However, the only software which
doesn’t need to change is dead software used by nobody. Anyone using your software
will always find some ideas about how to improve it or make friendly suggestions to make
some corrections.
If you avoid risk by never daring to make changes, your release process takes years
before you actually gather the courage to actually make the step. What you want instead
is to minimize the risk of change as opposed to reducing changes themselves.
How do you do that? You make the magnitude of the change small, you make those
changes often. This way you get used to making the changes and making changes to
your system is a normal, frequently executed operational process: not something that
you just do once in your lifetime. You release and deploy every day so that it becomes a
habit and you learn how to do it properly.
Also, you shouldn’t make changes without a safety net. Like database transactions:
begin the deployment; apply all the changes; commit. If something goes wrong during a
transaction, roll back to the initial state. If you can find a way to make a release of soft-
ware work like atomic database transactions, which go from a consistent state to another
consistent state, then you have the safety net which should reduce the risk of making a
failed deployment that will destroy your system, tarnish your organization’s reputation
and eventually get you fired.
456

Release Frequency
Build
Build
Run
Big Bang Release
Development
Operation
Run
Continuous Release
Change
Change
Development
Operation
If it hurts, do it more often (so that practice makes perfect)
In the classical waterfall, a lot of time is spent in development building the software
until it is feature complete. All tests are green. Eventually the development project is
finished. Developers package and ship the release. And then they throw it over the wall
to the operations team. The operations team has never done anything until this point
and they take over. They install the system, they run it and after this transition one
could literally fire your developers because you don’t need them anymore. The software
is done, the project has been completed. So developers can move on to another project
as the operations department runs the system.
What can go wrong from a development point of view? you think that you’re done
because you have clicked compile. And you generated some executable artifacts. Given
some tests, maybe you even run them and check they pass. Still, even if it passes all the
tests, there is no guarantee that is actually correct. The real test will start when actual
users start to run it. But that’s no longer your job. That is somebody else’s problem.
While it’s very rare that the system is perfect on the first shot, in the first release. The
idea is that if this transition is expensive, complicated, risky – in other words: if it is a
pain – then you should do it more often so that it becomes less painful. This situation is
called ’Big Bang’ release. Big bang releases are a once-in-a-lifetime event (The Big Bang
only happened at the beginning of the universe). That’s the only chance you have. So if
it goes well, you are done. If it goes wrong, there is no second chance.
There are many reasons making challenging to achieve a successful Big Bang release,
but this was the traditional model where you have the software built by one part of the
organization and then a different group takes over to execute it and will need to deal with
whatever operational problems emerge as a consequence of a successfully shipped devel-
opment project. In this setup, there is very little feedback going back from operations to
developers.
How do we make releases happen in a continuous way? How do we do it more often?
457

We are going to make the operation and the development happen in parallel, so they’re
no longer sequential steps. They’re no longer: first we develop and then you operate, but
while we develop, we also operate the system. The transitions between the system being
built and the changes being introduced during development cycles and the deployment
and operation happen continuously.
This doesn’t mean that continuous deployments will be always successful. You can
also have failed builds, broken releases and unsuccessful deployments. Look at the time-
line for the red dots: The first releases are good, but then something goes wrong while we
keep improving the system and eventually we hit another good release. What is funda-
mental here is that we have a continuous feedback cycle between developers that make
the changes and operations that watch the impact of changes on users and feed their
feedback back to the developers for further improving the system.
There is no separation anymore between who is responsible for applying the changes
and who is living with the consequences of that. Because of such tight feedback cycle
between the two sides and by making the release something that happens continuously,
we have a lot of opportunities for trying out new ideas for making improvements that
might not necessarily stick. Maybe not all the users appreciate them, so we can always
go back and undo those changes.
This is a paradigmatic shift between building first and operating later and doing both
at the same time, as well as going from separate teams dedicated to each activity to the
same team involved with both activities.
458

Speed vs. Quality
Slow
Speed
Fast
Low
Quality
High
?
Thanks to Automation, Speed vs. Quality is no longer a Trade-Off
Another change in in perspective that happens with DevOps and continuous releases
is that there used to be a trade off between the speed with which you develop and release
and the quality of the result.
Conventional wisdom was that you can either have low quality when you go fast. The
faster you develop something, the less time you can dedicate of doing so with good quality
(both external and internal). Would you like to improve the quality? That’s OK, but then
you have to slow down.
Doing continuous releases wouldn’t work if this trade off was still true, because we
cannot sacrifice quality to increase the release throughput. Otherwise we will always be
in a broken state with a failed release.
When you start to automate all your quality assurance and testing tasks, when you
automate your production process to build and package the images and releases so that
the process becomes not only fast but also error free, controllable and repeatable, then
you can break free of this trade off and have a continuous stream of releases of high
quality.
459

Fix
Fix
Tune
Tune
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Run
Run
D
e
v
e
l
o
p
m
e
n
t
O
p
e
r
a
t
i
o
n
s
Software Production Processes are Software too
Let’s go through the DevOps cycle once again: you start with the back log, prioritize
it and make a plan for the next iteration. You implement the stories. You build your
system, you write and run the tests. You package the release. Deploy it, install it and start
it. While the system is running you monitor its behavior, gather feedback, try to learn
something from observing the users. Get all the bug reports, put them in the backlog and
go around once again.
This sequence of steps is what we call a process: because your development and oper-
ations team goes through a certain number of activities. When you describe a process,
you are basically describing a piece of software. The question is: how can we automate
this process? How can we write software that helps us to build, test, release, deploy, run,
monitor software?
If you have automated such infrastructure, if you have tools which can support each
of these activities, then the whole cycle can be fast, reliable and produce a high quality
outcome. The only bottleneck that is left concerns the developers who need to write
the code as well as the tests, but the actual execution of the other steps should be fully
automated.
The cycle may stop from time to time: you do not necessarily want to make a change,
build it, release it, and then wait until the user complains and then scramble back to fix
all their issues as quickly as possible. That’s why we want to have this little cycle on
the development side in which you catch as early as possible all the problems, thanks to
testing and test-driven development. If the test is red, we go back and fix the code and
then test it again before shipping the release. The same is true also on the other side.
Sometimes when you monitor some performance parameters you may be able to correct
those issues by allocating more resources, by changing the configuration of your system,
and those can be done directly in the operational side. You don’t have to go back and
make a change to the system to do it, especially if the system is configurable.
460

Software Production Pipeline
Build
Compile
Unit Test
Integration
Integration
Test
Acceptance
User
Test
Capacity
Performance
Test
Release
Approval
Ok
?
Production
Deploy
Smoke
Tests
Source Code
Repository
Push and Merge
Changes
Release 
Approval
How can we design a piece of software that supports this process? What kind of soft-
ware can enable such continuous life cycle? This is called a software production pipeline.
The input data to this pipeline is piece of software. It’s the code of your software
which gets processed going through serious series of stages. On the other side, out of
the pipeline comes an image, a piece of software ready for deployment. This is an exe-
cutable software artifact that can be put in so called production: it can be made accessible
to your users so that they can work with it.
The software production pipeline transforms the software from its original source form
into executable form, but it’s not just a compilation step, that’s only the very beginning.
The build is actually not only compilation, but it’s also checking that the unit tests pass.
We want to make sure that all the components that we compile separately are passing
the quality control.
Once you have built all the components you have to integrate them so you won’t have
to release and deploy individual components, but a single artifact with your software
and as many of its dependencies as possible packaged inside. This can be subject to the
integration test, another quality assurance step.
If this is green, further testing, for example, in terms of performance, helps to check
that the system is not getting slower. So you compare the performance results with the
previous release. And we run a benchmark to do that. And also you can already get some
users to give them a preview and they tell you if they accept how the new features have
been implemented.
Based on the outcome of all these tests, the process reaches a fundamental decision:
to release or not to release. That is the architect’s responsibility: to approve the release,
to sign off and state: ”yes, we reached a state which we can publish our work and make it
available to the users so they can start working with it in production“. Once this happens,
then you deploy the system in production and before you open the door to the users you
still do a quick sanity check to make sure that everything is supposed to work as intended.
The most frequent word that you find in this slide is the word test. Software production
pipelines emphasize automated quality assurance. At every step that you do, you need
461

some automatic way to check that your intermediate product meets expectations. You
have to be able to encode these expectations so they can be checked automatically, both
in terms of correctness, but also in terms of extra functional requirements.
How often do you run this process? Do you rebuild and retest every time a developer
pushes a new change? Do you do it on a time basis? For example, you run it every night.
How often do you meet and decide that we are ready now to make a new release? That’s
another decision that you have to take. How often would you like to potentially disrupt
the production environment? In some conservative companies, for example, they say
that they freeze their systems in November, because the last part of the year is critical
from a business point of view, and they don’t want to risk the negative impacts of any
change. Either you make a release before November or you wait until the New Year and
stop the last part of the pipeline during this window. You can still work over in the first
part of the pipeline, but you cannot access production. That’s why it’s important to take
responsability for the decision to ship a new release.
Software Production Pipeline
Build
Compile
Unit Test
Integration
Integration
Test
Acceptance
User
Test
Capacity
Performance
Test
Release
Approval
Ok
?
Production
Deploy
Smoke
Tests
Source Code
Repository
Continuous Integration
Push and Merge
Changes
Release 
Approval
Frequent commit, push and merge of changes with automated
build with tests (At least once per day)
We use the term continuous integration to refer to the first part of this pipeline. De-
velopers commit, push and merge changes into the build with a certain frequency (e.g.,
at least once per day). The pipeline will pull the changes from the repository and go as far
as building, testing and integrating the changes with the rest of the system. The result
is something that we can run. There is a working version of the system that has passed
unit and integration tests.
462

Software Production Pipeline
Build
Compile
Unit Test
Integration
Integration
Test
Acceptance
User
Test
Capacity
Performance
Test
Release
Approval
Ok
?
Production
Deploy
Smoke
Tests
Source Code
Repository
Manual
Continuous Delivery
Push and Merge
Changes
Release 
Approval
Frequent automated packaging of releases, fully tested and ready
for manual deployment in production
Continuous delivery goes one step further. In this case we obtain not only something
that is ready to be executed, but it’s also fully tested. It passed all the tests that we have
and it’s packaged ready to be released for manual deployment in production. If we do
continuous delivery we go all the way until the decision step, in which somebody has to
take the decision. And then if the decision is yes, we are ready to start the deployment
in production. The deployment itself can be manual or automated, but it will only start
if someone takes responsability for it. This is difficult if you want to deploy new releases
every second.
During the compilation step, the compile targets a certain runtime platform. The goal
is to run it in an environment that is as close to production as possible. If your users
have different platforms (e.g., different operating systems) then all of your build and test
pipeline have to reproduce those platform. So you will need to do a build for Windows
OS, Mac, Linux, Android and iOS. The infrastructure to do this gets rather expensive be-
cause you need to instantiate all of the pipeline stages for every one of those platforms,
and especially for running tests. The platform is defined not only by the OS, or the Web
Browser, but it may also include other configuration parameters, describing the assump-
tions your architecture makes on the environment in which users will run your system.
For example, different screen sizes, input/output devices. If you want to run a system-
atic test of how your responsive user interface adapts to all of these different platforms,
browsers and device screens, the size of the configuration space to test will explode.
463

Software Production Pipeline
Build
Compile
Unit Test
Integration
Integration
Test
Acceptance
User
Test
Capacity
Performance
Test
Release
Approval
Ok
?
Production
Deploy
Smoke
Tests
Source Code
Repository
Automatic 
Release 
Approval
Continuous Deployment
Push and Merge
Changes
Release 
Approval
Every release is automatically deployed to production
(as long as it passes all automated tests)
If your goal is to continuously deploy, then you have to get rid of this manual decision
point and also make the decision automatic. You need to write some rules that determine
that as long as our new version passes all the tests, it will get deployed automatically.
Such approach puts a lot of trust in the quality of your automated quality assurance tasks.
There should be a very extensive coverage and you need to really trust your tests actually
mimic the conditions under which your users will work with the system.
We can also break the decision in two steps. On the provider side, you approve the
system as being stable and ready for prime time. On the consumer side, you decide if you
want to accept the risk deploying the release. This also depends on the kind of runtime
environment is targeted by your architecture. If you split into the classical Cloud/Mobile,
you can continuously push to the Cloud, but there will be a significant delay before mobile
clients will receive their side of the release, both due to AppStore approval processes
(those are not automated and can add significant delay) as well as mobile phone users
who give a low priority to keeping their apps up to date.
Do you really want to have automatic updates on your mobile OS and the apps deployed
on it? If you accept that, you trust who is making the release decision. You assume that
if they send you an update, this is actually an improvement, in your best interest. For
example, if you want to get the attention of users and nudge them towards accepting
the update, you should always tell them: it’s a security update. If you don’t update your
OS, you’re going to get blamed if you get hacked because you didn’t install the secu-
rity patches. What if instead the OS update would slow down your hardware to increase
the likelihood that you buy a new phone sooner? In the long term, the big question is
how much control users are going to have over ”their own“ hardware. Ownership, trans-
parency and control of hardware devices is becoming questionable, since a lot of software
running on ”your own“ computers is deployed without full knowledge or awareness of the
end users.
464

Overall, automated release approval and deployment acceptance gives you the per-
formance that you need to avoid blocking the software production pipeline because a
committee has to meet and agree that your software can be shipped. If you have a release
that is automatically built, tested, released as well as deployed, you joined the Brave New
World in which every time your users connect to your system they are potentially using
a brand new system. Probably it will not be completely different than the one they used
the day before, because the changes that you introduce are small, but over time these
changes accumulate. Congratulations: your software has reached the age of continuity.
Software Production Pipeline
Build
Compile
Unit Test
Integration
Integration
Test
Acceptance
User
Test
Capacity
Performance
Test
Release
Approval
Ok
?
Production
Deploy
Smoke
Tests
Source Code
Repository
Continuous Integration
Manual
Continuous Delivery
Automatic 
Release 
Approval
Continuous Deployment
Push and Merge
Changes
Release 
Approval
To summarize, continuous integration is developer oriented. Developers already ben-
efit from this because they have a safety net as they make changes to the code. The
pipeline will reliably produce integrated artifacts that they can already test and execute
in their environment.
With continuous delivery we extend continuous integration with a manual decision to
make a release. The decision is based on the information gathered during the automated
tests.
Continuous deployment extends continuous delivery automating both the release ap-
proval decision as well as the upgrade process to deploy the new release in production.
Which is the most common option? It depends on the maturity of the organization
and also on whether they trust their tests to give them sufficient information to make
the decision automatically. When you introduce continuous deployment also the users
need to be aware that they cannot stay behind and need to be exposed to a continuous
stream of changes.
465

High Quality at High Speed
• Every team member can see and inѺuence the build process
outcome
• Any pushed change can be released at any time
• Small, frequently committed changes are less likely to conѺict
• Keep the build fast (Get feedback as early and quickly as
possible)
• Build after every commit, build every night (and run more tests)
• Reliably and reproducibly build any version of the software
(Keep a snapshot of all dependencies)
• Released images are immutable
If you work with continuous integration, delivery and deployment of your software,
you follow these best practices as you work at high speed to produce high quality software
releases.
Every team member has an impact and visibility into the build process. This is about
making everybody responsible and giving them the possibility to fix the problems that
they introduce. It should not be somebody else’s problem to watch over the build, but
everybody has to keep it green.
It’s a big responsibility if you push your changes into one of these pipelines because,
potentially, your commit – if it passes all the tests and there is an automatic decision –
can end up in a release anytime. Developers should be aware about the impact of their
changes, keep the changes small and commit them as frequently as possible to minimize
conflicts with other developers.
It’s important to keep at least the early stages of the build very fast because when you
push a change and you make a mistake you want to see that the build breaks and you
want to get this critical feedback as soon as possible. So that you can still have fresh in
your mind what exactly did you touch and how to undo that problematic change.
Depending on how expensive it is to do run the build, you should do at least an incre-
mental build after every commit. During the nightly builds, since there is more time, run
more tests to extensively check the system as opposed to after every commit when you
want to give an outcome as fast as possible. How are nightly build handled when you
have developers across the world? If the sun never sets on your software development
teams, you need to just make sure you regularly schedule a full build and only some of
your developers will get the results when they wake up.
The build process should be not only reliable, but also reproducible. Based on the
code repository, on your versioning system, you should be able to build any version ever
released in the entire history of your software. The outcome should be the same no matter
466

when you run the build, even years later. As a consequence, you do not only want to
version your code but also take a snapshot and version your dependencies as well. This
is a typical beginners mistake: you do not version your dependencies and then years later
when you try to build an earlier release to reproduce a bug, you’re no longer able to do
so because the dependencies have changed and you cannot access the old versions of the
old dependencies that you used in the old release. Once you depend on something you
want to grab a copy of it, and store it in the repository together with your own code. You
want to keep the copy around because dependencies change as well. And your code is
not necessarily forwards or backwards compatible.
Likewise, once you make a release, you tag it. You don’t change it anymore, so that
the version number and the build number refer to an immutable artifact. While there is
always the temptation to go into your Docker image and make a few quick changes, try to
avoid doing that. You’re doing something manual and the whole process is supposed to be
automatic. In the same way that you typically do not edit the bytecode or assembly code
produced by your trusty compiler, if you want to do those small, last-minute corrections,
you do them to the input source. Then you make a new build, you have a new image with
the new build number.
467

Source Code
Build Stage
Compile
Unit Test
Integration Stage
Static
Analysis
Fetch
Changes
Fetch
Dependencies
Generate
Documentation
Package
Images
Build
Logs
Coding
Style
Test
Coverage
Reports
Container/VM
Images
Deploy
Images
Deploy
Dependencies
Smoke
Tests
Integration
Tests
Integration
Outcome
Deploy
Images
Deploy
Dependencies
Smoke
Tests
User
Tests
Capacity Stage
Deploy
Images
Deploy
Dependencies
Smoke
Tests
Performance
Tests
Capacity
Tests
Production
Deploy
Images
Deploy
Dependencies
Smoke
Tests
Conѹguration
Acceptance
Outcome
Capacity
Outcome
Conѹguration
Release
Approval
Conѹguration
Conѹguration
Push
Changes
Revert
Changes
Ok
No
Acceptance Stage
Developers
Architects
Testers
Users
Release
Approval
The software build pipeline can grow to become a relatively complex piece of software
on its own. There is a whole set of products that software development companies use
to actually automate this whole process. We start from the top where developers push
changes into their code repository. From here the software becomes a piece of data that
has to be processed, checked and transformed. Eventually it flows down towards pro-
duction, where we have the users. Before the final deployment in production step, we
have to make a decision to whether to approve the release based on all of the artifacts,
metrics, and reports which are the outcome of the build and the tests phases.
The first stage is about building the original source code. Building it involves a compi-
lation step, but there can also be a static analysis step. This is also where one can gener-
ate documentation. Unit tests run after a successful compilation. Their coverage can be
measured. And since these tests are focused on each individual component, mocks help
to isolate the component under test. As a result, you have packaged your component as
a deployable artifact: an image.
What happens to the artifacts obtained along the pipeline? They get stored in an image
repository with a certain version number called the build number. The process works
with these images, checking they can be successfully deployed. In the integration stage,
components as they get combined together with their dependencies will be further tested.
Finally, it becomes possible to run an end to end test with the complete system, including
all of its dependencies.
If the integration is successful, we move on to the acceptance testing where sometimes
you can automate this, but sometimes you have a dedicated crowd of test users. The goal
is to observe the end to end behavior by stimulating the system from the user interface.
Different use case scenarios can be validated to reproduce real world usage of the system.
At the same time, the capacity, performance, or even scalability testing stages check
how many resources are required to achieve a certain level of performance. Here the
468

focus is on the extra functional quality attributes. Acceptance reports the system works
correctly, here you care whether the correct result is delivered on time, given the available
resources. The capacity stage deals with the question: how much processing power do
we need to allocate so that the system meets the performance targets? Can we estimate
how much does it cost to run the system under a given workload?
If all of these outcomes are green and the architects (or the automated release approval
rules written by them) agree that this is a good release, then it is pushed in production.
If there is a problem with the build, you have to fix the broken build. One way to fix
it is to undo the changes that broke the build. So this is always a possibility. If it turns
out that it was not possible to correct the problems, then you go back to the latest known
state in which the system used to work. And you restart again from there.
Why do you still need to do testing even after you deploy your system in production?
Because there should be an automatic way to check that the system is successfully ini-
tialized and to assess whether it’s available: did it start correctly? Is it ready to work with
the users?
Such pipeline looks complicated and also very expensive to set up. Remember: soft-
ware to build software is still software, make sure you at least use version control to keep
track of how you change it, if not a fully fledged production pipeline to build your main
pipeline.
You could try to get away with a simpler pipeline. Maybe it’s enough if you build your
system: compile it, package it, but once you have an image, just send it to your users,
skip all the time consuming testing. This is called testing in production. If you put the
new version in production, the user will be in charge to report whether it works or not.
If it doesn’t you will hear loud complaints from the user soon enough. This is a form
of outsourcing the quality assurance for your software to your user community. If users
are willing to pay you to become your testers, then you can consider skipping most of the
pre-release testing. Clearly, users should be aware they are using partially tested releases
(sometimes these proudly sport a greek letter in their name: alpha, or beta). Otherwise
your reputation will suffer.
In every piece of software, testing in production is always the case. Only in some cases
one can afford to also do unit, integration, acceptance testing and try to avoid pushing
out bad releases.
469

Types of Testing
Unit
Test individual components immediately after they
are built (if necessary mocking their
dependencies)
Integration
Test the complete system (integrated with all
dependencies)
User
Test the end-to-end behavior from the user
interface (check use-case scenarios work) - only
partially automated
Capacity
Observe whether extra-functional quality targets
(e.g., performance, scalability, reliability, capacity)
are met to determine the resources required for a
given workload
Types of Testing
Note: Every company performs testing in production with end-
users, some companies can afford a dedicated staging
environment for running all of the other types of tests before
shipping a release
Acceptance
Test the system in an environment that is as close
as possible to production to determine whether or
not a system satisѹes the acceptance criteria and
to enable the customers to decide whether to
accept or reject the new version of the system.
Smoke
Quickly probe the system to check its successful
initialization and state of readiness and availability
470

Types of Release
• Big Bang/Plunge: Everyone switches over to the new release
after some down-time (with high hopes and no way to go back)
• Blue/Green: Everyone switches over to the new release at once
and can revert back in case of problems
• Shadow: The release is deployed but not used 
(Dark launches test the release with real user input without
revealing the output)
• Pilot: The deployed released is evaluated by a few users for a
limited time
How do we make different types of releases?
The original, oldest type of release is the Big Bang release, where we do it in one shot:
every user goes to the new version. Deployment takes some effort, produces some down-
time, some disruption because of the release requires to first switch off the existing sys-
tem before the new one can replace it. Hopefully the new one will work. It must work, as
there is no way to go back in case it doesn’t. The Big Bang release is a high risk proposi-
tion, which you want to avoid.
If you have a good deployability of your architecture, you can support other types of
releases.
A blue/green release still involves switching as atomic step: every user will go to the
new version. But in case there are some problems, you still have the old software de-
ployment around and you can easily switch back and forth between the two. This already
gives you a very important advantage, which is the ability to undo a failed release. This
however makes it more expensive because you need both the blue system and the green
system. So the number of virtual machines that you need doubles. The old ones should be
kept immutable. Once the new ones are ready, you switch. If everything works fine, then
after a certain time you can stop and decommision the old. If the new version doesn’t
work, you can go back.
As opposed to the blue/green release where all users will see either the new or the old
system, if you do a shadow release, you make the release but you don’t use it. Why? Why
do we go to the trouble or building the system, testing and releasing it and then we don’t
give users access to it? What you want to do really is to connect the new release to the
input of the users so that the system is actually undergoing the production workload.
But you don’t show the output to the users, yet. Users still sees the output from the old
version, since the new one is still kept in the shadow. If you do so, you have a chance
to observe and compare the output. And if the output is different and you don’t expect
it to be different, maybe you just detected a problem. Shadow releases are subtle way to
let real users test your system with the real production workload but without affecting
the users in case something goes wrong. Once you are confident that the behavior of the
new version doesn’t have any regressions, you can bring it out of the shadow and make
also its output visible to the users.
471

Another technique is to do a pilot release. When you make a change, you deploy it,
but you only make this visible to a small set of pioneer users. The users are going to give
feedback and then you can decide if it is worth to extend the access to this new feature
to the rest of the user population. Here the main challenge is: how do you select the
users that are willing to try the new version? You need to have a set of trusted users who
provide constructive criticism and – in case something goes wrong – they will not blame
you. One way is to call for volunteers. Have you every started an app and there was a pop
up asking you to try to switch over to a preview of the new version? You could have been
randomly selected. Or maybe they observe your interactions (or your poor reviews) and
they see that you could potentially benefit from these new features so chances are you’re
willing to give it a try.
Types of Release
• Gradual Phase-in: The deployed release is used by a gradually
increasing number of users. Requires to support multiple active
versions in production.
• Canary: A gradual phase-in release with very few initial users
who do not mind dealing with failures
• A/B Testing: Roll-out multiple experimental versions to different
subsets of users to pick the best version to be eventually be
used for everyone
There are also releases in which the switch between old and new version happens grad-
ually. To do so, you run multiple versions of your system in production, and gradually
spill over more and more users to the new version. The pilot is just with a small set of
users and you’re not sure yet if you will go ahead with every user. After a successful pi-
lot, you can gradually get more and more users on board. This way you can increase your
confidence that the new version is actually stable and powerful enough.
Canary is a similar term for a pilot (the term comes from the Canary in the coal mine).
In this case you make a Canary release because you don’t trust its quality but you make
it accessible to users that are aware of this. Still, they don’t mind having a few crashes
because they know they’re working with an unstable system.
A/B Testing is also related to the concept of having multiple active versions in pro-
duction. It is used if you do not know when you make a change, whether it is actually
an improvement or not. You make multiple experimental versions available to different
users. Based on some metric (typically this is some kind of monetary metric), you com-
pare which version outperforms the others. For example, which color of the links makes
people click on the ads so that we can make more money? That’s one of the original use
cases for this technique.
To support these advanced types of releases, the infrastructure for your continuous
472

integration does not only contain all the quality assurance phases that we have discussed,
but it actually becomes much more complicated because you have to collect feedback
metrics that you measure from the user behavior in production. Then you need to feed
them into some statistical inference to see whether your experiment is validating the
hypothesis that version A is better than version B or not. You can afford this when you
have millions of users coming to your system every day. Very quickly you get the results
necessary to support your experiments. If you just have 10 users it’s very difficult to draw
any conclusions by passively observing their behavior.
Related to this is also this idea of making releases which have partially implemented
functionality. For example, you show buttons on the screen. But you don’t implement
the corresponding behavior of the application. If the user never clicks on these buttons,
it means that the feature is not wanted by your user population. What’s the point of
investing in the implementation of a feature that nobody is interested to click on? If it
turns out that people are actually clicking on the button, then you show them a little
message that says: ”Please sign up to be notified when we release this new feature“. If
they sign up, they are really committed and you have a chance to deliver to them a rough
implementation as quickly as possible. But now you know that some users are actually
interested about it. Now you can justify your investment into building it, as opposed to
building it in the hope that users will come. You will be surprised about what you can
learn from this: it can completely change the way you prioritize your backlog.
Gradual Phase-In
Version 2 Launch
Version 3 Launch
Version 4 Launch
Version 5 Launch
Version 1 Retired
Version 3 Recalled
Version 2 Retired
Version 4 Retired
Time
Clients Routed to Each Version
1
2
4 5
3
Here is a visualization of the gradual phasing type of release. On the vertical axis we
have the number of users for each version and horizontally we have time.
At the beginning we have version one in production. And every user is using this ver-
sion one. Then we launch the new version, but we don’t do a Big Bang release where
every user switches over at the same time. Instead, we actually have a gradual phasing
were initially only small number of users have access. As we observe the logs, we can
conclude that it doesn’t crash, users seem to be happy with it. There are not a lot of bug
reports, so we gradually move over more and more users. Until version one gets retired:
nobody is using it anymore. So we might as well switch it off.
473

These transitions can take some time, especially if you leave a choice for users whether
to upgrade or not. While we’re still in that first transition, we attempt a pilot for version
three. We launch it, but very soon discover that it’s not a good one. There is no accep-
tance by the user that have a chance to look at it, so you quietly pull it back and switch
back every user to the previous version 2. Since version 3 was only seen by a small mi-
nority of the user population, the failed pilot is not disruptive. Version 4 happily works
better, so gradually every users will be migrated to it.
Depending on how each transition takes, you might end up for a given point in time
to having to maintain multiple versions of your system in production at the same time.
This is more expensive than a simple istantaneous switch. You have to ask yourself how
many of those can you afford to keep running, and this will naturally give you a limit on
how often you can make new releases depending on how long does such transition takes.
474

Jez Humble and David Farley
Essential Continuous Engineering Practices
˾. Automate everything
˿. Push changes regularly
̀. Don't push on a broken build
́. Commit only if locally green
̂. Watch the build until it's green
̃. Never go home on a broken build
̄. Never comment out failing tests
̅. Always be prepared to revert
̆. Time-box ѹxes before reverting
˾˽. Direct changes in production are forbidden
Now that we have seen both how to build and release, let’s conclude the continuous
delivery discussion with a number of best practices that you may want to adopt into your
development toolbox from now on.
Make sure that the process that you use to write your software and test it is automated:
don’t try to do things manually because you wouldn’t do them anyway and if you did, you
would be slow.
Push your changes regularly. That means at least once a day. You attend the stand up
meeting in the morning, you work, and then before you leave you should push.
If the build is broken, you shouldn’t push more stuff on it to break it even more. Before
you push, make sure that your local build is green. Never push something that doesn’t
compile, because then you would break the build for everybody else.
After you push your change, don’t go away. Watch the build until it is green. Also,
never go home on a broken build, so if you break it, it’s your responsibility to fix it. In
the worst case, you just have to undo the work of the day and tomorrow it will go better.
A big temptation for fixing a broken build is to remove failing tests. You can always
blame the tests and not your code. My code is good and these tests are obsolete so might
as well comment them out. If you do that, you reduce the coverage and of course then you
get a green build, but it will be a fake green build (just like when someone recommended
to slow testing down to improve the pandemic statistics, but I digress). If the tests are
failing and you blame the test, then fix the test. Don’t just skip them. How long will you
need to stay to fix a red build? Some developers never go home. You should time box it:
if there is a problem with my latest change and I cannot fix it within one hour, then I can
revert the changes. The work of one day has disappeared, but I can go home to rest.
There is always the temptation to fix problems directly in production. You forgot a
475

comma. You open into the config file, you edit it. And the lights are on again. But this is
not enough, because where did the missing comma come from? After rescuing the day,
don’t forget to fix it at the source. And then trust your tools to smoothly produce a new
release, push it in production and everything is fixed. It might take a little bit longer, but
the problem will be fixed for good and not come back to haunt you again the next time.
If you choose to adopt this distilled wisdom, then you are actually practicing continu-
ous software engineering.
476

Tools
Continuous Integration
• Cruise Control
• Jenkins
• Travis
• UrbanCode
• GoCD
• Packer
• GitHub Actions
Container Technology
• chroot
• Docker
• Docker Compose
• Docker Swarm
• Kubernetes
• Mesos
Build Pipeline Demo Videos
Explain how to setup automatic software production pipeline for
your tool:
• How do you build a component?
• How do you test a component?
• How do you plan integration testing in your pipeline?
• How do you plan capacity and acceptance testing in your
pipeline?
• How do you package a new release?
• Which types of release is supported by your tool? Demo at least
one type.
477

Build Pipeline Demo Videos
Deployment:
• How do you deploy an application?
• Can you automatically deploy the application if the tests are
green?
Failure Scenarios:
• What happens if something goes wrong in your build?
• Does the tool provide reports or logs after each phase?
• Can you show different examples of a broken build and how to
ѹx them?
Container Orchestration Demo Videos
• What is a cluster? How do you create a cluster?
• How do you conѹgure Kubernetes?
• How do you create a deployment?
• How do you create nodes and pods?
• How do you open an application to the users?
• How do you create multiple instances of the an application?
• Which types of release is supported by Kubernetes? Show at
least one.
478

Today’s XKCD is about the benefits of automation. Should you just get something done
or invest into automating it? When you expect that you will do it very frequently, it may
be worth to invest some time and effort into automating it so that eventually you will have
more free time. This is also our ultimate goal too: let your automated build pipeline work
on your release. In theory, automation takes over and your life is better.
The reality: it turns out the software that you have to develop for running the software
production pipeline needs to be written, tested, debugged, or it needs to be selected,
deployed and configured, and tested and debugged. As usual the development never
ends: instead of having more free time, you don’t have any time left for the original task.
But what if you could automate the writing of software production pipelines as well?
Before we jump into infinite recursion, remember the lessons of how to bootstrap pro-
gramming language compilers and see if you can apply them to this other problem.
479

Virtualization
Real Physical Hardware Machine
Virtualization Software (Hypervisor)
Virtual Machine 1
Virtual Machine 2
The enabling technology for deployability is virtualization. More recently virtualiza-
tion evolved or specialized into containers. Virtualization is about isolating your code
from the actual environment in which it runs, like when you deploy it up in the Cloud.
480

Virtualization
Hardware
Hypervisor
VM 1
OS 1
App 1
VM 2
OS 2
A 2
VM 3
OS 3
A 3
A 4
A 5
Hardware made into Software
Hypervisor: Intermediate
software layer that decouples
the above software stack from
the underlying hardware
Virtualization can be applied
to different hardware
resources (CPU, memory,
storage, and network)
At the bottom we have the actual hardware. On top, we place a layer of isolation: the
hypervisor. On top of it, we obtain virtual machines on which an operating system can
run your applications. From the perspective of your application, the virtual machines are
like hardware, but they’re actually simulated or emulated by this piece of software, itself
running on the real hardware.
This isolation layer makes it possible to deploy any application on any physical run-
time environment, since the application is decoupled from it. Clearly, there is a cost.
The larger the gap between the actual hardware and the one emulated by the virtual ma-
chine, the more expensive it will be to run the application with a sufficiently good level
of performance.
Even if the virtual CPU and the actual CPU are the same, the other important goal of
virtualization (which is also shared with containerization) is to isolate the various ap-
plication and their operating systems from each other. If you run an operating system
directly on the hardware, all the applications or processes in this operating system are
both aware of one another and can somehow interfere with one another. This may be
a feature, since they can use, for example, the shared memory or shared file system to
exchange data. This may be a security issue, if the applications run on behalf of differ-
ent users, who may be business competitors. Running these conflicting applications on
separate virtual machines guarantees isolation between them, almost if they would be
deployed on physically distinct machines.
Other advantages of virtualization include so-called elasticity: How much of the un-
derlying physical hardware should be dynamically allocated to each of the virtual ma-
chines? Depending on the workload of each application, the virtual hardware can be
made more or less powerful.
So in other words, as we’ve seen many times in software architecture, adding some
intermediate software layer helps to decouple what is above from what is underneath.
Let’s see different examples of how we can play this layering game.
481

Virtualization
Hardware
Host Operating System
Hypervisor
Virtual Machine
Guest Operating System
Applications
Hosted hypervisors require an
operating system to run
Example: VirtualBox, VMWare
Workstation
In this scenario, we have two operating systems running: the host and the guest. The
host runs directly on the actual physical hardware. From its perspective, the hypervisor
is an application. Within this application we can run simulated hardwares on which we
are going to have the guest operating system that will run our own applications.
This was one of the early virtualization approaches. This is also what you still do to-
day when you run virtual machines on your computer. For example, you have the MacOS
operating system running on the Apple hardware. On top of it you can run Linux, Win-
dows, or whatever other operating system. This is great because the applications which
depend on Windows can still run on the Mac. From the point of view of the application,
they are not aware, but it can be convenient for users that cannot afford to buy separate
computers to run different operating systems. For some reason, the opposite scenario,
running MacOS in a virtual machine on top of a different OS, is not so common.
Here we also have many layers and the performance suffers. Let’s try to optimize this
architecture.
482

Virtualization
Hardware
Hypervisor
Virtual Machine
Operating System
Applications
High performance, native
hypervisors run directly on the
hardware
Example: original IBM
Mainframe z/VM, Xen, VMWare
ESX
What if we blend the bottom layers? If we decide that the only purpose of this partic-
ular piece of hardware is to host virtual machines, then we do not need a fully fledged
operating system on the hardware. We can delegate the role of the hypervisor to just
abstract the hardware and use it to offer virtual machines that we can use to run mul-
tiple (guest) operating systems. This reduces the redundancy in the stack, and can be
optimized for the specific purpose.
483

Hardware
Host Operating System
Container
Applications
Lightweight Virtualization with
Containers
Containers are lightweight
solutions to isolate
applications that can share the
same operating system
Example: Docker, rkt, OpenVZ,
Hyper-V
The other approach comes from the opposite side. There are lots of layers between the
application and the hardware. What if we can assume that all applications will run on
the same guest operating system and this happens to be the same as the host? Then the
goal becomes to isolate them as if they were running in separate virtual machines, but
without the overhead.
This is why containers are sometimes called ”lightweight virtualization“ within the
same host operating system. If you can spot the difference, the applications are directly
running on the host operating system, but the applications and operating system are sep-
arated by the container. This layer provides the needed isolation without the overhead
of having a hypervisor underneath.
484

Virtual Machine
Container
Footprint
GB
MB
Boot Time
Minutes
Sub-Second
Guest OS
Different in each VM
Uniform for all
containers (Linux)
Deployed
Components
Multiple, Dynamic
Single, Fixed
Isolation
Hardware
Namespace OS
Sandboxing
Persistence
Stateful
Stateless
Overhead
High (with Emulation)
Low (Native)
Baking Time
Slow
Fast
So which one is better? Containers or virtual machines (VM)? In terms of resources,
the term lightweight virtualization is justified for containers. Virtual machines need to
simulate whole hardware environment in which to run a full operating system: they con-
sume gigabytes of memory. The footprint of containers typically depends much sooner
on what you will deploy inside, since they have a smaller overhead.
How long does it take to start a virtual machine? You have to boot the emulated hard-
ware, you have to start the operating system, finally you have to wait for your application
to start. Depending on the operating system and application, this may be a relatively slow
process. If you have a container, the operating system is already running, you just have
to initialize the container and start the application. This tends to be much faster.
What about security? What about the isolation? If the virtual machine has full control
over the hardware running a dedicated hypervisor, it could take advantage of specific pro-
cessor features to achieve isolation. With containers, it is questionable whether two ap-
plications deployed in separate containers sharing the same operating system are really
as isolated. Namespacing and other sandboxing techniques pretend that processes run-
ning on the same operating system are inside a separate container and therefore should
not detect the presence of each other nor interfere with each other. Ultimately you get
what you pay for.
Baking refers to the process to build the images which can contain a whole operating
system, including the whole application and its data. This is a slow process when building
an image for virtual machine. With a container this is relatively faster because the image
simply needs to store less bits.
485

Hardware
Hypervisor
Virtual Machine
Operating System
Applications
Container
Containers inside VMs
Containers and VMs can be
combined to get the best of
both worlds (however
performance will suffer
compared to raw iron)
Example: VMWare vSphere
integrated containers
Good ideas can be combined with each other. Typically one needs to pay a Cloud
provider to rent virtual machines. If you choose an infrastructure as a service provider,
you get billed for the time during which your virtual machine is running. The more appli-
cations you run, the more expensive it gets because each should be deployed in a separate
VM to keep it isolated.
Still, within a VM you can run whatever operating system you want. What if you install
docker inside a virtual machine? Then each application can be deployed within its own
container and you just need to pay for one virtual machine, which needs to be allocated
enough capacity to host all containers. Also in this case, the operating system can be
optimized for the specific purpose of hosting containers.
486

Images and Snapshots
Hardware
Hypervisor
VM 1
VM 2
VM 3
Image
Repository
Snapshot
Repository
Virtual Machines or Containers are booted using a preconѹgured
image selected from a repository
Snapshots or checkpoints of the entire virtual machine state or
the container ѹle system can be saved, backed up and restored
What are these images in which software component releases are packaged as?
While simulating or virtualizing hardware, it becomes possible for the hypervisor to
take a snapshot of the state of execution of the virtual machine. This includes a copy of
the whole memory, a copy of the processor state. This snapshot can be written into a file.
By reading it, it is possible to restore the state of the virtual machine, which can continue
running from a known state.
This uses the same techniques that your laptop uses to save a snapshot of your system
state just before it runs out of battery. The laptop is going to take all the volatile infor-
mation, write it out the disk so that when you plug it back in and you switch it on, it will
restore the running applications and operating system from the image.
We can benefit from this technique to provide an image from which we can start our
application in a known state. This image is prepared (during the image baking process)
by creating a virtual machine, installing the operating system as well as all of the appli-
cations. We start them and then we freeze it. Later we can copy it and deploy where we
want to run it. Since it’s just a matter of initializing the virtual machine from the given
image and all the necessary software is already installed and ready to go.
A similar process is used for container images as well.
487

Virtual Machine Migration
Hardware
Hypervisor
VM 1
VM 2
VM 3
Hardware
Hypervisor
VM 3
Virtual Machines can be moved between different physical hosts
for load balancing, workload consolidation and planned hardware
maintenance
Some hypervisors support live migration during which the VM does
need to be stopped before it is restarted elsewhere
Virtual machines also support live migration from one hosting environment to an-
other. In case the hardware fails, we can recover the virtual machine from its latest snap-
shot and keep running it elsewhere. If the hardware needs to be upgraded or changed,
we can temporarily move the virtual machines to a different physical hosts.
Live migration is something that used to be unthinkable and impossible to achieve with
physical hardware. In other words, thanks to virtualization, the deployment decisions
you make can be easily changed also after the system has been started. As we will see,
this can also help with elastic scalability, load balancing, or workload consolidation.
488

Inverted Hypervisor
Hardware
Hypervisor
VM 1
OS 1
App 1
VM 2
OS 2
App 2
VM 3
OS 3
App 3
HW 1
Inverted Hypervisor
Virtual Machine
Guest Operating System
Applications
HW 1
HW 1
HW 2
HW 1
HW 3
The hypervisor supports
multiple isolated virtual
machines sharing the same
hardware
The inverted hypervisor runs
one virtual machine over
multiple distributed hosts
What is the purpose of the hypervisor? The hypervisor multiplexes multiple virtual
machines over one shared physical machine. This helps with using the hardware as ef-
ficiently as possible. The single physical processor will be mapped to multiple virtual
processors that are going to run over it. If each application is not always fully utilizing
its processor, it would be inefficient to allocate a full physical processor to it. By sharing
the hardware, we can make sure it gets fully utilized all the time by multiple applications.
If we flip the structure then we have the inverted hypervisor. Here we can take one
application and its operating system and then run it across multiple physical hardware
resources. What if the hardware that you have is not powerful enough for one applica-
tion? What if there it not enough memory? To reach the capacity needed to run your
application, you can compose multiple pieces of hardware and thanks to the inverted
hypervisor, they will look like it’s a single virtual machine, a very large one, which your
application can take advantage of. It is the inverted hypervisor responsability to aggre-
gate multiple physical hardware resources and partitions the large application’s workload
among them.
489

Virtual Hardware =
Software
Deployment Conѹguration and Resource Allocation (CPU, RAM,
Disk, Network) becomes part of the Software code (and should be
managed accordingly with versioning, change management,
testing)
Different, independent and isolated deployment conѹgurations
should be used at different stages of the software production
pipeline (never mix production and testing environments)
Virtual hardware becomes software. This is the second technological advance that
brought us into the age of continuity. The first one was that the production of software
is software too. Now we also realize that the hardware itself becomes software.
All of the configuration details about the container that you need in order to be able to
run the software in production becomes a piece of software. This is specified using the
corresponding language and then you can use version control, the proper change man-
agement processes. You can even do testing with it to make sure it doesn’t get modified
incorrectly.
We saw that in the software production pipeline we have to specify configuration for
the integration stage, the capacity stage, the acceptance stage and the production stage.
This configuration is not only the configuration of your software application, but it’s also
a configuration of the virtual hardware environment that you will use to run the system.
This configuration may change as your software evolves (adding features requires more
processing power). It may also change as a given version is running (adding more users
requires more memory). Resource allocation becomes as easy as pushing a new commit
of a configuration file. Let the Cloud provider scramble to install the hardware capacity
that you allocated with a few keystrokes.
This configuration information becomes part of your software. The code you write to
implement your component needs to include both the tests that will be run during the
pipeline but also the specification of the virtual execution environment in which it will
be deployed into.
This lecture on deployability concludes our transition from design time to run time.
In the next lectures we will discuss how to design architectures which can deliver critical
runtime qualities: scalability, availability and flexibility.
490

References
• Jez Humble, David Farley, Continuous Delivery. Reliable Software Releases Through Build,
Test, and Deployment Automation Addison-Wesley, 2010, ISBN 978-0-321-60191-9
• Len Bass, Ingo Weber, Liming Zhu. DevOps: A Software Architect's Perspective, 2015, ISBN 978-
0134049847
• Michael T. Nygard, Release It! Design and Deploy Production-Ready Software, 2018, ISBN 978-
1-68050-239-8
• Gene Kim, Kevin Behr, George Spafford, The Phoenix Project: A Novel about IT, DevOps, and
Helping Your Business Win, 2013, ISBN 978-0988262591
• Mendel Rosenblum, 
, ACM Queue, Volume 2, issue 5,
August 31, 2004
•
•
• XKCD 1319: 
The Reincarnation of Virtual Machines
The Twelve-Factor App
Open Container Initiative
Automation
 
 
 
 
 
 
 
 
491

Software Architecture
Scalability
Prof. Cesare Pautasso
cesare.pautasso@usi.ch
10
http://www.pautasso.info/
 
 
Contents
• Scalability: Workloads and Resources
• Scale up or Scale out?
• Scaling Dimensions: Number of Clients (Workload), Input Size,
State Size, Number of Dependencies
• Location Transparency: Directory and Dependency Injection
• Scalability Patterns: Scatter/Gather, Master/Worker, Load
Balancing and Sharding
492

Scalability: this is the first of the quality attributes beyond raw performance that have
to do with our system being in production at runtime. The question is: how do we scale
the architecture?
When you hear about scalability, the term is often tossed around in discussions, scala-
bility by itself doesn’t mean anything. You have to be a bit more precise. You have to be
able to put it into context. In which dimension do you want to scale your architecture?
You can then produce a completely different design if your goal is to scale the number
of concurrent clients that you want to support or you want to scale your system to be
able to produce or consume a large amount of data. Or you want to be able to scale your
architecture to be deployed over millions of different devices.
First we will define what is scalability and what are the challenges involved into de-
signing systems that can scale. And then we will see what kind of solutions, what kind
of patterns we can introduce in our architecture so that we can claim that our system is
going to scale.
We start by defining scalability in terms of workload, or scalability because we want our
system to fully use a large number of resources. We will see the difference between scal-
ability techniques that involve decentralization or scaling up towards larger and larger
centralized systems. What are the basic mechanisms at the foundation of architectures
that can scale? We need to be able to enable the dynamic discovery of components, their
identity and location: who is running where. And then we will look at more detailed
scalability patterns: master/worker, load balancing and sharding.
493

Scalability and Workload
Workload
Response
Time
Workload
Throughput
(req/s)
Workload = trafѹc, number of clients or their number of
concurrent requests
Ideal system: response time not affected by the workload;
throughput grows proportionally to the workload
Real systems will show this behavior up to their capacity limit
To observe the scalability of our system, we need to measure first its performance. The
performance can be measured in terms of latency with the response time. To perform a
certain amount of work, the system takes some time. We can also look at performance in
terms of throughput: how much work over time can our system perform?
In these charts we will see that the performance of the system is a function of the
workload of the system. The busier your system is, the longer it will take to process
the work. We start from a situation in which there is no impact of the workload on the
response time. As the workload increases, the response time stays flat. It always takes
the same amount of time no matter how busy the system is. Also we can see that the
throughput grows together with the workload. The workload can also be defined in terms
of requests per second. We can observe that there is a growing number of clients sending
traffic to our system but the system can absorb the traffic.
The more work you send, the more the system gets busy. But since there is no impact
on the response time, we can say that it scales and it will do so until it reaches its capacity
limit. That’s where it stops scaling with this behavior. And we say that it has reached a
saturation point. The system is saturated because even if you keep increasing the work-
load (as shown with the dotted line) and send more and more requests, the system cannot
cope with them. We recognize the saturation phase in which the throughput stays flat
and the response time starts to increase.
Let’s not confuse the throughput with the response time. When the system scales,
the response time remains flat and the throughput increases proportionally to the work-
load. When the system saturates, the throughput flattens and the response time starts to
increase. This can be intuitive to understand: imagine there is a queue of request some-
where. Why would requests stay longer and longer in the queue? because the processing
capacity is limited and only so many requests can be processed per second.
494

If you further increase the workload, you go from saturation to overload. And here we
see that the system is completely overwhelmed. As a consequence, its behavior becomes
uncontrolled, its queues get clogged, the throughput goes down and the response time
goes beyond what the clients will accept. That’s when you start to see disconnections.
Clients start to get timeouts. Clients will react by resending their requests, further grow-
ing the length of your queue and overall the system becomes unstable.
To summarize: the workload your system is subject to can change over time. Depend-
ing on the level of the traffic, the number of clients, the number of concurrent requests
from the clients, we can observe the performance of the system in relationship to the
workload. Linear scalability occurs when the response time is not affected by the work-
load and the throughput grows proportionally. The more work you give, the more the
system will be busy, but the system can keep up. Infinite scalability does not exist, as ev-
ery real system has a limited capacity. The linear scalability behavior will occur only for a
certain workload range. There will be a point in which there is just too much work. There
is too much traffic and this system has reached its saturation point and if the workload
keeps growing it becomes overloaded.
So this is true for every scalable architecture: there is always going to be a capacity
limit. The question is how do you find the limit and how do you push the limit a little bit
higher.
495

Scalability and Workload: Centralized
Workload
Response
Time
Workload
Response
Time
 Many
1
Where do these curves come from? There is a growing number of clients – this is where
the work comes from – which call the component in the middle. In your architecture
model you just draw two boxes, but then you know that one of the boxes will be instanti-
ated many times, deployed all over the world and there will be many of those at runtime
that depend on this central component to perform some work and they have some ex-
pectations about its performance.
If we look at this particular scenario, then we can say that the limit to the scalability
that we have comes from the fact that this is a centralized architecture, in which there is
one element that has to work for servicing requests coming from many elements. There
is a fundamental imbalance: we have one element in the center working for all the others,
which becomes increasingly busy the more clients you attach to it.
We can also observe its behavior by introducing a message queue connector. When
the system is working within its capacity, it can consume the messages produced by the
clients without them stacking up in the queue. In this producer/consumer scenario, the
producer and consumer work at the same rate: every message to be processed will take
the same time as we can see from the response time.
However, if we add more clients, there is more work to do: there will be a point in
which the system is falling behind and the queue doesn’t get emptied anymore. In the
worst case scenario, also the queue gets filled up beyond its capacity and it has to start
preventing clients from adding more messages.
In a centralized architecture the workload of a single component comes from a variable
number of clients. Their number can grow or change dynamically. There is only one
component that is going to service them. We can say that this component becomes the
bottleneck: if this component is not fast enough, the architecture will not scale beyond
the capacity of its centralized components.
496

How to scale?
˾. Work faster
˿. Work less
̀. Work later
́. Get help
The question that I would like to ask is: How would you solve that scenario so that we
can avoid completely filling up the queue? Clustering over many servers. Distribute the
workload on multiple machines: good ideas. So let’s spread out the load over multiple
servers. If one server is busy, we find another one.
Another interesting idea is to put a time limit on the messages, so once we enter a
message in the queue, we expect that this message will be processed within a certain time
window, and if it takes too long, we drop expired messages. We can empty the queue this
way without doing any work.
Another way is to make the server more powerful and in general to balance the pro-
cessing power between producers and consumers. The queue helps to even out tem-
porary unbalances, but overall you need to make sure the queue length does not grow
unbounded.
Caching is also a great idea which helps to scale. This will basically make it unnecessary
to send a message into the queue, since the client already knows the result. Avoid to
recompute already known results. Invest into extra storage for the cache so that you
save CPU cycles.
The problem of scaling comes from having to deal with a workload peak. Such peaks
could be predictable or they can be totally unpredictable. Once your architecture has to
absorb such a peak, you have to figure out how you design it by taking advance of your
suggestions.
First, a very abstract and very simple principle that can help you is: Whatever you’re
trying to do, just do it faster. If you do it faster, it takes less time. That means that in
the same time you can process more requests. This is where your algorithm optimization
skills come in to play. Reducing the complexity of algorithm makes your system more
efficient. If you cannot squeeze the algorithmic complexity any more, you can throw
money at the problem: if you can afford it, you can buy a faster computer. Or you just
wait a couple of years and then Moore’s law is going to come to your rescue. Eventually
things that used to be impossible or too expensive, they become affordable and easy to
497

do, thanks to faster and cheaper hardware.
The second idea is also simple: instead of performing the whole computation, we’re
going to compute an approximate result. This should also takes less time, the result may
not be as good, but at least clients do not timeout.
Another variation is not to return the freshest result, but to add a cache. Caching helps
to reduce the workload because you remember historical results and then you look them
up before sending or processing an incoming request.
Or you can always push back and say I’m not going to do this for you. You do it yourself.
And this goes even beyond caching. In this case we are actually offloading the work: we
are not doing this here because I have limited capacity and you do it yourself that you have
plenty of processing power. If you think about how powerful the latest smartphones are,
you might as well off load the work on so-called edge devices. Parts of the components
that used to require a powerful server to execute them, get to run elsewhere: this way
you will make your server in the centre more scalable because the work is done on the
mobile devices at the edge.
If you are in an emergency situation, sometimes you can attempt to control which
features are you going to offer while you disable unimportant features. So you partially
degrade the user experience, but you can support a much larger number of clients. Users
will understand: you are undergoing a huge load at the moment and here is a very simple
version of the application that still covers 80% of the use cases, but we have cut off the
expensive ones. Come back later after the peak is past and we will reactivate them.
Another technique that you can use if you know that the peak is short lived is to buffer
the work and then empty the queue after the peak is gone. If you survive the worst, you
can catch up later. You don’t necessarily need to perform the work right now if you don’t
have such real time constraints. If there is no deadline, you can absorb the peak when
you have a time in which you’re not so busy.
If all of these ideas are not enough. There is still another one which is also what you
suggested right away: If you cannot keep up by yourself – there is always a limit on how
much you can do – you will need to ask your friends for some help. So in this case, what
you want is to be able to create multiple servers, add multiple cores, start additional
parallel threads. Will introducing parallelism into your architecture help you to scale? Is
your software going to benefit by adding more computational resources?
498

Scalability and Resources: Decentralized
Resources
Speedup
Resources
Speedup
 Many
1
Let’s look at how parallelism affects scalability. In this scenario, when you want to
spread out the work on a cluster, you reverse the relationship between the clients and
the central server. Here we have actually one client that now has to spread out his work
across multiple servers. So don’t be confused by the location of the boxes. In the previous
picture, the clients were outside sending work to the server in the center that was getting
overloaded.
With a decentralized architecture we are inverting these roles, we have one client in
the center which instead of talking to only one server, it is actually going to send the work
across a large set of servers. The boxes on the outside represent multiple copies of the
server which can receive the work.
If we look at it from the queue point of view, we are in a situation in which the client
is overloading the server. The queue is getting full. There is too much work. So if you
want to scale beyond the capacity of one consumer, you need to add more copies of the
server. Working all together, they will be able to work in parallel and consume the queue
and bring back the system in balance.
This is what we call a system in which the bottleneck has been solved by replicating the
server components consuming the work to do from the queue. Even if you have a single
client that is producing too much work for one server, you can absorb it and handle it in
a scalable way by adding more and more resources to process the work.
Here we don’t see performance as a function of the workload. Here we can try to study
how does the performance behave in terms of the amount of allocated resources? How
many copies of the servers we need to process the work? We still can measure the re-
sponse time, but the X axis has changed. More precisely, we fix the workload. If we had
more resources, if instead of just one server we add a whole cluster, or if we go from a
cluster to a data center, or if we go from a data center to multiple regions all over the
world: how is the increase in resources going to affect the performance?
499

Scalability and Resources
Resources
Response
Time
Resources
Speedup
For the same workload, will performance improve by adding more
resources?
Ideal system: linear speedup with inѹnite resources
Real systems will only beneѹt up to a limited amount of resources
We have a scalable architecture if the response time decreases when we increase the
resources available to process the given workload. With an ideal system, we will achieve
linear speed up. The speedup (the performance relative to a centralized solution) im-
proves linearly. It scales as long as the speed up grows linearly when you increase the
resources.
For example, if you start from one server and then you double the capacity of the sys-
tem, you now have two servers. With a scalable architecture you would expect the re-
sponse time to be half of the previous configuration. And if you double the resources
again, then you expect the response time to four times lower than the original one.
Can you keep doing this with an arbitrary amount of resources? Can you always double
the amount of servers, double the number of cores with a processor and expect the per-
formance to improve just like that? Also in this case, there is a limit after which actually
you don’t get any benefit. Sometimes it even gets worse: you buy one more server and
the performance no longer improves.
There is a point in which the speedup doesn’t grow linearly, but it actually starts to
become flat. This is another limit of the scalability in terms of resources, with a fixed
workload. Sometimes this is also called the law of diminishing returns. So if you dou-
ble, if you go from one to two CPU cores. You have a big performance improvement in
absolute terms. But then if you double again, for example, go from 20 to 40 resources,
the improvement is not as big anymore. There will be a point in which it becomes too
expensive to add even more resources for the very small improvement that you actually
get.
500

Centralized or Decentralized?
Centralized
Bottleneck
Consistent
Client/Server
Single Point of Failure
Decentralized
Partial Failure
Churn
Hot Spot
Peer to Peer
Since we have seen these two sides of scalability with a variable workload in a central-
ized system or with a decentralized system with an increasing amount of resources, how
are you going to design your architecture? Do you choose a centralized or a decentralized
one?
Can you connect these two concepts with their implications?
In a centralized architecture some elements become a scalability bottleneck. Com-
pared to other elements with a bigger capacity, the bottleneck is the weakest link along
your chain where the performance is lost: the first element whose capacity gets fully
utilized.
Similar to a bottleneck, a hot spot is the element most affected by a workload peak.
Even if you have a decentralized system, when you have a hot spot, one element reaches
its capacity limit sooner than the others and you need to devote more and more resources
to it.
Client/Server in its simplest form implies a centralized architecture when you have
multiple clients that share the same server. But you can also have a server architecture
that is decentralized, with multiple replicas of the server. I would like to use the term to
refer to an active client sending work to a passive server.
This should be seen as the opposite to a peer to peer architecture in which both client
and server act as client and server at the same time. So this means that you have elements
in your architecture which can both send and receive work to and from others, which is
mostly found in decentralized architectures. Still, also large peer to peer systems will use
so-called ”super peers” in order to scale to run on a large number of resources.
If you want to have consistency in your state, obtaining it with a centralized architec-
ture is much easier. Basically, if you have only one copy in one place, then by definition
this is equal to itself, so this is easy to keep consistent. As soon as you introduce decen-
tralization, e.g., with multiple replicas, you will copy the data in different places. You can
501

still try to keep it consistent, but it will be much more expensive to do so. So when you
need to deliver consistency as a requirement, then you tend to pick a centralized solu-
tion. When dealing with scalability requirements, you may need to introduce replication
within a decentralized architecture. In this case, what is the impact on the consistency
of the data?
The decision also impacts possible failure modes. We will talk about failures more in
detail when we go into the availability and reliability discussion. But clearly if you have a
centralized architecture you have one element which becomes the single point of failure.
Also with a centralized architecture, either everything works or nothing works.
Partial failure instead happens with a decentralized architecture: to have a partial fail-
ure, it means that you can split the system into more than one element. Some of those
elements fail while others don’t. This is a challenging situation to be in, especially when
you have to recover your system, because while only some of the elements are down,
so the rest of the system continues to work, when you restart them they will need to be
brought into a consistent state with the others.
Churn is also typical of decentralization: the more elements are part of your system,
the higher the probability that some of them will not be available. As your architec-
ture scales to run across a very large number of resources, be prepared to deal with a
continuous stream of disappearing, reappearing and lost devices, which will need to be
continuously replaced with new ones just to keep the system running.
502

Scalability at Scale
Constraint: Limit the number of edges into each node
Let’s turn the design of a scalable architecture into a graph problem. No matter whether
you choose a centralized or decentralized design, you will find a limit. It’s typically net-
working problem: the amount of connections that we can establish into the various ele-
ments of our system is limited.
If you look at these graphs. What is the maximum number of connections N at each
node? In the first case, there is no center: every element is connected to every other
one. If you look at the second graph, this number is no longer a constant value for every
element. What is the limit here? Most of the elements have only one connection, but in
the center, here comes the bottleneck. How many connections does the black node can
support? Maybe it will be thousands, a high number, but still limited. This intrinsically
limits how many components you can attach to the center one. So if you go for this
solution – with the centralized architecture – there will be a physical limit to the number
of network connections that you can have towards all the clients that you put around
your server. The limit may depend on the hardware, operating system configuration,
networking protocols, but there will be a limit.
What if you choose a decentralized solution? Does this limit disappear? Decentralized
means that you now remove the server from the picture and go peer to peer. This means
that all the elements in your architecture will potentially connect to everyone else. What
is the maximum number of connections that you can have now in this fully connected
graph? Exactly, so the number is still N. Getting rid of the server doesn’t really solve
your problem, because now every node behaves like the original server and every node is
limited like the central server used to be.
How to scale beyond this limit? You have to remove the assumption of establishing
a fully connected graph. You have to work on your graph topology under the constraint
that there is a limit on how many connections each node may have.
You can take this idea of having a central element. And you can do it recursively (or
503

hierarchically): you can see here we have the black central element, which is connected
itself to the white central element of the central elements. So we have a tree of two
levels. We call this hierarchical architecture. Every element of the system can find a route
through the tree to talk with all the other ones. There will be a lot of traffic towards the
root of the tree: the next limiting factor will not be the number of connections of each
node but the bandwidth of each edges.
One way to scale is to remove the root and have a hybrid topology, which sometimes
is called a peer to peer system with super peers. Peers that are stable and perform better,
and therefore we attach to them a local neighborhood of clients. Then the super peer
themselves establish an overlay network, which can deliver more bandwidth than having
to go through a single root note. We split the root and allow the black elements to connect
directly to ourselves.
There is a third approach not shown in the pictures which is the epidemic or gossip
based design. Here we have a random subset of the elements which connect in a way to
keep the graph fully connected while minimizing the number of hops between the nodes.
With high churn it’s not easy to keep the tree topology in place and a random graph is
more resilient, although more difficult to visualize.
I hope this idea helps you to design something that can grow beyond these fundamental
limits. Removing the centralized element is not enough, since the connectivity bottle-
neck is still there for every element of the architecture. Instead, you have to be clever
with the interconnection structure that you establish and learn from the experience of
peer to peer networks.
504

Scale Up or Scale Out?
Scale Up
Scale Out
Let’s look now at how to scale the nodes themselves. How can we increase the storage
capacity? How can we get more powerful processing?
There are two ways, in which either we grow the size of the center or the architecture
becomes more an more decentralized.
Scaling up means that we start from something small. As it grows, we keep it cen-
tralized. We keep it within the same element. We just make it bigger and bigger. For
example, you buy your phone and you estimate: one terabyte of storage will be enough.
What to do if it gets full? Buy a bigger phone, a phone with more space. The same holds
for the memory and processor speed and cores. Is your system too slow, run it on a faster
CPU with more cores. Also in this case, you reach a limit. The maximum available storage
capacity is technologically limited, or its cost simply becomes too high.
Once you reach the limit of scaling up, you can still scale out if you need to grow fur-
ther. Just take multiple copies of the same thing and place them side by side. Even if the
size of the individual disk is limited, we just use two or more discs instead of one. As a
consequence, we have twice as much storage. One processor is not fast enough: if you
can, parallelize your code to take advantage of multiple cores.
Which is more expensive? Scaling up or out? While the price of adding capacity to an
existing node may become increasingly expensive, the price of adding more nodes grows
linearly with the number of copies. Still, there is a coordination overhead. When you add
more disks, you need to invest in the RAID controller. When you add more servers, you
need to invest in a good load balancer.
Scaling up and out are not alternative, they are often used in combination. First you
try to scale up. Once it becomes too expensive, you can still scale out.
505

Scaling Dimensions
Number of 
Clients
State Size
Input Size
Number of 
Dependencies
So far we have been trying to define the challenges of scalability. Let’s summarize them
with a map to understand the different solutions, because the problems that we face are
not exactly the same.
We need to scale our system because its workload grows. The workload of the system is
generated by the clients that concurrently send requests into our system. So the number
of clients is dynamic and unpredictable. It really depends on how successful your appli-
cation is, the more clients you will need to serve. Even if you have only one client, they
may send you a growing amount of data (larger requests or more frequent requests). So
these two factors affect the workload independently.
Another dimension that we will consider is how large is the amount of state that you
need to manage. Some systems are stateless. These are easy to scale in terms of a large
workload. In the extreme case you can start a server for each client. Stateful components
are not so easy to scale. And special sharding techniques are used to grow the amount of
state beyond the capacity of individual disks or storage elements.
While we discussed the case of one server under the load of a growing number of clients,
we can look at the opposite perspective of one client which needs to work with many
servers. Will this decentralized architecture scale in terms of the number of dependencies
that this client can have? How to take full advantage of a growing amount of resources?
How to share the load among a larger and larger pool of workers?
506

Scalability Patterns
Number of Clients (Workload)
Load Balancing
Input Size
Master/Worker
State Size
Sharding
Number of Dependencies
Directory
Dependency Injection
Scatter/Gather
If you want to scale in terms of the number of clients, if this is your challenge, you’re
facing a peak of workload. Then load balancing as a solution comes to mind. If you
assume the request of each client to be independent from every other one, you can share
the growing load by adding multiple servers and making sure that, on average, they’re all
busy in a similar way so you have a fair allocation of the work.
What if you don’t have many clients, but you just have one client that is sending you
a huge amount of work so the input size grows beyond your capacity? Is it possible to
partition this work in independent units? You introduce the master worker architecture
so you can still take advantage of a growing amount of resources.
If what you’re trying to grow is the amount of storage, look into data sharding.
In general, all of these solutions require to scatter the work (or the data) and then
gather the results back together when it’s been processed. To know where you have to
send it, you have to discover which workers are available, where they are and we will see
that this can be done in two different ways with the directory or with the dependency
injection patterns.
507

Directory
use a directory to ѹnd interface endpoints based on
abstract descriptions
Clients avoid hard-coding knowledge about required interfaces as they
lookup their dependencies through a directory which knows how and
where to ѹnd them
How to facilitate location transparency?
Let’s start from the foundations so that we can build a system that can dynamically
grow and scale.
This foundation is about being able to discover where are the components and the
resources that we can count on to process the work. Given this queue of incoming work,
we want to be able to switch between different instances, replicas or alternative workers
that execute the same component. To deliver the capacity we need to work at scale, we
need to be able to switch between them so that we can send the work where there is
capacity available.
If you will talk about scalability in terms of storage, in terms of the amount of state
that you have to store, then the question is: where is this data going to be located?
If you go back to the lecture about continuous delivery pipelines and deployability,
there we can also use discovery as a tool to configure the system to use the right depen-
dencies along the pipeline. You have a system that is in production. It will have some
precise dependencies that cannot be mixed with the dependencies that you use for test-
ing. Typically this is a fundamental for what concerns the storage of the data of your
system. The state of the system in production is critical, whereas the one that you use
for testing should be separate. If it gets corrupted you can still regenerate it. While if you
lose the production state, it’s a major problem. You have to be able to configure this sys-
tem so that when you start in a testing environment it will not use the same dependencies
that you use in production.
If you think about different types of releases, when we do a blue/green release or we
do these pilots, then you need a way for binding clients with different versions of your
implementation. When you make a new release, you only want 10
In other words, when we talk about scalability, you have typically a homogeneous sys-
tem in which the component implementations that you instantiate are all the same, you
just have multiple replicas if you need more capacity. If you talk about the evolution of
your system, then you have a chance to have a system that is not exactly homogeneous
and you might have different versions and different traffic being routed to different ver-
sions of the implementation.
If you use discovery you’re trying to make components independent from each other
508

regarding their location. Even if most components have dependencies, they should not
care where they are: this is called location transparency. You can use a component with-
out knowing where it is. Because when you need to talk to it, somebody will find it for you
and will put you in contact. That’s the job of the directory, a solution for many discovery
problems.
If we want to facilitate location transparency, if we want to make it possible for a com-
ponent to discover where is another component, so that for example it can send some
work there to be processed in parallel, then you can use a directory to look up its lo-
cation, to look up which component I should talk to based on some requirements, to
perform some match making between what you need and what the component provides.
The goal of the directory is to know about what, where and who is available, which
components are available and where they are and to perform the matchmaking between
the components they depend on the components that need to be found to satisfy those
dependencies. So this works because the clients do not need to know where the depen-
dencies are and they can look up these dependencies through the directory at the latest
possible moment.
509

Directory
Client
Interface
Clients use the Directory to lookup published interfaces
descriptions that will enable them to perform the actual
invocation of the component they depend on
Directory
Interface
Description
1. Register
2. Lookup
3. Invoke
When a client is about to make a call, it needs to know where to find the component to
be called. The directory can give its location. Here is how a directory works.
We start from the provider side. There is a component offering an interface with a cer-
tain description. This description is registered with the directory to announce its avail-
ability. Registration is where we say that this is who we are. This is what we can do, and
this is where we are. So the directory remembers that.
Later, at a certain point in time, a client comes and it performs a look up. The client
asks: I’m looking for a component that matches my requirements. The requirements
come into the directory so that the directory goes over the set of registered interfaces,
perform the matchmaking, and decide if there is a suitable interface that can satisfy your
requirements.
If the matchmaking is successful, the lookup response will contain the location for the
implementation of that interface.
Then the client can directly talk to the other component: the third step will be the
actual invocation. This will be the actual message that goes between the client making
the call and the server being called.
The idea of the directory is that clients are actively looking for suitable implementa-
tions of their dependencies. The directory knows where to find them and tells the client
about their location.
Before we do the look up, we know what we need, but we don’t know whether it exists
nor where to find it. And after a successful lookup, we know it exists and where to find it.
Then we can use it. This assumes that the directory has a successful matchmaking. It’s
also possible to try to look up something in that directory, and the directory doesn’t find
anything suitable, so a lookup may fail or the result of the lookup may no longer work.
The information that was registered may become obsolete. So even with a successful
match, there is still a risk that it was based on obsolete information. Only after you do
510

the actual invocation you will know if everything has worked.
If you have a whole architecture with many components and with many dependencies
among them, typically there will be one element of the architecture which plays the role
of directory. It will be responsible for managing the location of the components and
knowing which components are there and keeping track of where they are. This critical
element will be used by all the clients to do their look ups. One important issue that
we have is: we have a single directory for the whole system. As with every centralized
design: is this directory going to become a single point of failure? If the directory doesn’t
answer the lookups, the clients are basically unable to perform their work because they
don’t find where to send their calls.
The directory may also become a performance bottleneck: if all the client lookups hap-
pen at the same time, how long does it take to get an answer? Before a client can do the
actual work, they’re waiting for the directory to provide the information needed to find
where to send their invocation. Lookups need to happen as fast as possible. So in which
way can we improve the design to deal with this issue?
We can apply some of the concepts that we discussed before to this concrete design
problem. We have a slow, centralized directory. So we could have multiple directory
instances and load balance each client to a different replica. The good thing about this
replication is that the data of the directory doesn’t change very often: we start the system,
the interfaces are registered and then if the system is not too dynamic, we can just assume
that the information in the directory is pretty static, so it’s easy to make a replica.
We can try a radical approach, fully decentralized, so each client keeps his own di-
rectory. So what we’re going to do is to deploy these two elements (clients and their
directory) closely together so they run in the same host. The local directory lookup table
can be synchronized through gossip, so each client will transfer all the changes to his
neighbors.
Once an interface wants to register, it has to find out where at least one of these copies
is in order to send information about its description there. Eventually this will be prop-
agated around and reach the other clients. This is a nice idea if we want to remove com-
pletely the central element and have a copy of the directory running in each client. The
downside would be probably that it takes time before the updates reach all the clients
because of the huge redundancy that we introduce in the system where everybody has
their own independent copy.
What if I ask you: do clients need to perform the look up every time? In this sequence
diagram we see a look up before we do each invocation. Is this really necessary? If you
want to avoid the cost of doing a look up before every invocation, we can use a cache in
the client. This is also a copy of the directory content that gets populated on demand.
After clients send a look up request to the central directory, they will store a copy of the
result in the cache. If they need to look up the same interface twice, the second time
they already have the result locally. So this is a way to scale the directory because you do
not send so many requests to the directory, you only send them the first time. What if
the cache becomes obsolete? This means that the invocation will fail because the client
attempts to reach an obsolete endpoint. They are trying to reach a location that is no
longer there. In this case, clients can look up again with the directory to refresh their
cache. So this is an example of how we can scale the directory by caching the results.
You can generalize this idea and apply it anywhere you have a client retrieving informa-
tion from a component, and this information remains valid for a certain amount of time.
You can avoid resending the same request if you can recycle the results from previous
invocations.
Let me ask you a different question now. We said that the directory helps the clients
to locate the implementation of a certain interface, so the directory knows where to find
511

the components in the system, and the client uses the directory to find out where are the
components. However, the directory itself is a component. The directory is a component
which has a very simple interface that contains the register operation and the look up
operation. The directory is a component which needs to be deployed and started on some
container like any other component in your architecture. We can use the directory to
locate all the other components; but how do we find out where is the directory going to
be located?
The location of the directory has to be known by the clients in advance. In other words,
the directory gives us location transparency for all the components in architecture apart
from itself. Clients do not have to know where to find the components, but they have to
know where to find the directory. Introducing a directory makes it cheap to move around
components because you just have to update the directory and then the clients will dy-
namically retrieve this updated information, but if you want to move around the directory
itself, then you have to probably reconfigure all rebuild or your clients depending on how
strongly hardcoded is this information.
Another way to discover the directory’s location is to use some networking tricks, such
using broadcasting. Clients will advertise their existence and the directory will be listen-
ing for such messages and perform a rendezvous. This would generate too much traffic
for all components, but you can afford to do run this protocol to let each component dis-
cover the directory on startup. That can be another solution that avoids too hard coding
the knowledge into the clients themselves.
What kind of information do you store in a directory? This is about domain modeling
for interface description and discovery. For example, consider the domain name system
(DNS). This is a very simple type of look up where you go from a symbolic name to a nu-
meric IP address. Directories can also store a complete copy of the interface, or the actual
interface is a stored by the component itself and the directory just stores a reference to
it. That depends on how easy you want to make it to update the directory: if you store
a copy and you have to replace it every time you change it. If you store a reference then
you just update your local copy and the directory will follow the reference to the latest
version.
How complicated is the matchmaking performed by the directory? Given the syntax
and semantics of the client dependency the directory can attempt to retrieve a compatible
implementation. You can also have business constraints such as usage prices or rate
limits.
Whatever metadata the directory collects, it is fundamental that it can be trusted. Di-
rectories are not only a performance and scalability bottleneck, not only a single point
of failure, but also a security weakness. If you want to attack the system, you pollute
its directory. You can call it a phishing attack or just an attack which exploits the trust
the clients have in the lookup results from the directory. Clients will invoke whatever
component the directory tells them to talk to. This requires strict validation of who is
performing the registration. If you have an attacker, the attacker will register or update
the address of an existing component and in the future clients will send all their messages
to the attacker. To prevent this, while the look up can be relatively open, the registration
has to be done only by authorized components.
If you run a directory for several years, the directory will contain a history of all your de-
ployments of the system because every time you deploy the system, the components will
register themselves and the directory will store their location. Every time you redeploy,
maybe you change the location so the question is whether you want to keep historical
record or you want to clean up the directory from time to time and have an expiration
date associated with the registrations. If you register a component, the registration will
remain valid for a certain time window, for example, a couple of months. After that time,
512

unless the component register itself again and this information will be purged from the
directory. This avoids that it just accumulates there and it becomes obsolete. Obsolete
information means that clients will fail their invocations due to the out of date lookup
results.
In this simple design that we have shown here, there is no feedback: if the directory
information is obsolete, the client invocation fails, but clients don’t have a mechanism
to inform the directory: ”by the way, the location was incorrect, because the invocation
failed. More sophisticated directories will either have a monitoring solution inside the
directory that periodically checks whether the registered implementations are still where
they claim to be, or will accept feedback from the clients and keep track of success/failure
rates to estimate the availability of a given interface.
This was the first example of one of the basic building blocks of scalable architecture.
The directory is an architectural pattern that delivers location independence, and every
time you connect two components you will need to decide: How are these two compo-
nents going to discover each other? How do they find out where they are so they can talk
to each other?
We will see that directories are not the only way to solve this problem. With directories,
the client is actively going to look up its dependencies. It’s also possible to completely flip
the relationship between the client and the directory. This is called dependency injection.
513

Dependency Injection
use a container which updates components with
bindings to their dependencies
Clients avoid hard-coding knowledge about required interfaces as they
expose a mechanism (setter or constructor) so that they can be
conѹgured with the necessary bindings
How to facilitate location transparency?
In the directory pattern, the client calls the directory to perform lookups. The inter-
action starts from the client, while the directory is passive. This relationship can be in-
verted. We can also use a directory to configure the dependencies of a passive client,
which does not or is unable to call the directory. That’s what the dependency injection
is about.
With dependency injection, we assume there is a container in which components are
deployed. The container is responsible to update its components with knowledge about
where to find their dependencies.
In the same way as we had the directory, clients do not need to know where to find their
dependencies. But they have to expose a mechanism so that they can be configured with
the information they need to locate their dependencies. This way, the container can feed
the information into them so that they can locate where are their dependencies without
having to ask for them, and that’s the most important difference.
514

Dependency Injection
Interface
As components are deployed in the container they are
updated with bindings to the interfaces they require
Container
Interface
Description
0. Registe
2. Configure
3. Invoke
Client
1. Get Dependencies
So you can see how the interaction works. We still have the registration phase – the
same as with the directory – where the interface advertises its location to the container.
When the component is deployed inside the container, the container would ask the
component for its dependencies. The interaction starts from the container and the client
is passively answering this request. The client will typically do so with some configura-
tion file and there will be a way to get the metadata for this particular component. The
container will inspect it. It will check that these dependencies can be satisfied based on
the registrations that have occurred so far.
If the match making is successful, it will inform the component by configuring it so
that when the component starts processing and needs to invoke a certain interface, it
already knows where to find it.
These steps happen during the initialization of the container. As you start the system,
the dependencies are injected and afterwards the components can operate normally and
proceed to the invocation under the assumption that their dependencies have been sat-
isfied and located.
As you can see the configuration part of the dependencies of the client is passive, so
the client – the component that needs to be configured – is not doing anything by itself,
it is just reacting to the container that injects the dependencies.
When you use a directory, you can wait until the latest possible moment before you
need to know where to find your dependencies. So this is late binding, very dynamic
while here we anticipate the binding at startup time. So with dependency injection you
have a startup phase in which you configure the dependencies so that you don’t have to
do it later. This can save time during the actual invocation, because the invocation target
is already bound.
If the location of some dependency changes between the startup time and the time
of the call, you will need to reconfigure the clients. Whenever the registration changes,
you have to see which clients are affected. Then you can still deal with it to avoid failed
invocations due to obsolete registrations.
515

Dependency Injection
• Used to design architectures that follow the inversion of control
principle:
• “don't call us, we'll call you”, Hollywood Principle
• Components are passively conѹgured (as opposed to actively
looking up interfaces) to satisfy their dependencies:
•  Components should depend on required interfaces so that
they are decoupled from the actual component
implementations (which may be changed anytime)
Such inversion of the relationship between the directory and the component that is
looking for dependencies is called “Hollywood principle”.
When you go to an audition, the usual answer at the end of the audition is that we
need to decide whether you got the part: Please don’t call us, we will call you (in case
your audition was successful).
This clarifies the relationship between who has the information and who needs the
information. We know the dependencies that you need. We will tell you how to satisfy
them. You don’t have to ask us, we’ll do it ourselves.
With dependency injection components are passive. And they do not know how to look
up for the interfaces that they need. They just sit there and wait to be configured. The
idea is that you have components that know what their dependencies are and they are
configured by an external entity so they can actually satisfy these dependencies.
516

Dependency Injection
• Flexibility:
• Systems are a loosely coupled collection of components that
are externally connected and conѹgured
• Component bindings can be reconѹgured at any time
(multiple times)
• Testability:
• Easy to switch components with mockups
The environment in which would deploy the component is in control on when and
where these connections are established. This happens, of course when you start, but
there can also happen later. In case you need to move some of the dependencies to a
different location with the directory, you would reconfigure the directory. As soon as the
client would try to call the old location, the call would fail. The client will look up again in
the directory and be given the updated information. With the dependency injection you
can prevent the failure, so you can just whenever you know that something has changed,
you reconfigure the components with the updated information.
Another advantage of using dependencies injection is that it helps you to use the cor-
rect configuration along the build and continuous integration pipeline. If you deploy a
component in a testing or staging environment, the container will configure the depen-
dencies so that you can use for example mockups. Or components that are supposed
to be used only during testing. When you deployed in production, the container that is
configured as a production environment, it will use the correct dependencies.
Since the component is passive and the responsibility for setting up the right envi-
ronment is all outside of the component, if for some reason the component forgets to
make the look up with the right directory and relies hard coded information, this will be
difficult to rectify without rebuilding the component (e.g., to avoid using testing depen-
dencies when deployed in production) to avoid deploying the system with inconsistent
dependencies. It can be easier to check that this doesn’t happen by controlling the envi-
ronment as opposed to each component.
517

Directory vs. Dependency Injection
Client
Conﬁgure
Register
Container
Component
Invoke
Dependency Injection
Client
Lookup
Register
Registry
Component
Invoke
Directory
To summarize the difference between directory and dependency injection, let’s take
a look at the logical perspective of how the three elements depend on each other. The
component the client depends on is going to register with the container. And here we
have the same: the component registers itself with the registry, which is a synonym for
the directory. Also the invocation is the same on both sides, where the client invokes the
component.
What is the only difference? It’s a small one, but very important: the relationship
between the client and the container/registry. With the directory, we have the client
looking up the location of the component that wants to invoke in the registry, so the client
is active while the registry is passive. With the dependency injection we have exactly
opposite. The container is going to configure the component that is deployed inside the
container setting the location about the component’s dependencies. The container is
active, while the client is passive.
518

Scatter/Gather
Broadcast the same request and aggregate the replies
Send the same request message to all recipients, wait for all (or some)
answers and aggregate them into a single reply message
How to compose many equivalent interfaces?
Scatter/Gather is a structure that we are going to use for understanding how scalability
patterns such as load balancing, master/worker and sharding work.
Let’s take it from a more abstract perspective first. The problem here is that we are
in this decentralized design in which we have multiple equivalent interfaces that we can
use. The question is: how do we make use of them as a whole, single unit? How do
we compose them together so that from the client perspective they behave as a single
component?
What we can do is, when we receive a request from the client, if we send this request
to only one element, this would soon become a bottleneck when filling up its capacity,
as a single element processing requests would be centralized. So we decompose the ar-
chitecture by creating multiple copies of these bottleneck elements. Then we broadcast
the request of the client to all of the copies, to all replicas. Once the replicas process the
request we aggregate the replies. And we can send them back in one response message
to the client.
How do we perform the request broadcast? And how to perform the response aggrega-
tion?
519

Scatter/Gather
Client
A
Scatter/
Scatter/
Gather
Gather
B
C
Broadcast
Request
Aggregate
Results
Example:
• Contact N airlines simultaneously for price quotes
• Buy ticket from either airline if price <= 200 CHF
• Buy the cheapest ticket if price > 200 CHF
• Make the decision within 2 minutes
We have one request message coming in from the client. This request can be copied so
we can send the same request to all the replicas. But then we may have different answers
from each of them, both in terms of their content and their timing.
If we have a set of answers, how do we reduce the set of answer to only one? Because
the client expects a single response.
Do we really need to wait for all of them? If our goal is to keep the latency under control,
in order to produce a fast response, we could just send back to the client the response of
the fastest replica.
One example: you need to travel somewhere. Different airlines receive the same input
about your trip and respond with an offer describing a possible connection and its price.
The rule for aggregating the results can be as follows: If you get a cheap price, you take the
offer, there’s no need to wait any longer. However, if the prices are a bit more expensive,
you want to wait long enough to compare different answers. And then you will take the
cheapest. However, you cannot wait forever, so if you found an answer within 2 minutes
which is within your budget, you will buy it.
This gives you both a way to aggregate the results because you will take the cheapest
together with a strategy to decide until when do you want to wait.
If one of the backends that you contact is overloaded, it will not answer within the
deadline. It’s better not having to wait forever for all replies, but to increase the chances
that at least one replica will reply before the timeout.
520

Scatter/Gather
Which components should be involved?
• The recipients are kept hidden from the client
They can be dynamically discovered using subscriptions or
directory registrations
• The recipients are known a priori by the client
The request includes a distribution list with the targeted client
addresses
How do we know which replicas you want to invoke? There are two options to decide
this. One is to keep the fact that you’re scattering out the request hidden from the client,
so the client thinks that it’s talking to one component, but behind it you have multiple
ones. Alternatively, the client is actually going to tell you who do you want to invoke.
This makes a very big difference in terms of how transparent your interface is. If you
keep it hidden, it means that you are responsible to manage the replication of the imple-
mentation, the discovery of the back-end replicas, (you can use a directory behind the
scenes) and scattering the client request and gathering the replies. The client doesn’t
need to know. Otherwise, you push all the directory management and all the knowledge
about where to forward the request to the client. Whenever the client sends a request,
the request will include the list of addresses of the components that the clients wants
you to target.
Considering the airline example, it makes sense that the client can give you some con-
straints about which airlines is interested for you to contact. There are other travel reser-
vation agencies that play with this and they make you an offer with particularly good
prices, but they do not tell you what is the airline behind it. Only if you accept the offer
they will reveal this information.
521

Scatter/Gather
How to aggregate the responses?
• Send all (packaged into one message)
• Send one, computed using some aggregation function (e.g.,
average)
• Send the best one, picked with some comparison function
• Send the majority version (in case of discrepancy)
On the other direction, once we get multiple responses, how do we aggregate them?
These aggregation strategies do not only contribute to enhance the scalability, but can
also help to ensure the result is correct.
The simplest solution is to just package multiple responses in one reply message. A
message can always carry within a collection of messages, so it’s easy to do that and
delegate the aggregation to the client. We can compress multiple messages with some
statistics, e.g., calculating the cheapest, calculating the average, selecting the best ac-
cording to some utility function.
In case of a Byzantine environment in which you do not necessarily trust all of the back
ends. You can use a voting strategy. If out of five copies of the same input, one of the
replies is a completely different result but the other four agree with each other, we drop
the outlier and forward the majority response.
522

Scatter/Gather
Synchronization strategies
• When to send the aggregated response?
• Wait for all messages
• Wait for some messages within a certain time window
• Wait for the ѹrst N out of M messages (N < M)
• Return fastest acceptable reply
• Warning: the response-time of each component may vary and
the response-time of the scatter/gather is the slowest of all
component responses
In terms of how long do you wait before sending back the answer? Waiting for all the
replicas to respond is bound by the slowest. Follow this strategy only if you really need
to wait for all of them to guarantee complete coverage. If you expect the result will be
the same, maybe you can disregard some replicas if they are too slow.
There will be a time window in which the client is expecting a response, and you will
wait only until this window is about to expire before sending the best aggregated reply
that has been gathered until then.
If the time window is unclear, it is possible also just to count how many replies have ar-
rived: once the first N out of messages have arrived, you have enough responses and you
can already answer the client. Sometimes you really want to make it as fast as possible,
so you take the first, or the first that is acceptable. This requires to validate the answers
to avoid accepting super fast replies full of random bits (or high velocity garbage).
523

Master/Worker
split a large job into smaller independent partitions
which can be processed in parallel
The master divides the work among a pool of workers and gathers the
results once they arrive
Synonyms: Master/Slave, Divide-and-Conquer
How speed up processing large amounts of input data?
How can we refine the scatter gather pattern so that we can use it to scale, not neces-
sarily to a large number of clients, but to deal with a single client that is however sending
a large amount of input data. The idea is to split the input data into smaller partitions
which are processed independently and therefore in parallel. This is the assumption that
we make when applying this master worker pattern.
The master role is to accept the input data, divide it and then schedule the work among
pool of identical workers. Workers are supposed to work independently and in parallel
on each of the partitions. Once the results come back, then it’s like in the scatter/gather
pattern: we have to accumulate and concatenate the result. Once all the workers have
complete we can send it back.
This pattern is also known as divide and conquer, especially when it is applied recur-
sively. The problem is too large to be solved as a whole, so we split it and we solve each
of the parts and then we have to reassemble the solution together.
524

Master/Worker
Client
Partition
Master
Master
Worker A
Worker B
Assign to
Workers
Merge
Results
Example:
• Matrix Multiplication (compute each row independently)
• Movie Rendering (compute each picture frame independently)
•
 (volunteer computing)
Seti@home
 
 
We can see the interaction has a similar structure than the scatter gather with a little
but important difference: after the request that comes in, it is not directly forwarded to
the workers. But before doing so, there is a partitioning step which takes the large input
request and partitions it into smaller ones.
Each of these partitions needs to be assigned to a worker. While the workers perform
the same computation, they don’t necessarily complete it with the same performance.
When all of them have processed their partition, the results are merged into a single
response.
From the client perspective, such partitioning and parallel processing is also some-
thing that you can do completely transparently. The client doesn’t see that its request is
partitioned and then the results are reassembled back together. There is only one request
coming in and one result going out. The only observation the client can make is that for
some reason this implementation scales because, despite the fact that you’re sending
larger and larger amounts of data, the response time doesn’t grow as much as one would
expect. This holds if you can add more and more workers to deal with larger and larger
amounts of data.
We have examples from scientific computing where this is happening quite often with
parallel supercomputers. If you are trying to render a movie, for example, you know that
each frame can be rendered independently, so you can actually use master/worker to scale
the longer movies with higher resolution. You set up a so-called rendering farm, where
you have a huge number of graphics card and processor’s; you input the movie and then
you don’t have to actually do the computation sequentially.
If you scale this to the whole Internet, where the workers are actually running on desk-
tops, recycling wasted computing cycles all over the world, then you have something
called volunteer computing. The amount of work that you have to do is so large that you
need millions of these workers all over the place. This was introduced for the search of
525

extraterrestrial life. Where the signals from radio telescopes were partitioned into blocks
of time and also different frequencies and each block was sent out to screen savers run-
ning on desktop computers all over the world. After downloading a block, they would
analyze it to search for interesting signals and possibly send back some detection events.
This application became very popular, with the number of workers growing beyond the
initial expectations.
Here’s another scalability issue: when you really have millions of these workers coming
and asking for work, you must have a huge queue of these partitions that need to be
processed. Eventually they ran out of data, so they started to re-process the same data
with more expensive detection algorithms.
They also came up with incentives to keep the workers interested: a reward system for
people to participate and donate their CPU cycles. You could spot your name on the top
10 or the top 100 of individuals or organizations that were contributing their resources to
SETI at home. Not surprisingly, when you put gamification into play, then some people
are driven to win the competition. And others hacked their way to the top by running
modified screensavers, which would just return some random result after downloading
a piece of work without really analyzing it, to get the reward while saving on CPU usage
and the corresponding energy bills.
If you do not trust the workers, how can you check that they are actually doing what
they are supposed to do? What you can do is send the same partition to multiple workers,
scatter, gather and compare the outcome. Or you can have some expectations about how
long it is going to take to process it. If you get an answer too soon, there is something
suspicious going on.
526

Master/Worker
Master Responsibilities
• Transparency: Clients should not know that the master
delegates its task to a set of workers
• Partitioning strategies: Uniform, Adaptive (based on available
worker resources), Static/Dynamic
• Fault Tolerance: if a worker fails, resend its partition to another
one
• Computational Accuracy: scatter the same partition to multiple
workers and compare their results to detect inaccuracies
(assuming workers are deterministic)
• Master is application independent
There are many second-order design decisions that you have to make when you decide
to follow the master/worker pattern. Let’s discuss them more in detail.
From the master perspective, the goal is to hide from clients the fact that they are
splitting the work and forwarding it to a set of parallel workers. When partitioning the
work, there can be different ways to do it. If you know that workers are homogeneous
– every worker has the same computational power – then you can split the work in a
uniform way. If you know some workers are faster than others, you can give them more
work while assigning less work to the slowest workers. This requires a model to predict
how much work can every worker perform so that at the end you get back the result at
the same time.
The set of workers can also fluctuate. You don’t necessarily know at the time in which
the partitioning happens how many workers will be available. If the computation takes
a few seconds, maybe you have a reasonable expectation about how many chunks are
needed, but if the computation takes a few months, it’s difficult to foresee in advance the
availability of workers. In this case, there can be a dynamic partitioning strategy which
starts to slice the work progressively, while keeping some work from being allocated in
case more workers show up. If more workers will happen to join the system, then it will
either repartition, reshuffle or reassign work that was not given out yet.
We can also have a work stealing strategy when a fast worker looks for work that has
been assigned to other workers that are slower.
There is also a little problem of failures, so if workers fail, we need to be aware of
that because before we can send back the response to the client, we need to gather the
results from all the partitions. It’s important that all the workers successfully completed
their job. In case of failure we have to resend the partition to be processed somewhere
else and whenever we use a retry strategy to deal with failures – as we’re going to see
in the reliability lecture – the assumption is that it’s possible to repeat the work. That
527

processing a partition does not have any side effects.
With deterministic computations the same result should be obtained no matter which
worker produces it. In this case, it is possible to take the same input, the same partition,
and have it processed by multiple workers. This way we can compare the results to detect
whether some of the workers are cheating by sending back some incorrect results. The
more replicas you make of each partition, the more expensive it becomes. There will be a
tradeoff between how much fault tolerance you want to have and how much performance
you want to have and how many replicas you want to use for processing each partition.
All of these design decisions that you make about partitioning, about retrying failed
jobs, about sending out multiple copies of the same partition are design decisions that
are typically independent of what the processing is about. As long as it is possible to
partition the input and merge the results back together, the master does not care about
its content. The part of the system that is of course application dependent is the worker.
The worker needs to know how to process the input and what the application domain is
about.
528

Master/Worker
Worker Responsibilities
• Each worker runs its own parallel thread of control and may be
distributed across the network
• Worker churn: they may join and leave the system at any time
(may even fail)
• Workers do not usually exchange any information among
themselves
• Workers should be independent from the algorithm used to
partition the work
• Workers are application domain-speciѹc
To speed up the work by scaling over more resources, or to keep the response time flat
with a larger input, you need to parallelize the work: each worker should be distributed
across different cores or cluster nodes.
How stable are your workers? Here we see again, the term churn coming up. Can you
assume that once you get the request from the client, you have a set of workers and they’re
exactly the same throughout the whole computation? Or will you have an unpredictable
set of workers that come and go anytime? What if some disappear and never send you
back a result?
Another assumption that simplifies applying the master/worker pattern is that when
you give a processing job to a worker, the worker can perform the computation on its own.
It doesn’t need to know who the workers are. It doesn’t have to exchange any information
with them. The only communication flow is from the master to the worker and back. But
workers don’t have to talk to each other. There can be more complex, less embarrassing,
schemes in which workers periodically need to exchange some information. But that
makes running the parallel computation much more problematic.
Finally, the partitioning should be exclusive responsibility of the master. Workers
should just know how to process one piece and they don’t care about how the partition-
ing works. Unless the pattern is applied recursively, with divide and conquer, workers
will decide whether it’s worth to further split their partition and delegate themselves to
some sub-workers the parallel processing.
529

Load Balancing
deploy many replicated instances of stateless
components on multiple machines
The Load Balancer routes requests among a pool of workers, which
answer directly to the clients
How to speed up processing multiple requests of many
clients?
Let’s see now how do we design a scalable architecture which instead of supporting one
client sending a huge amount of data, it can grow to handle multiple requests of many
clients. This was the original challenge of building a scalable architecture in terms of the
workload.
You have one component that is the bottleneck. You can tell by measuring its response
time when you have too many clients that are sending requests and they need to wait
longer and longer for the responses.
If we can assume that the bottleneck component is stateless, if the responses to the
requests only depend on the input and don’t depend on previous requests, we can deploy
many copies, many replicas of the component on different machines, and we can spread
the request across all of them using a special component called the load balancer.
The load balancer is involved in routing the requests. The responses can go directly to
the clients. As opposed to scatter/gather, there is no need to aggregate responses because
for each request going to be processed by a worker, the response can go back directly as
soon as it’s ready.
530

Load Balancing
Clients
Directory
Load
Load 
Balancer
Balancer
Worker A
Worker B
Assign to
Worker
This is how the flow works: we have lots of clients. Here’s the request from one client.
The load balancer needs to find a suitable worker for processing this request. Here we
have a directory that is managing the knowledge about which workers are available and
where to find them. The load balancer will use the directory to perform the matchmaking
between what the client wants to do and where is a worker that can do it.
Once the assignment has been made, the request will be forwarded to the worker. The
worker will take it, process it, and send back the response directly.
Also in this case the client doesn’t need to know where the worker is, because that is
hidden behind a load balancer that is going to make the decision for the client where
to send the message.The worker should have an address and where to send back the re-
sponse. If you have headers in the messages, the request message will have a ”reply to“
header which contains the identity of the client that is supposed to receive the response.
When the next client will come in, the directory will make a different decision because
this time the first worker is busy with the first client request. The decision is about:
does it make sense to queue a request for this worker? What if we have another one that
is free? In this case, the request will be forwarded to the worker that is free. And the
worker would process it and the response will come back to the client.
As you can see, there is a scatter phase, during which different requests go to different
places, but there is no gather phase at the end, because the responses go back directly.
This implementation with the directory is also dynamic, so the set of workers can
change. Over time we can add more workers when there is increasing traffic and we can
remove them when there is not so much traffic.
The function of the load balancer can be implemented also the lower level of the net-
work, since it is about routing and forwarding packets to the worker endpoint addresses,
which are set by the directory.
We can split the mechanism of forwarding the work, which is something that has to
happen really fast – otherwise the load balancer becomes a bottleneck itself – and the
directory which is the place where the strategy can be implemented to decide: how to
rotate the work between different available workers? how to keep track of them? how to
detect if a worker is busy or not? What if a worker fails?
531

The directory takes care of this, helping to keep the load balancer simple: it takes an
incoming message, picks the next available worker, and forwards the message to it. These
operations should be performed rather quickly. If you really want to scale, you perform
them using hardware, like when introducing a specialized networking device dedicated
to spraying packets around your cluster.
Load Balancing
Strategies
• Round Robin
• Random
• Random Robin
• First Available
• Nearest
Location
• Server (transparent from client)
• Client (aware of choice between
alternative workers)
• Directory (e.g., DNS)
Layer
• Hardware (network device)
• Software (OS, or user-level)
There can be different strategies for balancing the work among different workers.
The simplest one is to use a counter. You know how many workers there are and for
every request you increase the counter and use its value to select the worker. If you run
out of workers, you start again from zero. We call this the round Robin strategy. It’s super
fast, but doesn’t take into account any information about the availability or the current
load of the workers. The assumption is that by the time you go around, the worker will
be free again.
Depending on how long it takes to process each request, this assumption may be true
or not. What if instead of incrementing the counter, one can skip the counter and as-
sign requests to a random worker? As long as you pick random workers with uniform
probability, we can say that statistically the load will be distributed evenly. But we don’t
necessarily do it always in the same order. And there can be a benefit in doing that.
You can also combine the two strategies. So you can go around once. And then shuffle
the workers for the next round. And when you assign the work to all of them, then you
reshuffle them for the next round.
If there is a way to keep track of the availability of the workers, it is possible to create
a feedback loop between the worker that takes the work and the directory. Workers have
to notify the directory about their availability state. And the directory just keeps a list of
available workers and takes the first one off the list.
So there are feed-forward and feed-back strategies. The former just keep a list of work-
532

ers and they go through the list in some sequence. The latter assume that the worker
inform the directory about their state of availability.
The nearest strategy can be used when scaling to “Web scale”: if you have clients all
over the world, it can make sense to route requests to workers which are located near
where the clients are. Do you know where the client request is coming from? You can
forward it to a data center which is located in the same region of where the client is
located so that you can minimize the latency and the ping between the worker and the
client, especially when you want to send the answer back.
Sometimes this is done at the DNS level, so when you do a DNS look up, the DNS server
can see where the look up is coming from and can reply with a different numeric IP ad-
dress depending on where the client is located.
This brings us to the awareness about the location of the workers. How transparent
do we want this to be? The idea is that the client can send a request to a certain server.
The client knows the location of the server (he has to discover it using some directory).
But, behind the server, the fact that the request will be load balanced is kept transparent
from the client. So the load balancer knows where to find the workers, but the client
doesn’t. From the client perspective there is only one single endpoint. There is only one
place where you talk to and behind the scenes we will do the load balancing. This leaves
a bottleneck intrinsically, because of the single endpoint that all the clients have to talk
to. Which implies that the load balancer becomes a hot spot since every client will have
to send a message with this particular IP address even if behind it there will be others
that can take care of servicing the requests. But this first step will eventually limit how
much we can scale.
To avoid this, we can push the work out to the clients and this is one of the basic scaling
strategy that we discussed last week. If you cannot handle the work yourself. Just get
whoever is asking you to do the work to do it for you. In this case, we can give a list
of endpoints to the clients. And if a client notices that one endpoints is becoming slow,
then you can switch and send requests to another one. And clients can follow themselves
a random strategy: out of these possible addresses, I will pick one randomly and over
millions of clients that are trying to do the same,statistically the work will even out so
that all the endpoints will be loaded in a similar way.
The downside is that the client has to be aware of the fact that there are different work-
ers and you have to not only implement a client that sends a request, but you also have
to implement a decentralized load balancing strategy inside the client. And you have to
keep the list of workers up to date, because if you even if you choose one randomly and
your choice and ends up in an obsolete address and then, then you have a problem. But
if you remember about caching and the directory, the idea was that the client looks up
the list in the in the directory. Keeps in the cache and only when it expires it can repeat
the look up again.
If we have a load balancer that uses a directory behind the server, the client sends a
request to the load balancer and the directory is located behind. However, we could also
expose the directory to the client, so the client can do the look up and then send directly
to the worker without all having to go through this single point.
As an alternative, we can have the directory perform the load balancing. We can have
a load balancing DNS, for example. Client need to look it up anyway, because they don’t
know the server is, so what’s the point of asking the directory: give me the location of a
server? And then I will call the server and the server will have to load balance my request
of our pool of workers. You might as well have the directory send back the location of the
worker because the client will do the look up anyway.
If you want to build a highly scalable system, you will do a combination of these. You
will start by having a load-balancing DNS. The DNS will send addresses to clients that
533

are, for example, nearest to them on the network. The clients will contact one of these
addresses – maybe they get multiple lookup results, maybe they get only one. They will
choose randomly on one of them. And once they get to these addresses, the server again
will have a pool of workers behind. And will do the load balancing, so these are not
necessary alternatives. But they can also be introduced in combination. When you use
them in combination, you’re talking about serious investment in terms of components
dedicated to scalability, which can also be an overkill for more scenarios, but if you really
need to go all the way, you know that you have a lot of tools at your disposal.
The load balancer can become a bottleneck for the reason that we discussed so typically
also another way to mitigate this can be too implemented in hardware. There are network
devices that are just doing this. They don’t even open the packets, they just rewrite the
addresses in the packet headers.
You can also do it at the OS level. You can have also have load-balancing Web server
proxies, or you can also always implement load balancing inside your user applications.
Load Balancing
Variants
• Stateless: every request from any client goes to any worker
(which must be stateless)
• Session-based: requests from the same client always go to the
same worker (which could be stateful)
• Elastic: the pool of workers is dynamically resized based on the
amount of trafѹc
Load balancing as we have introduced it makes the assumption that the components
that you are replicating are stateless. And this is the simplest way that you can do load
balancing if every request is independent. If you have a client that sends multiple re-
quests, you don’t have to remember your decision about where to send each of them.
You can just take every request, send it to the next available worker and forget about it.
If however, we have stateful components, which means that the order of the requests
is important and the sequence in which they are processed is important, you cannot pro-
cess a request without having seen the previous ones. This puts a constraint on the load
balancer: because it has to remember the previous requests from a given client so that
they are sent to the same worker. The worker is accumulating state from the stream of
requests which cannot be processed unless they all arrive in the right order at the worker.
We can relax the assumption that we were making before. Load balancing is easy to in-
troduce as a scalability technique, if you assume that components are stateless. If they’re
stateful, you have this constraint and you solve it by establishing a session between the
load balancer and the client. The load balancer, in other words, has to distinguish the
534

identity of the client and remember for each client which worker was assigned. Unknown
clients can be assigned a worker using the previously mentioned strategies. But follow
up requests from the same client must be routed to the same worker.
This also limits how flexible and how much churn, how much change you can you can
expect from your pool of workers. If you have a stateless load balancer, there is no prob-
lem if you have seen a worker before and you don’t care if the worker is new or if a worker
disappears. The load balancer is completely free to assign any request to any worker.
If you use a session-based load balancer, you need to be careful that new workers only
take request of new clients once a client has already sent you request before then he has
to go to a worker that has seen the client before.
Another important design decision is: how long do you keep this information? How
long are your sessions? If you keep them too short, then your clients will complain be-
cause they will start to get incorrect results. And if you keep them too long, you may
over-constraint the life cycle of the workers which need to stay available for a long time.
There is also an overhead in terms of how much session information do you need to keep
track of. Since we are talking about scalability, you talk about millions of clients. It is
expensive to remember all of them within the mapping table connecting each client with
each worker.
The complexity of the of the protocol between the client and the load balancer also
increases. There needs to be a mechanism to establish the session, but also a way to stop
the session. Also after a session expires, the client will need to discover this. There could
also be a way for client to prolong the session or negotiate its duration.
The last variant of load balancing is about the relationship between the pool of workers
and the amount of traffic. Are you in a static environment where you have a limit on the
workers that is fixed? So the capacity of your system is fixed and then just have to use
them as efficiently as possible. Or is it possible when you have a traffic spike to increase
the size of the pool of workers? While at one point in time you need additional capacity
if the traffic decreases then you can free up some resources to do something else when
the spike goes away.
Traditional load balancing assumes a static environment in which you have a set of
workers and you spread the work between them using the strategies that we have dis-
cussed before and the constraint on whether the workers are stateless or stateful. If you
have an elastic load balancer, things are even more complicated: the load balancer is
going to measure the amount of traffic and make decisions on whether to dynamically
allocate more workers or dynamically stop some of the workers.
More in detail, we have a queue of pending client requests. The load balancer consumes
them by assigning them to a worker. And if the queue gets above a certain threshold, you
can think that the currently available workers are not enough. You don’t have enough
capacity, so you might as well look for additional workers and try to increase the capacity
of the system so the queue of request goes under the threshold again. If the queue gets
empty, you will notice that also workers are not doing anything. Idle workers can self-
destruct if they don’t receive work within a given time, or they can be stopped by the load
balancer and restarted only as the next wave starts to rise again.
535

Sharding
partition the data across multiple databases
Route the queries to the corresponding data partition ensuring each
partition remains independent and balanced
How to scale beyond the capacity of a single database?
The last pattern focuses on how to scale beyond the limits of a single storage element,
for example, a single database. When we talk about the capacity of a single database, this
is can be limited by the capacity of the physical storage. For example, going beyond 10s
of terabytes. How to scale to storing petabytes of data when there is a clear physical limit
on how much each storage device can provide space for?
To go beyond, it should be possible to partition the data into pieces. Partition it and
store it across multiple databases, so this is clearly a scale out strategy. You have been
scaling up until you reach the limit, so there is no disk that can store more than 10 ter-
abytes. Just to say a number. You can take two discs, put them side by side and now we
have a storage for 20. However, we will need to partition the data so that each part fits
in within the limit of the amount of storage space that we have available.
Once we partition the data then we have to know and remember where it is located. So
we again we have a directory problem. We have a discovery problem. Where do we find
the data? Where do we find the partition to which we can route the queries?
When a client needs to read the information or update the information we need to send
the query to the corresponding partition. The challenge is to make sure that most queries
get routed to only one partition. If all queries are routed everywhere, then all partitions
of the whole system will be busy to service all the queries, thus limiting its scalability in
terms of how many queries it can process.
While the main goal of sharding is to scale out beyond the limits of individual stor-
age element, an additional goal is to also take advantage of the partitions to do load
balancing. Different queries can be serviced in parallel by reading or writing to differ-
ent partitions, which will be stored on different disks, thus increasing the amount of I/O
operations which can be performed by the storage.
536

Sharding
Shard A
Shard A
Component
Component
Query Router
Query Router
query
query
query
query
shard_key
shard_key
Partitioned Dataset
shard_key
shard_key
Shard B
Shard B
Common Dataset
Shared
Shared
query
query
query
query
Queries are directed to the corresponding shard (partition)
Queries should not involve more than one independent shard,
even if they may use some shared non-sharded dataset that may
need to be replicated on each shard
Sharding is also a relatively complex pattern, which can be understood as it combines
together all of the ideas on how to scale that we have discussed so far. We have a query,
we have to find where is the data that needs to be read or written by the query. This is
discovery problem.
We have lots of queries: we have to balance the load among the partitions. We need
to do so without creating hot spots, without resulting in a disk that is always going to be
involved for all queries. If you can keep the traffic balanced, each storage resources will
be uniformly busy and fill up at a similar rate.
Sharding works by load balancing each query coming into your storage system. There
is an element that is going to make the decision on where to send the query and this will
be done based on some property of the data that you’re trying to access. Information
extracted from the query can be used to compute the location of the target partition. We
call this the key of the shard.
Shard is a synonym for partition and the key identifies which shard are you going to use.
So based on the shard key, we’re executing the query on the corresponding shard. The
result is sent back to the original component. If a different query comes in, depending
on the corresponding shard key, it may end up in a different shard. This is a way to scale
the bandwidth for reading or writing into physical disks. If you have multiple partitions
stored on separate disks, the amount of data which can be transferred grows if read and
write operations are performed in parallel on the various disks.
What if each query reads data from a specific partition but also needs to access common
data from a shared partition? If this shared data is read-only it can be replicated on each
partition to ensure the queries can still be processed independently.
537

Sharding
• Two goals:
• scale capacity (storage size, bandwidth, concurrent
transactions)
• balance query workload (avoid hotspots)
• What to shard?
• Large data collections (they do not ѹt)
• Hot spots (increase throughput of queries targeting a subset
of the data)
When we talk about sharding, we have two goals. The first is to go beyond the lim-
its of the available amount of storage space, the second is to grow the I/O bandwidth.
How many concurrent transactions a database can process? When we hit the limit, we
replicate and partition the data accordingly.
The second challenge is to how to balance the queries so that we avoid sending all the
queries to the same partition. We use sharding to work with very large amounts of data
which do not fit in a single storage element. To do so, we have to partition it. In case we
detect a hot spot, we can use sharding to replicate it and thus increase the throughput
and raise the capacity of the database to process concurrent queries.
If there is a limit on how many read transactions your database can perform, you can
always make a copy of the data (it doesn’t change anyway) and then you can double the
throughput that you can achieve because you can read from each copy independently.
538

Sharding
Computing the Shard Key
• Key Ranges
• Geo-spatial (country shards)
• Time Range (year, month, day)
• Hash Partitioning
• Modulo (Number of Shards)
Usually assumes a ѹxed and static number of shards
The key decision that you need to make when you decide to introduce sharding is: how
are you going to compute the key that you use to decide where to place the different
partitions? And you already know that in your database you have keys that identify the
elements that you are storing. And so one easy solution is to take the values that you
assigned to your keys and map the range of values to fit within the number of partitions
you plan to use.
For example, if you have to use integer keys that go from zero to 1,000,000. Then
you can make 2 shards. One will be for entries from zero to 500,000 and the next shard
will store the ones above 500,000. This also gives you an easy way to figure out how to
transform the key into the shard identifier.
If you are storing geographical data, you can, for example, partition your data set into
different countries. Since some countries are much bigger than others, you may not end
up with uniform partitions.
If you work with time series, you have data that has historical perspective, going back
many years. You could consider storing each year in a different shard. Given the time,
it’s easy to determine where to find the corresponding data. The current year is the place
where you read and write, while past years go in a read-only database, since old data
should no longer change.
The granularity of the partitioning is critical. If you shrink the time range and make a
shard per month, week, or even day, it will be very efficient to access information on a
daily basis, but if you have to run a query to aggregate the quarterly results, you will need
to scatter the query and gather the results from many different partitions.
Based on the level of aggregation of your system data access patterns, you can decide
how to partition it. You have to balance being too fine grained and risking that then you
have to aggregate information for multiple shards or keeping it not so fine grained and
then having hot spots.
When everything else fails, you can always try hashing. Take a piece of data, you hash
it, and the hash can be mapped to the shard key. If hashing is expensive, you can take
the modulo to map some numeric identifier within the number of shards. For example,
if you have 10 shards you will take the last digit of the numeric identifier to know where
539

to store each object.
No matter which strategy you choose, the assumption is that the set of shards is fixed
and static. So sharding is one of those decision that you make up front. How you plan
to partition the data will constrain how you can access it efficiently (and vice versa).
Since sharding affects the physical storage location of the information, if you change
your decision, you will have to physically move data around and this is super expensive
for large amounts of data.
Sharding
Looking up the Shard Key
• Business domain-driven (customer/tenant id)
• Helps to keep data balanced
• Requires to maintan a master index (i.e., a directory for shards,
a ZooKeeper)
Computing the shard in general requires to come up with a function which maps your
data to the shard key. The function should be deterministic, if you want to be able to
consistently find where the data was placed. The advantage of computing the shard key
is that you can derive the shard key from the data itself. The disadvantage is that it may
not be trivial to come up with such a function.
Mapping data objects or tuples to partitions can also be seen as a lookup problem. If
the previous strategies don’t work out well, you can always store the key. Somewhere in
your data model there will be a column which will store the address of the shard in which
the data is located.
The advantage is that if you notice that some shards are getting bigger than others,
you can just change the key to move data around and you don’t have to change the way
that you compute the key because the key is just another attribute that you associated
with your data.
However, if you follow this strategy, then you have to remember where are the shards
located. In other words, how do you go from the shard key to the actual location of the
data? A directory can help. Given a piece of data which contains a shard key, you have a
logical identifier which can be used to lookup the physical location of the corresponding
shard, the address of the database server in which the shard is found.
It is important to keep this indirection, because if you store the physical location of
the database and you ever need to move it, every single piece of data referring to another
shard in your in your system will need to be updated, which again you may want to avoid.
If you just remember the logical identity of the Shard, then you can just change the look
up table an you will know where to go and look for the corresponding storage element.
540

Re-sharding your data is expensive and typically involves down time, because while
you’re shuffling the data around, you really don’t want to change it, and depending on
how much data you’re talking about, this takes time.
Sharding
• Changing the number of shards or the sharding strategy may
require an expensive and complex repartitioning operation
• Transactions should only involve one shard (some shared data
may need to be replicated on each shard)
• Sharding was originally implemented outside the data layer,
sometime as part of the application logic. Some databases are
starting to offer native sharding support
Sharding was introduced when people were hitting the limits of how much a single
database could manage, and if you have such limit in a component you keep, you keep
the component as it is. You make copies of it and in front of it you develop something
that can give you a unified view over all of these copies.
Sharding was originally implemented by the applications accessing the database. Over
time, this feature, since it was used in many large-scale applications, migrated down into
the databases themselves. Nowadays when you buy a database that can scale beyond a
certain size limit, it will offer you some native sharding support and the advantages that
from the application side, it looks like the database can indeed store petabytes of data.
Behind the scenes there will be all of these strategies available to be configured which
can help you to scale.
541

Sharding
Different systems use different terms to name data partitioning
for scalability
Shard
MongoDB, Elasticsearch, SolrCloud
Region
HBase
Tablet
Bigtable
vnode
Cassandra, Riak
vbucket
Couchbase
542

References
• Martin L. Abbott,  Michael T. Fisher, The Art of Scalability, Pearson, 2015
• Gregor Hohpe and Bobby Woolf, 
, Addison-Wesley, October
2003, ISBN 0321200683
• Martin Kleppmann, Designing Data-Intensive Applications: The Big Ideas Behind Reliable,
Scalable and Maintainable Systems, O' Reilly, 2017, ISBN 978-1-449-37332-0
Enterprise Integration Patterns
 
 
543

Software Architecture
Availability
and Services 11
Contents
• Components vs. Services
• Monitoring Availability
• Impact of Downtime
• Control Cascading Failures: Circuit Breaker, Canary Call
• Redundancy with Replication: Consistency vs. Availability
• Event Sourcing
544

After discussing about how can we scale, let’s look into the availability of the architec-
ture. And as we have been looking at different qualities, we have been trying to find what
is the construct, for example, the architectural concepts (e.g., components and modular-
ity, or interface and reusability and so on) that are conceptually close with the corre-
sponding quality attribute.
Today we will make the jump between software components and services. We will use
the difference between components services to define the challenge of availability. When
we look into the quality of flexibility and how can the architecture deal with change, we
will make the next jump going from services to microservices.
As part of the lecture, we’re going to give an operational definition of availability. What
does availability imply? and what is the impact if the system is not available? What could
possibly go wrong if we have a downtime? This is particularly critical for services. We will
show how we can monitor it and how to design systems using patterns that can help us
to design systems that are highly available. In the same way that we looked into how to
scale and learn how to design a scalable architecture, we are going to look into how do
we design highly available architectures.
545

Example
Components vs. Services
Map Drawing Component
• What is the "size" of this component?
• How to deliver the component to customers so that it can be
included in their own applications?
//lat, lng - any GPS coordinate on Earth
//zoom - show details up to 1m resolution
Map getMap(lat, lng, zoom)
To introduce the idea of what is a service and what is the difference between a service
and a component, I would like to present you with this simple component interface.
The component purpose is to be able to display a geographical map. In the interface
there is a single operation which receives GPS coordinates anywhere on the planet, and
then it has also a zoom level. The result is a map. You can see it as an image, or some
other visual representation.
Knowing that you can show a very detailed map, up to 1 meter resolution, or you can
zoom out and just show the borders between the different countries, what I would like
to ask you is: if you have to build the component and then you have to deliver it to your
customers which will take the component and for example use it to display a map in their
application – What is the size of this component?
We know that we have to deploy components before we can use them in production
and to deploy them we have to package them so they can be released. And if we have to
make them self-contained so that there are no dependencies, all the software and all of
the data (the most important element in this case) that is necessary for the component
to work will be included in the package.
Do you have an estimate for how big is this component going to be? How many bits?
If you try to go back in time, from your personal experience, do you remember a time
in the history of software architecture in which the Internet didn’t exist? All software
had to work offline.
There used to be a time in which if you had to package and release the component, you
had to actually copy it from a golden master on some physical media: Floppy disks, CDs,
DVDs, USB sticks. Clearly, you are limited by the capacity of the media.
If you have to deliver the map component whose size is 7 terabytes, how many discs
do you need? How much is that going to cost you?
546

Example
Components vs. Services
The software industry before the Internet looked like this. We were writing code, but
there was a physical aspect to it at the end of the build process you had to burn the soft-
ware into CDs. Imagine how much plastic e-waste was produced with every upgrade to
deliver the latest version, how much energy to ship those disks around the world. Also,
someone had to physically unwrap them and use them to load the software onto each
piece of hardware to deploy the upgrade.
One problem with maps is that they tend to change. New houses are built, new roads
and bridges are built, and the map tends to become obsolete over time. We need a stream
of CDs that you ship whenever the map needs to be updated.
After the Internet came into existence, people started to find ways for the software
to be more efficiently delivered. You can take an image of the CD and you can make it
available as a file and then you can put it on a website or an FTP server. Users can connect
their computers to the FTP server, download the image of the software and install it.
This was greatly streamlining the software delivery experience. There was no longer
the ”commercial off the shelf” (COTS) software metaphor, because there was no shelf
from which you would take a beautifully designed box to open the box and get the CD
out of it. Now you just had to find out the proper address of the website. And maybe
after exchanging some payment, you would download and install the app.
After many attempts, App stores were born and you had a huge selection of software.
But this was still about finding an efficient mechanism for transferring the software. The
bits to unpackage and install from their source into the environment in which they get
deployed into so that they can run.
Another opportunity for delivering software through the Internet is actually to switch
to a distributed architecture. A client/server style in which your software is not being
delivered at all. It runs somewhere else, and if you want to use it, you’re just sending
messages to it and getting results and interacting with it from a client.
In this case you have the option of whether you have to install a specific client. You
solve the problem in half: the majority of the software, for example, the data, you keep
547

it on the server. Makes it easier to keep it up to date and we don’t have to download
terabytes with the map of the entire planet if your client just needs to draw the map of a
city. Just keep it on the server and let the clients access the part that they are interested
about.
However, you still have the problem of delivering a compatible client that can connect
to your server. Eventually this problem was solved by building a universal client, which
is called a Web browser that can connect to any other application in the cloud. You don’t
have to change the browser when you change the application that you’re using. The
software in the client is the same and it’s just so flexible that it can work with any other
back-end system.
Business model: how to sell
Components vs. Services
• Component developers charge on
a per-deployment basis: whenever
a new client downloads the
component.
• Component upgrades may be sold
separately to generate a revenue
stream
• Components can be licensed to be
redistributed within larger systems
and developers can demand
royalties from the revenue of the
ѹnal product
• Service providers can charge on a
per-call basis: each time an
existing client interacts with a
service by exchanging a new
message.
• Service providers can charge a
monthly/yearly Ѻat access fee
• Services can be made available for
free and providers can support
them with advertising revenue
If you are trying to make money with software, what are the opportunities to do so if
you work with a component-based approach? The idea is that you make money every
time someone needs to deploy your software. In the old days your customers would need
to pick up the box with the CD and would have to pay to carry it out of the store. Now
the equivalent is that your credit card gets charged when you click on the ‘Install App‘
button in the App Store.
Every time this installation happens, every time there is a new deployment, you make
money.
We all know that software is never finished, never completely done, especially software
that is being actually used. Every time you make a change, every time you improve it, is
another opportunity to extract money from your user community. At some point you
decide that you need more money, so instead of releasing free updates, you decide that
the next one about to be shipped is a major upgrade and they have to pay for it. The
toolbar icons and the app logo design have changed so much after all.
Sometimes, after you buy the software, updates are free for a year, but afterwards you
548

have to pay again to get additional improvements to keep the revenue stream flowing for
the developers.
These examples hold if you’re doing retail: if you’re selling the software to the end
users directly.
However, there is a whole other industry of OEM integration, where you are selling
components to companies that integrate the components into applications for the end
users. In this case the users pay for the applications and then you get a royalty because
your component was used inside the application that was sold.
That’s another business model. The advantage is that you don’t have to worry about
interacting with and supporting so many users; you just have one client and then the
client will resell your software to the larger audience, maybe in combination with other
components.
These are the opportunities to make money which are implied by this simple – but rev-
olutionary – idea that you can actually make people pay to get the right to use a piece
of software. It was not always like this, because the software used to come for free to-
gether with the hardware. And there was no point in selling software by itself because
it wouldn’t work without the hardware. So you pay for the hardware and the software
is just an afterthought. At some point somebody thought that they could actually make
money by selling the software by itself. And with this simple idea, you know the story,
one of the largest fortunes in the world was made.
However, at some point the Internet showed up and disrupted this model.
When we turn our component into a service available over the Internet. Other oppor-
tunities to make money appear. For example: every time users click, every time they
interact with your service running in the cloud, you can charge them for that.
If you think about how often does someone install a piece of software, as opposed to
how often they use it: this is a paradigm shift.
Having one exchange of value with one interaction when users download and install
your software doesn’t provide any feedback on what they are doing with the software.
Switching to a business model in which every day every time that they interact with your
system you are going to be paid changes everything.
Pay-per-use could be too fine grained. Maybe it’s complicated to track. And there is a
risk that it becomes too expensive if users use it a lot. It’s also possible to select a flat-
rate subscription model: to access my system, you have to pay every month. You get a
discount if you pay for a year in advance. This is often found with software as a service
(SaaS) providers.
More and more software is available for free. So if you make it available for free, how
can you make money with it? One answer is the so-called surveillance economy where
users are hooked into using free applications and then everything that they do with the
applications is tracked, mined and monetized, for example, to sell targeted advertising. If
people give you something for free, you should wonder how can they survive financially.
And what are they doing with your data? They may be going back to the original model
where the software is free, but you pay for the hardware to run it. This will tempt you in
using the software as an incentive to drive hardware sales. Like when you need to buy a
new phone to get the latest free updates of your favourite apps.
549

IT as a "manufacturing" industry
(ship software components)
IT as a "service" industry
(publish software on the Web)
In general, while going through the software lifecycle there is a point in which money
is exchanged. For example, between the release and the installation, at deployment time,
or later at run-time.
While pay-per-deploy is the original model tied to software components, pay-per-use
was not possible before because after the software gets installed, users run it on their
system, which will be offline. This makes it challenging to track what the user is doing.
We can see here the two reference examples of the two different models for the soft-
ware industry. One follows the metaphor that we are manufacturing software compo-
nents. This means that at certain point they come out of the factory. As components get
finished, they are shipped as part of the release; and that’s when you can sell them. It
turned out to be a highly successful concept, especially before the Internet.
Exploiting the opportunities of the Internet, we can make the software available as
a service through the web. We still need to build the software, but the software is not
something physical that you will ship out. You just deploy it on some servers in the Cloud.
What really matters is: What is the quality of service that you provide to your users?
You switch from a product-oriented type of industry into a service business where you
establish long-term relationships with your customers, where customers come together
with you to create value, as opposed to just taking a box off the shelf and walking out the
store.
550

Design decisions
Components vs. Services
• Constraint: Encapsulate
implementation behind well-
deѹned interfaces, free choice of
connector
• Decision: to buy or to make a
component?
• Promote reusability and
independent component
development
• Constraint: Remote connector
(e.g., Message bus)
• Services operated by different
organizations: high availability,
security/trust and loose coupling
(independent evolution)
• Many service providers compete to
deliver "Software as a Service"
from the Cloud
Let’s look from a technical perspective at what are the differences between components
and services.
Let’s just recap the definition of a software component. We draw a boundary between
the component in the rest of the system so that we can encapsulate the implementation
behind a well defined interface. Components are meant to be connected to each other
using a suitable software connector. In this case we don’t have a constraint on the type
of connector that we can use.
The main decision once we identify that there is a specific component in our system
is whether we are going to buy it or to make it ourselves. If we buy it, it means that
somebody is selling it to us and we go back to the previous discussion on the business
model. If we make it in house, we need to write the code and go through the all the
quality assurance processes before we can release it in production.
The purpose of all the component technology that we have seen so far is to make it
possible to reuse components. And make it possible for them to be developed indepen-
dently from each other. You decompose an architecture into different components so
you can parallelize the construction work: you don’t want to wait for a component to be
finished before you start with the next one. You can do all of them together because they
have well defined interfaces.
If we switch to services, we assume we can access our software through the Internet.
The critical decision becomes: how are we going to connect to the software if it is deliv-
ered as a service? We have to go through the Internet therefore we will have a remote or
a distributed deployment, as a consequence this puts a limit on which type of connectors
we can use.
It doesn’t make sense to invoke a service running in the Cloud while attempting to
use a share memory buffer to exchange the data. The service is over there, your client
is over here, there is no way that you can use local shared memory to send and receive
information. Still, there are various connectors that are possible. As an example, the
most frequently mentioned when we hear about services, is the message bus, which used
to be known as the enterprise service bus (ESB). Due to its asynchronous nature, it also
gives you the better availability.
551

While in the original component paradigm you would need to download and install
locally the software, this would imply that somehow - not necessary in a legal sense –
you would be owning the piece of software. The license to use it gives you the right to
install and start the application on your computer. Nobody else is doing it for you: you
are in control of the operational side. You don’t necessarily build it yourself, but once
you buy it, you download it and you run it so you operate it yourself.
With services, it’s exactly the opposite: the software that you use is operated by some-
one else. This other organization is responsible for making it accessible to you with good
enough quality, including availability. Additionally, there are multitenancy issues re-
lated to trusting that when you send your data into their system, this data is not, for ex-
ample, revealed to your competitors. It is theoretically possible to send encrypted data
into the cloud so that it can be processed there without decrypting it, but it is super ex-
pensive to do so. These are things you have to do if you cannot run locally the software
when you send the data to the other side, you trust that the data is forgotten once the
response comes back.
The fact that services are operated by someone else means that someone else is going
to make decisions on their life cycle: they will switch it on and switch it off whenever
they want. And they will apply changes to their services without necessarily warning
you. Some popular service APIs post countdown timers on websites announcing breaking
changes years in advance so that client developers are supposed to know in advance that
the changes are coming. Especially if the changes are incompatible, they need time to
prepare for that. In other cases you wake up one morning, you discover that your system
is broken, because the service provider went bankrupt and they have disappeared. This
is similar if you buy a flight ticket you go to the airport and you discover that the airline
is bankrupt and your ticket is not going to take you anywhere.
This can always happen and when you make a choice to rely on a service, your software
depends on its availability and you are willing to take the risk it may disappear.
The other interesting aspect is that once you can access the software as a service in
the cloud you have a whole market place with a set of competing providers that offer
you their software. You have a choice where to go and which software to use at different
price points, with different features, delivered with certain qualities and, most important,
associated with a brand, which you may or not trust, also depending on the reputation of
the provider.
Components were born in an offline world, so component technology is much older
than services. Still, you can have remote and distributed components. We still call them
components if the organization that is running them is the organization that bought
them. So you buy the component, you run it yourself even if you deploy it across a dis-
tributed environment. You are in control of its life cycle.
A service is a component that is remote but is also provided to you by a separate orga-
nization that is in full control of its lifecycle. All you can do is call the service and hope
that it answers you. There can be some expectations and guarantees, but you cannot do
anything if the service provider is offline. Or when a service disappears.
With the component you still have the bits of the software which can run as long as
you can deploy them in a compatible environment.
In other words, if you are in a company and you have the IT department that is sup-
porting the applications within the company, when the lights go out, if the application
stops working you have a phone number that you can call and there is somebody from
the same company that answers it. This is internal support for ensuring the availability
of the component. If the component is delivered as a service, the number that you call
will be answered by an external entity, and you hope that they still answer and fix the
problem with a comparable speed.
552

Overall, the main difference between remote components and services is not really a
technical one: it’s really about ownership and responsibility: who to blame for unavail-
ability.
Technology: how to use
Components vs. Services
• Component need to be packaged
to be deployed as part of some
larger application system
• Components need to be
downloaded and installed,
assuming they are compatible with
the existing framework used to
develop the system
• Problem: There are (too) many
incompatible component
frameworks
• Services need to be published on
the Web by advertising their
location to potential clients
• Services are invoked using
standard protocols based on HTTP,
XML or JSON
• Problem: Services are distributed,
remote components outside your
control. Beware if they become
unavailable!
Let’s look closer to more detailed technological steps to deliver the software so that
it can be integrated within our architectures. Delivering software as a service from the
cloud is different than just releasing your software, publishing it on a website so that
users can download and install it and run it locally.
We’ve seen already the build pipeline that we use to go from the original source of the
software into the component that we can release, and we can install. There is a point in
which you are transferring bits and you copy them on some environment that you control,
and assuming that you are downloading multiple components and you want to use them
together in the same architecture, you make an assumption that the components are
compatible.
Component interfaces must be compatible with each other, but also they need to be
compatible with the underlying framework they require, with the programming language
runtime. In other words, if you to take a random example, if you go to the Eclipse mar-
ketplace and you look for plugins, you look for components meant to be deployed on this
platform, you expect that you will find components written in Java. The chances that you
can easily take a component that is written in Ruby – they are called gems – which are
meant to work within the Rails framework, and deploy it within Eclispe are slim.
Those are completely different technological platforms. NPM modules cannot be in-
stalled using “apt-get”. Package managers work great as long as you stay within the same
platform. If you try to mix components from different languages and different frame-
works then you will have some problems due to the large number of programming lan-
guages and even larger number of different, incompatible by design, frameworks.
553

Some of you might mention the word ”Docker”. This is a recent development which
is trying to solve such component incompatibility problem and provide standard way of
packaging and installing the component no matter programming language it is written in
as long as it can run on a Linux-flavoured runtime. Components deployed in containers
expose programming language independent interfaces which make it easier to connect
them together. Such standard interfaces were originally invented to connect services.
If we switch to services, the goal is completely different. One of the reasons why there
are so many frameworks is that vendors make money with the frameworks as they at-
tempt to establish a virtuous cycle where you have a successful framework because there
are many developers that write components for it, and the more components are there,
the more developers will use the framework to find components for their applications.
And therefore, if you establish a new framework you want to lock in the developers and
– even if the programming language appears to be the same – you will define framework
interfaces and APIs that are incompatible. Once you write a component for that frame-
work, it will be very challenging to port it somewhere else due to its framework-specific
dependencies.
This makes sense if your business model is tied to getting people to use your technology
and make them dependent on it. If your goal is to make your software accessible through
the Internet, then you want the software to be as compatible as possible. You try to make
it accessible from clients that are written in any programming language. You seek the
freedom to implement your service behind a standard interface running in the cloud so
that you can implement it with the most suitable language.
You also don’t want this information to be actually visible or known to your clients. It
wouldn’t make sense to state: you can use Google, as long as if you use this particular
version of C++ with this compiler to invoke its search API. If you switch the compiler,
then you risk becoming unable to call this particular Web API.
This has been a huge evolutionary shift in mindset and also concerning the technology
base. After decades spent into standardizing all the necessary protocols so that you can
exchange data, you can represent it using a programming language independent way,
today most interfaces use HTTP as a way to transfer the data and then on top of it they
use either XML or JSON to represent the payloads, the message content sent back and
forth.
Another important aspect is the discovery of matching interfaces. We already dis-
cussed about registries, about directories. They are also relevant with services because
you need to find out where on the Web a reputable service provider that you can trust can
be found.
For these reasons we have a big difference, services put emphasis on interoperability,
so that the service interface is accessible from as many clients as possible. Components
face the challenge of portability: components written in some languages need to run in
different, potentially incompatible operating environments. This is not a problem with
services, because the service provider is in control of the operation environment. They
can optimize it and choose the most suitable one to ensure good performance, scalability,
and availability.
Components vs. services also impact software piracy. If you want to steal a copy of
the software, if your business model depends on people downloading and installing the
software, then it’s very easy to make a copy. If you are delivering the software as a service,
nobody will ever see your software. The software stays safe locked inside your data center
and people just call it from all over the Internet. To run a google search you do not have
to install your personal copy of google.exe – such piece of software is never going to
be downloaded anywhere. It stays within the boundaries of the service provider, which
however will need to deal with account sharing and fake API keys.
554

While services can be made accessible from anywhere, the remaining issue is that they
run in a distributed environment outside of your control. Once you draw a dependency in
your architectural model to represent: our system depends on a service API, there should
be a big red flag going up: What if this service API becomes unavailable? Such unavail-
ability could be temporary – the service becomes unreachable for just a few minutes and
nobody notices. Or the service disappears for a few days and then you read about the
outage on the newspapers. Or maybe the service provider goes bankrupt and then your
system depends on something that doesn’t exist anymore.
If you deliver a service which depends on external services, their availability will have
a strong impact on your own availability. How can you guarantee your own system will
actually work if you are not sure whether your dependencies will be there when you need
them?
555

When software runs in the cloud, sometimes we have thunderstorms. The system is
either not reachable because there are network problems – what if you try to access a
system while you’re traveling around with your mobile device? What if you are crossing
the Swiss Alps and you don’t have very good connectivity? Will the software work in of-
fline mode? You have experienced a network partition. The user device is disconnected.
How is this going to affect the user experience? Are you going to just disable the user in-
terface and stop the application from working? Once you reconnect you enable the user
interface and everything works again because the backend in the cloud is reachable. How
can you deal with this problem? Add a backup link. Make the connectivity more reliable.
Switch from the WiFi to the 4G or 5G antenna (or viceversa).
We can generalize this concept: this is a fundamental idea that we can use to improve
availability is to introduce redundancy into the architecture.
If something stops working, you have a backup. You have an alternative that hopefully
will take over so that the system still works.
We have seen that redundancy and replication also help with scalability. For example,
consider a content delivery network, we need to serve read-only assets and we can spread
them out throughout the CDN so that we can improve the bandwidth and latency towards
a large number of clients.
If we make multiple copies of the data, we have introduced redundancy: somewhere
we have a mirror which we can use if the main site is offline or unreachable.
But is it a good idea to stop the user from working when offline? If there is a glitch
with the Internet, should the world stop? Within every architecture there should be the
possibility to revert back to a state before the Internet existed. Once upon a time, we
used to know how to run software without the Internet. We should still be able to do it
556

today. Just download a local copy of the software and the relevant data. Even if we are
disconnected, we can still allow users to work with it. Then at some point we assume
that the connection comes back and we can synchronize: apply the local changes to the
server-side state, while fetching any updates from the server. There is a price to pay:
conflicts need to be detected and possibly resolved.
Availability Questions
Is it running?
Where is it running?
Is it reachable?
How long does it take to reply?
Workload
Response
Time
Resources
Response
Time
How long are you willing to wait for the reply?
The first question you can ask to measure whether something is available concerns
whether it is running or not.
What is the current state of execution of your system? The system started, it’s running,
it’s available, it’s ready for the users, or not: the system stopped, crashed, it’s not run-
ning, it is running through an infinite loop, it takes forever to respond, it’s not available.
Now if it is running, you also may need to know where is it running. That’s a different
level of awareness: from the user perspective, it is enough to know: ”if I need to use it,
it’s running and I can use it“. But from an operational perspective, you also should have a
more precise idea: we started it and is now running on this machine. We deployed it using
this container and while we run the container on this machine, if something happens to
that particular machine, this component will be affected.
If you just know that is running somewhere, you have no ability to predict what kind
of failures will impact, whether it keeps running or not. If you know where it is running,
then you know which machines you should protect and you should be careful not to un-
plug them at the wrong time. This is a most important concern from the service provider
side: I’m running it, it’s over there and I’m watching that nothing bad happens to that
machine.
Since it’s remote, you also have a concern from the client perspective. Even if you
say it’s running, you can reach it, but it doesn’t work for me. So availability may be
557

affected by network reachability issues. If you get disconnected, having a second network
connection can help mitigate the problem. This may help to improve the reachability of
the service.
The fact that it is running somewhere is necessary, but it’s not sufficient if you cannot
reach it, so both of these conditions have to be true: It has to be running and it has to
be reachable. Then we can start to look into the details and we can ask: how long does it
take to reply?
I’m going to upload my video on Panopto. And it takes one hour and a half to process it.
Is that really available? Yes, it’s reachable, I can transfer the data and it’s answering when
I refresh, but there is this progress bar that is going up and down. You see something
that tells you we are 95%. And then after one minute it’s gone back down to 20%. So it’s
particularly hard to estimate when you’re going to get the final answer.
One way to explain this is to look again at the scalability curves: we can see the re-
sponse time depends on the workload: how busy the system is. Green values show that
the system has an acceptable response time. How long does it take to reply? Not too
long, if I can get the video uploaded before the lecture starts, it’s fine. But if we are in
the red zone, we still get a reply, eventually, but the reply is late. If the reply is late you
have to ask yourself, are you willing to wait that long for the reply? If the reply comes
too late, it may be correct, but it may be completely useless.
When you worry about the availability of a system. Only as a first approximation: It
can be a black or white. If it’s running and reachable, it’s available. When is down and
offline, it is not available.
Then you can start to wonder, do I really care about availability unless I need to use it?
And when I use it, when I need to use it, is it going to give me a timely reply? This way
we can translate the availability question to ask: how long does it take and how long am
I willing to wait for it? This can also be seen as: How soon will the client timeout? When
you send a message, you may expect a response; but if the response doesn’t come within
a reasonable amount of time, you can conclude the response will never come. You’re
simply not going to wait anymore. Even if the response comes later, you no longer wait
for it, so you are going to miss it, because you already concluded that the service is not
available.
From your perspective, the perspective of the observer, or the client: if a system is too
slow, it is not available.
558

Watchdog
Watchdog
Watchdog
Service
Service
probe
ok
Ok
probe
ok
Slow
probe
timeout!
probe
Service is down!
Heartbeat
Clock
Clock
Monitor
Service
Service
beat
Ok
tick
No Beat
beat
Ok
tick
No Beat
tick
Service is down!
tick
Monitor
Monitoring Availability
You cannot know if a service is available unless you try to interact with it. To keep
track of the availability, however, we don’t want to rely on the users reporting outages.
Instead, we want to have an automated system that can continuously check whether the
availability state of a service.
Given the need of an observer to determine the availability of a system, there are two
different options for monitoring the availability of services, depending on whether the
service is passive or active.
One is by periodically sending probes from another system, called the watchdog. The
watchdog periodically probes the service whose availability we want to check. In case the
service is active, we use a heartbeat message which is periodically sent from the service
back to the monitor (or passive watchdog).
The watchdog is a component whose purpose is to call the service and check whether
the service gives an answer within the expected time. The watchdog (active) calls the
service (passive). If it receives a timely answer, we can conclude at this point in time the
service is available. If we send another request, another probe, after some time, maybe
it takes a bit longer to answer. The watchdog can not only measure whether we get an
answer or not. That’s black and white, but it can measure how long does it take to get
an answer. Based on the response time, we can determine whether the system is fast
enough. However, if we send a probe and we don’t get an answer. We cannot wait for-
ever: there will be a point in time in which we decide we have waited long enough. The
watchdog times out. And then we try again. If we don’t get an answer after a certain
number of attempts, then we can conclude that there is a availability problem with this
particular service. We go from a state of availability where we get a timely answer, to an
intermediate state in which the performance is degrading. Eventually we lost the service
because we have been patiently waiting for a very long time but the service still doesn’t
answer. So that’s the worst case scenario: a slow failure. It’s also possible that when we
559

try to connect to the service, we immediately get an error. This will be a faster way to
determine that the service is not available.
With this type of watchdog, the service is passive. So the service is just servicing yet an-
other request. It doesn’t care or it should not need to know whether requests are coming
from a normal client or from the watchdog. The watchdog is just one more client that will
check whether the service is capable of processing some probe request. Still, you need
to decide whether the watchdog calls a normal operation of the service, or if you have a
special interface that you just use from the watchdog to check whether the whole service
available. If you have a dedicated and separate probing endpoint, you need to deal with
false positives and true negatives, when the probe times out but the main interface is
still working. The same holds if you use separate network planes to transfer production
traffic independently of the systems management messages.
If our goal is to monitor the availability of the service. Is this solution enough? What
could possibly go wrong?
We’re making an assumption here. We have two components. One is watching the
other. What happens to the watchdog itself? Who’s watching the watcher? Consider
setting up a cross configuration, with a ring or a chain of watch dogs that are checking
each other if they’re available. And then they also collectively check the service. We need
not only to worry about the availability of the service, but also of the infrastructure that
is checking whether the service is available or not.
The service is available if it answers to the probe within the expected time. The probe
can be any message accepted by the service API. The challenge of having a watchdog that
probes the service is to make sure that if the answer is successful, this is a representative
observation where the same behavior would also be perceived by any other client. Better
avoid the situation in which the watchdog thinks that the service is available, but then
the service will not work for any other client.
If you don’t get answers, you insist, you keep sending multiple probes. Only if no
answers comes back, after multiple attempts, you can conclude that the service was not
available. How many times should you keep trying?
It’s also possible that when you try to connect, you cannot reach the servers. Also in
this case, from your perspective there is an availability issue. But the problem could be
due to networking issues on your side and although the service is running, you cannot
reach it. This is why you may need multiple watchdogs to reach the conclusion: the
service appears not to be available from everyone. Is it down for you too?
There are some scenarios in which you can afford to have a service which is active. This
means that the service will be sending periodically heartbeat messages back to a moni-
toring component, which will basically check whether the messages arrive when they are
expected or not. This alternative design is also used in practice with the assumption that
the service is actually going to be notifying the watchdog about its existence, for example
when registering its location with a directory.
We have a service that periodically notifies the passive watchdog of its availability.
That is, the watchdog expects to hear from the service every so often. In case this doesn’t
happen, then basically the service has skipped a heartbeat, or the watchdog missed it.
Probably something happened on the other side. There will be a window in which the
watchdog is listening for messages; if the message arrives on time, it can conclude that
the service was running. If you don’t get any message you can start to worry, but the
message could still arrive late. We have a slow service that for some reason is behind the
schedule.
If however this situation persists like in the example, with two missing messages – it
could be more, depending on how sensitive is the watchdog – we could conclude that
the service was lost. While the monitor can only show when was the last time we heard
560

from the service and how many heartbeat went missing, if this time grows above a certain
threshold, the watchdog may start to actively probe the service to confirm that indeed it
is no longer running.
In the same way the directory helps to discover the location the other components,
also the watchdog helps to keep track of the availability state of the other components.
Both suffer from the same limitation, since the watchdog cannot monitor itself and the
directory location cannot be looked up on the directory itself.
What is the role of the clock? The clock tells the monitor when to expect a new heart-
beat message. There has to be an agreement between the clock and the service regarding
how often the message is sent. This is similar to the decision on how often the watch-
dog should probe the service. You don’t want to do it too often, because otherwise the
watchdog is overloading the service. But the less frequently you do this, the longer the
delay will be between when the service goes down and when you can actually can detect
it.
In the same way that you have to decide for how long or how many attempts are you
willing to wait before you give up and declare a connection timeout, we also need to
configure how frequently to perform each attempt. It may also be a good idea to employ
exponential back off in case the probe itself may overload the service.
Another aspect to consider, for example with deployments in an Internet of Things
environment, is the energy consumption induced by the watchdog or monitoring system.
Sending probes and heartbeat messages has an energy cost. The more you send, the
faster the battery gets depleted. So you need to trade off the delay with which you detect
outages against the duration of the window in which a battery powered watchdog can run
unattended.
561

Which kind of monitor?
Watchdog
Passive Service
Track Response Time
Reachable Service
Connection Refused
Missing Response
Heartbeat
Missing Request
Reachable Monitor
Active Service
To understand the difference between these two different solutions, I would like to ask
you if you can fill out the different properties for each of them.
Unlike a monitor, with a watchdog you can monitor not only whether the service is
available or not, but you can also track its response time.
If you use a watchdog, the service has to be reachable, since the interaction starts from
the watchdog which has to be able to contact the service.
You can use an active watchdog when the service is passive: the service by itself is not
aware that is getting probed. It does not have to do anything extra so that you can track
its availability. The service is just deployed and started, and then the watchdog calls it
and the service replies as if it would receive requests from any other client.
The watchdog is going to periodically send messages to the service. This is the differ-
ence with the heartbeat, in which the service has to send the messages to the watchdog.
And this means that the service, in addition to all the other things that is doing, has to
do one extra thing, it has to remember to send the hartbeat on time. For this to work, the
monitor has to be reachable.
When it comes to the errors, then you should put yourself from the perspective of the
component that is monitoring the system, so it’s the perspective of the watchdog or the
monitor. If you send a probe and after a while you don’t get a response, then you know
that the service is probably not available. If you send another probe and the response
is also missing, then you can raise an issue with the availability. If you try to send a
probe and the connection is refused. That’s even a stronger sign. You don’t need to wait
to detect that there is a problem, since the service does not accept your connection, you
know that it will be impossible to send out the probe. As opposed to waiting for a missing
response after sending the probe – the probe goes out into outer space and you listen into
the void for a faint echo – if when you try to send the probe, you cannot even do that,
then it’s clear the service is not available.
562

From the hardware perspective the heartbeat will help to detect availability issues
when there is no request coming from the service that is supposed to send a heartbeat
messages. You can still get a connection refused or a missing response, but since the
service is active, these errors will tell the service that monitor that is supposed to receive
the heartbeat is not available. From the monitoring system perspective this is not useful
information.
If you introduce a watchdog, you have a service that is just there waiting for clients
and the watchdog becomes yet another client that has of course to be able to reach the
service. It has to be able to connect and send the request. If the response comes back, you
know that the service is available. If you choose the heartbeat alternative, you are in the
opposite situation: you have a service that is active and needs to be able to send messages
to the monitor. If the monitor doesn’t get the request from the heartbeat message from
the service, it knows that the service has disappeared.
In terms of connectors, since there is no response required for heartbeat messages,
they can be sent asynchronously over a message bus, while the watchdog uses a call in-
teraction with a request followed by a response message.
563

Availability Incidents
No Impact: The service is down while no-one needs it. A client may
retry the call and ѹnd the service back up again (assuming
idempotent interactions). Automated failover mechanisms can
mitigate the failure.
Low Impact: Manual intervention required (few hours) to recover
the service, some customers start to complain. Easy to spot the
cause of the failure, more difѹcult to ѹnd the operator/developer
in charge to ѹx it.
High Impact: Media visibility of the (1+day-long) outage. A whole
disaster recovery team needed to bring it back up.
Let’s see what happens when something is not available. Of course, if you are down
when nobody calls, nobody notices. There is no impact. Just like software nobody uses
has no bugs, services nobody calls are always available.
What happens when things go wrong? Sometimes you might have experienced a tem-
porary glitch. You just retry the call and it works again. Something failed, but there is
some automated recovery process that takes over and fixes the issue. The impact is very
limited, the failure has been contained. Maybe only one user notices something, but the
other users are not really affected.
We can also have incidents with low impact. There is no automated recovery. We need
to have some manual intervention. Typically the delay is caused by the fact that we have
to find the right people who know which is the right button to click on and have the right
to restart the system. It’s easy to fix it once you find the right person that knows how to
do it.
When the down time grows beyond a few hours, many people start to notice and you
start to hear complaints. Since, however, the system works again after you restart it, user
disruption is contained.
You can also have a major impact when recovering from the disaster requires to set up
a whole team and execute a small project that takes days to succeed. Imagine a popular
service, some kind of social network, when such incident happens there will be some
visibility in the news. The media will report the outage. Or the outage will be reported
by users through their social network accounts.
In the same way you need multiple watchdogs to monitor one another, we need mul-
tiple social networks because people can vent on one social network and complain about
not being able to use the other one and vice versa.
If you subscribe to the view that there is no bad publicity, as long as people are talking
about you; you can also take advantage of the spotlight to enhance your brand visibility,
be creative with the error messages (the big failure whale comes to mind). Even though
people cannot achieve what they want to do when they try to use your service that is
not available, they still get a little bit of entertainment and they don’t go away totally
frustrated.
564

Downtime Impact
Revenue Loss: While the service is not available, the business is
disrupted and the revenue may stop Ѻowing
Reputation: Customers are unlikely to trust a service with their
trafѹc and their data if the service is not responsive and the data
is out of reach when they need it. Planning and announcing
outages in advance as well as some degree of transparency in the
recovery process of unexpected downtimes may help to manage
customer expectations.
Recovery Cost: The activities to diagnose and repair faults may
require a signiѹcant effort and to provision backup resources
Refund: Long lasting outages may violate Service Level Agreements
and trigger refunds (in cash or credit for future usage) to avoid
litigation
What’s the impact of worst-case scenarios? You lose money. When something doesn’t
work, the business stops, economic value creation stops, financial flows stop. For ex-
ample, there is a particular system to get payments from your customers. If this doesn’t
work, you’re not getting the money. This can quickly become a critical issue, especially
for high throughput money making machines, every second downtime can translate to
millions of lost revenue.
There is also an impact on your reputation. Clients over time learn to expect a certain
quality of service. If when they come to you, you don’t deliver on the expectations you
created. If you break your promises: your service is not responsive, users entrusted your
service with their data and oops, your irreplaceable bits have been lost, we are sorry for
the inconvenience. This is a major issue. It takes a long time to build up a reputation and
a very short time to tarnish it.
Expectations matter. There is a big difference between planned outages, which can be
announced beforehand and the ones that happen without any previous warning. If you
promise that you will be available again, after a short down time, then as long as clients
are informed, they will not be so disappointed.
In 2020, this can also happen in the opposite way. There is now an expectation that
when you try to order something in an online shop, typically the delivery windows are
never available because they are so overloaded at the moment. One day I was surprised
to find an open delivery window right away. I connected and everything was green. I
placed an order and they actually took my order. Just like in the good old days. You need
something and they are available. They will take care of it. And then a few hours later,
I received one of those emails: actually there was an error, our service was mistakenly
available at that time. It was supposed to be closed, but for some reason it was miscon-
figured and it was open and taking the orders. So we are going to cancel the order and
565

refund the payment. Thank you for the understanding.
Availability is affected by overload and lack of capacity to scale beyond a certain work-
load, and thus it can be improved by investing in additional resources (assuming the ar-
chitecture can scale to benefit from them). Restoring availability after failures, requires
to pay the cost of recovery.How much effort do you need to bring it back up? So is it just
a matter of restarting something? Or do you have to rebuild an entire data center some-
where else because the hurricane flooded the current one? How much time do you need
to get the backup resources online?
Depending on your service business model, if you have some subscribers that are not
able to use your service during a certain time because of the outage, you might start to
get some liability. Not only you lose revenue from pay-per-usage clients, but you also
have to refund subscriptions or give them credit for future usage because your system
was not available.
566

In general, with services, the expectation is that you offer them with 24/7 availability.
Users from all over the world should be able to access and use your system no matter
what, no matter when or where they are. And this is a very challenging thing to achieve.
Let’s see what kind of architectural patterns we can introduce to be able to actually do
that. How can we contain the impact of failures? As you can see in this model, there
was this idea that even though an iceberg may punch a hole through the hull of the ship,
only one compartment would get flooded. The ship will not sink. How many flooded
compartments can the ship survive?
The idea is that you try to isolate the impact of the failure, you try to protect the overall
system, even though parts of it might suffer. You need to prevent cascading failures to
bring down the whole system. You install fireproof doors which should remain closed.
You put travelers in quarantine to stop the plague from spreading. You dig a second side-
tunnel across the mountain. To access the Internet: wireless as well as wired access,
thank you, even for laptops.
What’s the simplest idea that we can introduce in our software architecture to be able
to do this? It boils down to the notion of redundancy. You want to have redundancy in
your architecture if you want to increase the availability and reliability of your system.
But with redundancy, the software becomes more expensive to build and to operate. You
can ask your accountant who is skeptical about the cost of the redundancy if you can
afford the cost of the recovery.
567

Retry
Retry the failed operation
If the call fails or no response is received (a timeout occurs), repeat it.
If a message is lost, resend it.
How to recover from temporary failures?
Let’s start with a very simple form of redundancy over time. If you can expect the
failure to be temporary. From the client side, when you try to invoke a service and it
doesn’t work, what can you do? You just try again. Try again later, hoping for the best.
You don’t give up, keep trying until it works.
There are many events which may lead to the opportunity to retry a synchronous call.
The call doesn’t go through, the call might fail, or maybe a timeout or a disconnection
occurs during the call. You are sure the request was sent, but you don’t get a response.
After waiting long enough, just hit the refresh button, maybe it will work now.
If something goes wrong while sending a message, just resend it. This implies that
you need to keep a copy of the message until you’re sure that the other acknowledges
its receipt. Only when the other side confirms that they received the message, you can
forget it.
Doing this will improve the chances that the message gets across, eventually.
568

Retry
Client
Service
Service
Down
timeout
retry
Ok
timeout
random wait
retry
success
Retries eventually succeed 
(with temporary failures)
Client
Service
Service
Slow
1
timeout
retry
2
late 
success
1
success
(again)
2
Retries introduce duplicates 
(redundant rework)
How does the retry mechanism works? We try to talk with the service. But the service
is down, so there is no answer. Even if we wait for the answer, the answer doesn’t come
back. At some point, we have a timeout that will trigger the repetition of the same call,
the same request will be resent. We’re lucky, the service is back up. Now the service is
available and we get the reply back.
To retry is to be optimistic. It assumes that the service will eventually recover, that the
failure is temporary. It also assumes that the messages can be resent as many times as
you want. What kind of messages can be repeated? Can you always do that with arbitrary
messages with arbitrary operations over service interfaces? There is a problem.
When we repeat a message, when we retry, we introduce redundancy in the system and
this can be a problem. Let’s take a look at this other case, in which we have a slow service.
Even if the service is slow, the first message still gets across. The message, gets processed,
but it takes longer than the client expects. If the client is not patient, if the client resends
the messag too soon, we have now a copy of the message that will be processed. The client
will actually receive not only the response to the original message, but it will also receive
the response for the second one.
Since have introduced a duplicate message into our system, an operation that was com-
pleted slowly once is actually done slowly twice. As a result, we have more load on the
service due to the client timeout being too short. The client was not willing to wait any-
more, so the service had to repeat the work twice.
569

Retry
• Which operations can be safely retried?
• Idempotent operations (result does not depend on how
many times they are executed)
• Stateless components (result only depends on input, no
side-effects on the component state)
• How soon?
• Retry immediately (risk of overload by refresh)
• Retry after delay (exponential backoff)
• Which is better?
• fail after the ѹrst try (soon)
• keep retrying until it works (maybe forever)
Think about what kind of messages can be retried? Can you give me an example of a
message which is not possible to safely process more than once? What could go wrong if
we process the message twice?
Maybe the request is about reading some information. If you retry, you may get 2 copies
of what you were trying to read. Maybe one copy is more recent than the other. But there
is no side effect on the service. Read operations can be retried.
What if, for example, we have a financial transaction. The semantics of the messages
is about to deposit some money into the users account. Of course the user is happy if you
do it twice. But the bank is not so happy. However, if you attempt to withdraw money.
You want to make sure that you withdraw the money exactly once.
Any kind of interaction which changes the state of the service incrementally, based on
the previous state of the service, cannot be retried. Otherwise, side effects accumulate
and both the result and the state of the service will be different depending on whether
the request was retried or not.
The idea is that retries should enhance the availability but not change the basic func-
tionality.
If the service interface offers operations in which the result does not depend on how
many times you do it, they are called idempotent operations, then retries are fine. You
can also have stateless components – as we’ve seen the result of stateless operations
depends only on the input. Therefore there is no side effect. Therefore it’s possible to
just retry the computation. You will waste CPU cycles, but there is no problem with the
consistency of the system.
How can you turn every every other type of operation into something that can be re-
tried? You have to put unique identifiers on the original messages and then you have to
do message deduplication. You have to remember that you’ve already seen a message and
you shouldn’t process it twice. There is a cost to check every incoming message against
570

the identifiers of all messages that have been previously received. The duration of the
time window used to remember previous message identifiers should be set according to
the retry timeout.
If the retry doesn’t work, we can keep retrying for a while. Maybe wait longer and longer
between retries, to avoid flooding your service with too many of these retries. When a
service is overloaded – it is not yet down but just on the brink of collapse – a few of these
extra retries from impatient clients may be enough to bring it down for good.
This happens more often than you think. Most users have been trained: If it takes
too long, just refresh. If the service is slow, it takes too long, what to do? Click refresh
and therefore the service gets even slower, even more loaded with even more requests.
This can start one of these positive feedback loops in which the longer it takes, the more
people refresh and then it takes even longer. It’s hard to recover from these situations
unless you’re willing to stop retrying, at least for a while.
Since we can protect the service from duplicate messages induced by retries, should
we always retry? Should we always keep retrying forever? The service is bound to come
back up, right? What if we recover the service on a separate server? What if the client
keeps sending messages to the previous address? When is the client going to give up and
consider doing a directory lookup to discover that the service has moved? And yes, it was
already available at the new location?
We should not retry forever, since in the worst case failures are never temporary. The
question is: How soon do we give up? When can we escape the retry loop to attempt a
higher level recovery strategy? What if we skip the retry at the first sign of failure so that
we can report the error as soon as possible?
Retry is a good strategy if you have temporary failures that can be expected to be recov-
ered soon, so called glitches. While stuck in a retry loop, the system will not necessarily
report there is a problem, because it’s hoping that it will work, at the next retry. This
means that users may perceive the system as hanging, and their usual recovery strategy
(click refresh) is already being attempted by the system, so it’s no use for them to add
another manual retry on top of the automatic ones.
The message is that if infinite loops are the enemy of developers, infinite retry loops
are the enemy of operators dealing with outages. It would be a good idea to be able to
exit the loop and just fail, as opposed to waiting forever while delaying the inevitable.
Somewhere there should be a configuration option called: maximum number of retries.
571

Circuit Breaker
turn a slow failure into a fast failure
If the failure/timeout occurs during a remote call the circuit breaker trips
and avoids performing future calls to the failed service
How to avoid retrying forever synchronous calls?
Wouldn’t it be nice to be able to remember from the past whether you were trying to
interact with a service that was not available? You learned the hard way it’s down because
you kept retrying for two days, but it still didn’t work. So don’t even try to enter the retry
loop the next time. If it didn’t work before, what’s the use of trying so hard only to give
up after reaching the maximum number of allowed retry attempts?
While it’s clear that skipping retries altogether is for pessimists, there is some prag-
matism in trying to learn from previous experience.
When we introduce a circuit breaker into the architecture, we specialize the remote
procedure call connector to use a smart retry strategy.
The problem of remote procedure calls is that they block the client until the call has
completed. While this may not be a problem if the service is available and the call com-
pletes with the usual speed. In case retries come into play, completing the call will start
to take much much longer than usual.
The circuit breaker is a special kind of connector, a variation over the remote procedure
call connector which deals with these temporary failures, these availability issues and
helps clients to avoid getting stuck into an endless retry loop while attempting to call a
service which is already known to be unavailable.
In other words, we want to remember what is the state of availability, or at least an
estimate over the last known state availability of the service that we’re trying to call. If
we expect the service not to be available, we give up immediately. While if we expect the
service to be available, we call it. If our expectation is invalid, we switch between the two
states.
If it becomes possible to immediately respond that there is a problem, it may be pos-
sible to take some alternative action or recover from the client or simply inform the user
right away and let them decide if they want to refresh.
For example, if the call didn’t work, you can display the cached results from the pre-
vious call. You can make users aware of the unavailability without having to slow down
their system.
572

Circuit Breaker
Service
Service
Client
Client
Circuit
Circuit
Breaker
Breaker
failure
failure
timeout
timeout
!
call
call
not available
not available
!
slow
slow
failure
failure
fast
fast
failure
failure
Let’s see how it works. If you make a call and the call fails or it times out after some re-
tries, then you will stop calling the service in the future. While the first time it takes some
time, you have to wait for the timeout after all, the next time the call fails immediately.
We can see here how the interaction works. We have the client making the call. The
circuit breaker is in the middle, so this can be absracted as a connector which takes the
call and forwards it to the service. If the service is not available, it takes time before we
detect that there is no answer. When there is a time out, we have an error, which gets
forwarded to the original client: This service you are trying to call is not available, despite
several attempts, we didn’t receive an answer.
Additionally, the circuit breaker remembers this outcome. The next time we try to
make a call to that service, the circuit breaker will immediately respond with the error.
This may seem like a minor optimization. However, imagine a complex architecture
in which you have not only a client calling a service, but you have a client calling service
which calls another service which calls another service. If there are a lot of nested calls,
a slow failure with one call will affect all the other calls. And it may take a very long time
before the client notices that somewhere down the stack a service is unavailable, after
everyone in between finally gives up retrying after going into timeouts several times.
573

Circuit Breaker
Service
Service
Client
Client
Normal
Normal
Operation
Operation
Failure
Failure
Detected
Detected
Recovery
Recovery
Within the circuit breaker, the decision on whether to forward calls or stop them de-
pends on the state of the breaker, which reflects the latest known state of availability of
the service protected by the breaker. A closed circuit breaker lets the call through when
the service is assumed to be available. When the client makes the call, the service breaker
forwards it to the service and the answer is returned within the usual time, depending on
the performance and the workload of the service.
However, if the circuit breaker detects that there is a problem with the service, it will
switch to the open state in which the next call will be bounced back to the client imme-
diately. The detection can happen by observing the failed outcome of previous calls, or
with the help of a watchdog.
The metaphor works like in electrical circuits. When accidents like short circuits hap-
pen, the circuit breaker can save your life because it disconnects and removes the power
automatically. To restore the original state from the disconnected state, you have to re-
set the breaker manually, that is after solving the problem which tripped the breaker in
the first place.
Tracking the state of the breakers can help to monitor the availability of the corre-
sponding services. One can easily build a monitoring dashboard showing the state of all
circuit breakers in the architecture. You notice the ones that have tripped and after re-
covering the corresponding services, you can manually reset them to let the calls through
again. Or you can also try to do it automatically: every once in a while when you get a call
the breaker will try to forward it to the service to see if the service is back up. And in case
this particular call works, then the breaker goes back to the normal operation state. You
start to recover it after a certain amount of time, to give enough breathing room to the
people trying to recover the service so that you don’t immediately flood it with traffic.
574

Circuit Breaker
• Isolate the impact of failures (errors and timeouts)
• Need to determine how many failures can be tolerated before
tripping the breaker
• Recovery: the circuit breaker needs to be reset (manually or
automatically)
• Apply whenever you do not trust your dependencies to be
available at all times
• The client needs to expect failures and tolerate them (use
default or cached response value, temporarily disable
functionality)
The main idea of the circuit breaker is that for a synchronous interaction, the client is
calling the service and the service needs to be there to take the call. The success of this
interaction is highly sensitive to the availability of the service. If you make a call and the
service is not available, you typically end up waiting before you give up and conclude the
answer is never going to arrive.
The idea is that we can close the circuit breaker and immediately fail the call: “sorry,
you’re trying to call something that is not going to answer you“. Clients want to know this
as soon as possible. What can clients do when they get such a response? This response
is not coming from the service because the service is not available. If clients are trying
to read information from the service, this can be cached. If the service is not available
with the latest updates, a previous version can be fetched from the cache, whose avail-
ability should be independent from the one of the service. Clients will not get the latest
information, but at least get a copy from the past and maybe that’s good enough.
If clients are trying to write into the service, the circuit breaker quickly informs them
that the update didn’t go through. They will need to retry it again later when the service
recovered. So it is not the responsibility of the circuit breaker to buffer messages and
automatically resend them. The circuit breaker is not a message queue. Depending on
the client, the rejected updated will need to be stored locally, or an error is reported to
the users, who can wait for better times to retry the operation.
One important decision regards: how sensitive is the breaker? Is it going to trip on the
first failure? Or how many failures should happen before it switches to the closed state?
Also, how do you recover it? How do you reset the state? Not only you have to recover
the service, after the service is back up, remember to reset its breaker, or clients will not
be able to call it.
We introduce this type of solution when we do not trust our dependencies. In case
dependencies are expected to go up and down, and we want our system to avoid get-
575

ting stuck either in a retry loop or just waiting for a timeout, we add a circuit breaker in
between. When the dependencies become not available, clients will behave more pre-
dictably. They will fail rapidly and consistently, as opposed to just hanging and timing
out after some time.
This idea only works if the client is built in a way that can handle failures. When the
circuit breaker trips, calls are guaranteed to fail fast. The circuit breaker does not re-
move the exception, it just makes it happen faster. Instead of waiting for 10 minutes and
then raising the exception, when the circuit breaker trips, clients need to deal with the
exception right away.
Clients need a strategy to tolerate unavailable services. They can disable part of the
user interface, but keep the main user workflow available. Maybe the call was to get
some optional information about a minor subcomponent of the user interface. If it takes
time before you realize you are never going to get the answer, you are going to block the
whole system for nothing. Instead if you can just hide or disable the failing component,
but show everything else immediately, that’s a much better experience for the user.
576

Canary Call
use an heuristic to evaluate the request
Try the potentially dangerous requests on one recipient and scatter the
request only if it survives
How to avoid crashing all recipients of a poisoned request?
The Canary call is a useful pattern when there is a chance that when you send a request,
the recipient may crash directly as a consequence of your request. We call this a poisoned
request.
The idea of the canary call is that you do not forward suspicious requests to everyone.
But you first observe how one behaves when processing the request. If you get more con-
fidence that this is not a poisoned request, then you can broadcast it. First try the request
with one recipient and scatter it to all the others only if the first recipient survives. We
use the Canary in the coal mine metaphor, if this particular recipient has a problem and
then you can expect that everybody else will also have the same problem. So we are in
the context for this is the scatter gather scenario in which just naively forwarding the
request is going to crash all of your workers. And the goal is to avoid a potential high
recovery cost.
577

Canary Call
Client
A
Scatter/
Scatter/
Gather
Gather
B
C
!
There is a chance that scattering the request, 
will result in crashing the workers receiving it.
We illustrate the problem in the context of the scatter/gather pattern. You are not
only sending the request to one component, but you send it to many components at the
same time. For example, with a master worker architecture, what if there is something
wrong with the input data and after partitioning it, you send the chunks out to a cluster
with thousands of parallel workers and you crash all of them? To recover, you will need to
reboot the cluster. It will take a long time before the system is available again. You didn’t
just loose one node, you lost everything, so all other requests got stuck in the queue.
What we can do is first try the request on one worker. And if it crashes, we don’t forward
it to the others, we just reject it.
This is a simple way to evaluate how dangerous is the request, with the advantages that
if the request is rejected after one worker crashed, the other workers survive to process
another set of requests. If the request is successful on one worker, we can send it to
everybody else.
This is a pattern that we can apply in the context of a scalable architecture in which
we want to spread out requests across multiple elements, and we want to try to protect
these elements, prevent them from crashing when they receive a bad request.
578

Canary Call
Client
A
B
C
Try 
Request
The ѹrst request is sent before all others to check if it would
harm the worker receiving it (canary request).
Scatter/
Scatter/
Gather
Gather
Worker 
Crashes
Request 
Rejected
If the canary request crashes the worker, it is not forwarded to the others
which survive the attack.
Other
Workers
Survive
Canary Call
Client
A
B
C
Try 
Request
If it worked 
scatter 
in parallel
The ѹrst request is sent before all others to check if it would
harm the worker receiving it (canary request).
The decision on whether all other requests should be scattered depends on
an heuristics: continue if the canary request is successful.
Scatter/
Scatter/
Gather
Gather
Request 
successful
579

Canary Call
• Performance/Robustness Trade-Off:
• Decreased Performance: the scatter phase waits for the
canary call to succeed (response time doubles)
• Increase Robustness: most workers survive a poisonous call,
which would have failed anyway
• Apply when worker recovery is expensive, or when there are
thousands of workers involved
• Heuristic: Failed canary calls are not necessarily poisonous
If we introduce this pattern, what’s the advantage in terms of failures? Only one ele-
ment will fail as opposed to every element.
In the successful case, when no failure occurs, what is the disadvantage? What do we
lose? Compared to the original case in which we just risk to directly forward the request
to all the elements, it takes more time.
Canary call requires to trade off between: How many of our workers we want to save?
How much longer will it take before we can send the final response?
First we have to try out the request in one place, so this will take some time. If the
request is valid, we can work in parallel as we scatter it out. We have just duplicated, in
the best case scenario, the time that it takes to service the request. Since we have to wait
first to see if it works and then we have to process it in the other places. It now takes
twice as long.
Canary call is not something to be introduced without being aware of its performance
cost. If you want to protect the availability of your cluster, you will just make every re-
quest run twice as slow.
You want to apply it when recovering a worker is very expensive. Or you have so many
that could potentially fail that it will severely impact your data center. For example, if
you send a request to modify the server configuration or install a security patch. Would
you fire off such request to all of your servers without first upgrading one and observing
whether it remains stable for a little while? If you lose half a data center, then it takes
time before you can recover it and you may not have any backup while you are doing so.
If a request fails one worker, is it a good indication to predict that all workers will fail?
How can you be sure that a crash in one place means that it is was poisonous request?
Or was that machine going to fail anyway? That’s difficult to tell, the pattern relies on a
heuristic decision. You try, you observe and then you draw an inference about what may
happen elsewhere.
What’s interesting is that today there are many tools to make predictions. Machines
can learn. Canary call is applicable also with more advanced heuristic to validate re-
quests. If too many requests are discarded, clients may complain. If poisonous requests
580

do not always crash a worker, they may make it past the filter and wreck havoc on your
data center. A request could be perfectly valid, but it triggers a bug in the recipient and
until the recipient is not fixed, the request should be stopped from propagating.
In the simplest case, you can always try to run it somewhere and see what happens. If
you can anticipate and detect malicious requests without actually running them, that’s
even better.
581

Redundancy
High Availability is achieved by introducing redundancy for critical
components (primary and at least one backup)
Cost of redundancy < (1-Availability) * Cost of Unavailability
Fail-over is the process of automatically switching from the
primary to a backup instance, which is standing by:
• Hot, ready to begin working immediately
• Cold, needs to be initialized before it can take over
Load Balancing also helps to increase availability when faulty
workers are pulled from rotation
If you are going to fly on an airplane and something goes wrong with one of the engines.
Would you rather be flying on an airplane with four engines or an airplane that only has
one engine?
Redundancy is about paying the price to increase the reliability of your architecture
by dedicating enough spare resources so that in case something goes wrong you can still
land safely.
If you cut costs in the name of being lean, then you will probably sacrifice the minimum
level of redundancy that will make it possible for you to survive when something goes
wrong. It may be a good idea to have some spare parts. To invest in cash reserves. To
anticipate expected or unexpected problems in the future. Keep a few extra masks around
the house for the next wave.
When we design a software architecture, we can decide to introduce redundancy. That
means that we will make multiple copies, replicas, instances at different levels of gran-
ularity. The entire system could be deployed in different data centers. Individual com-
ponents will be installed and be running in different containers, hopefully running on
different physical hosts, installed across different data centers. Or the same interface
could be re-implemented by different independent development teams to avoid repli-
cating bugs.
Redundancy will have a cost, but these costs will be less than the penalty that you will
need to pay in case something goes wrong.
If, when something goes wrong, there is no problem – after all users are always glad to
take some unplanned vacations while you rebuild their server, restore the database from
backups, and will just start happily working again when the problem is resolved.
But if you cannot afford that, then it’s better to invest up front to introduce enough
redundancy. It will pay for itself, or even be cheaper when things go wrong, but your
company workflows are not disrupted thanks to such redundant components: when one
582

of them fails, we can still use the others. If one of the engines of the airplane doesn’t
work, then we can still fly because we have the others.
To minimize disruption, in some cases you will keep these backup components hot.
This means that as soon as something stops, you can immediately continue by using the
back up. We say that the failover is instantaneous.
Alternatively, you have to start from a cold backup, which takes longer to initialize and
bring up to speed. If the hardware is not running to save energy, you have to power up
the hardware. This takes time. Then you have to start the software and make sure it has
the latest version of the data.
Cold standby means that it’s ready to go, but you have to switch it on and wait for it to
come online. If it’s hot there is a higher cost to keep it running, but the advantage is that
fail over is supposed to happen instantaneously.
You can also use redundancy in combination with load balancing strategies that we
have discussed earlier. With load balancing you introduce redundancy for the purpose
of scalability because you need more capacity. As a consequence, if one of the workers
becomes problematic, you remove it from the pool. You stop sending work to this partic-
ular failed component. The rest of the system can survive because you have already the
other workers which are still available.
State Replication
Storage
Client
read
write
Replicated Storage
read
write
High Availability
read
write
Strong Consistency
3. read
1. write
2. write
Weak Consistency
2. read
1. write
3. write
Partition = Inconsistency
read
write
Client
Client
Client
Client
Client
It’s easy to build a redundant architecture if you have stateless components. Trivial:
you can start as many copies as you want. Quickly initialize the component so that it can
receive messages and process them without affecting its state, because there is none. So
we’re not going to talk about that.
What we will focus on is: how do we make redundant architecture out of stateful
components? In other words, how do keep replicated state synchronized or consistent?
583

That’s the main challenge.
When you have stateful components, you make multiple copies, each represented with
the usual cylinder. When there is no replication, life is easy because we can update the
information atomically. There is only one place where this information is stored and that
means that whenever we change it, it becomes immediately available for reading it.
After we introduce replication, to help in case one of the copies is lost, we have one
or more backups. From a high level perspective it should look the same. Clients can
interact with the component, change its state, read the updated made by other clients.
Ideally, there should not be any outside impact due to the internal replication. Clients
shouldn’t notice that there are now multiple replicas that need to be kept synchronized.
Also they should not notice when we lose one copy and fail over to the other one. We want
to keep the same interface for the stateful component and just deploy it in a redundant
configuration, which can tolerate the loss of some replicas.
However, imagine there are two copies; then we have two scenarios. You write into one
copy. And then you synchronize with the other before reading. The first step will be that
you send an update. You write something into the system. One copy transitions to the
new state, then the replica follows, and then you read. Since both copies synchronized
with the other, when you read it doesn’t matter where you read from, because they’re all
in the same state. We call this situation strong consistency. We guarantee that no matter
how many replicas you have, the information that you write can be read from any of the
copies. It’s like an ideal scenario, because you write something and then you can actually
read it back. There is a cost: before you can read it, you have to propagate the update to
all replicas. You have to synchronize the update, and this takes time.
The other option does not give such guarantee. You write into one copy, but when
you read, you may get the latest value or not. It can happen that you read before the
synchronization has happened. You read from a stale replica. This is what we call the
weak consistency. There is a risk that if you introduce replication, you not always able
to see the changes that you make immediately. There will be a delay before all replicas
have transitioned to the latest version.
As we have defined availability as: how long are you willing to wait? In some failure
scenarios the replication delay becomes very long, maybe it becomes impossible for the
two copies to synchronize because we have a partition between them. A network parti-
tion just means that the two sides cannot talk to each other, and therefore the synchro-
nization will never happen. At least until the partition is resolved. Even if the replicas
cannot synchronize, we can still write on one side and read from the other. Even if we
retry, we keep reading the old data because it doesn’t get synchronized because of the
partition. As a consequence, we observe an inconsistency between the different replicas.
This just a very high level view into what can happen when we try to replicate stateful
components. This problem occurs no matter whether they use a databases or just keep
their state in memory. As soon as we make another instance and deploy it somewhere,
there will be a second copy of the component state, which will suffer from this issue.
The advantage is that when something fails, you don’t lose the data. You can use the
other copy. In this scenario, you have the main copy and the backup. You have two
copies, so you can survive if one fails. When you lost a copy, you’re back into the non
replicated scenario. So your goal is to avoid that, since you only one more failure away
from losing everything. If something fail, you need to spawn a new replica as soon as
possible. Depending on how big is the amount of state that you have to replicate, it will
take time. Failures strike fast, disk crashes occur rapidly, while creating a new backup -
like if you need to copy several terabytes – will take a significant amount of time.
584

Which kind of replication?
Synchronous
Client
Write(X)
Write(X)
Ack
Ack
Strong Consistency
Asynchronous
Eventual Consistency
Client
Write(X)
Write(X)
Ack
Ack
Weak Consistency
What is the difference between synchronous and asynchronous replication? Here is a
more detailed view about what we have just discussed about strong and weak consistency.
When you are doing synchronous replication, you write some information into one of
the replicas, then you try to send the copy to the other replica. You wait for the replica
to confirm that the update it’s actually been stored, and only then you tell the client
that the write operation has completed. If you do this, you get strong consistency as a
consequence.
Since the client has to wait for all the replicas to agree that the new state is the one
that the client is writing, you have to wait until all replicas agree. This means that writes
takes as long as the slowest replica. If replicas are scattered around the globe in different
data centers, there will be a high latency before they confirm and you have to wait for
the slowest replica to confirm before you acknowledge the client. That’s the price to pay
if you want to guarantee a strong consistency using synchronous replication. Unless all
replicas agree, the write didn’t happen.
If the client is in a hurry, you can speed up the write with the asynchronous replication
alternative. You update one copy. As soon as you send out the update to the other replicas
you already – very optimistically – tell the client that the information has been stored. So
what does this mean? Well, it has been stored in one place, but the synchronization with
the others is still in progress. It can happen that the client receives the acknowledgement,
and for some reason wants to read the value. There is now a race between the client
read and the synchronization. It is possible that the read will not return the latest value
because it reads from replica that is not yet up to date.
We call it asynchronous because the synchronization happens in the background. The
main reason for doing that is to make the writes from the client faster. We give the
opportunity for the replicas to catch up at their own pace. As a consequence, we have
weak consistency since there is a point in time in which replicas do not agree. Since the
585

synchronization is in progress, eventually the replicas will actually get to the new state.
That’s why it’s also known as eventual consistency.
We have seen two simple replication protocols, with different behavior when some-
thing fails. If one waits for all the replicas to reach the new state, the information will
be redundantly stored before the write is confirmed. If it is not possible to duplicate
the data, the write will fail. With the optimistic approach, the synchronization happens
asynchronously. If you lose the only up-to-date copy, before you managed to propagate
it to the replica, you can have information loss after informing the client that the write
was successful. It is possible that as you acknowledge the client you discover that there
is a partition which doesn’t let the write through to the replica. This means that until the
partition is resolved, there will be only one replica which has the latest state. If you lose
it, there is no backup.
586

Eric Brewer
CAP Theorem
Consistency
Availability
Partition
Tolerance
CA
CP
AP
Not Consistent
Not Consistent
Not 
Not 
Available
Available
Not Replicated
Not Replicated
All replicas agree on the latest version of their state
Every request 
routed to a 
non-failing replica 
must result in 
a timely response
The network may
loose arbitrarily 
many messages 
between replicas
A distributed replicated system cannot have
both Strong Consistency and Availability
What we have discussed is also called the CAP theorem. When you have a stateful
component that is replicated across a distributed system, there is a limit that stated by
this theorem in which you cannot have both availability and strong consistency and also
you can be tolerant of partitions.
What does it mean when you have only two out of three properties? As you can see from
the Venn diagram, we have consistency if all of the replicas agree on the latest version of
the state. We have availability if when we send a request we get a timely answer. That’s
the quality we have been talking about today.
What does it mean to survive partitions between the different replicas? This is true if
the network is fully reliable and will never lose any of those synchronization messages.
The CAP theorem states that in the intersection of all of these three properties is not
possible. You have to choose which pair of properties you want to have. Which one do
you prefer? Availability or Consistency? Let’s explore the trade-off.
If you want to design an architecture in which you want to provide consistency, there-
fore, when a partition occur, you will have to sacrifice availability. If you prefer to have
availability, you will need to sacrifice the consistency.
Why do I say that only these two cases are interesting. Because the only kind of system
that is partition tolerant is a centralized system that is not replicated. Only in those
systems you can have both availability and consistency. As soon as you replicate, you
bring in distribution, decentralization, and then you have to choose. A real network will
fail. You will have partitions and therefore your architecture can either be not available
or not consistent.
587

CAP Theorem Proof
read = ?
write(X)
Partition
Partition
X
Y
Y
1. Timeout (Not Available)
2. Y (Not Consistent)
Client
This is probably the only proof that we do in the lecture.
Let’s go back to the previous scenario in which you have a client that is writing and
updating one of the replicas. So what can happen? The client wants to read? Check that
the state transition was successful.
Let’s say that we are not partition tolerant and a partition just happened. So it is not
possible to synchronize the replicas. Because of this problem, what could the read result
be?
Since the client wrote X but it reads Y from an out-of-date replica, the result is not
consistent. Since synchronization was not possible, the result is stale.
What if we wait to give just enough time for the partition to resolve itself and the
synchronization to work. Is the client going to be willing to wait that long? You mean
the client is going to time out and give up reading? But that means that the client will
say that the read was not available. Well, at least the client didn’t read an inconsistent
value. Actually, it didn’t read any value at all.
These are the only two cases when we have a partition. Either immediately read an
obsolete result (available, but inconsistent) or wait forever for the result to arrive, in
practice give up waiting (not available). Also in this case, if the client receives a result, it
will be the consistent one.
588

Eventual Consistency
A
A
B
B
B
B
Partition
Partition
B
C
Inconsistency
Inconsistency
Reconciliation
Reconciliation
C
C
Some applications prioritize
availability at the expense of
consistency.
During the recovery from a
partition, conѺicts between
multiple inconsistent states
need to be resolved manually
or automatically (e.g., last
writer wins).
The data semantics within
some applications may
tolerate this for short periods
of time.
While availability and consistency cannot be guaranteed at the time of the partition, if
we choose availability we can still recover the consistency, eventually, after the partition
has been resolved.
If one waits long enough, the replicas will get synchronized. Eventually, after reboot-
ing your WiFi router you will be online again.
Eventual consistency helps to keep your system available. You just don’t promise that
the latest state will be visible everywhere, but you just say that if you wait long enough
it will get across.
What does this mean? Here we have a replicated system that is in a certain state. Then
we have the partition. The two replicas can no longer talk to each other. Is the system
automatically inconsistent? Not yet. It’s just a partition, but the two copies are still the
same, so we’re still consistent. There’s still a chance that if you fix the network, you
reconnect while you never lost the consistency.
The inconsistency is only introduced when you do the update. If you change the state
only one of the replicas would be updated and the other one will be become obsolete and
it would be inconsistent.
Is the inconsistency visible at that time? Not unless you can reach all the replicas and
compare their state. After solving the partition, you have to decide which replica has the
latest state. How to solve this conflict? It is possible to associate a version counter or a
timestamp and see which has the most recent version.
You may have experienced something similar once you tried to pull changes into your
code and there was a merge conflict. Sometimes the merge works automatically, some-
times you need to resolve it manually.
If the inconsistency is small enough, if the amount of change that you have introduced
589

is not too big, if the time in which some replica was offline is not too long, then it’s pos-
sible to reconcile it in a finite amount of time. If there is a replica cut off for a number of
years, then you have a lot of work ahead if you want to reconcile all the changes accumu-
lated for such a long time. Still, despite the fact that you were inconsistent for awhile,
eventually you can go back to a consistent state. It all depends on how long “eventually“
means for you.
590

Event Sourcing
Keep an explicit representation of the state history
Log all events that led to state transitions so that they can be replayed.
How to reconstruct past states?
How to synchronize replicated states?
Speaking of accumulating changes, this is what the Event Sourcing pattern is about.
This is relevant for highly available, redundant architectures, in which there are repli-
cated stateful components. The idea is not to store only a snapshot of their latest state,
but to store the history of all the changes from which the current state can be obtained.
This helps when you have to deal with conflict resolution, since you don’t just have
two inconsistent states to compare, but you can see how you got there by tracking the
sequence of changes.
Even though your source code files conflict, you can resolve the conflict because de-
velopers have been editing different lines and you can merge them in the right sequence.
Instead of storing the latest value, keep a history of all the changes that you’ve done.
Since events trigger state transitions, we can use the term event to represent a change. If
still remember the previous state before the event occurred, the change that was applied,
then you can obtain the new state.
To store all events, we use a log structure. If you want to take it to the extreme, you
can use a blockchain, but you don’t have to. You just do it in a simple log in which you
have a time stamp and information associated with the change event. If timestamps are
not possible, it is enough to keep an ordering between the events. If you log all the input
that you gave to a stateful and deterministic component, given a known initial state, you
can always replay the log and stop at a certain point to reconstruct the past history of the
state of this particular component.
591

Event Sourcing
Client
Service
Service
Log
PUT
X
Append
X
[X]
GET
X
Replay
[X]
[]
DELETE
Append
D
[X,D]
GET
404
Replay
[X,D]
Track individual changes as opposed to complete snapshots
For example, if we make a change to the state of a certain service and we use event
sourcing, it means that we need to keep track of state changing operations by appending
them to the event log.
A PUT operation, which means that you want to write this value X as the next state
does not simply set the current state to X, but logs the event (the state is now X) into the
log.
The log is represented as an array of all the changes that have occurred so far. It’s easy
to reconstruct what is the current state, because there is only one entry in the log. The
initial state was empty and after the PUT operation we enter into the state X, which is
what the GET (a read operation) returns.
Here comes another change: DELETE should reset the state to the initial one. We’re
not simply going to do it directly. We’re not emptying the state, we’re actually storing the
fact that we have a request to delete the state. And the log grows to include this event
(represented as D).
When we read the current state again, we will replay the log. The state does not become
X anymore because of the delete event. As a consequence, we show to the outside that
after you delete something then the information is gone. However, in the log it is still
present. If you wanted to undo the effect of some of the changes, you can still time travel
along the past history.
592

Event Sourcing
Performance Tradeoff
• Append-only log of immutable events (fast write)
• Replay log to compute the latest state snapshot from known
initial state (slow read)
• Cheaper to store (or transfer) a log of state transitions vs. the
complete history of state snapshots
• Cheaper to store one snapshot than the log of all state
transitions to reproduce it
When you use event sourcing to implement state storage, writes are fast because it’s
enough to append them to the log, while reads can be slow because you need to replay
the whole log, unless you keep a cached version of the latest state.
The amount of data that you have to transfer is the critical bottleneck when you in-
troduce replication in your stateful components. Event sourcing can help to build an
eventually consistent, replicated solution in which you store the component state across
multiple copies. As opposed to transfer entire and potentially large snapshots every time
a few bits change, it is more efficient to send a stream of changes to keep the copies con-
sistent.
In general, event sourcing makes sense when the cost of logging individual state changes
is smaller than storing a snapshot of the whole dataset. Consider introducing it if the data
is large and there are few small changes. It may be inefficient if the data is small and there
are lots of changes, so the log size grows above the size of just taking a snapshot.
How long is the history of state transitions? If you keep your stateful component run-
ning for a long time with lots of clients invoking it, then you have a long history and it
will become cheaper just to store the current state and read it as opposed to replaying all
the log to compute the latest state.
There will be a point that even with event sourcing, you will take anyway a snapshot
and only keep the changes from the snapshots onwards. A good time to do so is when all
replicas are in sync.
By flattening a large log of state changes into a snapshot of the current state you can
save space but loose the ability to time travel.
593

Event Sourcing
Advantages
• Record audit trail (what happened when, who did what)
• Synchronization (replay from the last known event to catch up)
• ConѺict Resolution (make sure events are played in the same
order)
• Rollback (replay up to a given event)
• Reproduce bugs (replay production log on a test/upgraded
system)
In addition to the synchronization between replicas, and the fine-grained conflict res-
olution, event sourcing has more advantages.
You can establish some kind of a legal audit trail. You track not only which are all the
changes that happened, and when they took place, but also who is responsible for them.
This can be very useful also from a business point of view to store all the history. Also
implementing undo is just a matter of replaying the log and stopping before reaching the
final event.
It’s also great for testing and reproducing bugs. The log is a valuable source of input
for your tests. Replay the log and check if different versions of the component end up in
the same state. After you make a new release of the system, you can spot compatibility
issues or regressions by simply applying the event log and comparing the obtained state.
594

Event Sourcing
Disadvantages
• Storage size (logs tend to grow unbounded, take a snapshot
from time to time)
• Serializability (what's the right order of the events?)
• Consistency (what if events get lost?)
• Evolvability (how to version events in the log? how to migrate
old logs so that they can be played with newer clients?)
Disadvantage include the problem of an unbounded growth of the event log. The more
state transitions happen, the more events need to be listed in the log. If you want to join
the mining effort, you get the privilege of downloading and storing your own copy of all
bitcoin transactions since the origin block.
You have to be careful about the order in which you’re logging things, especially if you
have multiple logs in a decentralized architecture. It’s not trivial to know what is the
right sequence of events unless there is a centralized log or you are really careful with
the clock synchronization.
It is also critical not to lose any of the events. Given an incomplete log, there is no
guarantee that replaying it will result in the same state. That’s why blockchain miners
dedicate so much energy to continously re-checking all the hashes to make sure that the
log didn’t get tampered with.
What about if the format of the events changes? Can a new release of the component
still play back events logged from an older version of the component? There is an issue
of migrating the logs that due to their historical nature spanning across multiple releases
of the component is more complex as opposed to just migrating the snapshots. We will
see more about software evolution in the next lecture.
595

References
• Werner Vogels, 
, ACM Queue, Dec 2008
• Eric Brewer, CAP Twelve Years Later: How the "Rules" Have Changed, Computer, 45(2):23-29,
Feb. 2012
• Niall Richard Murphy, Betsy Beyer, Chris Jones, Jennifer Petoff (Eds.), 
: How Google Runs Production Systems, O'Reilly, April 2016, ISBN 978-1491929124
• Michael T. Nygard, Release It! Design and Deploy Production-Ready Software, 2018, ISBN 978-
1-68050-239-8
• Ali Basiri, Nora Jones, Aaron Blohowiak, Lorin Hochstein, Casey Rosenthal, Chaos Engineering,
O'Reilly, August 2017, ISBN 9781491988459
• Guy Pardon, Cesare Pautasso, Olaf Zimmermann: 
. In: IEEE Cloud Computing, 5(1): 49-59, 2018
•
Eventually Consistent
Site Reliability
Engineering
Consistent Disaster Recovery for
Microservices: the BAC Theorem
Is it down right now?
 
 
 
 
 
 
 
 
596

Software Architecture
Flexibility and
Microservices
Prof. Cesare Pautasso
cesare.pautasso@usi.ch
12
http://www.pautasso.info/
 
 
Contents
• API Evolution: To break or not to break
• Layers of defense against change
• Extensibility and Plugins
• Microservices
• Feature Toggles
• Splitting the Monolith
597

This lecture is organized following some of the quality attributes a software architec-
ture can have. We started from many design-time qualities. We’ve seen deployability as
we made the transition to operations and runtime. Then we talked about scalability and
availability. Today we close the cycle as we look at how easy it is to change a software
architecture. This goes under the overall umbrella term of flexibility.
We will see that the construct that is meant to make it easy to change a software archi-
tecture is called: “microservice”.
This is the latest addition to our zoo of architectural concepts, starting from the good
old days of component based software engineering, followed by the dawn of service-
oriented architectures. As we are about to enter microservices land – before we define
what microservices are by combining many ideas and concepts that we have already il-
lustrated in the previous lectures – I want to have a more general discussion on what it
means to change software, focusing on the most difficult part to change: the interfaces.
How do we change an API? We will see the most critical decision will be about whether
to break or not to break clients. From the client point of view, it becomes critical to
isolate clients and protect them from the impact of changes using layering. Layers give
you a powerful design mechanism to contain the impact of changes. For example, every
component is encapsulated by an interface. The interface can be seen as a layer mapping
between the external data representation and the internal data representation. We will
also take a look at how it is possible to design an open and extensible architecture with
the use of plugins.
As we finally introduce microservices, we will see how it is possible to develop highly
flexible architectures. In particular, we will discuss the feature toggle mechanism and
a procedure to gradually migrate monolithic architectures and split them into microser-
vices.
598

How do we evolve interfaces? You can see here a physical example which shows you
how hardware technology has evolved to feature very different kinds of ports available
on different type of devices. We can spot a trend: over the years the set of ports has been
shrinking as well as their diversity has been significantly reduced. One driver behind this
change has been this idea that laptops should become thinner.
Another observation is that we can abstract and generalize the type of interface. Each
USB C connector can play the role of any other port of the previous interface, and can
also be converted back to the previous interface by purchasing the corresponding don-
gle. What’s most surprising is that you can also power the machine through the same
connector which is used also to exchange data. This was pioneered on mobile phones,
where the available surface which can be dedicated to ports is however much more lim-
ited.
This interface evolution occurred through decades of technological improvement. Be-
yond reducing thickness, courage and simplification, what are the drivers that caused it?
Is it a sign of progress that you actually remove features and simplify the interface so
much that you can design a future where only one type of plug will be sufficient?
While life would be much simpler if one could cleanly break away from past decisions.
Of course such disruptive interface evolution will have a big impact if you try to connect
your shiny new laptop with older devices. Your decision to break backwards compatibility
can sometime lead to creating new business opportunities, like selling adapters as a so-
lution to the problem you created. With the additional benefit that if something breaks,
you can always blame the adapter and not the incompatible hardware.
How are the users of this type of system impacted? Will they just follow along and
silently live with the consequences of these upgrades?
599

Who has the power to force change across an interface boundary? This requires a closer
look at the relationship between the two sides of an interface. Do you have a collabo-
ration type of relationship where both parties are trying to build different components
that have to match based on previously negotiated agreements? Or as represented in the
picture: do you have an imbalanced relationship where one side is in control while the
captive side will need to deal with the consequences of whatever decisions you take?
Only one chance...
...to get the design right:
• File/Document Format
• Database Schema
• Wire/Message Representation Format
• Application Programming Interface
• Programming Language (and Standard Library)
• External Data Model:
Once the API becomes public, it is out of your hands
and it will be very expensive to change!
Why is it so critical to get the design of an interface right? Because you only have one
chance to get it right. Once you have designed your API, then other developers will build
applications on top of it. There will be hopefully many programs written on top of the
platform that you provide for them. If the platform is not stable or poorly designed, the
life of many developers will be a misery, unless they simply refuse to get near it.
This is the same position in which you find yourself if you invent a new programming
language. I am sure some of you in your future career will invent a programming language
or a domain-specific language. As you release the language specification, you release the
compiler and maybe also the standard library that usually goes with it, it will be out of
your hands. As other developers will try to write code in your language, they will notice
if you make a change to the language as they probably will have to throw away all the
code they have been writing and start from scratch.
We also have this type of situation when we design an external data representation:
for example, the way data is represented as it is exchanged through an interface; but also
documents written in a certain format; or data persisted in a database following a given
schema. In general, we decide which would be the syntax, structure and semantics of the
messages that we exchange between the outside world and our system.
600

These decisions are going to affect all the external entities that will ever interact with
our system. All components that will ever have to read or write data that we can under-
stand. And if all of a sudden you make a change to the format, every other component
that used to be able to exchange data with us will be affected.
So once you release the API, then as a designer you are no longer fully in control be-
cause you share it with other people who will depend on it.
While before the first release, you could iterate quickly and do whatever modification
and improvement you wanted, once it’s released this will become a much slower process
and also much more expensive in case you make a mistake, it will be super expensive or
even impossible to fix it. If you made a small mistake in a programming language which
eventually became highly used, people will still be laughing about it 25 years later.
API Evolution
• Once in the API, keep it there forever
• Never add something you do not intend to keep forever
• Easier to add than to remove
• Make sure you explicit version all changes (including
documentation) pointing out incompatibilities
• Publish preview (0.x) versions of an API before freezing it to get
early feedback from clients
• Rename a component if its API has changed too much so you
can start the redesign from scratch without breaking old clients
• Keep changes backwards and forwards compatible
Is there a simple rule we can follow to guide us with the evolution of our API? Once a
feature is in the API, the simplest thing to avoid breaking your clients is to keep it in the
API as it is, unchanged, forever.
If you are in doubt, leave it out. Never add something you do not intend to keep in
the API forever. It’s much easier to add something later rather than to remove it, or to
change it.
Evolution by addition has visible consequences in many APIs, languages or data for-
mats which have been around for a long time. Every change which requires to remove
a feature is hard. APIs hardly ever get simpler, cleaner. APIs are rarely refactored. For
example, if you discover a better name for a feature, you can easily introduce the new
name, but you may only deprecate the old name as removing it completely will break
clients. Instead, after each iteration, each new release, APIs just keep growing as they
accumulate new features. Since nothing should be removed, the size and complexity of
old APIs tends to grow.
601

Another important point is that there needs to be a clear specification of the interface.
If there is a place where you have to use version control is exactly in this documentation
of the API. This will help clients realize whether the new version becomes incompatible.
Every change that you introduce, you need to predict if this change breaks or not breaks
clients depending on the previous version of the interface. There are some conventions
that you can follow, for example using preview or experimental releases. So this means
that you are going to tell your clients that this version is not stable. The API is not yet
frozen. Only later the frozen API can be expected not to change anymore.
This helps to get feedback from interested clients: Is this feature useful? How can
we improve it? But you still warn people not to seriously depend on it because it may
still change. This is how you can avoid this one shot release that is very hard to get
right because you only have one chance. To converge towards the perfect API design you
need multiple iterations with clients that are willing and brave enough to depend on an
interface which is not yet stable. It is only fair to warn clients that the API is not finished
and may end up in a completely different shape.
If any API has evolved so much that you cannot recognize it anymore due to the amount
of new and deprecated features that have encrusted over the years, it is better at a certain
point to mark it as obsolete and start from scratch. Give it – this is very important – a new
name so that clients do not confuse it with the previous one and somehow still expect it
to be compatible. Especially if it’s going to be totally different, you might as well change
its name (not only the version identifier) as you make drastic changes to the interface.
602

1.0
1.0
2.0
2.0
1.0
2.0
backwards
1.0
2.0
forwards
API
Client
API Compatibility
• Backwards compatibility:
new version of API
compatible with old client
• Forwards compatibility:
old version of API
compatible with new client
What is important is to try to minimize breakage and to keep changes backwards and
forwards compatible. Let’s see what this means.
Compatibility requires that the interfaces match. Clients require something that is
provided and there is a perfect fit so you don’t need an adapter between the two inter-
faces.
Backwards and forwards compatibility use a spatial metaphor to describe how compat-
ibility is affected by the interface evolution.
Each side of an interface can evolve independently. Here we can see that there is a
client that depends on certain API and they’re both in the same version. And then we see
that this version evolves from 1.0 to version 2.0, so this is also a situation in which we
can expect the two sides to be compatible.
However, there is a point in which, since we’re talking about two different components
it is possible to think that they evolve at their own speed. So in this case we have a
scenario in which we have changed the API, we have changed the interface on which the
component depends, but the component itself hasn’t been touched. We have the old
client talking to the new API. From the perspective of the API, we’re looking backwards
in time. We’re trying to receive an incoming message that was sent from the past. If an
older client works with a newer API it means that the change that you introduce in the
API is backwards compatible.
How can you achieve such backwards compatible changes? The 2.0 version of the API
is a super set of the 1.0 version. That means that when you plug the old client into it, the
old API features they depend on are still there. So from the perspective of the old client,
they will actually see the interface as being unchanged and will just ignore all the new
features. That’s easy to achieve if you are not touching what is already there.
Forwards compatibility refers to the opposite scenario in which the client is from the
future and the API is from the past. So this is a bit more complicated to achieve because
the client could expect to depend on features that the API doesn’t provide yet. The ques-
tion for you to think about is: how can you make an interface that not only works with
today’s clients, but also can still work with tomorrow’s clients?
603

This was the definition of the difference between forwards and backwards compati-
bility. The term is defined from the perspective of the interface. If you go backwards, it
means that the client is older than the interface, and if you go forward, the client is newer
than the interface.
https://semver.org/
Semantic Versioning
MAJOR
incompatible
API changes
.
MINOR
new
functionality
but backwards-
compatible
.
PATCH
backwards-
compatible
bug/security
ѹxes
Additional versioning metadata:
• Build counter
• Release candidates
Once a version has been released, its content is immutable.
Any changes must be released as a new version.
One way to identify the amount and the type of change that we have introduced in a
certain element of the architecture is to use semantic versioning.
This convention uses three numbers: Major, Minor, and Patch. As opposed to floating
point numbers, where there is only one dot, in computer science we use version numbers
with two dots to identify how things evolve.
By convention when you increase the Patch number (on the right) this represents a
small, internal change which can be assumed to be backwards compatible. Bumping up
the patch counter typically occurs for fixes rather than new features. You might have a
security fix. You might have a bug fix. Apart from the improvement, the system is feature
equivalent as before.
If you make a minor change, this means that the system is growing with new features
but still keeps the backwards compatibility. As long as the first number doesn’t change,
you can expect some degree of backwards compatibility as well as some improvement.
When you touch the major component of the version identifier, then you are signaling
that there has been some incompatibility. This is a way to warn clients that they will not
work without putting some effort to bring them up to speed.
Sometimes you also include additional numbers: for example a build counter. While
setting the major.minor.patch is something that you have to decide manually based on
the impact of the change that you introduce, you can adjust the build counter automati-
cally in your continuous integration pipeline. This is a very simple counter that is incre-
604

mented every time you run the pipeline, and this can be appended to the version identifier
so that every build results in a unique identifier.
Sometimes as you are about to make a big jump, you know, while making a major re-
lease, you want to iterate quickly over it to polish and iron out the last showstoppers. To
do so, you can make release candidates. This is version 1.0RC1. It is not yet stable, it is
converging but it is meant to be tested and not for production. These RC version counter
will eventually disappear once the version stabilizes and the release is accepted.
Version identifiers help to enforce a very important general rule: immutable releases.
Anything produced from your automated build pipeline should be immutable and ver-
sioned. That means that this particular version that has a unique identifier corresponds
to a known entity built using a certain reproducible process. Every version identifier cor-
responds to an explicitly tagged source that you have in your version control system. If
you make any change to the source, you should run the build again and generate a new
version. No matter what you do, no matter how small the change. That’s why we have
a build counter anyway. Even if you don’t touch the other version counters, that build
will increase and that will help you to guarantee this property, which helps to track the
lineage of your release artifacts. Also, once you have generated the artifacts tagged with
a certain version, you should never touch those manually. You should always recompile,
regenerate, retest them through the automatic build pipeline.
605

Changes and the Build Pipeline
Does
it build?
Rebuild
needed?
yes
no
yes
Does it
pass the
tests?
no
Change
it
yes binary
compatibility
Does it
run?
no
broken
binary
compatibility
broken
source
compatibility
no
broken
semantic
compatibility
source
compatibility
semantic
compatibility
Speaking about the build pipeline, let’s see how different types of changes impact com-
patibility at different levels. Whenever we make a change we have to ask ourselves: Are
the dependencies going to be affected by this change? In which way?
For example, if you change your library, do you have to rebuild your clients or not? If
not, if the clients can run without rebuilding them, we just update the library and the
clients just need to be rebooted to load the new version. If this works, then you have
achieved binary compatibility. This is great because not only your clients are not affected,
you don’t have to recompile them, they still work even though you have changed their
dependencies.
If this is not the case, you have broken binary compatibility. You will need to rebuild
the clients. Clients need to be recompiled every time their dependencies change. After
you have checked out the new version of the library, does the build still work? If not,
you have also broken source compatibility: your client will need to be rewritten, to be
extended, to be modified so that it can work with the updated dependency.
However, let’s be optimistic: after pulling the updated dependencies, you rebuild and
it works. This means you have source compatibility of the change.
After you compile, of course you should re-run the tests. This is true also when you
have binary compatibility. After you update the library. You restart it, the new library
links successfully, but will it pass the tests? If you pass the test with the change in the
dependency then you have also a stronger level of compatibility. You have not broken
the compatibility of the semantics of the implementation between the two sides.
If the tests do not pass, then even if you achieved source or binary compatibility, af-
ter all, your change has broken the actual semantics which the client expected from its
dependencies.
606

You can use this flowchart to define this concept of binary vs. source or semantic com-
patibility. The three concepts fit with the major.minor.patch versioning scheme. Patch-
ing could be in some cases binary compatible, minor changes could still be kept source
compatible, while if you have a major change you can expect also the semantic compat-
ibility to be broken.
Version Identiﬁer
1.1.1
Client
Service
1.0.0
1.0.3
1.0.1
1.0.0
1.0.0
• Version Interface Descriptions
• Version Message Payloads
• Version Component Implementations
• Version Container Images and Addresses
• Version Database Schemas
After introducing version identifiers, let’s see which elements of the architecture can
be versioned. If you take it to the extreme, every architectural element can carry a version
identifier.
For sure interface descriptions should be versioned. If there is an API that you depend
on, this API is specified and there is a version identifier associated with it.
We can also version component implementations: the implementation behind the in-
terface can have his own version. Also the client component on the other side has his own
version. When a client looks up an interface, the directory will bind it to a certain imple-
mentation. Version identifiers associated with interfaces and implementations can be
used by the client to constrain or identify which version does it depend on. When clients
look up dependencies, they can look for the latest version, or a specific version within a
certain range.
Once client interact with implementations, then the messages themselves, the mes-
sages that are exchanged can also carry their own version identifier. These are much
more fine-grained than the whole interface. They can identify the version of the inter-
face endpoint to which the message is directed. The message version identifier can also
be used to check that it matches the version of the interface.
A more coarse-grained version identifier refers to the container or the image deployed
607

in a container. Images aggregates many components and interfaces which would need to
be labeled so that they can be versioned as a whole.
When you start a given image on a container or runtime environment (whose config-
uration should be versioned), you will run a service on a certain server. The server also
has network identifiers, for example, their IP address. IP addresses can also include ver-
sion identifiers, for example if multiple versions of a service are active at the same time.
Messages can be routed to their endpoints based on the version identifier embedded in
the corresponding IP addresses.
Finally, we also version the database schema used to store the state of stateful com-
ponents. Why is versioning the schema important? Because if we change the structure
of the data, we might need to do a migration from a certain version of the schema to an-
other one. Therefore it’s important to know what changed and the fact that we are going
to connect our stateful component to a database which uses a different schema.
Ultimately, the goal of using version identifiers is to be able to have an easy, lightweight
mechanism to spot differences. You can read an entire API description, read a schema
specification and compare it against an older one. This is quite a lot of information and
requires effort to spot the differences, not only by feeding them through an automated
diff checker to detect which parts have changed but also what is the importance or the
impact of such changes.
Or you can just read the metadata: there is an attribute that identifies the version and if
the numbers change, you assume that they are different and depending on which number
has changed then you will efficiently infer whether it’s a minor or major change.
608

Two in Production
1.0
2.0
Client
1.0
API
0%
100%
2.0
1.0
2.0
API Gateway
transition time
Gradually transition clients to the new API version without
breaking old ones
Once all clients have been migrated the old version can be retired
How can we use this notion of having different versions of interfaces and dependencies
to support the gradual evolution of our system?
At some point when it grows beyond a certain size, it is no longer possible to apply
a change in lockstep everywhere. If you have a very small system, you have the luxury
to be able to decide that you make a change and everything changes atomically. As the
system grows inevitably some elements fall behind. You will make a change on the server
side and the client remains the old one for a while. Therefore it helps to be able to avoid
breaking all clients and support their gradual transition over time.
If this transition takes time, it means that for a while you will have both old and new
clients talking to your service, with your interface. How to keep them both compatible as
cheaply as possible? Does this require any additional coding effort? It’s just a matter of
running side by side the old implementation for the old clients and the new implemen-
tation for the new clients.
You can directly connect the clients to the corresponding implementation. Or you can
try to make it transparent and have a single endpoint. So you basically do not touch the
address and then you use a special kind of load balancer that is going to route the requests
depending on the version of the client. Each request message carries a version identifier
which will help the load balancer to send it to the corresponding new or old implementa-
tion. It’s important to put the version in the messages, because once you get a message
coming from client 1.0, this component can forward it to the correct implementation.
If you have this setup to keep track of the traffic, you can keep track of how many
clients are there which have been updated and they already use the new version as well
as how many clients are still behind and they need to run against the old version. At the
beginning, most of your clients are routed to the old interface. Over time you will notice
more and more clients are shifting to the latest version.
This will also give you a chance to gain confidence that the new version is working.
Most of your clients are still depending on the old which may be out of date. Maybe it
doesn’t have the features that you need, but at least you know that it’s stable and it works.
609

And then you have some of the clients – the pioneers – going towards the future, they
can work with the new system. As you gain more confidence and you put additional effort
into migrating the clients then you will see that at the end you only have a few laggards,
a few clients that are stuck in the old ways and eventually they disappear.
Once this happens, you notice that there is no more traffic going to the old version
and you can switch it off and you just keep the new one. This pattern is called “two in
production”. It allows you to buy the necessary time to perform this transition because
there is a cost to migrate all the clients at once. Sometimes other people have to pay the
price, but they still need to be willing to do it. And this way you can support them during
this transition. The alternative would be to switch off the old version of the API imme-
diately. Let the old clients break and while they are upgrading or unless they upgrade
nothing will work for their site.
If you use two in production this doesn’t happen because the old still works. This
means that you have to somehow give them an incentive to switch because of course if
you keep supporting the old version forever then there is no incentive for them to do the
migration.
Two in production is just a simple example. In reality in some architectures you have
three in production: 1.0 will be the super old version that very few still use, but it still
necessary to run it. 2.0 is the majority, that’s the stable version; the 3.0. is the experi-
mental one about which you are starting to get feedback from the clients. You will soon
want to introduce 4.0, but it’s too expensive to run four versions side by side and you will
only do it when the 1.0 disappears. If you have three in production, you have a way to
handle very old clients. The majority is in the center and then you have also a chance to
test out new features oriented towards the future. So you have the past, the present and
the future represented in your architecture. By keeping track of the traffic, this solution
also helps you to know when you can discontinue the support for the past.
610

API Sunrise and Sunset
Manage expectations and avoid surprises:
warn clients that APIs they depend on are about to change and
inform them about which features may change or disappear
Experimental
Preview
Still likely to change, feedback welcome, help
give clients an early start
Deprecation
Will no longer be supported in future releases
Limited
Lifetime
Will be supported during a limited time only
Eternal
Lifetime
Will you be able to keep the promise?
Regarding change and the life cycle of dependencies, it’s important to manage the ex-
pectations of your clients. You want to give them a warning so that the APIs they depend
on may change. And sometimes they even disappear. To visualize the beginning and the
end of the life cycle, we can use the sunrise and sunset metaphors.
While change is positive because you grow the features, you make it more powerful,
you improve the performance, you improve the availability, correct bugs: everything gets
better and better all the time. But sometimes you also have to remove features that peo-
ple need. Because it’s too costly to maintain them. Your clients need to be warned that
this will happen. To do so, we use these terms associated with different maturity levels
of the interfaces and different kind of promises that we make towards the clients.
If you release an experimental preview, this means that your goal is to involve clients in
the design of the API and still change it based on their feedback. There’s a point in which
you say, I think this will work. This is a good enough design. Let me see whether the
clients agree so you open it up, you release it as an experimental preview. Some curious
clients are willing to try it and give you feedback. Maybe you need to pay them for this.
But in general their incentive is to get an early start with moving ahead with their side
so that later when you make the final stable release, they already are well on their way to
building something that can make use of it.
Of course, if during the experiment with the preview you discover that this will never
work and you have to rewrite and redesign the whole thing, then of course they will be
severely affected because whatever they invest into their client prototypes will also have
to be thrown away. The goal is to minimize the chances this happens and have a good
starting point in the first place.
That’s how you start right at the beginning with every API. Every interface should go
through such early feedback phase before you actually freeze it. Once you make a 1.0. it’s
frozen for good. But if you are at zero point something (0.99) then you know that you’re
in experimental preview mode.
611

On the other side, when you want to start to remove things, you should never make
something disappear from one day to the next. You should first tag features that you
plan not to support in the future as deprecated. If you write code there are standard an-
notations which you can use not only for the documentation, but even so that warnings
will show up in the compiler logs whenever you are calling something that has been dep-
recated. Of course it’s only a warning and can be ignored but you cannot say you have not
been warned if eventually it disappears and your build breaks. You should have already
stopped depending on it.
After this phase in which a feature that was stable becomes deprecated, then you can
also add additional pressure to add: while it is ”only“ deprecated now, it will disappear
within one year. This way, clients know they have one year of time to find an alternative
to rely on due to the limited lifetime during which they can use what we offer.
End of Life is an important aspect of technology. Due to the constant churn of technol-
ogy evolution, there are always better ways that are found, new discoveries, innovations,
and therefore obsolete stuff to be thrown away in the wastebin of technology history.
This evolution is also part of the business model: I sell you something that only works
for three months, and then if you want to keep using it, you have to buy a new item that
will be released right on time as the first one is expiring.
When they invented light bulbs, they made them so good that the light bulbs would
never burn out. Then they realized they would soon run out of customers. So they actu-
ally designed the light bulbs to burn out after some amount of time, so that people will
have to buy new ones to replace them. You can either know in advance when something
will expire or you can have a surprise that makes your life interesting. Why did it have to
really run out today?
Now, if you try to get a competitive edge, you can also try to release an interface and
promise that this will be supported forever. This should make your API more attractive.
If clients need to build a system which will depend on you, for them is much better if a
dependency will be stable and supported forever, as opposed to having uncertain infor-
mation about its lifetime or just a limited guarantee: we will support it for a couple of
years, but then it will most probably change in a non-backwards compatible way. Clients
will have to renew their license or sometime bribe us so that we can continue to support
the old version for an even longer time. Or they look for the cheapest upgrade path, if
they are unable to find a compatible replacement, in the worst case, clients will need to
adapt and rewrite their code to fit with the new version.
612

To break or not to break
• Sometimes it is unavoidable
• Cost(Keep Service Compatible) > Cost(Fix Broken Clients)
• Warn clients in advance 
• Provide migration path (to alternative provider) or fallback
solutions (shims, polyѹlls)
As you consider retiring an APIs – depending on the languages that you use to de-
scribe it – you can annotate its feature with these terms: deprecation or sunset. These
are mechanisms within the way that you describe interfaces (deprecation) or the way that
you interact with them (sunset headers) to annotate them with this ”expiration date” or
”guaranteed to work until” metadata. This particular feature is deprecated or this par-
ticular endpoint will disappear within a certain time. Even when we sunset or deprecate
APIs, callers will still come knocking!
The question is: why do these breaking changes happen? Sometimes it’s something
that is unavoidable. Maybe it’s not even your fault, like when the break is caused by a
dependency down the stack that is outside of your control. You wish you could maintain
the compatibility and support this interface for longer, but then you discover that some-
thing else stops working and then as a consequence your component stops working in
the way it used to and your client is broken.
There is a simple comparison you can make. Is the cost of keeping the service com-
patible and supporting all clients greater than the cost of fixing the clients that will be
broken by stopping the compatibility? The cost of keeping the service compatible is paid
by the service provider. Whereas the bill for fixing the broken clients is paid by the clients
themselves.
What kind of relationship have you established with your customers? If you pay the
price for maintaining compatibility, can you offset this additional cost to the customers
themselves? If you are unable to maintain compatibility and you break them, then they
will have to deal with the consequences. It may be cheaper for clients to avoid breaking
changes by paying the service provider to maintain the service compatible as opposed to
fix the clients.
Also, if you’re going to break it, don’t do it in the middle of the night when nobody
613

is watching. Break it with an advance warning. This is an old example, it’s called the
“OAuthpocalypse“: there was a Twitter API which changed on a fundamental level the
way that users authenticated. And this, as a result, ended up breaking all of the apps and
all of the clients trying to access the Twitter API. To warn developers (and users) they set
up a website prominently featuring a big countdown clock. The message was clear: you
have nine weeks left to update your application or to pressure its developers to release
an update for it.
Despite these advanced warning, the day they flipped the switch, there were still many
clients that stopped working. This is something related, in general, with how people react
to deadlines.
Too bad they could not simply redirect the old clients to the new endpoints. Together
with a warning, you should also provide a migration path. Or some fall back solution so
that the client can adapt and avoid the worst possible outcome.
614

Who should keep it compatible?
1.0
Client
1.0
Service
We’ve seen when you break compatibility, you introduce a mismatch between inter-
faces. A possible solution is an adapter, if we assume that there is an equivalent alterna-
tive way to do the same thing.
If a feature is removed from the new interface, the feature is gone and there is very
little an adapter can do. Unless the missing feature gets re-implemented from scratch
inside the adapter.
Let’s take a look at the interplay between adaptation and evolution. This can also help
us to explain more in detail who needs to pay the price to deal with the consequences of
breaking changes.
Here we start – nice and easy – from where everything is compatible.
615

Who should keep it compatible?
1.0
Client
1.0
Service
2.0
Service
Two in Production: Client still using old version
Now we introduce a change. There is a new version of the interface. Although there
is a new version, we decided that we’re not going to stop the old one, because otherwise
the old client will break.
To avoid that we are using two in production: two versions of the service are running
side by side and the client is still depending on the old one. With two in production, the
advantage is that the investment in the old version of the services is already amortized
and we keep running it as it originally was. Basically, we don’t touch it. Next there is a
new version of the service that has just been released. Freshly developed, maybe after a
complete rewrite from scratch, or just some changes which unfortunately break compat-
ibility.
616

Who should keep it compatible?
1.0
Client
1.0
Service
2.0
1.0->2.0
Provider keeps supporting old clients by running the adapter
As it is running on the side, conceptually, we could use an adapter to replace the old
implementation and switch to the new implementation while keeping the interface com-
patible with the old version, which is the one required by the old clients.
For example, such adapter may need to fill in default values for data fields which were
added in the new version, or rename a few fields from the old (and deprecated) names to
the new ones, which offer an equivalent semantics.
In general, if it’s possible to implement it using an adapter, the old clients will not no-
tice it as they still think that they are using the old version. Behind the old interface, the
adapter just forwards their calls to the new version by transforming them as necessary.
Why would you do this as opposed to keeping two in production? What if your service
is stateful? If you run both versions in parallel, their state will drift apart. The state of the
old service is isolated from the state of the new version: if the client makes a change to
the data through the old API, this data will not be visible from the new interface, because
it’s a completely separate system.
If you can merge the two versions (or at least their state) in this way, the old client and
the new client will be able to see the same state of the service, each from a compatible
interface. If you can manage to have a single database where you keep the state for both
interfaces, then, for example, a new client can write some information into the system
through the new interface and then you can read the same information from the old client
using the original version of the interface.
Still, the cost of running this adapter is something that the service provider is paying
for at the moment. Because we are still offering two versions of the interface, we have just
merged the underlying implementation and state, which are all running on the service
provider side.
617

Who should keep it compatible?
1.0
Client
1.0
Service
2.0
1.0->2.0
1.0
Client
1.0->2.0
Old clients need to run the adapter to use the new service version
We have reached the point in which the service provider is no longer willing to pay
the price of maintaining the old version of the interface and running the corresponding
adapter. While there is a way to do the adaptation, the service is no longer motivated
or interested to perform it for its old clients, whose population may have been steadily
disappearing. The client should actually upgrade and use the new version, but what if
the client cannot do that. What if the client is stuck in the stone age?
It may happen that clients are unable to upgrade due to their obsolete hardware/oper-
ating system version. Or simply their source code was lost or there are no developers left
on the team capable of correctly modifying their code.
In this case, it still possible to keep the adaptation running but just push it onto the
client.
From the service provider perspective, only the version 2.0 is now in production. Old
clients have to install and run a local adapter in between so that their old client can access
the new service interface through the adapter deployed on the client side.
This is what happens when you hear about installing a piece of software called a “shim”
or a “polyfill”: you have an old client, you augment it so that it can use the new service,
the new version of the API.
618

Layers
How to contain the impact of change?
• Change of Dependencies
• Change of Technology Platforms
• Change of External Service Provider APIs
• Change of Data Representation Formats
What if we expect changes in our dependencies? Something that we depend on will
change. We can have changes in the dependencies underlying the system, the techno-
logical platforms on which we build it tend to slowly evolve underneath. Also there can
be sudden changes from service provider APIs. Likewise, data sources move, data formats
shift and we have to avoid coupling too tightly the data that we use within our system
with the external one.
How can we control and limit the impact of changes? Add a layer of indirection.
619

Layers
Changes of one layer only affect the neighbours
Cross-layer changes may impact all layers
The simplest idea to isolate the impact of changes is to use this concept of architectural
layers.
Consider this layered architecture. What if we change the bottom layer? Which layer is
affected? Only the one next to it. If I tell you the changes are happening at the top layer,
then you probably see that the layer that is affected is only the one right underneath. If
you change something in the middle, this will probably affect both sides: these layers are
directly in contact with the one that is affected. It may also be possible to constrain the
impact only on one side, depending on the dependency relationship direction between
the components of each layer.
There can be layers where the impact of changes goes only in one direction. For exam-
ple, from the bottom to the top. If you change something on top, you don’t affect what is
underneath. This helps to constrain the impact of changes. But if we change fundamen-
tal layers at the bottom, also in this case if layers are done right, the change will impact
only the immediate layer on top and should not ripple further up the stack.
There are some changes however, which unfortunately happen to impact all the layers
of your architecture. Depending on the strategy you followed to introduce layering, it will
be difficult to anticipate all possible changes and just keep them isolated within certain
layers. So-called cross-cutting changes will impact all layers of the architecture.
While usually you can change the implementation, for example, optimize its perfor-
mance, without touching the interface so that clients above are not affected, what if you
need to add support for a new feature?
This new feature will require an extension of the implementation. It will also need to be
exposed through the interface. Likewise, the component above will need to be extended
so that it can make use of the new feature. Ultimately this will ripple all the way up to the
user interface so that users can actually see and interact with the new feature. Also, the
change will sink through the layers all the way to the database because the new feature
needs information from the user and this information has to be stored somewhere.
For such an extension, for such a change, your layering is completely useless because
you have to start from the top of the system, add a new format, the new UI widget, extend
620

the API, extend the business logic controllers, extend the model, make the new informa-
tion persistent, all the way to the bottom.
If you’re planning to grow the system in this way, you chose the wrong layering strat-
egy. Maybe your goal should have been like this to slice the architecture in vertical layers.
This way, your changes, your new features are encapsulated and the impact of adding
them stays within one of those vertical sections and the others are minimally affected.
Depending on the level of granularity, depending on the goal of your decomposition
strategy, you should not feel constrained to use only one kind of layering. If you zoom
out far enough layers will appear to be oriented vertically, while if you zoom into each of
the vertical slices, horizontal layers will appear.
The main concept remains valid: use layers to surround components likely to change
to control the impact of such changes on the rest of the system.
621

Layers
Application
Operating System
Hardware
Application
API
Operating System
Device Driver
Hardware
Application
API
Language Virtual
Machine
Container
Operating System
Virtual Machine
Hardware
Let’s take a look at some examples. The classical layers: application, operating system,
hardware. The OS layer isolates the application from as well as it abstracts the hardware
on which the application is running.
Within these layers we can add more intermediate layers. We can add the operating
system API which hides the implementation from the application. There is a clear point
in which the application touches on the operating system. The operating system can
change underneath without affecting the application, as long as the API remains stable.
We also have device drivers to abstract the low-level hardware and separate them from
higher levels of the operating system. Again, originally we have the operating system
on top of the hardware. More in detail, there is an additional layer in the middle that
will need to change if you change the hardware, but as long as the driver interface (e.g.,
for a generic class of devices) remains the same, the rest of the operating system is not
affected.
If we introduce virtualization, we add a few more layers. There are virtual machines
for specific programming languages that are running inside containers. There are guest
operating systems that run inside virtual machines. And you can have as many layers as
you want, so that if you make a change underneath what is above is not affected.
While we have seen such abstraction layers when we were discussing portability, here
we emphasize their benefits concerning the evolution of the system.
Users want to be able to upgrade the operating system to get performance improve-
ments, apply security patches and so on, but users do not want their applications to be
affected. If every time you ship an upgrade of the operating system you break all the ap-
plications, many users will be annoyed. Designing appropriate interface layers helps to
deal with this backwards compatibility requirement.
622

Layers
Presentation
Logic
State
User Interface
API
Business Logic
Object-Relational Mapper
Database
Another classical example is the layering of an architecture into: presentation for the
user interface interaction, the business logic, and then we have the persistence layer
where we separately store and manage the information processed by the application. The
persistence layer deals with consistency, concurrency control, optimized query planning,
and all the features that you get from a database.
Why does this layering help? If you make a change to the user interface because you
switch from using a command line terminal to a graphical user interface where you can
use the mouse to select menus, click on buttons and dialog boxes, and then you switch
to a touch based user interface, then you switch one more time to voice control but then
augmented or virtual reality becomes fashionable.
These are all superficial changes affecting the user interface of your system. They
should not affect the underlying logic nor the information content, only its visual repre-
sentation.
You can focus the impact of the user interface technology evolution on your system in
one place (the user interface layer) so that the rest can remain the same.
We can also zoom in so we can see that to isolate the business logic from the user
interface we can introduce again an API. We make an explicit interface. Likewise to iso-
late the business logic from the data storage we can have a dedicated mapping layer, like
for example an object relational mapper. This layer is going to take the memory data
structures that we use when we process the information and transform them so they can
be stored persistently. As a result, the business logic becomes independent from the
database schema. The representation that we use to store the data does not need to be
the same one that we use to process it.
623

Layers
External Data Representation
Validation, Mapping: Serialization/Deserialization
Internal Data Representation
Generalizing from the previous case, as a last example, we can also distinguish “Data
on the Outside versus Data on the Inside”.
The external representation for the information can be different from the internal one,
and this can happen every time you cross any component boundary, any interface in your
architecture. Each component has an inside and an outside. Any information that is
traveling between components will cross such boundaries. That is, the external data rep-
resentation is used to exchange information between components, which however may
store it as part of the state of the component using the internal representation, which is
local to each component.
You have the opportunity as you cross the boundary of the component to transform
between these two representations.
This allows you to couple the implementation of the component to the internal repre-
sentation, which has a local scope so that the implementation can evolve without being
coupled to the assumptions that you make on how the data is represented outside.
If you don’t take advantage of this opportunity, it means that every time you modify
your data (e.g., how it is represented, its syntax, structure and semantics) somewhere in
your architecture, every component will be affected. Every component will need to know
that the globally defined and universally shared data types have changed.
For example, the data structure to store the customer phone numbers has been up-
dated, so you have to rebuild all the components that deal with customer data. If you can
keep this change local within the internal data representation of one component, then
you keep components compatible and need to rebuild only one component.
Going from external to internal and vice versa, we find a mapping layer sandwiched in
between. This layer is responsible for serializing the internal data so that you can send as
the payload of an outgoing message, or deserializing data when you receive the message.
With every interaction, messages need to be received, buffered, read, parsed so that the
information needed by the component can be extracted and processed internally.
How much validation to do before or after this transformation? As the data enters (or
leaves) the boundary, there is an opportunity to check that what arrived from the outside
is correct. Ensure that it makes sense before you transform it into a suitable internal rep-
resentation. The layer not only builds a line of defense for design-time coupling between
data models but also can be used at run-time to detect illegal border crossings of invalid
messages.
The layers shown in this example help to reduce coupling because when change hap-
624

pens on either side, the only place that will be affected is the mapping layer. If the exter-
nal data representation has changed, adapting the mapping makes it possible to keep the
same internal one. Everything else within the component that depends on it will not be
affected. This is possible thanks to mapping layer which absorbs this change. Likewise, if
you want to change the way that you represent the data within your component, the rest
of the architecture is not affected because the mapping is going to transform the data on
the inside to the shared standard representation visible on the outside.
625

Tolerant Reader
Ignore unexpected content and fail only if required
content is missing
Tolerant Readers ignore new items, the absence of optional items, and
unexpected data values for which defaults can be used instead
How can clients or services function properly when receiving
some unexpected content?
Every time data is exchanged through the interface there is an opportunity to perform
validation, especially for what concerns external messages that arrive into your compo-
nent. Before accepting them, one should check them so that if the message goes past
the interface the rest of the system can make some assumptions about the content of the
message and will not have to worry that this might be an invalid or incorrect message.
When we do this validation we have two options: to be strict or to be lenient.
How can clients or services function properly when receiving some unexpected content
from the outside? The idea of introducing a tolerant reader is to ignore the unexpected
content and only reject the incoming message if content that is actually required is in-
valid or missing.
As opposed to saying that this message has one bit that doesn’t fit with my expecta-
tions, therefore I reject everything. If the bit that you didn’t expect is not something that
you actually depend on, then you can survive by ignoring it and just reading out what you
really need.
As we introduce a change in the structure of the messages that we exchange, and in
particular we add something new. We may wonder about whether the change is back-
wards and forwards compatible. If we have a tolerant reader, it is much easier to keep
the compatibility: any unexpected items (including the new ones) will be ignored. Also
if you have optional items that are missing, they will be ignored as long as the reader can
fall back to read a default value. Any value that is missing as well as out of range can be
reverted back to a default so that the tolerant reader can accept the input.
626

Tolerant Reader
Which kind of reader?
Strict
Crash as soon one bit is off
XML Parser
Fail if input validation fails no matter
what
Static Typing
Tolerant
Duck Typing
Survive non-critical invalid input
Validate the input only if actually
needed
Hide impact of malformed input
HTML Parser
Which type of behavior fits with the definition of tolerant reader? and which one is the
behavior of a strict reader?
A strict reader will crash as soon as one bit is off. And will fail if the input validation
fails no matter what, whereas the advantage of a tolerant reader is that it will only validate
the input which is needed. If something that is not needed is invalid, it will still accept
the message for downstream processing.
Attempting to hide the impact of malformed input can be seen as a good recipe for
being flexible and surviving the impact of change. It can also be a problem in case you
are trying to estimate or detect the impact of a change, because if you hide the impact of
such problematic changes then it will actually be more difficult to discover the original
source of such malformed input.
We can see two representative languages (HTML and XML): the behavior of the corre-
sponding parsers is a good example of each type of these readers. As most beginner Web
developers, you may have experienced that web browser is very forgiving when reading
strangely formatted HTML pages. Instead, if you try to parse XML messages, the same
kind of errors will not be tolerated. As soon as you forget to correctly nest one tag, then
the whole document will be rejected.
You can also think of the difference between languages that are strongly typed and
languages which are dynamically typed. You notice the difference when you try to take a
JSON document and you want to parse it into a Java object. If the structure of the JSON
document doesn’t fit your class definition, then there will be a problem to transform the
external representation, which uses a flexible representation format, into the internal
one which is much more constrained.
627

Let’s think about the future because the future will be here sooner than you think.
How does the future influence the design of our architecture?
There is a tension between your expectations about what the future will bring and
whether you should already act on them in the current version of your architecture.
We have already seen in a past lecture the term YAGNI (you are not going to need it)
and how this influences API design. Still, if there are some things that you can anticipate
that you will most probably need, then you might as well get your architecture ready for
supporting them in a future version.
If you might need it, you should prepare for it. You don’t have to build it yet, but it’s
a good idea to ensure your architecture is extensible so that these future needs can be
accommodated without having to redesign and rewrite the whole system from scratch.
628

Extensibility and Plugins
use plugins
Allow end-users to independently choose which components to deploy
anytime as long as they ѹt with the extension point interface
How to design an extensible architecture?
Plugins are a particularly useful mechanism to design an open and extensible archi-
tecture which can grow in anticipated ways.
With plugins, the choice of the components that make up the architecture of your sys-
tem is pushed and delegated all the way to the end users who are going to deploy and
run your architecture. The end users will choose which components – in this case we call
them plugins – they are going to configure your architecture to load, either on startup or
while it runs.
How do you control which components can be used? You can only use compatibility
as a constraint. So if users try to plug in something that is not compatible, clearly the
plugin is not going to work. But as long as it fits, the user can plug in any component
they want into your architecture.
Introducing a plugin system opens up your design of a highly flexible and extensible
system. It is also risky because the more the architecture can be extended by third parties,
the more you are willing to accept code developed by someone else. When something
goes wrong, you don’t know exactly who to blame. Adding unknown plugins from random
sources may void the warranty. If users attempt to extend your system in unsupported
ways, they breach your support contract. Not to mention if untrusted plugins act as trojan
attack vectors.
629

Extensibility and Plugins
Extensible
Component
Extension
Point
Extension
Plugin
Component
The architecture deѹnes explicit extension points so that the
system can be customized or even dynamically composed out of
plugins
Examples: Browser Extensions, Eclipse, Photoshop, WordPress
How does the plugin architecture work?
We are still in the context of a component-based architecture in which however you
define a specific type of interface which is called the “extension point”. This makes it
possible to design extensible components whose interfaces contain explicit extension
points in which other components can be connected (or plugged in) as long as they match
the assumption and the expectation encoded in the extension point interface.
We see this type of extensible architecture everywhere. Web browsers can be easily
extended and customized by downloading and installing so-called browser extensions.
Another example is the Eclipse technology platform where the whole development envi-
ronment is actually built out of plugins.
Opening up your system with plugins also can provide support for a business model
where you make money not because you sell the platform, but because you sell plugins.
Like Photoshop filters. Like WordPress, a content management system which is also the
foundation for a rich ecosystem of all kinds of extensions.
630

Extensibility and Plugins
Pre-requisite: Plugin interface description and discovery
Mechanism: Plug in and plug out
When are plugins plugged in?
• Static (conѹguration/deployment/installation)
• Startup
• Dynamic (without restarting the component)
Ecosystem: the extensible component becomes a platform where
alternative plugins compete
When are plugins initialized or selected? You can extend an architecture statically
when you install the system. As part of its configuration you can refer to a number of
pre-deployed plugins, which will be loaded when the system starts. Configuring which
plugins should be loaded can be as simple as storing the corresponding artifacts or pack-
ages in a given folder. Still, unless you stop and restart it, you will not be able to change
which plugins get loaded and activated at runtime.
We also have the option of a dynamic plugin system where the linkage between the
plugins and the base components is actually done at runtime. What’s interesting is that
you can plug in and also in some cases plug out components without having to restart
the whole architecture.
631

Extensibility and Plugins
Cardinality: Multiple Plugins can ѹt in the same extension points
Extensible
Component
Extension
Point
Plugin
Component
Extension
Point
Plugin
Component
Recursive: Plugins get extended by other plugins
In the simplest case, we have one component that is the foundation which can be ex-
tended by plugins. This design can be applied recursively. Not only we can have an ex-
tension point in which we plug multiple plugins, but we can also recursively plug plugins
into plugins. Plugins themselves, in other words, can be designed to be extensible and
offer their own extension points.
We have seen how to achieve extensibility with plugins and extension points. These
are solutions to design flexible architectures from the days of component-based software
engineering. Plugins assume that you physically download and install the components
to run them. As you do so, you can also download, configure and activate additional
plugins. Users enjoy the flexibility of this kind of open system because they can decide
how to extend it long after the original release has been shipped.
632

Martin Fowler and James Lewis
Microservices
The microservice architectural style is an approach to developing a
single application as a suite of small services, each running in its
own container and communicating with lightweight mechanisms,
often an HTTP resource API. These services are built around
business capabilities and independently deployable by fully
automated deployment machinery. There is a bare minimum of
centralized management of these services, which may be written
in different programming languages and use different data
storage technologies.
We have already discussed the evolution of components into services because of the
impact of the Internet on the software industry. While services focus on availability and
successful services also need scalability, what about flexibility?
We have now reached the need for the ultimate level of flexibility in which we not only
deliver the software as a service, but we do so in a way that it can be rapidly evolve and
it is easy to change.
Here you can read the classical microservice definition, in which we can see microser-
vices positioned as an architectural style: an approach to develop a single application
decomposed into multiple small services. So the definition builds on the notion of ser-
vice, as long as their size can be measured and be kept small.
Each of these microservices is deployed and runs in his own container. We already
discussed what containers are. Many of the concepts we have previously seen are coming
back. Each runs in a dedicated container so that it can have an independent operations
lifecycle.
To communicate, microservices use some lightweight connector mechanisms. For ex-
ample, some API based on HTTP. So this makes it possible for them to be deployed re-
motely. They don’t have to be all running in the same local environment, because they
can send to each other HTTP messages.
Then there is the sentence that describes how to decompose your architecture into
these microservices? Each of them should do something specific from a business point of
view. Each implements a different feature. Each delivers a different business capability.
Somehow these design heuristics should guide you in your quest to decompose them and
select which service you will need.
What is most important is that they are independently deployable. If each can be de-
ployed independently, your architecture is flexible. You can change a microservice and
you can deliver the change as soon as possible. You can release it, you can install it and
you can run it all independently of the other microservice that you have in your architec-
ture. This is fundamental and if it is not true for your architecture, then you can hardly
call its components microservices.
Since such deployment occurs often and you need to do it independently for many mi-
croservices, you shall do it supported by some automated deployment machinery. Yes,
633

we already have discussed how this works during the lecture on deployability with con-
tinuous build, integration, delivery and deployment pipelines.
You should also try to keep centralized management to a minimum. Every team that
you have is dedicated to an independent microservice and they need to coordinate only
concerning the interfaces, the microservice APIs. Regarding taking all other kinds of de-
cisions about how the microservice are implemented, for example, the choice of program-
ming language or the decision on: how are we going to store the data? Which database
should manage the state of our microservice? All of these decisions can be taken inde-
pendently.
Why is this important? There are many different programming languages. There are
many different storage technologies and you want to pick the best solution for the prob-
lem that you have. And it’s not true that if you use the same database, you will get the
same performance or the same expressiveness that you need to represent information in
the appropriate way for all microservices. The idea of using microservices is that you can
choose and pick the right technology for the job without any centralized constraint.
The only constraint which needs to be in place concerns the interfaces and the connec-
tors. If you want software written in different programming languages to be deployed you
can use Docker and if these heterogeneous components need to talk to each other, they
can speak HTTP. If you want to reliably change them, you use all the automated deploy-
ment machinery (containers) and to make it fast to evolve them, you must keep their size
small. Or at least, that’s according to this classical definition.
634

To share with you a visual image of the elements that we are going to discuss in this
lecture, I would like to introduce to you the Monolith.
The Big Monolith at the bottom represents the situation in which you do not want to
be. If you make a change to the monolith, you will do so slowly because it just takes too
much time and effort before you can make a new release for the whole thing. It will also
be very expensive to deploy it in the Cloud.
What we do is break it down into microservices, which are supposed to be small. Here
on top of the monolith you can see a couple of stacks of microservices. They are so
lightweight and easy to deploy, for example, as a cloud native application. If one stops
working, you can just throw it away and cut a better one.
635

Will this component
always terminate?
Development
function f() { 
     ... 
     return 42;
}
To understand how architectural abstractions have evolved all the way to microser-
vices, let’s take another look at components.
The main concern when developing software components is whether they are correct:
whether they give correct and timely results. Thanks to their interfaces, components
become reusable. They can also be interoperable thanks to standardized interfaces. We
can try to minimize their dependencies, and ensure they are self-contained.
The most challenging part of building high quality components is to prove that their
implementation is correct and just in case also write some tests to highlight the presence
of some bugs in case regressions may occur after releasing bug fixes.
You definitely want to avoid releasing components which when invoked will take for-
ever to reply.
636

Will this service
run forever?
Operations
while (true) {
     on f {  
       return f()  
     };
}
When you move to the cloud, when you publish your components on the Internet, then
you have another issue which is no longer to make sure that the component terminates
but it is actually the opposite. When a software component is delivered as a service, you
must ensure that it is provided with an acceptable level of availability.
This means that the service has to run forever.
You have to do whatever it takes to make sure that the service never stops. While the
service is running, clients will call the service and its developers then inherit the previous
challenge. The service has to be able to compute something and give an answer as fast
as possible, no matter how many concurrent clients are there.
We have discussed the concerns of scalability and availability before. These are quality
attributes achieved through operations. We’ve seen that you can introduce redundancy
into the architecture. You have to monitor heartbeats, you run a watchdog which will take
swift recovery actions when something fails to recover. Likewise, you will run an auto-
scaling controller which can allocate or deallocate resources depending on the workload.
Architects, developers and operators learn how to design, build and run service ori-
ented architectures which have to be available no matter what. And in some cases, if
they are successful, they also need to scale.
637

Will this microservice
continuously change?
DevOps
while (true) { 
     on f {  
-       return f()
+       //return f() 
+       return f2()
     };
}
As we evolve into microservices, we have another challenge ahead. We not only have
to build something that runs correctly and gives back a result; not only we have to keep
it available and possibly scale it, but we also have to be able to continuously change it.
The only way to be able to do all of the above requires developers and operators to
work together and close the software lifecycle. This is why it is called the development
and operation (DevOps) lifecycle. There’s no longer a boundary. There is no distinction
between the developers and the operations anymore since we are continuously changing
and redeploying our system.
It’s easier to do that if you keep the amount of changes that you do and the size of the
overall component as small as possible.
To summarize the differences between components and services: components are reusable;
they need to be deployed and installed and they are operated under the ownership and
control of who is using them. So typically you build a component, you release it. And
then you can forget about it, as it runs outside of your control.
If we make the component remotely accessible, to also give quality of service guaran-
tees about its availability we need to be in charge of running it, not only of developing it.
So we inherit all challenges component developer face, but also we become an operator
tasked to keep the service alive so that users can call it from all over the world.
If we can do all of that but also keep continuously evolving our system, then we have
a microservice architecture.
So I hope this has clarified the core differences between software components, services
and microservices. Do not worry too much about the size. We will see that it is not so
important. What is critical is this ability to continuously change all the elements of your
architecture in a way that doesn’t limit the rate of change and can push it to hundreds of
releases per day.
638

DevOps
Code
Code
Build
Build
Test
Test
Release
Release
Plan
Plan
Deploy
Deploy
Monitor
Monitor
Run
Run
Close the Feedback Loop between Operations and Development to
signiѹcantly speed up the development release cycle
Scripting of Automated Build, Continuous Integration, Testing,
Release, Deployment and Operations Management tasks
We have seen this DevOps cycle already. While before we focused on the deployment
transition between development and operation, with microservices we close the loop and
run it as rapidly as possible.
To do so, you need automation, you introduce continuous integration, build pipelines,
you ensure quality with tests. If you can make sure that making a release and a new
deployment is as easy as clicking a button. If there are no risks to try out a new version
with green/blue deployments, if doing experiments for A/B testing is not a major multi-
year, high-risk project but something common, which happens every day, then you are
already going through this cycle multiple times.
Adopting microservices implies adopting this type of practices and all the technologies
(automation, continuous build pipelines, containers) that go with them. The goal is to
be faster in doing iterations over the lifecycle. And if you can iterate faster, you can be
faster in improving your system as well as learning from your users by observing them,
getting explicit feedback and dealing with it.
Also you can afford to move fast and break things. It will happen: you will make mis-
takes, break things, but as fast as you break them you should also fix them. We have seen
you should be able to undo failed releases. Also with two in production you should not
have to expose all clients to a new service version.
Overall, with devops the goal is to move fast while keeping the quality high. Qual-
ity and speed are no longer a trade off, with the appropriate safety net of practices and
infrastructure.
639

Gregor Hohpe
DevOps Requirements
Conѹdence in the code (code reviews, automated tests, small,
incremental releases, feature toggles)
100% automated, dependable and repeatable deployment
Monitoring feedback (smoke tests, A/B testing, analytics)
Elastic runtime (dynamically adapt to workload changes)
Secure build and runtime (every change may introduce
weaknesses)
How can you check if you are ready to adopt microservices? Are you in a company
that not only wants to adopt this architectural style but also is willing to follow these
continuous development and release practices?
First, you need to have confidence in the quality of what you do. There are many tech-
niques which help boost your confidence. For example, you want to adopt code reviews.
I hope you have been hearing about what a code review is in some of the software en-
gineering lectures. You should also have automatic quality assurance. This requires to
write, run and maintain all kinds of tests: unit tests, integration tests, capacity tests,
performance and scalability tests. Regarding availability, you can use chaos engineering
to turn unlikely or infrequent events into regularly occurring ones: inject failures in a
controlled way and see if your system survives thanks to the automated recovery tools
you have introduced.
The larger your code, the more difficult it is to be confident about whether it meets its
functional and extra-functional requirements. Here is the microservice size argument
again: changes should be small so that their impact is easier to control. Also you want to
make frequent incremental releases. You do not want to make a Big Bang release every
year. You want to make continuous releases every few seconds. There is one technique
that we’re going to discuss that helps you to do that is called feature toggles.
Microservices cannot be adopted without continuous integration technology. 100%
automated means that the whole deployment and release process is reliable and repeat-
able. It does not depend on the presence or the good health of certain team members.
Everyone should be entrusted to push the code in production. And also everybody should
be responsible for the consequences. If things go wrong, you want to have a feedback loop
which makes it possible to observe what is happening, detect problems without or with
the help of your users so that you are able to react to them.
We saw, for example, that as soon as you activate the new release you have to run a
smoke test. You have to be able to quickly check if you did something stupid and you
640

want to actually pull the plug and revert back to the previous stable state. You want to
learn from your users, so you want to be able to do experiments with A/B testing.
In general, choose the right metrics so that you can measure the way your system op-
erates. You want to quantitatively observe and track users. This is no longer a piece of
software that someone in some unknown corner of the world takes off the shelf and in-
stalls on their machine which is disconnected from the Internet. Users instead are going
to interact with you every day. You get to know who they are and you can build a pro-
file and you can remember everything they ever did. With this knowledge you can try to
do all kinds of analysis that can give you actionable insight into – let’s put in a positive
light – how to improve your system so that they become more efficient at what they do.
A not-so-positive consequence now that you can observe and track your user is to study
their behavior and experiment with ideas on how to get more money out of their pockets,
to increase their engagement or addiction with your service, or to nudge them towards
paying for those extra features they may not actually need.
We’ve seen also that the runtime environment has not only to be highly available, but
we can expect that it would achieve elastic scalability: given a target performance and by
observing the workload, it can allocate or deallocate the resources that are necessary to
deliver that level of performance. As the workload is something that you cannot control
or predict, then you have to be able to adapt dynamically to changes in traffic.
What is also critical since you are doing these changes very frequently, you do not want
anybody to tamper with your build pipeline and inject malicious code into it. You have
to be really careful about the dependencies, where they come from and whether they do
something more than they claim to do. Today’s tools will happily fetch the dependencies,
package them together with your software as you ship it and deliver it to be deployed
everywhere. You have to trust the quality and safety of this code that you depend on.
More and more attacks nowadays are not focused on the systems in production because
they are well protected, after all the Cloud is secure, data centers are hard to get into.
But maybe the developer machines are weak from a security standpoint. Maybe the build
servers and the continuous integration pipelines are weak, so you attack those. And that’s
when you can inject into the new release, the backdoors that you can use later to get into
the system and exfiltrate sensitive information. You should invest into securing not only
the production environment, of course you have to protect the data you have to protect
the servers, but you also have to protect your build pipeline from which a stream of fresh
software releases flows on the servers.
As you can see, most of the concepts that we have introduced this semester are appli-
cable to microservices.
641

Feature Toggles
compose features using conѹguration
Activate or deactivate features with conѹguration switches, without having
to change, rebuild and redeploy the code
How to minimize the cost of undoing changes?
Let’s now focus on this particular technique for gradually and gracefully introducing
change into an architecture. Feature toggles help to keep the cost of introducing changes
low and also help to easily undo the effect of problematic changes.
You’re writing some code but you’re not so sure that this code is going to be an im-
provement. You should design your code in a way that it is as cheap as possible to add
or remove the feature. After you check if it works or if it doesn’t, you can still quickly go
back to the previous version.
To do so, configuration comes into play. We use configuration to postpone making
design decisions until our users are ready to make them.
Features can be activated or deactivated (toggled) as part of the system’s configura-
tion. The configuration composes the features that you want to use in production. Every
feature in your code is associated with configuration toggle, with a switch that you can
activate or deactivate. By definition, a configuration change is cheap because it does not
require you to recompile and reinstall the system, but only in the worst case to restart it.
642

Feature Toggles
Feature Toggle: active
Version 1
Version 1
Version 1
Version 1
Version 2
Version 1
Version 2
Version 2
Version 2
Version 2
Feature Toggle: inactive
Deploy intermediate 
release
Test ok
Test fails
Deploy new release
Given a stable, configurable system we can produce a new version with a new config-
uration option. Activating or deactivating such option is a very simple way to make the
feature come alive or remove it because it doesn’t work.
We are in this transition phase, going from a version to another. We know that when
we make this evolution step, the change is risky.
We’re going to build three different versions of the system. We start from the initial
version, where the new feature is missing. Then we have a version in which actually both
versions are embedded and the new feature can be activated or deactivated. And then
eventually we get to the new version, in which the new feature is stable, always present
and running.
Such hybrid, intermediary version is the one that contains also the feature toggle. In
the initial phase we are going to introduce the change, release it, but keep the change
inactive. In reality, we’re still running the old version. Users do not see the effect of your
change yet. But the change is already baked into your system. You may ask: what’s the
point of introducing a change into a system, building it, releasing it, but not using it?
Why would you want to go through all that? It is worth doing it, if you’re not yet sure
about the new version. You make a so-called Dark Launch. You launch the new version,
but the new feature is not visible. Still, you can check if the mere presence of the new
code has an effect on the previous version. You can see if there is some conflict between
a version assumed to be stable and the new code that you have added. The code is not
active, but if the original version is stable and this hybrid version is not stable without
running the new code yet, you know that there is already some problem. If you detect
some instability even before you activate the feature toggle, you can already go back to
version one and try again.
643

If, on the contrary, the dark launch result is still stable, then what you can do is to
finally switch the toggle. Now we go into a different state in which the new version is
active, so users can actually try to use it. And then you can observe what happens. This
transition is very cheap: just flip one bit to change and activate the new feature. If you
have a problem. It’s again only one bit that you have to switch to revert back to the
original version.
This idea really works if the switch between the two version is fast. If you have to re-
build your system to activate the feature then this is not a feature toggle. Feature toggles
are part of the configuration.
If you’re happy with the new version, then of course you can really make a stable new
release and it’s highly recommended to throw away the old version. Why do you want to
throw it away and not keep the feature toggle around? It’s meant as a rhetorical question:
You don’t need it anymore because now you trust the new version.
But if you want to keep your system flexible, why not be able to activate and deactivate
every feature all the time? What if is there is a version 3? Yes, you should be able to switch
between version 1, 2 and version 3. Keep in mind that this is not necessarily a version
toggle, but it’s a feature toggle. So for every feature that you introduce in your system,
you should be able to activate or deactivate the feature. How many possible versions are
there as a result of all possible combinations of each feature of your system? Here we
have two versions because we have one feature that can be active or inactive.
Imagine that you have F independent features and you want to control each of them
independently. How many versions do you need to test? How many possible versions or
variants of your architecture are there? Managing 2 to the power of F versions exponen-
tially grows to be a large number and it is simply not feasible to maintain such large and
fast-growing configuration space.
Feature toggles is a great technique to have flexibility during transitions between sta-
ble releases. You may think: once we built the feature toggle, we might as well keep it
over the entire history of our system, you know to keep the architecture flexible. Ok,
but you might end up with a combinatorial explosion of possible configurations, possi-
ble feature combinations, and if you really have to run tests for all of them, it will be very
expensive and take longer and longer to check all possible versions. So expensive that
you will probably not do it exhaustively and only manage to test the most frequently used
feature combinations.
There’s historical evidence of systems that failed because they forgot to remove the
feature toggle. After a number of years, somebody by mistake deactivated or activated
an old feature toggle and this led to a major failure in production. The toggle was still
there, but that particular combination was untested and they learned it wouldn’t work
the hard way.
644

Feature Toggles
• How to implement?
• Toggle Points: if, #ifdef, late binding, strategy, factory
• Toggle Routers: Conѹguration File, Conѹguration Store,
Environment Flag, Command Line Argument
• Context: A/B testing, Canary Releases
• When to toggle? On build, deploy, startup, live
• How to test? Alway run tests on both sides of the toggle
• Feature Toggle Combinatorial Explosion: remove toggles after
features become stable to keep the release validation
complexity under control and avoid potential unexpected
feature interactions
There are many ways that you can use to implement feature toggles. We can use com-
pilers, macros, simple if statements, late binding, virtual methods. Are you familiar with
the strategy pattern? You can also use the factory pattern to create an instance of differ-
ent classes depending on which feature is activated.
In general, you have variation points or branches into your code which you can use to
program how to toggle between an activated or deactivated feature. How do you control
what gets executed at those points? For example, an if statement is testing a value that
comes from a configuration file. This assumes that somewhere there is a file that you
specify using some language (nowadays YAML, but doesn’t have to be). This configu-
ration file may be centrally managed in a repository or in an operating system registry.
Depending on the platform there are conventions for locating config files, environmen-
tal variables, command line settings for each particular deployment (e.g., development,
testing, staging, production). There can also be specific databases that are just used for
storing configuration settings.
Configuration settings can be persistent, but also specified or overridden at startup
through environmental variables. They can be passed as command line arguments. Some
configuration changes can also be dynamically applied by the end users based on their
preferences.
For example, canary users are able to activate or deactivate feature toggles. They
signed a special agreement to indicate they are willing to risk it and run experimental
features on the latest version. They are aware that some feature toggles could be un-
stable but they’re willing to try it, knowing that they can easily revert back to the stable
version by changing a few flags.
This can also work with software as a service: when we get requests from such user,
based on their profile configuration we will activate the feature only for them, while all
the other users will run in the previous configuration with the feature that is not acti-
645

vated. This feature toggle is not about changing a config file option, but is about asso-
ciating the feature toggle with the user account or the API key that is sending you the
requests.
Depending on how the feature toggle is built we can constrain who can flip the switch
and when this can happen. Build time feature toggles typically use macro expansion
or similar metaprogramming constructs. Changes of configuration files can be done at
deployment time by installer scripts prompting operators. At startup, the system can
read command line arguments or environment flags. We can also ask users live, and this
helps to combine feature toggles with A/B testing or canary releases.
How do feature toggles interact with testing and quality assurance? Before and after
you activate a feature toggle you should run the tests. Green tests are a pre-requisite to
switch it on. Green tests are needed to keep it on.
After you are confident about the new feature, to avoid the feature toggle combinato-
rial explosion, remove the toggle. Once you trust the feature burn it, hard-wire it into
your system so that you cannot go back because you don’t need to go back anyway. The
more feature toggles you have open at the same time, the more variability you have and
the more difficult it is to keep unwanted feature interactions under control.
646

How small is a Microservice?
Full control of the devops (code, build, test, release, deploy and
operate) cycle by one team
Iterate fast: Many small frequent releases better than few large
releases
Rapid Evolution: If you have to hold a release until some other
team is ready you do not have two separate microservices
Avoid Cascading Failures: A failed microservice should not bring
down the whole system
Focused Investment: Scale and Replicate critical parts of the
system as opposed to the whole system
Let’s go back to the discussion about microservices and their size. What do we mean
by small? So how small is a microservice, really?
The limiting factor is how many developers you have or how many teams of developers
are there to build your architecture. Each team is responsible not only for writing the
code, but is also responsible for building and testing it, for making the release, deploying
it and operating the microservice. We are talking about a DevOps cross-functional team.
One of these teams will have between 5 and 10 people. What’s the largest possible
piece of software that a team of about 10 people can assemble? This already gives you a
limit on how large each microservice is going to grow. Then you can ask: is every team
going to run their own microservice? Or there are ambitious teams responsible for mul-
tiple microservices? For sure, you will not have more than one team working on the same
microservice. This would contradict the autonomy principle: every microservices is in-
dependent. Therefore you have an independent team that is going to both develop and
operate it.
Size also impacts the speed at which the team can iterate and incrementally grow their
microservice.
Let’s consider a client/server architecture. If you make a change in the server, can
you release it before the client has been upgraded? If client and server are developed
and operated by autonomous teams, one group can change the server without asking
the client team for permission. You have autonomy. You have the independence that
you need to set up a microservice architecture. If one microservice team is ready for
the release but they have to hold the release to ask for somebody else’s permission then
they’re not really independent microservices. There is a point in which their processes
block, their DevOps cycle has to synchronize with the other.
This is a very simple behavior to observe: We announce we want to make a change. We
will push the release tomorrow. If somebody calls you in the middle of the night and says
please, please don’t do it, because otherwise we’re going to be severely impacted by the
change, then you do not have a micro service architecture.
647

If you can make a change whenever you want wherever you want, then you manage to
keep your microservices decoupled and independent. This property is related with the
size, but it’s really a different property.
So far we have discussed examples of changes at design-time all the way to deploy-
ment time. Let’s talk about availability and run-time failures. One of the advantages of
microservices is that each microservices can fail independently of the others. If one fails,
then the rest will survive. While this is also known as partial failures, the idea is to avoid
bringing down everything if just one element fails. We’ve seen how to achieve this with
redundancy and also by introducing, for example, circuit breakers or asynchronous mes-
saging. These type of solutions help the whole system to survive loss of availability of
individual elements which can come and go, they can go up and down, but the rest of the
system should not be affected. If you have a monolith, the Monolith has the advantage
that you don’t have partial failures. Either everything works or nothing works. Of course,
if nothing works we have a major problem. With microservices, the idea is that most of
them would work most of the time. So users can still access the applications and their
workflow is not entirely compromised if only a few features or activities get disabled from
time to time.
If you have a hot spot – an element of your architecture that becomes a performance
bottleneck – you know where to invest to provide the additional capacity. Microservices
also help with scalability because can control each independently and decide how many
resources to allocate: how much computing power, how much storage, how much band-
width should be given to run this particular element. Overall this can be more efficient
as opposed to having to scale a large system. If you focus the investment, it would be
cheaper than having to come up with a fully redundant deployment for the entire sys-
tem, as opposed to just few critical elements.
These are all different forces that help us to keep the size of the microservice in check
because you want to encourage high frequency, incremental releases, therefore keep the
changes small. You want to grow the microservice, but not too much, otherwise you
would have to deal with all these synchronizations with other microservices that will
block your continuous stream of releases. You want to have something small that can
fail, but the rest can survive. If you make it too large there will be a limit on how much
you can grow. If you grow too big then if this part fails, maybe it’s so important that
the rest cannot survive and most users will start to notice as it may take a long time to
recover. The same is true for scaling: small microservices which become hotspots are
cheaper to scale up and scale out.
648

Continuous Evolution
Code
Large commit
1 month
Time
Small change
1 day
Another measurable aspect is the amount of change applied to a system. While the
overall system can be large or small, the change that we apply to it should be kept small. Is
it more likely that to make a change often, the change should be small. Only by changing
less frequently we can afford to apply larger changes.
On this picture we show the size as a function of time: the history of the growth of your
system measured with some metric counting the size of the code. It does not necessar-
ily grow monotonically. In most cases, the size will grow as more code, more features,
more extensions get integrated. But some changes may refactor, drop some features,
consolidate and reduce redundancy by shrinking the system.
Each point is measured when a new version is released. The vertical difference shows
the growth in the code, while the horizontal distance indicates how long it took to write,
build, test and release the new version, how long it took to apply the change.
We can make a big jump from the initial 1.0 version to the new 2.0 version. How long
did it take? Months, years? How much code was added or changed? Thousands, millions
of lines of code?
The alternative is to follow a continuous evolution path, in which we see that changes
are happening every day. But since we push multiple changes every day, there is only so
much that can change in one day. The amount of change will not be as large but it will
happen more often.
Which strategy will help you to reach the goal faster? A few big jumps or many small
ones? Are you sure you will end up in the same spot? In this chart, these two trajectories
are somehow overlapping: we may either make a big jump or a lot of small jumps, but we
actually end up in the same place. Also, we hope that we land in the right place.
What does a release bring? The opportunity to learn whether the work invested into
it works in production with actual users. Every time we make a new release, we have a
chance to fail, recover, and learn; whereas if you make one Big Bang release, you make a
big jump, the probability that you fail is much higher given the larger amount of change
involved. If you have to recover, maybe you have to work for a long time to fix it and
649

you will still struggle with the quality of a large fix. Also, experimenting with so many
changes together in one release will make it more difficult to observe the relationship
between individual changes and the resulting user satisfaction.
We have illustrated that small size of changes helps to gain confidence at every step
and iterations can thus proceed at a higher rate. The increased overhead of making more
releases (with small changes) compensates for the increased risk when making fewer re-
leases (with large changes).
650

Hexagon Model
UI
DB
Test
Test
Micro
Service
API
API
Micro
Service
Microservice
Microservice
Microservices are often visualized as hexagons. Where does this hexagon visualization
come from?
When representing an architecture either in the logical view or also sometimes in the
deployment perspective, you will see this shape associated with microservices. It looks
great, since you can tile many microservices side by side and it will look less boring than
the usual boxes.
However, the hexagon as a metaphor for components is even older. The idea is that
with a hexagon we have six sides and different sides represent different contexts in which
the component can be used.
The main vertical axis represents the production side of your system. When you put a
component in production it means that you have a user interface on top. If the compo-
nent is stateful (most of them usually are) it will use a database shown at the bottom to
store persistently its state. So this is the production axis: users access and process their
actual data.
Before you get to production, you are staging and testing your component and to do
manual tests through the user interface is expensive. Instead you run automated tests
that control the component. These are often encapsulated in test drivers and are shown
attached to the top right side of the hexagon. They exercise the code trying to cover 100%
of all the paths. When you run the tests, you shouldn’t do it on top of your production
database and potentially corrupt all the data. Instead, you want to attach as shown on
the bottom left side a copy of the database dedicated for testing. This has the advan-
tage that you can write test oracles which have expectations about the initial state of the
component. The visualization shows that for testing stateful components, they will be
attached to a test database and then the tests drivers will use it to check that expectations
are fulfilled when the component is using the test data. This is the quality assurance axis.
But this is a hexagon, so we have one more axis. That’s the integration axis. That’s
651

where we find the API of the component which will be used to integrate it with other
components.
The top left side represents the interface provided by our microservice to the rest of the
microservices which require it, while the bottom right side shows the opposite dependen-
cies, the ones required by our microservices and satisfied elsewhere in the architecture.
For unit testing, the microservice wouldn’t work without its dependencies: in the same
way stateful components will be bound to a test dataset, we can connect the microser-
vices to mock microservices, which will then be switched to the real dependencies for
integration testing.
This hexagonal component model – recently adopted for microservices – helps us to
distinguish three different types of interfaces along the production axis, the quality as-
surance axis and the integration axis. In general, elements attached to the top sides will
depend on our component, while elements attached to the bottom sides represent our
own dependencies.
652

Decomposition
Monolith
microservices
When we introduce microservices, the big challenge is how we transform an architec-
ture with a single element into one with multiple ones. While this is relatively easy to do
for the code following the good old maximize cohesion and minimize coupling rule, it is
more difficult to split the database. It’s more difficult to split a large schema with many
relationships into many small ones.
Once you have decomposed the monolith into multiple microservices you also need
to recompose them together using the right type of connectors. But first we go in the
opposite direction. We start from something large and we try to break it into little pieces.
653

Independent DevOps Lifecycle
Monolith
microservices
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Why do we want to do that? Because this way every one of those pieces will follow
its own independent development and operation life cycle. You will be able to introduce
changes into each of the microservices without affecting the life cycle of the others. In-
stead if you want to make a change in the monolith, you have to stop everything and
you have to restart the whole thing every time, and every change can potentially affect
everything else in the architecture, since there is no structure to contain and absorb the
impact of changes.
654

Isolated Microservices
Customer
Order
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Monolith
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Customer
Order
Code
Build
Test
Release
Plan
Deploy
Monitor
Operate
Microservices
One important constraint that is also part of the microservice definition regards what
kind of connections we can establish between. In this picture we can see that we are
using a shared database connector between two different components. They use the same
database to store their state as well as they can also communicate through this database.
The boundary surrounds both of them: on the left we are still talking about a modular
monolith. It is deployed and it uses a single database, but the code inside is already
modular. We can already distinguish the different subcomponents.
If we break this into microservices and then each of the code components will have
their own independent storage. The question is: how do these elements communicate?
There are two possibilities.
The Black one represents the correct solution in which one microservices asking the
other one for some information. The red arrow instead is bypassing the API of the mi-
croservices and directly going to the database to fetch the data. This mimics what orig-
inally happened in the original version. Here it was allowed because the database was
shared. With microservices the data is not meant to be shared at all, so using this con-
nector is actually a mistake.
Why is this such a big deal? It has to do with flexibility - today’s topic. It has to do with
how easy it is to introduce changes into the internal representation of the data owned
by each microservice. In both scenarios we have a schema for the database that will
be shared between the two components if both components access the same database.
With microservices we want to avoid that because if we do not share the schema, we can
change it without affecting external components. However, if a component directly reads
the information from the other database and we change the schema of this database, the
component will be affected.
Microservices should encapsulate their database behind their API so that from the out-
side you can still access the information, but you do not make any assumptions about the
way that they are represented inside. The constraint recommends using appropriate lay-
ers for having a compatible representation of shared data from the outside and keeping
the freedom to change the way it is represented inside the microservice.
655

Werner Vogels
Isolated Microservices
For us service orientation means encapsulating the data with the
business logic that operates on the data, with the only access
through a published service interface. No direct database access is
allowed from outside the service, and there’s no data sharing
among the services.
Introducing microservices requires to keep them isolated. This means that you use
encapsulation not only for the business logic but also for the data. The only way that
you can access the data inside the microservices is through its API, the service interface.
There is no data sharing, no direct database access is allowed.
This is one of those rules that if you want to call your architecture a microservice archi-
tecture, you have to follow. If for some reason you bypass the interface and you directly
extract the information from the database, you’re violating this design constraint. In the
short term, maybe you get an advantage. Maybe it’s faster to just send a query to the
database to read the data that another microservice needs. In the long term this will cre-
ate a problem, because it will prevent these microservices from independently evolving
the internal representation of its data.
The boundary you draw around the microservice does not only include the microservice
and its data but it should include all other microservices which share some assumption
about the data, which is no longer private as it is hidden inside the microservice but has
become shared potentially among all microservices of your architecture.
656

Martin Fowler
Splitting the Monolith
˾. Start from a working system with a modular, monolithic
architecture
˿. Decompose it into microservices as it grows too large for a
single team
It is faster to start with a working centralized system
Later it becomes easier to see where to draw the boundaries
How can we design a microservice architecture? Should we start from scratch intro-
ducing microservices? Or should we split an existing Monolith?
Let’s start from something that works but is just a single element. We notice that it is
now growing beyond the capacity of a single team. We hit a limit and we can no longer
manage the growth of this particular element. It is just too large and our velocity is
slowing down. This is a good starting point for decomposing it.
The messages is: don’t start from scratch with microservices, start with whatever it
takes to get it working. And once it works then you can improve the way you can evolve
it in the future by introducing the practices we have discussed so far. Microservices are
better for evolving our system, but they’re not as fast to start a new system from scratch.
How do we make the transition? Once you have a system that works but that is cen-
tralized, then you can start to see which possible boundaries you can draw around its
internal components. Along which lines can we split it?
This is an example of an architectural refactoring of a system. We are not going to
refactor the code. We will refactor the design so that we can make a transition from a
monolith that has grown too large into multiple microservices. As we split it into differ-
ent parts, the constraint is that we want to keep the client working during this transition.
From the client perspective, the system should be available all the time.
It’s like when they do some construction work on the highway but they cannot close
the highway. You want to keep the cars driving through. Or maybe a more appropriate
image regarding software is: you are flying an airplane and you decide that you need to
make a few changes but you cannot land anywhere to replace the engine.
657

Splitting the Monolith
Client
Monolith
Assumption: the Monolith should have some kind of interface
At the beginning we have a monolith. And we have some kind of an interface that
allows the client to interact with it.
This is the initial state of our architecture. The first thing that you should do is to
isolate the client from the system where you’re working on.
Splitting the Monolith
Proxy
Monolith
Client
Add an intermediate layer to intercept and redirect trafѹc
You should then introduce an intermediate layer so that we can control the communi-
cation between the two parties. We can intercept it and redirect it. This is done usually
through some kind of proxy. The HTTP protocol supports proxying for free. If the pro-
tocol is more complex, then you have to find a solution. In the worst case you can resort
to a man-in-the-middle attack.
658

Splitting the Monolith
Proxy
Monolith
Client
Micro
service
Carve out the ѹrst microservice and redirect requests towards it
To test if replacement works, run them in parallel and compare
results
Assumption: the microservice is stateless
Now that you have intercepted the traffic, you can start to carve out, extract, and pack-
age the first microservice. Redirect the relevant requests towards it.
The client depends on certain interface. Part of this interface is a subset we can im-
plement in the microservice. When the client wants to access that part, we will send the
request over to the microservice; when the client wants to access everything else, it still
goes back to the original system, from which here we see that we have been cutting off a
small piece. In reality the piece is still there, but we don’t use it anymore. Typically the
monolith you actually never touch, you are afraid to actually change it.
If you want to test how successful the replacement was, you can take the request, send
it to both sides and then compare the results. If the microservice and the monolith agree,
then you know that you have made a good replacement. If the microservice sends you
back a different result, you can blame it. The monolith is the ground truth. This helps to
catch mistakes in the transition using the original implementation as the oracle.
This is very easy to do if the microservice is stateless. You can just take the code and put
it in a new container; there is no state that you have to migrate. For stateful microservices
you also need to partition the database. You will need to extract a subset of the schema
which will be encapsulated within the microservice.
If you need to migrate part of the state of the monolith and bring it into the microser-
vice you can still follow this process, but at some point you will need to decide which
fork of the state will become the master version. While at the beginning the state within
the microservice is likely to drift and become incorrect. It will need to be often reseeded
based on the master copy maintained by the monolith. Eventually however you will stop
updating the state in a monolith after you fully trust the correctness of the microservice.
659

Splitting the Monolith
Proxy
Monolith
Client
Micro
service
Micro
service
Keep extracting microservices (most valuable, least risky ѹrst)
After first step, the first microservice is working. The client didn’t notice the switch.
We can try to repeat the process as many times as needed.
Here’s the second piece. We have prioritized the work to extract the most valuable
piece of code and the one that is least risky to repackage as a standalone microservice.
Sometimes during the transition we’re actually rewriting the code like from COBOL to
Java.
660

Splitting the Monolith
Client
Micro
service
Micro
service
Micro
service
API Gateway
This migration can be done gradually without affecting clients
Much easier to split stateless components than stateful ones
Only at the very end the monolith can be retired
This major undertaking can take a long time, but eventually you have managed to trick
the client into believing that is still talking to the old Monolith, while in reality behind
the scenes you have your microservice architecture and you have retired the Monolith.
This process can be done slowly and gradually and every step along the way you can check
that the clients do not notice.
661

In this lecture we have seen how to make architectures flexible. This included the mi-
croservice architectural style. The term emphasizes the small size of the software com-
ponents, with some even bringing mini-services or nano-services into play.
In my humble opinion, size doesn’t really matter. What matters is whether and to
which extent the elements in your architecture are coupled from each other. If they are
loosely coupled or not coupled, they can change independently and they can change of-
ten. Does size impact coupling? For sure, but you can also have large components which
have very low coupling and therefore can evolve independently or end up with many
small and tightly coupled components.
Another heuristic related to the size is known as the ”2-pizza team”. It should be pos-
sible to feed each microservice team with 2 pizzas. In Italy this would translate to maxi-
mum one developer and one operator. Elsewhere pizzas can feed a few more people.
As we have almost finished this lecture, let’s go back over the history of software archi-
tecture and see where some of these ideas that are being very popular nowadays with mi-
croservices come from. We will see that they are actually much older than the buzzword
they are associated with. This is a very common trend in our industry: repackaging old
ideas with new labels. It is easier to keep selling new technologies if you are just changing
their names. For students, this might be hard to believe because you’re trying to learn
the current state of the art, but after a few years of industry experience you will start to
observe these paradigm shifts sweeping through the technology landscape. You should
realize that the problems you are trying to solve always remain the same, so you can just
learn how to match problems with solutions no matter what they are called today.
662

Bezos's Mandate (2002)
˾. All teams will henceforth expose their data and functionality through
service interfaces.
˿. Teams must communicate with each other through these interfaces.
̀. There will be no other form of interprocess communication allowed: no
direct linking, no direct reads of another team's data store, no shared-
memory model, no back-doors whatsoever. The only communication
allowed is via service interface calls over the network
́. It doesn’t matter what technology they use. HTTP, Corba, Pubsub,
custom protocols — doesn’t matter.
̂. All service interfaces, without exception, must be designed from the
ground up to be externalizable. That is to say, the team must plan and
design to be able to expose the interface to developers in the outside
world. No exceptions.
Back almost 20 years ago, even before when the cloud was starting to appear, there
was a company in which one day, all the IT department got to work and received an email
from their boss.
The email informed them that from now on, every software developed in the team will
have to be exposed through a service interface. If you want to call other software provided
by other teams, you have to go through this interface. If they want to call your software,
they have to use its service interface. There is no other integration mechanism allowed.
You cannot directly read the data from somebody elses database. This is forbidden. You
cannot share data through memory. You cannot come up with any other kind of back
door. It is only possible and allowed to communicate through the front door: a service
interface. This is given as a constraint on an abstract level: call the service interface
across the network. It’s ok to use HTTP, CORBA, MQ, gRPC, you choose the protocol, you
pick the most appropriate connector. There is some flexibility in the choice of connector,
but you have to go through the service interface.
This constrain created a billion dollar business out at a very simple design decision. All
service interfaces without exception must be designed to be externalisable. What does it
mean? We need to be ready to support both internal and external clients. It means that
once you have an interface. Everyone may call the service. You can first build something
for internal use, and then later you can open it up to the rest of the world so that, for
example, you can make money with it. No exceptions.
If you, by the way, ignore these rules, you will be fired. That’s not part of the slide, but
it was actually part of the email.
This impulse started the service-oriented architecture trend which eventually evolved
into cloud computing, everything as a service and microservices.
663

Evans's Bounded Context (2004)
There is no single uniform model to design large systems. Models
are only valid and consistent within a given context boundary.
Some translation may be needed to cross boundaries.
From a similar time frame, domain-driven design (DDD) is also highly relevant to-
day, as with microservices we need to find a way to decompose a large system into small
pieces.
There is a limit not only in the number of people that you can work with to support the
system, but there is a limit also in the conceptual integrity of the content managed by the
system. If its domain grows beyond a certain size, its complexity will also grow as you
try to cover within the same system concepts that are becoming increasingly different.
Consider your natural language understanding skills. For example, you are born in
a certain village, you go to primary school everybody speaks the same language. Then
one day you will take a walk and you go to the next village and you start to realize that
people speak in a slightly different way. Eventually you grow up and move to the city.
Your accent is different than people can tell where you come from. One day you cross the
boundary to a new country and there the language is completely different.
The same happens with software systems: when they are young and small, they’re
very consistent because they just focus on doing precisely something that is very narrow.
Eventually they grow by adding more features, more concepts, more data and eventually
they hit a boundary. They need to inter-operate with another system and that’s when
you start to need to have a translation because the other side speaks a different language
to represent shared concepts. You can hardly mix data on the inside with data from the
outside.
Same holds with systems that are initially used only by one user. As the user commu-
nity grows, there is more and more pressure to support different use cases, personalized
scenarios. With a sufficiently large user population, it is impossible to find the only way
to do something or to keep doing things in the same way for everyone. The only way to
keep growing is to manage diversity by drawing boundaries.
Is there ever going to be the only Universal solution? Or just many failed attempts at
over-generalization.
664

Bob Martin's Single Responsibility
Principle (2003)
A class should have only one reason to change.
Gather together things that change for the same reason, and
separate those things that change for different reasons.
This is also a similar concept for keeping your components cohesive: the idea of the
single responsibility principle states that if you have a piece of software – like in this case
a class in object oriented programming, but one can also extend it to a component, or a
microservice – it should only have one reason to change.
If things change for the same reason, you keep them together (high coupling). And if
things change for different reasons, you separate them (low coupling)
This abstract advice can help you with your monolith decomposition efforts. Think
about who would request possible changes, trace their impact across the architecture and
bring together all elements that can be directly or indirectly affected by the requirements
provided by each key stakeholder.
665

M. Douglas McIlroy
UNIX Philosophy (1978)
Write programs that do one thing and do it well.
Write programs to work together.
Write programs to handle text streams, because that is a universal
interface.
Design and build software to be tried early, ideally within weeks.
Don't hesitate to throw away the clumsy parts and rebuild them.
We keep going back in time even further. In the early days of UNIX, they started to
reflect about its design. They didn’t call it architecture, but they wrote about the design
philosophy of the system. These ideas also resonate with today’s microservices, back
then they were called programs.
When you write some programs, each program should do one thing. And do it well.
There is no need to replace them with some other program if you have the best program
to do a certain thing, to solve your problem.
However, since they only do one thing, sometimes you have to combine them to ac-
tually do something useful. Since you cannot anticipate and should not constrain how
components can be composed, you have to write them so they can offer a universal in-
terface. Back then, the universal interface was textual data streams: standard input and
standard output. We still have it today. For composing distributed components, HTTP
could be considered as a universal interface.
As you start with a new project: design and build software to be tried and tested as
early as possible, ideally within one or two weeks. Do not wait for end of the project to
deliver the first result that finally works. You should have as early as possible, and then
you can keep growing it. Today we call this a minimum viable product (MVP).
Don’t hesitate to throw away the clumsy parts and rebuild them. This is also an idea
often cited with microservices. Since they are small, you can afford to throw them away
and rebuild them. If they become too large, you have invested so much into building
them and you will think twice before replacing them. If you hesitate before replacing a
microservice, it’s already grown beyond the limit of what keeps a microservice small.
666

Edsger W. Dijkstra
Separation of Concerns (1974)
One is willing to study in depth an aspect of one's subject matter
in isolation, for the sake of its own consistency, all the time
knowing that one is occupying oneself with only one of the
aspects.
But nothing is gained -- on the contrary! -- by tackling these
various aspects simultaneously.
The term “separation of concerns” is really old, but still helpful today as we struggle
with microservice decomposition.
Once you try to understand how to solve a problem, you want to separate different
aspects so that you can solve them in isolation. And this helps you to focus your problem
solving skills on something that is manageable. If you try to solve everything together at
the same time, then it will be very difficult to make it work.
Also, if you found multiple problems that can be solved independently, you can in-
volve multiple teams of developers to build the corresponding solutions, also known as
microservices.
667

Parnas's Criteria (1971)
One begins with a list of difѹcult design decisions or design
decisions which are likely to change. Each module is then
designed to hide such a decision from the others.
It should be possible to make drastic changes to one module
without a need to change others
Only three years after the Garmisch conference where the term software engineering
was born, David Parnas proposed his criteria to decompose systems into modules. At that
time they didn’t call them components. Microservices were yet to come.
You start by making a list of difficult design decisions, or what we have called archi-
tectural decisions. Hard to change later on, but which should make it possible to design
a flexible architecture.
Each module should be designed to hide such a decision from the others. This will
make it possible to change what’s inside one module without affecting the rest.
Encapsulation, modularity and information hiding lead to the constraint to use exclu-
sively the service interface as opposed to offer direct access to the underlying implemen-
tation. We can rewrite an entire microservice using a different programming language,
the rest will not notice because the service interface remains the same.
668

Conway's Law (1968)
Any organization that designs a system will inevitably produce a
design whose structure is a copy of the organization's
communication structure
We reach now probably the most cited concept when you hear a microservices talk in
the industry: Conway’s law, sometimes also mentioned as the “reverse Conway manoeu-
vre”.
Any organization which designs a software architecture will inevitably produce a de-
sign whose structure is the copy of the communication structure within the organization
itself.
If people are used to communicate in a hierarchical reporting structure, like where
every team talks to the manager, the managers talk to their manager, all the way up to
the CEO, the software that they design will actually look like a hierarchical decomposition
of components.
Taken to the extreme, depending on how large the organization is, the number of com-
ponents in the software architecture will be proportional to the number of developers,
the number of offices or the number of floors. If you would like the software architec-
ture to change, you will need to re-arrange the chairs around the table, or the cubicle
floorplan.
669

Conway's Law (1968)
Developers
UI Designers
System
Administrators
Database
Administrators
Testers
Support
Helpdesk
Users
Database
Database
Monolith
Monolith
UI
UI
The monolithic architecture shows an example in which development is kept separate
from operation. Developers build the system, they release it, and then they push it over
the wall to somebody else who is in charge of running it.
670

Conway's Law (1968)
Developers
UI Designers
System
Administrator
Database
Administrator
Testers
Users
Catalog
Catalog
Order
Order
Order
Order
Order
Order
Order
Order
Products
Products
Orders
Orders
API
API
API
API
Client User Interface
Client User Interface
Microservices
Microservices
Cross-Functional Team
Cross-Functional Team
Developers
System
Administrator
Database
Administrator
Tester
UI Designers
Users
Only by changing the organization of your team, you can actually achieve a flexi-
ble microservice architecture. Each microservice is developed and operated by a cross-
functional team with enough members to play all of the roles covering the whole life cycle
of the software.
671

Vogels's Lesson (2006)
The services model has been a key enabler in creating teams that
can innovate quickly with a strong customer focus.
Giving developers operational responsibilities has greatly
enhanced the quality of the services, both from a customer and a
technology point of view.
Why is it important to have cross-functional teams dedicated to each microservice?
Let’s go back to where we started from: service-orientation has been a key enabler in
creating teams that can innovate quickly with a strong customer focus.
Empowering developers with operational responsibilities also enhances the quality of
the services both from a customer as well as a technology point of view.
This is a polite way to say that if a developer gets a call in the middle of the night and
they have to fix their service which just went down, the developer will tend to be much
more careful in introducing bugs, performance or availability issues in their system as
opposed to when it’s somebody else’s problem to fix.
The same should hold for architects, I wonder why they never get to enter their beau-
tiful buildings and try to live inside them for a day or two.
672

References
• Stephan Murer, Bruno Bonati, Frank J. Furrer, Managed Evolution: A Strategy for Very Large
Information Systems, Springer, 2011, ISBN 978-3-642-01632-5
• Werner Vogels, 
,
ACM Queue, 4(4), June 30, 2006
• Melvin E. Conway, 
, Datamation, 14 (5): 28–31, April 1968
• Edsger W. Dijkstra, 
, In: Selected Writings on Computing: A
Personal Perspective, Springer, 1982. ISBN 0–387–90652–5.
• David L. Parnas, 
,
Communications of the ACM, 15(12): 1053-1058, December 1972
• Eric Evans. Domain-Driven Design: Tackling Complexity in the Heart of Software. Addison-
Wesley, 2004, ISBN 978-032-112521-7
• James Lewis, Martin Fowler, 
, 2014
• Sam Newman, Building Microservices, O'Reilly, February 2015, ISBN 978-1491950357
•
 (API Evolution Patterns)
Interviews Web Services: Learning from the Amazon technology platform
How do Committees Invent?
On the role of scientiﬁc thought
On the criteria to be used in decomposing systems into modules
Microservices
https://www.microservice-api-patterns.org/
 
 
 
 
 
 
 
 
 
 
 
 
673

